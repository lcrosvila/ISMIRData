[
    {
        "title": "Comparing RNN Parameters for Melodic Similarity.",
        "author": [
            "Tian Cheng 0001",
            "Satoru Fukayama",
            "Masataka Goto"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492529",
        "url": "https://doi.org/10.5281/zenodo.1492529",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/61_Paper.pdf",
        "abstract": "Melodic similarity is an important task in the Music Information Retrieval (MIR) domain, with promising applications including query by example, music recommendation and visualisation. Most current approaches compute the similarity between two melodic sequences by comparing their local features (distance between pitches, intervals, etc.) or by comparing the sequences after aligning them. In order to find a better feature representing global characteristics of a melody, we propose to represent the melodic sequence of each musical piece by the parameters of a generative Recurrent Neural Network (RNN) trained on its sequence. Because the trained RNN can generate the identical melodic sequence of each piece, we can expect that the RNN parameters contain the temporal information within the melody. In our experiment, we first train an RNN on all melodic sequences, and then use it as an initialisation to train an individual RNN on each melodic sequence. The similarity between two melodies is computed by using the distance between their individual RNN parameters. Experimental results showed that the proposed RNN-based similarity outperformed the baseline similarity obtained by directly comparing melodic sequences.",
        "zenodo_id": 1492529,
        "dblp_key": "conf/ismir/0001FG18",
        "keywords": [
            "Melodic similarity",
            "Music Information Retrieval",
            "Query by example",
            "Music recommendation",
            "Visualisation",
            "Local features",
            "Global characteristics",
            "Generative Recurrent Neural Network",
            "RNN parameters",
            "Distance between RNN parameters"
        ],
        "content": "COMPARING RNN PARAMETERS FOR MELODIC SIMILARITY\nTian Cheng, Satoru Fukayama, Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST), Japan\nftian.cheng, s.fukayama, m.goto g@aist.go.jp\nABSTRACT\nMelodic similarity is an important task in the Music In-\nformation Retrieval (MIR) domain, with promising appli-\ncations including query by example, music recommenda-\ntion and visualisation. Most current approaches compute\nthe similarity between two melodic sequences by compar-\ning their local features (distance between pitches, intervals,\netc.) or by comparing the sequences after aligning them.\nIn order to ﬁnd a better feature representing global charac-\nteristics of a melody, we propose to represent the melodic\nsequence of each musical piece by the parameters of a gen-\nerative Recurrent Neural Network (RNN) trained on its se-\nquence. Because the trained RNN can generate the identi-\ncal melodic sequence of each piece, we can expect that the\nRNN parameters contain the temporal information within\nthe melody. In our experiment, we ﬁrst train an RNN on\nall melodic sequences, and then use it as an initialisation\nto train an individual RNN on each melodic sequence. The\nsimilarity between two melodies is computed by using the\ndistance between their individual RNN parameters. Ex-\nperimental results showed that the proposed RNN-based\nsimilarity outperformed the baseline similarity obtained by\ndirectly comparing melodic sequences.\n1. INTRODUCTION\nMelodic similarity is a task to analyse the similarity be-\ntween melodies, which has been used for music retrieval,\nrecommendation, visualisation and so on. To compute the\nsimilarity, a melody is always represented by a sequence\nof monophonic, musical fragments/events (MIDI event,\npitch, etc.). Current approaches usually compare two\nmelodic sequences using the string edit distance [8, 9, 17],\ngeometric measures [19] and N-Gram based measures\n[5, 27]. Alignment-based methods are applied when two\nmelodic sequences are of different lengths [15, 23], or\nwhen events of two sequences are not corresponding to\neach other one by one [2]. Not only melodic sequence\nbut also melody slopes on continuous melody contours\nare aligned for comparing melodic similarity [28]. Read-\ners can refer to [25] for state-of-the-art melodic similar-\nc\rTian Cheng, Satoru Fukayama, Masataka Goto. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Tian Cheng, Satoru Fukayama, Masataka\nGoto. “Comparing RNN parameters for melodic similarity”, 19th In-\nternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.ity methods. The existing methods focus on local features\nextracted from melodic sequences, such as distances be-\ntween pitches or between subsets of melodic sequence (N-\nGram). In addition alignment is needed when two melodic\nsequences are not comparable directly.\nIn order to deal with these drawbacks, we propose to\ntrain a generative Recurrent Neural Network (RNN) on a\nmelodic sequence, and use the RNN parameters to repre-\nsent the melodic sequence. The proposed feature (RNN\nparameters) projects a melodic sequence to a point in the\nparameter space, having two characteristics described as\nfollows. Firstly, the feature is independent to the length of\nthe input melodic sequence because every sequence is rep-\nresented by its RNN parameters of the same dimension.\nSecondly, because the RNN can generate an identical se-\nquence, we can expect that the RNN parameters contain\nthe global, temporal information of the melody.\nIn our experiment, we ﬁrst train an RNN on all melodic\nsequences from 80popular songs as an initialisation. With\nthe initialisation, RNNs are trained on individual melodic\nsequences. All the networks are trained in tensorﬂow. We\ncompute the similarity between two melodic sequences by\nthe Cosine similarity of their RNN parameters. The results\nshow that the similarity based on RNN parameters outper-\nforms the baseline similarity obtained by comparing the\nmelodic sequences directly. To the best of our knowledge,\nthis is the ﬁrst study that uses parameters of generative\nRNNs for the purpose of computing melodic similarity.\n2. RELATED WORK\nIn this section, we introduce related work on RNN-\nbased melody generation models, and brieﬂy introduce re-\nsearches on word and sentence embedding for understand-\ning semantic meanings in natural language processing.\n2.1 RNN-based melody generation models\nWe discuss several state-of-the-art RNN-based melody\ngeneration models. The RNN-based generative models are\nusually applied with Long Short Term Memory (LSTM)\nunits in order to model a long time dependence, such as\nMelody RNN in Magenta [1] and folk-rnn [22]. Ma-\ngenta [1] uses 2-layer RNNs with 64 or 128 LSTM units\nper layer, while folk-rnn [22] uses a deeper network (RNN\nwith 3 hidden layers of 512 LSTM units for each layer).\nThe RNNs generate melody by predicting the next\nmelodic event based on its previous Nevents:\n[xt\u0000N;:::;x t\u00001]!xt;763Models Representation Architecture\nMagenta [1] MIDI event 2-layer RNN (LSTM)\nFolk-rnn [22] abc notation 3-layer RNN (LSTM)\nHierarchical bar proﬁle, beat 3 RNNs (2-layer LSTM)\nRNN [26] proﬁle and note for bar, beat and note\nTable 1 : Brief summary of RNN-based melody generation\nmodels.\nwherextdenotes the melodic event in time t. The melodic\nevent can be represented in many forms, for example MIDI\nevents [1], abc notation [22] and so on, as shown in Table 1.\nWith quantised time steps (in sixteenth notes, for example),\na melody can be represented as a sequence of pitches1or\nMIDI events (pitch onset, offset, and no event) [1].\nRhythm information can also be modelled for melody\ngeneration. One simple way is to concatenate beat infor-\nmation with the melodic event for each frame to feed into\nthe network [1]. There are also several hierarchical RNNs\nproposed with rhythm information. In [4], each note is\nrepresented by its pitch and duration, and 2 RNNs (rhythm\nand melody RNNs) are trained for duration and pitch, re-\nspectively. The rhythm network receives the current pitch\nand duration as inputs, and outputs the duration of the\nnext note. The melody network receives the current pitch\nand generated upcoming duration as inputs to generate the\npitch of the next note. [26] trains 3 RNNs for bar, beat, and\nnote, respectively. The ﬁrst RNN generated bar proﬁles.\nGenerated bar proﬁles are fed into the second network to\ngenerate beats, and then bar and beat proﬁles are fed into\nthe third network to generate notes.\nStudies of generative RNN models always list gener-\nated examples [1, 22] as results, or conduct a listening test\nfor evaluation [26]. We believe that the generative RNN\nactually learns something ‘musical’ and can be used for\nmusic analysis. In this paper we extend the utility of the\ngenerative RNN to represent a melody and evaluate it in a\nmelodic similarity task.\n2.2 Word embedding and sentence embedding\nIn natural language processing, word embedding and sen-\ntence embedding work on representing semantic meanings\nof words and sentences. There are two successful word\nembedding models introduced in [13, 14]: word represen-\ntations are learnt in order to predict surrounding words or\nto predict the current word by its content. In these ways,\nthe meaning of a word is related to its context. With the\nembedded words, a representative vector for a sentence (a\nsequence of words) can be learned at the same time of pars-\ning the sentence [21] or can be trained in a weakly super-\nvised way on the click-through data by making sentence\nvectors with similar meanings close to each other [18]. In-\nspired by word embeding, [11] learns to represent a para-\ngraph by predicting words in the paragraph using previous\nwords and a paragraph vector. The same paragraph vector\n1https://brangerbriz.com/blog/\nusing-machine-learning-to-create-new-melodies/is shared when predicting words in the paragraph and then\nis used to represent the paragraph.\nWe believe that word embedding may correspond to\nchord embedding [3, 12] in understanding music; and sen-\ntence embedding may correspond to representing a se-\nquence of chords (also an interesting topic to investigate).\nIn general, the musical meaning (of a sequence of pitches\nor chords) is less intuitive than the textual meaning (of a\nword or a sentence). Thus, it is more difﬁcult to learn a\ngood representation for a musical sequence. In this paper\nwe work on representing a melody (a sequence of pitches).\nWe train an RNN model to predict the current pitch by its\nprevious pitches in a melody and represent the melody by\nthe RNN parameters. To the best of our knowledge, this\nis the ﬁrst work to use network parameters directly as a\nrepresentation.\n3. TRAINING RNNS\nFor each melodic sequence, we train a generative RNN on\nit. The parameters of the trained RNN will be used as a fea-\nture to represent the melody. We ﬁrst train an initialisation\non all melodic sequences, and then train on individually\nmelodic sequences with the initialisation.\n3.1 Data\nWe conduct the experiment on the RWC Music Database\n(Popular Music) [7]. There is a subjective similarity study\n[10] undertaken on 80 songs (RWC-MDB-P-2001 No.1-\n80) of the RWC Music Database. In this study 27 partici-\npants are asked to vote the similarity (on melody, rhythm,\nvocals and instruments, respectively) for 200 pairs of clips\nafter listening to them. Each clip lasts for 30 seconds (start-\ning from the ﬁrst chorus starting time). For these pairs of\nclips, the similarity votes range from 0 to 27.2The larger\nthe vote is, the more similar the clips are. The melodic\nsimilarity matrix is shown in Figure 1, indicating the simi-\nlarity scores of 200 pairs of clips. The matrix is symmetric\nbecause if a is similar to b, it means that b is similar to a as\nwell. There are 400 non-zero values in the matrix (twice\nof 200 because of the symmetry).\nWe use the same 30-second clip as in the subjective\nstudy [10] from each song for training RNNs. We de-\nnote the clip from piece ‘RWC-MDB-P-2001 No.X’ as\nclip X, X2[1;80]. The melodic similarity results of this\nstudy [10] are used as the ground truth for evaluation.\n3.2 Arranging the training data\nWe train RNNs using the melody annotation of the RWC\nMusic Database (Popular Music) from the publicly avail-\nable AIST Annotation [6]. A melody in the annotation is\nrepresented as a fundamental frequency sequence in 10 ms\nframes as shown in Figure 2(a). We call the frames with\nfrequencies ‘melody frames’, and the frames without fre-\nquencies ‘silent frames’. We convert the frequencies ( f)\n2The dataset [10] has been publicly available on the web page of the\nRWC Music Database at http://staff.aist.go.jp/m.goto/\nRWC-MDB/AIST-Annotation/SSimRWC/ .764 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 201810 20 30 40 50 60 70 80\nClip No.1020304050607080Clip No.Figure 1 : The melodic similarity of 200 pairs of clips.\ninto pitches ( p– indicated by MIDI indices) for melody\nframes:\np= 69 + 12 log2f\n440: (1)\nThe histogram of the pitches in the training set is shown\nin Figure 3. We focus on pitches in 3 octaves ranging from\n43 to 78. Frames with pitches beyond this range are con-\nsidered as silent frames.\n3.2.1 Frame hop size\nThe original frames are arranged in a hop size of 10 ms.\nWe use a hop size of 50 ms (shown in Figure 2(b)) be-\ncause RNNs tend to repeat the previous frames with a small\nframe hop size.\n3.2.2 Skip silent frames\nBecause of the high ratio of the silent frames (shown in\nFigure 2(b)), there will be many invalid training samples\nwith a sequence of silent frames to predict a silent frame\nif we use all frames in the training data. Therefore, we\nsimply skip all the silent frames to discard those invalid\ntraining samples, resulting in a pitch sequence with only\nmelody frames (shown in Figure 5(b)).\nWe aim to look back for 2 seconds to predict the next\nframe. With a frame hop size of 50 ms, there are 40 frames\nin the input sequence: [xt\u0000N;:::;x t\u00001]!xt;N= 40:\n3.2.3 Zero-padding at the beginning\nWe ﬁnd if the ﬁrst training sample is [x0;:::;x 39]!x40;\nthen the generation of the ﬁrst 40 frames are not modelled\nin the RNN. In order to generate the whole sequence, we\nconcatenate a sequence of 40 silent frames in the front of\neach clip, with the ﬁrst training sample of [xS;:::;x S]!\nx0(xSis the silent frame padding in the front of the clip).\n3.3 Network architecture\nWe apply a network architecture similar to Megenta [1],\nbut with GRU cells instead of LSTM cells to reduce the\n(a) A sequence of fundamental frequencies.\n(b) A sequence of pitches\nFigure 2 : Melodic sequences with different frame hop\nsizes. Frames with values of 0 are silent frames.\nFigure 3 : The histogram of the pitches in the dataset.\nparameter dimensions. The RNN contains 2 hidden layers\nwith 64 GRU cells per layer. The output layer is a fully-\nconnected layer with a softmax activation function. The\ninputs are one-hot encoded vectors with a dimension of 37\n(36 pitches and a silent state). We hope the RNN can ﬁt\nthe individual pitch sequences as much as possible. In this\ncase, overﬁtting is intended and not a problem any longer;\nhence no drop out is applied.\nThe network is trained by minimising the cross entropy\nloss using Adam optimisation with learning rate of 0.001\n(other parameters of Adam are with default values in ten-\nsorﬂow).\n3.4 Initialisation and training on individual clips\nIn order to gain a consistent training, we use a ﬁxed ini-\ntialisation. The initialisation is trained on the training sam-\nples from all 80 clips for 100 epochs. Then with this ini-\ntialisation, we train an individual RNN on each melodic\nsequence for 500 iterations.3After data arrangement of\nSection 3.2, there are around 200-600 training samples for\n3An iteration means RNN parameters are updated once on a batch of\ntraining samples. In contrast, an epoch means a full training on all train-\ning samples. We use the iteration number to stop training because in this\nway RNN parameters are updated for the same times, hence more com-\nparable. However, when to stop training still needs further investigation.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 765Initialisation Individual RNNs\nNo. of RNNs 1 RNN 80 RNNs\nTraining data 80 clips each clip\nBatch size 512 64\nEarly stop 100 epochs 500 iterations\nTable 2 : RNN training settings.\n(a) Batch acc. for initialisation.\n (b) Batch loss for initialisation.\n(c) Batch acc. for training on clip 1.\n (d) Batch loss for training on clip 1.\nFigure 4 : Batch accuracies and losses of training for ini-\ntialisation and training on clip 1 with the initialisation.\nevery clip. We use a large batch size of 512 for initialisa-\ntion training because of a big number of training samples,\nand a smaller batch size of 64 for training for each individ-\nual sequence. Training settings are shown in Table 2.\nTraining for initialisation and training on clip 1 are\nshown in Figure 4. After training for initialisation, the\nbatch accuracy reaches 0.7 (Figure 4(a)) and the batch loss\ndecreases to around 0.8 (Figure 4(b)). After training on\nclip 1 with the initialisation, the batch accuracy further in-\ncreases from 0.7 to 1 (Figure 4(c)); and the batch loss re-\nduces from 0.8 to around 0.1 (Figure 4(d)). With the RNN\ntrained on clip 1, we can generate an identical melodic se-\nquence, as shown in Figure 5.\n3.5 Cosine similarity between RNN parameters\nThe parameter dimensions of an RNN are shown in Ta-\nble 3. The total number of parameters is 46;757.\nWe reshape matrices to vectors, and concatenate the\nvectors. The concatenated parameters of the initialisation\nRNN and RNNs trained on clip 3 and clip 80 are shown in\nFigure 6. The differences in parameters of different RNNs\nare subtle. The similarity between two clips is indicated\nby the Cosine similarity between their concatenated RNN\nparameters. The larger the Cosine similarity is, the more\nsimilar the clips are.\nIn the data arrangement stage (see Section 3.2), the\nmelody of a clip (30 seconds) is represented as a se-\nquence of pitches of 600 frames (including silent frames),\nas shown in Figure 2(b). We use the Cosine similarity be-\ntween two pitch sequences as the baseline similarity.\n(a) Generated pitch sequence.\n(b) Original pitch sequence of clip 1.\nFigure 5 : An identical pitch sequence generated by the\ntrained RNN.\nMatrix Dimension\ncell0/gru cell/gates/kernel (101, 128)\ncell0/gru cell/gates/bias (128)\ncell0/gru cell/candidate/kernel (101, 64)\ncell0/gru cell/candidate/bias (64)\ncell1/gru cell/gates/kernel (128, 128)\ncell1/gru cell/gates/bias (128)\ncell1/gru cell/candidate/kernel (128, 64)\ncell1/gru cell/candidate/bias (64)\nfully connected/weights (64, 37)\nfully connected/biases (37)\nall parameters 46,757\nTable 3 : Parameter dimensions.\n4. RESULTS ANALYSIS\n4.1 Evaluation metric and results\nIn the subjective similarity study, each clip is compared to\n4-6 other clips, usually 5 clips [10]. For example, clip 3 is\ncompared to clips as shown in Table 4(a). We measure the\nsimilarity of two clips by computing the Cosine similar-\nity between their RNN parameters. We compare the rank\nof votes to the rank of similarities for evaluation. For ex-\nample, as shown in Table 4(a), 8 people vote the melody\nof clip 80 is similar to that of clip 3, and 7 people vote\nthe similarity between clip 29 and clip 3. Based on these\nvotes we assume clip 80 is more similar to clip 3 than clip\n29. Thus, the Cosine similarity between clip 80 and clip\n3 should be larger than that between clip 29 and clip 3\nC(80;3)> C(29;3):We ﬁrst convert the similarity and\nvotes into ranks (as shown in Table 4(b)), and then use the\npair-wise evaluation metric–Kendall’s tau ( \u001c)– to compare\nthe ranks. For clip 3, the \u001cis 0.2 based on similarities be-\ntween RNN parameters, better than \u001c=\u00000:2based on766 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018(a) Parameters of the initialisation RNN\n(b) Parameters of the RNN trained on clip 3\n(c) Parameters of the RNN trained on clip 80\nFigure 6 : Parameters of different RNNs with subtle differ-\nences.\nsimilarities between pitch sequences.\nThe results for 200 pairs of clips are shown in Table 5.\nThe average \u001cs are 0.125 and 0.073 based on Cosine sim-\nilarities between RNN parameters and between pitch se-\nquences, respectively.4In the preliminary test, we found\nthat there is no improvement in performance by using a\ndimension-reducing technique, such as Principle Compo-\nnent Analysis (PCA), before computing Cosine similarity,\nor by using distances between eigenvectors (weighted by\neigenvalues) of parameter matrices.\n4.2 Visualisation\n4.2.1 Similarity v.s. vote\nWe assume if there are more votes on X than on Y when\ncomparing to A, then the X should be more similar to A\nthan Y . However, this may be too strict when votes are\nclose (8 on X and 7 on Y , for example). In order to show\nwhether there is a trend that the similarity value is larger\nfor pairs of clips with a higher vote in general, we show\nCosine similarity v.s. vote plots for RNN parameters and\nbaseline pitch sequences in Figure 7.\nWe know the RNN parameters of different clips are very\nsimilar to each other, as shown in Figure 6. Therefore, the\n4Using the Euclidean distance provides similar results as using the Co-\nsine similarity: 0.120 and 0.074 for RNN parameters and pitch sequences,\nrespectively.No. 80 29 59 62 5\nVotes 8 7 6 4 3\nCRNN 0.9975 0.9973 0.99717 0.9976 0.99725\nCpitch 0.6175 0.7146 0.6256 0.7097 0.6584\n(a) Cosine similarities between parameters of clips compared to clip 3.\nNo. 80 29 59 62 5 \u001c\nRVotes 1 2 3 4 5\nRRNN 2 3 5 1 4 0.2\nRpitch 5 1 4 2 3 -0.2\n(b) Ranks of Cosine similarities.\nTable 4 : Evaluation for clip 3. CRNNandCpitchare the\nCosine similarities between parameters and between pitch\nsequences, respectively.\nSimilarity \u001c\nCRNN 0.125\nCpitch 0.073\nTable 5 : Results.\nCosine similarities between RNN parameters are in a small\nrange from 0.995 to 0.999 (Figure 7(a)). The Cosine sim-\nilarities between melodic sequences are in a larger range\nfrom 0.4 to 0.9 (Figure 7(b)). However, neither RNN pa-\nrameters nor melodic sequences provide a clear trend of\nthe similarity increasing with number of votes.\n4.2.2 t-SNE\nTo visualise the 80 songs in a low-dimensional space, we\nﬁrst reduce the dimension of the features to 5 by PCA, then\nfurther reduce it to 2 by t-SNE, with the implementation\nof [20]. The visualisation based on RNN parameters and\npitch sequences is shown in Figure 8. For a clearer visu-\nalisation, we only indicate pairs of clips with higher votes\n(above 9 votes out of 27, as listed in Table 6) by connecting\nthose pairs with lines.\nBecause the t-SNE visualisation is not a linear pro-\njection from the similarity to the distance on the 2-\ndimensional space, we do not compare the vote against the\ndistance between two clips in t-SNE visualisation, but fo-\ncus on the grouping of clips. We observe some interesting\ngrouping of clips in Figure 8(a): the triangle at the top left\nfor (75, 79, 80), and two lines at bottom right connecting\n(15, 16) and (6,16). In Figure 8(b), no such grouping of\nclips can be obviously observed.\n5. DISCUSSIONS AND CONCLUSIONS\nFrom the t-SNE visualisation, we observe some interesting\ngrouping of clips based on RNN parameters (Figure 8(a)).\nHowever, visualisation based on the Cosine similarity be-\ntween RNN parameters does not show a clear relation be-\ntween the similarity and the vote (Figure 7(a)). It mayProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 7670 5 10 15 20\nNumber of votes0.9950.99550.9960.99650.9970.99750.9980.99850.999Cosine similarity between RNN parameters(a) Visualisation based on RNN parameters\n0 5 10 15 20\nNumber of votes0.40.50.60.70.80.9Cosine similarity between pitch sequences\n(b) Visualisation based on pitch sequences\nFigure 7 : Similarity v.s. vote plot based on different fea-\ntures.\nindicate that a direct comparison between RNN parame-\nters is too simple to infer the information in such a large\ndimension. Figure 6 also illustrates the difﬁculties with\nthe proposed approach, too many parameters with sub-\ntle differences. We would like to dig deeper to under-\nstand which parameters are most signiﬁcant for computing\nmelodic similarity.\nPerception studies show that changes in relative scale\nor relative duration do not have a major impact on melodic\nsimilarity [24]. The similarity measure should be invariant\nto music transformations, such as transposition in pitch and\ntempo changes [16,23]. The proposed generative RNN can\nmodel the input pitch sequence, but cannot deal with the\nNo. Pair V ote No. Pair V ote No. Pair V ote\n1 (79, 80) 23 11 (10, 63) 13 21 (10, 52) 11\n2 (47, 68) 19 12 (47, 76) 13 22 (7, 20) 10\n3 (65, 78) 18 13 (51, 63) 13 23 (7, 45) 10\n4 (6, 16) 17 14 (51, 77) 13 24 (29, 60) 10\n5 (12, 47) 16 15 (64, 66) 13 25 (47, 67) 10\n6 (12, 63) 16 16 (7, 49) 12 26 (70, 71) 10\n7 (15, 16) 16 17 (19, 20) 12 27 (75, 79) 10\n8 (67, 75) 16 18 (41, 43) 12 28 (75, 80) 10\n9 (54, 63) 15 19 (42, 44) 12\n10 (72, 75) 15 20 (68, 72) 12\nTable 6 : A list of pairs of songs with similarity votes above\n9 votes out of 27.\n-40 -30 -20 -10 0 10 20 30 40 50-60-40-20020406080\n123\n45\n67\n8\n910\n1112\n1314\n15161718\n192021\n22\n232425\n26\n2728\n29 30\n313233\n34\n35\n363738\n39\n404142\n434445\n4647\n484950515253\n5455\n56\n5758\n5960\n6162\n63\n646566\n676869\n70 71\n72\n737475\n7677\n7879\n80(a) Visualisation based on RNN parameters\n-800 -600 -400 -200 0 200 400 600-400-300-200-1000100200300400\n1\n23\n45\n678\n9\n10111213\n14\n15\n1617\n18\n1920\n212223\n24\n2526\n2728\n29\n30\n3132\n33343536\n37\n3839\n4041 4243\n44\n4546\n474849\n505152\n535455\n5657\n58596061\n62\n6364\n656667\n6869\n7071\n72\n737475\n76777879\n80\n(b) Visualisation based on pitch sequences\nFigure 8 : t-SNE visualisation based on different features.\nsimilarity under music transformations. In the future, we\nwould like to tackle this problem by training RNNs with\ncoordinate differences instead of absolute coordinates as\ninputs, such as intervals and durations instead of pitches\nand onsets [16].\nWe work on the melodic similarity based on the\nperformance-based representation of melodies, which\nseems to complicate the task. We hope we can achieve\nmore success on symbolic melody representation by using\nscore-based representation on a simpler dataset.\nIn this paper, we propose to represent a melodic se-\nquence by the parameters of its corresponding generative\nRNN, and test the utility of the melodic feature (RNN pa-\nrameters) in the melodic similarity task. The proposed fea-\nture contains temporal information within the melodic se-\nquence, and independent of the length of the sequence. We\nextend the utility of generative RNNs to use the network\nfor music similarity analysis rather than music generation.\nWe expect that the proposed feature (generative RNN pa-\nrameters) can be used in other tasks, such as musicological\nanalysis and music cognition.768 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20186. ACKNOWLEDGEMENT\nThis work was supported in part by JST ACCEL Grant\nNumber JPMJAC1602, Japan.\n7. REFERENCES\n[1] Magenta: Melody RNN. https://github.\ncom/tensorflow/magenta/tree/master/\nmagenta/models/melody_rnn . Accessed:\n2018-03-30.\n[2] D. Bountouridis, D. G. Brown, F. Wiering, and R. C.\nVeltkamp. Melodic Similarity and Applications Using\nBiologically-Inspired Techniques. Applied Sciences ,\n7(12), 2017.\n[3] G. Brunner, Y . Wang, R. Wattenhofer, and J. Wiesen-\ndanger. JamBot: Music Theory Aware Chord Based\nGeneration of Polyphonic Music with LSTMs. In Pro-\nceedings of the 29th International Conference on Tools\nwith Artiﬁcial Intelligence (ICTAI) , 2017.\n[4] F. Colombo, S. P. Muscinelli, A. Seeholzer, J. Brea,\nand W. Gerstner. Algorithmic Composition of\nMelodies with Deep Recurrent Neural Networks. Com-\nputing Research Repository (CoRR) , abs/1606.07251,\n2016.\n[5] J. S. Downie. Evaluating a Simple Approach to Mu-\nsical Information retrieval: Conceiving Melodic N-\ngrams as Text . PhD thesis, University of Western On-\ntario, 1999.\n[6] M. Goto. AIST Annotation for the RWC Music\nDatabase. In Proceedings of the 7th International Con-\nference on Music Information Retrieval (ISMIR) , pages\n359–360, 2006.\n[7] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC Music Database: Popular, Classical, and Jazz\nMusic Databases. In Proceedings of the 3rd Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR) , pages 287–288, 2002.\n[8] M. Grachten, J.-L. Arcos, and R. L. de Mantaras.\nMelodic Similarity: Looking for a Good Abstraction\nLevel. In Proceedings of the 5th International Society\nof Music Information Retrievall (ISMIR) , 2004.\n[9] P. Hanna, P. Ferraro, and M. Robine. On Optimiz-\ning the Editing Algorithms for Evaluating Similarity\nBetween Monophonic Musical Sequences. Journal of\nNew Music Research , 36(4):267–279, 2007.\n[10] S. Kawabuchi, C. Miyajima, N. Kitaoka, and\nK. Takeda. Subjective Similarity of Music: Data Col-\nlection for Individuality Analysis. In Proceedings of\nAsia Paciﬁc Signal and Information Processing As-\nsociation Annual Summit and Conference , pages 1–5,\n2012.[11] Q. Le and T. Mikolov. Distributed Representations\nof Sentences and Documents. In Proceedings of the\n28th International Conference on Machine Learning\n(ICML) , 2014.\n[12] S. Madjiheurem, L. Qu, and C. Walder. Chord2Vec:\nLearning Musical Chord Embeddings. In Proceedings\nof the Constructive Machine Learning Workshop at\n30th Conference on Neural Information Processing\nSystems (NIPS) , 2016.\n[13] T. Mikolov, K. Chen, G. S. Corrado, and J. Dean. Ef-\nﬁcient Estimation of Word Representations in Vector\nSpace. In Proceedings of International Conference on\nLearning Representations (ICLR) Workshop , 2013.\n[14] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado,\nand J. Dean. Distributed Representations of Words and\nPhrases and their Compositionality. In Proceedings of\nAdvances in Neural Information Processing Systems\n26 (NIPS) , pages 3111–3119, 2013.\n[15] D. M ¨ullensiefen and K. Frieler. Optimizing Measures\nof Melodic Similarity for the Exploration of a Large\nFolk Song Database. In Proceedings of the 5th Interna-\ntional Society of Music Information Retrievall (ISMIR) ,\npages 1–7, 2004.\n[16] D. M ¨ullensiefen and K. Frieler. Evaluating Different\nApproaches to Measuring the Similarity of Melodies.\nIn et al. V . Batagelj, editor, Data Science and Classiﬁ-\ncation , pages 299–306. Springer, Berlin, 2006.\n[17] K. S. Orpen and D. Huron. Measurement of Simi-\nlarity in Music: A Quantitative Approach for Non-\nparametric Representations. Computers in Music Re-\nsearch , 4:1 – 44, 1992.\n[18] H. Palangi, P. li, Y . Shen, J. Gao, X. He, J. Chen,\nX. Song, and R. Ward. Deep Sentence Embedding\nUsing Long Short-Term Memory Networks: Analysis\nand Application to Information Retrieval. IEEE/ACM\nTransactions on Audio, Speech and Language Process-\ning (TASLP) , 24(4):694–707, 2016.\n[19] M. W. Park and E. C. Lee. Similarity Measurement\nMethod between Two Songs by Using the Conditional\nEuclidean Distance. Wseas Transaction On Informa-\ntion Science And Applications , 10(12), 2013.\n[20] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duches-\nnay. Scikit-Learn: Machine Learning in Python. Jour-\nnal of Machine Learning Research , 12:2825–2830,\n2011.\n[21] R. Socher, C. Y . Lin, A. Y . Ng, and C. D. Manning.\nParsing Natural Scenes and Natural Language with Re-\ncursive Neural Networks. In Proceedings of Interna-\ntional Conference on Machine Learning (ICML) , 2011.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 769[22] B. L. Sturm, J. F. Santos, and I. Korshunova. Folk\nMusic Style Modelling by Recurrent Neural Networks\nwith Long Short Term Memory Units. In Extended ab-\nstracts for the Late-Breaking Demo Session of the 16th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2015.\n[23] J. Urbano, J. Llor ´ens, J. Morato, and S. S ´anchez-\nCuadrado. MIREX 2012 Symbolic Melodic Similarity:\nHybrid Sequence Alignment with Geometric Repre-\nsentations. In Music Information Retrieval Evaluation\neXchange (MIREX) , 2012.\n[24] M. R. Velankar, H. V . Sahasrabuddhe, and P. A. Kulka-\nrni. Modeling Melody Similarity Using Music Syn-\nthesis and Perception. Procedia Computer Science ,\n45:728 – 735, 2015.\n[25] V . Velardo, M. Vallati, and S. Jan. Symbolic Melodic\nSimilarity: State of the Art and Future Challenges.\nComputer Music Journal , 40(2):70–83, 2016.\n[26] J. Wu, C. Hu, Y . Wang, X. Hu, and J. Zhu. A Hierar-\nchical Recurrent Neural Network for Symbolic Melody\nGeneration. Computing Research Repository (CoRR) ,\nabs/1712.05274, 2017.\n[27] S. Yazawa, Y . Hasegawa, K. Kanamori, and\nM. Hamanaka. Melodic Similarity Based on Extension\nImplication-Realization Model. In Music Information\nRetrieval Evaluation eXchange (MIREX) , 2013.\n[28] Y . Zhu, M. Kankanhalli, and Q. Tian. Similarity\nMatching of Continuous Melody Contours for Hum-\nming Querying of Melody Databases. In Proceedings\nof IEEE Workshop on Multimedia Signal Processing ,\n2002.770 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "On the Relationships between Music-induced Emotion and Physiological Signals.",
        "author": [
            "Xiao Hu 0001",
            "Fanjie Li",
            "Tzi-Dong Jeremy Ng"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492425",
        "url": "https://doi.org/10.5281/zenodo.1492425",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/115_Paper.pdf",
        "abstract": "to optimize emotion-aware music retrieval.  Emotion-aware music information retrieval (MIR) has been difficult due to the subjectivity and temporality of emotion responses to music. Physiological signals are regarded as related to emotion and thus could potentially be exploited in emotion-aware music discovery. This study explored the possibility of using physiological signals to detect users' emotion responses to music, with consideration of individual characteristics (personality, music preferences, etc.). A user experiment was conducted with 23 participants who searched for music in a novel MIR system. Users' listening behaviors and self-reported emotion responses to a total of 628 music pieces were collected. During music listening, a series of peripheral physiological signals (e.g., heart rate, skin conductance) were recorded from participants unobtrusively using a researchgrade wearable wristband. A set of features in the time- and frequency- domains were extracted from the physiological signals and analyzed using statistical and machine learning methods. Results reveal 1) significant differences in some physiological features between positive and negative arousal and mood categories, and 2) effective classification of emotion responses based on physiological signals for some individuals. The findings can contribute to further improvement of emotion-aware intelligent MIR systems exploiting physiological signals as an objective and personalized input.",
        "zenodo_id": 1492425,
        "dblp_key": "conf/ismir/0001LN18",
        "keywords": [
            "emotion-aware music retrieval",
            "physiological signals",
            "emotion responses",
            "music preferences",
            "user experiment",
            "peripheral physiological signals",
            "wearable wristband",
            "time- and frequency- domains",
            "statistical and machine learning methods",
            "effective classification"
        ],
        "content": "On the Relationships between Music -induced Emotion  \nand Physiological Signals  \nXiao Hu  Fanjie Li  Jeremy T. D. Ng  \nThe University of Hong Kong  Shenzhen \nInstitute of Research and Innovation  \nxiaoxhu@hku.hk  Sichuan University   \nfjlichn@gmail.\ncom The University of  Hong Kong  \njeremyng@hku.hk  \nABSTRACT  \nEmotion -aware music information retrieval (MIR) has \nbeen difficult due to the  subjectivity and temporality of \nemotion responses to music. Physiological signals are re-\ngarded as related to emotion  and thus could potentia lly be \nexploited in emotion -aware music discovery. This study \nexplored the possibility of using physiological signals to \ndetect users’ emotion responses to music , with considera-\ntion of individual characteristics (personality, music pref-\nerences, etc.). A us er experiment was conducted with 23 \nparticipants who searched for music in a novel MIR sys-\ntem. Users’ listening behaviors and self -reported emotion \nresponses to a total of 628 music pieces were collected. \nDuring music listening, a series of peripheral phys iologi-\ncal signals (e.g., heart rate, skin conductance) were rec-\norded from participants unobtrusively using a research -\ngrade wearable wristband. A set of features in the time - \nand frequency - domains were extracted from the physio-\nlogical signals and analyzed  using statistical and machine \nlearning methods. Results reveal 1) significant  differ-\nences in some physiological features between positive \nand negative  arousal and mood categories, and 2) effec-\ntive classification of emotion  responses based on physio-\nlogical  signals for some individuals. The findings  can \ncontribute to further improvement of emotion -aware in-\ntelligent MIR systems  exploiting physiological signals as \nan objective and personalized input.  \n1. INTRODUCTION  \nMood -based music discovery is a typical scenari o of mu-\nsic information retrieval (MIR).  Previous research has \nadopted content -based [11][27], collaborative filtering  \n[7], or semantic -based [4] approach to recognize the emo-\ntion the music expressed and therefore enable emotion -\naware  MIR . Beyond research, Web -based music services \nare also available to support searching for music based on \nemotions, such as MoodFuse, Musicovery, etc.  \nHowever, emotion responses  to music are subjective, \nvarying from one user to another. They are also temporal \nin that the same user may respond to the same music dif-\nferently at different times  [33]. These  make it challenging  to optimize em otion -aware  music retrieval . \nPhysiolog ical signals such as heart rate, blood pressure \nand skin conductance were found to be related to people’s \nemotion status [1][12][20] and they are deemed objective \ncompared to self -reported emotion status which has been \ncriticized as being subjective and sometimes inaccurate  \n[2][3]. Therefore, physiological signals  could potenti ally \nbe exploited in emotion -aware music discovery.  In addi-\ntion, with the rapid development of wearable technology  \nin recent years , peripheral physiological signals  can be \ncollected  from users  through small and less noticeable \ndevices (e.g., wristband)  in naturalistic settings in unob-\ntrusive manners  [13]. This advantage  is not yet compara-\nble by methods of gathering  other physiological signals \nsuch as eye tracking and electroencephalography , and \nthus periphe ral physiological signals  are currently prefe r-\nable for studying users’ emotion re sponses to music  in \neveryday life . \nThis study , therefore , aims to explore to what extent  \nphysiolog ical signals measured by a wearable device  are \nrelated to users’ emotion  responses to music played  on an \nMIR  system . Specifically , we are i nterested in the follow-\ning research questions:  \nRQ1:  Among features extracted from  physiological \nsignals collected during music listening, which ones dif-\nfer significantly  across  different  emotion responses ?  \nRQ2:  To what extent can  physiological signal s col-\nlected during music listening be used to predict users’  \nemotion response to music?  \nAs emotion responses to music may vary across listen-\ners [37], we take into consideration lis teners’ characteris-\ntics by asking the following question:  \nRQ3: To what extent do  prediction performances vary \nacross different user s and user  characteristics (i.e. person-\nality, music preferences )?  \nTo answer these questions , a user experiment was \ncond ucted  to collect  data of  users’ interac tions with  a \nnovel MIR system  [17]. During the experiment, partici-\npants were asked to explore the music collection in the \nsystem  while users’  music listening behaviors and self-\nreported emotio n responses to the music were recorded  \nby the s ystem . Simultaneously,  physiological signals of \nthe users were collect ed using a research -grade wearable \nwristband.  Statistical tests and machine learning classifi-\ners were applied  to analyze the data, with cla ssification \nperformances compared across different classification  © Hu, X. Li, F. Ng, J. Licensed under a Creative Commons \nAttribution 4.0 International License (CC BY 4.0). Attribution:  Hu, X. \nLi, F. Ng, J. “On the Relationships between Music -induced Emotion  and \nphysiological signals”, 19th International Society for Music Information \nRetrieval Conference, Paris, France, 2018.  \n362  \n \nalgorithms and users . Furthermore, collected in the pre -\nexperiment questionnaire , participants’ personality and \nmusic preferences were analyzed to see if they played a \nrole in the relationsh ips between physiolog ical signals \nand emotion responses to music . As one of the first stud-\nies exploiting peripheral physiological signals  in MIR,  \nfinding s of this study can shed light on the feasibility of \npredicting users’ emotion responses to music based  on \nphysiological signals and contribute to future implemen-\ntation of  emotion -aware  MIR systems.  \n2. RELATED W ORK  \nWork related to this study  can be broadly categorized  into \nphysiological signal  analysis in information retrieval and  \nemotion -based music discovery . \n2.1 Physiological Signals in Information Retrieval  \nAlthough using physiological signals as an implicit \nmeasurement of users’ affective states during information \nretrieval  process is still an emerging research topic , sev-\neral recent studies have demonstrated i ts usefulness in \npredicting users’ relevance judgment s [2][3][28] and en-\ngagement level s [12]. Moshfeghi  and Jose [28] used \nphysiological features derived  from heart rate, galvanic \nskin response , and  skin temperature, along with facial ex-\npression features and behavioral features  (i.e. dwell time) , \nto predict users’ relevance  judgment s in video retrieval \ntasks . They found  that the combination of  dwell time and \nheart rate  features perform ed better for the task with en-\ntertainment -based search intention (i.e., when the main \npurpose of  video  search was  to adjust arousal level or \nmood).  Edwa rds and Kelly  [12] combine d skin conduct-\nance, heart rate with search behavior  measures  to evaluate \nusers’  levels of  engagement, frustration, and stress  when \nconducting searching  tasks on a Web search interface . \nThe results sugg ested that heart rate might be more asso-\nciated with negative arousal, and skin conductance with \npositive arousal.  \nThese studies in text and video information retrieval \nwere encouraging and inspiring, yet there is little research \non MIR exploiting  physiolog ical measures . Like many \nvideos , music is a strong stimulus in eliciting emotion \nfrom listeners [23], and many  users  indeed listen to music \nfor the very purpose of emotion modulation  [16][34]. \nConsidering the close relationship between music and \nemotion, as well as  that between  emotion and physiologi-\ncal signals, this study aims to help bridge the gap of in-\ncorporating physiological signals in MIR.  \n2.2 Emotion -aware MI R \nThe maj ority of previous research on music emotion \nrecognition adopted content -based  [11][27], collaborative \nfiltering  [7], and/or semantic -based [4] approach es which \nmay suffer various shortcomings such as ignoring  indi-\nvidual differences  and the “cold start” problem  (i.e., the \nrecommendation performance is poor when few user rat-\nings are available ) [25]. Physiological sig nals, on the oth-er hand , provide a new approach  to understand users’  \nemotion response to music . Several prior studies have \nprobed physiology -based approach in MIR and  yielded \npromising  initial  results . The Affective DJ  project  [9] \nused changes in users ’ skin conductance to detect users ’ \nmood based on which it helped users select music and \ngenerate playlist s. Their evaluation results  confirm ed that \nskin conductance has a significant correlation with per-\nceived excite ment level of a song. Oliver et . al [30] also \nproposed a  framework of  automatic playlist generation  by \nmonitoring user s’ purpose of music listening and physio-\nlogical response s (i.e. heart rate, galvanic skin response, \nrespirati on rate, and movement ) to music . As an exemplar \ncase of the framework, t he MPTrain system was designed  \nand implemented  for selecting songs for  runners  to ac-\ncompany their exercises . More recently, an affective mu-\nsic player (AMP) was developed to select musi c for mood \nenhancement by model ing the effects of music based on  \nchanges in skin conductance level  and skin temperature \n[22]. Validation of the AMP found  that lower skin tem-\nperature s were related  to more positive emotion s induce d \nby music listening . \nNotwithstanding the impact of these existing studies , \nthe investigation  on the relationship between physiolog i-\ncal signals and emotion responses to music  is still limited . \nMoreover, to the best of our knowledge, few studies have \nprobe d wheth er and how individual differences (in per-\nsonality, music preferences, etc.) may play a role in  such \nrelationships . This study aims to bridge these gaps.  \n3. USER EXPERIMENT  OF INTER ACTIVE \nMUSIC SEARCH  \nThe purpose of this  experiment  was to collect physiolog-\nical signals and  self-reported emotion response to music \nduring music  searching and  listening . To encourage par-\nticipants to interact with music , during the experiment,  \nparticipants  were asked to create a playlist using  a novel \nWeb -based  music retrieval system . Participants’  physio-\nlogical signals during music listening were collected , as \nwell as their self -reported  emotion responses to each \npiece  of music they listened to . \n3.1 The MIR  System  \nThe Moody  system  [18] (now in its 3rd version)  is a novel \nmusic retrieval  system which supports searching for \nsong s using several criteria: Genre (e.g ., folk, jazz) , Oc-\ncasion (e.g., party, workout) , Artist , Song, Album , and \npresents basic metadata and album image of each re-\ntrieved song  (shown in Figur e 1). Users can listen to any \nsongs they are interested in using  an HTML5  music play-\ner embedded  in the Web interface of the system . They \ncan also  select any songs to add into a playlist  at any \ntime. Users’ interactions with the systems ( e.g., search , \nplay) are recorded  in the system log s.  \nThe music collection hosted in the system is a subset \nof the Jamendo dataset, one of the world ’s largest digital \nservices for free music. The subset of 10K tracks was ob-Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 363  \n \ntained through the Grand Challenge in User Experienc e \nof the Music Information Retrieval Evaluation Exchange \n(MIREX) [17]. All the tracks are u nder the CC -BY license  \nand thus the full tracks (of 60+ Gigabytes in total) can be \nfreely listened to by the public. M etadata  of the trac ks \n(e.g., title, album, artist) as well as free  tags were also ob-\ntained  from Jamendo  and displayed in the system to facil-\nitate searching and browsing . \n \nFigure 1. Interface of the M oody  System (version 3)  \n3.2 Experiment Procedure  \nThe experiment consisted of four main phases: 1) pre -\nexperiment questionnaire; 2) instruction s of the Moody  \nsystem and the search  task; 3) participants searching and \nlistening to music; and 4) post -experiment questionnaire . \nThe pre -experiment questionnaire  gathered infor-\nmation on demographic s, music listening behavior s, mu-\nsic preference, and personality  as measured by the  Ten \nItem Personality  scale  (TIPI)  [14] which contains  10 \nquestion s in five dimensions: extrovert  - introvert ; agree-\nable - disagreeable; open -close; stable  - unstable; consci-\nentious  - unconscientious . \nDuring phase 3, participants were asked to use the \nMoody  system for no less than 40 minutes, looking for at \nleast 10 songs they like d to make a playlist. They were \nencouraged to search for different types of music for a \nmore diverse experience. For each music piece listened to \nfor more than 30 seconds, participants would  be prompt-\ned to  answer two questions on their current emotion. The \nfirst question asked participan ts to give a score of arousal  \n[32] on a scale of -10 (low arous al) to 10 (high ly \naroused), while the second question was to choose a \nmood  category from a set of options  adapted from [37] \n(Figure 2).   \nAlso during phase 3 , participants were asked to wear \nan Empatica E4 wristband [13] which  is one of the few \nwearable devices designed specifically for measuring \nphysiological signals for research purposes. This device \nsupports  real-time data acquisition and provides a secure \nAPI for raw data downloading. It has been used in emo-\ntion-related  studies in psychology, health sciences, etc. \nwith high reliability (e.g.,  [29]). For signal sta bilization, \nthe wristband was mounted on a participant ’s wrist 2 \nminutes before the search task started . After conducting the task, the last phase of the experi-\nment was for the participants  to fill out a post -experimen t \nquestionnaire concerning their emotional states and gen-\neral experience of the experiment including the search \nprocess.  \nThe experiment took place in a computer classroom \nwhere participants worked on iMac computers. Ear-\nphones were used during the music searching and listen-\ning. Ethical consent forms were signed at the beginning \nand a nominal  remuneration  was paid at the end to com-\npensate participants’ time.  \n \nFigure 2. Question on mood popped up in the Moody  \nsystem . (Tran slation of the instruction: “Please choose \none of the following moods that can best represent your \ncurrent mood. ( Note: This refers to  your mood, not the \nmood expressed by the music piece).” ) \n3.3 Data Collection  \n23 participants ( 15 male s, 8 female s) were recru ited to \njoin this experiment. All participants were undergraduate \nor graduate students in a comprehensive u niver sity in \nHong Kong, whose majors ranged from Social Sciences, \nScience, Engineering, Business, Hum anities & Arts to \nMedicine, with  a diverse  background  in music knowledge  \nand a relative ly high frequency of music listening  ranging \nfrom several times a week to  a daily basis .  \nPhysiological signal s collected includ ed electrodermal \nactivity (EDA), blood volume pulse (BVP), inter b eat in-\nterval (IBI), heart rate (HR) and skin temperature \n(TEMP) . The sampling rates of EDA, BVP, HR, and \nTEMP are 4 Hz, 64Hz, 1Hz, and 4Hz  respectively . \nAmong all participants , we collected arousal and mood  \nratings  for 6 28 pieces of music . Each piece of mu sic was \nlistened to for approximately 80 seconds on average.  \n4. DATA ANALYSIS  \n4.1 Data Prep rocessing  \nBefore extracting  features, we constructed two datasets: \none with raw physiological signals, and the other with \nnormalized physiological signals by z -score normal iza-\n364 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018  \n \ntion [31]. As physiological signals vary across individuals \n[26], the normalization was conducted within each indi-\nvidual participant.  \nTime-series of physiological data in both datasets were \nthen aligned with the starting and ending time of each \nmusic  piece  played in the experiment as recorded by the \nMoody  system logs . Physiological data were then split \ninto chunks corresponding to each song played by each \nparticipant in the experiment.  \nThe emoti on status reported by participants after lis-\ntening to each piece  was aligned with the physiological \nsignals and taken as the ground truth labels in the classi-\nfication analysis . The arousal and mood values rated by \nparticipants were then grouped  into three main  categories \n(i.e. positive, negative, neutral) for comparison using \nANOVA and t -tests as well as classification . This result-\ned in  436 positive, 175 negative, and 17 neutral ( i.e., 0) \narousal ratings . For mood ratings, we combined the mood \ncategories into positive (happy, blessed, etc.), negative \n(sad, fearful , etc.), and neutral moods (i.e. none), result-\ning in 387, 141, and 100 ratings respectively.  \n4.2 Feature Extraction   \nAfter data preprocessing, f eatures of physiological signals  \nduring each music listening period were extracted based \non time series and spectrum analysis. Table 1  summarizes \nthe features extracted in this study . \n \nCategory  Features  \nDescriptive s tatistic s \nof raw signals  Mean, Standard deviation, Medi-\nan, Range  [8], [20] \nTime series features  Means of the absolute values of \nthe 1st / 2nd differences of the raw \n/ normalized signals [31] \nFrequency domain  \nfeatures  HF, LF, LF/HF [36] \nPhysiological signal  \nspecific features  skin conductance response \n(SCR) [6],  \nheart rate variability (HRV) [1] \nTable 1. Features extracted from physiologi cal signals . \nFeatures considered in this study were those found \nclosely linked to emotions by previous studies \n[1][6][8][20][31][36], including descriptive statistics such \nas median, range, standard deviation (stdev), means of the \nfirst difference in raw values (MFDR) and in normalized \nvalues (MFDN), means of the second difference in raw \nvalues (MSDR) and in normalized values (MSDN), low \nand high frequency (LF, HF) in frequency spectrum \nwhich was obtained through a Fast Fourier Transfor-\nmation (FFT) on the time domain signals, as well as the \nratio of the two (LF/HF). In ad dition, two features specif-\nic to physiological signals were considered. The first is \nskin conductance response (SCR) which depicts  the phe-\nnomenon of the skin momentarily being a  better conduc-\ntor of electricity . The SCR is characterized by an increase \nin el ectrodermal response followed by a decrease in re-sponse [6]. It is generally related to stimulus arousal [24]. \nThe second is heart rate variability (HRV) which \nmeasures  the continuou s interplay between sympathetic \nand parasympathetic influences on  the heart rate . HRV \nhas been found relevant to arousal as well [1]. \n4.3 Feature Analysis  \nUsing a one -way ANOVA, we compared physiological \nfeatures across the three a rousal categories as well as the \nthree mood categories  (i.e., positive, negative, neutral ). \nWe also applied t -tests on the features between positive \nand negative categories of arousal and mood  (i.e., without \nconsideration of the neutral categories) . As mul tiple \ncomparisons were involved, Bonferroni correction  [15] \nand Benjamini –Hochberg procedure [5]were applied to \nANOVA and t -tests respectively  to control Type I error. \nFeatures with si gnificant differences across arousal and \nmood categories are identified.  \n4.4 Classification  and Evaluation  \nA machine learning approach was applied to measure the  \nextent to which physiological signals could be used  to \nrecognize users’  emotion response s to musi c listening , in \npositive and negative categories of arousal and mood . \nSpecifically, we trained and compared the performance of \nseveral well-adopted classification models  representative \nof different approaches , namely  decision tree, k-Nearest \nNeighbor (k-NN), naïve Bayes and SVM .  \nAs the sample distribution across the positive and neg-\native categories is unbalanced, for each classifier and \neach category pair (i.e., arousal , mood), we constructed a \nbalanced dataset by randomly selecting samples from the \nlarger categories and performed a classification experi-\nment on the balanced dataset. This process was repeated \n10 times and within each time a  10-fold cross -validation \nwas applied to evaluate the performances.  \nBesides classification based on the whole featur e sets, \nthe forward feature selection method was applied in com-\nbination with the classifiers t o remove redundant and \nnoisy features and improve classification performances . \nIn addition, to examine whether prediction perfor-\nmances vary across different user characteristics , we also \nconducted a classification experiment on data partitioned \nby participants , their  personality , as well as their music \npreferences.  \n5. RESULTS AND D ISCUSSION  \n5.1 Features  with Significant Differences across Emo-\ntion Categories  \nFeatures  found  with significant differences in the \nANOVA and t -test results  after Bonferroni correction  are \nshown in Table 2.  \nFor arousal, both tests indicated that BVP, HR and \nEDA features differed significantly across categories. For \nmood, HR_range  showed consistent s ignificance across \nthe two tests. In addition, HR and EDA seem more prom-Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 365  \n \nising than other physiological signals in predicting and/or \nmonitoring listeners’ emotion responses to music.  How-\never, TEMP features, SCR, and time domain measures of \nHRV were  not significant in either test. \nFeature  Arousal  Mood  \nANOVA  t-test  ANOVA  t-test  \nBVP_median  0.034 0.012  0.004  - \nBVP_HF  0.049  0.007  - - \nHR_ stdev  0.022  0.002  - 0.005  \nHR_range  0.010  < 0.001  0.039  0.003  \nHR_LF  0.022  < 0.001  - 0.004  \nHR_HF  0.028  0.001  - 0.006  \nEDA_MFDN  0.020  0.003  - - \nEDA_MSDN  0.017  0.003  - 0.008  \nEDA_LF/HF  0.036  - - - \nIBI_median  - - 0.006  - \nIBI_mean  - - 0.006  - \nTable 2. Significant results ( p values)  of ANOVA  and t -\ntests across extracted features . \n5.2 Classification  on the Dataset  of All Listeners  \nTable 3 shows the performances  of different classifiers  \nwith feature selection, on datasets balanced by repeated \nrandom sampling . Both accuracy and Cohen ’s kappa are \nused as performance measures. In general, the classifica-\ntion per formances on  the dataset aggregated across all \nusers were low, with the best performances (k -NN) being  \naround  60% in accuracy (baseline 50%) and lower than \n0.2 in Cohen’ s kappa  (indicating low agreement  [35]).  \n \nClassifier  Arou sal Mood  \nAccuracy  Kappa  Accuracy  Kappa  \nDecision Tree  55.43%  0.109  60.04%  0.201  \nk-NN 59.97%  0.199  60.78%  0.216  \nNaïve Bayes  57.74%  0.155  58.83%  0.177  \nSVM  58.40% 0.168  59.50% 0.190 \nTable 3. Classification results  on balanced datasets con-\nsisting of all listeners . \n5.3 Classification on Individual Listeners  \nTo examine  whether and how  prediction results differ \nacross participants , the k-NN classifier was applied to da-\ntasets of individual participants. As the sample sizes of \nsome participants were not sufficient  for constructing \nbalanced datasets of non -trivial sizes, th ese sets of exper-\niments on individual participants were performed on un-\nbalanced datasets. Therefore, F1 measure and Cohen’ s \nkappa were used  and reported in Table 4. From the Co-\nhen’s  kappa values in the results , we can see that the per-\nformances on individual listeners were much better than \nthose on the dataset of all participants (Table 3). This dif-\nference implies that individual variability on physiologi-\ncal signal analysis might be too large to bu ild generic \nclassifiers that work for most (if not all) listeners.   The results also indicate that , for some participants, \ne.g. Users 5, 8, and 18, the prediction worked well for \nboth arousal and mood , whereas for other participants, \nsuch as users 1 1, neither prediction exhibited good results \n(Cohen’ s  kappa values  were  lower than 0.2 ). The varia-\nbility of prediction performances across participants cor-\nroborates with findings in existing research that physio-\nlogical signals are highly individual depend ent [19][26].  \n \nUser  No. of \nsongs  Arousal  Mood  \nF \nmeasure  Kappa  F  \nmeasure  Kappa  \nUser 1  24 90.00%  0.400  78.57%  0.294  \nUser 2  24 81.48%  0.577  62.50%  0.262  \nUser 3  28  78.57%  0.571  90.00%  0.800  \nUser 4 45  80.00%  0.537  93.33%  0.700  \nUser 5  43  98.70%  0.788  93.55%  0.604  \nUser 6  30  81.25%  0.490  84.85%  0.516  \nUser 7  29  86.36%  0.435  93.03%  0.516  \nUser 8  17  96.54%  0.767  96.55%  0.767  \nUser 9  24  94.44%  0.694  96.30%  0.765  \nUser 10  33  87.80%  0.670  92.31 % 0.747  \nUser 11  21  93.75%  -0.063  97.14%  0.000 \nUser 12  18  88.89%  0.722  93.33%  0.843  \nUser 13  25  94.74%  0.614  90.48%  0.405  \nUser 14  29  91.30%  0.580  95.00%  0.750  \nUser 15  31  80.00%  0.427  83.72%  0.440  \nUser 16  30  92.59%  0.259  92.31%  0.423  \nUser 17  32  88.89%  0.595  85.00%  0.475  \nUser 18  33  98.36%  0.784  98.31%  0.784  \nUser 19  33  84.45%  0.507  90.32%  0.750  \nUser 20  35  84.00%  0.380  91.67%  0.586  \nUser 21  16  91.67%  0.673  96.00%  0.818  \nTable 4.  Classification performance on individual partic-\nipants . \n5.4 Classific ation on Participant  Groups  \nTable 5 shows classifi cation  performance of users with \ndifferent personalities. Personality was determined based \non responses to the TIPI questionnaire [14] which con-\nsisted of five personality dimen sions . For each dimen-\nsion, each user was categorized to either end based on \ntheir answers to the two question items in that dimension. \nThis set of experiments were also conducted on balanced \ndatasets, with accuracy and Cohen’ s kappa values report-\ned (Table 5). Compared to performances on the dataset  of \nall listeners  (Table 3), classification performances on \nsome of the personality dimensions were better . In partic-\nular, predictions on users with  Agreeable personality \nreached  Cohen’ s kappa values  of 0.581 (for arousal) and \n0.682 ( for mood ) which are  deemed as medium and high \nagreement levels respectively  [35].  \nAnother observation is  that the personality dimensions \nwith relatively high classification performances had low-\ner numbe rs of users compared to other personality dimen-\nsions. A correlation analysis revealed significant negative \ncorrelations between the number of users and classifica-\ntion performances (r = - 0.78 for both measures of arous-366 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018  \n \nal, p = 0.008; and r = - 0.62 for both  measures of mood \nprediction, p = 0.054) . This again implies the significance \nof individual differences in physiological signal analysis. \nA suggestion for future research is thus to analyze phys-\niological signals within individual users.    \n \nPersonality  No. of \nusers  Arousal  Mood  \nAccuracy  Kappa  Accuracy  Kappa  \nExtrovert  12 62.42%  0.248  65.00%  0.300 \nIntrovert  11 67.56%  0.351  63.65%  0.273  \nAgreeable  3 79.03%  0.581  84.09%  0.682  \nDisagreeable  20 62.50%  0.250  61.64%  0.233  \nOpen  7 64.51%  0.290  66.58%  0.332  \nClose 16 62.96%  0.259  74.69%  0.494  \nStable  9 64.07%  0.281  69.38%  0.388  \nUnstable  14 62.39%  0.248  62.41%  0.248  \nConscientious  6 69.75%  0.395  68.91%  0.378  \nUnconscien-\ntious  17 60.65%  0.213  61.05%  0.221  \nTable 5. Classification performances of each personality \ndimension .  \nBesides personality, users’ music preference might \nplay a role in their emotion responses to music [21]. \nTherefore, we grouped the participants  based on their \nself-reported genre preferences using the k -means cluster-\ning algorithm , with the optimal k value selected b y the \nDavies Bouldin ind ex [10]. The results yielded three clus-\nters corresponding to preferences as shown in Table 6 . \nWe then conducted classification experiments on partici-\npants  in each of the clusters , using the k -NN algorithm \nand balanced datasets . Prediction performances (Table 6) \nwere high er than those on the whole dataset (Table 3) , \nand the  performances on the mood classification of clus-\nter 2  were  comparable to those of some  individual users \n(Table 4).  These results indicate  that certain music pref-\nerences might play a role in predicting  emotion responses \nto music based on physiological signals.  Future work is \nneeded to further investigate this phenomenon , preferably \nwith larg er samples.   \nCluster  Preferences  No. of \nusers  Arousal  Mood  \nAccuracy  κ Accuracy  κ \n0 Pop only  7 64.26%  0.285  71.32%  0.426  \n1 Classical, \nFolk, Pop  10 65.17%  0.303  64.06%  0.281  \n2 Electronica, \nRock, Pop  6 69.02%  0.380 76.76%  0.535  \nTable 6. Classificatio n performances of participant clus-\nters based on music preferences . \n6. CONCLUSION AND FUTURE  WORK  \nThis paper presented a study  towards recognizing  users’  \nemotion response to music  using physiological data ob-\ntained from wearable sensor s. ANOVA and t -tests re-\nvealed that heart rate (HR) and electrodermal activity  (EDA) featu res were consistently significant in both \narousal and mood dimensions. This finding provides em-\npirical evidence for feature extraction and selection in fu-\nture studies. In predicting emo tion responses to music \nbased on physiological signals during music listening, \npredictions on individual participants showed promising  \nperformances as well as large performance  difference s \nacross individuals. These results  verif ied that the predict-\nability of emotion responses to music based on physio-\nlogical signals may vary from person to person. Addi-\ntionally, the classification experiments conducted on  data \npartitioned by personality  dimensions  and music prefer-\nences  illustrated  that classification based on  physiological \nsignals might be more  effective for users with certain per-\nsonality traits  or genre preferences , such as being agreea-\nble or preferring Electronica and Rock music . However, \nthese result s were con founded  with the sample size , as \nthe number of u sers in each personality category was \nnegatively correlated with performance measures, further \nindicating variability across users and suggesting that in-\ndividual -based analysis might be more fruitful for ex-\nploiting physiological signals. The results report ed in this \npaper  demonstrate the potential of physiological sensing \ntechnique s in emotion -aware MIR . This opens  up a num-\nber of possibilities  in future MIR systems and services , \nsuch as recommending music based on  users’ current \nphysiological measures  and maintainin g mood -based \nplaylist s which can be  adjust ed in real time based on  \nchanges in physiological signals , etc.  \nFuture studies  will be conducted to further investigate \nin which circumstances physiological signals are more \neffective in predicting emoti on responses to music. C om-\nbinations of  factors will be considered such as the match-\ning between music preferences and the music pieces be-\ning listened  to. In the next stage of our research , we will \nalso explore the effect of incorporating music features \n(e.g., acoustic, emotion, occasion, etc.) in the prediction  \nas well as users’ behavioral logs recorded in the user ex-\nperiment . Besides, the experiment  in this study  was run in \nlaboratory settings. To achieve  a higher level of ecologi-\ncal validity , future experiments can be extended to the \neveryday environment  of the participants and for longer \ntime span s. \n7. ACKNOWLED GEMENT  \nThe authors acknowledge the support given by the Na-\ntional Natural Science Foundation of China: 61703357 , \nand the Research Grants Council of the HKSAR Gov-\nernment, #T44 -707/16/N, under the Theme Based Re-\nsearch Scheme . We also thank the anonymous reviewers \nfor their helpful suggestio ns. \n8. REFERENCES  \n[1] B. M. Appelhans and L. J. Luecken, “Heart rate \nvariability as an index of regulated emotional Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 367  \n \nresponding.,”  Review of General Psychology , Vol. \n10, N o. 3, pp. 229 –240, 2006.  \n[2] I. Arapakis, I. Konstas, and J. M. Jose : “Using facial \nexpressions a nd peripheral physiological signals as \nimplicit indicators of topical relevance,” Proc . of the \n17th ACM international conference on Multimedia , \npp. 461 -470, 2009  \n[3] O. Barral, M. J. A. Eugster, T. Ruotsalo, M. M. \nSpapé, I. Kosunen, N. R avaja, S. Kaski, and G.  \nJacucci:  “Exploring peripheral physiology as a \npredictor of perceived relevance in information \nretrieval,” Proc . of the 20th International \nConference on Intelligent User Interfaces , pp. 389 -\n399, 2015.  \n[4] M. Barthet, G. Fa zekas, A. Allik, and M. Sandler:  \n“Moo dplay: an interactive mood -based musical \nexperience,” Proc . of the Audio Mostly 2015 on \nInteraction with Sound , p. 3, 2015.  \n[5] Y. Benjamini, Y. Hochberg: “ Controlling the False \nDiscovery Rate: A Practical and Powerful Approach \nto Multiple Testing ,” Journal of  the Royal Statistical \nSociety. Series B (Methodological) , Vol. 57 , No. 1,  \npp. 289-300, 1995 . \n[6] W. Boucsein:  “Electrodermal Activity,” Plenum \nSeries in Behavioral Psychophysiology and \nMedicine, Plenum Press , 199 2. \n[7] J. Broekens, A. Pronker, and M. Neuteboom : “Real \ntime labeling of affect in music using the affect  \nbutton ”. Proc . of the 3rd international workshop on \nAffective interaction in natural environments , pp. \n21-26, 2010 . \n[8] G. Chanel, C. Rebetez, M. Bétrancourt, and T. Pun : \n“Boredom, engagement and anxiety as indicators for \nadaptation to difficulty in games,”  Proc . of the 12th \ninternational conference on Entertainment and \nmedia in the ubiquitous era , pp. 13 -17, 2008  \n[9] F. Dabek , J. Healey, and  R. Picard: “ A new affect -\nperceiving interface and its application to \npersonalized music selection,” Proc. from the 1998 \nWorkshop on Perceptual User Interfaces,  1998  \n[10] D. L. Davies and D. W. Bouldin:  “A Cluster \nSeparation Measure,” IEEE Transactions on Pattern \nAnalysis and Machine Intelligence , Vol. PAMI -1, \nNo. 2, pp. 224 –227, Apr. 1979.  \n[11] J. J. Deng, C. H. C . Leung, A. Milani, and L. Chen:  \n“Emotional states associated with music: \nClassification, prediction of changes, and \nconsideration in recommendation ,” ACM \nTransactions on Interactive Intelligent Systems , Vol. \n5, No. 1, pp. 1 –36, Mar. 2015  \n[12] A. Edwards and D . Kelly: “Engaged or Frustrated : \nDisambiguating Emotional State in Search ,” Proc . of the 40th International ACM SIGIR Conference on \nResearch and Development in Information Retrieval , \npp. 125 -134, 2017.  \n[13] M. Garbarino,  M. Lai, D. Bender, R. W. Picard, and  \nS. Tognetti:  “Empatica E3 —A wearable wireless \nmulti -sensor device for real -time computerized \nbiofeedback and data acquisition ,” Proc. of the 4th \nInternational Conference on Wireless Mobile \nCommunication and Healthcare , pp. 39 -42, 2014  \n[14] S. D. Gosling, P. J. Rentfrow, and W. B. Swann:  “A \nvery brief measure of the Big -Five personality \ndomains,” Journal of Research in Personality , Vol. \n37, N o. 6, pp. 504 –528, Dec. 2003.  \n[15] S. Holm : “A simple sequentially rejective multiple \ntest procedure ,” Scandinavian Journal of Statistics  \npp. 65-70, 1979.  \n[16] X. Hu, N. Kando, “Evaluation of Music Search in \nCasual -Leisure Situations ,” Proc. of Search for Fun \nWorkshop at the Information Interaction in Context \nconference (IIiX) , 2014 . \n[17] X. Hu, J. Lee, D. Bainbri dge, K. Choi, P. \nOrganisciak and J. Downie, “The MIREX grand \nchallenge: A framework of holistic user -experience \nevaluation  in music information retrieval” , Journal \nof the Association for Information Science and \nTechnology , Vol. 68, no. 1, pp. 97 -112, 20 17. \n[18] X. Hu, V . Sanghvi, B . Vong, P . J. On, C . Leong, and \nJ. Angelica. “Moody: A web -based music mood \nclassification and recommendation system ”, Proc. of \n9th International Conference on Music Information \nRetrieval , Philadelphia , U.S. 2008.  \n[19] M. S.  Hussain , O. AlZoubi , R. A.  Calvo, and  S. K.  \nD’Mello: “ Affect detection from multichannel \nphysiology during l earning sessions with \nAutoTutor,” International Conference on Artificial \nIntelligence in Education , pp. 131 -138, 2011.  \n[20] M. S. Hu ssain, R. A. Calvo, and F. Chen:  \n“Automatic cognitive load detection from face, \nphysiology, task performance and fusion during \naffective interference,” Interacting with Computers , \nVol. 26, N o. 3, pp. 256 –268, Jun. 2013.  \n[21] M. Iwanaga and Y. Moroki:  “Subjective and \nPhysiological Responses to Music Stimuli \nControlled Over Activity and Preference,” Journal \nof Music Therapy , Vol. 36, N o. 1, pp. 26 –38, Mar. \n1999.  \n[22] J. H. Janssen, E. L. van den Broek, and J. H. D. M. \nWesterink, “Tune in to your emotions: a robust \npersonalized affective music player,”  User Modeling \nand User -Adapted Interaction , Vol. 22, N o. 3, pp. \n255–279, Oct. 2011.  368 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018  \n \n[23] P. Kenealy:  “Validation of a music mood induction \nprocedure: Some preliminary findings,” Cognition & \nEmotion , Vol. 2, N o. 1, pp. 41 –48, Mar. 1988.  \n[24] S. Khalfa, P. Isabell e, B. Jean -Pierre, and R. Manon:  \n“Event -related skin conductance responses to \nmusical emotions in humans,” Neuroscience Letters , \nVol. 328, N o. 2, pp. 145 –149, Aug. 2002.  \n[25] S. Khusro, Z. Ali, and I. Ullah:  “Recommender \nSystems: Issues, Challenges, and Research \nOpportunities,” Information Science and \nApplications (ICISA) 2016 , pp. 1179 –1189, 2016.  \n[26] A. M. Kiviniemi, H. Hin tsala, A. J. Hautala, T. M. \nIkaheimo, J. J. Jaakkola, S. Tiinanen, T. Sepp anen, \nand M. P. Tulppo:  “Impact and management of \nphysiological calibrat ion in spectral analysis of \nblood pressure variability,” Frontiers in Physiology , \nVol. 5, Dec. 2014.  \n[27] F. F. Kuo, M. F. Chiang, M. K. Shan, and S. Y. Lee:  \n“Emotion -based music recommendation by \nassociation  discovery from film music,” Proc . of the \n13th annual  ACM international conference on \nMultimedia , pp. 507 -510, 2005.  \n[28] Y. Moshfeghi and J. M. Jose:  “An effective implicit \nrelevance feedback technique using affective, \nphysiological and behavioral features,” Proc . of the \n36th international ACM SIGIR conference o n \nResearch and development in information retrieval , \npp. 133 -142, 2013.  \n[29] S. C.  Müller and  T. Fritz:  “Stuck and frustrated or in \nflow and happy: Sensing developers' emotions and \nprogress ,” Proc. of the 37th International \nConference on Software Engineering IE EE, Vol 1,  \n2015 . \n[30] N. Oliver,  L. Kregor -Stickles : “PAPA: Physiology \nand Purpose -Awar e Automatic Playlist Generation,”  \nProc . of 7th International Conference on Music \nInformation Retrieval , pp. 250 -253, Victoria, \nCanada. 2006  \n[31] R. W. Picard, E. Vyzas, and J. Hea ley: “Toward \nmachine emotional intelligence: analysis of affective \nphysiological state,” IEEE Transactions on Pattern \nAnalysis and Machine Intelligence , Vol. 23, N o. 10, \npp. 1175 –1191, 2001.  \n[32] J. A. Russell:  “A circum plex model of affect.”, \nJournal of Person ality and Social Psychology , Vol. \n39, N o. 6, pp. 1161 –1178, 1980.  \n[33] L. Su, C.  C. M. Yeh, J. -Y. Liu, J. -C. Wang, and Y. -\nH. Yang:  “A Systematic Evaluation of the Bag -of-\nFrames Representation for Music Information \nRetrieval,” IEEE Transactions on Multimedia , Vol. \n16, N o. 5, pp. 1188 –1200, Aug. 2014.  [34] T. F. M. Ter Bogt, J. Mulder, Q. A. W. R aaijmakers, \nand S. Nic Gabhainn:  “Moved by music: A typology \nof music listeners,” Psychology of Music , Vol. 39, \nNo. 2, pp. 147 –163, Aug. 2010.  \n[35] A. J.  Viera  and J. M.  Garrett: “ Understanding \ninterobserver  agreement: the kappa statistic,”  \nPhysical Therapy , Mar. 2005 . \n[36] W. Wu, Y.  Gil, and J. Lee:  “Combination of \nWearable Multi -Biosensor Platform and Resonance \nFrequency Training for Stress Management of the \nUnemployed Population,” Sensors, Vol. 12, N o. 12, \npp. 13225 –13248, Sep. 2012.  \n[37] Y. H. Yang and Y. C. Teng:  “Quantitative Study of \nMusic Listening Behavior in a Smartphone \nContext,” ACM Transactions on Interactive \nIntelligent Systems , Vol. 5, N o. 3, pp. 1 –30, Sep. \n2015.  Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 369"
    },
    {
        "title": "Investigating Cross-Country Relationship between Users&apos; Social Ties and Music Mainstreaminess.",
        "author": [
            "Christine Bauer 0001",
            "Markus Schedl"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492507",
        "url": "https://doi.org/10.5281/zenodo.1492507",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/130_Paper.pdf",
        "abstract": "We investigate the complex relationship between the factors (i) preference for music mainstream, (ii) social ties in an online music platform, and (iii) demographics. We define (i) on a global and a country level, (ii) by several network centrality measures such as Jaccard index among users' connections, closeness centrality, and betweenness centrality, and (iii) by country and age information. Using the LFM-1b dataset of listening events of Last.fm users, we are able to uncover country-dependent differences in consumption of mainstream music as well as in user behavior with respect to social ties and users' centrality. We could identify that users inclined to mainstream music tend to have stronger connections than the group of less mainstreamy users. Furthermore, our analysis revealed that users typically have less connections within a country than cross-country ones, with the first being stronger social ties, though. Results will help building better user models of listeners and in turn improve personalized music retrieval and recommendation algorithms.",
        "zenodo_id": 1492507,
        "dblp_key": "conf/ismir/0001S18",
        "keywords": [
            "FM-1b dataset",
            "listening events",
            "Last.fm users",
            "mainstream music",
            "social ties",
            "demographics",
            "user behavior",
            "network centrality measures",
            "Jaccard index",
            "closeness centrality"
        ],
        "content": "INVESTIGATING CROSS-COUNTRY RELATIONSHIP BETWEEN USERS’\nSOCIAL TIES AND MUSIC MAINSTREAMINESS\nChristine Bauer\nJohannes Kepler University Linz\nchristine.bauer@jku.atMarkus Schedl\nJohannes Kepler University Linz\nmarkus.schedl@jku.at\nABSTRACT\nWe investigate the complex relationship between the fac-\ntors (i) preference for music mainstream, (ii) social ties\nin an online music platform, and (iii) demographics. We\ndeﬁne (i) on a global and a country level, (ii) by several\nnetwork centrality measures such as Jaccard index among\nusers’ connections, closeness centrality, and betweenness\ncentrality, and (iii) by country and age information. Using\nthe LFM-1b dataset of listening events of Last.fm users,\nwe are able to uncover country-dependent differences in\nconsumption of mainstream music as well as in user be-\nhavior with respect to social ties and users’ centrality. We\ncould identify that users inclined to mainstream music tend\nto have stronger connections than the group of less main-\nstreamy users. Furthermore, our analysis revealed that\nusers typically have less connections within a country than\ncross-country ones, with the ﬁrst being stronger social ties,\nthough. Results will help building better user models of\nlisteners and in turn improve personalized music retrieval\nand recommendation algorithms.\n1. INTRODUCTION\nWhen meeting new people, they frequently tend to talk\nabout their favorite music as conversation starter [30]. In-\ndeed, several studies (e.g., [3, 23, 33, 43]) indicate that\nshared music preferences create and intensify social bonds.\nFor instance, Boer et al. found in a study that participants\nliked others with the same music preferences more than\nthose with different music preferences [3]. Based on this\nresult, the authors conclude that shared music preferences\ncan generate and increase social attraction.\nIn online social networks (OSN), such as Facebook, In-\nstagram, or Twitter, the social bonding effects of shared\nmusic preferences are expected to follow similar patterns\nas the ones observed in ofﬂine settings, i.e., in the physi-\ncal world. In the context of OSN, it is particularly interest-\ning to consider that connections between users are not con-\nstrained to any single country, which is frequently the case\nin ofﬂine scenarios [5]; indeed, many social ties between\nc\rChristine Bauer, Markus Schedl. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). Attri-\nbution: Christine Bauer, Markus Schedl. “Investigating Cross-Country\nRelationship between Users’ Social Ties and Music Mainstreaminess”,\n19th International Society for Music Information Retrieval Conference,\nParis, France, 2018.users are cross-country connections [1]. Yet, sometimes\nindividuals center their interactions within locally bounded\nsocial circles also in their online interaction behavior [10].\nWhether they do so or rather not, however, strongly de-\npends on the users’ cultural backgrounds. For instance,\nChoi et al. found that American users maintained larger\nbut looser networks, whereas Korean users had smaller but\ndenser networks [9]. Barnett and Beneﬁeld analyzed cross-\ncountry friendship connections on Facebook and found\nthat international ties tended to share borders, language,\ncivilization, and migration aspects [1].\nSimilarly, it has been found that music preferences are\nhighly inﬂuenced by the cultural background of listen-\ners [40]. In particular, they strongly depend on the country\nthe user lives in, and each country has its own characteris-\ntics with respect to which music is considered popular or\nmainstream in that very country [38].\nIn contrast to the above general studies on cross-country\nuser connections and music preferences, little is known\nabout how shared music preferences and social ties are re-\nlated in OSN and how the social bonding effect varies for\ncross-country ties. Against this background, the research\nquestions (RQ) we address are:\n\u000fRQ1: In which ways do listeners in different coun-\ntries differ in terms of their inclination to listen\nto mainstream music (considering both global and\ncountry-speciﬁc mainstream)?\n\u000fRQ2: In which ways do listeners in different coun-\ntries differ in terms of their social ties and con-\nnectedness in a music-related online social network\n(Last.fm)?\n\u000fRQ3: In which ways do the previous two aspects\ninterrelate, i.e., does maintaining strong social ties\n(within or between countries) interrelate with a pref-\nerence for mainstream music?\nThe answers to these questions will help building better\nmodels of listeners—individually and on a country level—\nand in turn improve personalized music retrieval and rec-\nommendation algorithms, as it has already been shown for\nother user characteristics, such as demographics [47], ac-\ntivity [49], or mood [26]. For instance, the intensity of\ncross-country ties of a user utogether with information\nabout the music mainstream of u’s country and the coun-\ntriesu’s friends originate from may be used to tailor rec-\nommendations for u. To give an example, if a Spanish user\numaintains very strong ties to users in Brazil, a music rec-\nommender system may include in its recommendation list678a few music items that are popular only in Brazil, to ideally\nprovoke serendipitous music encounters for u.\nThe remainder of this article is organized as follows.\nSection 2 presents related work on music mainstream, so-\ncial connectedness, and culture-aware listener analysis and\nmodeling. Section 3 details the methodology we apply to\nanswer the research questions. Section 4 presents and dis-\ncusses the obtained results. Eventually, Section 5 rounds\noff this work with a conclusion and pointers to future re-\nsearch.\n2. RELATED WORK\nThe work at hand connects to research on music prefer-\nences and mainstream, on user connectedness in social net-\nworks, and on culture-aware music and listener analysis.\nWe brieﬂy discuss the most important related literature in\nthese areas and connect our work to it.\n2.1 Music Mainstream\nA user’s music preferences are shaped by various fac-\ntors. Extant studies have investigated the relationship\nbetween music preferences and, amongst others, demo-\ngraphics (e.g., [4]), personality traits (e.g., [7]), or social\ninﬂuences (e.g., [3, 46]). Music tastes and preferences\nare measured in various ways, for instance, in terms of\ngenre (e.g., [3, 29, 32, 40]), artist (e.g., [36, 48]), or mood\n(e.g., [14, 18]) preference.\nAnother approach to distinguish music preferences is\nto consider the degree of people’s tendency to favor mu-\nsic that is considered mainstream , i.e., music that is most\npopular within the entire population [41]. In short, measur-\ning music preferences in terms of a user’s degree of main-\nstreaminess is a popularity-based approach that considers\nthe degree to which a user prefers music items that are\ncurrently popular or rather ignores such trends [34]. Fur-\nther studies revealed that people’s preferences vary across\ncountries, which holds true for both music genres [40] as\nwell as mainstream music [38]. Early research with re-\nspect to music mainstreaminess for the use in music rec-\nommendation systems shows that the population which a\nuser is compared to tremendously impacts the outcome\nwith respect to recommendation performance [2,34]. More\nspeciﬁcally, a user may be compared to the mainstream\nfrom a global perspective, but also from a country per-\nspective. Yet, an in-depth analysis of country-speciﬁc dif-\nferences concerning mainstreaminess—from a global per-\nspective and a country perspective—is a research gap.\n2.2 Social Connectedness\nResearch on the strength of social connections dates back\nto Granovetter’s paper entitled “The Strength of Weak\nTies” [15], describing the social network theory, which\nhe later revisited in [16]. In OSN research, social con-\nnectedness has been a target of research since the early\ndays of OSN. For instance, although theoretically not con-\nstrained to any single region [5, 9], social connections onOSN sometimes tend to center within locally bounded so-\ncial circles [10,51], because social ties in OSN may follow\nthe spatial, structural, and cultural perimeters of the soci-\netal system that OSN users belong to in ofﬂine settings,\ni.e., in the physical world [5].\nInitially, designing measures of tie strength had been\ndifﬁcult as Granovetter [15, 16] had not given a precise\nconceptual deﬁnition for it [24]. A scale of measures has\ndeveloped since then. Among the most common measures\nfor tie strength and derived measures for node importance\nare the overlap in users’ neighborhoods via Jaccard index\n(J), the closeness centrality (C), and the betweenness cen-\ntrality (B), which we therefore also use in our work, and\ndetail in Section 3.2.\nStudies have revealed that music preferences play an\nimportant role in creating and intensifying social bonds [3,\n23,33,43], because shared music preferences can generate\nand increase social attraction [3]. In other words, people\ntend to like people with the same music preferences more\nthan people with different music preferences [3].\nThis fact has been exploited, among others, in [25],\nwhere a social approach for music recommendation is pre-\nsented. It is based on the assumption that friendship rela-\ntions in OSN are similar to those ofﬂine and that Facebook\nrelationships are indicative of similar music tastes. The\nproposed system recommends YouTube music tracks to a\ntarget user, which have been positively rated (with at least\n3 on a 5-point Likert scale) by the target users Facebook\nfriends, but have not been rated by the target user him or\nherself.\nWhile previous research on music and social bonding\nmost often measures music preferences in terms of genre\n(e.g., [3,23,43]), we argue that music mainstreaminess may\nbe an additional, insightful indicator for music preferences\nwith regard to social bonding.\n2.3 Culture-aware Music and Listener Analysis\nGenerally, human preferences have shown to be rooted and\nembodied in culture [20], and also listeners’ music prefer-\nences are affected by cultural aspects (e.g., [11]). For in-\nstance, perception of music varies across cultures [22, 44,\n45], which obviously inﬂuences music preferences. Fur-\nthermore, national market structures, including local air-\nplay and subsidizing (e.g., local music quotas on radio)\nare different across countries [28, 31] and shape country-\nspeciﬁc popularity of artists and songs. This results,\namong others, in the fact that pop music preferences\ndisconverge rather than converge within European coun-\ntries [8].\nWith the increasing popularity of personalized music\nrecommender systems—i.e., systems that tailor recom-\nmendations for particular music items (e.g., artists, albums,\nor songs) to the preferences of individuals [42]—and the\nacknowledgement that tailoring recommendations to a lis-\ntener’s cultural speciﬁcities may substantially increase the\nperformance of a music recommender system [2, 38, 47],\nresearch investigating and describing music and listener\nproﬁles from a culture perspective has received attentionProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 679lately. To provide some examples, [27] show that incor-\nporating cultural characteristics allows for more precise\ncharacterization of listeners; [50] integrate cultural aspects\nfor modeling music similarity; [21] use culture-aware ap-\nproaches describing and modeling intonation of audio mu-\nsic recordings. Comparisons of listener proﬁles across\ncountries have been presented from many different an-\ngles [11, 37, 39], most frequently in terms of genres, while\nour work concentrates on mainstreaminess.\n3. METHODOLOGY\nFor our study, we use and extend the LFM-1b dataset [35],\nwhich comprises 1,088,161,692 listening events of\n120,322 unique Last.fm users. Since our investigation\naims at uncovering country-speciﬁc factors, we consider\nonly the subset of the LFM-1b dataset that includes lis-\ntening events of users who provide country information.\nTo reduce the likelihood of less signiﬁcant results due to\na sample bias of users within a given country, we further-\nmore ﬁlter countries with less than 100 users, which results\nin a dataset of 53,258 users from 47 countries. Some of\nthe users do not maintain any social ties on Last.fm. Ex-\ncluding those (because we cannot compute the respective\nmeasures), we ﬁnally end up with a stable dataset of 5,680\nusers from 18 countries, on which we conduct our analysis.\n3.1 Music Mainstreaminess\nTo quantify the proximity of a user to both the country-\nspeciﬁc and the global mainstream, we employ the ap-\nproach proposed in [2, 38]. Schedl and Bauer identiﬁed\ntwo rank-based measures as being best suited to estimate\nmainstreaminess of a user among his or her fellow citizens\nwithin the same country (Equation 1) and compared to a\nglobal mainstream (Equation 2). In the equations, which\nhave been simpliﬁed from [2], where a complex framework\nis proposed, M(u;c)denotes the rank-based mainstreami-\nness of user uin regard to country c(which is in our case\nalways the country of the user); M(u)denotesu’s global\nmainstreaminess. Furthermore, \u001cdenotes the rank-order\ncorrelation coefﬁcient according to Kendall [19]; AFde-\nnotes a vector containing the global artist frequencies of all\nartists in the dataset, keeping a ﬁxed order (i.e., the ﬁrst el-\nement in vector AFis the total number of listening events\nto the artist who is most frequently listened to globally, and\nso on);AF(c)is deﬁned analogously, but only considers\nlistening events in country c, maintaining the ordering of\nartists given by the global AFvector;AF(u)analogously,\nbut only considering listening events of user u(again main-\ntaining the global ordering); ranks (\u0001)represents the ranks\nof the real-valued artist frequencies given in vector (\u0001).\nLess formally, M(u;c)measures how well user u’s\nranking of artist preferences corresponds to that of all users\nin countryc;M(u)measures how well u’s ranking of artist\npreferences matches with the global ranking. Higher val-\nues indicate closer to the mainstream.\nM(u;c) =\u001c(ranks (AF(c));ranks (AF(u))) (1)\nM(u) =\u001c(ranks (AF);ranks (AF(u))) (2)3.2 Social Ties and Centrality Measures\nTo uncover social ties between users in the LFM-1b\ndataset, we ﬁrst enrich the dataset using the Last.fm API\nendpoint user.getFriends1to obtain the connections of all\nusers in LFM-1b. Since we are only interested in the intra-\nconnectedness between users in the dataset, we exclude\nall friendship connections to users that are not contained\nin the LMF-1b dataset. This results in a total of 79,254\nconnections by 11,801 users (5,680 users only considering\nthe 18 countries with at least 100 users). On the result-\ning network, we then compute tie strength and centrality\nscores that estimate the importance of nodes (users) in a\nnetwork. More precisely, we use Jaccard index (J), close-\nness centrality (C), and betweenness centrality (B) since\nthey are among the most common measures. Jaccard index\n(J) is deﬁned as the fraction of shared neighbors among\nall neighbors of the two users uandvunder considera-\ntion [17]. To obtain a single measure per user u, we com-\npute the arithmetic mean of the Jaccard indices between u\nand all users connected to u. Closeness centrality (C) of\nuseruis deﬁned as the reciprocal of the sum of the short-\nest path distances between uand all other users in the net-\nwork [13]. Higher values of closeness therefore indicate\nhigher centrality. Betweenness centrality (B) of user uis\ndeﬁned as the sum of the fraction of all shortest paths be-\ntween pairs of nodes v,w(6=u) that pass through u[12].\nBetweenness can therefore be regarded as how much in the\nway between two arbitrary users ulies. Users with high\nbetweenness are assumed to have more control in the net-\nwork, because more information will pass through them.\n4. RESULTS AND DISCUSSION\n4.1 Country vs. Mainstreaminess\nTo answer the ﬁrst research question, i.e., how listeners\nin different countries vary in terms of their inclination to\nlisten to mainstream music, Table 2 shows basic statis-\ntics (mean and standard deviation) of country-speciﬁc and\nglobal mainstreaminess, for the top countries in the dataset\n(those with at least 100 users). The grand means and SD\nare0:091\u00060:060forMcountry and0:103\u00060:062for\nMglobal . Additionally, mean, standard deviation, and\nmedian age of users are depicted. The countries with high-\nest local mainstreaminess are the Netherlands, the United\nKingdom, and Canada ( Mcountry = M(u;c)>0:1);\nthose with highest global mainstreaminess are Finland, the\nNetherlands, and Mexico ( Mglobal =M(u)>0:11).\nThis is in line with previous work [36], which used a dif-\nferent deﬁnition of mainstreaminess, nevertheless identi-\nﬁed the Netherlands, the United Kingdom, Belgium, and\nCanada as most mainstreamy countries.2The high rank\nof Finland in our results may be surprising since many citi-\nzens of this country are know to have a preference for metal\nmusic, cf. [38], which is rather not considered mainstream.\nAt the same time, however, also the standard deviation of\n1https://www.last.fm/api/show/user.getFriends\n2Note that Belgium is not included in our analysis because only 63\nBelgian users remained after ﬁltering.680 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Table 1 . Top 20 global artists and their deviations of\nFinnish preference from the global preference in terms of\nartist frequency.\nArtist Global rank Deviation\nThe Beatles 1 -47.44 %\nRadiohead 2 -43.95 %\nPink Floyd 3 -25.80 %\nMetallica 4 +126.72 %\nMuse 5 +131.66 %\nArctic Monkeys 6 -55.71 %\nDaft Punk 7 +96.84 %\nColdplay 8 -16.63 %\nLinkin Park 9 -11.17 %\nRed Hot Chili Peppers 10 -0.10 %\nSystem of a Down 11 +152.54 %\nNirvana 12 -30.23 %\nIron Maiden 13 +170.77 %\nRammstein 14 +171.76 %\nDepeche Mode 15 -22.87 %\nLana Del Rey 16 -28.33 %\nLady Gaga 17 +132.72 %\nLed Zeppelin 18 -34.54 %\nFlorence + the Machine 19 -29.49 %\nDavid Bowie 20 -19.43 %\nmainstreaminess is very high for Finland, which indicates\na strong dispersion over mainstream and non-mainstream\nmusic preferences among Fins. In fact, a deeper analysis\nreveals a large variety of music tastes in Finland, cf. Ta-\nble 1. On the one hand, metal bands such as Metallica, Sys-\ntem of a Down, and Iron Maiden are indeed more popular\namong Fins than globally. On the other hand, also artists\nsuch as Muse (top tags on Last.fm: alternative, rock), Daft\nPunk (electronic, house), and Lady Gaga (pop, dance) are\nhighly popular in Finland.\nAccording to our dataset, the least mainstreamy coun-\ntries are Germany, Australia, and the Czech Republic, re-\ngardless of whether mainstreaminess is computed on the\ncountry level or globally.\nAnother observation is that the Scandinavian countries\nNorway and Sweden both show low standard deviations\nin their citizens’ mainstreaminess level, indicating a sta-\nble inclination for a certain level of mainstream among the\nlisteners in these countries. Interestingly, for Norway this\ngoes together with a rather low mainstreaminess level (low\ntertile), while Sweden’s level ranges in the high tertile.\nWe further investigate the correlation between all\naspects in Table 2. Computing Pearson correla-\ntion coefﬁcients between all pairs of aspects and a\n2-tailed t-test to investigate signiﬁcance, we identify\nthe following signiﬁcant correlations at p\u00140:05:\n\u001a(Mcountry:mean ;Mglobal:mean) = 0 :819(p\u00190:0),\n\u001a(Mglobal:mean ;Age:mean) = 0 :280(p=0:05).4.2 Country vs. Social Ties and Centrality\nTowards answering the second research question, i.e., how\nlisteners in different countries vary in terms of their so-\ncial ties and their connectedness within the Last.fm social\nnetwork, Table 3 shows means and standard deviations of\nsocial tie strength (Jaccard index), closeness, and between-\nness (cf. Section 3.2), again for the top 18 countries in the\ndataset. The grand means and SD for tie strength (J), close-\nness, and betweenness are 0:285\u00060:101,0:150\u00060:067,\nand0:027\u00060:067, respectively. The countries with highest\naverage tie strength are Sweden ( J= 0:319) and Finland\n(J= 0:301), closely followed by Poland ( J= 0:299)\nand the Netherlands ( J= 0:297). TheseJvalues indicate\nthat, on average, users in these countries share nearly one\nthird of their neighbors with all users they are connected\nto. The lowest tie strength values are present for Ukraine\nand the Czech Republic ( J\u00190:26), closely followed by\nItaly, Spain, Russia, and Australia ( J\u00190:27).\nWith respect to closeness centrality, the countries with\nhighestCvalue are Ukraine, Italy, Spain, Russia, and Mex-\nico (C > 0:16), those with lowest closeness are Swe-\nden (C= 0:117), Poland, Finland, and the Netherlands\n(C\u00190:13). Interestingly, in the case of Sweden, the\nlowest mean closeness centrality is paired with the highest\nstandard deviation ( C= 0:117\u00060:084). Investigating the\nreason for this, we ﬁnd that there are many Swedish out-\nliers with very low closeness centralities. Quantitatively,\nthe 25-, 50-, and 75-percentiles for closeness in Sweden\nare 0.0002, 0.1500, and 0.1790, respectively, while being\n0.1248, 0.1672, and 0.1910, on average, among all other\ncountries.\nAs for betweenness, the countries with highest values\n(B > 0:0004 ) are Mexico and Italy, while lowest scores\n(B < 0:0002 ) are realized by users in the Netherlands,\nSweden, and France. Mexico and Italy, however, also show\nthe largest standard deviations. In fact, the median of their\nBvalues approaches zero. About half of Italian and Mexi-\ncan users therefore have no or very few connections. Still,\nthese countries’ 75-percentile as well as maximum Bis at\nthe same time the highest among all countries, B\u00190:0003\nandB\u00190:01, respectively. A few users in Italy and\nMexico are hence extremely well connected and can be\nassumed to have a high level of inﬂuence in the entire ana-\nlyzed network, i.e., sub-network of Last.fm [6].\nInvestigating which of the aspects in Table 3 correlate,\nPearson correlation coefﬁcients are signiﬁcant at p\u00140:05\nfor the following pairs of aspects: \u001a(B:mean;J:mean) =\n\u00000:363(p= 0:01) and\u001a(C:mean;J:mean) =\u00000:637\n(p\u00190:0). The negative correlations between tie strength\nand centrality measures indicate that while direct neigh-\nbors between connected users show signiﬁcant overlaps,\nthis does not generalize to the whole network. Our as-\nsumption, which we test in the next section, is that these\nlocal neighbors who are well connected are rather users in\nthe same country.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 681Table 2 . Statistics of country-speciﬁc and global main-\nstreaminess as well as age for countries with at least 100\nusers. Country names are abbreviated according to ISO\n3166-1 alpha-2.\nCountryUsersmeanstdmeanstdmeanstdmedian\nUS9270.0910.0620.0960.06720.813.622.0\nRU7890.0930.0570.1020.06118.912.021.0\nPL7750.0950.0660.1040.07019.210.320.0\nBR5310.0910.0650.1070.06919.710.021.0\nUK4700.1020.0570.1070.05721.213.823.0\nDE4630.0810.0620.0880.06620.713.322.0\nFI2170.0920.0940.1120.06520.210.322.0\nUA2070.0970.0520.1080.05219.311.522.0\nIT1750.0900.0580.1060.06723.114.023.0\nES1570.0880.0530.1040.05921.912.124.0\nNL1550.1060.0580.1120.05925.616.023.0\nSE1320.0940.0490.1050.05321.613.922.0\nCA1270.1010.0590.1080.06119.311.322.0\nCZ1240.0750.0570.0930.06319.210.422.0\nMX1090.0870.0600.1100.06221.711.423.0\nFR1080.0880.0550.1010.05822.311.825.0\nAU1070.0850.0610.0920.07020.011.421.0\nNO1070.0900.0480.1000.05820.613.922.0M_country M_global Age\nTable 3 . Statistics of social tie strength and centrality mea-\nsures for countries with at least 100 users. Country names\nare abbreviated according to ISO 3166-1 alpha-2.\nCountry Users mean std mean std mean std\nUS 927 0.287 0.102 0.152 0.066 0.023 0.061\nRU 789 0.270 0.103 0.162 0.064 0.031 0.073\nPL 775 0.299 0.106 0.132 0.072 0.023 0.061\nBR 531 0.287 0.102 0.159 0.060 0.028 0.075\nUK 470 0.290 0.098 0.149 0.067 0.028 0.069\nDE 463 0.286 0.106 0.145 0.072 0.024 0.056\nFI 217 0.301 0.112 0.133 0.080 0.022 0.057\nUA 207 0.261 0.098 0.165 0.054 0.027 0.055\nIT 175 0.268 0.086 0.163 0.059 0.040 0.125\nES 157 0.269 0.092 0.163 0.055 0.032 0.067\nNL 155 0.297 0.113 0.135 0.080 0.017 0.053\nSE 132 0.319 0.104 0.117 0.084 0.019 0.044\nCA 127 0.294 0.105 0.156 0.067 0.023 0.058\nCZ 124 0.265 0.098 0.152 0.063 0.027 0.065\nMX 109 0.295 0.100 0.161 0.064 0.042 0.122\nFR 108 0.282 0.100 0.154 0.062 0.019 0.039\nAU 107 0.270 0.096 0.157 0.060 0.025 0.060\nNO 107 0.291 0.103 0.142 0.068 0.026 0.067Betweenness (x100) Social Ties (J) Closeness\n4.3 Mainstreaminess vs. Social Ties and Centrality\nRegarding RQ3, i.e., in which ways do mainstream and so-\ncial connectedness interrelate, we analyzed various aspects\nwith respect to the 33,974 connections between the users\nin our sample. Most connections in our sample are cross-\ncountry (26,914 connections, i.e. 79%), while only 21%\n(or 7,060) are between users of the same country.\nIn a detailed analysis for differences between different\ndegrees of mainstreaminess vs. social ties and centrality,\nwe found two signiﬁcant differences: As conjectured, the\nsocial tie strength of users within the same country (mea-\nsured by the Jaccard index between the connections of the\ntwo users to compare, cf. Section 3.2) differs from the so-\ncial tie strength of cross-country connections. In a 2-tailed\nt-test, the difference between connections within a country(mean = 0:241,std= 0:109) and cross-country connec-\ntions (mean = 0:219,std= 0:095) is highly signiﬁcant\n(t=17:154;df=33972 ,p=0:000).\nComparing each user’s social tie strength (averaged\nover all his or her connections with his or her respective\nmainstreaminess level), in a t-test, we found that the differ-\nence between the group of users with a low preference for\nmainstream ( mean = 0:281,std= 0:102) and the group\nof high mainstream users ( mean = 0:289,std= 0:104)\nis highly signiﬁcant ( t=\u00002:819,df= 3777:883,p=\n0:005), when using the Mglobal measure. When using\ntheMcountry measurement, this effect disappears. We\nconjecture that from a country perspective of mainstreami-\nness, the different forms of mainstream per country and the\nmore focused music preference within a country levels the\neffect that can be seen from a global perspective.\nInvestigating individual countries, Table 4 shows that\nfor all countries, the social tie strength between users\nwithin the country is higher than for connections span-\nning two countries. The difference is highly signiﬁcant\n(p\u00140:001) for BR, CA, DE, FI, NO, PL, SE, UA, UK, and\nUS; the difference is signiﬁcant ( p\u00140:05) for ES, NL, and\nRU. So, although the number of cross-country connections\nis higher than the number of connections within a coun-\ntry, the social tie strength for inner-country connections is\nhigher for all countries under investigation.\n5. CONCLUSION\nUsing the LFM-1b dataset of country-speciﬁc listener and\nlistening information, we set out to answer three research\nquestions: In which ways do listeners in different coun-\ntries differ in terms of their inclination to listen to main-\nstream, on a global and a country level (RQ1)? In which\nways do listeners in different countries differ in terms of\ntheir social ties and connectedness in Last.fm (RQ2)? In\nwhich ways do mainstream and social connectedness in-\nterrelate (RQ3)?\nWe found large differences between countries in terms\nof the level of global and regional mainstream consump-\ntion of listeners as well as their ﬂuctuations, i.e., stan-\ndard deviations (RQ1). A particularly interesting exam-\nple is Finland with a mid (regional) to high (global) main-\nstreaminess level. While seeming surprising at ﬁrst glance,\na high standard deviation in mainstreaminess reveals that\nthere is a group of Finnish listeners that largely follows the\ntrend, whereas another large group established their own\npreferences, far away from the mainstream. Further analy-\nsis showed that this group’s inﬂuence foremost stems from\nmetal music. In contrast, Finland’s neighbors Sweden and\nNorway show a very stable level of preference for main-\nstream.\nIn terms of social ties and centrality measures (RQ2),\nwe found that, on average, Last.fm users share between one\nfourth (Italy, Spain, Russia, and Australia) and one third\n(Sweden and Finland) of their neighbors. Moreover, so-\ncial tie strength is negatively correlated with betweenness\nand closeness centrality, which indicates that direct neigh-\nbors between connected users show signiﬁcant overlaps,682 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Table 4 . Differences in social tie strength between connections within a country and cross-country connections. Country\nnames are abbreviated according to ISO 3166-1 alpha-2. Signiﬁcance levels are: * p<0:05, **p<0:01, ***p<0:001.\ncountry connections mean social ties (J) std t df p\nAUwithin country 34 0.25620 0.110581.784 35.268 0.083cross-country 760 0.22181 0.09615\nBRwithin country 1075 0.25639 0.115697.736 3622.000 0.000 ***cross-country 2549 0.22605 0.10438\nCAwithin country 28 0.29538 0.125473.259 874.000 0.001 ***cross-country 848 0.23501 0.09537\nCZwithin country 110 0.22807 0.097500.051 184.758 0.959cross-country 315 0.22753 0.09416\nDEwithin country 369 0.25107 0.115387.542 2704.000 0.000 ***cross-country 2337 0.21144 0.08993\nESwithin country 180 0.23885 0.099722.730 248.262 0.007 *cross-country 880 0.21680 0.09397\nFIwithin country 171 0.26051 0.120024.761 1252.000 0.000 ***cross-country 1083 0.22110 0.09719\nFRwithin country 42 0.25933 0.129161.114 44.620 0.271cross-country 673 0.23666 0.10755\nITwithin country 246 0.24656 0.087061.856 1261.000 0.064cross-country 1017 0.23359 0.10085\nMXwithin country 108 0.22272 0.111880.002 128.309 0.998cross-country 908 0.22270 0.10033\nNLwithin country 67 0.26510 0.123342.113 75.556 0.038 *cross-country 717 0.23217 0.10690\nNOwithin country 84 0.26555 0.102184.769 105.179 0.000 ***cross-country 578 0.20911 0.09553\nPLwithin country 958 0.25610 0.1153912.336 3270.000 0.000 ***cross-country 2314 0.20937 0.09075\nRUwithin country 1596 0.21208 0.099402.201 5299.000 0.028 *cross-country 3705 0.20598 0.08945\nSEwithin country 50 0.32686 0.119785.880 57.000 0.000 ***cross-country 474 0.22339 0.10359\nUAwithin country 160 0.22599 0.113703.547 1228.000 0.000 ***cross-country 1070 0.19947 0.08376\nUKwithin country 513 0.25545 0.110707.558 2869.000 0.000 ***cross-country 2358 0.22043 0.09135\nUSwithin country 1269 0.23737 0.100704.474 5595.000 0.000 ***cross-country 4328 0.22367 0.09456\nbut this does not generalize to the whole network.\nOur hypothesis that users whose neighborhoods are\nwell connected are likely from the same country could be\nveriﬁed (RQ3). For most analyzed countries, our analysis\nrevealed signiﬁcantly higher social tie strength for connec-\ntions within the same country compared to cross-country\nconnections. In other words, although users have less\nconnections within the same country than cross-country\nones, the social ties are stronger for inner-country connec-\ntions. Furthermore, our analysis identiﬁed that the group\nof mainstreamy users have stronger social ties compared to\nthe group of users less inclined to mainstream music con-\ncerning tie strength.\nThe logical next step in this line of research is to inte-\ngrate the ﬁndings into a music recommendation system.\nThe mainstreaminess and country information is highly\nuseful to alleviate cold-start; the information about cross-country social ties can be exploited to personalize recom-\nmendations depending on the tie strength between the tar-\nget user and connections to users in other countries. For in-\nstance, collaborative ﬁltering techniques could be extended\nby a mainstreaminess or social tie ﬁltering component, in\na fashion similar to [38].\nFinally, it would be worth investigating whether results\ngeneralize to platforms other than Last.fm. However, this\nresearch question may be hard to investigate externally and\nindependently in the absence of publicly available datasets\nfrom the big players.\n6. ACKNOWLEDGMENTS\nThis workshop is supported by the Austrian Science Fund\n(FWF): V579.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 6837. REFERENCES\n[1] George A Barnett and Grace A Beneﬁeld. Predicting\ninternational facebook ties through cultural homophily\nand other factors. New Media & Society , 19(2):217–\n239, 2017.\n[2] Christine Bauer and Markus Schedl. On the importance\nof considering country-speciﬁc aspects on the online-\nmarket: An example of music recommendation con-\nsidering country-speciﬁc mainstream. In 51st Hawaii\nInternational Conference on System Sciences (HICSS\n2018) , pages 3647–3656.\n[3] Diana Boer, Ronald Fischer, Micha Strack, Michael H\nBond, Eva Lo, and Jason Lam. How shared preferences\nin music create bonds between people: Values as the\nmissing link. Personality and Social Psychology Bul-\nletin, 37(9):1159–1171, 2011.\n[4] Arielle Bonneville-Roussy, P. Jason Rentfrow, Man K.\nXu, and Jeff Potter. Music through the ages: Trends in\nmusical engagement and preferences from adolescence\nthrough middle adulthood. Journal of Personality and\nSocial Psychology , 105(4):703–717, 2013.\n[5] Danah Boyd. Why youth (heart) social network sites:\nThe role of networked publics in teenage social life.\nMacArthur foundation series on digital learning–\nYouth, identity, and digital media volume , pages 119–\n142, 2007.\n[6] Ulrik Brandes. A faster algorithm for betweenness\ncentrality. The Journal of Mathematical Sociology ,\n25(2):163–177, 2001.\n[7] Richard A. Brown. Music preferences and personal-\nity among japanese university students. International\nJournal of Psychology , 47(4):259–268, 2012.\n[8] Oliver Budzinski and Julia Pannicke. Do preferences\nfor pop music converge across countries?–empirical\nevidence from the eurovision song contest. Creative In-\ndustries Journal , 10(2):168–187, 2017.\n[9] Sejung Marina Choi, Yoojung Kim, Yongjun Sung,\nand Dongyoung Sohn. Bridging or bonding? a cross-\ncultural study of social relationships in social net-\nworking sites. Information, Communication & Society ,\n14(1):107–129, 2011.\n[10] Nicole B Ellison, Charles Steinﬁeld, and Cliff Lampe.\nThe beneﬁts of facebook “friends:” social capital\nand college students’ use of online social network\nsites. Journal of computer-mediated communication ,\n12(4):1143–1168, 2007.\n[11] Bruce Ferwerda, Andreu Vall, Marko Tkal ˇciˇc, and\nMarkus Schedl. Exploring music diversity needs across\ncountries. In Proceedings of the 2016 Conference on\nUser Modeling Adaptation and Personalization , pages\n287–288. ACM, 2016.[12] Linton C. Freeman. A Set of Measures of Central-\nity Based on Betweenness. Sociometry , 40(1):35–41,\nMarch 1977.\n[13] Linton C. Freeman. Centrality in Social Networks Con-\nceptual Clariﬁcation. Social Networks , 1(3):215–239,\n1978-1979.\n[14] Ronald S. Friedman, Elana Gordis, and Jens F ¨orster.\nRe-exploring the inﬂuence of sad mood on music pref-\nerence. Media Psychology , 15(3):249–266, 2012.\n[15] Mark S Granovetter. The strength of weak ties. In So-\ncial networks , pages 347–367. Elsevier, 1977.\n[16] Mark S Granovetter. The strength of weak ties: A net-\nwork theory revisited. Sociological theory , pages 201–\n233, 1983.\n[17] Mangesh Gupte and Tina Eliassi-Rad. Measuring Tie\nStrength in Implicit Social Networks. In Proceedings\nof the 4th Annual ACM Web Science Conference , Web-\nSci ’12, pages 109–118, New York, NY , USA, 2012.\nACM.\n[18] Xiao Hu and Jin Ha Lee. A Cross-cultural Study of\nMusic Mood Perception Between American and Chi-\nnese Listeners. In Proceedings of the 13th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , Porto, Portugal, October 2012.\n[19] Maurice G. Kendall. A New Measure of Rank Correla-\ntion. Biometrika , 30(1-2):81–93, 1938.\n[20] Shinobu Kitayama and Hyekyung Park. Cultural shap-\ning of self, emotion, and well-being: How does it\nwork? Social and Personality Psychology Compass ,\n1(1):202–222, 2007.\n[21] Gopala Krishna Koduri. Culture-aware approaches to\nmodeling and description of intonation using multi-\nmodal data. In International Conference on Knowledge\nEngineering and Knowledge Management , pages 209–\n217. Springer, 2014.\n[22] Jin Ha Lee and Xiao Hu. Cross-cultural similarities\nand differences in music mood perception. iConference\n2014 Proceedings , 2014.\n[23] Adam J Lonsdale and Adrian C North. Musical taste\nand ingroup favouritism. Group Processes & Inter-\ngroup Relations , 12(3):319–327, 2009.\n[24] Peter V Marsden and Karen E Campbell. Measuring tie\nstrength. Social forces , 63(2):482–501, 1984.\n[25] Cedric S. Mesnage, Asma Raﬁq, Simon Dixon, and\nRomain Brixtel. Music Discovery with Social Net-\nworks. In Proceedings of the 2nd Workshop on\nMusic Recommendation and Discovery (WOMRAD) ,\nChicago, IL, USA, October 2011.684 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[26] Adrian C. North and David J. Hargreaves. Situational\ninﬂuences on reported musical preference. Psychomu-\nsicology: Music, Mind and Brain , 15(1-2):30–45,\n1996.\n[27] Martin Pichl, Eva Zangerle, G ¨unther Specht, and\nMarkus Schedl. Mining culture-speciﬁc music listen-\ning behavior from social media data. In 2017 IEEE\nInternational Symposium on Multimedia (ISM) , pages\n208–215. IEEE, 2017.\n[28] Dominic Power And and Daniel Hallencreutz. Com-\npetitiveness, local production systems and global com-\nmodity chains in the music industry: entering the us\nmarket. Regional Studies , 41(3):377–389, 2007.\n[29] Peter J Rentfrow and Samuel D Gosling. The do re mi’s\nof everyday life: The structure and personality corre-\nlates of music preferences. Journal of personality and\nsocial psychology , 84(6):1236, 2003.\n[30] Peter J Rentfrow and Samuel D Gosling. Message in\na ballad: The role of music preferences in interper-\nsonal perception. Psychological science , 17(3):236–\n242, 2006.\n[31] Paul Rutten. Local popular music on the national and\ninternational markets. Cultural Studies , 5(3):294–305,\n1991.\n[32] Thomas Sch ¨afer. The goals and effects of music lis-\ntening and their relationship to the strength of music\npreference. PloS one , 11(3):e0151634, 2016.\n[33] Thomas Sch ¨afer, Peter Sedlmeier, Christine St ¨adtler,\nand David Huron. The psychological functions of mu-\nsic listening. Frontiers in psychology , 4:511, 2013.\n[34] Markus Schedl. Ameliorating music recommendation:\nIntegrating music content, music context, and user con-\ntext for improved music retrieval and recommendation.\nInProceedings of International Conference on Ad-\nvances in Mobile Computing & Multimedia , MoMM\n’13, pages 3:3–3:9, New York, NY , USA, 2013. ACM.\n[35] Markus Schedl. The LFM-1b Dataset for Music Re-\ntrieval and Recommendation. In ACM International\nConference on Multimedia Retrieval (ICMR) , pages\n103–110. ACM, 2016.\n[36] Markus Schedl. Investigating country-speciﬁc music\npreferences and music recommendation algorithms\nwith the LFM-1b dataset. International Journal of\nMultimedia Information Retrieval , 6(1):71–84, 2017.\n[37] Markus Schedl. Investigating country-speciﬁc music\npreferences and music recommendation algorithms\nwith the lfm-1b dataset. International journal of multi-\nmedia information retrieval , 6(1):71–84, 2017.\n[38] Markus Schedl and Christine Bauer. Introducing\nGlobal and Regional Mainstreaminess for Improving\nPersonalized Music Recommendation. In Proceedingsof the 15th International Conference on Advances\nin Mobile Computing & Multimedia (MoMM 2017) ,\nSalzburg, Austria, December 2017.\n[39] Markus Schedl and Christine Bauer. Online music lis-\ntening culture of kids and adolescents: Listening analy-\nsis and music recommendation tailored to the young. In\n11th ACM Conference on Recommender Systems (Rec-\nSys 2017): International Workshop on Children and\nRecommender Systems (KidRec 2017) , New York, NY ,\n2017. ACM.\n[40] Markus Schedl and Bruce Ferwerda. Large-Scale\nAnalysis of Group-Speciﬁc Music Genre Taste from\nCollaborative Tags. In Proceedings of the 19th IEEE\nInternational Symposium on Multimedia (ISM 2017) ,\nTaichung, Taiwan, December 2017.\n[41] Markus Schedl and David Hauger. Tailoring Music\nRecommendations to Users by Considering Diversity,\nMainstreaminess, and Novelty. In Proc. of SIGIR ,\npages 947–950, Santiago, Chile, 2015.\n[42] Markus Schedl, Peter Knees, Brian McFee, Dmitry\nBogdanov, and Marius Kaminskas. Music recom-\nmender systems. In Recommender Systems Handbook ,\npages 453–492. Springer, 2015.\n[43] Maarten HW Selfhout, Susan JT Branje, Tom FM ter\nBogt, and Wim HJ Meeus. The role of music prefer-\nences in early adolescents friendship formation and sta-\nbility. Journal of Adolescence , 32(1):95–107, 2009.\n[44] Abhishek Singhi and Daniel G Brown. On cultural, tex-\ntual and experiential aspects of music mood. In ISMIR ,\npages 3–8, 2014.\n[45] Catherine J Stevens. Music perception and cognition:\nA review of recent cross-cultural research. Topics in\ncognitive science , 4(4):653–667, 2012.\n[46] Tom F.M. ter Bogt, Marc J.M.H. Delsing, Maarten van\nZalk, Peter G. Christenson, and Wim H.J. Meeus. In-\ntergenerational continuity of taste: parental and adoles-\ncent music preferences. Social Forces , 90(1):297–319,\n2011.\n[47] Gabriel Vigliensoni and Ichiro Fujinaga. Automatic\nMusic Recommendation Systems: Do Demographic,\nProﬁling, and Contextual Features Improve Their Per-\nformance? In 17th International Society for Music\nInformation Retrieval Conference (ISMIR) , pages 94–\n100, 2016.\n[48] Jef Vlegels and John Lievens. Music classiﬁcation,\ngenres, and taste patterns: A ground-up network anal-\nysis on the clustering of artist preferences. Poetics ,\n60:76–89, 2017.\n[49] Xinxi Wang, David Rosenblum, and Ye Wang.\nContext-aware Mobile Music Recommendation for\nDaily Activities. In Proceedings of the 20th ACM In-\nternational Conference on Multimedia , pages 99–108,\nNara, Japan, 2012. ACM.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 685[50] Daniel Wolff and Tillman Weyde. Learning music sim-\nilarity from relative user ratings. Information retrieval ,\n17(2):109–136, 2014.\n[51] Shanyang Zhao and David Elesh. Copresence as ’be-\ning with’: Social contact in online public domains. In-\nformation, Communication & Society , 11(4):565–583,\n2008.686 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Improving Bass Saliency Estimation Using Transfer Learning and Label Propagation.",
        "author": [
            "Jakob Abeßer",
            "Stefan Balke",
            "Meinard Müller"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492411",
        "url": "https://doi.org/10.5281/zenodo.1492411",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/143_Paper.pdf",
        "abstract": "In this paper, we consider two methods to improve an algorithm for bass saliency estimation in jazz ensemble recordings which are based on deep neural networks. First, we apply label propagation to increase the amount of training data by transferring pitch labels from our labeled dataset to unlabeled audio recordings using a spectral similarity measure. Second, we study in several transfer learning experiments, whether isolated note recordings can be beneficial for pre-training a model which is later fine-tuned on ensemble recordings. Our results indicate that both strategies can improve the performance on bass saliency estimation by up to five percent in accuracy.",
        "zenodo_id": 1492411,
        "dblp_key": "conf/ismir/AbesserBM18",
        "keywords": [
            "bass saliency estimation",
            "deep neural networks",
            "label propagation",
            "spectral similarity measure",
            "transfer learning",
            "isolated note recordings",
            "ensemble recordings",
            "pre-training",
            "accuracy",
            "five percent"
        ],
        "content": "IMPROVING BASS SALIENCY ESTIMATION USING LABEL\nPROPAGATION AND TRANSFER LEARNING\nJakob Abeßer1Stefan Balke2Meinard M ¨uller2\n1Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany\n2International Audio Laboratories Erlangen, Germany\njakob.abesser@idmt.fraunhofer.de\nABSTRACT\nIn this paper, we consider two methods to improve an algo-\nrithm for bass saliency estimation in jazz ensemble record-\nings which are based on deep neural networks. First, we\napply label propagation to increase the amount of training\ndata by transferring pitch labels from our labeled dataset to\nunlabeled audio recordings using a spectral similarity mea-\nsure. Second, we study in several transfer learning exper-\niments, whether isolated note recordings can be beneﬁcial\nfor pre-training a model which is later ﬁne-tuned on en-\nsemble recordings. Our results indicate that both strategies\ncan improve the performance on bass saliency estimation\nby up to ﬁve percent in accuracy.\n1. INTRODUCTION\nRecent developments in the ﬁeld of machine learning, in\nparticular deep learning, stimulated a signiﬁcant perfor-\nmance boost in various Music Information Retrieval (MIR)\ntasks [7] such as audio tagging [23], audio source separa-\ntion [27], and automatic music transcription (AMT) [15].\nOne major challenge in training deep neural networks\n(DNNs) that generalize well to unseen data lies in the large\namount of required labeled training data, which is often not\navailable.\nIn this context, semi-supervised learning strategies can\nhelp to solve this data problem. A ﬁrst approach is to ap-\nplytransfer learning , i. e., training a network on a related\nclassiﬁcation task and ﬁne-tune the model parameters for\nthe target task with the (usually smaller) amount of train-\ning data at hand [10, 18]. Both training steps are fully su-\npervised and therefore require labeled datasets. A second\napproach is label propagation , where labels from labeled\nfeature vectors are propagated to unlabeled feature vectors\nif some pre-deﬁned similarity measure exceeds a particu-\nlar threshold. Label propagation can help to signiﬁcantly\nincrease the amount of available training data.\nIn this paper, we focus on the task of estimating the\npitch salience of the bass instrument in jazz ensemble\nc\rJakob Abeßer, Stefan Balke, Meinard M ¨uller. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Jakob Abeßer, Stefan Balke, Meinard M ¨uller. “Im-\nproving Bass Saliency Estimation using Label Propagation and Trans-\nfer Learning”, 19th International Society for Music Information Retrieval\nConference, Paris, France, 2018.\nFigure 1 . Flowchart summarizing the main idea of training\na deep neural network to learn a mapping function from a\nconstant-Q spectrogram of a jazz ensemble recording (left)\ntowards a bass pitch saliency representation (right) using a\ndeep neural network.\nrecordings. In general, pitch saliency refers to a likelihood\nmeasure of an instrument playing certain pitch frequen-\ncies at given times. Figure 1 illustrates the DNN-based\napproach that we use. Given a time-frequency represen-\ntation of an audio recording of a jazz ensemble, the goal is\nto estimate a bass salience representation in a frame-wise\nfashion. As outlined in [1], these frame-wise estimates of\nthe bass saliency can then be aggregated using beat anno-\ntations to obtain a beat-wise pitch representation, which is\na musically meaningful approximation of the commonly\nplayed walking bass lines in jazz music.\nAs the main contributions of this paper, we investigate\ntransfer learning and label propagation strategies for im-\nproving fully-connected deep neural networks for the task\nof bass saliency estimation, as shown in Figure 2. Both\ntechniques aim to compensate the lack of available la-\nbeled data for the task of bass salience estimation. For\nlabel propagation, the core idea is to enrich an unlabeled\ndataset with labels from a labeled dataset. For transfer\nlearning, we investigate whether training models on mu-\nsic data of lower timbral complexity (e. g., isolated instru-\nment tones) is beneﬁcial for transferring them to complex\nmixture recordings.\nThe remainder of this paper is structured as follows. In\nSection 2, we review related work. In Section 3, we in-\ntroduce the underlying datasets used throughout our ex-\nperiments and propose additional data augmentation steps.\nSection 4 introduces the feature extraction approach, DNN\narchitecture, and the evaluation methodology. In Section 5,306Pre-Training\n!\nTraining!∗\nLabeledDatasetUnlabeledDataset\nLabel PropagationTransfer Learning\nBestSimilarityMeasureFigure 2 . Illustration of label propagation and transfer learning. In label propagation (left), an unlabeled audio dataset is\nenriched with frame-wise pitch labels from a labeled dataset. In transfer learning (right), a DNN is ﬁrst pre-trained on a\ndataset with lower complexity (isolated bass recordings) and then trained further on jazz ensemble recordings.\nwe present experiments towards hyperparameter optimiza-\ntion (Section 5.1), label propagation (Section 5.2), as well\nas transfer learning (Section 5.3). Finally, Section 6 con-\ncludes our work and gives perspectives towards future\nwork.\n2. RELATED WORK\nSalience representations are a common intermediate rep-\nresentation in many automatic music transcription (AMT)\nsystems prior to the formation of note events. Most previ-\nous approaches for bass saliency estimation rely on hand-\ncrafted algorithms rather than on automatically learnt map-\npings. For instance, Goto derives pitch saliency values\nfrom predominant peaks in a spectral representation based\non instantaneous frequency values [13]. Ryyn ¨anen and\nKlapuri compute a saliency measure for a given pitch from\na weighted sum over the spectral magnitude values at its\nharmonic frequencies [24]. Salamon et al. apply har-\nmonic summation based on a logarithmic frequency repre-\nsentation combined with instantaneous frequency estima-\ntion methods [25].\nIn [1], the mapping from a constant-Q spectrogram to a\nbass saliency function is automatically learnt using fully-\nconnected deep neural networks. The authors also investi-\ngated a semi-supervised learning step where parts of pre-\ndicted pitch saliency estimations on unlabeled audio data\nwere added to the training data based on a sparsity cri-\nterion. The modeling strategy was inspired by Balke et\nal. [2], who used a similar approach to estimate a saliency\nrepresentation of the predominant melody instrument in\njazz music recordings. Bittner et al. [4] proposed a fully\nconvolutional neural network (CNN) to extract a saliency\nrepresentations from different constant-Q transforms used\nas input for both multiple fundamental frequency estima-\ntion and melody tracking.\nModels with state-of-the-art performance in related dis-\nciplines such as image processing (mostly CNN-based\nmodels) are rarely trained from scratch due to the large\namount of required training data. Instead, only the lastDataset Usage Labels # Feature Vectors Duration [h]\nISO+Training X 448,626 5.79\nWJD+Training X 305,507 3.94\nWJD\u0000Training - 500,000 6.45\nWJD+-TEST Test X 8,318 0.1\nTable 1 . Summary of the datasets. The number of feature\nvectors after data augmentation and voiced frame selection\nas well as the corresponding duration in hours is given in\nthe last two columns. For the WJD\u0000dataset, 500,000 fea-\nture vectors were randomly selected due to memory limi-\ntations on the hardware in use for the experiments.\nlayers of existing “general purpose” classiﬁcation models\n(such as the ImageNet model [9]) are ﬁne-tuned for re-\nlated classiﬁcation tasks using smaller amounts of training\ndata [21]. Similarly, in the ﬁeld of MIR, Choi et al. [8]\nused a pre-trained CNN-based feature extractor trained on\nmusic tagging data for related music classiﬁcation and re-\ngression tasks. However, for the task of AMT, no such\ngeneral-purpose model was established so far.\n3. DATASETS\nThe spectral characteristics of the targeted upright bass\ntones are affected by different factors of variation such as\nthe pitch, the loudness, as well as the overlap with tones\nfrom simultaneously playing instruments. In our consid-\nered datasets, we use different sets of upright bass record-\nings that try to address these variations. All considered\naudio ﬁles used in this paper include an acoustic upright\nbass played with the plucked (pizzicato) plucking style—\nas opposed to using a bow—as this is the common playing\nstyle for jazz bass players. Table 1 gives an overview of\nthe datasets used, which we discuss in the following.\n3.1 Isolated Upright Bass Recordings (ISO+)\nThe ISO+dataset is a collection of isolated chromatic note\nrecordings. The recordings stem from various commercial\nand non-commercial upright bass sample datasets: AdamProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 307010Duration [min]ISO+\n010Duration [min]WJD+\nC2 C3 C4\nPitch0.00.5Duration [min]WJD+-TESTFigure 3 . Pitch histogram over labeled datasets ISO+,\nWJD+, and WJD+-TEST after data augmentation. Total\nduration of all notes in minutes is shown for each pitch.\nMonroe’s Upright Bass Sample Library , Meatbass , Tril-\nlian , Steinberg Halion Symphonic Orchestra, and SWAM\nDouble Bass. Furthermore, we collected recordings from\nthe Real World Computing Music Database (RWC) [14],\nthe McGill University Master Samples [11] and the Iowa\nClassical Instrument Samples1.\n3.2 Jazz Ensemble Recordings (WJD)\nThe Weimar Jazz Database (WJD) contains 456 manually\ntranscribed solos from famous jazz recordings [22]. For a\nsubset of 40 of these recordings, excerpts of walking bass\nlines using the Sonic Visualiser software [6]. All of the\nselected recordings are typical jazz ensembles that consist\nof upright bass, drums, piano, as well as melody instru-\nments such as trumpet or saxophone. We use 30 of these\nannotated recordings for training (WJD+) and 10 for test-\ning (WJD+-TEST). The remaining 416 recordings from\nthe WJD are denoted by WJD\u0000. These recordings come\nwithout bass pitch annotations and will be used in the label\npropagation experiment detailed in Section 5.2.\n3.3 Data Augmentation\nOn the datasets that we use for supervised training (ISO+\nand WJD+), we generated 15 augmented versions from\neach original audio ﬁle by combining three time-stretching\nsettings (stretch factors 0.9, 1, and 1.1) and ﬁve pitch-\nshifting settings (shifts between -2 and +2 semitones) using\nthe software package sox2.\nFor all labeled datasets, we discard all non-voiced\nframes. Furthermore, we only keep the spectral frames\nfrom the ﬁrst 75 % of the note duration as especially higher\nharmonics from upright bass tones decay much faster than\nthe fundamental frequency contours. In order to make the\nﬁnal results comparable to [1], data augmentation is not\napplied to the test set WJD+-TEST (compare Section 3).\nFigure 3 illustrates the pitch distribution over the three\nlabeled datasets after applying data augmentation. While\nthe WJD+and WJD+-TEST datasets similarly include\n1http://theremin.music.uiowa.edu/MIS.html\n2http://sox.sourceforge.netHyperparameter Search Space Importance\nMagnitude scaling flinear , logarithmicg 0.015\n# hidden layers n2f3;4;5;6g 0.020\nHidden layer size H= 2h,h2f7;8;9;10g 0.040\nLearning rate \u000b= 10r,r2[\u00003;\u00006], (-4.27 )0.485\nBatch normalization fno, yesg 0.017\n`2weight regularization \u00152f0;10\u00004;10\u00003;10\u00002g 0.038\nDropout ratio d2[0;0:5],(0.06) 0.385\nTable 2 . Search space for hyperparameter optimization.\nOptimal parameter set for aval, opt = 0:62is given in bold\nfont. Feature importance values in a random forest regres-\nsion model are shown in the last column.\nnotes up to C4, the isolated tones in ISO+cover a wider\npitch range and distribute among the pitches in a more bal-\nanced fashion.\n4. METHODOLOGY\n4.1 Feature Extraction\nAudio ﬁles are resampled to 22.05 kHz before constant-Q\nmagnitude spectrograms are computed with a hopsize of\n1024 samples (46.4 ms) and a frequency resolution of 12\nbins per octave between 34.65 Hz (MIDI pitch 25, note\nD[1) and 1567 Hz (MIDI pitch 91, note G6) using the li-\nbrosa Python library [19]. Hence, the input vectors have\nthe dimensionality of 67. In contrast to [1], we extend the\nfrequency range by a small margin in the low frequency\nrange and by two octaves in the upper frequency range in\norder to incorporate overtone frequencies of higher bass\nnotes. In Section 5.1 we evaluate, to which extent a loga-\nrithmic compression of the magnitude spectrogram is ben-\neﬁcial for bass saliency.\n4.2 Deep Neural Network Architecture\nThroughout this paper, we use a fully-connected network\narchitecture for the given task of bass saliency estimation,\nsee Table 2 for an overview of parameters. The model is a\ncascade ofnhidden layers of size Hwith optional interme-\ndiate layers for batch normalization (prior to the ReLU ac-\ntivation function) [16] and dropout (dropout ratio d) [26].\nIn contrast to [1], we do not use frame stacking here as\nwe aim to directly compare local feature vectors later in\nthe label propagation step described in Section 5.2. The\nmodel instead processes individual spectrogram frames as\ninput and predicts the corresponding pitch saliency vector.\nFurthermore, all hidden layers have an optional `2weight\nregularization (with regularization parameter \u0015) [12]. For\neach model training, we use 500 training epochs, a batch\nsize of 250, early stopping with a patience of 25 epochs\nbased on the validation accuracy, and the categorical cross-\nentropy as loss function. The keras3Python library is used\nfor all experiments in this paper.\nScore annotations are converted into frame-wise binary\npitch activities, which are used as targets for the model\ntraining. In the annotated datasets used in this paper, bass\nlines are strictly monophonic. In the ﬁnal layer, we use a\n3https://www.keras.io308 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018−6 −5 −4 −30.000.250.50avalr\n0.0 0.2 0.40.000.250.50avald\nlog None0.000.250.50avalMagnitude scaling\n5 6 7 8 90.000.250.50avalhFigure 4 . Validation accuracy avaland hyperparameter val-\nues for learning rate exponent r, dropout ratio d, magni-\ntude scaling, and layer size exponent hover all random pa-\nrameter conﬁgurations (black dots). Cubic regression lines\n(blue dashed lines) show trends in the data. Optimal values\nfor the other hyperparameters are given in Table 2.\nsigmoid instead of a softmax activation function to be able\nto model the activity of all pitches independently. This\nalso allows us to model polyphonic parts or rests within\nbass lines. However, in order to compare our results with\n[1], we only focus on bass saliency estimation from voiced\nframes in this paper and leave bass voicing detection open\nfor future work.\nAs pitch range for the targets, we use [26;69](notes\nD1 to G4) whereas in [1], a slightly smaller pitch range\n[28;67](notes E1 to F4) was used. The dimensionality of\nthe target vectors is 44.\n4.3 Evaluation\nWe derive pitch estimates by looking at the highest output\nvalue of the ﬁnal sigmoid layer. For the evaluation, we use\nthe standard evaluation measures Raw Pitch Accuracy (de-\nnoted asa) and Raw Chroma Accuracy (denoted asa12) as\nused in the MIREX Audio Melody Extraction task. For the\ndeﬁnition of these measures, we refer to [20]. During train-\ning, we randomly split the training dataset(s) into training\nand validation dataset based on a 80:20 split. Accuracy\nvaluesatrain,aval, andatestare computed on the training,\nvalidation, and test set, respectively.\n5. EXPERIMENTS\n5.1 Hyperparameter Optimization\nA systematic grid search of possible hyperparameter com-\nbinations is not feasible for deep neural networks due to\nthe high computational costs. In our approach, we train\n160 models with different combinations of hyperparame-\nters. These combinations are randomly sampled from the\nhyperparameters given in Table 2. The best hyperparame-\nter combination is then retrieved by testing the model per-\nformance on the validation set ( aval). Figure 4 shows the\nvalidation set accuracy for the different hyperparameter\ncosSimcosSimcosSimcosSimcosSimBb3Bb3Bb3A3G3Best MatchFigure 5 . Label propagation example: given a query\nconstant-Q spectrogram frame with unknown pitch (bot-\ntom, red), candidates with different cosSim similarity val-\nues are shown (above, black). The pitch label B[3 will be\ntransferred from the most similar candidate shown on top\n(s= 0:99).\nconﬁgurations. To get an intuition about the inﬂuence of\nthe different hyperparameters, we follow an approach pre-\nviously presented in [17]. In that approach, a random forest\nregression model [5] is ﬁtted to avalover all parameter con-\nﬁgurations. From the random forest regression model, we\ncan obtain the relative importance of all hyperparameters,\nsee Table 2 (third column) for the results.\nAs previously found in [17], the learning rate exponent\nris by far the most important hyperparameter (0.485) with\noptimal values around 10\u00004, as shown in Figure 4. Inter-\nestingly, as indicated in Table 2, the dropout ratio dalso\nhas a high relative importance (0.385) and an optimal value\nonly slightly above zero.\n5.2 Label Propagation\nA ﬁrst approach to enrich the available amount of training\ndata is to use label propagation. We derive pitch labels for\nfeature vectors in the unlabeled WJD\u0000dataset by transfer-\nring labels from their most similar counterparts in WJD+\ndataset. To this end, we compute a similarity score sifor\nthei-th feature vector in the WJD\u0000databasexWJD\u0000\ni2R67\nby maximizing its cosine similarity ( cosSim ) towards all\nfeature vectors in the WJD+database as\nsi= max\nkcosSim(xWJD\u0000\ni;xWJD+\nk ): (1)\nAn example is shown in Figure 5. Given a query spectro-\ngram frames (bottom), we show ﬁve example spectrogram\nframes with different similarity values. The most similar\nframe ( cosSim = 0:99) shows an almost identical overtone\nstructure, which motivates the transfer of its pitch label.\nAs shown in Figure 6, most feature vectors in the un-\nlabeled WJD\u0000dataset have very similar counterparts in\nthe WJD+dataset, which is somewhat intuitive as both\ndatasets originate from the Weimar Jazz Database (WJD).\nWe derive three similarity thresholds \u001c25= 0:914,\u001c50=\n0:940, and\u001c75= 0:96from the 25th, 50th, and 75th\npercentile of the distribution over s. The label-enriched\nWJD\u0000dataset is denoted as (WJD\u0000)+in the following.\nWe use the best model architecture obtained via hy-\nperparameter optimization (see Section 5.1) and trainProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 3090.75 0.80 0.85 0.90 0.95 1.00\ns0.000.050.10\nτ25 τ50τ75Figure 6 . Histogram over best-match cosine-similarity\nvalues for mapping feature vectors from the unlabeled\ndataset WJD\u0000to the labeled dataset WJD+. Similarity\nthresholds\u001c25,\u001c50, and\u001c75are derived from the respec-\ntive percentiles of the distribution over s.\nmodels from different training sets. For that purpose,\nwe combine the full WJD+dataset with feature vec-\ntors of the (WJD\u0000)+dataset based on the criterion\n\u001c\u0000\u0014si\u0014\u001c+. We test different pairs (\u001c\u0000;\u001c+)using\ncombinations of the percentile-based thresholds \u001c25,\u001c50,\nand\u001c75as well as\u001c0= 0 and\u001c100= 1 as shown in the\nlower subplot of Figure 7.\nFeature vectors with lower similarity scores more likely\nintroduce label noise to the mixed training dataset. Since\nthe WJD+dataset only contains voiced frames, even un-\nvoiced frames in the WJD\u0000will be be mapped to voiced\nframes. This is a drawback due to the given dataset conﬁg-\nurations. Another possible reason for low similarity scores\nare notes played by other instruments in the ensemble such\nas the piano or the soloist. However, voiced frames from\nthe WJD\u0000database with a lower similarity can provide\nnovel information for the classiﬁcation task, which can\nhelp to improve the existing model. At the same time, fea-\nture vectors with high similarity scores can be redundant\nwithout providing much novel information for the pitch\nsaliency estimation task. In contrast to [1], label propa-\ngation is performed based on feature vector similarity and\nnot based on predictions of existing models.\nFrom the results shown in Figure 7, we make the fol-\nlowing observations for all conﬁgurations. First, we ob-\nserve that difference between atrainandaval(overﬁtting)\nremains almost constant across different conﬁgurations.\nAlso, the raw chroma accuracy atest,12 is consistently about\n0.07 higher than the raw pitch accuracy atest, which indi-\ncates that octave errors make up only a small fraction of\nthe remaining pitch estimation errors.\nUsing the WJD+dataset alone or combined with the\nmost similar feature vectors in WJD\u0000(conﬁgurations 0:0\nand 75:100), we observe that the training and validation ac-\ncuracies are clearly higher than the test accuracy. The rea-\nson is that these conﬁgurations correspond to the best pa-\nrameter settings found in the hyperparameter optimization\nstep (compare Section 5.1), where maximizing avalwas the\nmain objective. Due to their small size, the data distribu-\ntion in the WJD+and WJD+-TEST datasets presumably\nis only similar to a certain degree although both are taken\nfrom the Weimar Jazz Database.\nIn contrast, by adding feature vectors from the WJD\u0000\ndataset with lower similarity values and higher novelty\n(conﬁgurations 0:25, 0:50, and 0:75), the modeling task\n0.50.60.7Accuracyatrain aval atest atest, 12\n0:00:25 0:50 0:7525:100 50:100 75:10025:75 0:100\nLower and upper bound τ−=τi and τ+=τj denoted as i:jτ0τ25τ50τ75τ100Figure 7 . Label propagation results for different dataset\nconﬁgurations (see Section 5.2). Training accuracy atrain,\nvalidation set accuracy aval, test set accuracy atest, and\nchroma pitch accuracy atest,12 are shown.\nbecomes harder and the training and validation accuracies\ndecrease. Interestingly, the models’ ability to generalize\nto the test set improves and atestincreases. The relatively\nhigh difference between validation and test accuracy of up\nto 0.09 indicates that the small test set size needs to be\nincreased in future work, as both, test and validation set,\nshould come from the same distribution.\nFor the conﬁgurations 0:50 and 0:100, we observe the\nhighest test accuracy of around atest= 0:57. This result\nis notable as by using label propagation, we are able to\ntrain a model which achieves a performance comparable\nto the highest test accuracy reported in [1] without requir-\ning additional temporal context information using frame\nstacking. Therefore, label propagation seems a promising\napproach to improve the model performance.\n5.3 Transfer Learning\nState-of-the-art music transcription algorithms based on\nspectral decomposition algorithms such as Non-Negative\nMatrix Factorization (NMF) are commonly initialized with\nisolated instrument tones, e. g., for learning spectral note\ntemplates [3]. We aim to investigate to which extent a sim-\nilar strategy can be used to improve neural networks for\npitch saliency estimation tasks. As an alternative, we want\nto ﬁnd out if it is instead better to train the networks solely\non more complex instrumental mixtures (ensemble record-\nings, see Section 3.2) as these are more similar to the ﬁnal\ntest data.\nWe compare three training scenarios in our experiment.\nFirst, we train the model solely using the isolated bass\ntones (ISO+dataset) to evaluate the generalization poten-\ntial of the trained model towards mixture signals in the test\nset. Secondly, we apply transfer learning, i. e., we pre-train\nan initial model for bass saliency estimation using isolated\nbass tracks (ISO+dataset) and then ﬁne-tune the model in\na second training step on the WJD+dataset. In a third sce-\nnario, we mix and shufﬂe the WJD+and ISO+datasets\nand perform a single training step. The model trained only310 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018on the WJD+dataset serves as baseline for comparison.\nWe train for 750 epochs for both training steps but apply\nearly stopping as detailed in Section 4.2 if possible.\nThe results are shown in Table 3. Accuracy values are\ncomputed on a macro-level by averaging across all spec-\ntrogram frames of the test set ﬁles. We observe that a pitch\nsaliency model, which is only trained on isolated tones\nof the target instrument, is not capable to generalize to\nmore complex mixtures as it performs poorly on the test\nset (atest= 0:095). Combining pre-training on the isolated\nnote database with ﬁne-tuning on the mixture dataset im-\nproves the performance by around four percent on the test\nset accuracy ( atest= 0:542) compared to a baseline model,\nwhich is only trained on the mixture recordings. The best\nconﬁguration improves on the test set accuracy by 6 per-\ncent (atest= 0:561) compared to the baseline model. It\ndoes not involve a pre-training step but uses a mix of iso-\nlated and mixed recordings (ISO+and WJD+) for training\ninstead.\nThe results of the transfer learning experiments suggest\nthat combining training data with different levels of com-\nplexity, i. e., different amount of instrumental overlap, can\nbe useful to improve DNN-based models for pitch saliency\nestimation in ensemble recordings. By using a mixture of\nboth isolated and mixed recordings in one training step, it\nappears as if the neural network learns best to “focus” on\nthe targeted instrument. Future work could address a dif-\nferent order in the training process, i. e., ﬁrst training on\nthe mixture tracks and then ﬁne-tuning the model on the\nisolated note recordings.\n6. CONCLUSION\nWe investigated strategies for label propagation and trans-\nfer learning in order to improve bass saliency estimation\nusing deep neural networks. We could show that unla-\nbeled feature vectors from datasets with a similar spec-\ntral distribution as the target scenario can be mapped to-\nwards labeled datasets to derive pitch labels. By combin-\ning labeled datasets and unlabeled datasets through label\npropagation, we were able to improve the model’s accu-\nracy by around six percent compared to a baseline model.\nSimilarly, we could show that by combining isolated note\nrecordings of the targeted instrument with mixture record-\nings as training set, we gain around ﬁve percent in accu-\nracy. This joint training slightly outperformed our consid-\nered transfer learning strategy with two successive training\nsteps. Future work could deal with strategies on how to\ncombine frame-stacking (compare [1]), label propagation,\nand transfer learning.\nFor a systematic bias–variance analysis of the given\nmodeling task, it remains challenging to deﬁne human\nlevel performance as we only focus on frame-wise pitch\nestimation here. Human experts, i. e., musicians or mu-\nsicologists, are capable to generate near-perfect note-wise\ntranscriptions. This related task, however, involves listen-\ning to longer audio excerpts and allows to include addi-\ntional cues from the metric structure, tone duration, and\nlocal harmony.Pre-\nTrainingTraining atrainavalatestatest,12\n- WJD+(baseline) 0.665 0.614 0.508 0.589\n- ISO+0.514 0.538 0.095 0.234\nISO+WJD+0.652 0.603 0.542 0.614\n- ISO+& WJD+0.507 0.531 0.561 0.655\nTable 3 . Performance comparison with and without trans-\nfer learning. All experiments were evaluated using the\nWJD+-TEST dataset.\n7. ACKNOWLEDGEMENTS\nThis work has been supported by the German Research\nFoundation (MU 2686/11-1, AB 675/2-1). The Interna-\ntional Audio Laboratories Erlangen are a joint institution\nof the Friedrich-Alexander-Universit ¨at Erlangen-N ¨urnberg\n(FAU) and Fraunhofer Institut f ¨ur Integrierte Schaltungen\nIIS.\n8. REFERENCES\n[1] Jakob Abeßer, Stefan Balke, Klaus Frieler, Martin\nPﬂeiderer, and Meinard M ¨uller. Deep learning for jazz\nwalking bass transcription. In Proceedings of the AES\nInternational Conference on Semantic Audio , pages\n202–209, Erlangen, Germany, 2017.\n[2] Stefan Balke, Christian Dittmar, Jakob Abeßer, and\nMeinard M ¨uller. Data-driven solo voice enhancement\nfor jazz music retrieval. In Proceedings of the IEEE\nInternational Conference on Acoustics, Speech, and\nSignal Processing (ICASSP) , pages 196–200, New Or-\nleans, USA, 2017.\n[3] Emmanouil Benetos and Tillman Weyde. An efﬁ-\ncient temporally-constrained probabilistic model for\nmultiple-instrument music transcription. In Proceed-\nings of the International Society for Music Informa-\ntion Retrieval Conference (ISMIR) , pages 701–707,\nM´alaga, Spain, 2015.\n[4] Rachel M. Bittner, Brian McFee, Justin Salamon, Pe-\nter Li, and Juan P. Bello. Deep salience representa-\ntions for F0 tracking in polyphonic music. In Proceed-\nings of the International Society for Music Information\nRetrieval Conference (ISMIR) , pages 63–70, Suzhou,\nChina, 2017.\n[5] Leo Breiman. Random forests. Machine Learning ,\n45(1):5–32, 2001.\n[6] C. Cannam, C. Landone, and M. Sandler. Sonic vi-\nsualiser: An open source application for viewing,\nanalysing, and annotating music audio ﬁles. In Pro-\nceedings of the ACM Multimedia 2010 International\nConference , pages 1467–1468, Firenze, Italy, October\n2010.\n[7] Keunwoo Choi, Gy ¨orgy Fazekas, Kyunghyun Cho, and\nMark Sandler. A Tutorial on Deep Learning for Music\nInformation Retrieval. ArXiv e-prints , September 2017.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 311[8] Keunwoo Choi, Gy ¨orgy Fazekas, Mark B. Sandler, and\nKyunghyun Cho. Transfer learning for music classiﬁ-\ncation and regression tasks. In Proceedings of the Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR) , pages 141–149, Suzhou, China, 2017.\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai\nLi, and Li Fei-Fei. Imagenet: A large-scale hierarchi-\ncal image database. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR) , pages 248–255, Miami, FL, USA, 2009.\n[10] Aleksandr Diment and Tuomas Virtanen. Transfer\nlearning of weakly labelled audio. In Proceedings of\nthe IEEE Workshop on Applications of Signal Process-\ning to Audio and Acoustics (WASPAA) , pages 6–10,\nNew Paltz, NY , USA, 2017.\n[11] Tuomas Eerola and Rafael Ferror. Instrument library\n(MUMS) revised. Music Perception , 25(3):253–255,\n2008.\n[12] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\nDeep Learning . MIT Press, 2016.\n[13] Masataka Goto. A real-time music-scene-description\nsystem: Predominant-F0 estimation for detecting\nmelody and bass lines in real-world audio signals.\nSpeech Communication (ISCA Journal) , 43(4):311–\n329, 2004.\n[14] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC music database:\nMusic genre database and musical instrument sound\ndatabase. In Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 229–230, Baltimore, Maryland, USA, 2003.\n[15] Curtis Hawthorne, Erich Elsen, Jialin Song, Adam\nRoberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev\nOore, and Douglas Eck. Onsets and Frames: Dual-\nObjective Piano Transcription. ArXiv e-prints , October\n2017.\n[16] Sergey Ioffe and Christian Szegedy. Batch normaliza-\ntion: Accelerating deep network training by reducing\ninternal covariate shift. In Proceedings of the Interna-\ntional Conference on International Conference on Ma-\nchine Learning (ICML) , pages 448–456, 2015.\n[17] Rainer Kelz, Matthias Dorfer, Filip Korzeniowski, Se-\nbastian B ¨ock, Andreas Arzt, and Gerhard Widmer. On\nthe potential of simple framewise approaches to piano\ntranscription. In Proceedings of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 475–481, New York City, USA, 2016.\n[18] Anurag Kumar, Maksim Khadkevich, and Christian\nF¨ugen. Knowledge Transfer from Weakly Labeled Au-\ndio using Convolutional Neural Network for Sound\nEvents and Scenes. ArXiv e-prints , November 2017.[19] Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W.\nEllis, Matt McVicar, Eric Battenberg, and Oriol Nieto.\nlibrosa: Audio and music signal analysis in python. In\nProceedings of the Scientiﬁc Computing with Python\nconference (Scipy) , Austin, Texas, 2015.\n[20] MIREX. Audio melody extraction task. Website\nhttp://www.music-ir.org/mirex/wiki/\n2016:Audio_Melody_Extraction , last ac-\ncessed 03/29/2018, 2016.\n[21] Sinno Jialin Pan and Qiang Yang. A survey on transfer\nlearning. IEEE Transactions on Knowledge and Data\nEngineering , 22(10):1345–1359, 2010.\n[22] Martin Pﬂeiderer, Klaus Frieler, Jakob Jakob Abeßer,\nWolf-Georg Zaddach, and Benjamin Burkhart. In-\nside the Jazzomat. New perspectives for jazz research .\nSchott Campus, Mainz, Germany, 2017.\n[23] Jordi Pons, Oriol Nieto, Matthew Prockup, Erik M.\nSchmidt, Andreas F. Ehmann, and Xavier Serra. End-\nto-end learning for music audio tagging at scale. In\nProceedings of the Conference on Neural Information\nProcessing Systems (NIPS) , pages 1–5, Long Beach,\nCA, USA, 2015.\n[24] Matti Ryyn ¨anen and Anssi P. Klapuri. Automatic tran-\nscription of melody, bass line, and chords in poly-\nphonic music. Computer Music Journal , 32(3):72–86,\n2008.\n[25] Justin Salamon and Emilia G ´omez. Melody extrac-\ntion from polyphonic music signals using pitch contour\ncharacteristics. IEEE Transactions on Audio, Speech,\nand Language Processing , 20(6):1759–1770, 2012.\n[26] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. Dropout: A\nsimple way to prevent neural networks from overﬁt-\nting. Journal of Machine Learning Research , 15:1929–\n1958, 2014.\n[27] Stefan Uhlich, Marcello Porcu, Franck Giron, Michael\nEnenkl, Thomas Kemp, Naoya Takahashi, and Yuki\nMitsufuji. Improving music source separation based on\ndeep neural networks through data augmentation and\nnetwork blending. In Proceedings of the IEEE Inter-\nnational Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP) , pages 261–265, New Orleans,\nUSA, 2017.312 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "A Data-driven Approach to Mid-level Perceptual Musical Feature Modeling.",
        "author": [
            "Anna Aljanaki",
            "Mohammad Soleymani 0001"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492491",
        "url": "https://doi.org/10.5281/zenodo.1492491",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/183_Paper.pdf",
        "abstract": "Musical features and descriptors could be coarsely divided into three levels of complexity. The bottom level contains the basic building blocks of music, e.g., chords, beats and timbre. The middle level contains concepts that emerge from combining the basic blocks: tonal and rhythmic stability, harmonic and rhythmic complexity, etc. High-level descriptors (genre, mood, expressive style) are usually modeled using the lower level ones. The features belonging to the middle level can both improve automatic recognition of high-level descriptors, and provide new music retrieval possibilities. Mid-level features are subjective and usually lack clear definitions. However, they are very important for human perception of music, and on some of them people can reach high agreement, even though defining them and therefore, designing a hand-crafted feature extractor for them can be difficult. In this paper, we derive the mid-level descriptors from data. We collect and release a dataset 1 of 5000 songs annotated by musicians with seven mid-level descriptors, namely, melodiousness, tonal and rhythmic stability, modality, rhythmic complexity, dissonance and articulation. We then compare several approaches to predicting these descriptors from spectrograms using deep-learning. We also demonstrate the usefulness of these mid-level features using music emotion recognition as an application.",
        "zenodo_id": 1492491,
        "dblp_key": "conf/ismir/AljanakiS18",
        "keywords": [
            "Musical features",
            "complexity",
            "three levels",
            "basic building blocks",
            "tonal and rhythmic stability",
            "harmonic and rhythmic complexity",
            "high-level descriptors",
            "genre",
            "mood",
            "expressive style"
        ],
        "content": "A DATA-DRIVEN APPROACH TO MID-LEVEL PERCEPTUAL MUSICAL\nFEATURE MODELING\nAnna Aljanaki\nInstitute of Computational Perception,\nJohannes Kepler University\naljanaki@gmail.comMohammad Soleymani\nSwiss Center for Affective Sciences,\nUniversity of Geneva\nmohammad.soleymani@unige.ch\nABSTRACT\nMusical features and descriptors could be coarsely di-\nvided into three levels of complexity. The bottom level\ncontains the basic building blocks of music, e.g., chords,\nbeats and timbre. The middle level contains concepts\nthat emerge from combining the basic blocks: tonal and\nrhythmic stability, harmonic and rhythmic complexity, etc.\nHigh-level descriptors (genre, mood, expressive style) are\nusually modeled using the lower level ones. The features\nbelonging to the middle level can both improve automatic\nrecognition of high-level descriptors, and provide new mu-\nsic retrieval possibilities. Mid-level features are subjective\nand usually lack clear deﬁnitions. However, they are very\nimportant for human perception of music, and on some of\nthem people can reach high agreement, even though deﬁn-\ning them and therefore, designing a hand-crafted feature\nextractor for them can be difﬁcult. In this paper, we de-\nrive the mid-level descriptors from data. We collect and\nrelease a dataset1of 5000 songs annotated by musicians\nwith seven mid-level descriptors, namely, melodiousness,\ntonal and rhythmic stability, modality, rhythmic complex-\nity, dissonance and articulation. We then compare several\napproaches to predicting these descriptors from spectro-\ngrams using deep-learning. We also demonstrate the use-\nfulness of these mid-level features using music emotion\nrecognition as an application.\n1. INTRODUCTION\nIn music information retrieval, features extracted from au-\ndio or a symbolic representation are often categorized as\nlow or high-level [5], [17]. There is no clear boundary\nbetween these concepts and the terms are not used consis-\ntently. Usually, features that were extracted using a small\nanalysis window that does not contain temporal informa-\ntion are called low-level (e.g., spectral features, MFCCs,\nloudness). Features that are deﬁned within a longer con-\n1https://osf.io/5aupt/\nc\rAnna Aljanaki, , Mohammad Soleymani. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Anna Aljanaki, , Mohammad Soleymani. “A data-\ndriven approach to mid-level perceptual musical feature modeling”, 19th\nInternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.text (and often related to music theoretical concepts) are\ncalled high-level (key, tempo, melody). In this paper, we\nwill look at these levels from the point of view of human\nperception, and deﬁne what constitutes low, middle and\nhigh levels depending on complexity and subjectivity of\na concept. Unambiguously deﬁned and objectively veri-\nﬁable concepts (beats, onsets, instrument timbres) will be\ncalled low-level. Subjective, complex concepts that can\nonly be deﬁned by considering every aspect of music will\nbe called high-level (mood, genre, similarity). Everything\nin between we will call mid-level.\nMusical concepts can best be viewed and deﬁned\nthrough the lens of human perception. It is often not\nenough to approximate them through a simpler concept or\nfeature. For instance, music speed (whether music is per-\nceived as fast or slow) is not explained by or equivalent to\ntempo (beats per minute). In fact, perceptual speed is bet-\nter approximated (but not completely explained) by onset\nrate [8]. There are many examples of mid-level concepts:\nharmonic complexity, rhythmic stability, melodiousness,\ntonal stability, structural regularity [10], [24]. Such meta\nlanguage could be used to improve search and retrieval, to\nadd interpretability to the models of high-level concepts,\nand may be even break the glass ceiling in the accuracy of\ntheir recognition.\nIn this paper we collect a dataset and model these con-\ncepts directly from data using transfer learning.\n2. RELATED WORK\nMany algorithms have been developed to model features\ndescribing such aspects of music as articulation, melodi-\nousness, rhythmic and dynamic patterns. MIRToolbox and\nEssentia frameworks offer many algorithms that can ex-\ntract features related to harmony, rhythm, articulation and\ntimbre [13], [3]. These features are usually extracted using\nsome hand-crafted algorithm and have a differing amount\nof psychoacoustic and perceptual basis.\nFor example, Salamon et al. developed a set of melodic\nfeatures which extract pitch contours from a melody ob-\ntained with a melody extraction algorithm [22]. There\nwere proposed measures like percussiveness [17], pulse\nclarity [12], danceability [23]. Panda et al. proposed a\nset of algorithms to extract descriptors related to melody,\nrhythm and texture from MIDI and audio [19]. It is out\nof our scope to review all existing algorithms for detecting615Perceptual Feature Criteria when comparing two excerpts Cronbach’s\u000b\nMelodiousness To which excerpt do you feel like singing along? 0.72\nArticulation Which has more sounds with staccato articulation? 0.8\nRhythmic stabilityImagine marching along with music.\nWhich is easier to march along with?0.69\nRhythmic complexityIs it difﬁcult to repeat by tapping?\nIs it difﬁcult to ﬁnd the meter?\nDoes the rhythm have many layers?0.27 (0.47)\nDissonanceWhich excerpt has noisier timbre?\nHas more dissonant intervals (tritones, seconds, etc.)?0.74\nTonal stabilityWhere is it easier to determine the tonic and key?\nIn which excerpt are there more modulations?0.44\nModalityImagine accompanying this song with chords.\nWhich song would have more minor chords?0.69\nTable 1 . Perceptual mid-level features and the questions that were provided to raters to help them compare two excerpts.\nwhat we call mid-level perceptual music concepts.\nAll the algorithms listed so far were designed with some\nhypothesis about music perception in mind. For instance,\nEssentia offers an algorithm to compute sensory disso-\nnance, which sums up the dissonance values for each pair\nof spectral peaks, based on dissonance curves obtained\nfrom perceptual measurements [20]. Such an algorithm\nmeasures a speciﬁc aspect of music in a transparent way,\nbut it is hard to say, whether it captures all the aspect of a\nperceptual feature.\nFriberg et al. collected perceptual ratings for nine fea-\ntures (rhythmic complexity and clarity, dynamics, har-\nmonic complexity, pitch, etc.) for a set of 100 songs and\nmodeled them using available automatic feature extractors,\nwhich showed that algorithms can cope with some con-\ncepts and fail with some others [8]. For instance, for such\nan important feature like modality (majorness) there is no\nadequate solution yet. It was also shown that with just sev-\neral perceptual features it is possible to model emotion in\nmusic with a higher accuracy than it is possible using fea-\ntures, extracted with MIR software [1], [8], [9].\nIn this paper we propose an approach to mid-level fea-\nture modeling that is more similar to automatic tagging [6].\nWe try to approximate the perceptual concepts by model-\ning them straight from the ratings of listeners.\n3. DATA COLLECTION\nFrom the literature ( [10], [24], [8]) we composed a list of\nperceptual musical concepts and picked 7 recurring items.\nTable 1 shows the selected terms. The concepts that we are\ninterested in stem from musicological vocabulary. Identi-\nfying and naming them is a complicated task that requires\nmusical training. This doesn’t mean that these concepts\nare meaningless and are not perceived by an average mu-\nsic listener, but we can not trust an average listener to apply\nthe terms in a consistent way. We used Toloka2crowd-\n2toloka.yandex.rusourcing platform to ﬁnd people with musical training to\ndo the annotation. We invited anyone who has music edu-\ncation to take a musical test, which contained questions on\nharmony (tonality, identifying mode of chords), expressive\nterms (rubato, dynamics, articulation), pitch and timbre.\nAlso, we asked the crowd-sourcing workers to shortly de-\nscribe their music education. From 2236 people who took\nthe test slightly less than 7% (155 crowd sourcing workers)\npassed it and were invited to participate in the annotation.\n3.0.1 Deﬁnitions\nThe terminology (articulation, mode, etc.) that we use is\ncoming from musicology, but it was not designed to be\nused in a way that we use it. For instance, the concept\nof articulation is deﬁned for a single note (or can also be\nextended to a group of notes). Applying it to a real-life\nrecording with possibly several instruments and voices is\nnot an easy task. To ensure common understanding, we of-\nfer the annotators a set of deﬁnitions as shown in Table 1.\nThe general principle is to consider the recording as a\nwhole.\n3.1 Pairwise comparisons\nIt is easier for annotators to compare two items using a\ncertain criterion, then to give a rating on an absolute scale,\nand especially so for subjective and vaguely deﬁned con-\ncepts [14]. Then, a ranking can be formed from pairwise\ncomparisons. However, annotating a sufﬁcient amount of\nsongs using pairwise comparisons is too labor intensive.\nCollecting a full pairwise comparison matrix (not counting\nrepetitions and self-similarity) requires (n2\u0000n)=2com-\nparisons. For our desired target of 5000 songs, that would\nmean\u001912:5million comparisons. It is possible to con-\nstruct a ranking with less than a full pairwise comparison\nmatrix, but still for a big dataset it is not a feasible ap-\nproach. We combine the two approaches. In order to do\nthat, we ﬁrst collected pairwise comparisons for a small616 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 1 . Distribution of discrete ratings per perceptual feature.\nFeature Articulation R. comlexity R. Stability Dissonance Tonal stability Mode\nMelodiousness \u00000:13 \u00000:22 0 :27 \u00000:59 0:58\u00000:22\nArticulation 0:39 0:60 0:45 \u00000:05\u00000:14\nR. complexity \u00000:009 0 :48 \u00000:30 0 :06\nR. stability 0:06 0 :36\u00000:17\nDissonance \u00000:55 0:23\nTonal stability \u00000:16\nTable 2 . Correlations between the perceptual mid-level features.\namount of songs, obtained a ranking, and then created an\nabsolute scale that we used to collect the rankings.\nIn this way, we also implicitly deﬁne our concepts\nthrough examples without a need to explicitly describe all\ntheir aspects.\n3.1.1 Music selection\nFor pairwise comparisons, we selected 100 songs. This\nmusic needed to be diverse, because it was going to be\nused as examples and needed to be able to represent the\nextremes. We used 2 criteria to achieve that - genre and\nemotion. From each of the 5 music preference clusters of\nRentfrow et al. [21] we selected a list of genres belong-\ning to these clusters and picked songs from the DEAM\ndataset [2] belonging to these genres (pop, rock, hip-\nhop, rap, jazz, classical, electronic), taking 20 songs from\neach of the preference clusters. Also, using the anno-\ntations from DEAM, we assured that the selected songs\nare uniformly distributed over the four quadrants of va-\nlence/arousal plane. From each of the songs we cut a seg-\nment of 15 seconds.\nFor a set of 100 songs we collected 2950 comparisons.\nNext, we created a ranking by counting the percentage of\ncomparisons won by a song relative to an overall number\nof comparisons per song. By sampling from that ranking\nwe created seven scales with song examples from 1 to 9\nfor each of the mid-level perceptual features (for instance,\nfrom the least melodious (1) to the most melodious (9)).\nSome of the musical examples appeared in several scales.\n3.2 Ratings on 7 perceptual mid-level features\nThe ratings were again collected on Toloka platform, and\nthe workers were selected using the same musical test. The\nrating procedure was as follows. First, a worker listened to\na 15-second excerpt. Next, for a certain scale (for instance,\narticulation), a worker compared an excerpt with examples\narranged from ”legato” to ”staccato” and found a proper\nrating. Finally, this was repeated for each of the 7 percep-\ntual features.3.2.1 Music selection\nMost of the dataset music consists of Creative Commons\nlicensed music from jamendo.com andmagnatune.\ncom. For annotation, we cut 15 seconds from the middle\nof the song. In the dataset, we provide the segments and the\nlinks to a full song. There is a restriction of no more than\n5 songs from the same artist. The songs from jamendo.\ncom were also ﬁltered by popularity, in a hope to get music\nof a better recording quality. We also reused the music\nfrom datasets annotated with emotion [7], [18], [15] which\nwe are going to use to indirectly test the validity of the\nannotations.\n3.2.2 Data\nFigure 1 shows the distributions of the ratings for every\nfeature. The music in the dataset leans slightly towards be-\ning rhythmically stable, tonally stable and consonant. The\nscales could be also readjusted to have more examples in\nthe regions of the most density. That might not necessar-\nily help, because the observed distributions could also be\nthe artifacts of people prefering to avoid the extremes. Ta-\nble 2 shows the correlation between different perceptual\nfeatures. There is a strong negative correlation between\nmelodiousness and dissonance, a positive relationship be-\ntween articulation and rhythmic stability. Tonal stability is\nnegatively correlated with dissonance and positively with\nmelodiousness.\n3.3 Consistency\nAny crowd-sourcing worker could stop annotating at any\npoint, so the amount of annotated songs per person var-\nied. An average amount of songs per worker was 187:01\u0006\n500:68. On average, it took \u00192minutes to answer all the\nseven questions for one song. Our goal was to collect 5\nannotations per song, which amounts to \u0019833man-hours.\nIn order to ensure quality, a set of songs with high qual-\nity annotations (high agreement by well-performing work-\ners) was interlaced with new songs, and the annotations of\nevery crowd-sourcing worker were compared against that\ngolden standard. The workers that gave answers very farProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 617Emotional dimension\nor categoryPearson’s\u001a\n(prediction)Important features\nValence 0.88 Mode (major), melodiousness (pos.), dissonance (neg.)\nEnergy 0.79 Articulation (staccato), dissonance (pos.)\nTension 0.84 Dissonance (pos.), melodiousness (neg.)\nAnger 0.65 Dissonance (pos.), mode (minor), articulation (staccato)\nFear 0.82 Rhythm stability (neg.), melodiousness (neg.)\nHappy 0.81 Mode (major), tonal stability (pos.)\nSad 0.73 Mode (minor), melodiousness (pos.)\nTender 0.72 Articulation (legato), mode (minor), dissonance (neg.)\nTable 3 . Modeling emotional categories in Soundtracks dataset using seven mid-level features.\nfrom the standard were banned. Also, the answers were\ncompared to the average answer per song, and workers\nwhose standard deviation was close to one one resulting\nfrom random guessing were also banned and their answers\ndiscarded. The ﬁnal annotations contain answers of 115\nworkers out of a pool of 155, who passed the musical test.\nTable 1 shows a measure of agreement (Cronbach’s \u000b)\nfor each of the mid-level features. The annotators reach\ngood agreement for most of the features, except rhyth-\nmic complexity and tonal stability. We created a differ-\nent musical test, containing only questions about rhythm,\nand collected more annotations. Also, we provided more\nexamples on the rhythm complexity scale. It helped a lit-\ntle (Cronbach’s \u000bimproved from 0.27 to 0.47), but still\nrhythmic complexity has much worse agreement than other\nproperties. In a study of Friberg and Hedblad [8], where\nsimilar perceptual features were annotated for a small set\nof songs, the situation was similar. The least consistent\nproperties were harmonic complexity and rhythmic com-\nplexity.\nWe average the ratings for every mid-level feature per\nsong. The annotations and the corresponding excerpts\n(or links to external reused datasets) are available online\n(osf.io/5aupt). All the experiments below are performed\non averaged ratings.\n3.4 Emotion dimensions and categories\nSoundtracks dataset contains 15 second excerpts from ﬁlm\nmusic, annotated with valence, arousal, tension, and 5 ba-\nsic emotions [7].\nWe show that our annotations are meaningful by using\nthem to model musical emotion in Soundtracks dataset.\nThe averaged ratings per song for each of the seven mid-\nlevel concepts are used as features in a linear regression\nmodel (10-fold cross-validation).\nTable 3 shows the correlation coefﬁcient and the most\nimportant features for each dimension, which are consis-\ntent with the ﬁndings in the literature [10]. We can model\nmost dimensions well, despite not having any information\nabout loudness and tempo.Cluster AUC F-measure\nCluster 1\npassionate, conﬁdent0.62 0.38\nCluster 2\ncheerful, fun0.7 0.5\nCluster 3\nbittersweet0.8 0.67\nCluster 4\nhumorous0.65 0.45\nCluster 5\naggressive0.78 0.64\nTable 4 . Modeling MIREX clusters with perceptual fea-\ntures.\n3.5 MIREX clusters\nMultimodal dataset contains 903 songs annotated with 5\nclusters used in MIREX Mood recognition competition\n3[18]. Table 4 shows results of predicting the ﬁve clusters\nusing the seven mid-level features and an SVM classiﬁer.\nThe average weighted F1 measure on all the clusters on\nthis dataset is 0.54. In [18], with an SVM classiﬁer trained\non 253 audio features, extracted with various toolboxes, F1\nmeasure was 44.9, and 52.3 with 98 melodic features. By\ncombining these feature sets and doing feature selection\nby using feature ranking, the F1 measure was increased\nto 64.0. Panda et al. hypothesize that Multimodal dataset\nis more difﬁcult than MIREX dataset (their method per-\nformed better (0.67) in MIREX competition than on their\nown dataset). In MIREX data, the songs went through an\nadditional annotation step to ensure agreement on cluster\nassignment, and only songs that 2 out of 3 experts agreed\non were kept.\n3www.music-ir.org/mirex618 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 2 . AUC per tag on the test set.\n4. EXPERIMENTS\nWe left out 8% of the data as a test set. We split the train\nset and test set by performer (no performer from the test\nset appears in the training set). Also, all the performers in\nthe test set are unique. For pretraining, we used songs from\njamendo.com , making sure that the songs used for pre-\ntraining do not reappear in the test set. The rest of the data\nwas used for training and validation (whenever we needed\nto validate any hyperparameters, we used 2% of the train\nset for that).\nFrom each of the 15-second excerpts we computed\na mel-spectrogram with 299 mel-ﬁlters and a frequency\nrange of 18000Hz, extracted with 2048 sample window\n(44100 sampling rate) and a hop of 1536. In order to use it\nas an input to a neural network, it was cut to a rectangular\nshape (299 by 299) which corresponds to about 11 seconds\nof music. Because the original mel-spectrogram is a bit\nlarger, we can randomly shift the rectangular window and\nselect a different set. For some of the songs, full-length\nsongs are also available, and it was possible to extract the\nmel-spectrogram from any place in a song, but in practice\nthis worked worse than selecting a precise spot.\nWe also tried other data representations: spectrograms\nand custom data representations (time-varying chroma for\ntonal features and time-varying bark-bands for rhythmic\nfeatures). Custom representations were trained with a two-\nlayer recurrent network. These representations worked\nworse than mel-spectrograms with a deep network.\n4.1 Training a deep network\nWe chose Inception v3 architecture [4]. First ﬁve layers are\nconvolutional layers with 3 by 3 ﬁlters. Twice max-pooling\nis applied. The last layers of the network are the so-called\n”inception layers”, which apply ﬁlters of different size in\nparallel and merge the feature maps later. We begin by\ntraining this network without any pretraining.\n4.1.1 Transfer learning\nWith a dataset of only 5000 excerpts, it is hard to prevent\noverﬁtting when learning features from the very basic mu-\nsic representation (mel-spectrogram), as it was done in [6]\non a much larger dataset. In this case, transfer learning can\nhelp.4.1.2 Data for pretraining\nWe crawl data and tags from Jamendo, using the API pro-\nvided by this music platform. We select all the tags, which\nwere applied to at least 3000 songs. That leaves us with\n65 tags and 184002 songs. For training, we extract a mel-\nspectrogram from a random place in a song. We leave 5%\nof the data as a test set. After training on mini-batches\nof 32 examples with Adam optimizer for 29 epochs, we\nachieve an average area under receiver-operator curve of\n0.8 on the test set. The AUC on the test set grouped by\ntag are shown on Figure 2 (only 15 best and 15 worst per-\nforming tags). Some of the songs in the mid-level feature\ndataset also were chosen from Jamendo.\n4.1.3 Transfer learning on mid-level features\nThe last layer of Inception, before the 65 neurons that pre-\ndict classes (tags), contains 2048 neurons. We pass through\nthe mel-spectrograms of the mid-level feature dataset and\nextract the activations of this layer. We normalize these ex-\ntracted features using mean and standard deviation of the\ntraining set. On the training set, we ﬁt a PCA with 30\nprinciple components (the number was chosen based on\ndecline of eigenvalues of the components) and then apply\nthe learned transformation on a validation and test set. On\na validation set, we tune parameters of a SVR with a ra-\ndial basis function kernel and ﬁnally, we predict the seven\nmid-level features on the test set.\n4.2 Fine-tuning trained model for mid-level features\nOn top of the last Inception layer we add two fully-\nconnected layers with 150 and 30 neurons, both with ReLU\nactivation, and an output layer with 7 nodes with no activa-\ntion (we train on all the features at the same time). First, we\nfreeze the pre-trained weights of the Inception and train the\nlast layer weights until there’s no improvement on the val-\nidation set anymore. At this point, the network reaches the\nsame performance on the test set as it reached using trans-\nfer learning and PCA (which is what we would expect).\nNow, we unfreeze the weights and with a small learning\nrate continue training the whole network until it stops im-\nproving on validation set.\n4.3 Existing algorithms\nThere are many feature extraction frameworks for MIR.\nSome of those (jAudio, Aubio, Marsyas) only offer tim-\nbral and spectral features, others (Essentia, MIRToolbox,Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 619Figure 3 . Performance of different methods on mid-level feature prediction.\nV AMP Plugins for Sonic Annotator) offer features, which\nare similar to the mid-level features of this paper. Figure\n3 shows the correlation of some of these features with our\nperceptual ratings:\n1.Articulation . MIRToolbox offers features describing\ncharacteristics of onsets (attack time, attack slope,\nleap (duration of attack), decay time, slope and\nleap. Out of this features leap was chosen (it had\nthe strongest correlation with perceptual articulation\nfeature).\n2.Rhythmic stability . Pulse clarity (MIRToolbox) [16].\n3.Dissonance . Both Essentia and MIRToolbox offer a\nfeature describing sensory dissonance (in MIRTool-\nbox, it is called roughness), which is based on the\nsame research of dissonance perception [20]. We ex-\ntract this feature and inharmonicity. Inharmonicity\nonly had a weak (0.22) correlation with perceptual\ndissonance. Figure 3 shows a result for the disso-\nnance measure.\n4.Tonal stability . HCDF (harmonic change detection\nfunction) in MIRToolbox is a feature measuring the\nﬂux of a tonal centroid [11]. This feature was not\ncorrelated with our tonal stability feature.\n5.Modality . MIRToolbox offers a feature called mode,\nwhich is based on an uncertainty in determining the\nkey using pitch-class proﬁles.\nWe could not ﬁnd features corresponding to melodious-\nness and rhythmic complexity. Perceptual concepts lack\nclear deﬁnitions, so it is impossible to say that the feature\nextractor algorithms are supposed to directly measure the\nsame concepts that we had annotated. However, from Fig-\nure 3 we can see that the chosen descriptors do indeed cap-\nture some part of variance in the perceptual features.\n4.4 Results\nFigure 3 shows the results for every mid-feature. For all\nthe mid-features, the best result was achieved by pretrain-\ning and ﬁne-tuning the network. Melodiousness, articula-\ntion and dissonance could be predicted with a much bet-ter accuracy than rhythmic complexity, tonal and rhythmic\nstability, and mode.\n5. FUTURE WORK\nIn this paper, we only investigated seven perceptual fea-\ntures. Other interesting features include tempo, timbre,\nstructural regularity. Rhythmic complexity and tonal sta-\nbility features had low agreement. It is probable that con-\ntributing factors need to be explicitly speciﬁed and studied\nseparately. The accuracy could be improved for modality\nand rhythmic stability. It is not clear whether strong cor-\nrelations between some features are an artifact of the data\nselection or music perception.\n6. CONCLUSION\nMid-level perceptual music features could be used for mu-\nsic search and categorization and improve music emotion\nrecognition methods. However, there are multiple chal-\nlenges in extracting such features: ﬁrst, such concepts lack\nclear deﬁnitions, and we do not quite understand the under-\nlying perceptual mechanisms yet. In this paper, we collect\nannotations for seven perceptual features and model them\nby relying on listener ratings. We provide the listeners\nwith scales with examples instead of deﬁnitions and crite-\nria. Listeners achieved good agreement on all the features\nbut two (rhythmic complexity and tonal stability). Using\ndeep learning, we model the features from data. Such\nan approach has its advantages as compared to speciﬁc\nalgorithm-design by being able to pick appropriate pat-\nterns from the data and achieve better performance than\nan algorithm based on a single aspect. However, it is also\nless interpretable. We release the mid-level feature dataset,\nwhich can be used to further improve both algorithmic and\ndata-driven methods of mid-level feature recognition.\n7. ACKNOWLEDGEMENTS\nThis work is supported by the European Research Council\n(ERC) under the EUs Horizon 2020 Framework Program\n(ERC Grant Agreement number 670035, project ”Con\nEspressione”). This work was also supported by an FCS\ngrant.620 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20188. REFERENCES\n[1] A. Aljanaki, F. Wiering, and R.C. Veltkamp. Compu-\ntational modeling of induced emotion using gems. In\n15th International Society for Music Information Re-\ntrieval Conference , 2014.\n[2] A. Aljanaki, Y .-H. Yang, and M. Soleymani. Devel-\noping a benchmark for emotional analysis of music.\nPLOS ONE , 12(3), 2017.\n[3] D. Bogdanov, N. Wack, E. Gomez, S. Gulati, P. Her-\nrera, and et al. O. Mayor. Essentia: an audio analysis\nlibrary for music information retrieval. In 14th Interna-\ntional Society for Music Information Retrieval Confer-\nence, pages 493–498, 2013.\n[4] S. Ioffe J. Shlens Z. Wojna C. Szegedy, V . Vanhoucke.\nRethinking the inception architecture for computer vi-\nsion. In IEEE Conference on Computer Vision and Pat-\ntern Recognition , 2016.\n[5] M.A. Casey, R. Veltkamp, M. Goto, M. Leman,\nC. Rhodes, and M. Slaney. Content-Based Music Infor-\nmation Retrieval: Current Directions and Future Chal-\nlenges. Proceedings of the IEEE , 96(4):668–696, 2008.\n[6] K. Choi, G. Fazekas, and M. Sandler. Convnet: Au-\ntomatic tagging using deep convolutional neural net-\nworks. In 17th International Society for Music Infor-\nmation Retrieval Conference , 2016.\n[7] T. Eerola and J.K. Vuoskoski. A comparison of the\ndiscrete and dimensional models of emotion in music.\nPsychology of Music , 39(1):1849, 2011.\n[8] A. Friberg and A. Hedblad. A comparison of percep-\ntual ratings and computed audio features. In 8th Sound\nand Music Computing Conference , pages 122–127,\n2011.\n[9] A. Friberg, E. Schoonderwaldt, A. Hedblad, M. Fabi-\nani, and A. Elowsson. Using listener-based perceptual\nfeatures as intermediate representations in music infor-\nmation retrieval. The Journal of the Acoustical Society\nof America , 136(4):1951–63, 2014.\n[10] A. Gabrielsson and E. Lindstrm. Music and Emotion:\nTheory and Research , chapter The Inﬂuence of Mu-\nsical Structure on Emotional Expression, page 22348.\nOxford University Press, 2001.\n[11] Christopher Harte, Mark Sandler, and Martin Gasser.\nDetecting harmonic change in musical audio. In Pro-\nceedings of the 1st ACM workshop on Audio and music\ncomputing multimedia - AMCMM ’06 , page 21. ACM\nPress, 2006.\n[12] O. Lartillot, T. Eerola, P. Toiviainen, and J. Fornari.\nMulti-feature modeling of pulse clarity: Design, vali-\ndation, and optimization. In 9th International Confer-\nence on Music Information Retrieval , 2008.[13] O. Lartillot, P. Toiviainen, and T. Eerola. A matlab\ntoolbox for music information retrieval. Data Analysis,\nMachine Learning and Applications, Studies in Clas-\nsiﬁcation, Data Analysis, and Knowledge Organizatio ,\n2008.\n[14] J. Madsen, Jensen B. S, and J. Larsen. Predictive Mod-\neling of Expressed Emotions in Music Using Pairwise\nComparisons. pages 253–277. Springer, Berlin, Hei-\ndelberg, 2013.\n[15] R. Malheiro, R. Panda, P. Gomes, and R. Paiva. Bi-\nmodal music emotion recognition: Novel lyrical fea-\ntures and dataset. In 9th International Workshop on\nMusic and Machine Learning MML2016 , 2016.\n[16] Petri Toiviainen Jose Fornari Olivier Lartillot, Tuo-\nmas Eerola. Multi-feature modeling of pulse clarity:\nDesign, validation, and optimization. In 9th Inter-\nnational Conference on Music Information Retrieval ,\n2008.\n[17] E. Pampalk. Computational Models of Music Similar-\nity and their Application in Music Information Re-\ntrieval . PhD thesis, Vienna University of Technology,\n2012.\n[18] R. Panda, R. Malheiro, B. Rocha, A. Oliveira, and\nR. P. Paiva. Multi-modal music emotion recognition:\nA new dataset, methodology and comparative analysis.\nIn10th International Symposium on Computer Music\nMultidisciplinary Research , 2013.\n[19] Renato Panda, Ricardo Manuel Malheiro, and Rui Pe-\ndro Paiva. Novel audio features for music emotion\nrecognition. IEEE Transactions on Affective Comput-\ning.\n[20] R. Plomp and W. J. M. Levelt. Tonal Consonance and\nCritical Bandwidth. The Journal of the Acoustical So-\nciety of America , 38(4):548560, 1965.\n[21] P. J. Rentfrow, L. R. Goldberg, and D. J. Levitin.\nThe structure of musical preferences: a ﬁve-factor\nmodel. Journal of personality and social psychology ,\n100(6):1139–57, 2011.\n[22] J. Salamon, B. Rocha, and E. Gomez. Musical genre\nclassiﬁcation using melody features extracted from\npolyphonic music signals. In 2012 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , pages 81–84. IEEE, 2012.\n[23] S. Streich and P. Herrera. Detrended ﬂuctuation analy-\nsis of music signals danceability estimation and further\nsemantic characterization. In AES 118th Convention ,\n2005.\n[24] L. Wedin. A Multidimensional Study of Perceptual-\nEmotional Qualities in Music. Scandinavian Journal\nof Psychology , 13:241257, 1972.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 621"
    },
    {
        "title": "Music Generation and Transformation with Moment Matching-Scattering Inverse Networks.",
        "author": [
            "Mathieu Andreux",
            "Stéphane Mallat"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492415",
        "url": "https://doi.org/10.5281/zenodo.1492415",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/131_Paper.pdf",
        "abstract": "We introduce a Moment Matching-Scattering Inverse Network (MM-SIN) to generate and transform musical sounds. The MM-SIN generator is similar to a variational autoencoder or an adversarial network. However, the encoder or the discriminator are not learned, but computed with a scattering transform defined from prior information on sparse time-frequency audio properties. The generator is trained by jointly minimizing the reconstruction loss of an inverse problem, and a generation loss which computes a distance over scattering moments. It has a similar causal architecture as a WaveNet and provides a simpler mathematical model related to time-frequency decompositions. Numerical experiments demonstrate that this MMSIN generates new realistic musical signals. It can transform low-level musical attributes such as pitch with a linear transformation in the embedding space of scattering coefficients.",
        "zenodo_id": 1492415,
        "dblp_key": "conf/ismir/AndreuxM18",
        "keywords": [
            "Moment Matching-Scattering Inverse Network (MM-SIN)",
            "generates and transforms musical sounds",
            "variational autoencoder or adversarial network",
            "encoder or discriminator not learned",
            "computed with a scattering transform",
            "sparse time-frequency audio properties",
            "generator trained by inverse problem",
            "generation loss",
            "causal architecture",
            "WaveNet"
        ],
        "content": "MUSIC GENERATION AND TRANSFORMATION WITH MOMENT\nMATCHING-SCATTERING INVERSE NETWORKS\nMathieu Andreux and St ´ephane Mallat\nD´epartement d’informatique de l’ENS, ´Ecole normale sup ´erieure,\nCNRS, PSL Research University, 75005 Paris, France\nABSTRACT\nWe introduce a Moment Matching-Scattering Inverse\nNetwork (MM-SIN) to generate and transform musical\nsounds. The MM-SIN generator is similar to a variational\nautoencoder or an adversarial network. However, the en-\ncoder or the discriminator are not learned, but computed\nwith a scattering transform deﬁned from prior information\non sparse time-frequency audio properties. The genera-\ntor is trained by jointly minimizing the reconstruction loss\nof an inverse problem, and a generation loss which com-\nputes a distance over scattering moments. It has a similar\ncausal architecture as a WaveNet and provides a simpler\nmathematical model related to time-frequency decomposi-\ntions. Numerical experiments demonstrate that this MM-\nSIN generates new realistic musical signals. It can trans-\nform low-level musical attributes such as pitch with a lin-\near transformation in the embedding space of scattering co-\nefﬁcients.\n1. INTRODUCTION\nThis paper investigates musical sound generation and\ntransformation with a simpliﬁed algorithmic architecture,\nwhich relates generative networks to time-frequency rep-\nresentations. Image generation has led the way through\nthe development of Generative Adversarial Networks\n(GANs) [7] and Variational Autoencoders [13] where im-\nages are generated from a Gaussian white noise vector,\nwhich deﬁnes a latent space. Arithmetic operations in this\nlatent space lead to controlled transformations over the im-\nages such as aging of faces or transforming women in men.\nThe problem is however different for audio signals which\nmust take into account time causality properties. Some au-\nthors have applied image generation algorithms over spec-\ntrograms [3, 10] but it then requires to invert the spectro-\ngrams with a vocoder or a Grifﬁn-Lim algorithm which is\nlong and has a reduced quality.\nDeep autoregressive neural networks such as\nWaveNet [15, 16] or SampleRNN [14] have achieved\nexceptional synthesis of music and speech signals. They\nc\rMathieu Andreux and St ´ephane Mallat. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Mathieu Andreux and St ´ephane Mallat. “Music Genera-\ntion and Transformation with Moment Matching-Scattering Inverse Net-\nworks”, 19th International Society for Music Information Retrieval Con-\nference, Paris, France, 2018.do not take as input a Gaussian white noise but estimate a\nprobability distribution with a Markov chain factorization,\nwhich computes conditional probabilities given from\npast values. The generated signals have an outstanding\nquality but these neural architectures are complex and\nlack interpretability. Several effective techniques have\nbeen introduced to modify audio generations [4–6, 9]\nby modifying the latent code to change the probability\ndistribution or by using reference signals as targets, which\nis more complex than arithmetic operations used for\nimages.\nThis paper introduces a simpliﬁed neural network ar-\nchitecture which synthesizes and modiﬁes music signals\nfrom Gaussian white noise, with two key contributions.\nAs opposed to image GANs or variational autoencoders,\nwe do not learn a discriminator or an encoder: both are\nprovided by prior information on audio signals, which is\ncaptured by their time-frequency regularity. This is done\nby adapting a result obtained in [2] for images, and intro-\nducing a moment matching technique. The second con-\ntribution is the introduction of a causal computational ar-\nchitecture allowing to progressively synthesize audio sig-\nnals, and which can be parallelized in GPU’s. The resulting\nScattering Autoencoder architecture has some similarities\nwith a Parallel WaveNet [16]. Similarly to warping proper-\nties over images, we show that arithmetic transformations\nin the latent space produce time-frequency deformations,\nwhich can modify the pitch of musical notes or interpolate\nmusic. Despite a lower synthesis quality than state-of-the-\nart generating methods, these preliminary results pave the\nway for a new approach to synthesize audio signals, with-\nout learning encoders or discriminators.\n2. SCATTERING AUTOENCODER\nThis section introduces the principles of a scattering au-\ntoencoder and its computational architecture. The encoder\nis not learned but computed based on prior information on\naudio time-frequency properties. Audio and musical sig-\nnals have sparse representations over time-frequency dic-\ntionaries such as audio wavelets [20]. Their perceptual\nproperties are not much affected by small time-frequency\nwarpings. We deﬁne a signal embedding which takes ad-\nvantage of these characteristics.\nThe architecture of a scattering autoencoder is illus-\ntrated in Figure 1. The random input audio signal X[t]\nis ﬁrst transformed into a nearly Gaussian random vec-327X[t]ScatteringSJ WhiteningL\nV ARL\u00001 CNNZ[2Jn]Fixed Encoder \b\nTrained Generator GSignal\nWaveformLatent\ncode\nTemporal\nVector\nFigure 1 . The audio waveforms is encoded with a time-\nfrequency scattering SJfollowed by a linear whitening op-\neratorL. The Scattering Inverse Network (SIN) Grestores\na signal from a Gaussian white noise by inverting LSJon\ntraining data.\ntorSJ(X)[2Jn]by applying the time-frequency scattering\ntransformSJ, whereJ\u00150is a hyperparameter. Gaus-\nsianization is achieved through a time averaging over a\nsufﬁciently long time interval thanks to the central limit\ntheorem. In order to preserve enough information on the\noriginal signal Xafter averaging, multiple sparse time-\nfrequency channels are built by applying iterative wavelet\ntransforms [1]. The Gaussian scattering SJ(X)is then\nmapped into a Gaussian white noise with a whitening op-\neratorL, which outputs the linear prediction errors of a fu-\nture scattering vector from its neighboring past. Section 4\ndetails the whitened scattering transformation LSJ.\nThe generator Ginverts the linear whitening operator\nand the joint scattering transform by synthesizing an ap-\nproximation of X[t]fromZ[2Jn]. It ﬁrst applies a vec-\ntor autoregressive (V AR) ﬁlter L\u00001, which can be deduced\nfromL. This operation is followed by a causal convolu-\ntional neural network (CNN), with the convolutions acting\nalong time. The network has Jlayers to invert a scattering\nat scale 2J. Section 3 describes the architecture which has\nsimilarities with a Parallel WaveNet [16]. The optimiza-\ntion of the generator Gamounts to inverting the scattering\ntransformSJin an adapted metric. The statistics of syn-\nthesized signals are constrained by ensuring that they have\nthe same moments in the scattering space as the input sig-\nnalX[t]. This network is thus called a Moment Matching-\nScattering Inverse Network (MM-SIN).\n3. MOMENT MATCHING-SCATTERING\nINVERSE GENERATOR\nA Moment Matching-Scattering Inverse Network Gis\ncomputed by inverting a scattering embedding computed\nat a scale 2J. It is a causal network which takes as input\na nearly Gaussian white noise vector Zcomputed with a\nscattering transform, to recover an estimation of the signal\nX. This network is trained with a loss which incorporates\nthe inverse problem loss computed with a scattering met-\nric, regularized by a moment matching term that can be\ninterpreted as a discriminative metric.\nThe MM-SIN generator is a linear recurrent neural net-\nworkL\u00001followed by a causal convolutional network im-\nplemented with a cascade of Jconvolutions and pointwise\nnon-linearities, illustrated in Figure 2. Each intermediateW1WJ\u00001\nReLUWJ\nReLUL\u00001\nReLU\nLayerX0[t0]LayerXJ[tJ]Input Noise Z[tJ] Z[2Jn]\nX0[2Jn]Zero insertion\nt0tJ\u00002tJ\u00001tJtJ\n2J\nFigure 2 . An MM-SIN is a linear recurrent network fol-\nlowed by a causal deep convolutional network with Jlay-\ners. It takes as input a vector of Gaussian white noise\nZ[2Jn](top right, red), and computes the corresponding\nscattering vector XJ[2Jn]by applying L\u00001. Intermedi-\nate layersXj[tj]are then computed with causal convolu-\ntions denoted by blue arrows and zero insertions (white\npoints). The single vector Z[2Jn]outputs 2Jvalues for\nX0[t0], marked with red points.\nnetwork layer is composed of vectors Xj[tj]havingkj\nchannels and sampled at intervals 2j, for0\u0014j\u0014J. All\nlayers in Figure 2 appear to be aligned but to understand\nthe causality structure one must realize that each layer is\nindexed by a time index tjwhich is shifted by 2jrelatively\nto the absolute time variable tof the original input signal\nX[t]:tj=t\u00002j. Using the absolute time t, we thus use\nthe noise vector at a time t= 2J(n+1) to generate 2Jnew\noutput signal values at times 2Jn\u00002J+ 1<t\u00142Jn+ 1.\nThe ﬁrst layer maps Z[tJ]toXJ[tJ]with a vector au-\ntoregressive ﬁlter L\u00001which inverts the whitening opera-\ntorL, followed by a ReLU non-linearity \u001a(u) = max(u;0)\nXJ=\u001a\u0000\nL\u00001Z\u0001\n: (1)\nA new noise vector Z[2Jn]outputs a new vector\nXJ[2Jn]. At depthj, this initial vector gives rise to 2J\u0000j\ntemporal vectors Xj[tj]for2Jn\u00002J\u0000j<tj\u00142Jn.\nAt layerj >0, the layerXjis mapped to Xj\u00001with an\n`a trous convolution followed by a ReLU non-linearity plus\nbias. We ﬁrst double the size of Xjwith a zero insertion:\n~Xj[n2j] =Xj[n2j]and~Xj[n2j+ 2j\u00001] = 0:(2)\nEachXj\u00001is then calculated with a causal convolution\nalong time and a linear operator along channels with a bias,\nWj, followed by a ReLU:\nXj\u00001=\u001a(Wj~Xj): (3)\nExcept for the autoregressive layer, the ReLU is pre-\nceded by a batch normalization [11]. The last convolu-\ntion is not followed by a ReLU so that we can output a328 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018signal with negative values. The ﬁnal output is X0[t0]\nfor2Jn\u00002J<t0\u00142Jnwhich corresponds to X[t]for\n2Jn\u00002J+ 1<t\u00142Jn+ 1. As in standard generative\nnetworks [18], the number of the channels kjdecreases\nwithjaccording to a geometric law ﬁtted such that X0has\nonly one channel and XJhaskJchannels, which is the\ndimensionality of each vector Z[2Jn].\nThe parameters of the network Gare optimized by in-\nverting the scattering transform followed by the causal\nwhitening operator L. The lossLminimized by Gis a\nsum of two terms, weighted by a hyperparameter \u0015 > 0.\nThe ﬁrst termLinvmeasures the accuracy of the inversion.\nThe second discriminative term LMM measures the dis-\ntance between the scattering moments of the synthesized\nsignals and the scattering moments of the original signals.\nmin\nGL=Linv+\u0015LMM: (4)\nThe inverse problem loss Linvcomputes the reconstruc-\ntion error on each training example xifrom its embedding\nzicomputed with the scattering transform SJfollowed by\nthe linear whitening operator L. The reconstruction er-\nror is calculated over scattering coefﬁcients computed at\na scale 2K<2J:\nLinv=1\nNNX\ni=1kSK(xi)\u0000SKG(zi)k1withzi=LSJ(xi):\n(5)\nThel1norm promotes the sparsity of the responses, while\nthe scattering SKallows to generate signals which may be\nlocally deformed, but are perceptually similar to the orig-\ninal ones. In order to avoid useless computations, we do\nnot applyL\u00001zibut directly input the vectors SJ(xi)to\nthe convolutional part of Gfor the computation of Linv.\nThe lossLinvdoes not control the quality of the gen-\nerated samples G(z)whenzis sampled as a Gaussian\nwhite noise. Similarly to GANs which have a discrimi-\nnator, the quality is controlled by introducing another loss\ntermLMM, which controls the distance between the gener-\nated distribution and the distribution of the original signals.\nThe moment matching term LMM computes the dis-\ntance between scattering coefﬁcients of generated signals\naveraged over time tand batch index i,SKG(zi)[t], and\nscattering coefﬁcients of the training signals averaged over\ntimetand training examples i,SKxi[t]:\nLMM=\r\r\rSKxi[t]\u0000SKG(zi)[t]\r\r\r2\n(6)\nThe codesfzigcorrespond to a batch of random vectors\nwhich is renewed at each iteration of the gradient descent\nalgorithm.\nThe lossLMMis similar to the Maximum Mean Dis-\ncrepancy regularization introduced in [19]. The moment\nmatching term can be interpreted as a distance with a scat-\ntering transform kernel [8]. However, in this case it can\ndirectly be implemented as a difference of moments.\n4. WHITENED TIME-FREQUENCY SCATTERING\nThis section details the time-frequency scattering trans-\nformSJ(X)originally introduced in [1] and its whiten-\nX\n Log-Spectrogr.\n 1\n`\n2D ﬁlter\nModulush\u0018\n 2\n`0\nAveraging\nSubsampling\n\u001eJScattering\n2J\nFigure 3 . Time-Frequency Scattering transform. The log-\nspectrogram is obtained with a ﬁrst wavelet transform  1\n`\nfollowed by a modulus. A joint time-frequency ﬁltering of\nthis log-spectrogram with the ﬁlters h\u0018\n 2\n`0regularizes the\ntime-frequency deformations of the signal. The low-pass\nconvolution with \u001eJGaussianizes the resulting tensor.\ningL, which results in the embedding Z[2Jn]. Figure 3\nsketches the different computational steps. This transform\nrelies on priors on musical signals in order to build a time-\ndependent vector representation SJX[2Jn]which is ap-\nproximately Gaussian and linearizes small time-frequency\ndeformations.\nMusical signals admit a sparse decomposition in time-\nfrequency representations with a spectrogram. Here, we\nﬁrst compute a spectrogram with frequencies sampled\non a logarithmic scale thanks to a wavelet ﬁlterbank\nf 1\n`g0\u0014`<J followed by a modulus non-linearity. The\nwavelets 1\n`are deﬁned by dilations of a single mother\nwavelet:\n 1\n`[t] = 2\u0000`=Q 1[2\u0000`=Qt] (7)\nWe use causal analytic Gammatone wavelets [20],\nwhich are good perceptual models of auditory ﬁlters,\nwithQ= 12 wavelets per octave in order to separate high-\nfrequency partials.\nOn this sparse spectrogram, small time-frequency de-\nformations of the input signal Xproduce small local trans-\nlations in the time-frequency plane. These deformations\nresult in smooth perceptual variations. As a consequence,\nthe embedding should be regular with respect to these de-\nformations This is obtained with a joint 2D ﬁltering of\nthe spectrogram in the time and log-frequency axis. One\ncan prove that the resulting representation is Lipschitz-\ncontinuous to these deformations, while preserving invert-\nibility thanks to the use of ﬁlters spanning all the energy\nof the signal [1]. This will imply a form a linearization of\nthese deformations, paving the way for meaningful arith-\nmetic in the latent space.\nThe time-frequency ﬁlters are built as a separable prod-\nucth\u0018\n 2\n`0of frequential ﬁlters h\u0018and temporal ﬁl-\nters 2\n`0. The frequential ﬁlters h\u0018are localized Fourier\natoms with a Hann window whose size Pmatches one oc-\ntave,P=Q= 12 . The convolution is computed in half-\noverlaps over the frequency axis. The temporal ﬁlters  2\n`0Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 329are also Gammatone wavelets, with only Q2= 1wavelet\nper octave. After performing the convolution, a modulus\nnon-linearity is also applied in order to remove the local\nphase, thereby regularizing the representation.\nThe time-frequency ﬁltering of the log-spectrogram re-\nsults in a large tensor with one temporal axis. Its co-\nefﬁcients are sparse along the channel axis and typically\ndecorrelate as they get farther apart in time. To obtain a\nvariable which is more Gaussian, we use the central limit\ntheorem which says that the averaging of a large number of\nindependant variables converges towards a Gaussian ran-\ndom variable. We perform this averaging with a window\nsize2Jwhich should be larger than the typical decorrela-\ntion length in order to average over enough independent\nevents. This averaging is carried out with a low-pass ﬁlter\n\u001eJalong the temporal axis. It is followed by a subsam-\npling by 2Jin order to remove redundant information.\nThe time-frequency scattering transform SJXis de-\nﬁned as:\nSJX[2Jn] =h\nx?\u001eJ[2Jn];\f\fjx? 1\n`j?h\u0018\f\f?\u001eJ[2Jn];\n\f\fjx? `j?(h\u0018\n 2\n`0)\f\f?\u001eJ[2Jn]i\n;(8)\nfor all`;\u0018;`0, where convolutions with h\u0018should be in-\nterpreted along the frequential `axis and other convolu-\ntions along time. SJXis subsampled in time by a fac-\ntor2J. Each vector SJX[2Jn]has dimension kJ=\n1 +Q(2J\u00003) +Q(J\u00002)2. ForJ= 10 andQ= 12 ,\nthis amounts to 973channels. The scale 2Jis chosen as\na trade-off between the Gaussianization condition which\nimproves as Jincreases, and the stability of the scattering\ninvertibility which improves when Jdecreases.\nThanks to the local averaging, the vectors SJXtend to\na Gaussian distribution. However, this distribution might\nnot be white, i.e. it has temporal and channel-wise cor-\nrelations. The whitening operator Lis a causal vector\nautoregressive linear ﬁlter which removes this correlation\nstructure. It is trained by minimizing a prediction error\nofSJX[2Jn]given previous vectors SJX[2J(n\u0000m)]\nfor1\u0014m\u0014M. The whitening operator Loutputs\nthe innovations of the ﬁtted vector autoregressive process\nfZ[2Jn]gn, which have an approximately Gaussian white\ndistribution.\n5. NUMERICAL EXPERIMENTS\nWe show that the MM-SIN is able to reconstruct wave-\nforms from their embeddings and generate new decent\nwaveforms from noise. Furthermore, we show that it is\npossible to manipulate low-level attributes of sounds such\nas pitch with a simple arithmetic in the embedding. In\naddition, a simple arithmetic in the embedding allows to\nmerge the contents of different inputs, while preserving the\nmusical structure of the resulting signal.\n5.1 Methods\nWe describe the numerical details which lay out experi-\nments. The source code supporting experiments is freelyavailable at http://github.com/AndreuxMath/\nismir2018 , where the reader may also ﬁnd the audio\nrecordings corresponding to the ﬁgures.\nThe time-frequency scattering transform SJis com-\nputed with an averaging window of size 2J= 210= 1024 .\nIt is implemented on GPU with a code inspired from [17].\nIn the case of the loss (4), we employ a ﬁrst-order scat-\nteringSK, which means that it is an averaged scalogram,\nwith an averaging window of size 2K= 25= 32 . The ﬁl-\nterbanks are normalized so as to have responses of average\nequal magnitude in each band over the training dataset.\nThe architecture of the SIN Gis deﬁned as follows. The\nwhitening operator Lhas a past size M= 4. All subse-\nquent convolutions have a kernel size equal to 7. These val-\nues were not tuned: results could likely be improved with\na careful hyperparameter search. Each network is trained\nby Adam [12] with a learning rate of 5\u000210\u00004for1200\nepochs and batches of size 128.\nWe use two different musical datasets: NSynth [5] and\nBeethoven [14]. NSynth is a dataset consisting of anno-\ntated musical notes from multiple instruments, thereby al-\nlowing to perform carefully controlled transformation ex-\nperiments. All recordings begin with the onset of the note\nand last 4s. We restrict ourselves to two types of acoustic\ninstruments, keyboards and ﬂutes, totalling 40different in-\nstruments with MIDI pitches ranging 20\u0000110, leading to\na varied and well-balanced dataset. In the original dataset,\ninstruments of the training and testing sets do not overlap.\nIn this paper, we use an alternative split based on the veloc-\nity’s attributes of the training samples: for each instrument,\na random velocity is picked to deﬁne the test set. We only\nuse the ﬁrst 2sof the recordings, as they concentrate most\nof the energy of the signals.\nThe Beethoven dataset is closer to an actual musical\ncomposition than NSynth, insofar as it consists in 8sex-\ntracts of Beethoven’s piano sonata. Therefore, it is a good\ntestbed for music generation experiments. We use the\ntrain-test split provided by the authors.\nFor both datasets, the amplitudes of all recordings are\nnormalized in [\u00001;1]. The sampling rate is reduced from\n16000Hz to4096Hz so as to reduce the computational\ncomplexity. It is very likely that the quality of the syn-\nthesis could be improved by increasing this sampling rate.\n5.2 Waveform generation\nWe ﬁrst show that the MM-SIN generator is able to recon-\nstruct and to generate realistic musical samples.\nIn Figure 4, we display two reconstructions of wave-\nforms from their embeddings, along with the correspond-\ning log-spectrograms, for each of the studied datasets. This\nshows that the network is able to generalize to a test set,\nand to adapt to the speciﬁcs of a given dataset. The recon-\nstruction is not perfect. The network can introduce small\ntime-frequency deformations because of the scattering en-\ncoder and the use of a scattering loss. As witnessed in the\nlog-spectrograms, the time-frequency content of the sig-\nnals is correctly retrieved, and perceptually the two signals\nsound similar, up to minor artifacts.330 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 4 . Reconstruction with the MM-SIN G. Left:\nOriginal test sample X. Right: Reconstruction G(Z)for\nZ=LSJX. Top: NSynth dataset. Bottom: Beethoven\ndataset. A different network was trained for each dataset.\nTable 1 provides quantitative reconstruction results on\nthe Beethoven dataset. Training a network with the mean-\nsquare error (MSE) metric kxi\u0000G(zi)k2\n2instead of the\nproposed perceptual metric within the inverse problem\nloss (5) negatively impacts results, both on the training and\ntesting sets. Further, the moment-matching term has a pos-\nitive effect on the reconstruction: even though it degrades\nreconstruction on the training set, it improves the general-\nization on the test set both in absolute and relative terms.\nWe now investigate the ability of the network to gen-\nerate new waveforms from Gaussian white noise. Fig-\nure 5 displays several samples generated from white noise\nthrough a network trained on the Beethoven dataset. De-\nspite the input being a pure white noise, the network is able\nto generate samples which alternate silences and more ac-\ntive phases. Further, the fundamental frequency which is\nplayed varies through the samples.\nIn order to measure the variability of the generated\nsamples, we measure the spread \u001bof the distribution of\nthe time-averaged scattering coefﬁcients SKXof the sam-\nples. This spread corresponds to the average Euclidean\ndistance between the time-averaged scattering of the wave-\nforms and the average scattering coefﬁcients of this distri-\nbution. In the case of the training distribution, we obtained\nLossLinvLinvLinv+\u0015LMM\nMetric MSESKSK\nTrain error 0.56 0.16 0.23\nTest error 0.77 0.37 0.31\nGap test/train\n(dB)1.36 3.53 1.21\nTable 1 . Reconstruction errors on the Beethoven dataset,\nexpressed in terms of the perceptual loss (5). MSE denotes\nthe mean-square error metric kxi\u0000G(zi)k2\n2. Using the\nperceptual metric for training instead of the MSE metric\nreduces the error. Further, adding the moment-matching\nterm during training improves the reconstruction results\nand the generalization.\nFigure 5 . Musical signal G(Z)generated from a white\nnoiseZ, whereGis learned on the Beethoven dataset.\nEach line corresponds to an independent sample obtained\nfrom a different white noise realization. The resulting sig-\nnals last about 4s.\n\u001b= 6:69, whereas\u001b= 3:51for the distribution generated\nfrom white noise. This shows that the generated samples\nexhibit a non-negligible variability, even though it is lower\nthan the one expressed in the training set.\nThe effect of the moment matching loss (6) on the gen-\nerated samples is difﬁcult to assess qualitatively, so we\nresort to quantitative measures. In the case of a network\ntrained without this loss, the moment matching distance\nbetween generated samples and the training set was equal\nto38:7, whereas the same distance was equal to 0:176\nwhen also optimizing this loss. As a comparison, the test-\ning set has a distance of 0:334with respect to the training\nset. Thus, using this loss term brings generated samples\nmuch closer to the natural signals’ statistics.\n5.3 Pitch modiﬁcation\nWe now study the ability of the algorithm to transform the\npitch of musical signals with an arithmetic operations in\nthe latent space. We use the NSynth dataset, whose care-\nful construction allows to perform modiﬁcations with ﬁxed\nfactors of variability. In the test set, we pick two sam-\nples belonging to the same instrument, but with a pitch\nseparated by 5MIDI scales. We compute their embed-\ndingsZ1andZ2, their mean embedding (Z1+Z2)=2, and\nreconstruct the corresponding signals with the generator:\nG(Z1),G(Z2)andG((Z1+Z2)=2).\nThe results are displayed in Figure 6. The interpolationProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 331Figure 6 . Pitch interpolation. Left column: G(Z1). Mid-\ndle column: G((Z1+Z2)=2). Right column: G(Z2).Z1\nandZ2are the embeddings of samples from the test set.\nThe generator interpolates the fundamental frequency with\na simple arithmetic. The frequential displacement from left\nto right corresponds to 5MIDI scales.\nin the latent space does not result in a linear interpolation\nwhich would double the number of harmonics. It yields\none fundamental frequency in each case. Furthermore, this\nfundamental frequency is indeed interpolated by this sim-\nple arithmetic. Observe that this is also the case of the\npartials, as can be seen in particular in the bottom exam-\nple. However, this interpolation suffers from some arti-\nfacts. For instance, in the middle example, the partials at\nhighest frequencies are cluttered and the resulting signal\nmisses harmonicity. Yet, these results showcase the ability\nto transform signals via linear interpolations in the latent\nspace with a simple unsupervised learning procedure and a\npredeﬁned embedding.\nThe algorithm owes the ability to perform such pitch\ninterpolations to the time-frequency scattering transform\nSJused as an encoder, which regularizes small time-\nfrequency deformations. As such, the pitch interval on\nwhich the interpolations can be performed is bounded by\nthe sizePof the Hann window used to ﬁlter the scalogram\nalong the frequency axis.\nFigure 7 . Interpolations in the latent and signal space. Top\ntwo signals: G(Z1)andG(Z2), whereZ1;Z2are Gaussian\nwhite noise realizations and Gis trained on the Beethoven\ndataset. Bottom left: Latent interpolation G((Z1+Z2)=2).\nBottom right: (G(Z1) +G(Z2))=2. The latent interpo-\nlation is able to merge both signals while preserving the\nmusical structure.\n5.4 Waveform interpolation\nLet us show results when interpolating waveforms from the\nBeethoven dataset, which have a high density of musical\nevents. We take two random white noise realizations Z1\nandZ2, and compare the effect on the waveforms of an\ninterpolation in the latent space and in the signal space,\nwith a network Gtrained on the Beethoven dataset.\nThe results are represented in Figure 7. The top two\nsignals are the original signals G(Z1)andG(Z2), while\nthe bottom left is the latent interpolation G((Z1+Z2)=2)\nand the bottom right the linear interpolation (G(Z1) +\nG(Z2))=2. The latent interpolation incorporates patterns\nfrom both signals but it respects the musical structure. It\nhas successive musical notes with their harmonics, and it\nrecovers a sound with silences. On the opposite, the linear\ninterpolation merges both signals, which eliminates the si-\nlence regions while producing a cluttered log-spectrogram.\n6. CONCLUSION\nThis paper introduces a causal musical synthesis network\noptimized through an inverse problem and which thus in-\nvolves no learned encoder or discriminator. The encoder is\ndeﬁned from time-frequency signal priors in order to Gaus-\nsianize the input signal. The generator network maps back\nthe resulting codes to raw waveforms. This network inverts\nthe encoder and generates new signals whose scattering\nmoments match those of the original signals. The resulting\nsystem synthesizes new realistic musical signals and per-\nforms the transformation of low-level attributes, such as\npitch, by simple linear combinations in the latent space.\nSynthesized signals do not reach the quality of state-of-\nthe-art generating architectures but these ﬁrst results show\nthat this approach is a new promising avenue to synthesize\naudio signals directly from Gaussian white noise, without\nlearning encoders or discriminators.332 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20187. ACKNOWLEDGEMENTS\nThis work is supported by the ERC InvariantClass grant\n320959 and an AMX grant from the MNESR.\n8. REFERENCES\n[1] J. Anden, V . Lostanlen, and S. Mallat. Joint time-\nfrequency scattering for audio classiﬁcation. In Proc.\nof IEEE MLSP , 2015.\n[2] Tom `as Angles and St ´ephane Mallat. Generative net-\nworks as inverse problems with scattering transforms.\nInInternational Conference on Learning Representa-\ntions , 2018.\n[3] M. Blaauw and J. Bonada. Modeling and transforming\nspeech using variational autoencoders. In Interspeech ,\npages 1770–1774, 2016.\n[4] J. Chorowski, R.J. Weiss, R. A. Saurous, and S. Ben-\ngio. On using backpropagation for speech texture gen-\neration and voice conversion. In International Confer-\nence on Audio and Speech Processing (ICASSP) , 2018.\n[5] J. Engel, C. Resnick, A. Roberts, S. Dieleman,\nM. Norouzi, D. Eck, and K. Simonyan. Neural au-\ndio synthesis of musical notes with WaveNet autoen-\ncoders. In Proceedings of the 34th International Con-\nference on Machine Learning , pages 1068–1077, 2017.\n[6] L. Gatys, A. S. Ecker, and M. Bethge. Texture synthe-\nsis using convolutional neural networks. In Advances\nin Neural Information Processing Systems , pages 262–\n270, 2015.\n[7] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio.\nGenerative adversarial nets. Advances in Neural Infor-\nmation Processing Systems , pages 2672–2680, 2014.\n[8] A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch ¨olkopf,\nand A. J. Smola. A kernel method for the two-sample-\nproblem. In Advances in neural information processing\nsystems , pages 513–520, 2007.\n[9] E. Grinstein, N. Duong, A. Ozerov, and P. P ´erez. Audio\nstyle transfer. HAL preprint hal-01626389 , 2017.\n[10] W.-N. Hsu, Y . Zhang, and J. Glass. Learning latent rep-\nresentations for speech generation and transformation.\nInInterspeech , pages 1273–1277, 2017.\n[11] S. Ioffe and C. Szegedy. Batch normalization: Accel-\nerating deep network training by reducing internal co-\nvariate shift. In International Conference on Machine\nLearning , pages 448–456, 2015.\n[12] D. P. Kingma and J. Ba. Adam: A method for stochas-\ntic optimization. arXiv preprint arXiv:1412.6980 ,\n2014.[13] D. P. Kingma and M. Welling. Auto-encoding varia-\ntional bayes. In International Conference on Learning\nRepresentations , 2014.\n[14] S. Mehri, K. Kumar, I. Gulrajani, R. Kumar, S. Jain,\nJ. Sotelo, A. Courville, and Y . Bengio. SampleRNN:\nAn unconditional end-to-end neural audio generation\nmodel. In International Conference on Learning Rep-\nresentations , 2017.\n[15] A. Van Den Oord, S. Dieleman, H. Zen, K. Simonyan,\nO. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and\nK. Kavukcuoglu. WaveNet: A generative model for\nraw audio. arXiv preprint arXiv:1609.03499 , 2016.\n[16] A. Van Den Oord, Y . Li, I. Babuschkin, K. Simonyan,\nO. Vinyals, K. Kavukcuoglu, G. van den Driessche,\nE. Lockhart, L. C. Cobo, F. Stimberg, et al. Parallel\nWaveNet: Fast high-ﬁdelity speech synthesis. arXiv\npreprint arXiv:1711.10433 , 2017.\n[17] E. Oyallon, E. Belilovsky, and S. Zagoruyko. Scal-\ning the scattering transform: Deep hybrid networks. In\nProc. of ICCV , 2017.\n[18] A. Radford, L. Metz, and R. Chintala. Unsupervised\nrepresentation learning with deep convolutional gener-\native adversarial networks. In International Conference\non Learning Representations , 2016.\n[19] I. Tolstikhin, O. Bousquet, S. Gelly, and B. Sch ¨olkopf.\nWasserstein auto-encoders. In International Confer-\nence on Learning Representations , 2018.\n[20] A. Venkitaraman, A. Adiga, and C. S. Seelaman-\ntula. Auditory-motivated gammatone wavelet trans-\nform. Signal Processing , 94:608–619, 2014.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 333"
    },
    {
        "title": "Audio-to-Score Alignment using Transposition-invariant Features.",
        "author": [
            "Andreas Arzt",
            "Stefan Lattner"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492485",
        "url": "https://doi.org/10.5281/zenodo.1492485",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/166_Paper.pdf",
        "abstract": "Audio-to-score alignment is an important pre-processing step for in-depth analysis of classical music. In this paper, we apply novel transposition-invariant audio features to this task. These low-dimensional features represent local pitch intervals and are learned in an unsupervised fashion by a gated autoencoder. Our results show that the proposed features are indeed fully transposition-invariant and enable accurate alignments between transposed scores and performances. Furthermore, they can even outperform widely used features for audio-to-score alignment on 'untransposed data', and thus are a viable and more flexible alternative to well-established features for music alignment and matching.",
        "zenodo_id": 1492485,
        "dblp_key": "conf/ismir/ArztL18",
        "keywords": [
            "audio-to-score alignment",
            "pre-processing step",
            "in-depth analysis",
            "transposition-invariant audio features",
            "unsupervised learning",
            "gated autoencoder",
            "pitch intervals",
            "accurate alignments",
            "transposed scores",
            "performances"
        ],
        "content": "AUDIO-TO-SCORE ALIGNMENT\nUSING TRANSPOSITION-INV ARIANT FEATURES\nAndreas Arzt1Stefan Lattner1;2\n1Institute of Computational Perception, Johannes Kepler University, Linz, Austria\n2Sony Computer Science Laboratories (CSL), Paris, France\nandreas.arzt@jku.at\nABSTRACT\nAudio-to-score alignment is an important pre-processing\nstep for in-depth analysis of classical music. In this pa-\nper, we apply novel transposition-invariant audio features\nto this task. These low-dimensional features represent lo-\ncal pitch intervals and are learned in an unsupervised fash-\nion by a gated autoencoder. Our results show that the\nproposed features are indeed fully transposition-invariant\nand enable accurate alignments between transposed scores\nand performances. Furthermore, they can even outperform\nwidely used features for audio-to-score alignment on ‘un-\ntransposed data’, and thus are a viable and more ﬂexible al-\nternative to well-established features for music alignment\nand matching.\n1. INTRODUCTION\nThe task of synchronising an audio recording of a music\nperformance and its score has already been studied exten-\nsively in the area of intelligent music processing. It forms\nthe basis for multi-modal inter- and intra-document nav-\nigation applications [6, 10, 35] as well as for the analysis\nof music performances, where e.g. aligned pairs of scores\nand performances are used to extract tempo curves or learn\npredictive performance models [12, 39].\nTypically, this synchronisation task, known as audio-to-\nscore alignment, is based on a symbolic score representa-\ntion, e.g. in the form of MIDI or MusicXML. In this paper,\nwe follow the common approach of converting this score\nrepresentation into a sound ﬁle using a software synthe-\nsizer. The result is a low-quality rendition of the piece, in\nwhich the time of every event is known. Then, for both\nsequences the same kinds of features are computed, and a\nsequence alignment algorithm is used to align the audio of\nthe performance to the audio representation of the score,\ni.e. the problem of audio-to-score alignment is treated as\nan audio-to-audio alignment task. The output is a map-\nping, relating all events in the score to time points in the\nc\rAndreas Arzt, Stefan Lattner. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Andreas Arzt, Stefan Lattner. “Audio-to-Score Alignment\nusing Transposition-invariant Features”, 19th International Society for\nMusic Information Retrieval Conference, Paris, France, 2018.performance audio. Common features for this task include\na variety of chroma-based features [8, 14, 15, 25], features\nbased on the semitone scale [4,6], and mel-frequency cep-\nstral coefﬁcients (MFCCs) [13].\nIn this paper, we apply novel low-dimensional features\nto the task of music alignment. The features represent local\npitch intervals and are learned in an unsupervised fashion\nby a gated autoencoder [23]. We will demonstrate how\nthese features can be used to synchronise a recording of a\nperformance to a transposed version of its score. Further-\nmore, as they are only based on a local context, the features\ncan even cope with multiple transpositions within a piece\nwith only minimal additional alignment error, which is not\npossible at all with common pitch-based feature represen-\ntations.\nThe main contributions of this paper are (1) the intro-\nduction of novel transposition-invariant features to the task\nof music synchronisation, (2) an in-depth analysis of their\nproperties in the context of this task, and (3) a direct com-\nparison to chroma features, which are the quasi-standard\nfor this task. A cleaned-up implementation of the code\nfor the gated autoencoder used in this paper is publicly\navailable1. The paper is structured as follows. In Sec-\ntion 2, the features are introduced. Section 3 brieﬂy de-\nscribes the alignment algorithm we are using throughout\nthe paper. Then, in Section 4 we present detailed experi-\nments on piano music, including a comparison of different\nfeature conﬁgurations, results on transposed scores, and a\ncomparison with chroma features. In Section 5 we discuss\nthe application of the features in the domain of complex\norchestral music. Finally, Section 6 gives an outlook on\nfuture research directions.\n2. TRANSPOSITION-INV ARIANT FEATURES\nFOR MUSIC SYNCHRONISATION\nTransposition-invariant methods have been studied exten-\nsively in music information retrieval (MIR), for example\nin the context of music identiﬁcation [2], structure analysis\n[26], and content-based music retrieval [19, 21, 36]. How-\never, so far there has been limited success in transposition-\ninvariant audio-to-score alignment. Currently, a typical ap-\nproach is to ﬁrst try to identify the transposition, transform\nthe inputs accordingly, and then apply common alignment\n1seehttps://github.com/SonyCSLParis/cgae-invar592techniques (see e.g. [33]). Another option is to perform\nthe alignment multiple times, with different transpositions\n(e.g. the twelve possible transposition options when us-\ning chroma-based features) and then select the alignment\nwhich produced the least alignment costs (see e.g. [34]).\nThese are cumbersome and error-prone methods. In this\npaper, we demonstrate how to employ novel transposition-\ninvariant features for the task of score-to-audio alignment,\ni.e. the features themselves are transposition-invariant.\nThese features have been proposed recently in [17] and\ntheir usefulness has been demonstrated for tasks like the\ndetection of repeated (but possibly transposed) motifs,\nthemes and sections in classical music.\nThe features are learned automatically from audio data\nin an unsupervised way by a gated autoencoder. The main\nidea is to try to learn a relative representation of the current\naudio frame, based on a small local context (i.e., n-gram,\nthe previous nframes). During the training process, the\ngated autoencoder is forced to represent this target frame\nvia its preceding frames in a relative way (i.e. via interval\ndifferences between the local context and the target frame).\nIn the following, we give a more detailed description of\nhow these features are learned. Speciﬁcs about the train-\ning data we are using in this paper can be found in the re-\nspective sections on applying this approach to piano music\n(Section 4) and orchestral music (Section 5).\n2.1 Model\nLetxt2RMbe a vector representing the energy dis-\ntributed over Mfrequency bands at time t. Given a tem-\nporal context xt\nt\u0000n=xt\u0000n:::xt(i.e. the input) and the\nnext time slice xt+1(i.e. the target), the goal is to learn a\nmapping mt(i.e. the transposition-invariant feature vector\nat time t) which does not change when shifting xt+1\nt\u0000nup-\nor downwards in the pitch dimension.\nGated autoencoders (GAEs, see Figure 1) are funda-\nmentally different to standard sparse coding models, like\ndenoising autoencoders. GAEs are explicitly designed to\nlearn relations (i.e., covariances) between data pairs by em-\nploying an element-wise product in the ﬁrst layer of the\narchitecture. In musical sequences, using a GAE for learn-\ning relations between pitches in the input and pitches in the\ntarget naturally results in representations of musical inter-\nvals. The intervals are encoded in the latent variables of\nthe GAE as mapping codes mt(refer to [17] for more de-\ntails on interval representations in a GAE). The goal of the\ntraining is to ﬁnd a mapping for any input/target pair which\ntransforms the input into the given target by applying the\nrepresented intervals. The mapping at time tis calculated\nas\nmt=\u001bh(W1\u001bh(W0(Uxt\nt\u0000n\u0001Vxt+1))); (1)\nwhere U;VandWkare weight matrices, and \u001bhis the hy-\nperbolic tangent non-linearity. The operator \u0001(depicted as\na triangle in Figure 1) denotes the Hadamard (or element-\nwise) product of the ﬁlter responses Uxt\nt\u0000nandVxt+1,\nFigure 1 . Schematic illustration of the gated autoencoder\narchitecture used for feature learning. Double arrows de-\nnote weights used for both, inference of the mapping mt\nand the reconstruction of xt+1.\ndenoted as factors . The target of the GAE can be recon-\nstructed as a function of the input xt\nt\u0000nand a mapping\nmt:\n~ xt+1=V>(W>\n0W>\n1mt\u0001Uxt\nt\u0000n): (2)\nAs cost function we use the mean-squared error be-\ntween the target xt+1and the target’s reconstruction ~ xt+1\nas\nMSE =1\nMkxt+1\u0000~ xt+1k2: (3)\n2.2 Training Data Preprocessing\nModels are learned directly from audio data, without the\nneed for any annotations. The empirically found prepro-\ncessing parameters are as follows. The audio ﬁles are re-\nsampled to 22.05 kHz. We choose a constant-Q trans-\nformed spectrogram using a hop size of 448 (\u001820ms),\nand Hann windows with different sizes depending on the\nfrequency bin. The range comprises 120frequency bins\n(24 per octave), starting from a minimal frequency of 65:4\nHz. Each time slice is contrast-normalized to zero mean\nand unit variance.\n2.3 Training\nThe model is trained with stochastic gradient descent in\norder to minimize the cost function (cf. Equation 3) using\nthe training data as described in Section 2.2. In an altered\ntraining procedure introduced below, we randomly trans-\npose the data during training and explicitly aim at transpo-\nsition invariance of the mapping codes.\n2.3.1 Enforcing Transposition-Invariance\nAs described in Section 2.1 the classical GAE training pro-\ncedure derives a mapping code from an input/target pair,\nand subsequently penalizes the reconstruction error of the\ntarget given the input and the derived mapping code. Al-\nthough this procedure naturally tends to lead to similarProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 593mapping codes for input target pairs that have the same in-\nterval relationships, the training does not explicitly enforce\nsuch similarities and consequently the mappings may not\nbe maximally transposition invariant.\nUnder ideal transposition invariance, by deﬁnition the\nmappings would be identical across different pitch trans-\npositions of an input/target pair. Suppose that a pair\n(xt\nt\u0000n;xt+1)leads to a mapping m(by Equation (1)).\nTransposition invariance implies that reconstructing a tar-\ngetx0\nt+1from the pair (x0t\nt\u0000n;m)should be as success-\nful as reconstructing xt+1from the pair (xt\nt\u0000n;m)when\n(x0t\nt\u0000n;x0\nt+1)can be obtained from (xt\nt\u0000n;xt+1)by a sin-\ngle pitch transposition.\nOur altered training procedure explicitly aims to\nachieve this characteristic of the mapping codes by pe-\nnalizing the reconstruction error using mappings obtained\nfrom transposed input/target pairs. More formally, we de-\nﬁne a transposition function shift(x;\u000e), shifting the values\n(CQT frequency bins) of a vector xof lengthMby\u000esteps:\nshift(x;\u000e) = (x(0+\u000e) modM;:::;x (M\u00001+\u000e) modM)>;\n(4)\nandshift(xt\nt\u0000n;\u000e)denotes the transposition of each single\ntime step vector before concatenation and linearization.\nThe training procedure is then as follows: First, the\nmapping code mt+1of an input/target pair is inferred as\nshown in Equation 1. Then, mt+1is used to reconstruct\natransposed version of the target, from an equally trans-\nposed input (modifying Equation 2) as\n~ x0\nt+1=\u001bg(V>(W>\n0W>\n1mt\u0001Ushift(xt\nt\u0000n;\u000e)));(5)\nwith\u000e2[\u000060;60]randomly chosen for each training\nbatch. Finally, we penalize the error between the recon-\nstruction of the transposed target and the actual transposed\ntarget (i.e., employing Equation 3) as\nMSE =1\nM\r\rshift(xt+1;\u000e)\u0000~ x0\nt+1\r\r2: (6)\nThis method amounts to both, a form of guided training\nand data augmentation.\n2.3.2 Training Details\nThe architecture and training details of the GAE are as fol-\nlows. In this paper, we use two models with differing n-\ngram lengths. The factor layer has 512units for n-gram\nlengthn= 16 , and 256units forn= 8. Furthermore, for\nall models, there are 128neurons in the ﬁrst mapping layer\nand64neurons in the second mapping layer, i.e. the fea-\ntures we will be using throughout this paper for the align-\nment task are 64-dimensional.\nL2 weight regularization for weights UandVis ap-\nplied, as well as sparsity regularization [18] on the top-\nmost mapping layer. The deviation of the norms of the\ncolumns of both weight matrices UandVfrom their av-\nerage norm is penalized. Furthermore, we restrict these\nnorms to a maximum value. We apply 50% dropout on\nthe input and no dropout on the target, as proposed in [23].\nThe learning rate (1e-3) is gradually decremented to zero\nover 300 epochs of training.3. ALIGNMENT ALGORITHM\nThe goal of this paper is to give the reader a good intuition\nabout the novel transposition-invariant features for audio\nalignment and focus on their properties, without being dis-\ntracted by a complicated alignment algorithm. Thus, we\nuse a simple multi-scale variant of the dynamic time warp-\ning (DTW) algorithm (see [25] for a detailed description of\nDTW) for the experiments throughout the paper, namely\nFastDTW [32] with the radius parameter set to 50. We\nperformed all experiments presented in this paper using\nthe cityblock, Euclidean and cosine distance measures to\ncompute distances between feature vectors. Because the\nchoice of distance measure did not have a sizeable impact,\nwe only report the results using the Euclidean distance. As\nFastDTW is a well-known and widely used algorithm, we\nrefrain from describing the algorithm here in detail and re-\nfer the reader to the referenced works.\nObviously, a large number of more sophisticated alter-\nnatives to FastDTW exists. This includes methods based\non hidden Markov and semi-Markov models [27–29],\nconditional random ﬁelds [16], general graphical models\n[5, 20, 30, 31], Monte Carlo sampling [7, 24], and exten-\nsions to DTW, e.g. multiple sequence alignment [37] and\nintegrated tempo models [3]. We are conﬁdent that the\npresented features can also be employed successfully with\nthese more sophisticated alignment schemes.\n4. EXPERIMENTS ON PIANO MUSIC\nIn this section we present a number of experiments, show-\ncasing the strengths of the proposed features as well as\ntheir weaknesses. We will do this on piano music ﬁrst, be-\nfore moving on to more complex orchestral music in Sec-\ntion 5. For learning the features, a dataset consisting of\n100 random piano pieces of the MAPS dataset [9] (subset\nMUS) was used. As discussed in Section 2, no annota-\ntions are needed, thus actually any available audio record-\ning of piano music could be used. For the experiments, we\ntrained two models, differing in the size of their local con-\ntext: an 8-gram model and a 16-gram model (referred to as\n8G Piano and16G Piano in the remainder of the paper).\nFor the evaluation of audio-to-score alignment, a col-\nlection of annotated test data (pairs of scores and exactly\naligned performances) is needed. We performed experi-\nments on four datasets (see Table 1). CBandCEconsist of\n22 recordings of the Ballade Op. 38 No. 1 and the Etude\nOp. 10 No. 3 by Chopin [11], MScontains performances\nof the ﬁrst movements of the piano sonatas KV279-284,\nKV330-333, KV457, KV475 and KV533 by Mozart [38],\nandRPconsists of three performances of the Prelude Op.\n23 No. 5 by Rachmaninoff [1]. The scores are provided in\nthe MIDI format. Their global tempo is set such that the\nscore audio roughly matches the mean length of the given\nperformances. The scores are then synthesised with the\nhelp of timidity2and a publicly available sound font. The\nresulting audio ﬁles are used as score representations for\nthe alignment experiments.\n2https://sourceforge.net/projects/timidity/594 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018ID Dataset Files Duration\nCE Chopin Etude 22 \u001830 min.\nCB Chopin Ballade 22 \u001848 min.\nMS Mozart Sonatas 13 \u001885 min.\nRP Rachmaninoff Prelude 3 \u001812 min.\nTable 1 . The evaluation data set for the experiments on\npiano music (see text).\nIn the experiments, we use two types of evaluation mea-\nsures. For each experiment, the 1stquartile, the median,\nand the 3rdquartile of the absolute errors at aligned refer-\nence points is given. We also report the percentage of refer-\nence points which have been aligned with errors smaller or\nequal 50 ms, and smaller or equal 250 ms (similar to [6]).\n4.1 Experiment 1: Feature Conﬁgurations\nThe ﬁrst experiment compares the performance of the two\nfeature conﬁgurations 8G Piano and16G Piano on the pi-\nano evaluation set (see Table 2). The differences between\nthe two conﬁgurations are relatively small, although the\n8-gram feature consistently works slightly better than the\n16-gram features. The danger of using a larger local con-\ntext is that different tempi can lead to very different con-\ntexts (e.g. faster tempi result in more notes contained in\nthe local context), which in turn leads to different features,\nwhich is a problem for the matching process. We will re-\nturn to this problem in a later experiment (see Section 4.4).\nBecause of space constraints, in the upcoming sections, we\nwill only report the results for 8G Piano .\n4.2 Experiment 2: Transposition-invariance\nNext, we demonstrate that the learned features are actu-\nally invariant to transpositions. To do so, we transposed\nthe score representations by -3, -2, -1, 0, +1, +2 and +3\nsemitones and tried to align the untransposed performances\nto these scores. The results for the 8G Piano features are\nshown in Table 3. The results for all transpositions, includ-\ning the untransposed scores, are very similar. Only minor\nﬂuctuations occur randomly.\nIn addition, we prepared a second, more challenging\nexperiment. We manipulated the scores such that af-\nter every 30 seconds another transposition from the set\nf\u00003;\u00002;\u00001;+1;+2;+3gis randomly applied. From\neach score, we created ﬁve such randomly changing score\nrepresentations and tried to align the performances to these\nscores. The results are shown in the rightmost column\nof Table 3. Again, there is no difference to the other re-\nsults. Basically, the transpositions only lead to at most\neight noisy feature vectors every time a new transposition\nis applied, which is not a problem for the alignment algo-\nrithm. We would also like to note that very few algorithms\nor features would be capable of solving this task (see [26]\nfor another option). Other methods that ﬁrst try to globally\nidentify the transposition and then use traditional methods\nfor the alignment are clearly not applicable here.Dataset Measure 8G Piano 16G Piano\nCB1stQuartile 10 ms 11 ms\nMedian 22 ms 24 ms\n3rdQuartile 39 ms 45 ms\nError\u001450 ms 83% 79%\nError\u0014250 ms 94% 95%\nCE1stQuartile 10 ms 12 ms\nMedian 21 ms 25 ms\n3rdQuartile 36 ms 45 ms\nError\u001450 ms 87% 79%\nError\u0014250 ms 96% 95%\nMS1stQuartile 6 ms 6 ms\nMedian 13 ms 14 ms\n3rdQuartile 25 ms 26 ms\nError\u001450 ms 90% 91%\nError\u0014250 ms 100% 100%\nRP1stQuartile 14 ms 16 ms\nMedian 34 ms 40 ms\n3rdQuartile 90 ms 92 ms\nError\u001450 ms 63% 57%\nError\u0014250 ms 90% 93%\nTable 2 . Comparison of the 8-gram and the 16-gram fea-\nture models.\n4.3 Experiment 3: Comparison to Chroma Features\nIt is now time to compare the 8G Piano features to well-\nestablished features for the task of music alignment in\nthe normal, un-transposed alignment setting. To this end,\nwe computed the chroma cqtfeatures3(henceforth re-\nferred to as Chroma ) as provided by librosa4[22] (with\nstandard parameters except for the normalisation param-\neter, which we set to 1; the hop size is roughly 20 ms),\nand aligned the performances to the scores. The results\nare shown in Table 4. On this dataset, the proposed\ntransposition-invariant features consistently outperform\nthe well-established Chroma features, which are based\nonabsolute pitches . To summarise, so far the proposed\nfeatures show state-of-the-art performance on the stan-\ndard alignment task, while additionally being able to align\ntransposed sequences to each other with no additional er-\nror.\n4.4 Experiment 4: Robustness to Tempo Variations\nNext, we have a closer look at the inﬂuence of different\ntempi on our features. As they are based on a ﬁxed lo-\ncal context (a ﬁxed number of frames), the tempo plays an\nimportant role in their computation. For example, if the\ntempo doubles, this means that musically speaking the lo-\ncal context is twice as large as at the normal tempo and\nadditional notes might be included in this context, which\nwould not be part of the local context in the case of the\n3We also tried the CENS features, which are a variation of chroma fea-\ntures, but as they consistently performed worse than the Chroma features,\nwe are not reporting the results here.\n4Version 0.6, DOI:10.5281/zenodo.1174893Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 595Transposition in Semitones\nDataset Measure -3 -2 -1 0 1 2 3 Rand. Transp.\nCB1stQuartile 10 ms 10 ms 11 ms 10 ms 10 ms 11 ms 10 ms 10 ms\nMedian 22 ms 22 ms 23 ms 22 ms 22 ms 23 ms 22 ms 22 ms\n3rdQuartile 39 ms 39 ms 41 ms 39 ms 40 ms 40 ms 38 ms 39 ms\nError\u001450 ms 84% 83% 81% 83% 82% 83% 84% 83%\nError\u0014250 ms 95% 94% 94% 94% 93% 95% 95% 95%\nCE1stQuartile 10 ms 10 ms 8 ms 10 ms 9 ms 9 ms 10 ms 9 ms\nMedian 21 ms 20 ms 18 ms 21 ms 19 ms 18 ms 20 ms 19 ms\n3rdQuartile 37 ms 32 ms 30 ms 36 ms 33 ms 32 ms 33 ms 32 ms\nError\u001450 ms 83% 90% 91% 87% 89% 88% 90% 90%\nError\u0014250 ms 93% 97% 98% 96% 97% 98% 97% 97%\nMS1stQuartile 6 ms 6 ms 6 ms 6 ms 6 ms 6 ms 6 ms 6 ms\nMedian 13 ms 13 ms 13 ms 13 ms 13 ms 14 ms 13 ms 13 ms\n3rdQuartile 24 ms 24 ms 24 ms 25 ms 25 ms 26 ms 25 ms 25 ms\nError\u001450 ms 91% 91% 92% 90% 91% 90% 90% 91%\nError\u0014250 ms 100% 100% 100% 100% 100% 100% 100% 100%\nRP1stQuartile 17 ms 16 ms 15 ms 14 ms 13 ms 12 ms 15 ms 14 ms\nMedian 45 ms 44 ms 36 ms 34 ms 35 ms 31 ms 34 ms 37 ms\n3rdQuartile 136 ms 151 ms 106 ms 90 ms 130 ms 90 ms 103 ms 122 ms\nError\u001450 ms 53% 53% 60% 63% 59% 64% 60% 58%\nError\u0014250 ms 84% 83% 88% 90% 85% 90% 89% 86%\nTable 3 . Results for 8-gram piano features on transposed versions of the scores (from -3 to +3 semitones). The rightmost\ncolumn gives the results on scores with randomly changing transpositions after every 30 seconds (see Section 4.2).\nDS Measure Chroma 8G Piano\nCB1stQuartile 15 ms 10 ms\nMedian 34 ms 22 ms\n3rdQuartile 80 ms 39 ms\nError\u001450 ms 64% 83%\nError\u0014250 ms 85% 94%\nCE1stQuartile 13 ms 10 ms\nMedian 29 ms 21 ms\n3rdQuartile 56 ms 36 ms\nError\u001450 ms 71% 87%\nError\u0014250 ms 94% 96%\nMS1stQuartile 7 ms 6 ms\nMedian 16 ms 13 ms\n3rdQuartile 31 ms 25 ms\nError\u001450 ms 85% 90%\nError\u0014250 ms 98% 100%\nRP1stQuartile 17 ms 14 ms\nMedian 43 ms 34 ms\n3rdQuartile 113 ms 90 ms\nError\u001450 ms 55% 63%\nError\u0014250 ms 91% 90%\nTable 4 . Comparison of the transposition-invariant 8G\nPiano features to the Chroma features on untransposed\nscores.normal tempo. To test the inﬂuence of tempo differences,\nwe created score representations using different tempi and\naligned the unchanged performances to them. Table 5 sum-\nmarises the results for the Chroma and the 8G Piano fea-\ntures on scores synthesised with the base tempo, as well as\nwith2\n3-times and4\n3-times the base tempo. Unsurprisingly,\ntempo in general inﬂuences the alignment results. How-\never, while the Chroma features are much more robust to\ndifferences in tempo between the sequences to be aligned,\nthe8G Piano features struggle in this experiment. We re-\npeated the experiment with more extreme tempo changes,\nwhich conﬁrmed this trend. While with the Chroma fea-\ntures it is possible to more or less align sequences with\ntempo differences of a factor of three, the transposition-\ninvariant features fail in these cases.\n5. FIRST EXPERIMENTS ON ORCHESTRAL\nMUSIC\nIn addition to the promising results on piano music, we\nalso present ﬁrst experiments on orchestral music. To this\nend, we trained an additional model on recordings of sym-\nphonic music (seven full commercial recordings of sym-\nphonies by Beethoven, Brahms, Bruckner, Berlioz and\nStrauss), which will be referred to in the following as 8G\nOrch . For comparison, we also evaluated the model from\nthe previous section ( 8G Piano ) and the Chroma features\non the evaluation data. The evaluation data consists of two\nrecordings of classical symphonies: the 3rdsymphony by\nBeethoven (B3) and the 4thsymphony by Mahler (M4).596 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Chroma 8G Piano\nDS Measure2\n3Tempo Base Tempo4\n3Tempo2\n3Tempo Base Tempo4\n3Tempo\nCB1stQuartile 19 ms 15 ms 13 ms 24 ms 10 ms 32 ms\nMedian 43 ms 34 ms 29 ms 63 ms 22 ms 120 ms\n3rdQuartile 137 ms 80 ms 66 ms 116 ms 39 ms 205 ms\nError\u001450 ms 54% 64% 67% 47% 83% 33%\nError\u0014250 ms 82% 85% 85% 87% 94% 84%\nCE1stQuartile 14 ms 13 ms 12 ms 27 ms 10 ms 26 ms\nMedian 30 ms 29 ms 25 ms 70 ms 21 ms 83 ms\n3rdQuartile 65 ms 56 ms 53 ms 116 ms 36 ms 176 ms\nError\u001450 ms 69% 71% 73% 40% 87% 38%\nError\u0014250 ms 90% 94% 94% 93% 96% 80%\nMS1stQuartile 8 ms 7 ms 9 ms 7 ms 6 ms 9 ms\nMedian 18 ms 16 ms 20 ms 16 ms 13 ms 21 ms\n3rdQuartile 42 ms 31 ms 49 ms 33 ms 25 ms 52 ms\nError\u001450 ms 79% 85% 75% 84% 90% 74%\nError\u0014250 ms 98% 98% 97% 99% 100% 98%\nRP1stQuartile 18 ms 17 ms 20 ms 22 ms 14 ms 30 ms\nMedian 44 ms 43 ms 58 ms 69 ms 34 ms 86 ms\n3rdQuartile 116 ms 113 ms 141 ms 184 ms 90 ms 202 ms\nError\u001450 ms 53% 55% 56% 43% 63% 37%\nError\u0014250 ms 92% 91% 87% 82% 90% 85%\nTable 5 . Comparison of Chroma and 8G Piano features for alignments to scores in different tempi (see Section 4.4).\nDS Measure Chroma 8G Piano 8G Orch\nB31stQuartile 20 ms 25 ms 22 ms\nMedian 48 ms 54 ms 49 ms\n3rdQuartile 108 ms 104 ms 111 ms\nErr.\u001450 ms 52% 47% 51%\nErr.\u0014250 ms 88% 90% 89%\nM41stQuartile 46 ms 50 ms 57 ms\nMedian 110 ms 129 ms 142 ms\n3rdQuartile 278 ms 477 ms 535 ms\nErr.\u001450 ms 27% 25% 23%\nErr.\u0014250 ms 73% 66% 62%\nTable 6 . Comparison of the transposition-invariant and\nchroma features on orchestral music (see Section 5).\nBoth have been manually annotated at the downbeat level.\nIn this alignment experiment, the Chroma features out-\nperform both the 8G Piano and the 8G Orch features, es-\npecially on the symphony by Mahler (see Table 6). We\nmainly contribute this to the fact that these rather long\nrecordings contain a number of sections with different\ntempi, which is not reﬂected in the score representations.\nAs has been established in Section 4.4, the transposition-\ninvariant features struggle in these cases. Still, we will\nhave to further investigate the use of these features for or-\nchestral music.\nIt is interesting to note that 8G Piano gives slightly bet-\nter results than 8G Orch , even though this dataset solely\nconsists of orchestral music. It turns out that the learnedfeatures are very general and can be readily applied to dif-\nferent instruments. We also tried to overﬁt on the test data,\ni.e., we trained a feature model using the audio ﬁles we\nwould later use for the alignment experiments. Even this\napproach only led to fractionally better results.\n6. CONCLUSIONS\nIn this paper, we reported on audio-to-score alignment ex-\nperiments with novel transposition-invariant features. We\nhave shown that the features are indeed fully invariant to\ntranspositions and in many settings can outperform the\nquasi-standard features for this task, namely chroma-based\nfeatures. On the other hand, we also demonstrated the\nweaknesses of the transposition-invariant features, espe-\ncially their fragility regarding different tempi, which is a\nserious limitation in the context of alignment tasks.\nIn the future, we will study this weakness in depth and\nwill try to alleviate this problem. Ideas include further ex-\nperiments with different n-gram lengths, the adoption of\nalignment schemes including tempo models which itera-\ntively adapt the local tempi of the representations, and to\ntry to include tempo-invariance as an additional goal in the\nlearning process of the features.\n7. ACKNOWLEDGEMENTS\nThis research has received funding from the European Re-\nsearch Council (ERC) under the European Union’s Hori-\nzon 2020 research and innovation programme (ERC grant\nagreement No. 670035, project CON ESPRESSIONE).Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 5978. REFERENCES\n[1] Andreas Arzt. Flexible and Robust Music Tracking .\nPhD thesis, Johannes Kepler University Linz, 2016.\n[2] Andreas Arzt, Sebastian B ¨ock, and Gerhard Widmer.\nFast identiﬁcation of piece and score position via sym-\nbolic ﬁngerprinting. In Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 433–438, Porto, Portugal, 2012.\n[3] Andreas Arzt and Gerhard Widmer. Simple tempo\nmodels for real-time music tracking. In Proceedings of\nthe Sound and Music Computing Conference (SMC) ,\nBarcelona, Spain, 2010.\n[4] Andreas Arzt, Gerhard Widmer, and Simon Dixon.\nAdaptive distance normalization for real-time music\ntracking. In Proceedings of the European Signal Pro-\ncessing Conference (EUSIPCO) , pages 2689–2693,\nBucharest, Romania, 2012.\n[5] Arshia Cont. A coupled duration-focused architecture\nfor real-time music-to-score alignment. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence ,\n32(6):974–987, 2010.\n[6] Simon Dixon and Gerhard Widmer. MATCH: A mu-\nsic alignment tool chest. In Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 492–497, London, UK, 2005.\n[7] Zhiyao Duan and Bryan Pardo. A state space model for\nonline polyphonic audio-score alignment. In Proceed-\nings of the IEEE International Conference on Acous-\ntics, Speech, and Signal Processing (ICASSP) , pages\n197–200, Prague, Czech Republic, 2011.\n[8] Daniel P.W. Ellis and Graham E. Poliner. Identifying\n‘cover songs’ with chroma features and dynamic pro-\ngramming beat tracking. In Proceedings of the IEEE\nInternational Conference on Acoustics, Speech, and\nSignal Processing (ICASSP) , volume 4, pages 1429–\n1432, Honolulu, Hawaii, USA, 2007.\n[9] Valentin Emiya, Roland Badeau, and Bertrand David.\nMultipitch estimation of piano sounds using a new\nprobabilistic spectral smoothness principle. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 18(6):1643–1654, 2010.\n[10] Christian Fremerey, Frank Kurth, Meinard M ¨uller, and\nMichael Clausen. A demonstration of the SyncPlayer\nsystem. In Proceedings of the International Conference\non Music Information Retrieval (ISMIR) , pages 131–\n132, Vienna, Austria, September 2007.\n[11] Werner Goebl. The vienna 4x22 piano corpus, 1999.\nhttp://dx.doi.org/10.21939/4X22 .\n[12] Maarten Grachten, Carlos Eduardo Cancino Chac ´on,\nThassilo Gadermaier, and Gerhard Widmer. Towardscomputer-assisted understanding of expressive dynam-\nics in symphonic music. IEEE Multimedia , 24(1):36–\n46, 2017.\n[13] Maarten Grachten, Martin Gasser, Andreas Arzt, and\nGerhard Widmer. Automatic alignment of music per-\nformances with structural differences. In Proceedings\nof the International Society for Music Information Re-\ntrieval Conference (ISMIR) , pages 607–612, Curitiba,\nBrazil, 2013.\n[14] Ning Hu, Roger B. Dannenberg, and George Tzane-\ntakis. Polyphonic audio matching and alignment for\nmusic retrieval. In Proceedings of the IEEE Workshop\non Applications of Signal Processing to Audio and\nAcoustics (WASPAA) , New Paltz, NY , USA, 2003.\n[15] Cyril Joder, Slim Essid, and Ga ¨el Richard. A compar-\native study of tonal acoustic features for a symbolic\nlevel music-to-score alignment. In Proceedings of the\nIEEE International Conference on Acoustics, Speech,\nand Signal Processing (ICASSP) , Dallas, Texas, USA,\n2010.\n[16] Cyril Joder, Slim Essid, and Ga ¨el Richard. A condi-\ntional random ﬁeld framework for robust and scalable\naudio-to-score matching. IEEE Transactions on Audio,\nSpeech, and Language Processing , 19(8):2385–2397,\n2011.\n[17] Stefan Lattner, Maarten Grachten, and Gerhard Wid-\nmer. Learning transposition-invariant interval features\nfrom symbolic music and audio. In Proceedings of\nthe 19th International Society for Music Information\nRetrieval Conference, ISMIR 2018, Paris, France,\nSeptember 23-27 , 2018.\n[18] Honglak Lee, Chaitanya Ekanadham, and Andrew Y .\nNg. Sparse deep belief net model for visual area V2.\nInProceedings of the Twenty-First Annual Conference\non Neural Information Processing Systems, Vancouver,\nBritish Columbia, Canada, December 3-6, 2007 , pages\n873–880, 2007.\n[19] Kjell Lemstr ¨om and Mika Laitinen. Transposition and\ntime-warp invariant geometric music retrieval algo-\nrithms. 2011 IEEE International Conference on Mul-\ntimedia and Expo , pages 1–6, 2011.\n[20] Akira Maezawa, Katsutoshi Itoyama, Kazuyoshi\nYoshii, and Hiroshi G. Okuno. Bayesian audio align-\nment based on a uniﬁed generative model of music\ncomposition and performance. In Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 233–238, Taipei, Taiwan,\n2014.\n[21] Matija Marolt. A mid-level representation for melody-\nbased retrieval in audio collections. IEEE Trans. Mul-\ntimedia , 10(8):1617–1625, 2008.598 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[22] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW\nEllis, Matt McVicar, Eric Battenberg, and Oriol Nieto.\nlibrosa: Audio and music signal analysis in python. In\nProceedings of the 14th python in science conference ,\npages 18–25, Bucharest, Romania, 2015.\n[23] Roland Memisevic. Gradient-based learning of higher-\norder image features. In IEEE International Confer-\nence on Computer Vision (ICCV), 2011 , pages 1591–\n1598. IEEE, 2011.\n[24] Nicola Montecchio and Arshia Cont. A uniﬁed ap-\nproach to real time audio-to-score and audio-to-audio\nalignment using sequential Montecarlo inference tech-\nniques. In Proceedings of the IEEE International Con-\nference on Acoustics, Speech, and Signal Processing\n(ICASSP) , pages 193–196, Prague, Czech Republic,\n2011.\n[25] Meinard M ¨uller. Fundamentals of Music Processing .\nSpringer Verlag, 2015.\n[26] Meinard M ¨uller and Michael Clausen. Transposition-\ninvariant self-similarity matrices. In Proceedings of\nthe International Conference on Music Information\nRetrieval (ISMIR) , pages 47–50, Vienna, Austria,\nSeptember 2007.\n[27] Eita Nakamura, Tomohiko Nakamura, Yasuyuki Saito,\nNobutaka Ono, and Shigeki Sagayama. Outer-product\nhidden markov model and polyphonic midi score fol-\nlowing. Journal of New Music Research , 43(2):183–\n201, 2014.\n[28] Nicola Orio and Franc ¸ois D ´echelle. Score following\nusing spectral analysis and hidden markov models. In\nProceedings of the International Computer Music Con-\nference (ICMC) , Havana, Cuba, 2001.\n[29] Christopher Raphael. Automatic segmentation of\nacoustic musical signals using hidden Markov models.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence , 21:360–370, 1999.\n[30] Christopher Raphael. A probabilistic expert system for\nautomatic musical accompaniment. Journal of Com-\nputational and Graphical Statistics , 10(3):487–512,\n2001.\n[31] Christopher Raphael. A hybrid graphical model for\naligning polyphonic audio with musical scores. In Pro-\nceedings of the International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , pages 387–394,\nBarcelona, Spain, 2004.\n[32] Stan Salvador and Philip Chan. FastDTW: Toward ac-\ncurate dynamic time warping in linear time and space.\nIntelligent Data Analysis , 11(5):561–580, 2007.\n[33] Sertan S ¸ent ¨urk, Andre Holzapfel, and Xavier Serra.\nLinking scores and audio recordings in makam music\nof turkey. Journal of New Music Research , 43(1):34–\n52, 2014.[34] Sertan S ¸ent ¨urk and Xavier Serra. Composition iden-\ntiﬁcation in Ottoman-Turkish makam music using\ntransposition-invariant partial audio-score alignment.\nInProceedings of 13th Sound and Music Computing\nConference (SMC 2016) , pages 434–441, Hamburg,\nGermany, 2016.\n[35] Verena Thomas. Music Synchronization, Audio Match-\ning, Pattern Detection, and User Interfaces for a Dig-\nital Music Library System . PhD thesis, University of\nBonn, 2013.\n[36] Thomas C. Walters, David A. Ross, and Richard F.\nLyon. The intervalgram: An audio feature for large-\nscale cover-song recognition. In From Sounds to Music\nand Emotions - 9th International Symposium, CMMR\n2012, London, UK, June 19-22, 2012, Revised Selected\nPapers , pages 197–213, 2012.\n[37] Siying Wang, Sebastian Ewert, and Simon Dixon. Ro-\nbust joint alignment of multiple versions of a piece\nof music. In Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 83–88, Taipei, Taiwan, 2014.\n[38] Gerhard Widmer. Discovering simple rules in com-\nplex data: A meta-learning algorithm and some\nsurprising musical discoveries. Artiﬁcial Intelligence ,\n146(2):129–148, 2003.\n[39] Gerhard Widmer, Simon Dixon, Werner Goebl, Elias\nPampalk, and Asmir Tobudic. In search of the\nHorowitz factor. AI Magazine , 24(3):111–130, 2003.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 599"
    },
    {
        "title": "Empirically Weighting the Importance of Decision Factors for Singing Preference.",
        "author": [
            "Michael D. Barone",
            "Karim M. Ibrahim",
            "Chitralekha Gupta",
            "Ye Wang 0007"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492469",
        "url": "https://doi.org/10.5281/zenodo.1492469",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/117_Paper.pdf",
        "abstract": "Although music cognition and music information retrieval have many common areas of research interest, relatively little work utilizes a combination of signal- and humancentric approaches when assessing complex cognitive phenomena. This work explores the importance of four cognitive decision-making factors (familiarity, genre preference, ease of vocal reproducibility, and overall preference) influence in the perception of \"singability\", how attractive a song is to sing. In Experiment One, we develop a model to validate and empirically determine to what degree these factors are important when evaluating its singability. Results indicate that evaluations of how these four factors impact singability strongly correlate with pairwise evaluations (ρ = 0.692, p  0.0001), supporting the notion that singability is a measurable cognitive process. Experiment Two examines the degree to which timbral and rhythmic features contribute to singability. Regression and random forest analysis find that some selected features are more significant than others. We discuss the method we use to empirically assess the complex decisions, and provide a preliminary exploration regarding what acoustic features may motivate these choices.",
        "zenodo_id": 1492469,
        "dblp_key": "conf/ismir/BaroneIGW18",
        "keywords": [
            "music cognition",
            "music information retrieval",
            "signal-centric",
            "humancentric",
            "cognitive decision-making",
            "familiarity",
            "genre preference",
            "ease of vocal reproducibility",
            "overall preference",
            "singability"
        ],
        "content": "EMPIRICALLY WEIGHING THE IMPORTANCE OF DECISION FACTORS\nWHEN SELECTING MUSIC TO SING\nMichael Mustaine1Karim M. Ibrahim1\nChitralekha Gupta1;2Ye Wang1\n1School of Computing, National University of Singapore, Singapore\n2NUS Graduate School for Integrative Sciences and Engineering,\nNational University of Singapore, Singapore\nbaronemda@gmail.com, wangye@comp.nus.edu.sg\nABSTRACT\nAlthough music cognition and music information retrieval\nhave many common areas of research interest, relatively\nlittle work utilizes a combination of signal- and human-\ncentric approaches when assessing complex cognitive phe-\nnomena. This work explores the importance of four cogni-\ntive decision-making factors (familiarity, genre preference,\nease of vocal reproducibility, and overall preference) in-\nﬂuence in the perception of “singability”, how attractive a\nsong is to sing. In Experiment One, we develop a model\nto validate and empirically determine to what degree these\nfactors are important when evaluating its singability. Re-\nsults indicate that evaluations of how these four factors\nimpact singability strongly correlate with pairwise evalu-\nations (\u001a= 0:692;p< 0:0001 ), supporting the notion that\nsingability is a measurable cognitive process. Experiment\nTwo examines the degree to which timbral and rhythmic\nfeatures contribute to singability. Regression and random\nforest analysis ﬁnd that some selected features are more\nsigniﬁcant than others. We discuss the method we use to\nempirically assess the complex decisions, and provide a\npreliminary exploration regarding what acoustic features\nmay motivate these choices.\n1. INTRODUCTION\nA fundamental task of MIR is to develop of acoustic fea-\nture extractors that capture unique characteristics from a\nrecorded piece of sound. However, some acoustic fea-\ntures may not be wholly represented in the acoustic sig-\nnal, and MIR has been criticized for failing to model anal-\nysis based on psychological research [3]. For example,\n“danceability” - the perceptual experience of grooviness\n[23, 48] - is a feature available in signal processing pack-\nc\rMichael Mustaine, Karim M. Ibrahim, Chitralekha\nGupta, Ye Wang. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: Michael Mustaine,\nKarim M. Ibrahim, Chitralekha Gupta, Ye Wang. “Empirically Weighing\nthe Importance of Decision Factors when Selecting Music to Sing”, 19th\nInternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.ages [9, 10], and open-access APIs1using a combination\nof beat salience and consistency [36]. However, based so-\nley on these acoustic properties the most danceable song\nwould be closer to a steady, metronomic pulse, which\nclearly does not capture the perceptual nuances of what\nmakes music danceable [14]. The inclusion of psycholog-\nical acoustic features using signal-only analysis is surpris-\ning, given that music is a dynamic system inﬂuenced by\ncognitive [20], cultural, market, and political forces [8].\nDespite this knowledge, research is relatively sparse as to\nhow, or to what degree, speciﬁc acoustic features inﬂuence\nmusical preference. Part of the scarcity may be due to the\nrelative difﬁculty in quantifying the inﬂuence of important\npsychological features empirically. This work examines\nthe extent to which a cognitive psychology, signal process-\ning, machine learning, and economic decision-making can\nbe used to investigate a previously unexplored psychologi-\ncal perception of “singability”: the degree to which a song\nis attractive to sing. To our knowledge, no empirical study\nhas been conducted which explores whether a feature such\nas singability can be extracted from a piece of music.\nDetermining a complex psychological process and de-\ncision making strategy like singability is a difﬁcult task.\nTo start, it is intuitively difﬁcult to quantify such a sub-\njective multiple criterion choice in a controlled, scientiﬁc\nmanner. Because singability will likely not contain a uni-\nversally agreed upon set of factors, the major challenge is\ndeﬁning a method that can quantify how - and to what de-\ngree - these factors should be incorporated into a model for\nevaluation. We ﬁrst introduce some background on closely\nrelated concepts to our interpretation of singability from\npsychological experiments and MIR applications.\n1.1 Related Work\nPerhaps the most historically relevant psychological re-\nsearch relating to singing preference was initially proposed\nby Berlyne [7]. Berlyne suggests that music exhibits an\ninverted-U-shaped relationship for preference, inﬂuenced\nby novelty, complexity, and tone. This model has been\nreplicated independently from a variety of perspectives in-\ncluding personality and preference research [29], and ﬂow\n1https://developer.spotify.com/web-api/\nget-audio-features/529states [12].\nPrior research on singability has focused on music\nrecommendation systems for digital karaoke applications\n[25, 26]; they used a competence-based evaluation, and\nrecommended music using an individual’s singing proﬁ-\nciency. These systems deﬁne singing preference solely on\nwhether one can recreate the original performance [16] and\nfails to consider other aspects of preference such as fa-\nmiliarity on preference; if the ability to recreate an orig-\ninal version of a song is the sole criteria for determining\nsingable tracks, a na ¨ıve extension to improve performance\nwould be to recommend songs based on demographic fea-\ntures such as age, sex, and height through automated as-\nsessment of singing voice [46].\n2. SINGABILITY FACTORS\nWe examine singability using a synthesis of multiple cri-\nterion decision making processes, acoustic feature extrac-\ntion, and machine learning founded on a theoretical back-\nground of music cognition. Based on the general research\ndiscussed above, we expand the interpretation of singabil-\nity from other research [25, 26] to include more factors\nthan just the ability to reproduce the original rendition of\na track. For the purpose of this work, singability is de-\nﬁned as a psychological process which includes how at-\ntractive a song is to sing without concern of social conster-\nnation for being unable to produce the original vocaliza-\ntions. Based on this reﬁned deﬁnition, we consider four\nfactors which could impact singability and include: i) fa-\nmiliarity, ii) genre, iii) preference to listen (listenability),\nand iv) producibility.\nTo maintain a realistic scope for exploratory research,\nwe did not include an exhaustive list of potential singabil-\nity factors. These factors were selected due to their rela-\ntive presence in the psychological literature. We also were\ninterested in selecting features that would be less demand-\ning to ask crowdsourced workers; other features we did\nnot explore, such as the importance of lyrics or social fac-\ntors, could be analysed using methodology speciﬁc to their\ndisciplines should compelling evidence for singabiliity be\nfound. Next, we highlight research speciﬁc to these fac-\ntors, then describe a method to quantify the prioritization\nof them when making a complex, multiple criterion deci-\nsion.\n2.1 Familiarity\nFamiliarity has important inﬂuences on preference forma-\ntion. The mere exposure effect, a foundational psycho-\nlogical process [28, 49], demonstrates that increased expo-\nsure to essentially anything increases your preference for\nit, even when unaware of it’s inclusion in your immediate\nenvironment [24]. In [32], the mere exposure effect was\nalso found to impact music preference; multiple repetitions\nof unfamiliar music [28], and random tone sequences [47]\nincreased preferences for them. A possible reason for why\nfamiliarity increases preference is because it improves ease\nof processing [30], impacting the complexity componentof Berlyne’s optimal complexity model described in Sec-\ntion 1.1.The relationship between familiarity through mere\nexposure appears to occur early in cognitive processing -\nKorsakoff amnesics demonstrate increased liking to musi-\ncal stimuli through increased exposure [19].\nHowever, it is important to consider that increased fa-\nmiliarity does not increase preference in all cases; most\npeople do not actively listen to extremely familiar songs\nsuch as Twinkle, Twinkle, Little Star. This still makes\nsense when considering Berlyne’s optimal complexity\nmodel (Section 1.1) - extremely familiar music is too sim-\nple or not novel enough to engage. Therefore, it is hypoth-\nesized that although familiar music is important for singa-\nbility, music that is too familiar will not be preferred.\n2.2 Genre\nGenre preference describes a speciﬁc aspect of the mere\nexposure effect through common acoustic features which\nare hallmark in the genres you typically listen to. For\nexample, Rap music has a high degree of speech, and\nMetal music generally is high tempo, and with negative\nvalence [4]. This form of familiarity is more active and\npersonal, aligning more closely to the role that individ-\nual preference plays in exposure. Neurological evidence\nfor an active mere exposure effect through genre has been\ndemonstrated in brain imaging studies. Using electroen-\ncephalography, Mismatch Negativity Responses (MMNs;\na spike in brainwave polarization when expectations are vi-\nolated) can be elicited with tone sequences in the ﬁrst few\ntrials regardless of formal musical training [38]. In a sub-\nsequent study, authors of [39] found that MMN responses,\nwere stronger when genre conventions were deﬁed in a\nparticipants preferred musical style. In a study contain-\ning 17 million users from over 30 countries, users down-\nload tracks of secondary genres acoustic features similar\nto those of their most preferred genre [4]. For example,\nusers who had clear preferences for Rap music preferen-\ntially downloaded tracks from other genres that contained\nmore speech sounds. We therefore hypothesize that genre\nplays an important role in the selection of a preferred song\nto sing.\n2.3 Listenability\nThe deﬁnition of listenability used for this work refers to\nhow attractive a song is to listen to. Although it may be ap-\npealing to suggest that songs that are listenable are by ex-\ntension singable, they must be considered mutually exclu-\nsive. Rap or Metal music for example may ﬁt this category\nas the vocalizations required are not conducive for singing,\nbut are still highly popular and can be very listenable. Fur-\nthermore, listenability is distinct from familiarity, but can\nbe inﬂuenced by it. As suggested in Section 1.1, nursery\nrhymes are highly familiar, but are likely not considered\nhighly listenable or singable by most. Highly listenable\nsongs may also not be familiar because older tracks are\nplayed signiﬁcantly less than newly released songs. Lis-\ntenability may be best differentiated from familiarity in530 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018that it can be an immediate process, requiring only a sin-\ngle exposure in order to be evaluated as attractive. Cogni-\ntive processing of various complex musical features such\nas genre [21] can happen at millisecond timescales. Lis-\ntenability is considered an important factor for singablility\nbecause it increases the likelihood that a song will be se-\nlected to or attended by users (thus directly inﬂuence the\nlikelihood it will be sung in the ﬁrst place) and because\nthey are more salient in memory.\n2.4 Producibility\nMusic cognition research has examined the distinction of\nsinging quality; the perceptual or acoustic features that\nmake trained singers sound better than amateurs. Qual-\nity of singing voice has been assessed with respect to\nfull upper resonance in a singer’s formant range (known\nas the singer’s formant, a prominent spectral envelope of\n3kHz) as of singing voice quality [5]. Professional singers\nhave higher formant intensity than untrained voices; rela-\ntive amplitudes of singer’s formants grew as vocal inten-\nsity increased and diminished as pitch rose [35], trained\nvoices have more energy in the formant range but not for\nall pitches, and males in general have higher formant in-\ntensity than females [35]. The singer’s formant appears to\nbe a particularly important property for classical operatic\nsingers to project above the orchestra [37].\nAlthough measures regarding whether an individual has\nvocal training can be assessed through the singer’s for-\nmant, producibility is not contingent on these features. For\nexample, untrained singers with self-expressed singing tal-\nent have identical pitch matching accuracies when com-\npared to trained singers [45]. Producibility based on vo-\ncal features which indicate professional training may also\nnot be appropriate because the correlation between genre\npreference and training does not align with what is pop-\nularly sung; individuals with more musical training show\nincreased preferences for “serious” genres such as Classi-\ncal and Jazz, but not other genres such as Pop [17].\n3. EXPERIMENT ONE: VALIDATING\nSINGABILITY\nTo our knowledge, there is no prior work that examines\nwhether what people think makes a song singable corre-\nlates with what they actually select in natural settings. For\nexample, [41] instructed professional musicians to evalu-\nate recordings of top-three placing performances from pi-\nano competitions under three conditions, recordings with:\ni) video only, ii) audio only, or iii) audio and video. Par-\nticipants accurately ranked the video-only condition more\nconsistently with who won the competition than in any\nother condition; the audio-only condition was the least\nconsistent. This work establishes that it is possible that our\nimpressions of what features are important in our musical\npreferences may not be internally consistent.\nWe combine a series of psychological analysis meth-\nods to establish whether singability can be consistently as-\nsessed among individuals using a set of 50 popular songexcerpts. To establish a bottom-up ground truth, a forced\nalternative choice (FAC) experiment is conducted with\npairs of songs; a complex decision-making model known\nas Analytic Hierarchical Process (AHP) [33] is used to de-\ntermine top-down impressions. We then rank songs based\non their assessed singability using both methods (FAC and\nAHP) to determine whether there is consistency between\nwhat we think is singable, and what our decisions end up\ninevitably being. An additional beneﬁt of using AHP is\nthat it can weigh the degree to which each of the four fea-\ntures described above contributes to an individuals choice\nto sing a song. Because AHP is less commonly used, we\nbrieﬂy describe AHP and how it is conducted prior to re-\nporting experimental structure.\n3.1 Analytic Hierarchical Process\nAHP is an technique to quantify how, and to what degree,\nsubjective criteria inﬂuence a complex decision making\ntask. The validity of the AHP has been examined exten-\nsively [44], and has been used within government, busi-\nness, and healthcare [42]. Figure 1 illustrates the ﬁnal im-\nportance values for each factor and are now described. De-\ntermining singability using AHP involves breaking down\nthe decision problem into a set of global priorities (green\nboxes). Global priorities are a set of general factors that are\nsuspected to inﬂuence the decision-making process. After\nglobal priorities are determined, levels within each priority\n(local priorities; blue boxes) are established. Once prior-\nities have been established, the importance of each factor\ncan be systematically evaluated to determine their contri-\nbution to the ﬁnal decision. Decision makers weigh the im-\nportance of each of these priorities using multiple pairwise\ncomparisons, and require the decision maker to evaluate\nevery priority relative to another. For instance, a worker\nis asked ”how important was it that the vocals were easy\nto reproduce, as opposed to moderately difﬁcult”. Because\nmore than one worker answered the same question multi-\nple times, we take the average importance value from all\ncomparisons as the ﬁnal importance value. Priorities are\ncalculated by dividing the importance of the ﬁrst compari-\nson over the other. A pairwise comparison matrix is gener-\nated after all evaluations are made by multiplying the en-\ntries of each row and taking the nth root of the product.\nThe roots are then summed and normalized to produce an\neigenvector representing the priority importance.2\n3.2 Methods\nThe dataset contains excerpts of 50 songs (ten songs from\nﬁve genres) from the top 50 Billboard chart songs between\nthe years of 2011-2015. Selected songs had equal num-\nbers of male and female singers (ﬁve per sex per genre).\nIn order to reduce high degrees of familiarity, songs from\nthe bottom of the list were selected. 15-seconds of audio\nwas extracted from each artist’s ofﬁcial YouTube channel.\nAudio was extracted from the video as mp3 ﬁles.\n2For in-depth example, see: http://rad.ihu.edu.gr/\nfileadmin/labsfiles/decision_support_systems/\nlessons/ahp/AHP_Lesson_1.pdfProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 531A two-part online survey was crowdsourced using\nAmazon’s Mechanical Turk.3Although some research\nsuggests that the quality of crowdsourced data is more di-\nverse and at times better than data collected in traditional\nlaboratory settings [6], additional metrics which validate\nor reﬁne analysis highlighted in Section 1.1 should be con-\nsidered. The ﬁrst part of the experiment consisted of a se-\nries of FACs. Workers were instructed to listen to excerpts\nof two songs. They were asked to determine which song\nwas more singable, listenable, and whether either of the\nsongs were familiar. Workers repeated this paradigm for\nﬁve pairs of songs in total.\nAfter completing the FAC section, workers were then\ninstructed to complete an AHP after brieﬂy reﬂecting on\nthe choices they made when selected between pairs of\nsongs. Different levels of local priorities are established\nfor each global priority. Five levels for genre were selected,\nRock, Pop, Alternative, Country, and Rap music were se-\nlected; two levels for familiarity (low and high); three lev-\nels for producibility (easy, medium, hard) and; three lev-\nels for preference to listen (low, medium, high). In order\nto keep the task as simple as possible for workers, we re-\nduced the number of local priorities to as little as possi-\nble - unlike producibility and listenability, only two levels\nwere selected for familiarity because we wanted to know\nwhether any prior knowledge of a piece would inﬂuence\ntheir choice.\nImportance values were calculated by taking the av-\nerage response for each priority across all respondents.\nLastly, we requested workers to report only their sex - we\ndid not collect information regarding worker age, socioe-\nconomic status, or ethnicity. The reasoning for this was\ntwo-fold: i) Mechanical Turk demographic variability is in\ngeneral more diverse than traditional laboratory data col-\nlection [6], and; ii) we were interested in establishing the\ngeneral existence of a psychological perception from a rar-\niﬁed set of possible inﬂuencers before examining how dy-\nnamic anthropological and sociological factors modulate\nthe preference. A beneﬁt of using AHP is that it is a sim-\nple process to add or remove global priorities and replicate\nthe experiment easily with new variables and interactions.\n3.3 Analysis\nPairwise comparisons were conducted for all 50 songs\n(1225 pairs), each job instructed users to evaluate 5 pairs\n(245 jobs), and each job was assessed 3 times (735 surveys\nconducted). 88 submissions (11%) were rejected for incor-\nrectly answering a conﬁrmatory test question.4A worker\nwas compensated $0.07 USD per job and could perform up\nto ﬁve surveys. 245 unique respondents (44% male, 56%\nfemale) completed the survey. On average, workers agreed\nwith each other that one song was more singable than an-\nother 77.8% of the time. We examined whether individuals\nselected a song as more singable based on the sex of the\n3https://www.mturk.com/\n4Workers were instructed to select whether an excerpt from Michael\nJackson’s Billie Jean was more familiar than an unreleased composition\nfrom one of the authorsartist. A binomial test indicated that individuals selected\nsame-sex singers slightly more often ( 53%;p < 0:001),\nthough the difference was marginal.\nOnce AHP priorities were calculated, songs were seg-\nmented into bins for the familiarity (high and low), and\nlistenability (high, medium, and low) categories based on\nthe survey responses. Figure 1 represents the global and lo-\ncal priority values generated through the Mechanical Turk\nsurvey. Song rankings for AHP were derived for each\nsong by producing a rank-order based off the product of\nlocal priority values for genre, listenability, and famil-\niarity. For example, a Rock song which was in the top\n50th percentile for familiarity, and the bottom 33rd per-\ncentile for listenability would receive a singability value of\n0:227\u00030:613\u00030:299 = 0:0416 . Producibility was not in-\ncluded in the calculation because this feature is relative to\nan individual’s skill at singing and can only be evaluated\nfor each user, as opposed to each song. Ranks for the FAC\nportion of the experiment were generated by ordering the\namount of times any given song within a pairwise compar-\nison was selected by the user as more attractive to sing.\nOnce ranks were generated for the bottom-up (FAC),\nand top-down (AHP) processes, we conducted a\nSpearman-\u001arank correlation. Ranks derived from\nFAC are highly correlated with ranks derived from the\nAHP (rs= 0:691;p < 0:0001 ). 47.61% of the variance\nin rank could be accounted for across ranked derived from\nFAC and AHP. Figure 2 plots the ranks derived for each\nsong excerpt. Each song’s coordinates represent the FAC\nderived rank (x-axis) to the AHP derived rank (y-axis).\nSigniﬁcant Spearman- \u001acorrelations were also found com-\nparing Billboard ranks to FAC ( rs= 0:518;p < 0:001)\nand AHP (rs= 0:540;p< 0:0001 ).\n3.4 Discussion\nThe purpose of experiment one was to derive a method\nthat can determine whether people’s heuristic impressions\nof preference reliably predicts their actual decisions. The\nhighly signiﬁcant correlation ( p<0:0001 ) and large effect\nsize (r2= 0:4761 ), supports the hypothesis that people’s\ntop-down assessments of singability are features they actu-\nally use when making the decision. This ﬁnding is signiﬁ-\ncant because it supports the notion that a less labour inten-\nsive process is needed for determining a music-cognitive\nprocess; you do not need to conduct a bottom-up com-\nparison for the entire corpus of music to determine gen-\neral preference. The results suggest that listenability is the\nmost important feature follwed by: familiarity, genre, and\nproducibility. The importance values for most local priori-\nties are generally intuitive; easily produced, familiar music\nwe like to listen to are important factors we use when de-\nciding to sing something. Rock was the most important\ngenre (22.7% importance), followed by Pop (21.3%), Al-\nternative (19.6%), Country (19.3%), and Rap (16.8%). The\nsigniﬁcant binomial correlation also indicates that user de-\nmographic information such as sex should be considered\nwhen recommending music to sing. Although the pref-\nerence for same-sex singers (3%) does not account for a532 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Singability\nGenre (0.236) Producibility (0.204) Listenability (0.286) Familiarity (0.271)\nRock (0.227)\nPop (0.213)\nCountry (0.193)\nAlternative (0.196)\nRap (0.168)Difﬁcult (0.303)\nAverage (0.333)\nEasy (0.363)Very (0.361)\nAverage (0.338)\nNot very (0.299)Very (0.613)\nNot very (0.386)\nFigure 1 . Analytic Hierarchy Process for Singability derived from Mechanical Turk experiment. The most important global\npriority was familiarity, followed by preference to listen, genre, and producibility.\nFigure 2 . FAC-to-AHP Rank Scatterplot. X-axis rep-\nresents ranks derived from the AHP analysis for a given\ntrack. Y-axis represents ranks deried from FAC analy-\nsis. Linear regression line for this data is plotted ( ^y=\n0:6922x+ 7:8490 )\nhigh degree of difference, recommendation systems based\non human behaviours are relatively rare and can improve\nuser satisfaction in generally unexplored ways.\nA downside of this current investigation is that the vari-\nation of importance of global and local priorities was quite\nlow, ranging between 2-3% across most factors. A more\npronounced effect may be achievable using a more con-\ntrolled, laboratory recruited participant pool. Producibility\nwas also not a factor used to generate AHP ranking. The\nrationale for this is that there is no clear or simple way to\nevaluate vocalization difﬁculty of an excerpt relative to an\nMTurk worker’s actual skill, whereas measures of familiar-\nity and preference have a high degree of comorbidity with\nqualitative assessments [49].\nAn important component missing from this analysis is\ndetermining whether speciﬁc acoustic features inﬂuence\nranking in meaningful way; is a song that is more singable\none that generally has more pronounced vocals, or a faster\ntempo? Experiment two is a preliminary exploration into\nassessing whether some acoustic features are more impor-\ntant than others for determining singability based on the\nranks generated through the AHP.4. EXPERIMENT TWO: FEATURE IMPORTANCE\nEXPLORATION\nAfter establishing that singability is a measurable cogni-\ntive process, the natural next step is analysis of acoustic\nfeatures. Evaluating the importance of acoustic features re-\nlated to singability may enable us to establish whether, or\nwhich, speciﬁc auditory signals contribute to this complex\ndecision-making task. Experiment two provides prelimi-\nnary, exploratory analysis into the importance of a speciﬁc\nset of acoustic features when evaluating singability.\n4.1 Methods\nSimilar to [43], we extract perceptually-relevant features\nfor singability under two categories: timbral and rhythmic.\nSignal processing is conducted using a combination of Li-\nbRosa [27], MIRToolbox [22], and vocal analysis work\nin [18]. 24 features (4 rhythmic and 20 timbral) in total\nwere assessed. An averaged value for each feature was ex-\ntracted for each song every 15-seconds. Timbral features\ninclude: V ocal-to-Accompaniment Ratio (V AR) [40], High\nFrequency Energy (HFE) [11], Mel-band Frequency Cep-\nstral Coefﬁcients (MFCC) 1-5 [13], spectral centroid mean\nand deviation [34], spectral roll off mean and deviation,\nand root mean squared (RMS) of energy mean and devia-\ntion [31]; rhythmic features include: tempo, zero-crossing\nmean and deviation [15], event density [2], and syllabic\nrate [18]. These features were selected for exploratory pur-\nposes due to their ubiquity in signal processing toolkits and\nMIR research.\n4.2 Analysis\nTo determine whether speciﬁc features are more common\nin singable songs, we ﬁrst conduct multiple linear regres-\nsion comparing the AHP generated numeric values to the\n24 extracted acoustic features. The multiple-comparisons\nF-Test was marginally signiﬁcant ( F(23;36) = 1:779;p=\n0:05915;r2= 0:2328 ), independent regressions yielded\nsigniﬁcant two features (Deviation of RMS and MFCC 5)\nand six marginally signiﬁcant features (Deviations of spec-\ntral roll off, MFCCs 1 and 3, and means of RMS, spectralProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 533Feature t-value p-value\nDeviation of MFCC 5 -2.919 0.00604**\nDeviation of RMS -2.539 0.01558*\nDeviation MFCC 1 1.994 0.05372 \u0001\nDeviation spectral roll off 1.934 0.06099 \u0001\nMean of zero crossings -1.864 0.0702 \u0001\nMean spectral centroid 1.738 0.09072 \u0001\nDeviation MFCC 3 1.713 0.09525 \u0001\nMean of RMS 1.703 0.09713 \u0001\nTable 1 . Individual Linear Regression Signiﬁcance Ta-\nble. Multiple comparisons F-Test was marginally signif-\nicant (F(23;36) = 1:779;p= 0:05915;r2= 0:2328 ).\n\u0001p<0:1;\u0003 \u0003p<0:01;\u0003p<0:05\nFigure 3 . Random forest with regression model. Z-scores\nrepresent the relative importance of a feature in the deter-\nmination of AHP-generated values for songs. Colours rep-\nresent signiﬁcance values: signiﬁcant (green), marginally\nsigniﬁcant (yellow), not signiﬁcant (red), and anchor val-\nues (blue).\ncentroid and zero-crossing). Table 1 provides a summary\nof analysis for all marginally signiﬁcant features.\nA statistical disadvantage of relying on standard lin-\near regression analysis only is that multiple comparisons\nincreasingly introduces type-I error with each added fea-\nture. We employ a random forest for regression and com-\npare signiﬁcant features across both models. An added\nbeneﬁt of using random forest is that it can assess the\nrelative importance of each feature in the evaluation of\nsingability. Three features signiﬁcantly inﬂuenced AHP-\ngenerated singability scores (Mean of spectral centroid and\nMFCC 4, and deviation of MFCC 2), and six marginally in-\nﬂuenced AHP-generated singability scores (Syllabic rate,\nV AR, HFR, mean of RMS, and deviation of MFCC). Fig-\nure 3 presents the relative importance of each feature (x-\naxis) as a Z-score (y-axis). Features that were at least\nmarginally signiﬁcant across both models included: mean\nof spectral centroid and RMS, and deviation of MFCC 1.\nFeatures that were at least marginally signiﬁcant in the ran-\ndom forest model that were not signiﬁcant using indepen-\ndent linear regressions included: mean of MFCC 4, spec-\ntral roll off, V AR, HFE, and syllabic rate.4.3 Discussion\nBoth sets of analyses suggest that acoustic features may\ninﬂuence perceptions of singability. However, the mod-\nels disagree on which features are maximally important\nin this decision. The three signiﬁcant features that were\nshared across models (mean of RMS, spectral centroid, and\ndeviation of MFCC 1) suggest that more singable songs\nare in general louder, brighter, and timbral ﬂuctuations in\nhigh frequency energy may be particularly important when\nselecting music to sing to. Features where there was a\ndisagreement in singability across models include zero-\ncrossings, spectral roll off, V AR, HFE, and syllabic rate.\nThis suggests that types of percussive sounds, pronounced\nvocals, and higher than average frequency in vocaliza-\ntions and syllabic rate, may also contribute to evaluations\nof singability. The marginal signiﬁcance of the multiple-\ncomparisons F-test indicate that acoustic features may in-\nﬂuence judgements of singability, however additional anal-\nysis needs to be conducted in order to demonstrate the va-\nlidity of this assertion (see Section 5). Future work should\ninvestigate whether less common features, such as chorus-\nness [1], are more relevant to singability.\nCompared to Experiment One, the results from Exper-\niment Two are less intrepretable. It may be that the our\ncorpus size, or that extracting high-level acoustic features\nfrom 15-second excerpts is insufﬁcient sampling for this\nkind of analysis.\n5. CONCLUSIONS\nThe methods utilized in both experiments may be useful\nfor others in the reﬁnement of psychologically-based mu-\nsic features such as danceability, or enable the exploration\nof other previously unexamined features.\nExperiment One establishes a method for measuring\ncomplex cognitive decision making processes like singa-\nbility in an operationalized manner. A major limitation\nof this operationalization is that it did not consider social\nand contextual features inﬂuencing singing preference. As\ndescribed in Section 3.2, a beneﬁt of using AHP is that\nincluding or removing global priorities is simple; future\nwork should consider the role that other factors (such as\nsocial context and song lyrics) may play in the evaluation\nof singability.\nExperiment Two provides a preliminary exploration of\nthe extent acoustic features inﬂuence singability scores\ngenerated in experiment one. Two statistical models, one\nsimple and the other more complex, were used to deter-\nmine what features may be contributing most to the evalu-\nation of singability. Signiﬁcant features in common across\nthe two models suggests that further signal analysis will be\nimportant future work.\nThis exploratory work does not deﬁnitively establish\nsingability as a core feature of the music. Rather we sug-\ngest that it provides compelling evidence to support a per-\nceptual process of singability, and a reﬁnable methodology\nto explore or support other properties involving cognition.534 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20186. ACKNOWLEDGEMENTS\nThis project is funded by Smule Inc. We would also like\nto thank our reviewers for their insightful comments and\nfeedback.\n7. REFERENCES\n[1] Hooked: a Game for Discovering what Makes Music\nCatchy. (Proceedings of the 12th International Society\nfor Music Information Retrieval Conference):245–250,\n2013.\n[2] Samer A Abdallah and Mark D Plumbley. Probability\nas metadata: event detection in music using ica as a\nconditional density model. In Proc. 4th Int. Symp. In-\ndependent Component Analysis and Signal Separation\n(ICA2003) , pages 233–238. Citeseer, 2003.\n[3] Jean-Julien Aucouturier and Emmanuel Bigand. Seven\nproblems that keep mir from attracting the interest of\ncognition and neuroscience. Journal of Intelligent In-\nformation Systems , 41(3):483–497, 2013.\n[4] Michael D Barone, Jotthi Bansal, and Matthew H\nWoolhouse. Acoustic features inﬂuence musical\nchoices across multiple genres. Frontiers in psychol-\nogy, 8:931, 2017.\n[5] Wilmer T Bartholomew. A physical deﬁnition of good\nvoice-quality in the male voice. the Journal of the\nAcoustical Society of America , 5(3):224–224, 1934.\n[6] Tara S Behrend, David J Sharek, Adam W Meade, and\nEric N Wiebe. The viability of crowdsourcing for sur-\nvey research. Behavior research methods , 43(3):800,\n2011.\n[7] Daniel E Berlyne. Novelty, complexity, and hedo-\nnic value. Attention, Perception, & Psychophysics ,\n8(5):279–286, 1970.\n[8] Denise D Bielby and C Lee Harrington. Managing\nculture matters: Genre, aesthetic elements, and the\ninternational market for exported television. Poetics ,\n32(1):73–98, 2004.\n[9] Dmitry Bogdanov, Joan Serra, Nicolas Wack, and Per-\nfecto Herrera. From low-level to high-level: Compara-\ntive study of music similarity measures. In Multimedia,\n2009. ISM’09. 11th IEEE International Symposium on ,\npages 453–458. IEEE, 2009.\n[10] Dmitry Bogdanov, Nicolas Wack, Emilia G ´omez,\nSankalp Gulati, Perfecto Herrera, Oscar Mayor, Ger-\nard Roma, Justin Salamon, Jos ´e R Zapata, and Xavier\nSerra. Essentia: An audio analysis library for music in-\nformation retrieval. pages 493–498. Citeseer, 2013.\n[11] Lauren B Collister and David Huron. Comparison of\nword intelligibility in spoken and sung phrases. Empir-\nical Musicology Review , 3(3):109–122, 2008.[12] Mihaly Csikszentmihalyi. Flow and the psychology of\ndiscovery and invention . New York: Harper Collins,\n1996.\n[13] Jeremiah D Deng, Christian Simmermacher, and\nStephen Craneﬁeld. A study on feature analysis for mu-\nsical instrument classiﬁcation. IEEE Transactions on\nSystems, Man, and Cybernetics, Part B (Cybernetics) ,\n38(2):429–438, 2008.\n[14] Anders Friberg, Erwin Schoonderwaldt, Anton Hed-\nblad, Marco Fabiani, and Anders Elowsson. Using per-\nceptually deﬁned music features in music information\nretrieval. arXiv preprint arXiv:1403.7923 , 2014.\n[15] Fabien Gouyon, Franc ¸ois Pachet, Olivier Delerue, et al.\nOn the use of zero-crossing rate for an application\nof classiﬁcation of percussive sounds. In Proceedings\nof the COST G-6 conference on Digital Audio Effects\n(DAFX-00), Verona, Italy , 2000.\n[16] Chu Guan, Yanjie Fu, Xinjiang Lu, Enhong Chen, Xi-\naolin Li, and Hui Xiong. Efﬁcient karaoke song recom-\nmendation via multiple kernel learning approximation.\nNeurocomputing , 2017.\n[17] David J Hargreaves, Chris Comber, and Ann Colley.\nEffects of age, gender, and training on musical prefer-\nences of british secondary school students. Journal of\nResearch in Music Education , 43(3):242–250, 1995.\n[18] K.M. Ibrahim, D. Grunberg, K. Agres, C. Gupta, and\nY . Wang. Intelligibility of sung lyrics: A pilot study.\nSuzhou, China, 2017.\n[19] Marcia K Johnson, Jung K Kim, and Gail Risse. Do\nalcoholic korsakoff’s syndrome patients acquire affec-\ntive reactions? Journal of Experimental Psychology:\nLearning, Memory, and Cognition , 11(1):22, 1985.\n[20] Stefan Koelsch and Walter A Siebel. Towards a neural\nbasis of music perception. Trends in cognitive sciences ,\n9(12):578–584, 2005.\n[21] Carol L Krumhansl. Plink:” thin slices” of mu-\nsic.Music Perception: An Interdisciplinary Journal ,\n27(5):337–354, 2010.\n[22] Olivier Lartillot, Petri Toiviainen, and Tuomas Eerola.\nA matlab toolbox for music information retrieval. Data\nanalysis, machine learning and applications , pages\n261–268, 2008.\n[23] Guy Madison. Experiencing groove induced by music:\nconsistency and phenomenology. Music Perception:\nAn Interdisciplinary Journal , 24(2):201–208, 2006.\n[24] George Mandler, Yoshio Nakamura, and Billie J\nVan Zandt. Nonspeciﬁc effects of exposure on stim-\nuli that cannot be recognized. Journal of Experimen-\ntal Psychology: Learning, Memory, and Cognition ,\n13(4):646, 1987.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 535[25] Kuang Mao, Ju Fan, Lidan Shou, Gang Chen, and\nMohan Kankanhalli. Song recommendation for social\nsinging community. In Proceedings of the 22nd ACM\ninternational conference on Multimedia , pages 127–\n136. ACM, 2014.\n[26] Kuang Mao, Lidan Shou, Ju Fan, Gang Chen, and\nMohan S Kankanhalli. Competence-based song recom-\nmendation: Matching songs to ones singing skill. IEEE\nTransactions on Multimedia , 17(3):396–408, 2015.\n[27] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW\nEllis, Matt McVicar, Eric Battenberg, and Oriol Nieto.\nlibrosa: Audio and music signal analysis in python. In\nProceedings of the 14th python in science conference ,\npages 18–25, 2015.\n[28] Max Meyer. Experimental studies in the psychol-\nogy of music. The American Journal of Psychology ,\n14(3/4):192–214, 1903.\n[29] Adrian C North and David J Hargreaves. Subjective\ncomplexity, familiarity, and liking for popular music.\nPsychomusicology: A Journal of Research in Music\nCognition , 14(1-2):77, 1995.\n[30] Nathan Novemsky, Ravi Dhar, Norbert Schwarz, and\nItamar Simonson. Preference ﬂuency in choice. Jour-\nnal of Marketing Research , 44(3):347–356, 2007.\n[31] Costas Panagiotakis and Georgios Tziritas. A\nspeech/music discriminator based on rms and\nzero-crossings. IEEE Transactions on multimedia ,\n7(1):155–166, 2005.\n[32] Isabelle Peretz, Danielle Gaudreau, and Anne-Marie\nBonnel. Exposure effects on music preference and\nrecognition. Memory & Cognition , 26(5):884–902,\n1998.\n[33] Thomas L Saaty. How to make a decision: the analytic\nhierarchy process. European journal of operational re-\nsearch , 48(1):9–26, 1990.\n[34] Emery Schubert, Joe Wolfe, and Alex Tarnopolsky.\nSpectral centroid and timbre in complex, multiple in-\nstrumental textures. In Proceedings of the international\nconference on music perception and cognition, North\nWestern University, Illinois , pages 112–116. sn, 2004.\n[35] H-J Schultz-Coulon, R-D Battmer, and H Riech-\ners. Der 3-khz-formant–ein mass f ¨ur die tragf ¨ahigkeit\nder stimme? Folia Phoniatrica et Logopaedica ,\n31(4):302–313, 1979.\n[36] Sebastian Streich and Perfecto Herrera. Detrended\nﬂuctuation analysis of music signals: Danceability es-\ntimation and further semantic characterization. In Pro-\nceedings of the 118th AES Convention , 2005.\n[37] Johan Sundberg. Level and center frequency of the\nsinger’s formant. Journal of voice , 15(2):176–186,\n2001.[38] Mari Tervaniemi, Minna Huotilainen, and Elvira Brat-\ntico. Melodic multi-feature paradigm reveals auditory\nproﬁles in music-sound encoding. Frontiers in human\nneuroscience , 8(July):496, 2014.\n[39] Mari Tervaniemi, Lauri Janhunen, Stefanie Kruck,\nVesa Putkinen, and Minna Huotilainen. Auditory pro-\nﬁles of classical, jazz, and rock musicians: Genre-\nspeciﬁc sensitivity to musical sound features. Frontiers\nin psychology , 6:1900, 2016.\n[40] Wei-Ho Tsai, Dwight Rodgers, and Hsin-Min Wang.\nBlind clustering of popular music recordings based on\nsinger voice characteristics. Computer Music Journal ,\n28(3):68–78, 2004.\n[41] Chia-Jung Tsay. Sight over sound in the judgment\nof music performance. Proceedings of the National\nAcademy of Sciences , 110(36):14580–14585, 2013.\n[42] Suppawong Tuarob and Conrad S Tucker. Quantify-\ning product favorability and extracting notable product\nfeatures using large scale social media data. Journal\nof Computing and Information Science in Engineering ,\n15(3):031003, 2015.\n[43] George Tzanetakis and Perry Cook. Musical genre\nclassiﬁcation of audio signals. IEEE Transactions on\nspeech and audio processing , 10(5):293–302, 2002.\n[44] Omkarprasad S Vaidya and Sushil Kumar. Analytic\nhierarchy process: An overview of applications. Eu-\nropean Journal of operational research , 169(1):1–29,\n2006.\n[45] Christopher Watts, Jessica Murphy, and Kathryn\nBarnes-Burroughs. Pitch matching accuracy of trained\nsingers, untrained subjects with talented singing\nvoices, and untrained subjects with nontalented singing\nvoices in conditions of varying feedback. Journal of\nVoice , 17(2):185–194, 2003.\n[46] Felix Weninger, Martin W ¨ollmer, and Bj ¨orn Schuller.\nAutomatic assessment of singer traits in popular music:\nGender, age, height and race. Proceedings of the 10th\nInternational Society for Music Information Retrieval\nConference , pages 37–42, 2011.\n[47] William R Wilson. Feeling more than we can know:\nExposure effects without learning. Journal of person-\nality and social psychology , 37(6):811, 1979.\n[48] Maria AG Witek, Eric F Clarke, Mikkel Wallentin,\nMorten L Kringelbach, and Peter Vuust. Syncopation,\nbody-movement and pleasure in groove music. PloS\none, 9(4):e94446, 2014.\n[49] Robert B Zajonc. Attitudinal effects of mere exposure.\nJournal of personality and social psychology , 9(2p2):1,\n1968.536 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Main Melody Estimation with Source-Filter NMF and CRNN.",
        "author": [
            "Dogac Basaran",
            "Slim Essid",
            "Geoffroy Peeters"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492349",
        "url": "https://doi.org/10.5281/zenodo.1492349",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/273_Paper.pdf",
        "abstract": "Estimating the main melody of a polyphonic audio recording remains a challenging task. We approach the task from a classification perspective and adopt a convolutional recurrent neural network (CRNN) architecture that relies on a particular form of pretraining by source-filter nonnegative matrix factorisation (NMF). The source-filter NMF decomposition is chosen for its ability to capture the pitch and timbre content of the leading voice/instrument, providing a better initial pitch salience than standard timefrequency representations. Starting from such a musically motivated representation, we propose to further enhance the NMF-based salience representations with CNN layers, then to model the temporal structure by an RNN network and to estimate the dominant melody with a final classification layer. The results show that such a system achieves state-of-the-art performance on the MedleyDB dataset without any augmentation methods or large training sets.",
        "zenodo_id": 1492349,
        "dblp_key": "conf/ismir/BasaranEP18",
        "keywords": [
            "polyphonic",
            "audio recording",
            "classification",
            "convolutional",
            "recurrent",
            "neural",
            "network",
            "source-filter",
            "nonnegative",
            "matrix factorization"
        ],
        "content": "MAIN MELODY EXTRACTION\nWITH SOURCE-FILTER NMF AND CRNN\nDogac Basaran1Slim Essid2Geoffroy Peeters1\n1CNRS, Ircam Lab, Sorbonne Université, Ministère de la Culture, F-75004 Paris, France\n2LTCI, Télécom ParisTech, Université Paris Saclay, Paris, France\ndogac.basaran@ircam.fr\nABSTRACT\nEstimating the main melody of a polyphonic audio record-\ning remains a challenging task. We approach the task from\na classiﬁcation perspective and adopt a convolutional re-\ncurrent neural network (CRNN) architecture that relies on\na particular form of pretraining by source-ﬁlter nonneg-\native matrix factorisation (NMF). The source-ﬁlter NMF\ndecomposition is chosen for its ability to capture the pitch\nand timbre content of the leading voice/instrument, pro-\nviding a better initial pitch salience than standard time-\nfrequency representations. Starting from such a musically\nmotivated representation, we propose to further enhance\nthe NMF-based salience representations with CNN lay-\ners, then to model the temporal structure by an RNN net-\nwork and to estimate the dominant melody with a ﬁnal\nclassiﬁcation layer. The results show that such a system\nachieves state-of-the-art performance on the MedleyDB\ndataset without any augmentation methods or large train-\ning sets.\n1. INTRODUCTION\nAutomatic dominant melody estimation (AME) is a pop-\nular and rather challenging task in Music Information Re-\ntrieval (MIR). In general, AME can be deﬁned as the esti-\nmation of fundamental frequencies that represent the pitch\nvalues of the dominant melody [24]. The source of the\ndominant melody could be a leading singing voice or an\ninstrument. The difﬁculty is that there is usually a poly-\nphonic accompaniment to the lead vocal/instrument, and\nthat this accompaniment follows the melody rhythmically\nandharmonically ,in the sense that chord progressions will\nnaturally contain the dominant F0and/or its harmonics. As\na consequence, it is not trivial to obtain a representation\nthat discriminates the main melody from the background\nmusic. Hence, one of the main research directions in AME\nremains ﬁnding a salience representation that enhances the\nc⃝Dogac Basaran, Slim Essid, Geoffroy Peeters. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0).Attribution: Dogac Basaran, Slim Essid, Geoffroy Peeters. “Main\nMelody Extraction\nwith source-ﬁlter NMF and CRNN”, 19th International Society for Music\nInformation Retrieval Conference, Paris, France, 2018.fundamental frequency of the dominant melody against the\npossibly polyphonic background.\nOne of the most popular and rather simple salience rep-\nresentations is theHarmonic Sum Spectrum (HSS) [18]\nthat consists of mapping the energy among harmonically\nrelated F0s. This has been used effectively in a popular\nmelody extraction algorithm, jbcorso-called Melodia [23].\nDurrieu et. al. [11, 12] proposed a salience function where\nthe dominant melody (singing voice or instrument) is mod-\neled with a Source-Filter Nonnegative Matrix Factoriza-\ntion (SF-NMF). This method was later combined with HSS\nin [7] in order to obtain an enhanced salience representa-\ntion. There also exist other methods that utilize a simple\ntime-frequency representation, e.g., the Short Time Fourier\nTransform (STFT) or Constant Q-Transform (CQT), as a\nlow-level representation of salience [13, 25].\nRecently, Bittner et. al. [6] proposed a Convolutional\nNeural Network (CNN) system to learn salience represen-\ntations based on harmonic CQT. The rationale forthis ap-\nproach is to learn harmonic relationships implicitly and to\nobtain a salience representation similar to (or better than)\nHSS.\nSalience-based melody estimation methods usually use\npitch tracking methods on top of salience representations\nto exploit the temporal relationships between dominant\nF0s. In [12], a Hidden Markov model (HMM) was adopted\nwhere the states represent the bins of the source activa-\ntions, i.e. F0s. Then a threshold-based voicing estima-\ntion (melody/non-melody estimation) was applied. An-\nother very popular pitch tracking method was proposed by\nSalamon et. al. [23] where the algorithm creates and char-\nacterizes pitch contours on top of HSS. Characteristics of\nthese contours have proven very effective in voicing esti-\nmation [7, 23].\nRecently, Deep Neural Networks (DNNs) have become\nvery popular in MIR applications such as sound event de-\ntection [2, 4] and chord estimation [20]. The ability of\nDNNs to approximate any function with linear weights and\nnon-linear activations, given enough data, makes such sys-\ntems attractive for MIR tasks. That said, comparatively\nfew attempts have been made to estimate dominant melody\nusing neural networks. In [19, 22], bidirectional Long\nShort-Term Memory (LSTM) [15], a special kind of Re-\ncurrent Neural Network (RNN), are used for singing voice\nseparation. Such networks are mostly used in modeling the82temporal information in time sequences. Recently, in [3],\na hierarchical CNN structure similar to a stacked denois-\ning autoencoder (SDA) [26] is used to learn a mapping be-\ntween an STFT representation and a transcription similar\nto a piano roll. A tutorial on deep learning techniques for\nMIR tasks can be found in [9].\nAlthough most of these DNNs perform end-to-end\ntraining, it has proven effective to use a more structured\ninput data, such as harmonic-CQT [6]. Recently, [4]\nachieved state-of-the-art results in sound classiﬁcation by\nusing NMF activations as input as a form of pretraining.\nContributions .Inspired by these works, we propose a\nConvolutional-Recurrent Neural Network (CRNN) model\nwhose pretraining is based on the SF-NMF model pro-\nposed in [12]. We show that with NMF-based pretrain-\ning, we can achieve state-of-the-art results without requir-\ning large training datasets or data augmentation methods,\nand using relatively simpler networks in terms of training\nparameters. Our results clearly demonstrate the usefulness\nof a good input salience representation to the network, sug-\ngesting that performance would climb even higher if the\nSF-NMF model were improved. Our results are obtained\non MedleyDB [5], which is a challenging dataset due to\ninclusion of singing voice and instrument melodies in a di-\nverse setof music genres.\nThe rest of the paper is organized as follows: the pro-\nposed CRNN system and pretraining with SF-NMF are\ndetailed in Section 2. Section 3 discusses the domi-\nnant melody estimation results obtained on the MedleyDB\ndataset, and also gives an analysis of SF-NMF-based\nsalience and the comparison between different CRNN vari-\nants. Finally, some conclusions and future directions are\ngiven in Section 4.\n2. SYSTEM OVERVIEW\nThe block diagram of the CRNN system we propose is\ngiven in Figure 1. In the ﬁrst stage (Pretraining), we es-\ntimate an initial salience representation using the SF-NMF\nmodel. Then this salience is fed into a CNN (CNN stage),\nwhere the salience representation is further enhanced by\nlearning local features. The CNN output activations are\nthen fed into an RNN to exploit the long-term relationships\nbetween fundamental frequencies (RNN stage). Then in\nthe ﬁnal Classiﬁcation stage, we classify the representa-\ntions as melody/non-melody and give an estimate for F0\nat each time-frame where each class represents a semitone\nfundamental frequency. Note that the same procedure is\napplied in both the training and testing ofthe system.\nIn the design of the CRNN system, we are inspired by\na similar CRNN proposed in [20] for chord recognition,\nwhere the network is interpreted as an encoder-decoder\nscheme. In the CRNN structure we propose, the CNN\nand RNN stages can also be treated together as an encod-\ning stage (input sequence to mid-level salience representa-\ntion)where the output is an enhanced salience representa-\ntion that captures both spatial and temporal features. Then\nthe classiﬁcation stage acts as a decoding stage (mid-level\nrepresentation to output sequence) where the salience is\nPretraining with SF-NMFRNN StageClassiﬁcationCNN StageSalience representation}Temporal pitch trackingMelody/non-melody F0 estimationFigure 1 : Block diagram of the proposed CRNN system\nwith pretraining\nmapped into a frame-based note representation.\n2.1 Pretraining with SF-NMF\nIn [12], the dominant melody (voice/instrument) is mod-\neled using a source-ﬁlter model. Assuming the mixing\nof the dominant melody and the accompaniment (back-\nground) is instantaneous, the source, ﬁlter and accompa-\nniment parts are modeled with the SF-NMF model as fol-\nlows:\nV\u0019^V=VF0⊙V\b+VB\n=WF0HF0⊙W\bH\b+WBHB(1)\n=WF0HF0⊙W\u0000H\u0000H\b+WBHB(2)\nwhereVrepresents the power spectrogram of thesignal,\ni.e.,V=jXj2(whereXis the STFT of the audio signal to\nbe analyzed );F0;\bandBrepresents the source, ﬁlter and\nbackground respectively; WandHrepresent the basis and\nactivation matrices; and ⊙denotes the Hadamard product.\nThe ﬁlter basis W\bis further modeled with yet another\nNMF representation , asin [11]:W\b=W\u0000H\u0000.\nIn this model, the source, VF0=WF0HF0, is assumed\nto have a harmonic structure. To ensure such a struc-\nture, the basis WF0is pre-constructed (not estimated) such\nthat each column represents the harmonic structure for one\nF0. Represented F0s start from a minimum frequency, i.e.,\nF0= 55 Hz, and they are logarithmically spaced, i.e., the\nratio between consecutive F0values would be 2(1=60)for a\nresolution of 5 bins per semitone. Such a construction en-\nforces the corresponding row in the activation matrix HF0\nto represent the activation of that speciﬁc F0, similar to a\nsaliency representation. That is the rationale behind using\nHF0asasaliency representation as in [7, 11, 12].\nThe main assumption with the ﬁlter, V\b, is to have a\nsmooth structure. One way to ensure such smoothness is\nto construct a basis W\bfrom smooth ﬁlters in advance,\nsimilar to enforcing harmonic structure in the source VF0.\nHowever it is not possible to directly construct W\bwith\nsmooth basis ﬁlter structures since it depends on the dom-\ninant melody. In [11], it is proposed to represent W\bwith\nanother NMF model, W\u0000H\u0000, where the columns of W\u0000\nare constructed (not estimated) as simple and smooth band\npass ﬁlters that are linearly spaced and overlapping. This\nstructure forcesW\bto be smooth, thus ensuring that V\b\nwill be smooth as expected.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 83The accompaniment/background, VB=WBHB, is\nalso represented with a standard NMF model where there\nare no constraints on the basis such as smoothness or being\nharmonic. In summary, thesource basis WF0and smooth\nﬁlter basis W\u0000are pre-constructed and the rest of the pa-\nrameters HF0,H\u0000,H\b,WBandHBare estimated using\nthe standard alternating scheme and heuristic multiplica-\ntive updates.\nIn this work, for the SF-NMF model, we follow the\nparametrization given in [7] where the minimum and max-\nimum frequencies represented in HF0are chosen as 55Hz\nand1760 Hzrespectively. We choose the resolution of the\nF0s as 5 bins per semitone which results in 60 bins per\noctave (bpo) and 301 bins in total per frame.\nNote that due to the logarithmic spacing of the F0s\nwhere the consecutive frequencies have a ratio of 21=60,\none can tune the represented F0s with proper choice of the\nminimum frequency F0;min.As an example, if F0;min =\n55Hz, the notes will be tuned to A4 = 440 Hzwhereas if\nF0;min = 55 :25Hz, they will be tuned to A4 = 442 Hz.\nThis choice of tuning might depend on the target dataset.\nHere, we choose the tuning A4 = 440 Hzassuming that\nsuch tuning is more widely used. It is important to men-\ntion that this construction of F0s inWF0cannot be gener-\nalized to all music genres, e.g., traditional Turkish music\nwith makams. Hence the methods based on SF-NMF, as\nwell as the proposed scheme, are limited in that sense.\nAlthough we aim to classify the fundamental frequen-\ncies at semitone resolution, we initially choose a higher\nresolution for the F0s inWF0. In practice, it is highly\nprobable that a dominant voice or instrument will be\nslightly out-of-tune, and hence will not ﬁt any of the repre-\nsented F0s. In such cases, a high resolution representation\nofF0s might better describe these out-of-tune notes.\n2.2 CNN stage\nIn order to enhance the HF0-salience, we propose two dif-\nferent CNN architectures, which we denote as CNN1 and\nCNN2. In CNN architecture 1 (CNN1), we ﬁrst decrease\ntheF0resolution to semitones, then we train CNN layers\nto learn local structures, i.e., the confusions between semi-\ntones. In the second approach (CNN2), we follow the net-\nwork proposed in [6]. Here, the network learns the features\nin the original resolution and within a semitone interval\nwith one additional layer that learns the octave patterns.\nNote that since each CNN architecture only applies 2D\nlinear ﬁlters and non-linear activations, the input structure\nis not lost through the layers of the network. This provides\nan advantage of interpretable hidden layer activations and\nleads to a new form of salience as output where each row\nstill represents the activation of a fundamental frequency.\nIn both architectures, rectiﬁed linear units (ReLu s) are\nused as non-linear activations and are applied to each CNN\nlayer output. Batch normalization is applied before each\nintermediate CNN layer input, as it has proven effective\nin the convergence of the network by reducing the internal\ncovariance shift [16]. The columns of HF0are normalized\nwithl1norm before being fed into the CNN network. Such\nFigure 2 : CNN Architecture 1 (CNN1).\na normalization is possible since the task at hand is the\nestimation of the melody; that is, only the position of the\nfundamental frequency is needed, not the exact energy.\n2.2.1 CNN Architecture 1 (CNN1)\nThere are 5 layers in theCNN1 architecture. The ﬁrst layer\ngathers the energy around each semitone by applying fo-\ncused ﬁlters centered around each semitone frequency. In\nthis layer, there are 64 (5x1) ﬁlters each with a stride (5,1).\nThis way, not only is the energy focused on the semitones,\nbut also the frequency resolution is decreased to the semi-\ntone scale from 5 bins per semitone (time resolution re-\nmains the same). The rationale behind the ﬁrst layer is\ntwo-fold: First, the number of parameters is severely de-\ncreased by lowering the frequency resolution, i.e., it takes\n5 times less ﬁlter parameters in order to learn features. Sec-\nond, out-of-tune notes would already be represented in the\nvicinity of the corresponding semitone in the HF0repre-\nsentation. Focused ﬁlters on semitones would gather the\nenergy on the semitone that is a way of retuning the melody\non the represented semitone fundamental frequencies.\nIn the following layers, zero padding is applied to con-\nvolutions to keep the dimensions unchanged . The second\nlayer has 64 (5 x 3) ﬁlters that cover \u00062 semitone inter-\nval and roughly 30ms in time. Then the third layer has 64\n(3 x 3) ﬁlters that cover \u00061 semitone and 30ms in time.\nThe fourth layer has 16 (15 x 3) ﬁlters to learn note con-\nfusions in one octave. Filters cover \u00067semitone interval\nand again 30ms in time. Then enhanced salience represen-\ntation is obtained as the output of the ﬁnal CNN layer that\nhas only one (1x1) ﬁlter as in [6] but with a rectiﬁed linear\nunit instead of a sigmoid. The overall structure of CNN\narchitecture 1 is shown in Figure 2.\n2.2.2 CNN Architecture 2 (CNN2)\nCNN2 is based on the network proposed in [6]. In this net-\nwork, the resolution of the input remains the same through-\nout the layers of the CNN, i.e., no pooling is applied. Note\nthat the input toCNN2 is HF0; therefore, the ﬁrst layer of\nthe network contains only a single channel instead of six.84 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018As mentioned before, the overall system targets semi-\ntone resolution for the output fundamental frequencies.\nThis requires a reduction in resolution somewhere inthe\nsystem. In this architecture, we left the dimensionality re-\nduction to the ﬁnal classiﬁcation layer.\n2.3 RNN stage\nRecurrent neural networks are mostly used in MIR and\naudio analysis tasks to model the dynamics of the obser-\nvations , typically for chord recognition [20] and speech\nrecognition [14]. Here, we use a single bidirectional Gated\nRecurrent Unit (BiGRU) layer to capture temporal rela-\ntionships between F0s. A GRU is a special kind of RNN\n[8] where the units are able to model long-term temporal\nrelationships whilst using a gate structure. It has the advan-\ntage of not suffering from the vanishing gradient problem\nof standard RNN and has proven to be easier to train com-\npared to the LSTM alternative.\nThe number of units in a BiGRU layer should be chosen\nhigher or equal to the output dimension of the preceding\nCNN network. In the BiGRU structure, actually two GRU\nlayers are trained with the same input but in reverse direc-\ntions to model the F0relationships from both directions.\nLater, the two layers are merged to have a single output.\n2.4 Classiﬁcation\nThe ﬁnal layer of the system is a classiﬁer where one class\nrepresents the non-melody and the rest of the61 classes\nrepresent semitone fundamental frequencies between A1\nand A6 ( inclusive ). The multiclass classiﬁcation output is\nobtained with a single dense layer and softmax activations.\nThe overall system is trained minimizing the cross en-\ntropy loss between the softmax activations and true proba-\nbilities. A frame is classiﬁed as a non-melody frame only\nif the probability of non-melody class is higher than the\nrest. Regardless of this decision, F0is estimated for each\nframe by simply picking the most probable F0class among\nthe 61 note classes. Note that even if the non-melody class\nhas the highest probability, the second-highest probability\ngives a good estimation of the pitch.\nAn example output of the classiﬁcation layer that is ob-\ntained from a CNN1 + RNN + Classiﬁcation architecture\nis shown in Figure 3. In this example, HF0input (top-left)\ngives a very good initial salience. Then the CNN1 output\nactivations (top-right) further enhance the dominant part\nagainst the harmonic background. It is observed that the\ndominant F0classes mostly have the highest probabilities\nagainst the rest of the class probabilities (bottom-left).\n3. EXPERIMENTS\nIn this section, we evaluate the proposed NMF-based\nCRNN system using the MedleyDB dataset [5]. For the\nannotations, we use the \"Melody2\" deﬁnition in Med-\nleyDB that is the F0of the dominant melody at each time\nstep, drawn from multiple sources. With this deﬁnition\nof melody, it is possible to have separate instruments or\nFigure 3 : (Top-left) HF0representation of a small au-\ndio excerpt as input to CRNN, (Top-right) CNN1 activa-\ntions, (Bottom-left) Classiﬁer activations of CRNN, (Bot-\ntom right) Ground-truth annotations.\nvoices as the source of dominant melody throughout a sin-\ngle song. Among 108 annotated songs in the dataset, 48\nsongs have predominant instrumental melody, 30 songs\nhave predominant vocal melody and 30 songs have both\npredominant instrument and vocal melodies.\nWe randomly split the MedleyDB set into train, valida-\ntion and test sets such that the tracks from the same artist\ndo not belong to different sets following the artist condi-\ntional random splitting asin [6,7]. There are 27 full-length\ntracks in the test set, 67 full-length tracks in the training set\nand 14 full-length tracks in the validation set. Note that we\nused the same test split with [6] in the MedleyDB in the\nrest of the experiments to be able compare the results.\nWe use theﬁve standard evaluation metrics given in\n[24], namely: Raw Pitch Accuracy (RPA), Raw Chroma\nAccuracy (RCA), Overall Accuracy (OA), Voicing False\nAlarm (VFA) and Voicing Recall (VR). All the codes are\nwritten in Python and available online1.CQT implemen-\ntation is based on thelibrosa python package [21].\n3.1 Network training\nWe trained three different networks with the following\ncombinations of the architectures given in Section 2:\nCRNN-1 : CNN1 + 1 layer BiGRU (128 Units) + Classiﬁ-\ncation layer;\nCRNN-2 : CNN2 + 1 layer BiGRU (160 Units) + Classiﬁ-\ncation layer;\nC-NN : CNN2 + Classiﬁcation layer.\nWe further denote the network variants by prepending\na label indicating the input to the network: \"SF\" for HF0\ninput and \"CQT\" for CQT input. Note that the CQT pa-\nrameters are chosen such that the representation of a signal\nviaHF0or CQT would have the same dimensions2.\n1github.com/dogacbasaran/ismir2018_dominant_melody_estimation\n2CQT parameters: Minimum F0=55Hz, #of octaves = 5, bpo = 60Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 85CRNN-1 CRNN-2 Baseline\n# of Param. 307,199 854,319 406,253\nTable 1 : The number of trainable parameters for CRNN-1,\nCRNN-2 and the baseline CNN network [6]\nIn the proposed CRNN structure, the purpose of the\nCNN stage is to learn local features, whereas the purpose\nof the RNN stage is to account for long term temporal re-\nlationships . This requires selecting relatively small patch\nlengths for the CNN layers but longer patch lengths for\nthe RNN layer. For this purpose, we used different patch\nlengths for the CNN and RNN parts while jointly training\nthem.\nIn all the models, the CNN layers are trained on either\n0.29-second (25-frame) or 0.58-second (50-frame) patches\nand the RNN layer is trained on 5.8-second (500-frame)\npatches. The training is performed using mini-batches of\n16 patches per batch. We use the ADAM optimizer [17],\nand reduce the learning rate if there is no improvement in\nvalidation loss after 20 epochs. The early stopping strat-\negy is used if the validation loss is not decreased after 20\nepochs. The maximum possible number of epochs is set to\n200. All models were implemented with Keras 2.0 [10]\nand Tensorﬂow 1.0 [1] and tested using NVIDIA-Tesla\nK80 GPUs. The number of parameters for each network\nmodel is given in Table 1.\nNote that, in the training, we do not beneﬁt from any\ndata augmentation method or from other larger datasets.\n3.2 Results\nWe compare the outputs of all three models to a CNN-\nbased melody tracking system [6], considered as a base-\nline, which proved to signiﬁcantly outperform the previous\nstate-of-the-art methods in [7, 23]. The evaluation results\nof [6] are available online.3By choosing the same test\nsplit from the MedleyDB, we are able to compare these\npublished results to ours without any re-evaluation. The\nevaluation results for all network variants (SF-CRNN-1,\nSF-CRNN-2, CQT-CRNN-2, SF-C-NN) and for the base-\nline are given in Figure 4. We use McNemar’s test on the\nclassiﬁcation results and provide p-values as a measure of\nsigniﬁcance whenever relevant4.\nCQT vs. HF0as salience\nWe explore the usefulness of pretrained input by com-\nparing the evaluation results of theCRNN-2 model when\nthe input is CQT orHF0—i.e., comparing CQT-CRNN-2\nand SF-CRNN-2 . The results show that CRNN-2 model\nperforms signiﬁcantly better in OA (p=0.0015) and RCA\n(p=0.0003) scores when the input to the network is HF0.\nOn average, results for SF-CRNN-2 are 6, 9 and 7 percent-\nage points higher for OA, RPA and RCA, respectively.\nThe reason the CRNN-2 model performs better with\npretrained input is that HF0provides a better initial\n3github.com/rabitt/ismir2017-deepsalience\n4Mcnemar test is based on statsmodel package in python.HF0 CQT\nRPA 0:538\u00060:141 0:210\u00060:16\nRCA 0:648\u00060:127 0:411\u00060:15\nTable 2 : The comparison of RPA and RCA scores for HF0\nfeature and CQT feature by simple peak-picking method.\nFigure 4 : Evaluation metrics for SF-CRNN-1, SF-CRNN-\n2, CQT-CRNN-2, SF-C-NN and the baseline [6].\nsalience representation than the CQT. Ideally, a salience\nrepresentation of melody should be discriminative for each\ntarget fundamental frequency against the polyphonic back-\nground music. We can analyze both HF0and CQT repre-\nsentations to see how well they ﬁt this deﬁnition of “ideal”\nsalience by performing a simple peak-picking strategy as\nin [6]. Speciﬁcally, the frequency with maximum ampli-\ntude/salience for each time frame point is chosen as the\nestimate of the fundamental frequency. We can compute\nthe RPA and RCA scores using those estimates to see their\nperformances as salience. The results obtained on the full\nMedleyDB dataset are given in Table 2. It can be seen\nthatHF0performs nearly twice as well as the CQT repre-\nsentation in both RPA and RCA scores, showing that HF0\nprovides a better initial salience to the CRNN networks.\nSF-CRNN-2 model vs. Baseline CNN Network\nTheSF-CRNN-2 model uses theCNN-2 architecture in the\nCNN stage, the same CNN asthe baseline. When we\ncompare the evaluation results given in Figure 4, we ob-\nserve that theSF-CRNN-2 model outperforms the baseline\nin the RPA (p = 0.0015) and VR (p=0.052) scores. The\nmodel has slightly higher OA andRCA scores on average\nthan the baseline. On the other hand, SF-CRNN-2 has a\nhigher number of network parameters ( 854;319) than the\nbaseline CNN ( 406;253). This is due to the additional\nRNN layer that exists in SF-CRNN-2.\nComparison between variants SF-CRNN-1, SF-CRNN-\n2 and SF-C-NN\nOn average, SF-CRNN-1 performs slightly better than all\nother models in all metrics aside from VFA. Comparing\nSF-CRNN-1 and SF-CRNN-2, we observe that a similar or86 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018OA RPA RCA VR VFA\nSF-CRNN-1 0.444 0.595 0.677 0.556 0.423\nBaseline 0.580 0.756 0.725 0.590 0.219\nTable 3 : Evaluation results for the track \"MatthewEn-\ntwistle_TheFlaxenField\" where the worst OA performance\noccurs against the baseline [6].\nhigher performance can be achieved by thelow resolution\nCNN1 architecture and with far fewer training parameters\n(see Table 1). VR rates forSF-CRNN-1 and SF-CRNN-2\nare signiﬁcantly higher than the SF-C-NN; however, VFA\nrates are higher as well. This behavior could be due to the\nactivations of the RNN layer that should force some sort of\ntemporal smoothing on the salience representation.\nOn the other hand, the signiﬁcantly better OA, RPA and\nRCA scores of SF-CRNN-2 relative to SF-C-NN suggest\nthat the temporal tracking with RNN effectively improves\nthe performance of the melody estimation.\nComparing the best performing network variant SF-\nCRNN-1 to the baseline, we observe that it outperforms\nthe baseline on the OA (p=0.052), RPA (p=0.0003) and\nVR (p=0.0015) scores, and achieve those results with a less\ncomplex network in terms of network parameters (see Ta-\nble 1). A track-level comparison by computing the overall\naccuracy differences for each track shows that SF-CRNN-\n1 performs better on 19 tracks out of 27.\nThe worst OA of SF-CRNN-1 occurs against the base-\nline with the \"MatthewEntwistle_TheFlaxenField\" track\nwhere the dominant melody consists only of instruments\nincluding Piano. The evaluation results for this track are\ngiven in Table 3. It is observed that both SF-CRNN-1\nand baseline have relatively high VFA; however, the effect\nof this is minimal since the track mostly contains voiced\nframes. On the other hand, the OA score would be highly\naffected bythecombination of high RPA and VR scores.\nFor this track, although the baseline and SF-CRNN-1 have\ncomparable VR rates, the RPA score of the baseline is bet-\nter, which explains the difference in OA performance.\nSinging voice vs. Instrument\nAmong the test set in MedleyDB, 16 tracks contain only\ninstrumental dominant melody, 3 tracks contain only dom-\ninant singing voice melody and 8 tracks contain both5.\nEvaluation results in Table 4 show that SF-CRNN-1 per-\nforms better for singing voice melodies than instrument\nmelodies. SF-CRNN-1 outperforms the baseline in over-\nall accuracy for singing voice melodies and instrument\nmelodies.\n4. CONCLUSIONS AND FUTURE WORK\nIn this work, we have introduced a novel audio-based dom-\ninant melody estimation architecture using source-ﬁlter\nNMF as pretraining for a new variant of deep network for\n5The ratio of the dominant singing voice melody frames and the dom-\ninant instrumental melody frames among all voiced frames is 0.238 and\n0.762, respectively.SF-CRNN-1 Baseline\nS.V. Ins. S.V. Ins.\nOA 0.638 0.466 0.598 0.424\nRPA 0.791 0.647 0.784 0.619\nRCA 0.804 0.726 0.823 0.717\nTable 4 : OA, RPA and RCA scores for singing voice (S.V.)\nmain melody and Instrument (Ins.) main melody for SF-\nCRNN-1 and baseline.\nthis task, namely a CNN-BiGRU scheme . We have shown\nthat the proposed system achieves state-of-the-art perfor-\nmance on standard evaluation metrics, even signiﬁcantly\nimproving onit while maintaining a lower system com-\nplexity .\nAnalysis ofHF0as a salience representation shows that\nit provides a good initial salience in general with high RPA\nand RCA, even when performing melody estimation us-\ning frame-based salience peak-picking. The evaluation re-\nsults clearly show the usefulness of SF-NMF-based pre-\ntraining in many aspects. We observe that when provided\nwith a good initial salience input to the CRNN structure,\nthe system performs considerably better without requir-\ning any augmentation or additional training data. This\nencourages the idea of improving the pretraining part to\nobtain even more discriminative salience representations\nwhich will surely increase the melody estimation perfor-\nmance. For such improvements, SF-NMF is a good can-\ndidate since many other variants with various constraints\nsuch as smoothness or sparsity exist in the literature.\nWe observe that in the proposed CRNN structure, the\nCNN stage help sto improve the quality of the salience\nrepresentation against HF0. In addition, exploiting tempo-\nral information with the RNN signiﬁcantly improves OA,\nRPA, RCA and VR. These two stages act similar lyto an\nencoder scheme and the classiﬁcation layer acts as the de-\ncoder. Therefore one can interpret the proposed CRNN as\nan encoder-decoder network where the encoder is used to\nobtain an enhanced salience representation and the decoder\nproduces a frame-based transcription.\nFrom a melody classiﬁcation viewpoint, the MedleyDB\ndataset is quite challenging due to itsdiverse range of in-\nstrumentation and music genres. Also, there is an im-\nbalance between the note classes and thenon-melody class\nin the dataset. The CRNN network has proven effective\nin handling such imbalance when pretrained with anSF-\nNMF model.\nA clear future direction to pursue is training the SF-\nNMF and CRNN jointly, learning theHF0representation\nwhile minimizing the classiﬁcation error.\n5. ACKNOWLEDGEMENT\nThis project is partly funded by theDigThatLick project.\nWe’d like to thank Rachel Bittner, Dr. Umut Simsekli and\nDr. Jordan Smith for their valuable technical support.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 876. REFERENCES\n[1]Martín Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,\nAndy Davis, Jeffrey Dean, Matthieu Devin, Sanjay\nGhemawat, Ian Goodfellow, Andrew Harp, Geoffrey\nIrving, Michael Isard, Yangqing Jia, Rafal Jozefow-\nicz, Lukasz Kaiser, Manjunath Kudlur, Josh Leven-\nberg, Dandelion Mané, Rajat Monga, Sherry Moore,\nDerek Murray, Chris Olah, Mike Schuster, Jonathon\nShlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar,\nPaul Tucker, Vincent Vanhoucke, Vijay Vasudevan,\nFernanda Viégas, Oriol Vinyals, Pete Warden, Martin\nWattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang\nZheng. TensorFlow: Large-scale machine learning on\nheterogeneous systems, 2015. Software available from\ntensorﬂow.org.\n[2]Sharath Adavanne, Konstantinos Drossos, Emre Çakir,\nand Tuomas Virtanen. Stacked convolutional and re-\ncurrent neural networks for bird audio detection. In\nSignal Processing Conference (EUSIPCO), 2017 25th\nEuropean , pages 1729–1733. IEEE, 2017.\n[3]Stefan Balke, Christian Dittmar, Jakob Abeßer, and\nMeinard Müller. Data-driven solo voice enhancement\nfor jazz music retrieval. In Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2017 IEEE International\nConference on , pages 196–200. IEEE, 2017.\n[4]Victor Bisot, Romain Serizel, Slim Essid, and Gaël\nRichard. Leveraging deep neural networks with non-\nnegative representations for improved environmental\nsound classiﬁcation. In IEEE International Workshop\non Machine Learning for Signal Processing MLSP ,\n2017.\n[5]Rachel Bittner, Justin Salamon, Mike Tierney,\nMatthias Mauch, Chris Cannam, and Juan Bello. Med-\nleydb: A multitrack dataset for annotation-intensive\nmir research.\n[6]R.M. Bittner, B. McFee, J. Salamon, P. Li, and J.P.\nBello. Deep salience representations for f0estima-\ntion in polyphonic music. In 18th International Society\nfor Music Information Retrieval Conference , ISMIR,\n2017.\n[7]J.J. Bosch, R.M. Bittner, J. Salamon, and E. Gómez.\nA comparison of melody extraction methods based on\nsource-ﬁlter modelling. In 17th International Society\nfor Music Information Retrieval Conference , ISMIR,\n2016.\n[8]Kyunghyun Cho, Bart van Merrienboer, Çaglar\nGülçehre, Fethi Bougares, Holger Schwenk, and\nYoshua Bengio. Learning phrase representations using\nRNN encoder-decoder for statistical machine transla-\ntion. CoRR , abs/1406.1078, 2014.\n[9]Keunwoo Choi, György Fazekas, Kyunghyun Cho, and\nMark B. Sandler. A tutorial on deep learning for music\ninformation retrieval. CoRR , abs/1709.04396, 2017.[10] François Chollet. keras. https://github.com/\nfchollet/keras , 2015.\n[11] J. L. Durrieu, B. David, and G. Richard. A musically\nmotivated mid-level representation for pitch estimation\nand musical audio source separation. IEEE Journal of\nSelected Topics in Signal Processing , 5(6):1180–1191,\nOct 2011.\n[12] J. L. Durrieu, G. Richard, B. David, and C. Fevotte.\nSource/ﬁlter model for unsupervised main melody ex-\ntraction from polyphonic audio signals. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n18(3):564–575, March 2010.\n[13] B. Fuentes, A. Liutkus, R. Badeau, and G. Richard.\nProbabilistic model for main melody extraction using\nconstant-q transform. In 2012 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 5357–5360, March 2012.\n[14] A. Graves, A. r. Mohamed, and G. Hinton. Speech\nrecognition with deep recurrent neural networks. In\n2013 IEEE International Conference on Acoustics,\nSpeech and Signal Processing , pages 6645–6649, May\n2013.\n[15] Sepp Hochreiter and Jürgen Schmidhuber. Long\nshort-term memory. Neural Comput. , 9(8):1735–1780,\nNovember 1997.\n[16] Sergey Ioffe and Christian Szegedy. Batch normaliza-\ntion: Accelerating deep network training by reducing\ninternal covariate shift. In Proceedings of the 32Nd In-\nternational Conference on International Conference on\nMachine Learning - Volume 37 , ICML’15, pages 448–\n456. JMLR.org, 2015.\n[17] Diederik P. Kingma and Jimmy Ba. Adam: A method\nfor stochastic optimization. CoRR , abs/1412.6980,\n2014.\n[18] Anssi Klapuri. Multiple fundamental frequency esti-\nmation by summing harmonic amplitudes. In ISMIR ,\npages 216–221, 2006.\n[19] S. Leglaive, R. Hennequin, and R. Badeau. Singing\nvoice detection with deep recurrent neural networks.\nIn2015 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pages 121–\n125, April 2015.\n[20] B. McFee and J.P. Bello. Structured training for large-\nvocabulary chord recognition. In 18th International\nSociety for Music Information Retrieval Conference ,\nISMIR, 2017.\n[21] Brian McFee, Matt McVicar, Stefan Balke, Carl\nThomé, Colin Raffel, Dana Lee, Oriol Nieto, Eric Bat-\ntenberg, Dan Ellis, Ryuichi Yamamoto, Josh Moore,\nRachel Bittner, Keunwoo Choi, Pius Friesch, Fabian-\nRobert Stöter, Vincent Lostanlen, Siddhartha Kumar,\nSimon Waloschek, Seth, Rimvydas Naktinis, Douglas88 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Repetto, Curtis \"Fjord\" Hawthorne, CJ Carr, Waldir Pi-\nmenta, Petr Viktorin, Paul Brossier, João Felipe Santos,\nJackieWu, Erik, and Adrian Holovaty. librosa/librosa:\n0.6.1, May 2018.\n[22] François Rigaud and Mathieu Radenen. Singing voice\nmelody transcription using deep neural networks. In\nISMIR , 2016.\n[23] J. Salamon and E. Gomez. Melody extraction from\npolyphonic music signals using pitch contour charac-\nteristics. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 20(6):1759–1770, Aug 2012.\n[24] J. Salamon, E. Gomez, D. P. W. Ellis, and G. Richard.\nMelody extraction from polyphonic music signals: Ap-\nproaches, applications, and challenges. IEEE Signal\nProcessing Magazine , 31(2):118–134, March 2014.\n[25] Emmanuel Vincent, Nancy Bertin, and Roland Badeau.\nAdaptive harmonic spectral decomposition for mul-\ntiple pitch estimation. IEEE Transactions on Audio,\nSpeech, and Language Processing , 18(3):528–537,\n2010.\n[26] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. Extracting and composing\nrobust features with denoising autoencoders. In Pro-\nceedings of the 25th international conference on Ma-\nchine learning , pages 1096–1103. ACM, 2008.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 89"
    },
    {
        "title": "Relevance of Musical Features for Cadence Detection.",
        "author": [
            "Louis Bigo",
            "Laurent Feisthauer",
            "Mathieu Giraud",
            "Florence Levé"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492423",
        "url": "https://doi.org/10.5281/zenodo.1492423",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/243_Paper.pdf",
        "abstract": "Cadences, as breaths in music, are felt by the listener or studied by the theorist by combining harmony, melody, texture and possibly other musical aspects. We formalize and discuss the significance of 44 cadential features, correlated with the occurrence of cadences in scores. These features describe properties at the arrival beat of a cadence and its surroundings, but also at other onsets heuristically identified to pinpoint chords preparing the cadence. The representation of each beat of the score as a vector of cadential features makes it possible to reformulate cadence detection as a classification task. An SVM classifier was run on two corpora from Bach and Haydn totaling 162 perfect authentic cadences and 70 half cadences. In these corpora, the classifier correctly identified more than 75% of perfect authentic cadences and 50% of half cadences, with low false positive rates. The experiment results are consistent with common knowledge that classification is more complex for half cadences than for authentic cadences.",
        "zenodo_id": 1492423,
        "dblp_key": "conf/ismir/BigoFGL18",
        "keywords": [
            "cadences",
            "breaths in music",
            "listener or theorist",
            "harmony",
            "melody",
            "texture",
            "other musical aspects",
            "cadential features",
            "correlated with the occurrence of cadences in scores",
            "properties at the arrival beat of a cadence"
        ],
        "content": "RELEV ANCE OF MUSICAL FEATURES FOR CADENCE DETECTION\nLouis Bigo1Laurent Feisthauer1Mathieu Giraud1Florence Lev ´e2;1\n1CRIStAL, UMR 9189, CNRS, Universit ´e de Lille, France\n2MIS, Universit ´e de Picardie Jules Verne, Amiens, France\nflouis,laurent,mathieu,florence g@algomus.fr\nABSTRACT\nCadences, as breaths in music, are felt by the listener or\nstudied by the theorist by combining harmony, melody,\ntexture and possibly other musical aspects. We formalize\nand discuss the signiﬁcance of 44 cadential features , cor-\nrelated with the occurrence of cadences in scores. These\nfeatures describe properties at the arrival beat of a cadence\nand its surroundings, but also at other onsets heuristically\nidentiﬁed to pinpoint chords preparing the cadence. The\nrepresentation of each beat of the score as a vector of ca-\ndential features makes it possible to reformulate cadence\ndetection as a classiﬁcation task. An SVM classiﬁer was\nrun on two corpora from Bach and Haydn totaling 162 per-\nfect authentic cadences and 70 half cadences. In these cor-\npora, the classiﬁer correctly identiﬁed more than 75% of\nperfect authentic cadences and 50% of half cadences, with\nlow false positive rates. The experiment results are con-\nsistent with common knowledge that classiﬁcation is more\ncomplex for half cadences than for authentic cadences.\n1. INTRODUCTION\n1.1 Cadences\nMusic, like all languages, is organized into structural units.\nIn Western tonal music, these units often end with strong\nharmonic formulas called cadences , from the Latin cadere ,\n“to fall.” Despite their structural function, cadences are\nhard to deﬁne. Based on a review of dozens of music the-\nory papers, Blombach deﬁned the cadence as “any musi-\ncal element or combination of musical elements, including\nsilence, that indicates relative relaxation or relative con-\nclusion in music” [3]. This deﬁnition highlights the way\na listener (whether musically trained or not) can hear the\npresence of a cadence by feeling that the music “breaths”.\nA cadence is generally characterized by local musical el-\nements, such as a speciﬁc harmonic progression and a\nfalling melody. However, these elements do not necessar-\nily imply a cadence. A global or high-level structure such\nas the sonata form may also induce the impression of a ca-\ndence [10].\nc\rLouis Bigo, Laurent Feisthauer, Mathieu Giraud, Flo-\nrence Lev ´e. Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: Louis Bigo, Laurent\nFeisthauer, Mathieu Giraud, Florence Lev ´e. “Relevance of musical fea-\ntures for cadence detection”, 19th International Society for Music Infor-\nmation Retrieval Conference, Paris, France, 2018.Cadences are usually classiﬁed by harmonic progres-\nsion. The authentic cadence is characterized by a dominant\nharmony (notated V) followed by a tonic harmony (no-\ntated I). In the American terminology, when both chords\nare in root position and the melody ends on the tonic, the\nauthentic cadence is said to be perfect (PAC), otherwise it\nisimperfect (IAC). If the IAC is in root position (but the\nmelody does not end on the tonic), it is said to be a rooted\nIAC(rIAC). The half cadence (HC) ends with a dominant\nharmony, generally in root position. The deceptive cadence\n(DC) is an authentic cadence where the expected ﬁnal tonic\nis replaced by another harmony (often VI). Some authors\ntheorize the evaded cadence as a particular IAC, while oth-\ners see it as a DC-like progression but including a melodic\nbreak, for instance while repeating a phrase [19]. Some\nscholars do not consider the plagal progression IV/Ias a\ncadence but rather as a post-cadential prolongation [4].\nEach cadence type provide a different feeling of clo-\nsure. The strongest cadence is the PAC, followed in turn by\nthe rIAC, IAC, HC, and the DC and related cadences [20].\nSome traditions consider the rIAC to be very conclusive.\nFor instance, French music teachers refer to both PAC and\nrIAC as cadence parfaite . Using a preparation chord be-\nfore the dominant chord, generally a subdominant har-\nmony (SD, that is II,IV, orV/V), strengthens the salience\nof a PAC/rIAC. In contrast, DC and related cadences renew\ntension, extending the musical phrase and delaying closure\nuntil a more conclusive cadence is used.\n1.2 Cadences, Musicology and MIR\nModeling cadences is a current challenge in musicol-\nogy [15]. Although cadence deﬁnitions found in music\neducation textbooks are often quite short, music theorists\nagree on the difﬁculty to deﬁne cadences because of the\nvariety of their realizations observed in the repertoire [4].\nCadences are therefore usually studied within the frame\nof one speciﬁc corpus – see for example Martin and\nPedneault-Deslauriers’s study of HC in Mozart’s piano\nsonatas [14]. However, more systematic analyses of large\ncorpora would help to understand the evolution of compo-\nsitional choices over time. Rohrmeier and Neuwirth sug-\ngested a ﬁrst characterization using grammars, based on\nthe degrees and the bass line [18].\nDetecting cadences throughout the score requires a spe-\nciﬁc training to ﬁnd clues pointing out to the breaths in mu-\nsic. Can we algorithmically detect cadences from a score\nencoded in symbolic notation? Some works in MIR have355/noteheads.s2/noteheads.s2X\n/noteheads.s2/noteheads.s2\n/flags.u3\n⑤/noteheads.s2/noteheads.s2/noteheads.s2\nii/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s2\n/noteheads.s2/noteheads.s2\n/dots.dot\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2▼Y\n/accidentals.natural/noteheads.s2\n/dots.dot\nV/noteheads.s2/noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2 /noteheads.s2\n/noteheads.s2 /noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2 /noteheads.s2/noteheads.s2 /noteheads.s2\n④/noteheads.s2/noteheads.s2/noteheads.s2▼\n/timesig.C44/accidentals.flat/accidentals.flat/accidentals.flat/clefs.C/noteheads.s1/timesig.C44/accidentals.flat/accidentals.flat/accidentals.flat/clefs.G/noteheads.s1/timesig.C44/accidentals.flat/accidentals.flat/accidentals.flat/clefs.G/rests.3\n/noteheads.s2/brackettips.up\n/brackettips.down/noteheads.s2(6)\ni/timesig.C44/accidentals.flat/accidentals.flat/accidentals.flat/clefs.F③/rests.2③/rests.2③/rests.2/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\nPAC▲i/noteheads.s2②/noteheads.s2/noteheads.s2①/noteheads.s2▼Z\n/accidentals.naturalFigure 1 . Haydn, op. 17/4, iv, PAC at measure 8 (off-\nsetZ). Compare to Figure 1 of [21]. Features describe\nhere the constitution of the Z chord ( Z-in-perfect-triad ,Z-\nin-perfect-triad-or-sus4 ,Z-highest-is-1 ), voice leading to\nZ (Z-1-comes-from-7 1\r,Z-3-comes-from-4 2\r), rests af-\nter Z ( R-after-Z-rest-lowest ,-middle 3\r) and the metric\nstructure ( R-Z-strong-beat ). Features also describe rela-\ntions with chord Y ( Y -Z-bass-moves-compatible-V-I 4\r,Y -\nZ-bass-same-voice ), and cadence preparation ( X-Y -bass-\nmoves-2nd-Maj 5\r).\nNote that the heuristic choice of a single offset Y implies\nhere that the features Y -in-V7-3 andY -has-7 are not true,\neven if the dominant chord actually contain several pitches\n3and 7 (circled notes). Nevertheless, these pitches are\ncaught by the tonality features ( Z-bass-compatible-with-I ,\nZ-bass-compatible-with-I-scale ) and some of them are con-\nsidered by the voice leading features ( 1\r,2\r).\nvi/scripts.staccato/noteheads.s2/noteheads.s2/noteheads.s2/scripts.staccato/noteheads.s2▼\n/dots.dot/noteheads.s2 /noteheads.s2/accidentals.natural\n/noteheads.s2/flags.u3\n/flags.d3/noteheads.s2X\n/noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2\n/flags.u3\nii③\n/noteheads.s2 /noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2▼Y\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2 /noteheads.s2①/noteheads.s2▼Z\n/noteheads.s2\n/flags.u3/noteheads.s2\nHC▲V/noteheads.s2①/noteheads.s2①/timesig.C44/accidentals.sharp/clefs.G/noteheads.s2/timesig.C44/accidentals.sharp/clefs.G/noteheads.s2/noteheads.s2\n/noteheads.s2/brackettips.up\n/brackettips.down/dots.dot/accidentals.sharp(7)\n/timesig.C44/accidentals.sharp/clefs.F/noteheads.s2/timesig.C44/accidentals.sharp/clefs.C\n/rests.2/rests.2/noteheads.s2/rests.2\n/noteheads.s2 /noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2\n/flags.u3/flags.u3/flags.u3\n/noteheads.s2/noteheads.s2②②\n/rests.3/rests.3/scripts.staccato/noteheads.s2/rests.3\nFigure 2 . Haydn, op. 17/5, i, HC at measure 8. Fea-\ntures notably describe here a voice leading ( Z-6-comes-\nfrom-7 ,Z-4-comes-from-5 ,Z-1-comes-from-2 1\r) contin-\nued by a 6/4 suspension ( Z-6-moves-to-5 ,Z-4-moves-to-\n32\r). Other features also describe the tonality compatibil-\nity of an HC ( bass-Z-compatible-with-V , circled notes, but\nboth Z-bass-compatible-with-I andZ-bass-compatible-with-\nI-scale are also true due to the squared c#) as well as bass\nmovements ( Y’-Y -bass-moves-chromatic ,Y -Z-bass-moves-\n2nd-Maj 3\r) and the metric structure ( R-Z-strong-beat ).focused on melodic cadences [25] and a few studies have\ntackled the problem of the identiﬁcation of harmonic pro-\ngressions [16] or their representation as musical trajecto-\nries [1]. The authors of [7] took Rohrmeier’s works further,\nextending it into a system deriving harmonic relations be-\ntween chords, where grammars rules were inferred for jazz\nharmony. Currently, only a few algorithms recognize sim-\nple cadences [12]. We previously suggested a rule-based\ndetection of PAC/rIAC in fugues [9] and used it in a study\non the sonata form [2]. Recently, Sears and colleagues [22]\nused the software IDyOM [17] on a corpus of Haydn string\nquartets to show that music predictability increases at ca-\ndential points and decreases on the following note.\n1.3 Contents\nOur goal is to identify binary ,musical , and local features\nthat coincide with cadences and that can be used to train a\nmodel that detects new cadences, either PAC/rIAC or HC.\nRather than agnostically discovering cadential features on\nthe musical surface, we intend here to conﬁrm and study\ntraditional music theory knowledge regarding cadences.\nThe proposed strategy avoids chord segmentation, which\nis itself a difﬁcult MIR problem. Section 2 details the se-\nlected features and Section 3 describes the learning pro-\ncess. Finally, Sections 4 and 5 discuss the application of\nthe method on Bach and Haydn corpora.\n2. MUSICAL FEATURES AT THREE ONSETS\nEach beatZof the score is considered as the potential ar-\nrival point of a cadence. A set of 44 binary features is com-\nputed at each beat. These features are then used to train a\nclassiﬁer whose aims to predict whether a beat corresponds\nto the arrival point of a cadence or not. The features aim at\ndetecting cadences at a local level, i.e. the surroundings of\nthe cadential beat including its immediate past, presumably\ncorresponding to the preparation of the cadence. The idea\nis to try and detect SD- V-Iprogressions for a PAC/rIAC,\nand progressions ending with Vfor an HC.\nWhat we propose here is a simple heuristic focusing on\nthree speciﬁc onsets: Z,Y(Z)andX(Z), or for short Z,\nY, andX. Most of the features describe sets of notes\nsounding at these onsets (even when they begin before),\nnamely chord(Z) ,chord(Y) , and chord(X) . We therefore do\nnot start from a complete harmony analysis nor a chord\nsegmentation, that can be error-prone. Even when the\nmethods ﬁnding Y(Z)andX(Z)return approximate on-\nsets, the computed features may be relevant.\n2.1 Features on the Arrival Point Z or around it\nThe arrival chord of a cadence is usually a perfect triad,\npossibly with some suspensions. A ﬁrst set of features de-\nscribes this chord and its immediate neighborhood:\n\u000fZ-in-perfect-major-triad (respectively Z-in-perfect-\ntriad):chord(Z) is included inf1,3M,5g1(resp.\nf1,3m,3M,5g)\n1Pitches in underlined ﬁgures (i.e. 1,3, etc.) are here computed by the\ninterval modulo octave relative to the bass. As some chords are not in root356 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018/accidentals.natural/noteheads.s2 /noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.flat\n/noteheads.s2/accidentals.sharp\n/noteheads.s2\nV/noteheads.s2 /noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2\n/noteheads.s2\n/noteheads.s2/flags.d3/noteheads.s2\nI/noteheads.s2/noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2\nvi/noteheads.s2/noteheads.s2/rests.4\n/noteheads.s2/accidentals.naturalX\nii/noteheads.s2 /noteheads.s2/noteheads.s2▼\n/noteheads.s2\n/noteheads.s2/noteheads.s2\nV/noteheads.s2▼Y\n/noteheads.s2 /noteheads.s2\nV/V68/accidentals.sharp/clefs.F/noteheads.s268/accidentals.sharp/clefs.G/noteheads.s2/noteheads.s2/brackettips.up\n/brackettips.down/accidentals.sharp\n/flags.d3/flags.u3(81)\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\nPACI/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2▼Z\n▲/noteheads.s2 /noteheads.s2Figure 3 . Bach, fugue #15 in G major BWV860, PAC at\nmeasure 83. V oice leading and highlighted notes as in the\nFigures 1 and 2. To cope with the faster harmonic rhythm,\nevery eighth before Zis considered as a potential Y.\n\u000fZ-in-perfect-triad-or-sus4 :chord(Z) is included in\nf1,3,4,5g\n\u000fZ-is-sus4 :chord(Z) is exactlyf1,4,5g\n\u000fZ-highest-is-1 (resp. Z-highest-is-3 ): The highest\nnote of chord(Z) is the tonic 1(resp. the major or\nminor third 3), as expected for a PAC (rIAC)\nAnother set of features describes voice leadings from\npreceding notes (see list on Table 2). Z-\f-comes-from- \u000b\nmeans that the note \finchord(Z) is an “immediate reso-\nlution” of a note \u000b(the interval being still relative to the\nbass ofZ) that is exactly before \f(even if this note is not\nat the onsetYthat will be deﬁned later). For example, Z-3-\ncomes-from- 4means that there is a 3inchord(Z) that is an\nimmediate resolution of a 4(dominant seventh in the case\nof a PAC, see 2\ron Figure 1).\nThere can also be a suspension at the arrival point, as on\nthe HC on Figure 2. We thus add symmetrical features Z-\u000b-\nmoves-to- \f(see list on Table 2). For example, Z-4-moves-\nto-3means that there is a suspended fourth 4inchord(Z)\nthat is immediately resolved to the third 3.\nFinally, features try to grasp the tonality in the neighbor-\nhood ofZ. We do not perform tonality estimation [13, 23]\nbecause of the usual difﬁculty of algorithms to disam-\nbiguate adjacent tonalities in the circle of ﬁfths.\n\u000fZ-bass-compatible-with-I (resp. Z-bass-compatible-\nwith-V ): Both notes 4and7of the tonality that would\nbe implied if the bass of ZisI(resp. V) are present\nin the four beats before Z\n\u000fZ-bass-compatible-with-I-scale : The 8 previous\nbeats exhibits the whole scale of the same implied\ntonality – Temperley suggesting that such PACs with\nSD before V-Ifeel more conclusive [24].\nFor example, on the PAC of Figure 1, Z-bass-\ncompatible-with-I and-with-I-scale are true (and not -with-\nV), and on the HC of Figure 2, Z-bass-compatible-with-\nVis true. However, these features may be triggered by\nposition, these pitches may differ from the actual function. For example,\nthe top voice don offset X on Figure 1 is the sixth 6of the chord II6but\nis actually the tonic of the IIchord.other events close on the circle of ﬁfths: Both Z-bass-\ncompatible-with-I and-with-I-scale may be triggered by a\nprevious V/V (as on Figure 2) or, in minor, when Zis ac-\ntually a IIIin root position.\n2.2 Rhythmic Features around the Arrival Point Z\nThese textural features intend to detect either breaks or\ncontinuation in music.\n\u000fR-Z-strong-beat :Zis a strong beat (beat 1 and 3 for\n4/4, and beat 1 for other time signatures)\n\u000fR-Z-same-rhythm-1 (resp. R-Z-same-rhythm-2 ):\nThere is exactly the same sequence of durations in\nthe one (resp. the two) beat(s) preceding Zthan on\nthe one (resp. the two) beat(s) at onset Z\n\u000fR-Z-sustained-note : At least one note sounding at Z\nstarted before Z\n\u000fR-after-Z-rest-highest ,R-after-Z-rest-lowest ,R-after-\nZ-rest-middle : There is a rest in some voice right af-\nter the note at onset Z(see Figure 1)\n\u000fR-after-Z-one-voice-ends :Zis the last onset in at\nleast one voice (end of the piece)\n2.3 Features on the Point Y or around it\nFor each arrival beat Z, we identify a point Yprior toZ\nsupposed to pinpoint the chord “preceding” Z. For ex-\nample, identifying chord(Y) as a dominant chord is a sign\nindicating a potential PAC at Z. Although Vchords gen-\nerally span over more than one beat, associating Ywith a\nsingle beat eases the computation of features.\nWe thus propose to identify the point Yas the latest beat\nprecedingZfor which the bass voice includes a sounding\nnote, limited to one measure in the past. If the bass in-\ncludes a rest just before Z, we look just before. The usual\ntime span corresponding to the preparation of a cadence\ndepends on the harmonic rhythm and varies among musi-\ncal styles. The beat resolution to search the Ypoint should\ntherefore depend on the corpus.\n\u000fY -Z-offsets-at-most-1 :Yis at most one quarter note\nbeforeZ\nSome features are concerned with chord(Y) :\n\u000fY -has-7 (resp. Y -has-9 ):chord(Y) contains 7(resp.\n9), that is the leading tone (resp. the dominant sev-\nenth or the dominant ninth) in the case of a candidate\nPAC\n\u000fY -in-V7 :chord(Y) is included within a dominant\nseventh chord\n\u000fY -in-V7-3 :chord(Y) is included within a dominant\nseventh chord and contains a third\nOther features focus on bass moves:\n\u000fY’-Y -bass-moves-8ve : The bass note preceding Y is\nat the same pitch but with an octave jump (expected\non some VorV64 chords)Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 357pieces voices beats PAC (ﬁnal) rIAC HC\nhaydn-quartets Haydn string quartets [22] 42 expositions 4 7173 99 (21) (8) 70\nbach-wtc-i Bach fugues [9] 24 fugues 2 to 5 4739 63 (23) 24 (5)\nTable 1 . Corpora with manual annotations of cadences. Cadences are labeled at about 2% of the beats. We narrow to sets\nwith signiﬁcant number of cadences (PAC and HC for the Haydn corpus, PAC and PAC+rIAC for the Bach corpus).\n\u000fY’-Y -bass-moves-chromatic : The bass note preced-\ning Y is at a distance of one semitone (HC)\n\u000fY -Z-bass-moves-2nd-min (resp. Y -Z-bass-moves-\n2nd-Maj )\n\u000fY -Z-bass-same-voice : Bass notes of both chords are\non the same voice\n\u000fY -Z-bass-moves-compatible-V-I (resp. Y -Z-bass-\nmoves-compatible-I-V ): The bass moves by an as-\ncending fourth or descending ﬁfth (PAC) (resp. as-\ncending ﬁfth or descending fourth, HC I-V)\n2.4 Features on the Cadence Preparation (Point X)\nWe identify the onset X as the latest beat before Y whose\nlowest sounding note has a different pitch (modulo octave)\nthan the lowest note of Y . Features focus on this bass move:\n\u000fX-Y -bass-moves-2nd-min (V/V-V-I)\n\u000fX-Y -bass-moves-2nd-Maj (IV-V-IorII6-V-I)\n\u000fX-Y -bass-moves-4th (expected in II-V-I)\n3. CLASSIFICATION PROCESS\nA model is built in order to reﬂect the correlation between\nthe features listed in Section 2 and the occurrences of ca-\ndences in corpora. These corpora bear manual annotations\nindicating the position of PAC, rIAC and HC. Assuming\nthat the arrival points of cadences do not fall between beats,\neach beat (quarter note, or three eights depending on the\ntime signature) of each piece is described by:\n\u000favector of boolean values corresponding to the set\nof features and computed from the musical score,\n\u000fa boolean class specifying whether the beat is anno-\ntated in the reference as a PAC/rIAC/HC or not.\nThis way of representing data enables us to reformu-\nlate cadence detection as a classiﬁcation task . To avoid\noverﬁtting, each dataset is randomly divided into two sub-\nsets: a training set used to train a classiﬁer and a test\nsetleft to evaluate the classiﬁer performance at the end.\nThe classiﬁer and the value of its hyper-parameters have\nbeen selected by performing Leave-One-Piece-Out cross-\nvalidation over the training set. This is done by evaluat-\ning the classiﬁcation on each piece of the training set by\na classiﬁer trained on the remaining pieces of the training\nset. Indeed, the traditional Leave-One-Out (LOO) cross-\nvalidation approach that would consist in leaving only one\nbeat of one piece out of the training set would result here\nin overﬁtting due to intra-piece musical repetitions.4. EXPERIMENTS AND DISCUSSION\n4.1 Corpora and Implementations\nTable 1 shows the corpora which was used in this study.\nThe corpus bach-wtc-i includes the 24 fugues of the\nﬁrst book of the Well-Tempered-Clavier by J.-S. Bach. Ca-\ndence annotations were taken from our previous work [9].\nThe corpus haydn-quartets includes 42 expositions\nfrom movements of Haydn string quartets in sonata form,\nannotated with cadences by Sears and colleagues [22].\nEven if these annotated corpora model cadences in the light\nof a global analysis of the form, we have used them as a\nbenchmark on our local feature-based detection. Only a\nminority of annotated PAC are ﬁnal in the sense that they\nare included in the last four measures of the piece (or of\nthe exposition).\nPieces were downloaded as voice-separated .krn ﬁles\nfromkern.ccarh.org [11]. Note that the features pro-\nposed here could also apply to non-separated ﬁles, except\nforafter-Z-rest-* and Y -Z-bass-same-voice . In this case,\nfeatures on voice leading would only check that the com-\ning note or the suspended note is found at the right place\nin the polyphonic texture.\nFor each beat Z(and their related onsets X,Y), the fea-\ntures described in Section 2 are extracted using code based\non the Python framework music21 [6]. Points Y and X\nare searched at a beat resolution of a quarter note (Haydn)\nor eight note (Bach, see Figure 3). Classiﬁers were com-\nputed thanks to the scikit-learn framework [8].\n4.2 Discussion on Feature Statistics\nTable 2 shows tallies of features, their correlation with\ncadences as well as an estimation of their signiﬁcance.\nMany features are signiﬁcant in both corpora, despite dif-\nferences in musical style. Unsurprisingly, features R-\nZ-strong-beat ,Y -Z-bass-moves-compatible-V-I ,Z-perfect-\ntriad-or-sus4 and Z-highest-note-is-1 are activated nearly\nfor every PAC. Note that PAC lacking the ﬁfth leap are the\nones where the bass passes by another note before tonic\nresolution. Rhythmic and break features are also quite sig-\nniﬁcant. Some features differ between corpora. For exam-\nple,R-Z-sustained-note is absent in nearly all PACs in the\nHaydn corpus, whereas it can be found in some PACs in\nBach fugues due to the contrapuntal writing.\nWe were expecting to ﬁnd more suspensions for both\nPAC and HC as a way to retain tension before the ultimate\nresolution but they do not appear signiﬁcantly in these cor-\npora. We also notably lack strong signiﬁcant features for\nHC. Indeed, the Y-Z bass move in a HC is variable (it is\ntypically similar to X-Y moves in PAC).358 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018bach-wtc-i haydn-quartets\nFeatures beats PAC rIAC beats PAC HCRythmic features RR-Z-strong-beat 1920 60\u0003/ 25 24\u0003/ 9 3126 98\u0003/ 43 70\u0003/ 30\nR-Z-same-rhythm-1 394 1 / 5 \u0001/ 1 1254 2\u0003/ 17 2\u0003/ 12\nR-Z-same-rhythm-2 176 \u0001/ 2 \u0001/ 0 448 0/ 6 1/ 4\nR-Z-sustained-note 2341 14\u0003/ 31 5/ 11 2521 1\u0003/ 34 8\u0003/ 24\nR-after-Z-rest-highest 166 14\u0003/ 2 1/ 0 501 56\u0003/ 6 10/ 4\nR-after-Z-rest-middle 477 22\u0003/ 6 9\u0003/ 2 1227 72\u0003/ 16 35\u0003/ 11\nR-after-Z-rest-lowest 194 15\u0003/ 2 13\u0003/ 0 1130 59\u0003/ 15 34\u0003/ 11\nR-after-Z-one-voice-ends 180 19\u0003/ 2 2/ 0 \u0001 \u0001 / 0 \u0001/ 0Arrival point ZZ-in-perfect-major-triad 1167 43\u0003/ 15 12/ 5 2760 94\u0003/ 38 53\u0003/ 26\nZ-in-perfect-triad 1819 56\u0003/ 24 19\u0003/ 9 3256 97\u0003/ 44 53\u0003/ 31\nZ-in-perfect-triad-or-sus4 2078 62\u0003/ 27 20\u0003/ 10 3434 97\u0003/ 47 55\u0003/ 33\nZ-is-sus4 680 20\u0003/ 9 1/ 3 1308 14 / 18 4/ 12\nZ-highest-is-1 592 55\u0003/ 7 1/ 2 1765 96\u0003/ 24 19/ 17\nZ-highest-is-3 1488 1\u0003/ 19 21\u0003/ 7 1596 1\u0003/ 22 28\u0003/ 15\nZ-bass-compatible-with-I 1724 63\u0003/ 22 23\u0003/ 8 2279 98\u0003/ 31 56\u0003/ 22\nZ-bass-compatible-with-V 1265 8/ 16 4/ 6 1616 3\u0003/ 22 44\u0003/ 15\nZ-bass-compatible-with-I-scale 1902 63\u0003/ 25 22\u0003/ 9 2104 91\u0003/ 29 46\u0003/ 20\nZ-1-comes-from-7 663 52\u0003/ 8 15\u0003/ 3 1016 89\u0003/ 14 30\u0003/ 9\nZ-1-comes-from-1 180 13\u0003/ 2 1/ 0 828 9 / 11 0\u0003/ 8\nZ-1-comes-from-2 523 23\u0003/ 6 7/ 2 893 65\u0003/ 12 27\u0003/ 8\nZ-3-comes-from-4 1078 25 / 14 16\u0003/ 5 1488 72\u0003/ 20 45\u0003/ 14\nZ-4-comes-from-5 197 4 / 2 \u0001/ 0 291 \u0001/ 4 9/ 2\nZ-5-comes-from-5 153 9\u0003/ 2 \u0001/ 0 769 2/ 10 13/ 7\nZ-5-comes-from-6 510 1 / 6 2/ 2 495 0/ 6 9/ 4\nZ-6-comes-from-7 200 \u0001/ 2 \u0001/ 1 130 \u0001/ 1 \u0001/ 1\nZ-2-moves-to-1 57 \u0001/ 0 \u0001/ 0 90 2 / 1 1/ 0\nZ-4-moves-to-3 160 2 / 2 1/ 0 340 2 / 4 11\u0003/ 3\nZ-6-moves-to-5 138 1 / 1 \u0001/ 0 180 \u0001/ 2 8\u0003/ 1\nZ-7-moves-to-1 7 \u0001/ 0 \u0001/ 0 105 2 / 1 1/ 1Point YY -in-V7 1267 52\u0003/ 16 17\u0003/ 6 3290 81\u0003/ 45 15\u0003/ 32\nY -in-V7-3 721 44\u0003/ 9 14\u0003/ 3 2413 69\u0003/ 33 14/ 23\nY -has-7 554 22\u0003/ 7 7/ 2 767 66\u0003/ 10 8/ 7\nY -has-9 607 1/ 8 4/ 3 486 2 / 6 5/ 4\nY -Z-offsets-at-most-1 4525 63 / 60 24/ 22 5668 90 / 78 66\u0003/ 55\nY -Z-bass-same-voice 4270 63 / 56 24/ 21 5297 98\u0003/ 73 70\u0003/ 51\nY -Z-bass-moves-2nd-min 1313 0\u0003/ 17 0\u0003/ 6 1328 1\u0003/ 18 35\u0003/ 12\nY -Z-bass-moves-2nd-Maj 880 0\u0003/ 11 \u0001/ 4 559 0\u0003/ 7 28\u0003/ 5\nY -Z-bass-moves-compatible-I-V 125 1 / 1 \u0001/ 0 448 2 / 6 6/ 4\nY -Z-bass-moves-compatible-V-I 512 62\u0003/ 6 23\u0003/ 2 578 95\u0003/ 7 6/ 5\nY’-Y -bass-moves-chromatic 1139 6/ 15 2/ 5 2050 10\u0003/ 28 33/ 20\nY’-Y -bass-moves-8ve 193 29\u0003/ 2 7\u0003/ 0 522 22\u0003/ 7 6/ 5Point XX-Y -bass-moves-2nd-min 433 2 / 5 1/ 2 1060 10 / 14 10/ 10\nX-Y -bass-moves-2nd-Maj 568 25\u0003/ 7 12\u0003/ 2 803 65\u0003/ 11 5/ 7\nX-Y -bass-moves-4th 670 11 / 8 4/ 3 1626 4\u0003/ 22 9/ 15\nTotal 4739 63 24 7173 99 70\nTable 2 . Feature tallies for PAC (both corpora), rIAC (Bach corpus) and HC (Haydn corpus). The table shows, for\neach feature, the number of beats where this feature occurs ( allbeats, cadential points or not), followed by its number\nof occurrences on beats labeled as cadences in the reference annotation, as well as, in small, its expected number should\nthe feature be random and uniformly distributed across the beats. ( \u0001means 0, and not signiﬁcant). For example, there\nare 70 HC out of 7173 beats in the Haydn quartets corpus. There are 35 beats corresponding to a HC with the feature\nY -Z-bass-2nd-min , out of 1328 beats with this feature, and compared to only 12 beats should this feature be random.\nFor each feature and each cadence type, p-values are estimated by an exact Fisher test computed by the Python scipy\npackage. Fisher tests are computed independently. To account for the large number of tests, only features with p-values\nunder .001 ( bold,\u0003) can be considered as signiﬁcant, either by their absence ( italic ) or their presence. For example, the\nfeature Y -Z-bass-2nd-min is signiﬁcantly absent in PACs of both corpora ( p < 10\u00007) and signiﬁcantly present in HCs of\nthe Haydn corpus ( p<10\u00008).Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 359/dots.dot/dots.dot\n/noteheads.s1\nii/noteheads.s2/noteheads.s2Y\n▼\n/noteheads.s2\n/noteheads.s1\n/noteheads.s1/noteheads.s1\n/noteheads.s1\nV/dots.dot/rests.2/noteheads.s2\n/rests.2\n/rests.2/rests.2\n/rests.2\n/rests.2\n/rests.2/noteheads.s2/noteheads.s2X\n▼\n/noteheads.s2\n/noteheads.s1/noteheads.s2\n/noteheads.s1/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2\n/noteheads.s2/accidentals.natural\n/dots.dot/noteheads.s2/noteheads.s2\n/dots.dot\n/dots.dot/dots.dot/accidentals.natural\nI/accidentals.naturalZ\n▼\n/noteheads.s1\n/noteheads.s1\n/noteheads.s1\n/rests.2\n▲\nPAC EEC\n(Sears)/noteheads.s2/noteheads.s2\n/noteheads.s2\n/noteheads.s2\n/noteheads.s2/accidentals.natural/noteheads.s1\n/noteheads.s1\n/noteheads.s1\n/rests.2/accidentals.flat/accidentals.flat34\n/noteheads.s1/clefs.C/accidentals.flat/accidentals.flat34\n/noteheads.s1/clefs.F/accidentals.flat/accidentals.flat34\nI/clefs.G64\n/dots.dot/brackettips.up\n/brackettips.down/noteheads.s2/clefs.G/accidentals.flat/accidentals.flat34\n/noteheads.s1/noteheads.s1\n/noteheads.s1\n/noteheads.s1\n/rests.2/dots.dot\n/dots.dot▼/noteheads.s2\n/noteheads.s2Y\n/noteheads.s2/noteheads.s2/accidentals.natural/noteheads.s2\n/dots.dotZ\n▼\n/noteheads.s2\n/noteheads.s2\n/noteheads.s2\n/noteheads.s2\nI\n▲\nPAC ?/noteheads.s2\n/noteheads.s2\n/noteheads.s2/noteheads.s2\n/accidentals.natural\n/noteheads.s2X\n▼\n/accidentals.natural\n/dots.dot\nV/noteheads.s1\n/noteheads.s1\n/noteheads.s1\n/noteheads.s1Figure 4 . Haydn, op. 55/3, i, potential PACs at m67 and\nm71. The PAC at m67 is hard to detect with the silence\nat the bass. In their global analysis of the form, Sears et.\nal see the end of the secondary theme (called the EEC, for\nEssential Expositional Closure by [10]) at m67 and discard\nany further PAC in the following concluding section [22]:\nThe PAC candidate at m71, found by the proposed strategy,\nis thus counted here as a FP. It could be debated whether\nthe EEC is indeed at m67 (ﬁrst cadential I, but weakened by\nthe bass rest) or rather at m71 (cadential feeling augmented\nby the following rests and bass note on upbeat, m67 con-\nsidered as an evaded cadence).\n4.3 Learning Process\nA linear Support Vector Machine (SVM) classiﬁer was\ntrained on each training set as explained in Section 3,\nsplitting the feature space with a hyperplane [5]. As\ndatasets are unbalanced (about 98% of the beats are “non-\ncadential”), we assigned stronger weights to data belong-\ning to the under-represented class, here the cadential beats.\nOther classifying algorithms such as k-nearest-neighbor or\ndecision trees were tested and turned out to provide com-\nparable or inferior results.\n4.4 Discussion on Detection Results\nTable 3 shows the comparison between the predictions of\neach classiﬁer on the test set of each corpus and the ref-\nerence annotations. The detection of PAC is good, with\nmore than 75% PAC detected and a low false positive rate\n(<1%). Note that we previously reported 82% of PAC de-\ntection in fugues [9] but with manual hard-coded rules that\nmay have resulted in overﬁtting.\nFalse positives (FP) beats may still have many cadential\nfeatures. An inspection of the 28 PAC reported as FP in the\nHaydn corpus shows that at least 5 FP can be seen as actual\ncadences, for example measure 71 in Haydn op. 55/3, i,\nshown on Figure 4. Other notable sources of FP are tonic\nchords following actual HC cadences activating signiﬁcant\nfeatures for PAC. The same Figure 4 shows an example of\nFN, where a silence in the bass makes the computation of\nmany features fail.\nAdding rIAC (Bach corpus) lowers the results, but there\nmay be too few such cadences to efﬁciently build the\nmodel. The detection of HC is difﬁcult (Haydn corpus),\nas there is not a single feature applicable to every case.\nHalf of them are detected, with about 2% FP.beats ref TP FP FN F1\nhaydn-quartets PAC 3583 51 42 28 9 0.69\n(21 quatuors) HC 3583 32 18 73 14 0.29\nbach-wtc-i PAC 2357 36 26 3 10 0.80\n(12 fugues) PAC+rIAC 2357 46 30 12 16 0.68\nTable 3 . Detection of cadences on the test sets of both cor-\npora using all features: Number of beats annotated in the\nreference annotation (ref), true positives (TP), false posi-\ntives (FP), false negatives (FN), and F1measure (harmonic\nmean of the recall and the precision).\nhaydn-quartets bach-wtc-i\nPAC HC PAC PAC+rIAC\nAll features XYZR 0.69 0.29 0.80 0.68\nFeatures YZR 0.69 0.27 0.71 0.68\nFeatures ZR 0.59 0.24 0.52 0.34\nFeatures XYZ 0.72 0.25 0.74 0.54\nTable 4 .F1measure while detecting cadences on the test\nsets of both corpora with different sets of features.\nTable 4 further studies these results while varying the\nset of considered features. Some features in Zalready\nconsider the past. Nevertheless, the features around Yare\nessential to improve the overall detection. Features on X\nbring a small but signiﬁcant gain for PAC. Rhythmic fea-\ntures (R) bring an improvement especially for HC, in par-\nticular with R-Z-strong-beat that correctly ﬁlters out more\nthan half of the beats.\n5. CONCLUSION\nDifferent musical clues give the cadential impression of a\n“breath in music”. We evaluated cadential features on and\naround three onsets at the arrival and in the preparation\nof cadences. Without performing any chord segmentation,\nthese features describe the underlying harmony, the voice\nleading as well as structural aspects of the music.\nThese features reﬂect common knowledge of music: we\nhave shown that some of them are speciﬁc to cadential\npoints. They make it possible to learn how to predict ca-\ndences – PAC/rIAC, and, to a lesser extent, HC – in corpora\nwith reference annotations. Such features may also be used\nin other systematic musicology approaches.\nPerspectives include the extension of our set of features\nto cadential and non-cadential positions. Some features\ncould be not necessarily theory driven and could possi-\nbly have metric values. Coupled with automatic selection,\nthis could lead to the discovery of signiﬁcant but unex-\npected features. More generally, the method used to iden-\ntify points X and Y could be compared to other heuris-\ntics. Cadence preparations could for example be described\nby features regarding contiguous “spans” of onsets rather\nthan single onsets X and Y , in order to improve the har-\nmony relevance of the model. Research along these lines\ncould signiﬁcantly improve HC detection.360 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Acknowledgements. The authors are grateful to Robin Alais,\nMikaela Keller, Marie Perrier, and the Algomus team for fruit-\nful discussions and proofreading of the manuscript. We also\nthank the anonymous reviewers for their insightful comments.\nThis work is partially funded by French CPER MAuVE (ERDF,\nR´egion Hauts-de-France) and by a grant from the French Re-\nsearch Agency (ANR-11-EQPX-0023 IRDIVE).\n6. REFERENCES\n[1] Louis Bigo, Jean-Louis Giavitto, Moreno Andreatta,\nOlivier Michel, and Antoine Spicher. Computation and\nvisualization of musical structures in chord-based sim-\nplicial complexes. In Mathematics and Computation in\nMusic (MCM 2013) , pages 38–51, 2013.\n[2] Louis Bigo, Mathieu Giraud, Richard Groult, Nico-\nlas Guiomard-Kagan, and Florence Lev ´e. Sketching\nsonata form structure in selected classical string quar-\ntets. In International Society for Music Information Re-\ntrieval Conference (ISMIR 2017) , 2017.\n[3] Ann Blombach. Phrase and cadence: A study of termi-\nnology and deﬁnition. Journal of Music Theory Peda-\ngogy , 1:225–51, 1987.\n[4] William E. Caplin. The classical cadence: Conceptions\nand misconceptions. Journal of the American Musico-\nlogical Society , 57:51–117, 2004.\n[5] Corinna Cortes and Vladimir Vapnik. Support-vector\nnetworks. Machine learning , 20(3):273–297, 1995.\n[6] Michael Scott Cuthbert and Christopher Ariza. mu-\nsic21: A toolkit for computer-aided musicology and\nsymbolic music data. In International Society for Mu-\nsic Information Retrieval Conference (ISMIR 2010) ,\npages 637–642, 2010.\n[7] W. Bas De Haas, Martin Rohrmeier, Remco C.\nVeltkamp, and Frans Wiering. Modeling harmonic\nsimilarity using a generative grammar of tonal har-\nmony. In International Society for Music Information\nRetrieval Conference (ISMIR 2009) , pages 549–554,\n2009.\n[8] Fabian Pedregosa et al. Scikit-learn: Machine learn-\ning in Python. Journal of Machine Learning Research ,\n12:2825–2830, 2011.\n[9] Mathieu Giraud, Richard Groult, Emmanuel Leguy,\nand Florence Lev ´e. Computational fugue analysis.\nComputer Music Journal , 39(2), 2015.\n[10] James Hepokoski and Warren Darcy. Elements of\nSonata Theory: Norms, Types, and Deformations in\nthe Late-Eighteenth-Century Sonata . Oxford Univer-\nsity Press, 2006.\n[11] David Huron. Music information processing using the\nHumdrum toolkit: Concepts, examples, and lessons.\nComputer Music Journal , 26(2):11–26, 2002.[12] Pl ´acido R. Illescas, David Rizo, and Jos ´e M. I ˜nesta.\nHarmonic, melodic, and functional automatic analysis.\nInInternational Computer Music Conference (ICMC\n2007) , pages 165–168, 2007.\n[13] Carol L. Krumhansl and Edward J. Kessler. Tracing the\ndynamic changes in perceived tonal organisation in a\nspatial representation of musical keys. Psychological\nReview , 89(2):334–368, 1982.\n[14] Nathan John Martin and Julie Pedneault-Deslauriers.\nThe Mozartean Half Cadence , pages 185–213. In\nNeuwirth and Berg ´e [15], 2015.\n[15] Markus Neuwirth and Pieter Berg ´e, editors. What Is a\nCadence? Theoretical and Analytical Perspectives on\nCadences in the Classical Repertoire . Leuven Univer-\nsity Press, 2015.\n[16] Jean-Franc ¸ois Paiement, Douglas Eck, and Samy Ben-\ngio. A probabilistic model for chord progressions. In\nInternational Conference on Music Information Re-\ntrieval (ISMIR 2005) , 2005.\n[17] Marcus Thomas Pearce. The construction and evalua-\ntion of statistical models of melodic structure in music\nperception and composition . PhD thesis, City Univer-\nsity London, 2005.\n[18] Martin Rohrmeier and Markus Neuwirth. Towards a\nSyntax of the Classical Cadence , pages 287–338. In\nNeuwirth and Berg ´e [15], 2015.\n[19] Janet Schmalfeldt. Cadential processes: The evaded\ncadence and the “one more time” technique. Journal\nof Musicological Research , 12(1-2):1–52, 1992.\n[20] David Sears, William E. Caplin, and Stephen\nMcAdams. Perceiving the classical cadence. Music\nPerception , 31(5):397–417, 2014.\n[21] David R. W. Sears, Andreas Arzt, Harald Frostel,\nReinhard Sonnleitner, and Gerhard Widmer. Model-\ning harmony with skip-grams. In International Soci-\nety for Music Information Retrieval Conference (IS-\nMIR 2017) , pages 332–338, 2017.\n[22] David R. W. Sears, Marcus T. Pearce, William E.\nCaplin, and Stephen McAdams. Simulating melodic\nand harmonic expectations for tonal cadences using\nprobabilistic models. Journal of New Music Research ,\n47(1):29–52, 2018.\n[23] David Temperley. What’s key for key ? the Krumhansl-\nSchmuckler key-ﬁnding algorithm reconsidered. Music\nPerception , 17(1):65–100, 1999.\n[24] David Temperley. Music and probability . The MIT\nPress, 2007.\n[25] Peter van Kranenburg and Folgert Karsdorp. Cadence\ndetection in western traditional stanzaic songs using\nmelodic and textual features. In International Society\nfor Music Information Retrieval Conference (ISMIR\n2014) , pages 391–396, 2014.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 361"
    },
    {
        "title": "Timbre Discrimination for Brief Instrument Sounds.",
        "author": [
            "Francesco Bigoni",
            "Sofia Dahl"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492361",
        "url": "https://doi.org/10.5281/zenodo.1492361",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/249_Paper.pdf",
        "abstract": "Timbre discrimination, even for very brief sounds, allows identification and separation of different sound sources. The existing literature on the effect of duration on timbre recognition shows high performance for remarkably short time window lengths, but does not address the possible effect of musical training. In this study, we applied an adaptive procedure to investigate the effect of musical training on individual thresholds for instrument identification. A timbre discrimination task consisting of a 4-alternative forced choice (4AFC) of brief instrument sounds with varying duration was assigned to 16 test subjects using an adaptive staircase method. The effect of musical training has been investigated by dividing the participants into two groups: musicians and non-musicians. The experiment showed lowest thresholds for the guitar sound and highest for the violin sound, with a high overall performance level, but no significant difference between the two groups. It is suggested that the test subjects adjust the weightings of the perceptual dimensions of timbre according to different degrees of acoustic degradation of the stimuli, which are evaluated both by plotting extracted audio features in a feature space and by considering the timbral specificities of the four instruments.",
        "zenodo_id": 1492361,
        "dblp_key": "conf/ismir/BigoniD18",
        "keywords": [
            "Timbre discrimination",
            "Identification and separation",
            "Musical training",
            "Adaptive procedure",
            "Individual thresholds",
            "Instrument sounds",
            "Varying duration",
            "Four-alternative forced choice",
            "Musicians",
            "Non-musicians"
        ],
        "content": "TIMBRE DISCRIMINATION FOR BRIEF INSTRUMENT SOUNDS\nFrancesco Bigoni\nSound and Music Computing\nAalborg University – Copenhagen\nfbigon17@student.aau.dkSoﬁa Dahl\nDept. of Architecture, Design and Media Technology\nAalborg University – Copenhagen\nsof@create.aau.dk\nABSTRACT\nTimbre discrimination, even for very brief sounds, allows\nidentiﬁcation and separation of different sound sources.\nThe existing literature on the effect of duration on timbre\nrecognition shows high performance for remarkably short\ntime window lengths, but does not address the possible ef-\nfect of musical training. In this study, we applied an adap-\ntive procedure to investigate the effect of musical training\non individual thresholds for instrument identiﬁcation. A\ntimbre discrimination task consisting of a 4-alternative for-\nced choice (4AFC) of brief instrument sounds with varying\nduration was assigned to 16 test subjects using an adaptive\nstaircase method. The effect of musical training has been\ninvestigated by dividing the participants into two groups:\nmusicians and non-musicians. The experiment showed lo-\nwest thresholds for the guitar sound and highest for the\nviolin sound, with a high overall performance level, but no\nsigniﬁcant difference between the two groups. It is sug-\ngested that the test subjects adjust the weightings of the\nperceptual dimensions of timbre according to different de-\ngrees of acoustic degradation of the stimuli, which are eva-\nluated both by plotting extracted audio features in a fea-\nture space and by considering the timbral speciﬁcities of\nthe four instruments.\n1. INTRODUCTION\nTimbre is a primary vehicle for sound source recognition\nand, from a cognitive perspective, sound identity [10]. The\nauditory system is designed to identify sound sources: this\nenables us to discern a melody in a complex soundscape,\nfollow what is being said by a speaker, or step aside when\nsomething fast and dangerous appears to be approaching.\nAs an example which is more related to music consump-\ntion, listeners are able to identify musical genres better\nthan chance in a fraction of a second (the shortest duration\ntested is 125 ms [9]).\nAlthough so important to our auditory system, timbre is\noften deﬁned in a negative manner — as, for instance, in\nPlomp’s (1970) operational deﬁnition: “Timbre is that at-\ntribute of sensation in terms of which a listener can judge\nc\rFrancesco Bigoni, Soﬁa Dahl. Licensed under a Crea-\ntive Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Francesco Bigoni, Soﬁa Dahl. “Timbre Discrimination for\nBrief Instrument Sounds”, 19th International Society for Music Informa-\ntion Retrieval Conference, Paris, France, 2018.that two steady complex tones having the same loudness,\npitch and duration are dissimilar” (quoted in [12]). Tim-\nbre can be described as a set of perceptual attributes which\nare either continuously varying (timbral semantics such as\nattack sharpness, brightness, richness) or discrete (percep-\ntual features such as the pinched offset of a harpsichord\nsound) [10]. For the former category of attributes, a num-\nber of objective acoustic correlates can generally be found\namong spectro-temporal audio features, e.g. spectral cen-\ntroid, attack time and spectral envelope; for the latter, the\nobjective correlates are harder to identify [10].\nBeing complex and multidimensional, timbre is usually\nmodelled employing a so-called multidimensional scaling\n(MDS), i.e. ﬁtting the dissimilarity ratings given by a group\nof listeners on a set of sounds to a timbre space of per-\nceptual attributes and respective acoustic correlates [10].\nWhile the basic MDS model assumes the same percep-\ntual dimensions for all listeners, more recent models (e.g.\nCLASCAL by McAdams et al. [11]) account for different\nweightings of the perceptual dimensions (by individual lis-\nteners or classes of listeners) and for the effect of the fea-\ntures that are speciﬁc to an individual timbre, called “spe-\nciﬁcities” (basically related to the aforementioned discrete\nfeatures).\nThe studies that evaluate the effect of brief duration on\ntimbre perception exhibit a decidedly different approach\nfrom MDS research: quoting Suied et al., MDS models\naim at ﬁnding the most prominent perceptual dimensions\nof speciﬁc sounds through dissimilarity ratings, whereas\nthe problem of timbre recognition for brief sounds asks to\nidentify the most informative ones [21]. Inside this ﬁeld,\nthe prevalent area of interest is speech: in a seminal paper\nfrom 1942, Gray investigated phoneme cues for short vo-\nwel sounds, and coined the term “phonemic microtomy”\n[5]. More recently, Robinson and Patterson found that tim-\nbral cues for brief vowel stimuli are not pitch-assisted [18].\nGenerally, the measured performance is above chance for\ndurations as short as a single glottal pulse cycle, on the\norder of 3 ms.\nOnly a few studies deal with non-speech sounds: Clark\net al. asked their test subjects to identify orchestral instru-\nments for varying window length and position for gated sti-\nmuli [3]; Robinson and Patterson replicated their previous\nstudy using synthesized instrument sounds, achieving slig-\nhtly lower performance levels than for voice stimuli [17].\nIn later articles, the sound recognition problem has been\nstated in different terms, by taking the subjective reaction128times rather than the temporal thresholds of the stimuli into\naccount [1, 22].\nIn 2014, Suied et al. published what we consider by far\nthe most exhaustive contribuition on the topic, as well as\nthe most relevant reference for our paper [21]. In a series\nof timbre discrimination experiments, participants were as-\nked to identify whether a sound belonged to a target cate-\ngory (e.g. strings, percussion, voice) or a distractor cate-\ngory. Very short duration thresholds were found, on the or-\nder of 8-16 ms. The best performance was for voice, follo-\nwed by percussion (marimba and vibraphone). Subsequent\nexperiments rejected the effect of feedback and training on\nthe performance for the voice stimuli; and, ﬁnally, demon-\nstrated that source recognition based on timbral cues is fast\nand robust to stimulus degradation, with a clear advantage\nfor voice signals.\nWhile it may not be surprising that humans are highly\ntrained to identify sounds as belonging to the “voice” ca-\ntegory, one could expect more variability in the exposure\nto instrumental sounds. Suied et al. [21] did not report any\ninformation regarding the musical training of their partici-\npants. We would expect that listeners who are trained as\nmusicians would exhibit lower threshold values compared\nto non-musicians.\nPrevious studies [17, 21] have used constant stimuli\nlengths, with durations that are doubled. The increasing\ndifferences in durations help to reduce the test time, but\nalso make it difﬁcult to pinpoint where and how thres-\nholds differ between individuals or groups of listeners. We\nexpect musically trained and untrained listeners to differ\nin the overall threshold of instrument discrimination, but\nthere may be an interaction with the instrument type. Suied\net al. [21] found a lower performance for the “strings” ca-\ntegory compared to “percussion”. In order to efﬁciently\ntarget the listeners’ individual thresholds, an adaptive ap-\nproach is an attractive alternative.\nIn this paper, we investigate whether musical training\nhas an effect on the perceptual interaction between timbre\nand duration through a timbre discrimination task, using\nbrief sounds of varying length. Our goals were threefold:\n1) applying an adaptive staircase method to estimate the\ntemporal thresholds of timbre discrimination for a small\nsound set (four instruments: guitar, clarinet, trumpet and\nviolin); 2) determining if musical training has an effect on\nthe task; 3) relating the degradation of timbre descriptors\n(caused by the length shortening) to the perceptual adapta-\ntion strategies of the participants.\n2. METHOD\nWe anticipated the range of thresholds to vary between par-\nticipants, and therefore opted for an adaptive test proce-\ndure. Adaptive methods are designed to be time-efﬁcient\nand focus the presentation of stimuli around the percep-\ntual threshold of interest by adapting the level of presenta-\ntion according to the past responses of the participant (in\nour case, the indication of the heard instrument). Com-\npared to the method of constant stimuli, the adaptive pro-\ncedure can quickly move from presenting clearly audible\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\nTime [ms]00.20.40.60.811.2Window gainFigure 1 . Time window employed for stimulus gating (in\nthis case, the window length is 5 ms).\nand distinguishable stimuli to a range where performance\nis more difﬁcult. By gradually decreasing the step size after\na change in test subject performance, the method allows to\nzoom in rapidly on the threshold level. Depending on the\ncriteria for changes in presentation level and step size, the\nadaptive procedure can be designed to target different per-\nformance levels on the psychometric function (see [7] for\nan overview). For this study, we chose the simple up-down\nstaircase method [8], as this does not require assumptions\non the shape of the psychometric function.\n2.1 Stimuli\nThe four stimuli (guitar, clarinet, trumpet and violin) were\npicked from an existing database of anechoic recordings\nof acoustic instruments [20] [23]. The audio ﬁles were re-\ncorded at a sample rate of 48 kHz and a resolution of 24\nbits, using a 32-channel microphone array. The audio edi-\nting was performed in the digital audio workstation Rea-\nper. Four source ﬁles were created by mixing down the re-\nspective 32 channels to a mono track (with no instrument-\nspeciﬁc mix), bounced at 16-bit/44.1 kHz. In the source\nﬁles, the instrumentalists are playing a C4 at a ffdynamic.\nThe pitch of the source ﬁles was already normalized, as the\ninstruments were all tuned at A4 = 443 Hz1. Sounds were\nloudness-normalized to -18 LUFS using the SWS exten-\nsion in Reaper. The sound snippets were prepared on the\nﬂy in MATLAB between the presentations, by applying a\nsuitable window (i.e. a rectangular window with 4 samples\nof silence at the start and a 1 ms equal-power fade-out at\nthe end) of the required duration, starting from time 0: an\nexample is shown in Figure 1. Thus, onset information has\nbeen included in each snippet.\n2.2 Participants\nA convenience sample of 16 participants was tested, con-\nsisting of 13 males and 3 females with ages ranging from\n22 to 50 (\u0016age = 32 ,\u001bage = 9) recruited through aut-\nhor Bigoni’s personal network. Participants indicated their\nage and sex (if willing to disclose) and whether they had\nnormal hearing (no testing was made to assess this); they\n1This gives a fundamental frequency of 443=2(9=12)= 263 :41Hz\nat C4.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 129were informed that their personal data would be anonymi-\nzed. Each test subject was assigned to one of two groups\n(musicians ornon-musicians ) by asking if he/she had 5 or\nmore years of formal musical training and/or performance\nexperience. This left the border between the two groups\nsomewhat ﬂexible, giving the option to music students and\namateur musicians to choose their group according to their\nconﬁdence level. The groups were fairly balanced with re-\nspect to sample size: 9 musicians and 7 non-musicians. Of\nthe 9 musicians, 5 are primarily performing on wind in-\nstruments, whereas the other 4 play different combinations\nof guitar, piano, drums and electronics. Despite this inter-\ngroup difference, we do not assume that any of the subjects\nwere biased towards a speciﬁc instrument type.\n2.3 Setup\nThe playback system consisted of the laptop internal sound\ncard, driven with ASIO4ALL drivers for Windows, and a\npair of Beyerdynamic DT 990 Pro, 250 Ohm headphones.\nEven though the DT 990 Pro do not have a ﬂat frequency\nresponse, we assume that the sound coloration introduced\nby the headphones did not alter the relative timbre percep-\ntion.\n2.4 Procedure\nThe experimental setup was implemented in MATLAB. It\nfeatures a simple GUI and consists of three steps: 1) cre-\nation of a test subject entry in a database; 2) soundcheck:\nthe subject can click on four buttons to play the source ﬁ-\nles (guitar, clarinet, trumpet, violin) while adjusting the he-\nadphones volume to a comfortable level. The soundcheck\nalso constitutes a small training session on the four tim-\nbres, to avoid confusion at the beginning of the discrimina-\ntion task; 3) timbre discrimination task: an adaptive stair-\ncase method (simple up-down) with four interleaved tracks\n(the four instrument timbres). For each sound presentation,\nthe participants made a 4-alternative forced choice (4AFC)\ntest. For each track, the following procedure applied: when\na participant correctly identiﬁed the instrument, the du-\nration of the next presentation (of the same instrument)\nwould be reduced by the step size (initially, 40 ms); on\nthe other hand, the duration of the next presentation would\nbe augmented by the step size after a wrong answer. In\nthe literature, right and wrong answers usually get repre-\nsented by positive (+) and a negative (-) signs respectively.\nIn the light of this notation, a runconsists of a sequence\nof presentations that get answers of the same sign, and a\nreversal occurs at each change of sign. Thus, after the ﬁrst\nmisidentiﬁcation, the ﬁrst reversal would occur, the ﬁrst\nrun would end, the step size would be halved and the dura-\ntion would be lengthened (by 20 ms) for each wrong ans-\nwer; at the next right answer, another reversal would occur,\nthe second run would would end and the next presentation\nwould, again, be shortened by the step size. For each track,\nthe initial snippet length was set to 500 ms, the step sizes\n(halved at the end of each odd run) to 40/20/10/5/2 ms and\nthe stop criterion to 8 reversals.\nguitar clarinet trumpet violin0100200300400500Threshold [ms]musicians\nnon-musiciansFigure 2 . Thresholds for each group and instrument sound,\nboth individual (grey) and group-based means (colour-\nﬁlled symbol) for musicians (circle) and non-musicians\n(square). The different variability in thresholds between\ninstruments is clearly seen.\nThe order of stimuli presentations was made by inter-\nleaving the four tracks using a random permutation of a\n4x4 integer sequence of indices. This technique allows the\nsame timbre to be replayed before a sequence of 4 is com-\npleted, removing a potential bias by avoiding the possibi-\nlity of the subject anticipating the next sound. The typical\ntest time was 15-20 minutes (setup + 150-200 presentati-\nons), with a shortest played duration of 1 ms.\n2.5 Analysis\nThe typical shortest durations played during tests ran-\nged between 1 and 10 ms across all participants. The\nfour thresholds (one per instrument) were computed as the\nmean of the thresholds at reversals. The simple up-down\nestimates point p= 0:50on the psychometric function,\nwhich is well above chance performance for 4AFC ( p=\n0:25).\nThe performance difference between the two groups\n(musicians and non-musicians) was estimated by perfor-\nming a mixed ANOV A (between-subjects variable: 2 levels\nof musical training, within-subjects variable: 4 instrument\nsounds).\nMoreover, eight sound snippets were created using the\nfound thresholds and two audio descriptors (spectral cen-\ntroid and spectral irregularity) were computed in MAT-\nLAB using MIRtoolbox 1.7 [6].\n3. RESULTS\nThe outcome of the experiment is shown in Figure 2, with\nthe threshold means and standard deviations re-stated in\nTable 1. It can be seen that the threshold values vary con-\nsiderably across groups and instruments, with very low\nmean values for guitar and trumpet (for non-musicians\nonly), and mean values almost 10 times higher for vio-\nlin. Furthermore, variability is large for all thresholds,\nexcept guitar. A Q-Q plot showed that the data violates130 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Mean (std) [ms]\nStimulus Mus Non-mus\nGuitar 6.4 (1.6) 12.2 (8.8)\nClarinet 21.3 (26.1) 64.7 (102.9)\nTrumpet 34.4 (58.2) 7.4 (2.7)\nViolin 58.9 (145.7) 63.2 (63.7)\nTable 1 . Mean and standard deviation of thresholds of tim-\nbre discrimination. Mus = values from 9 musicians, Non-\nmus = values from 7 non-musicians.\nthe normality assumption, while a Levene’s test indica-\nted that group variances are homogeneous. After impro-\nving normality with a 10-log transformation, we proceeded\nwith a mixed ANOV A ( \u000b= 0.05) on the transformed data,\nlooking for statistically signiﬁcant effects of musical trai-\nning and stimulus. The between-subjects factor was group\n(musicians/non-musicians) and the within-subjects factor\nwas target (instrument). The Q-Q plot of the ANOV A resi-\nduals is approximately linear, so we assume that this ana-\nlysis is robust with respect to our dataset. While the stimu-\nlus effect was statistically signiﬁcant ( F(3;42) = 5:035,\np= 0:005), the musical training effect on timbre dis-\ncrimination of brief sounds was not ( F(1;14) = 1:134,\np= 0:305). The interaction effect was not signiﬁcant\neither (F(3;56) = 2:416,p= 0:080).\nPost-hoc pairwise t-tests (two-sided, Holm-Bonferroni\ncorrection) on the instrument thresholds showed that the\nguitar mean threshold was signiﬁcantly different from vi-\nolin (t(15) = \u00001:833,p= 0:024), but not from clarinet\n(t(15) = 4:873,p= 0:095) or trumpet ( t(15) = \u00001:187,\np= 0:871). No other contrasts were signiﬁcant — trum-\npet vs violin ( t(15) = \u00001:780,p= 0:081), clarinet vs\ntrumpet (t(15) = \u00001:913,p= 0:472), clarinet vs violin\n(t(15) = \u00002:097,p= 0:871).\nAs a rough approximation of an MDS model (with\nequal perceptual weightings and no speciﬁcities), we crea-\nted a feature space using two calculated audio descriptors:\nspectral centroid and spectral irregularity. Spectral irre-\ngularity is a measure of the amplitude deviation between\nsuccessive peaks of the spectrum (implemented in MIR-\nToolbox 1.7 [6]), a feature analogous to spectral deviation.\nThe two descriptors were chosen for two reasons: 1) they\nare informative as a set, as they are not strongly correlated\n(see e.g. [15]); 2) they can be computed as single-number\nfeatures, and are thereby easy to visualize and more robust\nto the short snippet durations than other descriptors which\nrequire frame-based analysis, e.g. spectral ﬂux.\nThe feature space is seen in Figures 3 (mean thresholds)\nand 4 (individual thresholds), with colours denoting the\nfour instruments: guitar (black), clarinet (blue), trumpet\n(red), and violin (green). The values for the 2 s source ﬁ-\nles are not plotted in Figure 4, thereby the different x-axis\nscale. As a general trend, the spectral centroid gets lowe-\nred for reduced duration. On the other hand, the spectral\nirregularity seem to either stay constant (clarinet), increase\n(guitar and violin), or ﬂuctuate (trumpet) depending on the\ninstrument sound.\n1000 1500 2000 2500 3000 3500 4000 4500\nSpectral centroid [Hz]00.20.40.60.811.2Spectral irregularity [-]gtThrMus\ngtThrNonmus\ngtFullclThrMusclThrNonmus clFull\ntpThrMus\ntpThrNonmustpFullvlnThrMusvlnThrNonmus\nvlnFullFigure 3 . Mean thresholds for the four instruments in a\nfeature space spanned by two audio descriptors: spectral\ncentroid and spectral irregularity (computed using [6]) for\nthe four instruments. Labels of the format xyz, withx\ndeﬁning the instrument ( gt= guitar (black), cl= clarinet\n(blue),tp= trumpet (red), vln= violin (green)), ydeﬁ-\nning the duration of the audio ﬁle ( Thr = audio snippet\ncut at threshold length, Full = 2 s long source ﬁle) and\nzdeﬁning the test group ( Mus = musicians, Nonmus =\nnon-musicians).\n1450 1500 1550 1600 1650 1700 1750 1800\nSpectral centroid [Hz]0.40.60.811.21.41.61.8Spectral irregularity [-]guitar (mus)\nguitar (non-mus)\nclarinet (mus)\nclarinet (non-mus)\ntrumpet (mus)\ntrumpet (non-mus)\nviolin (mus)\nviolin (non-mus)\nFigure 4 . Thresholds for all participants and the four in-\nstruments in a feature space spanned by two audio descrip-\ntors: spectral centroid and spectral irregularity (computed\nusing [6]) for the four instruments. The 2 s long source ﬁ-\nles are labelled xFull , withxdeﬁning the instrument ( gt=\nguitar (black), cl= clarinet (blue), tp= trumpet (red), vln\n= violin (green).\nAdditionally, we computed the log attack times of the\nfour source ﬁles ( LAT guitar =\u00001:921,LAT clarinet =\n\u00000:930,LAT trumpet =\u00000:506,LAT violin =\u00001:092).\nHowever, since the attack phase is incomplete for the thres-\nhold snippets, this feature was less informative in relation\nto the perceptual result.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 1314. DISCUSSION\nUsing an adaptive procedure, we investigated the tempo-\nral thresholds for timbre discrimination of different sounds\nfor musically trained and untrained listeners. Our ﬁndings\nagree with the existing literature with respect to the overall\nhigh performance of both groups. The overall performance\nwas best for guitar, both with respect to duration thresholds\nand variability, as conﬁrmed by post-hoc tests.\nWhile we measured an average violin threshold of about\n60 ms, Suied et al. [21] report window lengths correspon-\nding to above chance performance as small as 8 ms for\nstring sounds in a ﬁrst experiment, then doubled to 16 ms\nin a subsequent trial (with other instruments as distractors).\nSuied et al. offer two plausible interpretations of the high\nperformance levels: a successful adjustment of the audi-\ntory representation of the stimuli, which is speciﬁc to the\nsignal gating setup – which is described as a computatio-\nnally challenging form of unsupervised learning – or an ef-\nﬁcient activation of spectral cues even for deteriorated sti-\nmuli. Both interpretations could apply to our experiment.\nSome differences between our method and that used by\nSuied et al. [21] are worth mentioning. While we asked our\nparticipants to indicate the instrument heard in a 4AFC-\ntask that got more challenging over time, Suied et al. asked\ntheir subjects to indicate whether the sound was a target\nsound (50% of presentations) or not. The range of possi-\nble sounds was also wider in their study, with seven other\ninstruments beside those belonging to the target. Before\ntheir test, however, the participants listened to the targets\nrepeatedly for all stimuli durations. It is therefore difﬁ-\ncult to judge whether this would result in a harder task for\nthe participants compared to the one we chose. The task\nof categorizing (target vs distractor) or identifying (4AFC)\nsound are different: although the thresholds are still in the\nsame range, the peculiarity of the tasks might explain the\ndiscrepancy between string instrument thresholds.\nOur results showed no effect of musical training, pos-\nsibly as a result of the moderate sample size and our ope-\nrational deﬁnition of musician. Rather than dividing the\nparticipants in two groups (musicians and non-musicians),\nusing an index of musical sophistication (e.g. [13, 14])\ncould provide a more sensitive measure and allow for a\nregression analysis. Moreover, differences between mu-\nsicians and non-musicians have been shown in a combi-\nned instrument/voice discrimination task [2] as well as in\nbrain activity [4], so group differences in terms of cog-\nnitive strategies should not be dismissed. On the other\nhand, our result agrees with the thesis of MDS researchers,\nmeaning that the inter-individual differences in perceptual\nweighting of different timbral dimensions are independent\nof musical training [10].\nThe very low thresholds and low variability for guitar\n(both groups) and trumpet (non-musicians) seem to indi-\ncate the presence of early acoustical markers that could\nbe identiﬁed by listeners. Even though it is commonly\nassumed that onset is highly signiﬁcant for sound recog-\nnition (see e.g. [16] and [19]), this premise is not uni-\nversally accepted by timbre/duration studies. It has beendoubted by Clark et al. [3] and then strongly disputed by\nSuied et al., who argue that onset information might even\nbe misinformative for the discrimination of string instru-\nments (due to the noisy transients caused by the initial\ncontact between bow and string) [21]. However, the re-\nsults shown by Suied et al. seem to indicate that the per-\nformance difference between the two window constraints\n(random and onset) is both stimulus-speciﬁc and inconsis-\ntent across window lengths. The gating used in their expe-\nriment applied a raised-cosine window, while we applied\na rectangular window with a ﬁxed fade-out length (1 ms)\nfor all durations, as shown in Figure 1. Thus, our approach\nwould be more likely to preserve the original amplitude for\nlonger time (but with a sharper fade-out), while the stimuli\nprepared by Suied et al. would decrease in amplitude in a\nquicker and smoother manner. As for the acoustic analy-\nsis of the stimuli, Suied et al. explain the effect of gating\nin terms of “spectral splatter” (the smearing of spectral fe-\natures when short time windows are applied) and refute\nthe assumption that trivial spectral features are relevant to\nthe timbre discrimination task, based on a simulation of\nauditory excitation patterns derived by the employed sti-\nmuli [21].\nAs a direct investigation of the stimuli, we placed the\nsource ﬁles and threshold sound snippets in a feature space\n(Figures 3 and 4). Without perceptual weightings, this re-\npresentation lacks the depth of MDS models, but it is use-\nful to trace the deterioration of a set of audio descriptors\n(spectral centroid and spectral irregularity) for reduced du-\nration. Even though the full set of thresholds forms three\nclusters (Figure 4) and most of the guitar data points are\nlocated in one of the clusters (lower right), it is hard to\nconclude that the guitar advantage is due to the fact that\nthe stimulus retains speciﬁc audio features for brief dura-\ntions. The threshold differences could instead be explai-\nned by the different placement of discrete timbral featu-\nres (speciﬁcities), which are hard to correlate to the audio\ndescriptors. The guitar advantage might by explained by\nour choice of the onset condition, which preserves the cha-\nracteristic ”twang” even for very brief window lengths. A\nmore systematical investigation of the evolution of a set\nof audio descriptors for different stimuli and progressively\ndecreasing duration would be worth considering for future\nwork.\n5. CONCLUSION\nIn this paper, we have investigated the temporal thresholds\nof timbre discrimination for four instrument sounds. Alt-\nhough the thresholds from the staircase method varied sig-\nniﬁcantly between stimuli, with means ranging from <15\nms (guitar) to \u001960ms (violin), there was no signiﬁ-\ncant effect of musical training on timbre discrimination.\nThe guitar advantage can be explained by considering our\nchoice of window position (always including the sound on-\nset) and the timbral speciﬁcities of the guitar sound. The\noverall low thresholds agree with the ﬁndings of the ex-\nisting literature, and the adaptive staircase method seems\nto constitute a viable alternative to the method of constant132 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018stimuli for the chosen task. As a result, participants were\nable to adjust the weights of the perceptual dimensions of\ntimbre to the acoustic degradation of the stimuli as the du-\nrations were reduced. Future research could use this inves-\ntigation as a point for departure for a further examination\nof the duration thresholds, using larger sound sets and mul-\ntiple window conditions.\n6. ACKNOWLEDGEMENTS\nThe authors would like to thank all the participants and two\nanonymous reviewers for helpful comments on an earlier\nversion of the manuscript.\nAuthor Bigoni performed the main part of this work as\npart of a course in Music Perception and Cognition at the\nMSc program Sound and Music Computing, Aalborg Uni-\nversity – Copenhagen. Author Dahl supervised the work\nand participated in writing the manuscript.\n7. REFERENCES\n[1] Trevor R. Agus, Clara Suied, Simon J. Thorpe, and Da-\nniel Pressnitzer. Fast recognition of musical sounds ba-\nsed on timbre. The Journal of the Acoustical Society of\nAmerica , 131(5):4124–4133, May 2012.\n[2] Jean-Pierre Chartrand and Pascal Belin. Superior voice\ntimbre processing in musicians. Neuroscience Letters ,\n405(3):164–167, Sep 2006.\n[3] Jr Clark, David Luce, Robert Abrams, Howard\nSchlossberg, and James Rome. Preliminary Experi-\nments on the Aural Signiﬁcance of Parts of Tones of\nOrchestral Instruments and on Choral Tones. Journal\nof the Audio Engineering Society , 11(1):45–54, Janu-\nary 1963.\n[4] Garry C. Crummer, Joseph P. Walton, John W. Way-\nman, Edwin C. Hantz, and Robert D. Frisina. Neu-\nral processing of musical timbre by musicians, non-\nmusicians, and musicians possessing absolute pitch.\nThe Journal of the Acoustical Society of America ,\n95(5):2720–2727, May 1994.\n[5] Giles Wilkeson Gray. Phonemic microtomy: The mini-\nmum duration of perceptible speech sounds. Communi-\ncations Monographs , 9(1):75–90, 1942.\n[6] Olivier Lartillot and Petri Toiviainen. A Matlab tool-\nbox for musical feature extraction from audio. In In-\nternational Conference on Digital Audio Effects , pages\n237–244, 2007.\n[7] Marjorie R Leek. Adaptive procedures in psychophysi-\ncal research. Perception & psychophysics , 63(8):1279–\n1292, 2001.\n[8] H. Levitt. Transformed Up–Down Methods in Psy-\nchoacoustics. The Journal of the Acoustical Society of\nAmerica , 49(2B):467–477, Feb 1971.[9] Sandra T Mace, Cynthia L Wagoner, David J Te-\nachout, and Donald A Hodges. Genre identiﬁcation\nof very brief musical excerpts. Psychology of Music ,\n40(1):112–128, 2012.\n[10] Stephen McAdams. Musical Timbre Perception. In Di-\nana Deutsch, editor, The Psychology of Music , pages\n35–67. Elsevier, 3rd edition edition, 2013.\n[11] Stephen McAdams, Suzanne Winsberg, Sophie Don-\nnadieu, Geert De Soete, and Jochen Krimphoff. Per-\nceptual scaling of synthesized musical timbres: Com-\nmon dimensions, speciﬁcities, and latent subject clas-\nses. Psychological Research , 58(3):177–192, Dec\n1995.\n[12] Brian Moore. An Introduction to the Psychology of He-\naring: Sixth Edition , pages 284–286. BRILL, 2013.\n[13] Daniel M ¨ullensiefen, Bruno Gingras, Jason Musil, and\nLauren Stewart. The musicality of non-musicians: an\nindex for assessing musical sophistication in the gene-\nral population. PloS one , 9(2):e89642, 2014.\n[14] Joy E Ollen. A criterion-related validity test of selected\nindicators of musical sophistication using expert ra-\ntings . PhD thesis, The Ohio State University, 2006.\n[15] Geoffroy Peeters, Bruno L. Giordano, Patrick Susini,\nNicolas Misdariis, and Stephen McAdams. The tim-\nbre toolbox: Extracting audio descriptors from musical\nsignals. The Journal of the Acoustical Society of Ame-\nrica, 130(5):2902–2916, 2011.\n[16] Jean-Claude Risset and David L Wessel. Exploration\nof timbre by analysis and synthesis. In Diana Deutsch,\neditor, The Psychology of Music , pages 113–169. Else-\nvier, 1999.\n[17] Ken Robinson and Roy D. Patterson. The Duration Re-\nquired to Identify the Instrument, the Octave, or the\nPitch Chroma of a Musical Note. Music Perception: An\nInterdisciplinary Journal , 13(1):1–15, October 1995.\n[18] Ken Robinson and Roy D. Patterson. The stimulus du-\nration required to identify vowels, their octave, and\ntheir pitch chroma. The Journal of the Acoustical So-\nciety of America , 98(4):1858–1865, October 1995.\n[19] E. L. Saldanha and John F. Corso. Timbre Cues and the\nIdentiﬁcation of Musical Instruments. The Journal of\nthe Acoustical Society of America , 36(11):2021–2026,\nNov 1964.\n[20] Noam R. Shabtai, Gottfried Behler, Michael V orl ¨ander,\nand Stefan Weinzierl. Generation and analysis of an\nacoustic radiation pattern database for forty-one musi-\ncal instruments. The Journal of the Acoustical Society\nof America , 141(2):1246–1256, Feb 2017.\n[21] Clara Suied, Trevor R. Agus, Simon J. Thorpe, Nima\nMesgarani, and Daniel Pressnitzer. Auditory gist: Re-\ncognition of very short sounds from timbre cues.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 133The Journal of the Acoustical Society of America ,\n135(3):1380–1391, March 2014.\n[22] Clara Suied, Patrick Susini, Stephen McAdams, and\nRoy D. Patterson. Why are natural sounds detected fas-\nter than pips? The Journal of the Acoustical Society of\nAmerica , 127(3):EL105–EL110, March 2010.\n[23] Stefan Weinzierl, Michael V orl ¨ander, Gottfried Behler,\nFabian Brinkmann, Henrik von Coler, Erik Detz-\nner, Johannes Kr ¨amer, Alexander Lindau, Martin\nPollow, Frank Schulz, and Noam R. Shabtai. A\nDatabase of Anechoic Microphone Array Measure-\nments of Musical Instruments, Apr 2017. Available at\nhttp://dx.doi.org/10.14279/depositonce-5861.2.134 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "MIDI-VAE: Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer.",
        "author": [
            "Gino Brunner",
            "Andres Konrad",
            "Yuyi Wang 0001",
            "Roger Wattenhofer"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492525",
        "url": "https://doi.org/10.5281/zenodo.1492525",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/204_Paper.pdf",
        "abstract": "We introduce MIDI-VAE, a neural network model based on Variational Autoencoders that is capable of handling polyphonic music with multiple instrument tracks, as well as modeling the dynamics of music by incorporating note durations and velocities. We show that MIDI-VAE can perform style transfer on symbolic music by automatically changing pitches, dynamics and instruments of a music piece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separate style validation classifiers. Our model can also interpolate between short pieces of music, produce medleys and create mixtures of entire songs. The interpolations smoothly change pitches, dynamics and instrumentation to create a harmonic bridge between two music pieces. To the best of our knowledge, this work represents the first successful attempt at applying neural style transfer to complete musical compositions.",
        "zenodo_id": 1492525,
        "dblp_key": "conf/ismir/BrunnerKWW18",
        "keywords": [
            "MIDI-VAE",
            "polyphonic music",
            "multiple instrument tracks",
            "note durations",
            "velocities",
            "style transfer",
            "symbolic music",
            "style validation classifiers",
            "interpolations",
            "music pieces"
        ],
        "content": "MIDI-V AE: MODELING DYNAMICS AND INSTRUMENTATION OF\nMUSIC WITH APPLICATIONS TO STYLE TRANSFER\nGino Brunner Andres Konrad Yuyi Wang Roger Wattenhofer\nDepartment of Electrical Engineering and Information Technology\nETH Zurich\nSwitzerland\nbrunnegi,konradan,yuwang,wattenhofer@ethz.ch\nABSTRACT\nWe introduce MIDI-V AE, a neural network model based\non Variational Autoencoders that is capable of handling\npolyphonic music with multiple instrument tracks, as well\nas modeling the dynamics of music by incorporating note\ndurations and velocities. We show that MIDI-V AE can per-\nform style transfer on symbolic music by automatically\nchanging pitches, dynamics and instruments of a music\npiece from, e.g., a Classical to a Jazz style. We evalu-\nate the efﬁcacy of the style transfer by training separate\nstyle validation classiﬁers. Our model can also interpolate\nbetween short pieces of music, produce medleys and cre-\nate mixtures of entire songs. The interpolations smoothly\nchange pitches, dynamics and instrumentation to create a\nharmonic bridge between two music pieces. To the best of\nour knowledge, this work represents the ﬁrst successful at-\ntempt at applying neural style transfer to complete musical\ncompositions.\n1. INTRODUCTION\nDeep generative models do not just allow us to generate\nnew data, but also to change properties of existing data\nin principled ways, and even transfer properties between\ndata samples. Have you ever wanted to be able to cre-\nate paintings like Van Gogh or Monet? No problem! Just\ntake a picture with your phone, run it through a neural net-\nwork, and out comes your personal masterpiece. Being\nable to generate new data samples and perform style trans-\nfer requires models to obtain a deep understanding of the\ndata. Thus, advancing the state-of-the-art in deep genera-\ntive models and neural style transfer is not just important\nfor transforming horses into zebras,1but lies at the very\ncore of Deep (Representation) Learning research [2].\nWhile neural style transfer has produced astonishing re-\nsults especially in the visual domain [21, 37], the progress\n1https://junyanz.github.io/CycleGAN/\nc\rGino Brunner, Andres Konrad, Yuyi Wang, Roger Wat-\ntenhofer. Licensed under a Creative Commons Attribution 4.0 Interna-\ntional License (CC BY 4.0). Attribution: Gino Brunner, Andres Kon-\nrad, Yuyi Wang, Roger Wattenhofer. “MIDI-V AE: Modeling Dynamics\nand Instrumentation of Music with Applications to Style Transfer”, 19th\nInternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.for sequential data, and in particular music, has been\nslower. We can already transfer sentiment between restau-\nrant reviews [30, 36], or even change the instrument with\nwhich a melody is played [33], but we have no way of\nknowing how our favorite pop song would have sounded\nif it were written by a composer who lived in the classi-\ncal epoch or how a group of jazz musicians would play\nthe Overture of Mozart’s Don Giovanni. In this work we\ntake a step towards this ambitious goal. To the best of\nour knowledge, this paper presents the ﬁrst successful ap-\nplication of unaligned style transfer to musical composi-\ntions. Our proposed model architecture consists of paral-\nlel Variational Autoencoders (V AE) with a shared latent\nspace and an additional style classiﬁer. The style classiﬁer\nforces the model to encode style information in the shared\nlatent space, which then allows us to manipulate existing\nsongs, and effectively change their style, e.g., from Clas-\nsic to Jazz. Our model is capable of producing harmonic\npolyphonic music with multiple instruments. It also learns\nthe dynamics of music by incorporating note durations and\nvelocities.\n2. RELATED WORK\nGatys et al. [14] introduce the concept of neural style trans-\nfer and show that pre-trained CNNs can be used to merge\nthe style and content of two images. Since then, more pow-\nerful approaches have been developed [21,37]; these allow,\nfor example, to render an image taken in summer to look\nlike it was shot in winter. For sequential data, autoencoder\nbased methods [30, 36] have been proposed to change the\nsentiment or content of sentences. Van den Oord et al. [33]\nintroduce a V AE model with discrete latent space that is\nable to perform speaker voice transfer on raw audio data.\nMor et al. [26] develop a system based on a WaveNet au-\ntoencoder [12] that can translate music across instruments,\ngenres and styles, and even create music from whistling.\nMalik et al. [23] train a model to add note velocities (loud-\nness) to sheet music, resulting in more realistic sounding\nplayback. Their model is trained in a supervised manner,\nwith the target being a human-like performance of a music\npiece in MIDI format, and the input being the same piece\nbut with all note velocities set to the same value. While\ntheir model can indeed play music in a more human-like\nmanner, it can only change note velocities, and does not747learn the characteristics of different musical styles/genres.\nOur model is trained on unaligned songs from different\nmusical styles. Our model can not only change the dy-\nnamics of a music piece from one style to another, but also\nautomatically adapt the instrumentation and even the note\npitches themselves. Apart from style transfer, our model\ncan also generate short pieces of music, medleys, interpo-\nlations and song mixtures. At the core of our model thus\nlies the capability to produce music. In the following we\nwill therefore discuss related work in the domains of sym-\nbolic and raw audio generation. For a more comprehen-\nsive overview we refer the interested readers to these sur-\nveys: [4, 13, 16].\nPeople have been trying to compose music with the help\nof computers for decades. One of the most famous early\nexamples is “Experiments in Musical Intelligence” [9],\na semi-automatic system based on Markov models that\nis able to create music in the style of a certain com-\nposer. Soon after, the ﬁrst attempts at music composition\nwith artiﬁcial neural networks were made. Most notably,\nTodd [31], Mozer [27] and Eck et al. [11] all used Recur-\nrent Neural Networks (RNN). More recently, Boulanger-\nLewandowski et al. [3] combined long short term memory\nnetworks (LSTMs) and Restricted Boltzmann Machines to\nsimultaneously model the temporal structure of music, as\nwell as the harmony between notes that are played at the\nsame time, thus being capable of generating polyphonic\nmusic. Chu et al. [7] use domain knowledge to model\na hierarchical RNN architecture that produces multi-track\npolyphonic music. Brunner et al. [5] combine a hierar-\nchical LSTM model with learned chord embeddings that\nform the Circle of Fifths, showing that even simple LSTMs\nare capable of learning music theory concepts from data.\nHadjeres et al. [15] introduce an LSTM-based system\nthat can harmonize melodies by composing accompany-\ning voices in the style of Bach Chorales, which is con-\nsidered a very difﬁcult task even for professionals. John-\nson et al. [18] use parallel LSTMs with shared weights to\nachieve transposition-invariance (similar to the translation-\ninvariance of CNNs). Chuan et al. [8] investigate the use\nof an image-based Tonnetz representation of music, and\napply a hybrid LSTM/CNN model to music generation.\nGenerative models such as the Variational Autoencoder\n(V AE) and Generative Adversarial Networks (GANs) have\nbeen increasingly successful at modeling music. Roberts\net al. introduce MusicV AE [29], a hierarchical V AE model\nthat can capture long-term structure in polyphonic music\nand exhibits high interpolation and reconstruction perfor-\nmance. GANs, while very powerful, are notoriously dif-\nﬁcult to train and have generally not been applied to se-\nquential data. However, Mogren [25], Yang et al. [34] and\nDong et al. [10] have recently shown the efﬁcacy of CNN-\nbased GANs for music composition. Yu et al. [35] were\nthe ﬁrst to successfully apply RNN-based GANs to music\nby incorporating reinforcement learning techniques.\nResearchers have also worked on generating raw au-\ndio waves. Van den Oord et al. [32] introduce WaveNet,\na CNN-based model for the conditional generation ofspeech. The authors also show that it can be used to gen-\nerate pleasing sounding piano music. More recently, En-\ngel et al. [12] incorporated WaveNet into an Autoencoder\nstructure to generate musical notes and different instru-\nment sounds. Mehri et al. [24] developed SampleRNN, an\nRNN-based model for unconditional generation of raw au-\ndio. While these models are impressive, the domain of raw\naudio is very high dimensional and it is much more difﬁcult\nto generate pleasing sounding music. Thus most existing\nwork on music generation uses symbolic music represen-\ntations (see e.g., [3,5,7–10,15,18,23,25,27,29,31,34,35]).\n3. MODEL ARCHITECTURE\nOur model is based on the Variational Autoencoder [20]\n(V AE) and operates on a symbolic music representation\nthat is extracted from MIDI [1] ﬁles. We extend the stan-\ndard piano roll representation of note pitches with veloc-\nity and instrument rolls, modeling the most important in-\nformation contained in MIDI ﬁles. Thus, we term our\nmodel MIDI-V AE. MIDI-V AE uses separate recurrent en-\ncoder/decoder pairs that share a latent space. A style clas-\nsiﬁer is attached to parts of the latent space to make sure the\nencoder learns a compact latent style label that we can then\nuse to perform style transfer. The architecture of MIDI-\nV AE is shown in Figure 1, and will be explained in more\ndetail in the following.\n3.1 Symbolic Music Representation\nWe use music ﬁles in the MIDI format, which is a sym-\nbolic representation of music that resembles sheet music.\nMIDI ﬁles have multiple tracks. Tracks can either be on\nwith a certain pitch and velocity, held over multiple time\nsteps or be silent . Additionally, an instrument is assigned\nto each track. To feed the note pitches into the model\nwe represent them as a tensor P2f0;1gnP\u0001nB\u0001nT(com-\nmonly known as piano roll and henceforth referred to as\npitch roll), where nPis the number of possible pitch val-\nues,nBis the number of beats and nTis the number of\ntracks. Thus, each song in the dataset is split into pieces\nof lengthnB. We choose nBsuch that each piece cor-\nresponds to one bar. We include a “silent” note pitch to\nindicate when no note is played at a time step. The note\nvelocities are encoded as tensor V2[0;1]nP\u0001nB\u0001nT(ve-\nlocity roll). Velocity values between 0.5 and 1 signify a\nnote being played for the ﬁrst time, whereas a value below\n0.5 means that either no note is being played, or that the\nnote from the last time step is being held. The note veloc-\nity range deﬁned by MIDI (0 to 127) is mapped to the in-\nterval [0:5;1]. We model the assignment of instruments to\ntracks as matrix I=f0;1gnT\u0001nI(instrument roll), where\nnIis the number of possible instruments. The instrument\nassignment is a global property and thus remains constant\nover the duration of one song. Finally, each song in our\ndataset belongs to a certain style, designated by the style\nlabelS2fClassic;Jazz;Pop;Bach;Mozart g.\nIn order to generate harmonic polyphonic music it is\nimportant to model the joint probability of simultane-748 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Dense GRUPitch roll \n(  )\nGRUInstrument roll \n(  )\nGRUVelocity roll \n(  )Dense Dense \nDense ᷘz\nᷟzStyle\nClassifier Style \nprediction \n(  )\n GRU Dense \nGRU Dense \nGRU Dense Reconstructed \nPitch roll \n(  )\nReconstructed \nInstrument roll \n(  )\nReconstructed \nVelocity roll \n(  )zt zstyle Style label \n(  )\nḋ\nḋ ̴ N(0, ᶥḋ)\nEncoders \nDecoders Figure 1 . MIDI-V AE architecture. GRU stands for Gated Recurrent Unit [6].\nously played notes. A standard recurrent neural network\nmodel already models the joint distribution of the sequence\nthrough time. If there are multiple outputs to be produced\nper time step, a common approach is to sample each out-\nput independently. In the case of polyphonic music, this\ncan lead to dissonant and generally “wrong” sounding note\ncombinations. However, by unrolling the piano rolls in\ntime we can let the RNN learn the joint distribution of si-\nmultaneous notes as well. Basically, instead of one nT-hot\nvector for each beat, we input nT1-hot vectors per beat\nto the RNN. This is a simple but effective way of model-\ning the joint distribution of notes. The drawback is that\nthe RNN needs to model longer sequences. We use the\npretty midi [28] Python library to extract information from\nMIDI ﬁles and convert them to piano rolls.\n3.2 Parallel V AE with Shared Latent Space\nMIDI-V AE is based on the standard V AE [20] with a hy-\nperparameter \fto weigh the Kullback-Leibler divergence\nin the loss function (as in [17]). A V AE consists of an en-\ncoderq\u0012(zjx), a decoderp\u001e(xjz)and a latent variable z,\nwhereqandpare usually implemented as neural networks\nparameterized by \u0012and\u001e. In addition to minimizing the\nstandard autoencoder reconstruction loss, V AEs also im-\npose a prior distribution p(z)on the latent variables. Hav-\ning a known prior distribution enables generation of new\nlatent vectors by sampling from that distribution. Further-\nmore, the model will only “use” a new dimension, i.e., de-\nviate from the prior distribution, if it signiﬁcantly lowers\nthe reconstruction error. This encourages disentanglement\nof latent dimensions and helps learning a compact hidden\nrepresentation. The V AE loss function is\nLVAE =Eq\u0012(zjx)[logp\u001e(xjz)]\u0000\fDKL[(q\u0012(zjx)jjp(z)];\nwhere the ﬁrst term corresponds to the reconstruction loss,\nand the second term forces the distribution of latent vari-\nables to be close to a chosen prior. DKLis the Kullback-\nLeibler divergence, which gives a measure of how similar\ntwo probability distributions are. As is common practice,\nwe use an isotropic Gaussian distribution with unit vari-\nance as our prior, i.e., p(z) =N(0;I). Thus, both q\u0012(zjx)andp(z)are (isotropic) Gaussian distributions and the KL\ndivergence can be computed in closed form.\nAs described in Section 3.1, we represent multi-track\nmusic as a combination of note pitches, note velocities\nand an assignment of instruments to tracks. In order to\ngenerate harmonic multi-track music, we need to model\na joint distribution of these input features instead of three\nmarginal distributions. Thus, our model consists of three\nencoder/decoder pairs with a shared latent space that cap-\ntures the joint distribution. For each input sample (i.e., a\npiece of length nBbeats), the pitch, velocity and instru-\nment rolls are passed through their respective encoders,\nimplemented as RNNs. The output of the three encoders is\nconcatenated and passed through several fully connected\nlayers, which then predict \u001bzand\u0016z, the parameters of\nthe approximate posterior q\u0012(zjx) =N(\u0016z;\u001bz):2Using\nthe reparameterization trick [20], a latent vector zis sam-\npled from this distribution as z\u0018N(\u0016z;\u001bz\u0003\u000f)where\u0003\nstands for element-wise multiplication. This is necessary\nbecause it is generally not possible to backpropagate gra-\ndients through a random sampling operation, since it is not\ndifferentiable. \u000fis sampled from an isotropic Gaussian dis-\ntributionN(0;\u001b\u000f\u0003I), where we treat \u001b\u000fas a hyperparam-\neter (see Section 4.2 for more details). This shared latent\nvector is then fed into three parallel fully connected lay-\ners, from which the three decoders try to reconstruct the\npitch, velocity and instrument rolls. The note pitch and\ninstrument decoders are trained with cross entropy losses,\nwhereas for the velocity decoder we use MSE.\n3.3 Style Classiﬁer\nHaving a disentangled latent space might enable some con-\ntrol over the style of a song. If for example one dimension\nin the latent space encodes the dynamics of the music, then\nwe could easily change an existing piece by only varying\nthis dimension. Choosing a high value for \f(the weight of\nthe KL term in the V AE loss function) has been shown to\nincrease disentanglement of the latent space in the visual\ndomain [17]. However, increasing \fhas a negative effect\n2We use notation \u001bfor both a variance vector and the corresponding\ndiagonal variance matrix.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 749Dataset #Songs #Bars Artists\nClassic 477 60523 Beethoven, Clementi, ...\nJazz 554 72190 Sinatra, Coltrane, ...\nPop 659 65697 ABBA, Bruno Mars, ...\nBach 156 16213 Bach\nMozart 143 17198 Mozart\nTable 1 . Properties of our dataset.\non the reconstruction performance. Therefore, we intro-\nduce additional structure into the latent space by attaching\na softmax style classiﬁer to the top kdimensions of the la-\ntent space (zstyle), wherekequals the number of different\nstyles in our dataset. This forces the encoder to write a\n“latent style label” into the latent space. Using only kdi-\nmensions and a weak classiﬁer encourages the encoder to\nlearn a compact encoding of the style. In order to change\na song’s style from SitoSj, we pass the song through the\nencoder to get z, swap the values of dimensions zi\nstyle and\nzj\nstyle, and pass the modiﬁed latent vector through the de-\ncoder. As style we choose the music genre (e.g., Jazz, Pop\nor Classic) or individual composers (Bach or Mozart).\n3.4 Full Loss Function\nPutting all parts together, we get the full loss function of\nour model as\nLtot=\u0015PH(P;^P) +\u0015IH(I;^I) (1)\n+\u0015VMSE (V;^V) +\u0015SH(S;^S)\u0000\fDKL(qjjp);\nwhereH(\u0001;\u0001),MSE (\u0001;\u0001)andDKL(\u0001jj\u0001)stand for cross\nentropy, mean squared error and KL divergence respec-\ntively. The hats denote the predicted/reconstructed values.\nThe weights \u0015and\fcan be used to balance the individual\nterms of the loss functions.\n4. IMPLEMENTATION\nIn this section we describe our dataset and pre-processing\nsteps. We also give some insight into the training of our\nmodel and justiﬁcation for hyperparameter choices.\n4.1 Dataset and Pre-Processing\nOur dataset contains songs from the genres Classic, Jazz\nand Pop. The songs were gathered from various online\nsources;3a summary of the properties is shown in Ta-\nble 1. Note that we excluded symphonies from our Clas-\nsic, Bach and Mozart datasets due to their complexity and\nhigh number of simultaneously playing instruments. We\nuse a train/test split of 90/10. Each song in the dataset\ncan contain multiple instrument tracks and each track can\nhave multiple notes played at the same time. Unless stated\notherwise, we select nT= 4 instrument tracks from each\nsong by ﬁrst picking the tracks with the highest number of\n3Pop: www.midiworld.com / Jazz: http://midkar.\ncom/jazz/jazz_01.html / Classic (including Bach, Mozart):\nwww.reddit.com/r/WeAreTheMusicMakers/comments/\n3ajwe4/played notes, and from each track we choose the highest\nvoice, meaning picking the highest notes per time step. If\na song has fewer than nTinstrument tracks, we pick ad-\nditional voices from the tracks until we have nTvoices in\ntotal. We exclude drum tracks, since they do not have a\npitch value. We choose the 16thnote as smallest unit. In\nthe most widely used time signature4\n4there are 16 16th\nnotes in a bar. 91% of Jazz and Pop songs in our dataset\nare in4\n4, whereas for Classic the fraction is 34%. For songs\nwith time signatures other than4\n4we still designate 16 16th\nnotes as one bar. All songs are split into samples of one bar\nand our model auto-encodes one sample at a time. During\ntraining we shufﬂe the songs for each epoch, but keep the\nbars of a song in the correct order and do not reset the\nRNN states between samples. Thus, our model is trained\non a proper sequence of bars, instead of being confused by\nrandom bar progressions.\nThere are 128 possible pitches in MIDI. Since very low\nand high pitches are rare and often do not sound pleasing,\nwe only use nP= 60 pitch values ranging from 24 ( C1) to\n84 (C6).\n4.2 Model (Hyper-)Parameters\nOur model is generally not sensitive to most hyperparame-\nters. Nevertheless we continuously performed local hyper-\nparameter searches based on good baseline models, only\nvarying one hyperparameter at a time. We use the recon-\nstruction accuracy of the pitch roll decoder as evaluation\nmetric. Using Gated Recurrent Units (GRUs) [6] instead\nof LSTMs increases performance signiﬁcantly. Using bidi-\nrectional GRUs did not improve the results. The pitch roll\nencoder/decoder uses two GRU layers, whereas the rest\nuses only one layer. All GRU state sizes as well as the size\nof the latent space zare set to 256. We use the ADAM opti-\nmizer [19] with an initial learning rate of 0.0002. For most\nlayers in our architecture, we found tanh to work better\nthan sigmoid or rectiﬁed linear units. We train on batches\nof size 256. The loss function weights \u0015P,\u0015I,\u0015Vand\u0015S\nwere set to 1.0, 1.0, 0.1 and 0.1 respectively. \u0015pwas set to\n1.0 to favor high quality note pitch reconstructions over the\nrest.\u0015Vwas also set to 1.0 because the MSE magnitude is\nmuch smaller than the cross entropy loss values.\nDuring our experiments, we realized that high values\nof\fgenerally lead to very poor performance. We further\nfound that setting the variance of \u000fto the value of \u001b\u000f= 1,\nas done in all previous work using V AEs, also has a neg-\native effect. Therefore we decided to treat \u001b\u000fas a hyper-\nparameter as well. Figure 2 shows the results of the grid\nsearch.\u001b\u000fis the variance of the distribution from which\nthe\u000fvalues for the reparameterization trick are sampled,\nand is thus usually set to the same value as the variance of\nthe prior. However, especially at the beginning of learn-\ning, this introduces a lot of noise that the decoder needs to\nhandle, since the values for \u0016zand\u001bz, output by the en-\ncoder, are small compared to \u000f. We found that by reducing\n\u001b\u000f, we can improve the performance of our model signif-\nicantly, while being able to use higher values for \f. An\nannealing strategy for both \fand\u001b\u000fmight produce better750 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 201810−310−210−11000.550.600.650.700.750.800.85\nσ/epsilon1Pitch Roll Reconstruction Acc.Beta: 10.0\nBeta: 1.0\nBeta: 0.1\nBeta: 0.01\nBeta: 0.001\nBeta: 0.0001Figure 2 . Test reconstruction accuracy of pitch roll for\ndifferent\fand\u001b\u000f.\nPitch Instrument Style Velocity\nTrain Test Train Test Train Test Train Test\nCvJ 0.90 0.85 0.99 0.87 0.98 0.92 0.008 0.029\nCvP 0.96 0.88 0.99 0.89 0.96 0.91 0.017 0.036\nJvP 0.88 0.80 0.99 0.86 0.94 0.69 0.043 0.048\nBvM 0.91 0.75 0.99 0.82 0.94 0.74 0.010 0.033\nTable 2 . Train and test performance of our ﬁnal models.\nThe velocity column shows MSE loss values, whereas the\nrest are accuracies.\nresults, but we did not test this. In the ﬁnal models we use\n\f= 0:1and\u001b\u000f= 0:01. Note that during generation we\nsamplezfromN(0;\u001b^z), where\u001b^zis the empirical vari-\nance obtained by feeding the entire training dataset through\nthe encoder. The empirical mean \u0016^zis very close to zero.\n4.3 Training\nAll models are trained on single GPUs (GTX 1080) un-\ntil the pitch roll decoder converges. This corresponds to\naround 400 epochs, or 48 hours. We train one model for\neach genre/composer pair to make learning easier. This re-\nsults in four models that we henceforth call CvJ (trained\non Classic and Jazz), CvP (Classic and Pop), JvP (Jazz and\nPop) and BvM (Bach and Mozart). The train/test accura-\ncies/losses of all ﬁnal models are shown in Table 2. The\ncolumns correspond to the terms in our model’s full loss\nfunction (Equation 1).\n5. EXPERIMENTAL RESULTS\nIn this section we evaluate the capabilities of MIDI-V AE.\nWherever mentioned, corresponding audio samples can be\nfound on YouTube.4\n5.1 Style Transfer\nTo evaluate the effectiveness of MIDI-V AE’s style transfer,\nwe train three separate style evaluation classiﬁers. The in-\nput features are the pitch, velocity and instrument rolls re-\n4https://goo.gl/vb8YrhTrain Songs Test Songs\nBefore After Diff. Before After Diff.\nCvJ 0.92 0.38 0.54 0.87 0.39 0.48\nCvP 0.94 0.43 0.51 0.92 0.45 0.47\nJvP 0.72 0.60 0.12 0.72 0.62 0.10\nBvM 0.77 0.45 0.32 0.66 0.47 0.19\nTable 3 . Style transfer performance (ensemble classiﬁer\naccuracies before and after) between all style pairs.\nspectively. The three style classiﬁers are also combined to\noutput a voting based ensemble prediction. The accuracy\nof the classiﬁers is computed as the fraction of correctly\npredicted styles per bar in a song. We predict the likelihood\nof the source style before andafter the style change. If the\nstyle transfer works, the predicted likelihood of the source\nstyle decreases. The larger the difference, the stronger the\neffect of the style transfer. Note that for all experiments\npresented in this paper we set the number of styles k= 2,\nthat is, one MIDI-V AE model is trained on two styles, e.g.,\nClassic vs. Jazz. Therefore, the style classiﬁer is binary\nand a reduction in probability of the source style is equiv-\nalent to an increase in probability of the target style of the\nsame magnitude. All style classiﬁers use two-layer GRUs\nwith a state size of 256. Table 3 shows the performance\nof MIDI-V AE’s style transfer when measured by the en-\nsemble style classiﬁer. We trained a separate MIDI-V AE\nfor each style pair. For each pair of styles we perform a\nstyle change on all songs in both directions and average\nthe results. The style transfer works for all models, albeit\nto varying degrees. In all cases except for JvP, the predic-\ntor is even skewed below 0.5, meaning that the target style\nis now considered more likely than the source style.\nTable 4 shows the style transfer results measured by\neach individual style classiﬁer. We can see that pitch and\nvelocity contribute equally to the style change, whereas in-\nstrumentation seems to correlate most with the style. For\nCvJ and CvP, switching the style heavily changes the in-\nstrumentation. Figure 3 illustrates how the instruments of\nall songs in our Jazz test set are changed when switch-\ning the style to Classic. Only few instruments are rarely\nchanged (piano, ensemble, reed), whereas most others are\nmapped to one or multiple different instruments. The\ninstrument switch between genres with highly overlap-\nping instrumentation (JvP, BvM) is much less pronounced.\nClassifying style based on the note pitches and velocities of\none bar is more difﬁcult, as shown by the “before” accura-\ncies in Table 4, which are generally lower than the ones of\nthe instrument roll based classiﬁer. Nevertheless, the style\ntransfer changes pitch and velocity towards the target style.\nMIDI-V AE retains most of the original melody, while of-\nten changing accompanying instruments to suit the target\nstyle. This is generally desirable, since we do not want\nto change the pitches so thoroughly that the original song\ncannot be recognized anymore. We provide examples of\nstyle transfers on a range of songs from our training and\ntest sets on YouTube (see Style transfer songs ).Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 751piano\npercussion\norgans\nguitar\nbass\nstrings\nensemble\nbrass\nreed\npipe\nsynth lead\nsynth pad\nsynth effects\nethnic\npercussive\nsound effectssound effectspercussiveethnicsynth effectssynth effectssynth leadpipereedbrassensemblestringsbassguitarorganspercussionpiano\nClassicJazz\n0.00.20.40.60.81.0Figure 3 . The matrix visualizes how the instruments are\nchanged when switching from Jazz to Classic, averaged\nover all Jazz songs in the test set.\nPitch Velocity Instrument\nBf. Af. Bf. Af. Bf. Af.\nCvJ Test 0.77 0.66 0.67 0.57 0.90 0.20\nCvP Test 0.77 0.67 0.71 0.60 0.91 0.27\nJvP Test 0.65 0.63 0.67 0.64 0.67 0.55\nBvM Test 0.55 0.47 0.60 0.49 0.64 0.47\nTable 4 . Average before and after classiﬁer accuracies for\nall classiﬁers (pitch/instrument/velocity) for the test set.\n5.2 Latent Space Evaluation\nFigure 4 shows a t-SNE [22] plot of the latent vectors for\nall bars of 20 Jazz and 20 Classic pieces. The darker the\ncolor, the more “jazzy” or “classical” a song is according\nto the ensemble style classiﬁer. The genres are well sepa-\nrated, and most songs have all their bars clustered closely\ntogether (likely thanks to the instrument roll being con-\nstant). Some classical pieces are bleeding over into the\nJazz region and vice versa. As can be seen from the light\ncolor, the ensemble style classiﬁer did not conﬁdently as-\nsign these pieces to either style.\nWe further perform a sweep over all 256 latent dimen-\nsions on randomly sampled bars to check whether chang-\ning one dimension has a measurable effect on the generated\nmusic. We deﬁne 27 metrics, among which are total num-\nber of (held) notes, mean/max/min/range of (speciﬁc or all)\npitches/velocities, and style changes. Besides the obvious\ndimensions where the style classiﬁer is attached, we ﬁnd\nthat some dimensions correlate with the total number of\nnotes played in a song, the highest pitch in a bar, or the\noccurrence of a speciﬁc pitch. The changes can be seen\nwhen plotting the pitches, but are difﬁcult to hear. Fur-\nthermore, the dimensions are very entangled, and chang-\ning one dimension has multiple effects. Higher values for\n\f2f1;2;3gslightly improve the disentanglement of la-\n[Classic (o) ] [ Jazz (+) ]Figure 4 . t-SNE plot of latent vectors for bars from 20 Jazz\nand Classic songs. Bars from the same song were given the\nsame color. Lighter colors mean that the ensemble style\nclassiﬁer was less certain in its prediction.\ntent dimensions, but strongly reduce reconstruction accu-\nracy (see Figure 2). We added samples to YouTube to show\nthe results of manipulating individual latent variables.\n5.3 Generation and Interpolation\nMIDI-V AE is capable of producing smooth interpolations\nbetween bars. This allows us to generate medleys by con-\nnecting short pieces from our dataset. The interpolated\nbars form a musically consistent bridge between the pieces,\nmeaning that, e.g., pitch ranges and velocities increase\nwhen the target bar has higher pitch or velocity values. We\ncan also merge entire songs together by linearly interpolat-\ning the latent vectors for two bar progressions, producing\ninteresting mixes that are surprisingly fun to listen to. The\noriginal songs can sometimes still be identiﬁed in the mix-\ntures, and the resulting music sounds harmonic. We again\nuploaded several audio samples to YouTube (see Medleys ,\nInterpolations andMixtures ).\n6. CONCLUSION\nWe introduce MIDI-V AE, a simple but effective model for\nperforming style transfer between musical compositions.\nWe show the effectiveness of our method on several differ-\nent datasets and provide audio examples. Unlike most ex-\nisting models, MIDI-V AE incorporates both the dynamics\n(velocity and note durations) and instrumentation of mu-\nsic. In the future we plan to integrate our method into a\nhierarchical model in order to capture style features over\nlonger time scales and allow the generation of larger pieces\nof music. To facilitate future research on style transfer for\nsymbolic music, and sequence tasks in general, we make\nour code and data publicly available.5\n5https://github.com/brunnergino/MIDI-VAE752 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20187. REFERENCES\n[1] Midi association, the ofﬁcial midi speciﬁcations.\nhttps://www.midi.org/specifications .\nAccessed: 01-06-2018.\n[2] Yoshua Bengio, Aaron C. Courville, and Pascal Vin-\ncent. Representation learning: A review and new per-\nspectives. IEEE Trans. Pattern Anal. Mach. Intell. ,\n35(8):1798–1828, 2013.\n[3] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Modeling temporal dependencies in\nhigh-dimensional sequences: Application to poly-\nphonic music generation and transcription. In Proceed-\nings of the 29th International Conference on Machine\nLearning, ICML 2012, Edinburgh, Scotland, UK, June\n26 - July 1, 2012 , 2012.\n[4] Jean-Pierre Briot, Ga ¨etan Hadjeres, and Franc ¸ois Pa-\nchet. Deep learning techniques for music generation-a\nsurvey. arXiv preprint arXiv:1709.01620 , 2017.\n[5] Gino Brunner, Yuyi Wang, Roger Wattenhofer, and\nJonas Wiesendanger. JamBot: Music theory aware\nchord based generation of polyphonic music with\nLSTMs. In 29th International Conference on Tools\nwith Artiﬁcial Intelligence (ICTAI) , 2017.\n[6] Kyunghyun Cho, Bart van Merrienboer, C ¸ aglar\nG¨ulc ¸ehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. Learning phrase\nrepresentations using RNN encoder-decoder for sta-\ntistical machine translation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2014, October 25-29,\n2014, Doha, Qatar, A meeting of SIGDAT, a Special\nInterest Group of the ACL , pages 1724–1734, 2014.\n[7] Hang Chu, Raquel Urtasun, and Sanja Fidler. Song\nfrom PI: A musically plausible network for pop music\ngeneration. CoRR , abs/1611.03477, 2016.\n[8] Ching-Hua Chuan and Dorien Herremans. Modeling\ntemporal tonal relations in polyphonic music through\ndeep networks with a novel image-based representa-\ntion. 2018.\n[9] David Cope. Experiments in music intelligence (EMI).\nInProceedings of the 1987 International Computer\nMusic Conference, ICMC 1987, Champaign/Urbana,\nIllinois, USA, August 23-26, 1987 , 1987.\n[10] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-\nHsuan Yang. Musegan: Multi-track sequential gener-\native adversarial networks for symbolic music genera-\ntion and accompaniment. In Proceedings of the Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence,\nNew Orleans, Louisiana, USA, February 2-7, 2018 ,\n2018.[11] Douglas Eck and Juergen Schmidhuber. A ﬁrst look\nat music composition using lstm recurrent neural net-\nworks. Istituto Dalle Molle Di Studi Sull Intelligenza\nArtiﬁciale , 103, 2002.\n[12] Jesse Engel, Cinjon Resnick, Adam Roberts, Sander\nDieleman, Mohammad Norouzi, Douglas Eck, and\nKaren Simonyan. Neural audio synthesis of musical\nnotes with wavenet autoencoders. In Proceedings of\nthe 34th International Conference on Machine Learn-\ning, ICML 2017, Sydney, NSW, Australia, 6-11 August\n2017 , pages 1068–1077, 2017.\n[13] Jose D. Fern ´andez and Francisco J. Vico. AI methods\nin algorithmic composition: A comprehensive survey.\nJ. Artif. Intell. Res. , 48:513–582, 2013.\n[14] Leon A Gatys, Alexander S Ecker, and Matthias\nBethge. Image style transfer using convolutional neural\nnetworks. In Computer Vision and Pattern Recognition\n(CVPR), 2016 IEEE Conference on , pages 2414–2423.\nIEEE, 2016.\n[15] Ga ¨etan Hadjeres, Franc ¸ois Pachet, and Frank Nielsen.\nDeepbach: a steerable model for bach chorales gener-\nation. In Proceedings of the 34th International Confer-\nence on Machine Learning, ICML 2017, Sydney, NSW,\nAustralia, 6-11 August 2017 , pages 1362–1371, 2017.\n[16] Dorien Herremans, Ching-Hua Chuan, and Elaine\nChew. A functional taxonomy of music generation sys-\ntems. ACM Comput. Surv. , 50(5):69:1–69:30, 2017.\n[17] Irina Higgins, Loic Matthey, Arka Pal, Christopher\nBurgess, Xavier Glorot, Matthew Botvinick, Shakir\nMohamed, and Alexander Lerchner. beta-vae: Learn-\ning basic visual concepts with a constrained variational\nframework. 2016.\n[18] Daniel D. Johnson. Generating polyphonic music us-\ning tied parallel networks. In Computational Intel-\nligence in Music, Sound, Art and Design - 6th In-\nternational Conference, EvoMUSART 2017, Amster-\ndam, The Netherlands, April 19-21, 2017, Proceed-\nings, pages 128–143, 2017.\n[19] Diederik P. Kingma and Jimmy Ba. Adam: A method\nfor stochastic optimization. CoRR , abs/1412.6980,\n2014.\n[20] Diederik P. Kingma and Max Welling. Auto-encoding\nvariational bayes. CoRR , abs/1312.6114, 2013.\n[21] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin\nLu, and Ming-Hsuan Yang. Universal style transfer via\nfeature transforms. In Advances in Neural Information\nProcessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 December\n2017, Long Beach, CA, USA , pages 385–395, 2017.\n[22] Laurens van der Maaten and Geoffrey Hinton. Visu-\nalizing data using t-sne. Journal of machine learning\nresearch , 9(Nov):2579–2605, 2008.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 753[23] Iman Malik and Carl Henrik Ek. Neural translation of\nmusical style. CoRR , abs/1708.03535, 2017.\n[24] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani,\nRithesh Kumar, Shubham Jain, Jose Sotelo, Aaron C.\nCourville, and Yoshua Bengio. Samplernn: An un-\nconditional end-to-end neural audio generation model.\nCoRR , abs/1612.07837, 2016.\n[25] Olof Mogren. C-RNN-GAN: continuous recurrent\nneural networks with adversarial training. CoRR ,\nabs/1611.09904, 2016.\n[26] Noam Mor, Lior Wolf, Adam Polyak, and Yaniv Taig-\nman. A universal music translation network. CoRR ,\nabs/1805.07848, 2018.\n[27] Michael C. Mozer. Neural network music composi-\ntion by prediction: Exploring the beneﬁts of psychoa-\ncoustic constraints and multi-scale processing. Con-\nnect. Sci. , 6(2-3):247–280, 1994.\n[28] Colin Raffel and Daniel PW Ellis. Intuitive anal-\nysis, creation and manipulation of midi data with\npretty midi. In 15th International Society for Music\nInformation Retrieval Conference Late Breaking and\nDemo Papers , pages 84–93, 2014.\n[29] Adam Roberts, Jesse Engel, and Douglas Eck. Hier-\narchical variational autoencoders for music. In NIPS\nWorkshop on Machine Learning for Creativity and De-\nsign, 2017.\n[30] Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi\nJaakkola. Style transfer from non-parallel text by cross-\nalignment. In Advances in Neural Information Process-\ning Systems , pages 6833–6844, 2017.\n[31] Peter M Todd. A connectionist approach to algorith-\nmic composition. Computer Music Journal , 13(4):27–\n43, 1989.\n[32] A ¨aron van den Oord, Sander Dieleman, Heiga\nZen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew W. Senior, and Koray\nKavukcuoglu. Wavenet: A generative model for raw\naudio. In The 9th ISCA Speech Synthesis Workshop,\nSunnyvale, CA, USA, 13-15 September 2016 , page 125,\n2016.\n[33] A ¨aron van den Oord, Oriol Vinyals, and Koray\nKavukcuoglu. Neural discrete representation learning.\nInAdvances in Neural Information Processing Sys-\ntems 30: Annual Conference on Neural Information\nProcessing Systems 2017, 4-9 December 2017, Long\nBeach, CA, USA , pages 6309–6318, 2017.\n[34] Li-Chia Yang, Szu-Yu Chou, and Yi-Hsuan Yang.\nMidinet: A convolutional generative adversarial net-\nwork for symbolic-domain music generation. In Pro-\nceedings of the 18th International Society for Music In-\nformation Retrieval Conference, ISMIR 2017, Suzhou,\nChina, October 23-27, 2017 , pages 324–331, 2017.[35] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\nSeqgan: Sequence generative adversarial nets with pol-\nicy gradient. In Proceedings of the Thirty-First AAAI\nConference on Artiﬁcial Intelligence, February 4-9,\n2017, San Francisco, California, USA. , pages 2852–\n2858, 2017.\n[36] Junbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexan-\nder M. Rush, and Yann LeCun. Adversarially regu-\nlarized autoencoders for generating discrete structures.\nCoRR , abs/1706.04223, 2017.\n[37] Jun-Yan Zhu, Taesung Park, Phillip Isola, and\nAlexei A. Efros. Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. In IEEE\nInternational Conference on Computer Vision, ICCV\n2017, Venice, Italy, October 22-29, 2017 , pages 2242–\n2251, 2017.754 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Camera-PrIMuS: Neural End-to-End Optical Music Recognition on Realistic Monophonic Scores.",
        "author": [
            "Jorge Calvo-Zaragoza",
            "David Rizo"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492395",
        "url": "https://doi.org/10.5281/zenodo.1492395",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/33_Paper.pdf",
        "abstract": "The optical music recognition (OMR) field studies how to automate the process of reading the musical notation present in a given image. Among its many uses, an interesting scenario is that in which a score captured with a camera is to be automatically reproduced. Recent approaches to OMR have shown that the use of deep neural networks allows important advances in the field. However, these approaches have been evaluated on images with ideal conditions, which do not correspond to the previous scenario. In this work, we evaluate the performance of an end-to-end approach that uses a deep convolutional recurrent neural network (CRNN) over non-ideal image conditions of music scores. Consequently, our contribution also consists of Camera-PrIMuS, a corpus of printed monophonic scores of real music synthetically modified to resemble camera-based realistic scenarios, involving distortions such as irregular lighting, rotations, or blurring. Our results confirm that the CRNN is able to successfully solve the task under these conditions, obtaining an error around 2% at music-symbol level, thereby representing a groundbreaking piece of research towards useful OMR systems.",
        "zenodo_id": 1492395,
        "dblp_key": "conf/ismir/Calvo-ZaragozaR18",
        "keywords": [
            "automate",
            "reading",
            "musical",
            "notation",
            "image",
            "deep",
            "neural",
            "networks",
            "performance",
            "end-to-end"
        ],
        "content": "CAMERA-PRIMUS: NEURAL END-TO-END OPTICAL MUSIC\nRECOGNITION ON REALISTIC MONOPHONIC SCORES\nJorge Calvo-Zaragoza\nPRHLT Research Center\nUniversitat Polit `ecnica\nde Val `encia, Spain\njcalvo@prhlt.upv.esDavid Rizo\nInstituto Superior de Ense ˜nanzas Art ´ısticas\nde la Comunidad Valenciana (ISEA.CV)\nUniversidad de Alicante, Spain\ndrizo@dlsi.ua.es\nABSTRACT\nThe optical music recognition (OMR) ﬁeld studies how\nto automate the process of reading the musical notation\npresent in a given image. Among its many uses, an in-\nteresting scenario is that in which a score captured with\na camera is to be automatically reproduced. Recent ap-\nproaches to OMR have shown that the use of deep neural\nnetworks allows important advances in the ﬁeld. However,\nthese approaches have been evaluated on images with ideal\nconditions, which do not correspond to the previous sce-\nnario. In this work, we evaluate the performance of an\nend-to-end approach that uses a deep convolutional recur-\nrent neural network (CRNN) over non-ideal image condi-\ntions of music scores. Consequently, our contribution also\nconsists of Camera-PrIMuS, a corpus of printed mono-\nphonic scores of real music synthetically modiﬁed to re-\nsemble camera-based realistic scenarios, involving distor-\ntions such as irregular lighting, rotations, or blurring. Our\nresults conﬁrm that the CRNN is able to successfully solve\nthe task under these conditions, obtaining an error around\n2%at music-symbol level, thereby representing a ground-\nbreaking piece of research towards useful OMR systems.\n1. INTRODUCTION\nThe optical music recognition (OMR) discipline was born\nseveral decades ago [28], and nowadays there are still too\nmany open problems to consider it a solved task. This ap-\nplies not only for handwritten notation but also for the case\nof printed scores [4]. Unfortunately, unlike other auto-\nmatic content transcription domains, such as speech recog-\nnition [23] or optical character recognition [24], the latest\nadvances in pattern recognition and machine learning—\nnamely deep learning—have not deﬁnitively broken the\nlong-term glass ceiling.\nActually, other computer music domains are taking ad-\nvantage of these advances, but quite often, especially in\nsymbolic music research, the lack of big enough datasets\nc\rJorge Calvo-Zaragoza, David Rizo. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Jorge Calvo-Zaragoza, David Rizo. “Camera-PrIMuS:\nNeural End-to-End Optical Music Recognition on Realistic Monophonic\nScores”, 19th International Society for Music Information Retrieval Con-\nference, Paris, France, 2018.block their improvement. If OMR technologies were able\nto convert the massive printed scores libraries1into struc-\ntured, symbolic scores, all those ﬁelds would obtain inter-\nesting corpora to work on.\nFurthermore, out of the scientiﬁc community, the avail-\nability of tools that transcribe sheet music without errors\ninto symbolically-encoded music would help professional\nand amateur musicians to take advantage of the plenty of\ncomputer music tools at hand that cannot work directly\nwith digital images.\nFollowing the steps of other aforementioned disciplines,\nwe claim that the problem can be appropriately addressed\nwith holistic approaches, i.e., end-to-end, where systems\nlearn with just pairs of inputs and their corresponding tran-\nscripts. Here, these pairs consists of sheet music and their\nsymbolic encoding.\nIn this work, we extend previous proposals that applied\nneural network models over monodic digitally-rendered\nmusic scores [8]. However, we evaluate here their per-\nformance with a set of scores that are rendered simulat-\ning camera-based conditions. Our objective is to study\nwhether the approach is feasible for non-ideal image con-\nditions. Although we do not experiment with fully-ﬂedged\nscores yet, we believe that this avenue is promising for\nreaching the ﬁnal objective of dealing with any kind of\ninput score. Thus, in this work we introduce the so-\ncalled Camera-Printed Images of Music Staves (Camera-\nPrIMuS) dataset of monodic single-staff printed scores,\nthat have been distorted to resemble photographed scores\nand encoded in such a way a neural network recognizer can\nmanage.\nOur experiments demonstrate that the considered neural\nmodels are able to learn even in difﬁcult situations where\nnone of the current commercial OMR systems might be\nsuccessful. The results reﬂect that an error rate below 2%,\nat symbol level, can be attained.\nThe paper is organized as follows: ﬁrst, a brief back-\nground about OMR is given in Sect. 2; then, the construc-\ntion of Camera-PrIMuS dataset is detailed in Sect. 3; the\nneural end-to-end framework is described and formalized\nin Sect. 4; the experimental results that demonstrate the\nsuitability of the approach are reported in Sect. 5; and ﬁ-\nnally, the conclusions are discussed in Sect. 6.\n1Libraries such as http://imslp.org2482. BACKGROUND\nMost of the existing OMR approaches work in a multi-\nstage fashion [38]. These systems typically perform an ini-\ntial processing of the image that consists of several steps of\ndocument analysis, not always strictly related to the musi-\ncal domain. Examples of this stage comprise the binariza-\ntion of the image [10], the detection of the staves [11], the\ndelimitation of the staves in terms of bars [45], or the sep-\naration among the different sources of information [5].\nThe staff-line removal stage requires a special mention.\nAlthough staff lines represent a very important element in\nmusic notation, their presence hinders the automatic seg-\nmentation of musical symbols. Therefore, much effort has\nbeen devoted to successfully solving this stage [14,15,18].\nRecently, results have reached values closer to the opti-\nmum over standard benchmarks [7, 17].\nIn the next step, remaining symbols are classiﬁed into\nmusic-notation categories. A number of works can be\nfound in the literature that deal with this task [30, 37], in-\ncluding deep learning classiﬁcation as well [6, 32].\nRecently, it has been demonstrated that the traditional\npipeline up to symbol classiﬁcation can be replaced by\ndeep region-based neural networks [31], which both local-\nize and classify music-notation primitives from the input\nimage. Either way, once graphical symbol are identiﬁed,\nthey must be assembled to eventually obtain actual music\nnotation. Previous attempts to this stage proposed the use\nof heuristic strategies based on graphical and syntactical\nrules [13, 36, 40, 43].\nFull approaches are more common when recognizing\nmensural notation, where the OMR challenge is more re-\nstricted than that of modern Western notation because of\nthe absence of simultaneous written voices in the same\nstaff and a lower number of symbols to be recognized [9,\n33, 44].\n3. THE CAMERA-PRIMUS DATASET\nThe training of a machine learning based system requires\na good quality training dataset with enough size to statis-\ntically include a representative sample of the problem to\nbe solved. The Camera-based Printed Images of Music\nStaves (Camera-PrIMuS) dataset has been devised to ful-\nﬁl both requirements2. Thus, the objective pursued when\ncreating this ground-truth data is not to represent the most\ncomplex musical notation corpus, but to collect the high-\nest possible number of scores readily available to be repre-\nsented in formats suitable for heterogeneous OMR experi-\nmentation and evaluation.\nCamera-PrIMuS is an extension of a previously pub-\nlished PrIMuS dataset [8]. It contains 87 678 real-music\nincipits,3each one represented by six ﬁles: the Plaine and\nEasie Code (PAEC) source [3], an image with the rendered\nscore, the same image distorted resembling a camera-based\nscenario, the music symbolic representation of the incipit\n2The dataset is freely available at https://grfia.dlsi.ua.\nes/primus/ .\n3An incipit is a short sequence of notes from the beginning of a\nmelody or musical work usually used for identifying itOrder Filter Ranges of used parameters\n1-implode [0;0:07]\n2-chop [1;5],[1;6],[1;300],[1;50]\n3-swirl [\u00003;3]\n4-spread -2\n5-shear [\u00005;5],[\u00001:5;1:5]\n6-shade [0;120],[80;110]\n7-wave [0;0:5],[0;0:4]\n8-rotate [0;0:3]\n9-noise [0;1:2]\n10-wave [0;0:5],[0;0:4]\n11-motion-blur [\u00007;5],[\u00007;7],[\u00007;6]\n12-median [0;1:1]\nTable 1 . GraphicsMagick ﬁlter sequence\nboth in Music Encoding Initiative format (MEI) [39] and\nin an on-purpose simpliﬁed encoding (semantic encoding),\nand a sequence containing the graphical symbols shown in\nthe score with their position in the staff, without any musi-\ncal meaning (agnostic encoding). These two agnostic and\nsemantic representations, that will be described below, are\nespecially designed to be considered in our framework.\nPursuing the objective of considering real music, and\nbeing restricted to use short single-staff scores, an export\nin PAEC format of the RISM dataset [29] has been used\nas source. The PAEC is then formatted to be fed into the\nmusical engraver Verovio [34], that outputs both the musi-\ncal score in SVG format—that is posteriorly converted into\nPNG format (Fig. 1(a))—and the MEI encoding containing\nthe symbolic semantic representation of the score in XML\nformat. Verovio is able to render scores using three differ-\nent fonts, namely: Leipzig, Bravura, and Gootville. This\ncapability has been used by randomly choosing one of the\nthose fonts in the rendering of the different incipits, lead-\ning to a higher variability in the dataset. The on-purpose\nsemantic and agnostic representations (Figs. 1(c) and 1(d))\nhave been obtained as a conversion from the MEI ﬁles. Fi-\nnally, the PNG image ﬁle is distorted, as described below,\nin order to simulate imperfections introduced by taking a\npicture of the sheet music from a (bad) camera (Fig. 1(b)).\nTo simulate distortions, the GraphicsMagick image pro-\ncessing tool4has been used. Among the huge amount of\nﬁlters this tool contains, a number of them have been used\nand tweaked empirically. Table 1 contains the ﬁlters used\nand the ranges considered for each parameter, from which\nrandom values are selected at each instance. Filters have\nbeen applied using the order shown in the table.\n3.1 Semantic and agnostic representations\nThe suitable encoding of input data for the neural network\ndetermines the scope of its performance. Most of the avail-\nable symbolic representations [41], being devised for other\npurposes such as music analysis (e.g. **kern ), or music\n4http://www.graphicsmagick.orgProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 249(a) Clean image.\n (b) Distorted image.\nclef-G2, keySignature-GM, timeSignature-2/4, note-G4 sixteenth., note-B4 thirty second, barline, note-D5 eighth, rest-sixteenth,\nnote-B4 sixteenth, note-D5 eighth., note-C5 thirty second, note-A4 thirty second, barline, note-F#4 quarter, rest-eighth,\nnote-A4 sixteenth., note-C5 thirty second, barline, note-E5 eighth, rest-sixteenth, note-C#5 sixteenth, note-E5 eighth.,\nnote-D5 thirty second, note-B4 thirty second, barline, note-G4 eighth, rest-eighth\n(c) Semantic encoding.\nclef.G-L2, accidental.sharp-L5, digit.2-L4, digit.4-L2, note.beamedRight2-L2, dot-S2, note.beamedLeft3-L3, barline-L1,\nnote.eighth-L4, rest.sixteenth-L3, note.sixteenth-L3, note.beamedRight1-L4, dot-S4, note.beamedBoth3-S3, note.beamedLeft3-S2, barline-L1,\nnote.quarter-S1, rest.eighth-L3, note.beamedRight2-S2, dot-S2, note.beamedLeft3-S3, barline-L1,\nnote.eighth-S4, rest.sixteenth-L3, accidental.sharp-S3, note.sixteenth-S3, note.beamedRight1-S4, dot-S4,\nnote.beamedBoth3-L4, note.beamedLeft3-L3, barline-L1, note.eighth-L2, rest.eighth-L3\n(d) Agnostic encoding.\nFigure 1 . Example of a short item in the corpus: Incipit RISM ID no. 000100367, Incipit 28.1.1 30 Canons , Luigi\nCherubini. MEI and Plaine and Easie Code ﬁles are also included in the corpus but omitted here.\nnotation (such as MEI [39] or MusicXML [20])—for nam-\ning just a few—do not encode a self-contained chunk of in-\nformation for each musical element. This is why two repre-\nsentations devised on-purpose compliant with this require-\nment were introduced in [8], namely the semantic and the\nagnostic ones. For practical issues, none of the representa-\ntions is musically exhaustive, but representative enough to\nserve as a starting point from which to build more complex\nsystems.\nThe semantic representation contains symbols with mu-\nsical meaning, e.g., a G Major key signature (see Fig. 1(c));\nthe agnostic encoding (see Fig. 1(d)) consists of musical\nsymbols without musical meaning that should be eventu-\nally interpreted in a ﬁnal parsing stage [16], e.g. a D Major\nkey signature is represented as a sequence of two sharp\nsymbols. This way, the alphabet used for the agnostic\nrepresentation is much smaller, which allows to study the\nimpact of the alphabet size and the number of examples\nshown to the network for its training. Note that in the ag-\nnostic representation, a sharp symbol in the key signature\nis the same pictogram as a sharp accidental altering the\npitch of a note. A complete description of the grammars\ndescribing these encodings can be found in [8].\nMore speciﬁcally, the agnostic representation contains a\nlist of graphical symbols in the score, each of them tagged\ngiven a catalogue of pictograms without a predeﬁned mu-\nsical meaning, and located in a position in the staff ( e.g.,\nthird line, ﬁrst space). The Cartesian plane position of\nsymbols has been encoded relatively, following a left-to-\nright, top-down ordering (see encoding of fractional me-\nter in Fig. 1(d)). In order to represent beaming of notes,\nthey have been vertically sliced generating non-musical\npictograms (see elements with preﬁx note.beamed in\nFig. 1(d)).\nAs mentioned above, this new way of encoding com-\nplex information in a simple sequence allows us to feed\nthe network in a relatively easy way. Note that the agnostic\nrepresentation is different from a primitive-based segmen-\ntation of the image, which is the usual internal representa-\ntion of traditional OMR systems [12, 25].The agnostic representation has an additional advan-\ntage: in other less known music notations, such as the\nearly neumatic and mensural notations, or in the case of\nnon-Western notations, it might be easier to transcribe\nthe manuscript through two stages: one stage performed\nby any non-musical expert that only needs to identify\npictograms (agnostic representation), and a second stage\nwhere a musicologist, maybe aided by a computer, inter-\nprets them to yield a semantic encoding.\n4. NEURAL END-TO-END APPROACH FOR\nOPTICAL MUSIC RECOGNITION\nAs introduced above, some previous work have proved that\nit is possible to successfully accomplish the recognition of\nmonodic staves in an end-to-end approach by using neural\nnetworks [8]. This section contains a brief description of\nsuch framework.\nA single-voice monophonic staff is assumed to be the\nbasic unit; that is, a single monodic staff will be processed\nat each instance. Formally, let S=f(x1; y1);(x2; y2); :::g\nbe our end-to-end application domain, where xirepresents\na single staff image and yiis its corresponding sequence of\nmusic symbols, each of which belongs to a ﬁxed alphabet\nset\u0006.\nGiven an input staff image, the OMR problem can be\nsolved by retrieving its most likely sequence of music sym-\nbols^y:\n^y= arg max\ny2\u0006\u0003P(yjx) (1)\nA graphical scheme of the considered framework is\ngiven in Figure 2. The input image depicting a monodic\nstaff is fed into a Convolutional Recurrent Neural Network\n(CRNN), which consists of two sequential parts: a con-\nvolutional block and a recurrent block. The convolutional\nblock is in charge of learning how to deal with the input\nimage [47]. In this way, the user is prevented from per-\nforming a pre-processing of the image because this block is\nable to learn adequate features from the training set. These250 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018LSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nPredictions\nF1\nF2\nFW\nFeatures\nCNN\nInput\nOutput\nY1\nY2\nYN\nCTC\nGround-truth\nBackpropagationTRAININGFigure 2 . Graphical scheme of the end-to-end neural approach considered.\nextracted features are provided to the recurrent block [21],\nproducing the sequence of musical symbols that approxi-\nmates Eq. 1.\nSince both convolutional and recurrent blocks are con-\nﬁgured as feed-forward models, the training stage can be\ncarried out jointly. This scheme can be easily implemented\nby connecting the output of the last layer of the convolu-\ntional block with the input of the ﬁrst layer of the recurrent\nblock, concatenating all the output channels of the convo-\nlutional part into a single image. Then, columns of the\nresulting image are treated as individual frames for the re-\ncurrent block.\nThe traditional training mechanisms for a CRNN need a\nframewise expected output, where a frame is a ﬁxed-width\nvertical slice of the image. However, as the goal is to not\nrecognize frames but complete symbols, either semantic or\nagnostic, and Camera-PriMuS does not contain sequences\nof labelled frames, a Connectionist Temporal Classiﬁca-\ntion (CTC) loss function [22] has been used to solve this\nmismatch.\nBasically, CTC drives the CRNN to optimize its pa-\nrameters so that it is likely to give the correct sequence\nygiven an input x. As optimizing this likelihood exhaus-\ntively is computationally expensive, CTC performs a lo-\ncal optimization using an Expectation-Maximization al-\ngorithm similar to that used for training Hidden Markov\nModels [35]. Note that CTC is only used for training,\nwhile at the decoding stage the framewise CRNN output\ncan be straightforwardly decoded into a sequence of music\nsymbols (details are given below).\n4.1 Implementation details\nThe speciﬁc organization of the neural model is given\nin Table 2. As observed, variable-width single-channel\n(grayscale) input image are rescaled at a ﬁxed height of\n128pixels, without modifying their aspect ratio. This in-\nput is processed through a convolutional block inspired\nby a VGG network, a typical model in computer vision\ntasks [42]: four convolutional layers with an incremental\nnumber of ﬁlters and kernel sizes of 3\u00023, followed by\na2\u00022max-pool operator. In all cases, Batch Normal-\nization [27] and Rectiﬁed Linear Unit activations [19] are\nconsidered.Input(128 \u0002W\u00021)\nConvolutional block\nConv (32;3\u00023), MaxPooling (2\u00022)\nConv (64;3\u00023), MaxPooling (2\u00022)\nConv (128;3\u00023), MaxPooling (2\u00021)\nConv (256;3\u00023), MaxPooling (2\u00021)\nRecurrent block\nBLSTM (256)\nBLSTM (256)\nDense (j\u0006j+ 1)\nSoftmax ()\nTable 2 . Instantiation of the CRNN used in this work,\nconsisting of 4convolutional layers and 2 recurrent lay-\ners. Notation: Input (h\u0002w\u0002c)means an input image of\nheight h, width wandcchannels; Conv (n; h\u0002w)denotes\na convolution operator of nﬁlters and kernel size of h\u0002w;\nMaxPooling (h\u0002w)represents a down-sampling operation\nof the dominating value within a window of size (h\u0002w);\nBLSTM(n) means a bi-directional Long Short-Term Mem-\nory unit of nneurons; Dense(n) denotes a dense layer of n\nneurons; and Softmax() represents the softmax activation\nfunction. \u0006denotes the alphabet of musical symbols con-\nsidered.\nAt the output of this block, two bidirectional recurrent\nlayers of 256neurons, implemented as Long Short-Term\nMemory (LSTM) units [26], try to convert the resulting\nﬁltered image into a discrete sequence of musical sym-\nbols that takes into account both the input sequence and\nthe modelling of the musical representation. Note that\neach frame performs an independent classiﬁcation, mod-\nelled with a fully-connected layer with as many neurons as\nthe size of the alphabet plus 1 (a blank symbol necessary\nfor the CTC function). The activation of these neurons is\ngiven by a softmax function, which allows interpreting the\noutput as a posterior probability over the alphabet of music\nsymbols [2].\nThe learning process is carried out by means of stochas-\ntic gradient descent (SGD) [1], which modiﬁes the CNN\nparameters through back-propagation to minimize theProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 251CTC loss function. In this regard, the mini-batch size is\nestablished to 16samples per iteration. The learning rate\nof the SGD is updated adaptively following the Adadelta\nalgorithm [46].\nOnce the network is trained, it is able to provide a pre-\ndiction in each frame of the input image. These predictions\nmust be post-processed to emit the actual sequence of pre-\ndicted musical symbols. Thanks to training with the CTC\nloss function, the ﬁnal decoding can be performed greed-\nily [22]: when the symbol predicted by the network in a\nframe is the same as the previous one, it is assumed that\nthey represent frames of the same symbol, and only one\nsymbol is concatenated to the ﬁnal sequence. There are\ntwo ways to indicate that a new symbol is predicted: either\nthe predicted symbol in a frame is different from the pre-\nvious one, or the predicted symbol of a frame is the blank\nsymbol, which indicates that no symbol is actually found.\nThus, given an input image, a discrete musical symbol\nsequence is obtained. Note that the only limitation is that\nthe output cannot contain more musical symbols than the\nnumber of frames of the input image, which in our case is\nhighly unlikely to happen.\n5. EXPERIMENTS\n5.1 Experimental setup\nOnce introduced the Camera-PrIMuS dataset, and a model\nable to learn the OMR task from it, some experiments have\nbeen performed whose results may serve as a baseline to\nwhich other works can be compared.5\nCurrently, there is an open debate on which evaluation\nmetrics should be used in OMR [4]. This is especially\narguable because of the different points of view that the\nuse of its output has: it is not the same whether the inten-\ntion of the OMR is to automatically play the content or to\narchive it in a digital library. Here we are only interested in\nthe computational aspect itself. Hence, we shall consider\nmetrics focused on the symbol and sequence recognition,\navoiding any music-speciﬁc consideration, such as:\n\u000fSequence Error Rate (ER) (%): ratio of incorrectly\npredicted sequences (at least one error).\n\u000fSymbol Error Rate (SER) (%): the average number\nof elementary editing operations (insertions, dele-\ntions, or substitutions) needed to produce the refer-\nence sequence from the one predicted by the model,\nnormalized by its length.\nNote that the length of the agnostic and semantic se-\nquences are usually different because they are encoding\ndifferent aspects of the same source. Therefore, the com-\nparison in terms of Symbol Error Rate, in spite of being\nnormalized, may not be totally fair. On the other hand,\nthe Sequence Error Rate allows a more reliable compar-\nison because it only takes into account the perfectly pre-\n5For the sake of reproducible research, source code and trained\nmodels are available at https://github.com/calvozaragoza/\ntf-deep-omr .dicted sequences (in which case, the outputs in different\nrepresentations are equivalent).\n5.2 Performance\nWe show in this section the results obtained in our experi-\nments. We consider three different data partitions: 80% of\nthe data is used as training set, to optimize the network ac-\ncording to the CTC loss function; 10% of the data is used\nas validation set, which is used to decide when to stop the\noptimization to prevent over-ﬁtting; the evaluation results\nare computed with the remaining 10%, which constitutes\nthe test partition.\nIn order to study the ability of the system to learn in\ndifferent situations, four scenarios have been evaluated de-\npending upon which set of images are used for training and\ntesting, either the clean original ﬁles or the synthetically\ndistorted ones. We report in Table 3 the whole evaluation.\nThe results show that the system, trained with the ap-\npropriate set, is able to correctly recognize in almost all\nscenarios, with error rates at symbol level below 2%. In\nan ideal scenario, where only clean images are given, the\nsemantic encoding outperforms the agnostic one. The be-\nhaviour is different when distorted images are used, for\nwhich the agnostic representations behave much better.\nWhat seems most interesting from these results is the abil-\nity of the system to learn from distorted images and cor-\nrectly classify both distorted and clean versions. This leads\nus to conclude that the networks are being able to abstract\nthe content from the image condition. As a qualitative ex-\nample of the performance attained, the sample of Figure 1\nwas correctly classiﬁed using both encodings.\nIn an informal analysis, we observed that the most re-\npeated error, both in agnostic and semantic encodings, is\nthe incorrect classiﬁcation of the ending bar line. In ad-\ndition to it, no other repeating mistake has been found.\nAlso, we checked that most of the wrongly recognized\nsamples only failed at 1 symbol. Another interesting fea-\nture to emphasize is that we observed an independence of\nthe mistakes with respect to the length of the ground-truth\nsequence, i.e., errors are not accumulated and, therefore,\nthe number of mistakes do not necessarily increase with\nlonger sequences. Figures 3 and 4 depict two examples of\nwrongly recognized sequences.\n6. CONCLUSIONS\nThe suitability of a neural network approach to solve the\nOMR task in an end-to-end fashion has been evaluated\non realistic single-staff printed monodic scores from a\nreal world dataset. To this end, the new Camera-PrIMuS\ndataset has been introduced, containing 87 678 images syn-\nthetically distorted to resemble a camera-based scenario.\nThe neural network model considered consists of a\nCRNN, in which convolutions process the input image\nand recurrent blocks deal with the sequential nature of the\nproblem. In order to train this model directly using symbol\nsequences, instead of ﬁne-grained annotated images, the\nso-called CTC loss function has been utilized.252 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Evaluation\nClean Distortions\nAgnostic Semantic Agnostic Semantic\nTrainingClean 1.1 / 21.7 0.8 / 12.5 44.3 / 94.1 59.7 / 97.9\nDistortions 1.4 / 24.9 3.3 / 44.6 1.6 / 24.7 3.4 / 38.3\nTable 3 . Average SER (%) / ER (%) reported in all possible combinations of training and evaluation conditions.\n(a) Distorted image ﬁle of Incipit RISM ID no. 000104754, Incipit 1.1.1 Achille in Sciro. Excerpts . Niccol `o\nJommelli.\nclef-G2, keySignature-DM, timeSignature-C, note-D5 half, tie, note-D5 quarter., note-F#4 eighth, barline, note-G4 half,\nnote-F#4 quarter, rest-quarter, barline, note-B4 eighth, rest-eighth, note-A4 eighth, rest-eighth, note-B4 half ,[rest-eighth-L3]\nnote-E5 eighth., note-C#5 sixteenth, barline, note-F#5 half, tie, note-F#5 quarter., note-F#4 eighth,\nbarline, note-G4 half, note-F#4 quarter, rest-quarter, barline\n(b) Semantic encoding network output. The symbol in italics should be classiﬁed as note-B4 eighth , and the bold symbol between brackets has\nbeen omitted by the network.\nclef.G-L2, accidental.sharp-L5, accidental.sharp-S3, metersign.C-L3, note.half-L4, slur.start-L4, slur.end-L4,\nnote.quarter-L4, dot-S4, note.eighth-S1, barline-L1, note.half-L2, note.quarter-S1, rest.quarter-L3, barline-L1,\nnote.eighth-L3, rest.eighth-L3, note.eighth-S2, rest.eighth-L3, fermata.above-S6 ,note.quarter-L3 , note.beamedRight1-S4,\ndot-S4, note.beamedLeft2-S3, barline-L1, note.half-L5, slur.start-L5, slur.end-L5, note.quarter-L5, dot-S5, note.eighth-S1,\nbarline-L1, note.half-L2, note.quarter-S1, rest.quarter-L3, barline-L1\n(c) Agnostic encoding network output. Wrong symbols have been highlighted in italic face symbols. They should be note.eighth-L3 and\nrest.eighth-L3 , respectively.\nFigure 3 . This incipit contains distortions that are very hard to recognize, such as the scratch at the beginning of the staff\nand some overlapped ink. Despite these difﬁculties, just two symbols in each encoding have been wrongly recognized.\n(a) Distorted image ﬁle of Incipit RISM ID no. 000100170, Incipit 1.1.1 Trios . Joseph Haydn.\nclef-G2, keySignature-FM, timeSignature-C, note-F4 quarter, rest-quarter, rest-eighth, note-A4 sixteenth, note-Bb4 sixteenth, note-C5 eighth,\nnote-C5 eighth, barline, note-C5 eighth, note-F5 eighth, note-A4 eighth, note-A4 eighth, note-A4 eighth, note-C5 eighth, note-F4 eighth,\nnote-F4 eighth, barline, note-E4 eighth, note-D4 eighth, note-D4 quarter, tie, note-D4 eighth, note-C5 sixteenth, note-Bb4 sixteenth,\nnote-A4 sixteenth, note-G4 sixteenth, note-F4 sixteenth, note-D4 thirty second , barline\n(b) Semantic encoding network output. The italic font face symbol should be classiﬁed as a sixteenth note.\nclef.G-L2, accidental.flat-L3, metersign.C-L3, note.quarter-S1, rest.quarter-L3, rest.eighth-L3, note.beamedRight2-S2, note.beamedLeft2-L3,\nnote.beamedRight1-S3, note.beamedLeft1-S3, barline-L1, note.beamedRight1-S3, note.beamedBoth1-L5, note.beamedBoth1-S2, note.beamedLeft1-S2,\nnote.beamedRight1-S2, note.beamedBoth1-S3, note.beamedBoth1-S1, note.beamedLeft1-S1, barline-L1, note.beamedRight1-L1, note.beamedLeft1-S0,\nnote.quarter-S0, slur.start-S0, slur.end-S0, note.beamedRight1-S0, note.beamedBoth2-S3, note.beamedLeft2-L3, note.beamedRight2-S2,\nnote.beamedBoth2-L2, note.beamedBoth2-S1, note.beamedLeft2-S0, barline-L1\n(c) Agnostic encoding network output. All symbols are correctly detected.\nFigure 4 . Incipit correctly recognized using the agnostic representation but with one mistake using the semantic encoding.\nOur experiments have reﬂected the correct construction\nand the usefulness of the corpus. The end-to-end neural\noptical recognition model has demonstrated its ability to\nlearn from adverse conditions and to correctly classify both\nperfectly clean images and imperfect pictures. In regard to\nthe output encoding, the agnostic representation has been\nshown to be more robust against the image distortions,\nwhile semantic encoding maintains a fair performance.\nGiven these promising results, from the musical point of\nview, the next steps seem obvious: ﬁrst, we would like to\ncomplete the catalogue of symbols, thus including chords\nand multiple-voice polyphonic staves. In the long-term, the\nintention is to consider fully-ﬂedged real piano or orches-\ntral scores. Concerning the most technical aspect, it would\nbe interesting to study a multi-prediction model that usesall the different representations at the same time. Given the\ncomplementarity of the agnostic and semantic representa-\ntions, it is feasible to think of establishing a synergy that\nends up with better results in all senses.\n7. ACKNOWLEDGEMENT\nThis work was partially supported by the Spanish Ministe-\nrio de Econom ´ıa, Industria y Competitividad through His-\npaMus project (TIN2017-86576-R) and Juan de la Cierva\n- Formaci ´on grant (Ref. FJCI-2016-27873), and the Social\nSciences and Humanities Research Council of Canada.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 2538. REFERENCES\n[1] L. Bottou. Large-scale machine learning with stochas-\ntic gradient descent. In Proceedings of COMP-\nSTAT’2010 , pages 177–186. Springer, 2010.\n[2] H. Bourlard and C. Wellekens. Links between markov\nmodels and multilayer perceptrons. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence ,\n12(11):1167–1178, 1990.\n[3] B. Brook. The Simpliﬁed ’Plaine and Easie Code Sys-\ntem’ for Notating Music: A Proposal for Interna-\ntional Adoption. Fontes Artis Musicae , 12(2-3):156–\n160, 1965.\n[4] D. Byrd and J. G. Simonsen. Towards a Standard\nTestbed for Optical Music Recognition: Deﬁnitions,\nMetrics, and Page Images. Journal of New Music Re-\nsearch , 44(3):169–195, 2015.\n[5] J. Calvo-Zaragoza, F. J. Castellanos, G. Vigliensoni,\nand I. Fujinaga. Deep neural networks for document\nprocessing of music score images. Applied Sciences ,\n8(5):654–674, 2018.\n[6] J. Calvo-Zaragoza, A.-J. Gallego, and A. Pertusa.\nRecognition of handwritten music symbols with con-\nvolutional neural codes. In 14th IAPR International\nConference on Document Analysis and Recognition ,\npages 691–696, 2017.\n[7] J. Calvo-Zaragoza, A. Pertusa, and J. Oncina. Staff-\nline detection and removal using a convolutional neural\nnetwork. Machine Vision & Applications , 28(5-6):665–\n674, 2017.\n[8] J. Calvo-Zaragoza and D. Rizo. End-to-end neural op-\ntical music recognition of monophonic scores. Applied\nSciences , 8(4):606–629, 2018.\n[9] J. Calvo-Zaragoza, A. H. Toselli, and E. Vidal. Early\nhandwritten music recognition with hidden markov\nmodels. In 15th International Conference on Frontiers\nin Handwriting Recognition , pages 319–324, 2016.\n[10] J. Calvo-Zaragoza, G. Vigliensoni, and I. Fujinaga.\nPixel-wise binarization of musical documents with\nconvolutional neural networks. In Fifteenth IAPR Inter-\nnational Conference on Machine Vision Applications ,\npages 362–365, 2017.\n[11] V . B. Campos, J. Calvo-Zaragoza, A. H. Toselli, and E.\nVidal-Ruiz. Sheet music statistical layout analysis. In\n15th International Conference on Frontiers in Hand-\nwriting Recognition , pages 313–318, 2016.\n[12] L. Chen, E. Stolterman, and C. Raphael. Human-\nInteractive Optical Music Recognition. In 17th Inter-\nnational Society for Music Information Retrieval Con-\nference , pages 647–653, 2016.[13] B. Couasnon. Dmos: A generic document recognition\nmethod, application to an automatic generator of musi-\ncal scores, mathematical formulae and table structures\nrecognition systems. In 6th International Conference\non Document Analysis and Recognition , pages 215–\n220, 2001.\n[14] C. Dalitz, M. Droettboom, B. Pranzas, and I. Fujinaga.\nA comparative study of staff removal algorithms. IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence , 30(5):753–766, 2008.\n[15] J. Dos Santos Cardoso, A. Capela, A. Rebelo, C.\nGuedes, and J. Pinto da Costa. Staff Detection with\nStable Paths. IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 31(6):1134–1139, 2009.\n[16] H. Fahmy and D. Blostein. A graph grammar program-\nming style for recognition of music notation. Machine\nVision and Applications , 6(2-3):83–99, March 1993.\n[17] A. Gallego and J. Calvo-Zaragoza. Staff-line removal\nwith selectional auto-encoders. Expert Systems with\nApplications , 89:138–48, 2017.\n[18] T. G ´eraud. A morphological method for music score\nstaff removal. In 21st International Conference on Im-\nage Processing , pages 2599–2603, Paris, France, 2014.\n[19] X. Glorot, A. Bordes, and Y . Bengio. Deep sparse recti-\nﬁer neural networks. In Fourteenth International Con-\nference on Artiﬁcial Intelligence and Statistics , pages\n315–323, 2011.\n[20] M. Good and G. Actor. Using MusicXML for File In-\nterchange. International Conference on Web Deliver-\ning of Music , page 153, 2003.\n[21] A. Graves. Supervised sequence labelling with recur-\nrent neural networks . PhD thesis, Technical University\nMunich, 2008.\n[22] A. Graves, S. Fern ´andez, F. Gomez, and J. Schmid-\nhuber. Connectionist temporal classiﬁcation: Labelling\nunsegmented sequence data with recurrent neural net-\nworks. In 23rd International Conference on Machine\nLearning , pages 369–376, 2006.\n[23] A. Graves, A.-R. Mohamed, and G. Hinton. Speech\nrecognition with deep recurrent neural networks. In\nIEEE International Conference on Acoustics, Speech\nand Signal Processing , pages 6645–6649, 2013.\n[24] A. Graves and J. Schmidhuber. Ofﬂine handwriting\nrecognition with multidimensional recurrent neural\nnetworks. In Advances in neural information process-\ning systems , pages 545–552, 2009.\n[25] J. Hajic and P. Pecina. The MUSCIMA++ dataset for\nhandwritten optical music recognition. In 14th IAPR\nInternational Conference on Document Analysis and\nRecognition , pages 39–46, 2017.254 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[26] S. Hochreiter and J. Schmidhuber. Long short-term\nmemory. Neural Computation , 9(8):1735–1780, 1997.\n[27] S. Ioffe and C. Szegedy. Batch normalization: accel-\nerating deep network training by reducing internal co-\nvariate shift. In 32nd International Conference on Ma-\nchine Learning , pages 448–456, 2015.\n[28] M. Kassler. Optical character-recognition of printed\nmusic: A review of two dissertations. Perspectives of\nNew Music , 11(1):250–254, 1972.\n[29] K. Keil and J. A. Ward. Applications of RISM data in\ndigital libraries and digital musicology. International\nJournal on Digital Libraries , 50(2):199, January 2017.\n[30] S. Lee, S. J. Son, J. Oh, and N. Kwak. Handwritten\nmusic symbol classiﬁcation using deep convolutional\nneural networks. In 3rd International Conference on\nInformation Science and Security , 2016.\n[31] A. Pacha, K.-Y . Choi, B. Co ¨uasnon, Y . Ricquebourg,\nR. Zanibbi, and H. Eidenberger. Handwritten music ob-\nject detection: Open issues and baseline results. In 13th\nIAPR Workshop on Document Analysis Systems , 2018.\n[32] A. Pacha and H. Eidenberger. Towards a universal mu-\nsic symbol classiﬁer. In 12th International Workshop\non Graphics Recognition , pages 35–36, 2017.\n[33] L. Pugin. Optical music recognition of early typo-\ngraphic prints using hidden markov models. In 7th\nInternational Conference on Music Information Re-\ntrieval , pages 53–56, 2006.\n[34] L. Pugin, R. Zitellini, and P. Roland. Verovio - A li-\nbrary for Engraving MEI Music Notation into SVG. In\nInternational Society for Music Information Retrieval ,\n2014.\n[35] L. Rabiner and B.-H. Juang. Fundamentals of speech\nrecognition . Prentice hall, 1993.\n[36] C. Raphael and J. Wang. New Approaches to Optical\nMusic Recognition. In 12th International Society for\nMusic Information Retrieval Conference , pages 305–\n310, 2011.\n[37] A. Rebelo, A. Capela, and J. S. Cardoso. Optical recog-\nnition of music symbols: A comparative study. Inter-\nnational Journal on Document Analysis and Recogni-\ntion, 13(1):19–31, March 2010.\n[38] A. Rebelo, I. Fujinaga, F. Paszkiewicz, A. R. S.\nMarc ¸al, C. Guedes, and J. S. Cardoso. Optical mu-\nsic recognition: state-of-the-art and open issues. Inter-\nnational Journal of Multimedia Information Retrieval ,\n1(3):173–190, 2012.\n[39] P. Roland. The music encoding initiative (MEI). In\nProceedings of the First International Conference on\nMusical Applications Using XML , pages 55–59, 2002.[40] F. Rossant and I. Bloch. Robust and adaptive omr sys-\ntem including fuzzy modeling, fusion of musical rules,\nand possible error detection. EURASIP Journal on Ad-\nvances in Signal Processing , 081541, 2007.\n[41] E. Selfridge-Field. Beyond MIDI: The handbook of mu-\nsical codes . MIT Press, 1997.\n[42] K. Simonyan and A. Zisserman. Very deep convo-\nlutional networks for large-scale image recognition.\narXiv preprint arXiv:1409.1556 , 2014.\n[43] M. Szwoch. Guido: A musical score recognition sys-\ntem. In 9th International Conference on Document\nAnalysis and Recognition , pages 809–813, 2007.\n[44] L. J. Tard ´on, S. Sammartino, I. Barbancho, V . G ´omez,\nand A. Oliver. Optical music recognition for scores\nwritten in white mensural notation. EURASIP Journal\non Image and Video Processing , 2009.\n[45] G. Vigliensoni, G. Burlet, and I. Fujinaga. Optical mea-\nsure recognition in common music notation. In 14th\nInternational Society for Music Information Retrieval\nConference , pages 125–30, 2013.\n[46] M. D. Zeiler. Adadelta: an adaptive learning rate\nmethod. arXiv preprint arXiv:1212.5701 , 2012.\n[47] M. D. Zeiler and R. Fergus. Visualizing and under-\nstanding convolutional networks. In 13th European\nConference on Computer Vision — Part I , pages 818–\n833, 2014.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 255"
    },
    {
        "title": "Using Musical Relationships Between Chord Labels in Automatic Chord Extraction Tasks.",
        "author": [
            "Tristan Carsault",
            "Jérôme Nika",
            "Philippe Esling"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492329",
        "url": "https://doi.org/10.5281/zenodo.1492329",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/231_Paper.pdf",
        "abstract": "Recent research on Automatic Chord Extraction (ACE) has focused on the improvement of models based on machine learning. However, most models still fail to take into account the prior knowledge underlying the labeling alphabets (chord labels). Furthermore, recent works have shown that ACE performances have reached a glass ceiling. Therefore, this prompts the need to focus on other aspects of the task, such as the introduction of musical knowledge in the representation, the improvement of the models towards more complex chord alphabets and the development of more adapted evaluation methods. In this paper, we propose to exploit specific properties and relationships between chord labels in order to improve the learning of statistical ACE models. Hence, we analyze the interdependence of the representations of chords and their associated distances, the precision of the chord alphabets, and the impact of performing alphabet reduction before or after training the model. Furthermore, we propose new training losses based on musical theory. We show that these improve the results of ACE systems based on Convolutional Neural Networks. By analyzing our results, we uncover a set of related insights on ACE tasks based on statistical models, and also formalize the musical meaning of some classification errors.",
        "zenodo_id": 1492329,
        "dblp_key": "conf/ismir/CarsaultNE18",
        "keywords": [
            "prior knowledge",
            "musical knowledge",
            "representation",
            "alphabet reduction",
            "evaluation methods",
            "statistical ACE models",
            "Convolutional Neural Networks",
            "chord labels",
            "chord alphabets",
            "precision"
        ],
        "content": "USING MUSICAL RELATIONSHIPS BETWEEN CHORD LABELS IN\nAUTOMATIC CHORD EXTRACTION TASKS\nTristan Carsault1J´erˆome Nika2;1Philippe Esling1\n1Ircam, CNRS, Sorbonne Universit ´e, UMR 9912 STMS,2L3i Lab, University of La Rochelle\nfcarsault, jnika, esling g@ircam.fr\nABSTRACT\nRecent research on Automatic Chord Extraction (ACE)\nhas focused on the improvement of models based on ma-\nchine learning. However, most models still fail to take\ninto account the prior knowledge underlying the labeling\nalphabets (chord labels). Furthermore, recent works have\nshown that ACE performances have reached a glass ceil-\ning. Therefore, this prompts the need to focus on other\naspects of the task, such as the introduction of musical\nknowledge in the representation, the improvement of the\nmodels towards more complex chord alphabets and the de-\nvelopment of more adapted evaluation methods.\nIn this paper, we propose to exploit speciﬁc properties\nand relationships between chord labels in order to improve\nthe learning of statistical ACE models. Hence, we ana-\nlyze the interdependence of the representations of chords\nand their associated distances, the precision of the chord\nalphabets, and the impact of performing alphabet reduc-\ntion before or after training the model. Furthermore, we\npropose new training losses based on musical theory. We\nshow that these improve the results of ACE systems based\non Convolutional Neural Networks. By analyzing our re-\nsults, we uncover a set of related insights on ACE tasks\nbased on statistical models, and also formalize the musical\nmeaning of some classiﬁcation errors.\n1. INTRODUCTION\nAutomatic Chord Extraction (ACE) is a topic that has been\nwidely studied by the Music Information Retrieval (MIR)\ncommunity over the past years. However, recent results\nseem to indicate that the rate of improvement of ACE per-\nformances has diminished over the past years [20].\nRecently, a part of the MIR community pointed out the\nneed to rethink the experimental methodologies. Indeed,\ncurrent evaluation methods do not account for the intrinsic\nrelationships between different chords [10]. Our work is\nbuilt on these questions and is aimed to give some insights\non the impact of introducing musical relationships between\nchord labels in the development of ACE methods.\nc\rTristan Carsault, J ´erˆome Nika, Philippe Esling. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Tristan Carsault, J ´erˆome Nika, Philippe\nEsling. “Using musical relationships between chord labels in Automatic\nChord Extraction tasks”, 19th International Society for Music Informa-\ntion Retrieval Conference, Paris, France, 2018.Most ACE systems are built on the idea of extracting\nfeatures from the raw audio signal and then using these\nfeatures to construct a chord classiﬁer [4]. The two major\nfamilies of approaches that can be found in previous re-\nsearch are rule-based andstatistical models. On one hand,\nthe rule-based models rely on music-theoretic rules to ex-\ntract information from the precomputed features. Although\nthis approach is theoretically sound, it usually remains brit-\ntle to perturbations in the spectral distributions from which\nthe features were extracted. On the other hand, statistical\nmodels rely on the optimization of a loss function over an\nannotated dataset. However, the generalization capabilities\nof these models are highly correlated to the size and com-\npleteness of their training set. Furthermore, most training\nmethods see musical chords as independent labels and do\nnot take into account the inherent relations between chords.\nIn this paper, we aim to target this gap by introducing\nmusical information directly in the training process of sta-\ntistical models. To do so, we propose to use prior knowl-\nedge underlying the labeling alphabets in order to account\nfor the inherent relationships between chords directly in-\nside the loss function of learning methods. Due to the\ncomplexity of the ACE task and the wealth of models avail-\nable, we choose to rely on a single Convolutional Neural\nNetwork (CNN) architecture, which provides the current\nbest results in ACE [19]. First, we study the impact of\nchord alphabets and their relationships by introducing a\nspeciﬁc hierarchy of alphabets. We show that some of the\nreductions proposed by previous researches might be inad-\nequate for learning algorithms. We also show that relying\non more ﬁnely deﬁned and extensive alphabets allows to\ngrasp more interesting insights on the errors made by ACE\nsystems, even though their accuracy is only marginally bet-\nter or worse. Then, we introduce two novel chord distances\nbased on musical relationships found in the Tonnetz-space\nor directly between chord components through their cate-\ngorical differences. These distances can be used to deﬁne\nnovel loss functions for learning algorithms. We show that\nthese new loss functions improve ACE results with CNNs.\nFinally, we perform an extensive analysis of our approach\nand extract insights on the methodology required for ACE.\nTo do so, we develop a speciﬁcally-tailored analyzer that\nfocuses on the functional relations between chords to dis-\ntinguish strong andweak errors. This analyzer is intended\nto be used for future ACE research to develop a ﬁner un-\nderstanding on the reasons behind the success or failure of\nACE systems.182. RELATED WORKS\nAutomatic Chord Extraction (ACE) is deﬁned as the task\nof labeling each segment of an audio signal using an alpha-\nbet of musical chords. In this task, chords are seen as the\nconcomitant or successive combination of different notes\nplayed by one or many instruments.\n2.1 Considerations on the ACE task\nWhereas most MIR tasks have beneﬁted continuously from\nthe recent advances in deep learning, the ACE ﬁeld seems\nto have reached a glass ceiling. In 2015, Humphrey and\nBello [10] highlighted the need to rethink the whole ACE\nmethodology by giving four insights on the task.\nFirst, several songs from the reference annotated chord\ndatasets (Isophonics, RWC-Pop, McGill Billboard) are not\nalways tuned to 440Hz and may vary up to a quarter-tone.\nThis leads to multiple misclassiﬁcations on the concomi-\ntant semi-tones. Moreover, chord labels are not always\nwell suited to describe every song in these datasets.\nSecond, the chord labels are related and some subsets of\nthose have hierarchical organizations. Therefore, the one-\nto-K assessment where all errors are equivalently weighted\nappears widely incorrect. For instance, the misclassiﬁca-\ntion of a C:Maj as aA:min orC#:Maj , will be considered\nequivalently wrong. However, C:Maj andA:min share two\npitches in common whereas C:Maj andC#:Maj have to-\ntally different pitch vectors.\nThird, the very deﬁnition of the ACE task is also not\nentirely clear. Indeed, there is a frequent confusion be-\ntween two different tasks. First, the literal recognition of\na local audio segment using a chord label and its precise\nextensions, and, second, the transcription of an underlying\nharmony , taking into account the functional aspect of the\nchords and the long-term structure of the song. Finally, the\nlabeling process involves the subjectivity of the annotators.\nFor instance, even for expert annotators, it is hard to agree\non possible chord inversions.\nTherefore, this prompts the need to focus on other as-\npects such as the introduction of musical knowledge in the\nrepresentation of chords, the improvement of the models\ntowards more complex chord alphabets and the develop-\nment of more adapted evaluation methods.\n2.2 Workﬂow of ACE systems\nDue to the complexity of the task, ACE systems are usually\ndivided into four main modules performing feature extrac-\ntion,pre-ﬁltering ,pattern matching andpost-ﬁltering [4].\nFirst, the pre-ﬁltering usually applies low-pass ﬁlters\nor harmonic-percussive source separation methods on the\nraw signal [12, 26]. This optional step allows to remove\nnoise or other percussive information that are irrelevant\nfor the chord extraction task. Then, the audio signal is\ntransformed into a time-frequency representation such as\nthe Short-Time Fourier Transform (STFT) or the Constant-\nQ Transform (CQT) that provides a logarithmically-scaled\nfrequencies. These representations are sometimes summa-\nrized in a pitch class vector called chromagram . Then, suc-cessive time frames of the spectral transform are averaged\nin context windows. This allows to smooth the extracted\nfeatures and account for the fact that chords are longer-\nscale events. It has been shown that this could be done\nefﬁciently by feeding STFT context windows to a CNN in\norder to obtain a clean chromagram [13].\nThen, these extracted features are classiﬁed by relying\non either a rule-based chord template system or a statistical\nmodel. Rule-based methods give fast results and a decent\nlevel of accuracy [21]. With these methods, the extracted\nfeatures are classiﬁed using a ﬁxed dictionary of chord pro-\nﬁles [2] or a collection of decision trees [12]. However,\nthese methods are usually brittle to perturbations in the in-\nput spectral distribution and do not generalize well.\nStatistical models aim to extract the relations between\nprecomputed features and chord labels based on a train-\ning dataset in which each temporal frame is associated\nto a label. The optimization of this model is then per-\nformed by using gradient descent algorithms to ﬁnd an ad-\nequate conﬁguration of its parameters. Several probabilis-\ntic models have obtained good performances in ACE, such\nas multivariate Gaussian Mixture Model [3] and convolu-\ntional [9, 14] or recurrent [1, 25] Neural Networks.\nFinally, post-ﬁltering is applied to smooth out the clas-\nsiﬁed time frames. This is usually based on a study of\nthe transition probabilities between chords by a Hidden\nMarkov Model (HMM) optimized with the Viterbi algo-\nrithm [17] or with Conditional Random Fields [15].\n2.3 Convolutional Neural Network\nA Convolutional Neural Network (CNN) is a statistical\nmodel composed of layers of artiﬁcial neurons that trans-\nform the input by repeatedly applying convolution and\npooling operations. A convolutional layer is characterized\nby a set of convolution kernels that are applied in parallel\nto the inputs to produce a set of output feature maps . The\nconvolution kernels are deﬁned as three-dimensional ten-\nsorsh2RM\u0002U\u0002Vwhere Mis the number of kernels, U\nis the height and Vthe width of each kernel. If we note the\ninput as matrix X, then the output feature maps are deﬁned\nbyY=X\u0003hmfor every kernels, where \u0003is a 2D discrete\nconvolution operation\n(A\u0003B)i;j=(T\u00001)X\nr=0(F\u00001)X\ns=0Ar;sBi\u0000r;j\u0000s (1)\nforA2RT\u0002FandB2RU\u0002Vwith0\u0014i\u0014T+U\u00001\nand0\u0014j\u0014F+V\u00001.\nAs this convolutional layer signiﬁcantly increases the\ndimensionality of the input data, a pooling layer is used\nto reduce the size of the feature maps. The pooling opera-\ntion reduces the maps by computing local mean, maximum\nor average of sliding context windows across the maps.\nTherefore, the overall structure of a CNN usually consists\nin alternating convolution, activation and pooling layers.\nFinally, in order to perform classiﬁcation, this architectureProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 19Figure 1 . Hierarchy of the chord alphabets (blue: A0, or-\nange: A1, green: A2)\nis typically followed by one or many fully-connected lay-\ners. Thus, the last layer produces a probability vector of\nthe same size as the chord alphabet. As we will rely on the\narchitecture deﬁned by [9], we redirect interested readers\nto this paper for more information.\n3. OUR PROPOSAL\n3.1 Deﬁnition of alphabets\nChord annotations from reference datasets are very precise\nand include extra notes (in parenthesis) and basses (after\nthe slash) [7]. With this notation, we would obtain over\na thousand chord classes with very sparse distributions.\nHowever, we do not use these extra notes and bass in our\nclassiﬁcation. Therefore, we can remove this information\nF:maj7(11) =3!F:maj7 (2)\nEven with this reduction, the number of chord qualities (eg.\nmaj7, min, dim ) is extensive and we usually do not aim for\nsuch a degree of precision. Thus, we propose three alpha-\nbets named A0,A1andA2with a controlled number of\nchord qualities. The level of precision of the three alpha-\nbets increases gradually (see Figure 1). In order to reduce\nthe number of chord qualities, each one is mapped to a par-\nent class when it exists, otherwise to the no-chord classN.\nThe ﬁrst alphabet A0contains all the major and minor\nchords, which deﬁnes a total of 25 classes\nA0=fNg[fP\u0002maj; ming (3)\nwhere Prepresents the 12 pitch classes.\nHere, we consider the interest of working with chord\nalphabets larger than A0. Therefore, we propose an alpha-\nbet containing all chords present in the harmonization of\nthe major scale (usual notation of harmony in jazz music).\nThis corresponds to the orange chord qualities and their\nparents in Figure 1. The chord qualities without heritage\nare included in the no-chord class N, leading to 73 classes\nA1=fNg[fP\u0002maj; min; dim; maj 7; min 7;7g(4)\nFinally, the alphabet A2is inspired from the large vo-\ncabulary alphabet proposed by [19]. This most complete\nchord alphabet contains 14 chord qualities and 169 classesA2=fNg[fP\u0002maj; min; dim; aug; maj 6; min 6;\nmaj7; minmaj 7; min 7;7; dim 7; hdim 7; sus2; sus4g\n(5)\n3.2 Deﬁnition of chord distances\nIn most CNN approaches, the model does not take into ac-\ncount the nature of each class when computing their differ-\nences. Therefore, this distance which we called categorical\ndistance D0is the binary indicator\nD0(chord 1; chord 2) =\u001a0 ifchord 1=chord 2\n1 ifchord 16=chord 2\n(6)\nHowever, we want here to include the relationships be-\ntween chords directly in our model. For instance, a C:maj7\nis closer to an A:min7 than a C#:maj7 . Therefore, we in-\ntroduce more reﬁned distances that can be used to deﬁne\nthe loss function for learning.\nHere, we introduce two novel distances that rely on the\nrepresentation of chords in an harmonic space or in a pitch\nspace to provide a ﬁner description of the chord labels.\nHowever, any other distance that measure similarities be-\ntween chords could be studied [8, 18].\n3.2.1 Tonnetz distance\nATonnetz-space is a geometric representation of the tonal\nspace based on harmonic relationships between chords.\nWe chose a Tonnetz-space generated by three transforma-\ntions of the major and minor triads [5] changing only one\nof the three notes of the chords: the relative transforma-\ntion (transforms a chord into his relative major / minor),\ntheparallel transformation (same root but major instead\nof minor or conversely), the leading-tone exchange (in a\nmajor chord the root moves down by a semitone, in a mi-\nnor chord the ﬁfth moves up by a semitone). Representing\nchords in this space has already shown promising results\nfor classiﬁcation on the A0alphabet [11].\nWe deﬁne the cost of a path between two chords as the\nsum of the succesive transformations. Each transformation\nis associated to the same cost. Furthermore, an extra cost is\nadded if the chords have been reduced beforehand in order\nto ﬁt the alphabet A0. Then, our distance D1is:\nD1(chord 1; chord 2) =min(C) (7)\nwithCthe set of all possible path costs from chord 1to\nchord 2using a combination of the three transformations.\n3.2.2 Euclidean distance on pitch class vectors\nIn some works, pitch class vectors are used as an inter-\nmediate representation for ACE tasks [16]. Here, we use\nthese pitch class proﬁles to calculate the distances between\nchords according to their harmonic content.\nEach chord from the dictionary is associated to a 12-\ndimensional binary pitch vector with 1 if the pitch is\npresent in the chord and 0 otherwise (for instance C:maj720 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018becomes (1;0;0;0;1;0;0;1;0;0;0;1)). The distance be-\ntween two chords is deﬁned as the Euclidean distance be-\ntween the two binary pitch vectors.\nD2(chord 1; chord 2) =vuut11X\ni=0(chordi\n1\u0000chordi\n2)2(8)\nHence, this distance allows to account for the number\nof pitches that are shared by two chords.\nTheD0,D1orD2distance is used to deﬁne the loss\nfunction for training the CNN classiﬁcation model.\n3.3 Introducing the relations between chords\nTo train the model with our distances, we ﬁrst reduce the\noriginal labels from the Isophonics dataset1so that they ﬁt\none of our three alphabets A0,A1,A2. Then, we denote\nytrueas the one-hot vector where each bin corresponds to\na chord label in the chosen alphabet Ai. The output of\nthe model, noted ypred, is a vector of probabilities over all\nthe chords in a given alphabet Ai. In the case of D0, we\ntrain the model with a loss function that simply compares\nypredto the original label ytrue. However, for our proposed\ndistances ( D1andD2), we introduce a similarity matrix M\nthat associates each couple of chords to a similarity ratio.\nMi;j=1\nDk(chord i; chord j) +K(9)\nK is an arbitrary constant to avoid division by zero. The\nmatrix Mis symmetric and we normalize it with its max-\nimum value to obtain \u0016M. Afterwards, we deﬁne a new\n\u0016ytrue which is the matrix multiplication of the old ytrue\nand the normalized matrix \u0016M.\n\u0016ytrue=ytrue\u0016M (10)\nFinally, the loss function for D1andD2is deﬁned by\na comparison between this new ground truth \u0016ytrueand the\noutput ypred. Hence, this loss function can be seen as a\nweighted multi-label classiﬁcation.\n4. EXPERIMENTS\n4.1 Dataset\nWe perform our experiments on the Beatles dataset as it\nprovides the highest conﬁdence regarding the ground truth\nannotations [6]. This dataset is composed by 180 songs\nannotated by hand. For each song, we compute the CQT\nby using a window size of 4096 samples and a hop size\nof 2048. The transform is mapped to a scale of 3 bins\nper semi-tone over 6 octaves ranging from C1 to C7. We\naugment the available data by performing all transpositions\nfrom -6 to +6 semi-tones and modifying the labels accord-\ningly. Finally, to evaluate our models, we split the data into\na training (60%), validation (20%) and test (20%) sets.\n1http://isophonics.net/content/\nreference-annotations-beatles4.2 Models\nWe use the same CNN model for all test conﬁgurations,\nbut change the size of the last layer to ﬁt the size of the se-\nlected chord alphabet. We apply a batch normalization and\na Gaussian noise addition on the inputs layer. The archi-\ntecture of the CNN consists of three convolutional layers\nfollowed by two fully-connected layers. The architecture\nis very similar to the ﬁrst CNN that has been proposed for\nthe ACE task [9]. However, we add dropout between each\nconvolution layer to prevent over-ﬁtting.\nFor training, we use the ADAM optimizer with a learn-\ning rate of 2:10\u00005for a total of 1000 epochs. We reduce the\nlearning rate if the validation loss has not improved during\n50 iterations. Early stopping is applied if the validation\nloss has not improved during 200 iterations and we keep\nthe model with the best validation accuracy. For each con-\nﬁguration, we perform a 5-cross validation by repeating a\nrandom split of the dataset.\n5. RESULTS AND DISCUSSION\nThe aim of this paper is not to obtain the best classiﬁcation\nscores (which would involve pre- or post-ﬁltering meth-\nods) but to study the impact on the classiﬁcation results of\ndifferent musical relationships (as detailed in the previous\nsection). Therefore, we ran 9 instances of the CNN model\ncorresponding to all combinations of the 3 alphabets A0,\nA1,A2and 3 distances D0,D1,D2to compare their re-\nsults from both a quantitative andqualitative point of view.\nWe analyzed the results using the mireval library [22] to\ncompute classiﬁcation scores, and a Python ACE Analyzer\nthat we developed to reveal the musical meaning of classi-\nﬁcation errors and, therefore, understand their qualities.\n5.1 Quantitative analysis: MIREX evaluation\nRegarding the MIREX evaluation, the efﬁciency of ACE\nmodels is assessed through classiﬁcation scores over dif-\nferent alphabets [22]. The MIREX alphabets for evalua-\ntion have a gradation of complexity from Major/Minor to\nTetrads. In our case, for the evaluation on a speciﬁc al-\nphabet, we apply a reduction from our training alphabet\nAito the MIREX evaluation alphabet. Here, we evaluate\non three alphabet : Major/Minor, Sevenths, and Tetrads.\nThese alphabets correspond roughly to our three alphabets\n(Major/Minor\u0018A0, Sevenths\u0018A1, Tetrads\u0018A2).\n5.1.1 MIREX Major/minor\nFigure 2 depicts the average classiﬁcation scores over all\nframes of our test dataset for different distances and alpha-\nbets. We can see that the introduction of the D1orD2\ndistance improves the classiﬁcation compared to D0. With\nthese distances, and even without pre- or post-ﬁltering, we\nobtain classiﬁcation scores that are superior to that of sim-\nilar works (75.9% for CNN with post-ﬁltering but an ex-\ntended dataset in [10] versus 76.3% for A2\u0000D1). Sec-\nond, the impact of working ﬁrst on large alphabets ( A1and\nA2), and then reducing on A0for the test is negligible on\nMaj/Min (only from a quantitative point of view, see 5.2).Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 21A0A1A2D0D1 D2D0D1 D2D0D1 D2.72.74.76.78Maj/Min Figure 2 . Results of the 5-folds: evaluation on MIREX\nMaj/Min (\u0018reduction on A0).\nA1A2D0 D1 D2 D0 D1 D2.56.60.64.68Sevenths\nFigure 3 . Results of the 5-folds: evaluation on MIREX\nSevenths (\u0018reduction on A1).\n5.1.2 MIREX Sevenths\nWith more complex alphabets, the classiﬁcation score is\nlower than for MIREX Maj/Min. This result is not surpris-\ning since we observe this behavior on all ACE systems.\nMoreover, the models give similar results and we can not\nobserve a particular trend between the alphabet reductions\nor the different distances. The same result is observed for\nthe evaluation with MIREX tetrads ( \u0018reduction on A2).\nNonetheless, the MIREX evaluation uses a binary score to\ncompare chords. Because of this approach, the qualities of\nthe classiﬁcation errors cannot be evaluated.\n5.2 Qualitative analysis: understanding the errors\nIn this section, we propose to analyze ACE results from\na qualitative point of view. The aim here is not to intro-\nduce new alphabets or distances in the models, but to in-\ntroduce a new type of evaluation of the results. Our goal\nis twofold: to understand what causes the errors in the ﬁrst\nplace, and to distinguish “weak” from “strong” errors with\nafunctional approach.\nIn tonal music, the harmonic functions qualify the roles\nand the tonal signiﬁcances of chords, and the possible\nequivalences between them within a sequence [23, 24].\nTherefore, we developed an ACE Analyzer including two\nmodules discovering some formal musical relationshipsModel Tot.\u001aMaj\u001amin\nA0-D034.93\nA0-D136.12\nA0-D235.37\nA1-D052.40 23.82 4.37\nA1-D157.67 28.31 5.37\nA1-D255.17 25.70 4.21\nA2-D055.28 26.51 4.29\nA2-D160.47 31.61 6.16\nA2-D255.45 25.74 4.78\nTable 1 . Left: total percentage of errors corresponding to\ninclusions or chords substitutions rules, right: percentage\nof errors with inclusion in the correct triad (% of the total\nnumber of errors).\nModel rel. M rel. m T subs. 2 m!M M!m\nA0-D0 4.19 5.15 2.37 7.26 12.9\nA0-D1 4.40 5.20 2.47 7.66 13.4\nA0-D2 5.13 4.87 2.26 8.89 10.89\nA1-D0 2.63 3.93 1.53 4.46 8.83\nA1-D1 3.05 3.36 1.58 5.53 7.52\nA1-D2 3.02 4.00 1.62 5.84 8.07\nA2-D0 2.54 4.15 1.51 4.96 8.54\nA2-D1 2.79 2.97 1.54 5.29 7.46\nA2-D2 3.11 4.26 1.63 5.34 7.59\nTable 2 . Left: percentage of errors corresponding to usual\nchords substitutions rules, right: percentage of errors “ma-\njor instead of minor” or inversely (% of the total number\nof errors).\nbetween the target chords and the chords predicted by ACE\nmodels. Both modules are generic and independent of the\nclassiﬁcation model, and are available online.2\n5.2.1 Substitution rules\nThe ﬁrst module detects the errors corresponding to hierar-\nchical relationships or usual chord substitutions rules: us-\ning a chord in place of another in a chord progression (usu-\nally substituted chords have two pitches in common with\nthe triad that they are replacing).\nTable 1 presents: Tot., the total fraction of errors that\ncan be explained by the whole set of substitution rules we\nimplemented, and\u001aMajand\u001amin, the errors included in\nthe correct triad ( e.g. C:maj instead of C:maj7 ,C:min7 in-\nstead of C:min ). Table 2 presents the percentages of errors\ncorresponding to widely used substitution rules: rel. m and\nrel. M , relative minor and major; T subs. 2 , tonic substitu-\ntion different from rel. m orrel. M (e.g. E:min7 instead or\nC:maj7 ), and the percentages of errors m!MandM!m\n(same root but major instead of minor or conversely). The\ntables only show the categories representing more than 1%\nof the total number of errors, but other substitutions (that\nare not discussed here) were analyzed: tritone substitution,\nsubstitute dominant, and equivalence of dim7 chords mod-\nulo inversions.\nFirst, Tot.in Table 1 shows that a huge fraction of errors\ncan be explained by usual substitution rules. This percent-\n2http://repmus.ircam.fr/dyci2/ace_analyzer22 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Model Non-diat. targ. Non-diat. pred.\nA0-D0 37.96 28.41\nA0-D1 44.39 15.82\nA0-D2 45.87 17.60\nA1-D0 38.05 21.26\nA1-D1 37.94 20.63\nA1-D2 38.77 20.23\nA2-D0 37.13 30.01\nA2-D1 36.99 28.41\nA2-D2 37.96 28.24\nTable 3 . Errors occurring when the target is non-diatonic\n(% of the total number of errors), non-diatonic prediction\nerrors (% of the subset of errors on diatonic targets).\nage can reach 60.47%, which means that numerous clas-\nsiﬁcation errors nevertheless give useful indications since\nthey mistake a chord for another chord with an equivalent\nfunction. For instance, Table 2 shows that a signiﬁcant\namount of errors (up to 10%) are relative major / minor\nsubstitutions. Besides, for the three distances, the percent-\nage in Tot. (Table 1) increases with the size of the alpha-\nbet: larger alphabets seem to imply weaker errors (higher\namount of equivalent harmonic functions).\nWe can also note that numerous errors (between 28.19%\nand 37.77%) correspond to inclusions in major or minor\nchords (\u001aMaj and\u001amin, Table 1) for A1andA2. In the\nframework of the discussion about recognition andtran-\nscription mentioned in introduction, this result questions\nthe relevance of considering exhaustive extensions when\nthe goal is to extract and formalize an underlying harmony.\nFinally, for A0,A1, and A2, using D1instead of D0\nincreases the fraction of errors attributed to categories in\nthe left part of Table 2 (and in almost all the conﬁgurations\nwhen using D2). This shows a qualitative improvement\nsince all these operations are considered as valid chord\nsubstitutions. On the other hand, the impact on the (quite\nhigh) percentages in the right part of Table 2 is not clear.\nWe can assume that temporal smoothing can be one of the\nkeys to handle the errors m!MandM!m.\n5.2.2 Harmonic degrees\nThe second module of our ACE Analyzer focuses on har-\nmonic degrees . First, by using the annotations of key in\nthe dataset in addition to that of chords, this module de-\ntermines the roman numerals characterizing the harmonic\ndegrees of the predicted chord and of the target chord ( e.g.\nin C, if a chord is an extension of C,I; if it is an extension\nofD:min ,ii; etc.) when it is possible ( e.g.in C, if a chord\nis an extension of C#it does not correspond to any degree).\nThen, it counts the errors corresponding to substitutions of\nharmonic degrees when it is possible ( e.g.in C, A:min in-\nstead of Ccorresponds to I\u0018vi). This section shows an\nanalysis of the results using this second module. First, it\ndetermines if the target chord is diatonic ( i.e.belongs to\nthe harmony of the key), as presented in Table 3. If this\nis the case, the notion of incorrect degree for the predicted\nchord is relevant and the percentages of errors correspond-\ning to substitutions of degrees is computed (Table 4).Model I\u0018IV I\u0018V IV\u0018V I\u0018vi IV\u0018ii I\u0018iii\nA0-D017.41 14.04 4.54 4.22 5.41 2.13\nA0-D117.02 13.67 3.33 4.08 6.51 3.49\nA0-D216.16 13.60 3.08 5.65 6.25 3.66\nA1-D017.53 13.72 3.67 5.25 4.65 3.50\nA1-D115.88 13.82 3.48 4.95 6.26 3.46\nA1-D216.73 13.45 3.36 4.70 5.75 2.97\nA2-D016.90 13.51 3.68 4.45 5.06 3.32\nA2-D116.81 13.60 3.85 4.57 5.37 3.59\nA2-D216.78 12.96 3.84 5.19 7.01 3.45\nTable 4 . Errors ( >2%) corresponding to degrees substitu-\ntions (% of the subset of errors on diatonic targets).\nA ﬁrst interesting fact presented in Table 3 is that\n36.99% to 45.87% of the errors occur when the target\nchord is non-diatonic. It also shows, for the three alpha-\nbets, that using D1orD2instead of D0makes the frac-\ntion of non-diatonic errors decrease (Table 3, particularly\nA0), which means that the errors are more likely to stay\nin the correct key. Surprisingly, high percentages of errors\nare associated to errors I\u0018V(up to 14.04%), I\u0018IV(up to\n17.41%), or IV\u0018V(up to 4.54%) in Table 4. These errors\nare not usual substitutions, and IV\u0018VandI\u0018IVhave re-\nspectively 0 and 1 pitch in common. In most of the cases,\nthese percentages tend to decrease on alphabets A1orA2\nand when using musical distances (particularly D2). Con-\nversely, it increases the amount of errors in the right part\nof Table 4 containing usual substitutions: once again we\nobserve that the more precise the musical representations\nare, the more the harmonic functions tend to be correct.\n6. CONCLUSION\nWe presented a novel approach taking advantage of musi-\ncal prior knowledge underlying the labeling alphabets into\nACE statistical models. To this end, we applied reduc-\ntions on different chord alphabets and we used different\ndistances to train the same type of model. Then, we con-\nducted a quantitative and qualitative analysis of the classi-\nﬁcation results.\nFirst, we conclude that training the model using dis-\ntances reﬂecting the relationships between chords im-\nproves the results both quantitatively (classiﬁcation scores)\nand qualitatively (in terms of harmonic functions). Second,\nit appears that working ﬁrst on large alphabets and reduc-\ning the chords during the test phase does not signiﬁcantly\nimprove the classiﬁcation scores but provides a qualitative\nimprovement in the type of errors. Finally, ACE could\nbe improved by moving away from its binary classiﬁca-\ntion paradigm. Indeed, MIREX evaluations focus on the\nnature of chords but a large amount of errors can be ex-\nplained by inclusions or usual substitution rules. Our eval-\nuation method therefore provides an interesting notion of\nmusical quality of the errors, and encourages to adopt a\nfunctional approach or even to introduce a notion of equiv-\nalence classes. It could be adapted to the ACE problem\ndownstream and upstream: in the classiﬁcation processes\nas well as in the methodology for labeling the datasets.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 237. ACKNOWLEDGMENTS\nThe authors would like to thank the master’s students who\ncontributed to the implementation: Alexis Font, Gr ´egoire\nLocqueville, Octave Roulleau-Thery, and T ´eo Sanchez.\nThis work was supported by the DYCI2 project ANR-14-\nCE2 4-0002-01 funded by the French National Research\nAgency (ANR), the MAKIMOno project 17-CE38-0015-\n01 funded by the French ANR and the Canadian Nat-\nural Sciences and Engineering Reserch Council (STPG\n507004-17), the ACTOR Partnership funded by the Cana-\ndian Social Sciences and Humanities Research Council\n(895-2018-1023).\n8. REFERENCES\n[1] N. Boulanger-Lewandowski, Y . Bengio, and P. Vin-\ncent. Audio chord recognition with recurrent neural\nnetworks. In International Symposium on Music Infor-\nmation Retrieval , pages 335–340, 2013.\n[2] C. Cannam, E. Benetos, M. Mauch, M. E. P. Davies,\nS. Dixon, C. Landone, K. Noland, and D. Stowell.\nMirex 2015: Vamp plugins from the centre for digi-\ntal music. In Proceedings of the Music Information Re-\ntrieval Evaluation eXchange (MIREX) , 2015.\n[3] T. Cho. Improved techniques for automatic chord\nrecognition from music audio signals . PhD thesis, New\nYork University, 2014.\n[4] T. Cho, R. J. Weiss, and J. P. Bello. Exploring com-\nmon variations in state of the art chord recognition sys-\ntems. In Proceedings of the Sound and Music Comput-\ning Conference (SMC) , pages 1–8, 2010.\n[5] R. Cohn. Neo-riemannian operations, parsimonious tri-\nchords, and their” tonnetz” representations. Journal of\nMusic Theory , 41(1):1–66, 1997.\n[6] C. Harte. Towards automatic extraction of harmony in-\nformation from music signals . PhD thesis, 2010.\n[7] C. Harte, M. B. Sandler, S. A. Abdallah, and E. G ´omez.\nSymbolic representation of musical chords: A pro-\nposed syntax for text annotations. In International\nSymposium on Music Information Retrieval , volume 5,\npages 66–71, 2005.\n[8] C-Z. A. Huang, D. Duvenaud, and K. Z. Gajos. Chor-\ndripple: Recommending chords to help novice com-\nposers go beyond the ordinary. In Proceedings of the\n21st International Conference on Intelligent User In-\nterfaces , pages 241–250. ACM, 2016.\n[9] E. J. Humphrey and J. P. Bello. Rethinking automatic\nchord recognition with convolutional neural networks.\nInMachine Learning and Applications (ICMLA), 2012\n11th International Conference on , volume 2, pages\n357–362. IEEE, 2012.[10] E. J. Humphrey and J. P. Bello. Four timely insights on\nautomatic chord estimation. In International Sympo-\nsium on Music Information Retrieval , pages 673–679,\n2015.\n[11] E. J. Humphrey, T. Cho, and J. P. Bello. Learning\na robust tonnetz-space transform for automatic chord\nrecognition. In Acoustics, Speech and Signal Process-\ning (ICASSP), 2012 IEEE International Conference on ,\npages 453–456. IEEE, 2012.\n[12] J. Jiang, W. Li, and Y . Wu. Extended abstract for mirex\n2017 submission: Chord recognition using random for-\nest model. MIREX evaluation results , 2017.\n[13] F. Korzeniowski and G. Widmer. Feature learning for\nchord recognition: The deep chroma extractor. arXiv\npreprint arXiv:1612.05065 , 2016.\n[14] F. Korzeniowski and G. Widmer. A fully convolutional\ndeep auditory model for musical chord recognition.\nInMachine Learning for Signal Processing (MLSP),\n2016 IEEE 26th International Workshop on , pages 1–\n6. IEEE, 2016.\n[15] J. Lafferty, A. McCallum, and F. C. N. Pereira. Condi-\ntional random ﬁelds: Probabilistic models for segment-\ning and labeling sequence data. 2001.\n[16] K. Lee. Automatic chord recognition from audio us-\ning enhanced pitch class proﬁle. In International Com-\nputer Music Conference , 2006.\n[17] H-L. Lou. Implementing the viterbi algorithm. IEEE\nSignal Processing Magazine , 12(5):42–52, 1995.\n[18] S. Madjiheurem, L. Qu, and C. Walder. Chord2vec:\nLearning musical chord embeddings. In Proceedings of\nthe Constructive Machine Learning Workshop at 30th\nConference on Neural Information Processing Systems\n(NIPS2016), Barcelona, Spain , 2016.\n[19] B. McFee and J. P. Bello. Structured training for large-\nvocabulary chord recognition. In Proceedings of the\n18th International Society for Music Information Re-\ntrieval Conference (ISMIR2017). ISMIR , 2017.\n[20] M. McVicar, R. Santos-Rodr ´ıguez, Y . Ni, and T. De\nBie. Automatic chord estimation from audio: A re-\nview of the state of the art. IEEE/ACM Transactions\non Audio, Speech and Language Processing (TASLP) ,\n22(2):556–575, 2014.\n[21] L. Oudre, Y . Grenier, and C. F ´evotte. Template-based\nchord recognition: Inﬂuence of the chord types. In\nInternational Symposium on Music Information Re-\ntrieval , pages 153–158, 2009.\n[22] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nieto, D. Liang, D. P. W. Ellis, and C. C. Raffel.\nmireval: A transparent implementation of common\nmir metrics. In Proceedings of the 15th International\nSociety for Music Information Retrieval Conference,\nISMIR , 2014.24 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[23] A. Rehding. Hugo Riemann and the birth of mod-\nern musical thought , volume 11. Cambridge University\nPress, 2003.\n[24] A. Schoenberg and L. Stein. Structural functions of\nharmony . Number 478. WW Norton & Company,\n1969.\n[25] Y . Wu, X. Feng, and W. Li. Mirex 2017 submission:\nAutomatic audio chord recognition with miditrained\ndeep feature and blstm-crf sequence decoding model.\nMIREX evaluation results , 2017.\n[26] X. Zhou and A. Lerch. Chord detection using deep\nlearning. In Proceedings of the 16th International Sym-\nposium on Music Information Retrieval Conference ,\nvolume 53, 2015.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 25"
    },
    {
        "title": "Document Analysis of Music Score Images with Selectional Auto-Encoders.",
        "author": [
            "Francisco J. Castellanos 0001",
            "Jorge Calvo-Zaragoza",
            "Gabriel Vigliensoni",
            "Ichiro Fujinaga"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492397",
        "url": "https://doi.org/10.5281/zenodo.1492397",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/93_Paper.pdf",
        "abstract": "The document analysis of music score images is a key step in the development of successful Optical Music Recognition systems. The current state of the art considers the use of deep neural networks trained to classify every pixel of the image according to the image layer it belongs to. This process, however, involves a high computational cost that prevents its use in interactive machine learning scenarios. In this paper, we propose the use of a set of deep selectional auto-encoders, implemented as fully-convolutional networks, to perform image-to-image categorizations. This strategy retains the advantages of using deep neural networks, which have demonstrated their ability to perform this task, while dramatically increasing the efficiency by processing a large number of pixels in a single step. The results of an experiment performed with a set of high-resolution images taken from Medieval manuscripts successfully validate this approach, with a similar accuracy to that of the state of the art but with a computational time orders of magnitude smaller, making this approach appropriate for being used in interactive applications.",
        "zenodo_id": 1492397,
        "dblp_key": "conf/ismir/CastellanosCVF18",
        "keywords": [
            "deep selectional auto-encoders",
            "fully-convolutional networks",
            "image-to-image categorizations",
            "interactive machine learning",
            "high-resolution images",
            "Medieval manuscripts",
            "computational time",
            "similar accuracy",
            "orders of magnitude smaller",
            "interactive applications"
        ],
        "content": "DOCUMENT ANALYSIS OF MUSIC SCORE IMAGES WITH\nSELECTIONAL AUTO-ENCODERS\nFrancisco J. Castellanos1Jorge Calvo-Zaragoza2\nGabriel Vigliensoni3Ichiro Fujinaga3\n1Software and Computing Systems, University of Alicante, Spain\n2PRHLT Research Center, Universitat Polit `ecnica de Val `encia, Spain\n3Schulich School of Music, McGill University, Canada\nfcastellanos@dlsi.ua.es\nABSTRACT\nThe document analysis of music score images is a key step\nin the development of successful Optical Music Recog-\nnition systems. The current state of the art considers\nthe use of deep neural networks trained to classify ev-\nery pixel of the image according to the image layer it be-\nlongs to. This process, however, involves a high compu-\ntational cost that prevents its use in interactive machine\nlearning scenarios. In this paper, we propose the use of\na set of deep selectional auto-encoders, implemented as\nfully-convolutional networks, to perform image-to-image\ncategorizations. This strategy retains the advantages of us-\ning deep neural networks, which have demonstrated their\nability to perform this task, while dramatically increas-\ning the efﬁciency by processing a large number of pix-\nels in a single step. The results of an experiment per-\nformed with a set of high-resolution images taken from\nMedieval manuscripts successfully validate this approach,\nwith a similar accuracy to that of the state of the art but with\na computational time orders of magnitude smaller, making\nthis approach appropriate for being used in interactive ap-\nplications.\n1. INTRODUCTION\nThe Optical Music Recognition (OMR) is a computational\nprocess that reads musical notation from images, with the\naim of automatically exporting the content to a structured\nformat [1]. Given the complexity of the task, the process\nis usually divided into different stages, the ﬁrst of which\nis the document analysis. This stage consists of detecting\nand categorizing the different sources of information that\nappear in images of musical scores—e.g., classifying each\npixel into one of four possible categories: background,\nstaff line, musical note, or lyrics—and it is important for\ncreating robust OMR systems [29]. That is, if subsequent\nc\rFrancisco J. Castellanos, Jorge Calvo-Zaragoza, Gabriel\nVigliensoni, Ichiro Fujinaga. Licensed under a Creative Commons At-\ntribution 4.0 International License (CC BY 4.0). Attribution: Fran-\ncisco J. Castellanos, Jorge Calvo-Zaragoza, Gabriel Vigliensoni, Ichiro\nFujinaga. “Document Analysis of Music Score Images with Selectional\nAuto-Encoders”, 19th International Society for Music Information Re-\ntrieval Conference, Paris, France, 2018.stages receive the image in a reliable state, systems tend to\ngeneralize more easily.\nMany researchers have proposed different algorithms\nto deal with speciﬁc steps within the document process-\ning stage of the Optical Music Recognition (OMR) work-\nﬂow. Traditionally, these strategies consist of heuristic\nworkﬂows speciﬁcally designed for the scores at hand, ex-\nploiting speciﬁc details of the images to improve the per-\nformance of the detection. Music documents, however, es-\npecially from the Medieval and Renaissance era, come in\na wide variety of notational styles and formats, resulting\nin a heterogeneous collection. Therefore, the previous ap-\nproaches may be beneﬁcial in the short term but they do not\nscale well [4, 6]. In many cases, a workﬂow must be de-\nveloped anew for dealing with manuscripts with different\nnotation, from a disparate time period, or with a differing\nlevel of image degradation.\nRecent work has demonstrated the feasibility of using\nmachine learning for document analysis [21, 25, 36]. In\ncomparison to systems with hand-crafted heuristic rules,\nthe advantage of using machine learning-based techniques\nlies in their generalizability, only needing labeled exam-\nples to build a new classiﬁcation model [12]. In addition\nto this important advantage, the use of these techniques,\nin particular Convolutional Neural Networks (CNN), has\nproven to outperform the traditional strategies considered\nfor document analysis in the OMR domain [6]. The main\nidea behind this approach is training a CNN to distinguish\nthe category to which each pixel of the image belongs.\nThat is, given a pixel of the image, and taking into ac-\ncount the pixels of its neighborhood, a model is trained\nto predict the category (e.g., note, staff line, and lyrics). In\nthis way, the document analysis process consists of classi-\nfying every single pixel of the image into its actual cate-\ngory, thus separating the different layers of the document\naccordingly. Given that the classiﬁcation is performed at\npixel level, thin elements such as staff lines, note stems, as\nwell as small artifacts, can be properly detected.\nThe problem with the aforementioned process is that it\nentails a high computational cost because it needs to clas-\nsify every single pixel of an image. Since OMR is a pro-\ncess that lends itself to be used interactively [8, 9], there is\na need of accelerating the processing of documents with-\nout sacriﬁcing the classiﬁcation quality, in order to present256a user-friendly environment.\nWe present in this paper a new framework based on ma-\nchine learning that replaces the current pixel-wise model\nby a patch-wise model. In this approach, we process a\ncomplete sub-image (patch) in a single step, making pre-\ndictions of many pixels simultaneously. This can be car-\nried out by means of neural networks that learns how to\ncompute an image-to-image prediction.\nWe evaluate the new approach over a set of high-\nresolution images taken from Medieval music manuscripts.\nThe patch-wise model attains a similar accuracy to that of\nthe state of the art but reducing the computational cost by\nseveral orders of magnitude.\nThe rest of the paper is organized as follows. We give a\nbrief review of related work in Section 2. A formalization\nof the task, as well as the proposed solution, is detailed in\nSection 3. We empirically demonstrate in Section 4 that\nour model drastically reduces the computational time by\norders of magnitude without the classiﬁcation quality. Fi-\nnally, we summarize the main conclusions of the present\nwork in Section 5, pointing out some potential future work.\n2. BACKGROUND\nThe classical workﬂow for OMR considers an initial doc-\nument analysis stage [29], to process the input image be-\nfore proceeding to the automatic recognition of the content.\nThis ﬁrst stage is crucial to increase the robustness of the\nsystem and to reduce the complexity of subsequent stages\nby providing correctly segmented images.\nA common ﬁrst step within the document analysis stage\nis binarization, in which background and foreground layers\nare separated. In addition to typical document image bina-\nrization techniques [15,19,30], some music-speciﬁc docu-\nment binarization techniques have been proposed [28, 35].\nNext, if the lyrics are part of the musical content, they need\nto be recognized as well. This is why there have been\nsome proposals to separate the staves and the text [3, 7].\nOnce staff sections have been isolated, staff-line removal\nmay take place. Although staff lines are necessary for mu-\nsic interpretation, most OMR workﬂows are based on de-\ntecting and removing the staff lines to perform connected\ncomponent analysis on the remaining musical symbols.\nA comprehensive review and comparison of the ﬁrst at-\ntempts for staff-line removal can be consulted in Dalitz et\nal. [10], and new techniques are being continuously devel-\noped [11, 13, 16]. In addition to these stages, we also ﬁnd\nvery speciﬁc processes that depend on the speciﬁc char-\nacteristics of the manuscript of interest, such as measure\nisolation [33], page-border removal [26], or frontispiece\ndetection in Medieval manuscripts [31].\nRecently, the full document processing of music score\nimages has been implemented using CNNs, which learn\nto classify each pixel of the image according to its cate-\ngory [6]. This approach allows the analysis of entire doc-\numents with a generic method to any type of manuscript\nas long as there is appropriate training data. In addition to\nthese advantages, this approach has proven to outperform\nthe traditional strategies, and so it can be considered thestate of the art in document analysis of music score im-\nages.\nHowever, this process takes a long time because it has\nto perform an independent classiﬁcation for each pixel of\nthe image. Since images used are usually at high resolu-\ntion involving millions of pixels, the resulting long com-\nputational time prevents its use in an interactive machine\nlearning environment, where the user expects quick re-\nsponses from the machine learning process while training\nit. Hence, in this work, we propose an image-to-image ap-\nproach using neural networks, with the aim of maintaining\nthe advantages of the state of the art but dramatically re-\nducing the temporal cost.\n3. FRAMEWORK\nFormally, we deﬁne the task of document analysis of mu-\nsic score images as the process of assigning a category to\neach pixel of the image based on the layer of information to\nwhich it belongs. Speciﬁcally, we instantiate the task to the\nset of categoriesfbackground ,note ,staff line ,\ntextg. The reasoning behind this set is that it consist of\nthe layers that lead to a general analysis of the image for\nthe purpose of OMR, given that: musical notes are essen-\ntial to recover the musical information; staff lines are nec-\nessary to divide the score into staves, as well as to estimate\nthe pitch of the notes; text is also key for music interpreta-\ntion but its information must be recognized with different\nalgorithms (i.e., Optical Character Recognition); the rest\nof pixels can be considered as background. However, we\nshow below that the chosen formulation can be extended\nto any other type of category set provided that sufﬁcient\nlabeled data is available.\nAs mentioned above, the aim of this work is to alleviate\nthe computational cost involved in a pixel-wise classiﬁca-\ntion approach. We address the issue here by using a set of\nauto-encoders, which learn an image-to-image mapping.\nWithin our context, this means that the image can be pro-\ncessed in one step at a higher order of efﬁciency.\nConventional auto-encoders consist of feed-forward\nneural networks for which the input and output must be\nexactly the same. The network typically consists of two\nstages that learn the functions fandg, which are called en-\ncoder and decoder functions, respectively. Formally speak-\ning, given an input x, the network must minimize a di-\nvergence L(x; g(f(x))). An auto-encoder might initially\nappear to be pointless because it is trained to learn the\nidentity function. Nevertheless, the encoder function fis\ntypically forced to produce a representation with a lower\ndimensionality than the input. The encoder function there-\nfore provides a meaningful compact representation of the\ninput, which might be of great interest for feature learning\nor dimensionality reduction [37].\nIn our case, we modify this traditional behavior so that\nthe model specializes in selecting the pixels that belong to\neach of the elements from the category set. This type of\nmodel is referred to as Selectional Auto-Encoder (SAE)\n[13]. An SAE is trained to perform a function such that\ns:R(w\u0002h)![0;1](w\u0002h). In other words, it learns aProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 257Text\nNote\nStaff\nBackground\nENCODING DECODING\nCOMBINATION\nFigure 1 . Graphical scheme of the SAE-based 1-vs-all approach for document analysis of music scores images. The outputs\nof the individual SAE are represented as grayscale masks in which the white color represents the maximum selectional\nvalue. Coloring for the ﬁnal combination: background in white, music symbols in black, staff lines in blue, and text in red.\nbinary map over a w\u0002himage that preserves the input\nshape. The predicted value for each pixel indicates its se-\nlection level, representing 1as the maximum. Then, the\nnetwork is trained to minimize the divergence between a\nbinary image in which only the pixels that belong to the\ncategory of interest are activated.\nActually, an SAE represents a two-class categorizer\nwith one class represented by the value 0and another rep-\nresented by the value 1. To perform a multi-class document\nanalysis like the one formalized above, we follow a 1-vs-all\nstrategy, much in the same way as other binary classiﬁers\nsuch as the Support Vector Machine [20]. That is, we train\na different SAE focused on each category, assuming the\ncategory of interest as 1and the remaining ones as 0. At\nthe time of inference, the outputs of all the trained SAEs\nare combined to obtain a global analysis of the document.\nWe ﬁnd two important advantages of predicting each\nlayer separately. On the one hand, the extraction of a spe-\nciﬁc layer only requires the ground-truth data of the tar-\ngeted category, thus reducing the effort involved in prepar-\ning the training set if only a subset of the categories is pur-\nsued. On the other hand, the predictions provided by each\nSAE could be processed separately—e.g., to apply differ-\nent thresholds to each result or to resolve inconsistencies\nwhen many predictions disagree about a speciﬁc region—\nwhich might be interesting depending on the way the sub-\nsequent stages of the OMR workﬂow operate.\nBelow we discuss more details about the actual imple-\nmentation of the described framework for the present work.3.1 Implementation details\nAn SAE can be conﬁgured in many ways. We speciﬁcally\nconsider a Fully-Convolutional Network (FCN) topology,\ngiven the good results obtained by this type of neural net-\nworks in this task [32], and in general for any image-\nrelated task [23].\nAn FCN is a type of neural network that is entirely\nbased on ﬁlters (i.e., convolutions). These ﬁlters are con-\nﬁgured in a hierarchy of layers that provide multiple rep-\nresentations from the input image with different levels of\nabstraction: while the ﬁrst layers emphasize details of the\nimage, the last layers focus on high-level entities [22]. The\nparameters of the convolutions are typically optimized by\nbackpropagation [24] through a training set, with the ob-\njective of generalizing to unseen data.\nConsequently, the hierarchy of layers of our SAE con-\nsists of a series of convolutional plus pooling layers, until\nan intermediate layer is attained. As these layers are ap-\nplied, ﬁlters are able to relate parts of the image that were\ninitially far apart. Then, it follows a series of convolu-\ntional plus up-sampling layers that reconstruct the image\nup to the same input size copying neighboring pixels. The\nlast layer consists of a set of neurons with sigmoid activa-\ntion that predict a value in the range of [0;1], depending on\ntheselectional level predicted for the corresponding input\npixel. This selectional level is expected to approach 1as\nthe model is more conﬁdent that the pixel belongs to the\ncategory of interest. This speciﬁc conﬁguration needs to\nbe tweaked for the problem at issue, and so we will per-\nform some preliminary experiments to evaluate different\noptions.\nThe training stage consists of providing the SAE with258 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Corpus Salzinnes Einsiedeln\nPages 10 10\nAvg. height and width\nper page (in pixels)5 100\u00023 200 5 550\u00023 650\n%Background 80.6 79.1\nNote 11.2 10.0\nStaff line 4.5 6.9\nText 3.7 4.0\nTable 1 . Overview of the corpus used in our experiments:\nnumber of pages, average size per page, and class distribu-\ntion (in %).\nexamples of images and their corresponding ground truth,\nthat is, binary maps over the pixels that belong to the cat-\negory of interest. The cross-entropy loss function between\neach output activation and its expected activation is com-\nputed. Then, ﬁlters are tuned using stochastic gradient de-\nscent optimization [2] with a mini-batch size of 16and the\nadaptive learning rate strategy proposed by Zeiler [38].\nOnce all the corresponding SAEs for the categories\nconsidered in this work (SAE background , SAE note, SAE staff,\nSAE text) are trained, they can be used to perform the doc-\nument analysis process. In order to compute a single cat-\negory for each pixel, we select the category whose SAE\nretrieves the highest selection value. A graphical scheme\nof this operation is depicted in Figure 1.\nGiven that our SAE is conﬁgured as a fully-\nconvolutional model (i.e., without any dense layer), the\ninput and the output layers can be of an arbitrary size.\nIn practice, however, processing a high-resolution musi-\ncal score has a high memory consumption. This is why in\nour case we need to divide the input music score into equal\npatches of 256\u0002256pixels, which was the largest size fea-\nsible with our computational resources. Theoretically, this\nlimitation should not affect the performance of the mod-\nels except for the case of the edges of the input patches.\nThis can be palliated by considering overlap at the time\nof splitting the input image, and ignoring the edges of the\npredictions made.\n4. EXPERIMENTS\n4.1 Experimental setup\nFor the evaluation of our approach, we consider high-\nresolution image scans of two ancient music manuscripts.\nThe ﬁrst corpus is a subset of 10 pages of the Salzinnes\nAntiphonal manuscript (CDM-Hsmu M2149.14),1music\nscore dated 1554–5. The second corpus is 10 pages of the\nEinsiedeln, Stiftsbibliothek, Codex 611(89), from 1314.2\nTable 1 gives an overview of this corpora with some of\ntheir speciﬁc features. For our experiments, the images\nhave been considered in their grayscale format.\n1https://cantus.simssa.ca/manuscript/133/\n2http://www.e-codices.unifr.ch/en/sbe/0611/The ground-truth data was created manually by labeling\npixels into the four categories mentioned above. Although\nin this work we circumscribe the experiments to corpora\nfrom Medieval music manuscripts, we believe that their\ndifﬁculty and wealth of information (at the image level) al-\nlows us to generalize the conclusions to any type of music\nscore image.\nIn order to provide a more reliable assessment, we fol-\nlow a corpus-wise 5-fold cross validation scheme. In each\niteration of each corpus, 2complete pages—not necessar-\nily consecutive ones—are used for test evaluation, 2pages\nare used as validation, and 6pages for training the SAE\nmodels. The reported results will represent averages over\nthese 5independent evaluation processes. It be should\nnoted that the experiments in both corpora have been per-\nformed individually, since in the context of machine learn-\ning, it could be assumed that the samples belong to the\nsame domain. Despite this assumption, future research\naims to expand the experimental setup to include more re-\nalistic scenario with cross-manuscript experiments.\nAs can be observed, the distribution of each class is\nhighly biased, background being the most represented\nclass. Given this distribution, we consider appropriate met-\nrics for such imbalanced datasets. For instance, the F1typ-\nically represents a fair metric in these scenarios. In a two-\nclass classiﬁcation problem, this measure can be computed\nas\nF1=2\u0001TP\n2\u0001TP+FP+FN; (1)\nwhere True Positive (TP) stands for the correctly classi-\nﬁed elements of the relevant class, False Positive (FP) rep-\nresents the misclassiﬁed elements from the relevant class,\nand False Negative (FN) stands for the misclassiﬁed ele-\nments of non-relevant class.\nTo compute single values encompassing all possible\ncategories, this metric can be reformulated into macro F1\n[27], which is computed as the average of all class-wise\nmetrics.\n4.2 Network selection\nIn this section we carry out a preliminary study to evalu-\nate how some of the parameters of the SAE conﬁguration\naffect the accuracy of the classiﬁcation. It is worth men-\ntioning that the different conﬁgurations may behave dif-\nferently according to the category of interest (background,\ntext, note, or staff). In this regard, however, we assume\nfor this study a general assessment taking into account all\nclasses simultaneously.\nThere exist a huge number of possibilities for establish-\ning the organization of the neural model [18]. In order to\nreduce the search, we restrict ourselves to evaluate only\nthe most interesting hyper-parameterization, namely the\ndepth of the encoding/decoding blocks and whether encod-\ning and decoding layer actually perform down-sampling\nand up-sampling operators. The latter points to an inter-\nesting issue: performing down- and up-sampling opera-\ntions allows intermediate ﬁlters to focus on different levelsProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 259Down/Up-Sampling\nDepth No Yes\n1 90.0 90.7\n2 91.5 94.9\n3 93.3 96.0\n4 94.2 95.4\nTable 2 . Macro average F1(%) of the 5-fold cross-\nvalidation over the validation partitions, with respect to the\ndepth of the encoding/decoding layers and whether or not\nconsidering sampling operators.\nof abstraction within the image, also reducing the intrin-\nsic complexity—since the image in the intermediate lay-\ners would be smaller. However, keeping the original size\nthroughout the process avoids having to learn to recon-\nstruct the image, at the cost of losing the beneﬁts discussed\nabove for the opposite case.\nThe rest of the parameters are ﬁxed manually, based on\ninformal testing, as follows: the number of ﬁlters per con-\nvolution are set to 128and the size of the convolutional\nkernels to 5\u00025. Also, all intermediate convolutional ﬁl-\nters use Rectiﬁed Linear Unit (ReLU) activations [17].\nTable 2 shows the macro average F1attained by each\ndifferent SAE conﬁguration on the validation sets.\nConcerning the depth of the encoding/decoding blocks,\na progress towards an upward trend is observed. In the case\nof using sampling operations, this trend ﬁnds a peak at 3\nlayers. In the opposite case (i.e., with no sampling), the\nimprovements are more subtle and the peak is not reached\nwithin the number of layers considered. Due to computa-\ntional resources, we were not able to carry out experiments\nwith more layers, so it is not possible to know when the\npeak would be reached.\nOn the other hand, regardless of the number of layers\nchosen, we can observe that there is a clear tendency in\nthe advantage of doing down- and up-sampling operations,\nsince the latter case is always better than its analog for the\nsame depth in the experiments carried out.\nAccording to these results, the ﬁnal SAE conﬁguration\nfor all the categories is shown in Table 3.\n4.3 Results\nIn this section we analyze in detail the performance that\nwas attained using the best SAE conﬁguration of the pre-\nvious section in comparison to the pixelwise CNN-based\napproach, that currently represents the state of the art in\nthis task [5]. All experiments have been performed in\nsimilar conditions on a general-purpose computer with the\nfollowing technical speciﬁcations: Intel(R) Core(TM) i7-\n7700HQ CPU @2.8GHz \u00024, 32GB RAM, GTX1070 GPU\nand Linux Mint 18.2 (64 bits) operating system. The code\nhas been written using Python language (v2.7) and Keras\nframework.\nGiven that the objective of this paper is not only to mea-\nsure the accuracy of the new model but also its efﬁciency,Table 5 shows a comparison of both aspects in terms of\nmacro F1and the approximated time needed to process a\ndocument. Traditionally, the training cost is not taken into\naccount when evaluating these systems because the pro-\ncess is usually performed ofﬂine. Note, however, that both\napproaches involved a similar training cost in the order of\nseveral hours on Graphical Processing Units.\nAccuracy results show a visible difference between the\ncorpora considered. While results are closer to the opti-\nmum in Salzinnes, both approaches seem to ﬁnd more dif-\nﬁculties in Einsiedeln. However, this difference is not ob-\nvious in a qualitative evaluation, as depicted in Table 4.\nIt can be observed that the SAE-based strategy gener-\nally obtains a higher F1than that based on CNNs. Note,\nhowever, that the objective of this experiment is not to\ndemonstrate that the SAE-based approach outperforms sig-\nniﬁcatively the state of the art, but to obtain results that can\nbe considered similar, which is clearly reported according\nto these ﬁgures. On the other hand, the computation time\nneeded to process a complete manuscript page is drasti-\ncally lower with the SAE, going from several hours to a\nfew minutes. This happens because the CNN approach\nhas to classify each pixel of the image, whereas the SAE\napproach can make predictions of many pixels simultane-\nously (in our experiments, 256\u0002256). Obviously, the net-\nwork of the latter approach is more complex, but it clearly\ncompensates with respect to the temporal cost.\nThus, this comparison with the state of the art demon-\nstrates that the proposed approach allows obtaining a sim-\nilar performance when performing the document analysis,\nwith a radically lower computational cost, thus making an\nimportant contribution to the ﬁeld of OMR.\n5. CONCLUSIONS\nIn this paper we have presented a machine-learning strat-\negy for the document analysis of music score images. The\nstrategy consists in training SAE, conﬁgured as convo-\nlutional neural networks, that allow to extract the differ-\nent layers of information found in documents through an\nimage-to-image formulation.\nIn a preliminary study, we have determined some of\nthe parameters that lead to a better conﬁguration of the\nSAE. In particular, we have evaluated the depth of the en-\ncoder/decoder layers, as well as the relevance of whether\nperforming or not down- and up-sampling operations.\nGenerally, increasing the number of layers is beneﬁcial, to\na certain extent, while sampling operators lead to a much\nmore effective network.\nAlthough we did not exhaustively test the various pos-\nsible network conﬁgurations for this ﬁrst study, we have\nshown that the proposed approach can achieve the accu-\nracy similar to the state-of-the-art algorithms, and more\nimportantly, with an efﬁciency improvement of orders of\nmagnitude.\nOur results represent the ﬁrst step towards an interac-\ntive scenario in which the user and the system can interact\nto solve the OMR task. This scenario has already been de-\nvised before [34]; however, our approach allows us to be260 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Input Encoding Decoding Output\nConv(128,5,5,ReLU) Conv(128,5,5,ReLU)\nMaxPool(2,2) UpSamp(2,2)\n[0;255]256\u0002256Conv(128,5,5,ReLU) Conv(128,5,5,ReLU) [0;1]256\u0002256\nMaxPool(2,2) UpSamp(2,2)\nConv(128,5,5,ReLU) Conv(128,5,5,ReLU)\nMaxPool(2,2) UpSamp(2,2)\nConv(1,5,5,Sigmoid)\nTable 3 . Detailed description of the selected SAE architecture, implemented as a FCN. Conv(f,h,w,a) stands for a convolu-\ntion operator of fﬁlters, with h\u0002wpixel kernels with an aactivation function; MaxPool(h,w) stands for the max-pooling\noperator with a w\u0002hkernel and stride; UpSamp(h,w) denotes an up-sampling operator of hrows and wcolumns; ReLU\nandSigmoid denote Rectiﬁer Linear Unit and Sigmoid activations, respectively.\nOriginalPredictionResult\nBackground Staff Note Text\nTable 4 . Qualitative examples of document analysis over selected patches of the corpora (Salzinnes, ﬁrst row; Einsiedeln,\nsecond row), depicting the original piece of the document along with the individual SAE predictions, and the resulting\nanalysis. The predictions of the individual SAE are represented as grayscale masks in which the white color represents the\nmaximum selectional value. Coloring for the ﬁnal result: background in white, music symbols in black, staff lines in blue,\nand text in red.\nStrategyMacro F1Time per page\nSalzinnes Einsiedeln\nSAE 95.5 90.3 \u00181minute\nCNN 91.3 88.4 \u00186hours\nTable 5 . Comparison of our SAE-based approach with\nthe state-of-the-art (CNN) performance taking into account\nboth accuracy and efﬁciency of the document analysis pro-\ncess.\ncloser to real practice since the document analysis process-\ning stage no longer implies a bottleneck.\nNevertheless, the costly training process is still an ob-\nstacle for this scenario in which models must re-trained\naccording to user’s corrections. Therefore, addressing this\nmatter is essential in future work. Among the possible op-\ntions, we want to consider the use of pre-trained models\nthat can be adapted with few new samples and less de-\nmanding training procedures.\nAlso, we are especially interested in the aspect of cross-\nmanuscript adaptation. That is, how to exploit modelsspeciﬁcally trained for a manuscript in other manuscripts\nwith a different layout organization. In this way, the ini-\ntial effort to obtain ground-truth data from the manuscript\nat issue can be reduced. We believe that semi-supervised\nlearning algorithms could be of interest in this case, for\nwhich the models learn to adapt to a new manuscript by\njust providing them with new (unlabeled) images. This\ncan be performed by promoting convolutional ﬁlters that\nare both useful for the classiﬁcation task and invariant with\nrespect to the differences among manuscript types [14].\n6. ACKNOWLEDGEMENT\nThis work was supported by the Spanish Ministerio de\nEconom ´ıa, Industria y Competitividad through HispaMus\nproject (TIN2017-86576-R) and Juan de la Cierva - For-\nmaci ´on grant (Ref. FJCI-2016-27873), and the Social Sci-\nences and Humanities Research Council of Canada.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 2617. REFERENCES\n[1] D. Bainbridge and T. Bell. The challenge of opti-\ncal music recognition. Computers and the Humanities ,\n35(2):95–121, 2001.\n[2] L. Bottou. Large-scale machine learning with stochas-\ntic gradient descent. In Proceedings of COMP-\nSTAT’2010 , pages 177–186. Springer, 2010.\n[3] J. A. Burgoyne and I. Fujinaga. Lyric extraction and\nrecognition on digital images of early music sources. In\nProceedings of the 10th International Society for Mu-\nsic Information Retrieval Conference , pages 723–728,\n2009.\n[4] J. A. Burgoyne, L. Pugin, G. Eustace, and I. Fujinaga.\nA comparative survey of image binarisation algorithms\nfor optical recognition on degraded musical sources.\nInProceedings of the 8th International Conference on\nMusic Information Retrieval , pages 509–512, 2007.\n[5] J. Calvo-Zaragoza, F. J. Castellanos, G. Vigliensoni,\nand I. Fujinaga. Deep neural networks for document\nprocessing of music score images. Applied Sciences ,\n8(5):654–674, 2018.\n[6] J. Calvo-Zaragoza, G. Vigliensoni, and I. Fujinaga.\nOne-step detection of background, staff lines, and sym-\nbols in medieval music manuscripts with convolutional\nneural networks. In Proceedings of the 18th Interna-\ntional Society for Music Information Retrieval Confer-\nence, Suzhou, China , pages 724–730, 2017.\n[7] V . B. Campos, J. Calvo-Zaragoza, A. H. Toselli, and E.\nVidal. Sheet music statistical layout analysis. In 15th\nInternational Conference on Frontiers in Handwriting\nRecognition, Shenzhen, China , pages 313–318, 2016.\n[8] L. Chen and C. Raphael. Human-directed optical music\nrecognition. Electronic Imaging , 2016(17):1–9, 2016.\n[9] L. Chen, E. Stolterman, and C. Raphael. Human-\ninteractive optical music recognition. In ISMIR , pages\n647–653, 2016.\n[10] C. Dalitz, M. Droettboom, B. Pranzas, and I. Fujinaga.\nA comparative study of staff removal algorithms. IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence , 30(5):753–766, 2008.\n[11] J. Dos Santos Cardoso, A. Capela, A. Rebelo, C.\nGuedes, and J. Pinto da Costa. Staff detection with sta-\nble paths. IEEE Transactions on Pattern Analysis and\nMachine Intelligence , 31(6):1134–1139, 2009.\n[12] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Clas-\nsiﬁcation . John Wiley & Sons, New York, NY , 2nd edi-\ntion, 2001.\n[13] A. Gallego and J. Calvo-Zaragoza. Staff-line removal\nwith selectional auto-encoders. Expert Systems with\nApplications , 89:138–48, 2017.[14] Y . Ganin and V . Lempitsky. Unsupervised domain\nadaptation by backpropagation. In International Con-\nference on Machine Learning , pages 1180–1189, 2015.\n[15] B. Gatos, I. Pratikakis, and S. J. Perantonis. Adaptive\ndegraded document image binarization. Pattern Recog-\nnition , 39(3):317–327, 2006.\n[16] T. G ´eraud. A morphological method for music score\nstaff removal. In International Conference on Image\nProcessing , pages 2599–2603, 2014.\n[17] X. Glorot, A. Bordes, and Y . Bengio. Deep sparse rec-\ntiﬁer neural networks. In Proceedings of the Fourteenth\nInternational Conference on Artiﬁcial Intelligence and\nStatistics, Fort Lauderdale, FL , pages 315–323, 2011.\n[18] D. Graupe. Principles of Artiﬁcial Neural Networks .\nWorld Scientiﬁc Publishing Co., Inc., River Edge, NJ,\nUSA, 2nd edition, 2007.\n[19] N. R. Howe. Document binarization with automatic\nparameter tuning. International Journal on Document\nAnalysis and Recognition , 16(3):247–258, 2013.\n[20] C.-W. Hsu and C.-J. Lin. A comparison of methods\nfor multiclass support vector machines. IEEE Trans-\nactions on Neural Networks , 13(2):415–425, 2002.\n[21] F. D. Julca-Aguilar and N. S. T. Hirata. Image oper-\nator learning coupled with CNN classiﬁcation and its\napplication to staff line removal. In 14th IAPR Interna-\ntional Conference on Document Analysis and Recogni-\ntion, ICDAR 2017, Kyoto, Japan , pages 53–58, 2017.\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Ima-\ngenet classiﬁcation with deep convolutional neural net-\nworks. In 26th Annual Conference on Neural Informa-\ntion Processing Systems , pages 1106–1114, 2012.\n[23] Y . LeCun, Y . Bengio, and G. Hinton. Deep learning.\nNature , 521(7553):436–444, 2015.\n[24] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner.\nGradient-based learning applied to document recog-\nnition. Proceedings of the IEEE , 86(11):2278–2324,\n1998.\n[25] B. Moysset, C. Kermorvant, C. Wolf, and J. Louradour.\nParagraph text segmentation into lines with recurrent\nneural networks. In 13th International Conference on\nDocument Analysis and Recognition , pages 456–460.\nIEEE, 2015.\n[26] Y . Ouyang, J. A. Burgoyne, L. Pugin, and I. Fujinaga.\nA robust border detection algorithm with application\nto medieval music manuscripts. In Proceedings of the\n2009 International Computer Music Conference , pages\n101–104, 2009.\n[27] A. ¨Ozg¨ur, L. ¨Ozg¨ur, and T. G ¨ung¨or. Text categorization\nwith class-based and corpus-based keyword selection.\nInInternational Symposium on Computer and Infor-\nmation Sciences , pages 606–615, 2005.262 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[28] T. Pinto, A. Rebelo, G. A. Giraldi, and J. S. Cardoso.\nMusic score binarization based on domain knowledge.\nIn5th Iberian Conference on Pattern Recognition and\nImage Analysis, Las Palmas de Gran Canaria, Spain ,\npages 700–708, 2011.\n[29] A. Rebelo, I. Fujinaga, F. Paszkiewicz, A. R. S.\nMarc ¸al, C. Guedes, and J. S. Cardoso. Optical mu-\nsic recognition: State-of-the-art and open issues. Inter-\nnational Journal of Multimedia Information Retrieval ,\n1(3):173–190, 2012.\n[30] J. Sauvola and M. Pietik ¨ainen. Adaptive document im-\nage binarization. Pattern Recognition , 33(2):225–236,\n2000.\n[31] C. Segura, I. Barbancho, L. J. Tard ´on, and A. M. Bar-\nbancho. Automatic search and delimitation of fron-\ntispieces in ancient scores. In 18th European Signal\nProcessing Conference , pages 254–258, 2010.\n[32] E. Shelhamer, J. Long, and T. Darrell. Fully con-\nvolutional networks for semantic segmentation. IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence , 39(4):640–651, 2017.\n[33] G. Vigliensoni, G. Burlet, and I. Fujinaga. Optical mea-\nsure recognition in common music notation. In Pro-\nceedings of the 14th International Society for Music In-\nformation Retrieval Conference , pages 125–130, 2013.\n[34] G. Vigliensoni, J. Calvo-Zaragoza, and I. Fujinaga. An\nenvironment for machine pedagogy: Learning how to\nteach computers to read music. In Proceedings of IUI\nWorkshop on Music Interfaces for Listening and Cre-\nation, Tokyo, Japan , pages 1–4, 2018.\n[35] Q. N. V o, S. H. Kim, H. J. Yang, and G. Lee. An MRF\nmodel for binarization of music scores with complex\nbackground. Pattern Recognition Letters , 69(Supple-\nment C):88–95, 2016.\n[36] Q. N. V o, S. H. Kim, H. J. Yang, and G. Lee. Bina-\nrization of degraded document images based on hierar-\nchical deep supervised network. Pattern Recognition ,\n74:568–586, 2018.\n[37] W. Wang, Y . Huang, Y . Wang, and L. Wang. Gener-\nalized autoencoder: A neural network framework for\ndimensionality reduction. In Workshops of the IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, pages 490–497, June 2014.\n[38] M. D. Zeiler. ADADELTA: An adaptive learn-\ning rate method. Computer Research Repository ,\nabs/1212.5701, 2012.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 263"
    },
    {
        "title": "Functional Harmony Recognition of Symbolic Music Data with Multi-task Recurrent Neural Networks.",
        "author": [
            "Tsung-Ping Chen",
            "Li Su 0004"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492351",
        "url": "https://doi.org/10.5281/zenodo.1492351",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/178_Paper.pdf",
        "abstract": "Previous works on chord recognition mainly focus on chord symbols but overlook other essential features that matter in musical harmony. To tackle the functional harmony recognition problem, we compile a new professionally annotated dataset of symbolic music encompassing not only chord symbols, but also various interrelated chord functions such as key modulation, chord inversion, secondary chords, and chord quality. We further present a novel holistic system in functional harmony recognition; a multi-task learning (MTL) architecture is implemented with the recurrent neural network (RNN) to jointly model chord functions in an end-to-end scenario. Experimental results highlight the capability of the proposed recognition system, and a promising improvement of the system by employing multi-task learning instead of single-task learning. This is one attempt to challenge the end-to-end chord recognition task from the perspective of functional harmony so as to uncover the grand structure ruling the flow of musical sound. The dataset and the source code of the proposed system is announced at https://github.com/ Tsung-Ping/functional-harmony.",
        "zenodo_id": 1492351,
        "dblp_key": "conf/ismir/ChenS18",
        "keywords": [
            "chord recognition",
            "chord symbols",
            "functional harmony",
            "symbolic music",
            "recognition problem",
            "multitask learning",
            "recurrent neural network",
            "end-to-end scenario",
            "chord functions",
            "key modulation"
        ],
        "content": "FUNCTIONAL HARMONY RECOGNITION OF SYMBOLIC MUSIC DATA\nWITH MULTI-TASK RECURRENT NEURAL NETWORKS\nTsung-Ping Chen and Li Su\nInstitute of Information Science, Academia Sinica, Taiwan\nftearfulcanon, lisu g@iis.sinica.edu.tw\nABSTRACT\nPrevious works on chord recognition mainly focus on\nchord symbols but overlook other essential features that\nmatter in musical harmony. To tackle the functional har-\nmony recognition problem, we compile a new profession-\nally annotated dataset of symbolic music encompassing not\nonly chord symbols, but also various interrelated chord\nfunctions such as key modulation, chord inversion, sec-\nondary chords, and chord quality. We further present a\nnovel holistic system in functional harmony recognition;\na multi-task learning (MTL) architecture is implemented\nwith the recurrent neural network (RNN) to jointly model\nchord functions in an end-to-end scenario. Experimental\nresults highlight the capability of the proposed recognition\nsystem, and a promising improvement of the system by\nemploying multi-task learning instead of single-task learn-\ning. This is one attempt to challenge the end-to-end chord\nrecognition task from the perspective of functional har-\nmony so as to uncover the grand structure ruling the ﬂow of\nmusical sound. The dataset and the source code of the pro-\nposed system is announced at https://github.com/\nTsung-Ping/functional-harmony .\n1. INTRODUCTION\n.\nHarmony and tonality represent the essence of West-\nern tonal music. A complete analysis of the functional\nharmony in a musical piece needs one to utilize several\ninterrelated concepts, such as chord progression, diatonic\nfunction, chord inversion, key modulation, to name but a\nfew. These concepts are of fundamental importance in mu-\nsic theory, as they provide a systematic guide for one to\nunderstand how a phrase starts and how it ends, how one\nchord is related to another, how a chord is related to the\nkey of the music, and more generally, how music works.\nComputational approaches to analyzing musical har-\nmony have gained wide attention in the past decades.\nMany works related to this topic, such as chord recogni-\ntion [2,6,12,18,21,23,35], key detection [3,9,17,27], and\nc\rTsung-Ping Chen and Li Su. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Tsung-Ping Chen and Li Su. “Functional Harmony Recognition of\nSymbolic Music Data with Multi-task Recurrent Neural Networks”, 19th\nInternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.chord sequence modeling and generation [5, 10, 14, 28, 31,\n32], as the sub-problems of the complete functional har-\nmony recognition problem, have been extensively studied.\nAmong these sub-problems, chord recognition is arguably\nthe most widely-investigated one.\nChord recognition focuses on the identiﬁcation of chord\nsymbol , i.e., symbols which indicate the root note, the\nchord quality (e.g., Major), and occasionally an extra in-\nterval number (e.g., seventh) of a chord.1Such a notation\nsystem provides direct instructions on chord construction,\nand therefore becomes prevalent in jazz and pop music.\nHowever, this notation system is insufﬁcient for a more\nholistic analysis as it provides no information about chord\nfunctions .2For example, the secondary chord3that plays\nan important role in the analysis of the hierarchical struc-\nture in a chord sequence is rarely discussed in the literature.\nLittle efforts at such data annotation are due to it requires\nmusicology expertise. As a result, there is no systematic\nstudies on a more holistic recognition system based on all\nthe above-mentioned concepts of functional harmony anal-\nysis, to the best of our knowledge. Although this topic has\nbeen extensively studied in the ﬁeld of music information\nretrieval (MIR), the computers’ ability of harmonic analy-\nsis is still quite limited.\nIn this paper, we discuss the functional harmony recog-\nnition problem. To tackle this problem, we ﬁrst build a new\ndataset comprising ﬁve different chord functions, namely\nthe key, primary degree, secondary degree, quality, and\ninversion. Since there is no unique and exact deﬁnition\non functional harmony analysis of music, we alternatively\nconsider the functional harmony recognition problem as\nthe recognition of the above-mentioned ﬁve aspects, in or-\nder to facilitate the discussion in an engineering sense. We\nformulate this problem with the perspective of multi-task\nlearning (MTL), and implement the system using the re-\ncurrent neural networks (RNN) with long short term mem-\nory (LSTM) units, a network structure that has been found\nuseful in the audio chord recognition problem [6]. Exper-\niments on the dataset show that the chord functions can\nbe better resolved within the multi-task learning scenario\n1For example, a chord played with notes C-E-G-B is notated as CM7.\n2In the strict sense, the term chord function refers to the diatonic func-\ntion, namely the Roman numeral annotation and the functions like tonic\n(T), dominant (D) and sub-dominant (S). In this paper we opt to choose\na rather loose deﬁnition by regarding key, degree, and inversion also as\nsome generalized ‘functions’ of a chord.\n3In this paper, the term secondary chord refers to the chord that does\nnot serve the key. The borrowed chords, altered chords and the secondary\ndominant belong to this category.90compared to a single RNN structure, marking a step toward\na more advanced computational music analysis framework.\n2. RELATED WORK\n2.1 Chord recognition and key detection\nThe chord recognition problem has been widely investi-\ngated on both the audio and symbolic data. In recent years,\nvarious machine learning techniques have been applied in\nthis problem. In audio data processing, RNN-based meth-\nods such as the LSTM-based networks have been adopt\ndue to its potential to model the long-term dependency of\na time series [6,12,30]. Besides, [26] proposes a word2vec\nneural network to model the harmony tension , which also\nrepresents another perspective of chord function modeling.\nIn symbolic data processing, early studies based on hand-\ncrafted rules have considered the chord recognition of Ro-\nman numeral notations (i.e., chord symbol and tonality)\n[13]. [19] considered deep neural networks in chord recog-\nnition. Recent approaches based on machine learning, with\nevaluation performance include: [15] applies deep learning\nto identify non-cord tones in symbolic music data, and [21]\nuses a semi-Markov conditional random ﬁeld (CRF) model\nfor symbolic-level chord recognition.\nMost of the studies on the key detection problem inves-\ntigate the global key or home key detection [9, 17]. [17]\nproposes a global key ﬁnding algorithm with a convolu-\ntional neural network (CNN). The studies of key modu-\nlation detection are less seen, while there are still some\nrelated works such as local key detection [27].\n2.2 Multi-task learning (MTL)\nThe MTL technique is proposed to ﬁt one shared network\nto multiple related sets of labels, i.e., to learn multiple tasks\nat a time [20, 29]. If a primary task itself is difﬁcult or is\nshort of training data, its performance can be improved by\nintroducing some auxiliary tasks by assuming these tasks\nshare similar network structure.\nMTL has exhibited great potential in MIR [11] since\ndifferent attributes of music are often highly related. For\nexample, in [34], the neural network is shared by the chord\nrecognition task as well as the root note recognition task,\nand doing this can help to improve the accuracy of chord\nrecognition. Similar ideas can also be seen in other models\nsuch as the multi-chain hidden Markov model (HMM) [22]\nand the dynamic Bayesian network [24]. Therefore, it sug-\ngests that the functional harmony problem itself is a multi-\ntask learning problem, as determining one type of chord\nfunction usually needs the information of another.\n2.3 Datasets for functional harmony recognition\nAccurate annotation chord functions is hard to build in the\naudio domain, but rather feasible in the symbolic domain.\nThere are a few datasets including annotation of some, if\nnot all, chord functions: for example, the KSN dataset pro-\nvides the annotation of chord and key modulation (i.e., the\nRoman numeral annotation) [16], the Theme And Varia-\ntion Encodings with Roman Numerals (TA VERN) datasethas Roman number chord annotation [8], and the Yale\nClassical Archive Corpus (YCAC) dataset has local tonic\nlabel and chord [33].\n3. DATA AND LABELS\nWe propose the Beethoven Piano Sonata with Function\nHarmony (BPS-FH) dataset, which contains the symbolic\nmusical data and functional harmony annotations of the 1st\nmovements of 23 of Beethoven’s Piano Sonatas.4BPS-\nFH dataset provides a more consistent corpus in terms of\nmusical form and genre with concise annotations for the\nanalysis of harmony. As an ongoing work, the annotation\nwill be extended to all the 32 piano sonatas.\n3.1 Annotation process: harmonic analysis5\nThe BPS-FH dataset is annotated by an expert musicolo-\ngist with a basic harmonic analysis process step-by-step.\nAs opposed to the chord symbol annotation, the traditional\nharmonic analysis in music theory and musicology adopts\na relative representation for chords to emphasize the in-\nteraction between chords in a given context. To perform\nharmonic analysis, there are several implicit processes:\n\u000fKey identiﬁcation: the ﬁrst step of harmonic analy-\nsis is to identify the local key according to context.\nNote that in many classical musical pieces, there is\nno exact analysis on the local key, for key modula-\ntion usually occurs, making it hard to ﬁnd the local\nkey in a certain excerpt.6When the ambiguity oc-\ncurs, ﬁnding a later cadence which is in a key-steady\ncontext, and then analyzing chords backwards might\ngive a solution.\n\u000fSegmentation: since music itself is not represented\noriginally as a sequence of chords, it is important\nto identify reasonable segments for labeling chords.\nA convincing segmentation should take the tempo-\nral rhythm and the harmonic rhythm (i.e., the rate at\nwhich the chords change) into consideration.\n\u000fHarmonic reduction: after determining the seg-\nments, each segment is reduced to a chord symbol\n(including chord root and chord quality) according\nto the tones within it. Harmonic reduction is a non-\ntrivial and complicated process; there are many con-\nfusing factors, such as the non-chord tones, or the\nabsence of harmonic tones in the segment.\n\u000fInversion recognition: the inversion of a chord is de-\ntermined by which of the notes is the bottom note, or\nbass note, of the chord. Typically, the lowest note in\n4The 23 pieces are: No. 1, 3, 5, 6, 8, 11, 12, 13, 14, 16, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 31, and 32. And all the repetitions in the\nsonatas are unfold.\n5In this paper, harmonic analysis refers to Roman numeral analysis.\n6In music, modulation is the act of changing from one key (tonic, or\ntonal center) to another. Generally speaking, the key of a musical piece\nrefers to the global key which identiﬁes the global tonic note and the\nﬁnal point of rest for the piece, while a modulation conducts the piece\ntemporarily to another key, that is, a local key, which replaces the global\ntonic with a temporary tonic in a local area.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 91(a) Diatonic function in C major.\n(b) Diatonic function in C minor (harmonic minor scale).\nFigure 1 : Illustration of diatonic functions in relation to\nthe diatonic chords of the given keys. Note that in minor\nkey, the superscript+is added to the mediant because it is\nan augmented chord.\na segment would be considered as the bass of the re-\nduced chord. However, the lowest note is not always\nregarded as the bass notes; the pedal point is one of\nsuch examples.\n\u000fLabeling diatonic functions: after determining the\nkey and the chord symbol, a function is assigned\nto the chord. In a major key, the following Ro-\nman numerals are used to represent the functions of\ndiatonic chords: I (tonic), ii (super-tonic), iii (me-\ndiant), IV (sub-dominant), V (dominant), vi (sub-\nmediant), and viio(leading). The capital numerals\ndenote major chords, the lowercase numerals denote\nminor chords, and the superscriptodenotes dimin-\nished chords. Figure 1 shows the details of diatonic\nfunctions in both major and minor keys.\nFigure 3b exhibits a brief example of harmonic analysis\nfor the excerpt in Figure 3a. It is worth mentioning some\npossible confusions when analyzing harmony on this ex-\nample: at measure 83, there are two non-chord tones, G at\nthe 1st beat, and Ebat the second half of the 2nd beat, both\nof which might be confusing for harmonic reduction. Es-\npecially, the existence of the the non-chord tone G prevents\nthe note E (the last note of measure 82) from directly re-\nsolving to F, and blurs the boundary between F-minor key\nand Eb-major key. Hence, the key modulation might occur\nat measure 83 as labeled, but might also occur at measure\n84 or even 85. It should be acknowledged that harmonic\nanalysis is inherently subjective, and the confounding ef-\nfect of subjectivity may affect the performance of a chord\nrecognition system in many ways [25]. Details about the\nharmonic analysis techniques and labeling paradigms can\nbe found in [1] and [4] .\n3.2 Annotations in the BPS-FH Dataset\nA fundamental harmonic analysis provides the information\nof key, degree, quality and inversion. Therefore, the BPS-\nFH dataset has the corresponding annotations as follows:\u000fKey: the key to which a chord belongs in a local\narea. Since key modulation is essential in piano\nsonata, we trace the change of key, that is, we spec-\nify the local key , or temporary tonic, so as to show\nthat how a key deviates from the global one during\nthe course of the movement.\n\u000fPrimary degree and secondary degree: degree refers\nto the position of a chord’s root on the diatonic scale\nof a key.7There are seven possible degrees on a\ndiatonic scale, that is, 1, 2, ..., 7. We use a pair\nof degrees, primary degree andsecondary degree ,\nfor both diatonic chords and secondary chords. Pri-\nmary degree indicates the position of the temporary\ntonic on the scale, while secondary degree denotes\nthe position of the chord’s root based on the tempo-\nrary tonic; the couple of degrees is represented as\nsecondary degree/primary degree. In the case of dia-\ntonic chord, the primary degree is always 1. That is,\nthe temporary tonic is the same as that of the current\nkey. As for the secondary chord, both the primary\ndegree and the secondary degree can be any possible\ndegree. For example, the diatonic chord V is rep-\nresented as 5/1, while the secondary chord V =IV is\nrepresented as 5/4.\n\u000fQuality: chord quality is deﬁned by the intervals\nwithin a chord. For instance, a major triad has a ma-\njor third and a perfect ﬁfth above its root. 10 types of\nchord quality are identiﬁed in the dataset, which are\nmajor triad (M), minor triad (m), augmented triad\n(a), diminished triad (d), major seventh (M7), minor\nseventh (m7), dominant seventh (D7), diminished\nseventh (d7), half-diminished seventh (h7), and aug-\nmented sixth (a6).\n\u000fInversion: inversion of a chord describes which of\nthe tones in a chord is the bass note. For exam-\nple, the C-major triad has three candidates, C, E and\nG, as its bass, and thus has three possible inversions\n(root position is regarded as one inversion in the con-\ntext). For triads and seventh chords, there are totally\nfour possible inversions: the 0th inversion (root posi-\ntion), 1st inversion (6or6\n3for triad, and6\n5for seventh\nchord), 2nd inversion (6\n4for triad, and4\n3for seventh\nchord), and 3rd inversion (4\n2for seventh chord). Note\nthat only seventh chords have 3rd inversion.\nIn summary, the BPS-FH dataset contains 86,950 note\nevents, 29 different keys, 531 key modulations, and 7,394\nchord labels.8\n7For example, the chord C major triad has the degree 1 in C major\nkey, while has the degree 4 in G major key.\n8Among all the chords, 3,438 are inverted; 839 are secondary chords;\n2,951 are major triads; 1,356 are minor triads; 25 are augmented triads;\n286 are diminished triads; 30 are major seventh chords; 86 are minor sev-\nenth chords; 2,037 are dominant seventh chords; 453 are diminished sev-\nenth chords; 104 are half diminished seventh chords; 66 are augmented\nsixth chords.92 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018(a) MTL-BLSTM-RNN with 1 task-speciﬁc layer\n(b) MTL-BLSTM-RNN with 2 task-speciﬁc layers\nFigure 2 : Illustration of the MTL-based functional har-\nmony recognition system, with a BLSTM-RNN model tak-\ning a data stream as input.\n3.3 Data representation\nThe input data is represented in the format of a 61-key\npiano-roll, with the pitch range from C1 to C6 (middle C\n= C4); the duration of each note is measured in crotchet\nbeats. For time resolution, we deﬁne a 32th note as the\nminimal time step. All the note events out of this pitch\nrange are transposed to ﬁt in, while the durations of the\nnote events whose lengths are shorter than the minimal\ntime resolution are set to be the same as the time resolu-\ntion. A piano-roll at a time instance is called a frame .\nAs shown in Figure 2, the input of the LSTM cell is\na segment of data with 32 frames. That is, for a musical\npiece with 4/4 meter, the length of a segment is 4 beats\n(or equivalently 1 bar). And a musical clipcontaining 64\nsegments is fed to the neural networks. The hop size for\nthe neural networks is 4 frames (or half a beat.)\n4. MODEL\nWe employ recurrent neural networks (RNN) with bidirec-\ntional long-short-term memory (BLSTM) units (denoted as\nBLSTM-RNN hereafter) to model sequences of functional\nharmony, by using the above-mentioned data representa-Label Dim Content\nKey 24 24 major and minor keys\nPri. deg. 21 7 Roman numerals by 3 (neutral, ],[)\nSec. deg. 21 7 Roman numerals by 3 (neutral, ],[)\nQuality 10 M, m, a, d, M7, m7, D7, d7, h7, a6\nInversion 4 0th, 1st, 2nd, 3rd\nTable 1 : Chord function labels in the BPS-FH dataset, in-\ncluding key, primary degree (pri. deg.), secondary degree\n(sec. deg.), chord quality, and chord inversion.\nSet Piece No.\nTraining 1, 3, 5, 11, 16, 19 20, 22, 25, 26, 32\nValidation 6, 13, 14, 21, 23, 31\nTesting 8, 12, 18, 24, 27, 28\nTable 2 : The pieces in training, validation, and testing sets.\ntion as input. Such kind of model has been widely used\nin audio chord symbol recognition problems [6,7,12], and\nhas been found capable in learning long-term information\nsuch as music structure. Speciﬁcally, we consider the fol-\nlowing two types of networks:\n\u000fMTL-BLSTM-RNN with 1 task-speciﬁc layer: as\nshown in Figure 2a, we adopt a simple BLSTM\narchitecture with 1024 hidden units for multi-task\nleaning. The outputs of the forward and the back-\nward cells are concatenated and form a 1024-by-2\nmatrix. This matrix is ﬂattened and is connected to\nthe output layer through a fully-connected layer. The\noutput layer is a 80-D vector containing the classes\nfor the ﬁve tasks listed in Table 1. Each class is one-\nhot encoding, and the Softmax function is used for\nthe output vector.\n\u000fMTL-BLSTM-RNN with 2 task-speciﬁc layers: as\nshown in Figure 2b, the architecture is the same as\nthe above, but with an additional task-speciﬁc layer\nbefore the output layer, in order to further increase\nthe model capacity.\nMoreover, to verify the advantage of MTL, we also con-\nsider the single-task learning (STL) as a baseline approach,\nwhere the same BLSTM-RNN is used. As a result, there\nare ﬁve networks in the STL-BLSTM-RNN model, each\nfor one chord function recognition task respectively, and\nare trained individually in the experiment.\n5. EXPERIMENT\n5.1 Experimental settings\nIn the training stage, we divide the 23 pieces in the dataset\ninto three parts, namely the training set, the validation set,\nand the testing set. Each part contains overlapped clips\nwhich are the input instances of the BLSTM networks.\nEach clip contains 64 segments, and the overlap between\ntwo consecutive clips is 32 segments. To balance the dataProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 93(a)\n(b)\n(c)\n(d)\n(e)\nFigure 3 : (a) An excerpt from the 1st movement of Beethoven’s Piano Sonata No. 8, MM. 82-89. (b) The harmonic\nanalysis of this excerpt represented in both chord symbol and chord function. Note that the slash used in chord symbol\nstands for an inversion, and the note behind the slash denotes the bass of the chord. In the analysis, this expert starts from\nF minor, modulates to Ebmajor at measure 83, and ﬁnally ends with an authentic cadence. (c) 5 types of annotations\nrepresenting the functions in (b). (d) The testing result of chord function recognition of the excerpt. Wrong predictions are\nmarked in red. (e) The translation of the result in (d) to chord symbol. For the sake of concision, only the wrong predictions\nlasting at least one quarter note are translated.\ndistribution among all possible keys, We perform data aug-\nmentation by transposing all the clips into 12 keys. As a\nresult, there are 7,320 clips for training, 3,672 clips for\nvalidation, and 3,636 clips for testing. Table 2 shows the\nmusical pieces used in each set. In the experiment, we\ncompare the following two tasks:\n\u000fChord symbol recognition: with the symbolic data\nof music as inputs, the model outputs chord sym-\nbol predictions in a segment-wise manner. We used\n25 chord classes for the output layer, that is, 24\nclasses for 12 major triads and 12 minor triads, and\nan ‘other’ class for chords not belonging to either\nmajor triads or minor triads.\n\u000fChord function recognition: similar as the chord\nsymbol recognition, but the outputs of the model are\nchord functions containing ﬁve components.\nBoth the MTL and STL schemes are tested on the chordfunction recognition task, while the chord symbol recogni-\ntion is tested with STL. For the chord function recognition\ntask with MTL scheme, the outputs of the ﬁve chord func-\ntions are translated to chord symbol to evaluate the perfor-\nmance in terms of chord symbol recognition. And for the\nchord function recognition task with STL scheme , ﬁve dif-\nferent networks are trained individually for the evaluation\nof chord function recognition.\nAll networks are implemented with TensorFlow, and are\ntrained using stochastic gradient descent with the Adam\noptimization method. For training objective, we compute\ncategorical cross-entropy between targets labels and net-\nwork outputs, and include a L2 regularization term. More-\nover, to prevent over-ﬁtting and to speed up training con-\nvergence, recurrent batch normalization is applied, and the\ndropout rate at the input and the output of the LSTM cell\nis set to be 0.5.94 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Task Model Key Degree Secondary Quality Inversion Overall Translation\nChord Symbol STL-BLSTM-RNN – – – – – 72.71 -\nSTL-BLSTM-RNN 67.06 48.31 9.38 61.87 57.95 23.57 56.05\nChord Function MTL-BLSTM-RNN with 1 task-speciﬁc layer 68.48 50.49 10.96 62.31 60.04 25.53 56.91\nMTL-BLSTM-RNN with 2 task-speciﬁc layers 66.65 51.79 3.97 60.59 59.10 25.69 56.25\nTable 3 : Accuracy (in %) of functional harmony recognition and comparison between multi-task BLSTM and single-task\nBLSTM. In the table, Degree stands for the accuracy of correctly predicting both the primary and secondary degrees of all\nchords; while Secondary indicates the accuracy of correctly predicting the degrees of secondary chords.\n5.2 Evaluation metrics\nWe compute the segment-level accuracy , the ratio between\nthe number of correct detection and the number of total\nsegments in the testing set, for each category. Only one\naccuracy value is computed in the case of chord symbol\nrecognition, while six types of accuracies are computed in\nthe case of chord function recognition, namely the accu-\nracies of key, degree, secondary chord, quality, inversion,\nand ﬁnally, the overall accuracy. Note that the accuracy\nof secondary chord is computed when a secondary chord\ndoes exist. The overall accuracy counts the segments in\nwhich the ﬁve chord function detections are all correct. An\nextra translation accuracy is computed to examine the per-\nformance of chord function recognition in terms of chord\nsymbol recognition.\n5.3 Results\nTable 3 shows the results of chord symbol recognition and\nchord function recognition. In the task of chord symbol\nrecognition, the STL-BLSTM-RNN-based model gives an\naccuracy of 72.71%. In comparison to other existing works\nwhich also estimate chord symbols on classical music\ndatasets such as [12,21], this result is acceptable while also\nreveals the room for improvement in recognizing chords in\nwestern classical music.\nIn comparison with the chord symbol recognition task,\nperforming the chord function recognition task is much\nmore challenging. Speciﬁcally, the best overall accuracy\namong all chord function recognition tasks is only 25.69%,\nwhich is far from that of chord symbol recognition. This\nis partly because there are as many as 10 chord qualities\nfor the model to predict, and partly because tonal harmony\nitself is complicated and equivocal. On the other hand,\nMTL-BLSTM-RNN model with 1 task-speciﬁc layer out-\nperforms the single-task one for all chord functions. This\nindicates that employing multi-task learning results in a\npromising improvement. Among all chord functions, the\nimprovements of predicting degree and inversion are the\nmost signiﬁcant, with 2.18% and 2.09% increases in accu-\nracy respectively. This consequence may result from the\nfact that identifying the degree and identifying the inver-\nsion of a chord are relatively difﬁcult in classical music,\nand thus beneﬁt more from multi-task learning. Moreover,\nthe accuracies of secondary chord are very low for all ex-\nperiment settings; adding one more task-speciﬁc layer even\ndegrades its performance. This displays the difﬁculty of\nlearning the chord representation consisting of semanticinformation. Finally, we translate the predictions of chord\nfunction recognition tasks into chord symbol to examine\nthe performance in terms of chord symbol recognition. It\ncomes as no surprise that the all the translation accura-\ncies are lower than that of chord symbol recognition. This\nagain marks the challenge of chord function recognition,\nas it needs to consider not only the elements constructing a\nchord symbol, but also more high-level semantic informa-\ntion such as local key and degree.\nAn example of the chord function recognition result is\nshown in Figure 3d. Because the prediction is segment-\nwise, there are numbers of discontinuities in the predicted\nsequences. This issue can be addressed by further incorpo-\nrating temporal smoothing models such as the CRF [21]\nin the future. A close examination of this result shows\nthat although the model gives ‘wrong’ predictions, part of\nthe predictions does match the ground truth on the level of\nchord symbol. For instance, as demonstrated in Figure 3d\n&3e, there are whole-bar error predictions in key and sec-\nondary degree at measure 85; however, these detections be-\ncome correct if we translate them into chord symbol: they\nare both C minor triads, albeit in different keys. In fact,\nfurther analysis points out that the prediction of the modu-\nlation to C minor at measure 85 is also meaningful: there\ndoes exist a potential modulation for there is a tonicization\nof vi constructed by the previous chord viio\n7/vi at the sec-\nond half of the measure 84. From this point of view, the\nmodel does provide more insight into the analysis of tonal\nstructure in this excerpt, as an expert analyzer can do.\n6. CONCLUSION AND FUTURE WORK\nWe have given a systematic investigation on the problem\nof functional harmony recognition of symbolic data based\non deep learning techniques. Experiments on the proposed\nBeethoven Piano Sonata with Functional Harmony dataset\nindicate that functional harmony recognition is a task much\nmore challenging than the chord symbol recognition, and a\nmulti-task learning framework provides a promising solu-\ntion better than a single-task one. Detailed analysis results\nnot only give insightful interpretation, and also pose fur-\nther challenging problems on recognizing key modulation,\nsecondary degree, etc., all with its semantic level higher\nthan chord symbols. This work marks a preliminary step\ntowards a holistic approach of modeling functional har-\nmony, and also provide the potential for one to analyze\ninterpretable and meaningful music patterns from music,\nor to explore some alternative interpretation of music in\nthe study of computational music analysis.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 957. REFERENCES\n[1] Edward Aldwell and Carl Schachter. Harmony and\nVoice Leading . Belmont: Thomsom Schirmer, 3 edi-\ntion, 2003.\n[2] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Audio chord recognition with recurrent\nneural networks. In Proceedings of the 14th Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR) , pages 335–340, 2013.\n[3] Wei Chai and Barry Vercoe. Detection of key change in\nclassical piano music. In Proceedings of the 6th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR) , pages 468–473, 2005.\n[4] Nicholas Cook. A guide to musical analysis . London:\nJ. M. Dent &Sons, 1987.\n[5] W. Bas de Haas, Jos ´e Pedro Rodrigues Magalh ˜aes,\nFrans Wiering, and Remco C. Veltkamp. Automatic\nfunctional harmonic analysis. Computer Music Jour-\nnal, 37(4):37–53, 2013.\n[6] Junqi Deng and Yu-Kwong Kwok. Large vocabulary\nautomatic chord estimation using deep neural nets:\nDesign framework, system variations and limitations.\narXiv preprint arXiv:1709.07153 , 2017.\n[7] Junqi Deng and Yu-Kwong Kwok. Large vocabu-\nlary automatic chord estimation using bidirectional\nlong short-term memory recurrent neural network with\neven chance training. Journal of New Music Research ,\n47(1):53–67, 2018.\n[8] Johanna Devaney, Claire Arthur, Nathaniel Condit-\nSchultz, and Kirsten Nisula. Theme and variation en-\ncodings with roman numerals (TA VERN): A new data\nset for symbolic music analysis. In Proceedings of the\n16th International Society for Music Information Re-\ntrieval Conference (ISMIR) , 2015.\n[9] Emilia G ´omez. Tonal Description of Music Audio Sig-\nnals. PhD thesis, Universitat Pompeu Fabra, 2006.\n[10] Mark Thomas Granroth-Wilding. Harmonic Analysis\nof Music Using Combinatory Categorial Grammar .\nPhD thesis, The University of Edinburgh, 2013.\n[11] Philippe Hamel, Matthew E. P. Davies, Kazuyoshi\nYoshii, and Masataka Goto. Transfer learning in mir:\nSharing learned latent representations for music au-\ndio classiﬁcation and similarity. In Proceedings of the\n14th International Conference on Music Information\nRetrieval (ISMIR) , pages 9–14, 2013.\n[12] Takeshi Hori, Kazuyuki Nakamura, and Shigeki\nSagayama. Music chord recognition from audio data\nusing bidirectional encoder-decoder LSTMs. In Pro-\nceedings of the Asia-Paciﬁc Signal and Information\nProcessing Association Annual Summit and Confer-\nence (APSIPA ASC), 2017 , pages 1312–1315. IEEE,\n2017.[13] Pl ´acido R Illescas, David Rizo, and Jos ´e Manuel I ˜nesta\nQuereda. Harmonic, melodic, and functional automatic\nanalysis. In Proceedings of the International Computer\nMusic Conference (ICMC) , pages 165–168, 2007.\n[14] Nori Jacoby, Naftali Tishby, and Dmitri Tymoczko. An\ninformation theoretic approach to chord categorization\nand functional harmony. Journal of New Music Re-\nsearch , 44(3):219–244, 2015.\n[15] Yaolong Ju, Nathaniel Condit-Schultz, Claire Arthur,\nand Ichiro Fujinaga. Non-chord tone identiﬁcation us-\ning deep neural networks. In Proceedings of the 4th\nInternational Workshop on Digital Libraries for Musi-\ncology , pages 13–16. ACM, 2017.\n[16] Hitomi Kaneko, Daisuke Kawakami, and Shigeki\nSagayama. Functional harmony annotation database\nfor statistical music analysis. In Proceedings of the\n11th International Society for Music Information Re-\ntrieval Conference (ISMIR): Late Breaking session ,\n2010.\n[17] Filip Korzeniowski and Gerhard Widmer. End-to-end\nmusical key estimation using a convolutional neural\nnetwork. In Proceedings of the 25th European Signal\nProcessing Conference (EUSIPCO) , pages 966–970,\n2017.\n[18] Filip Korzeniowski and Gerhard Widmer. On\nthe futility of learning complex frame-level lan-\nguage models for chord recognition. arXiv preprint\narXiv:1702.00178 , 2017.\n[19] Pedro Kr ¨oger, Alexandre Passos, Marcos Sampaio, and\nGivaldo de Cidra. Rameau: A system for automatic\nharmonic analysis. In Proceedings of the International\nComputer Music Conference (ICMC) , pages 273–281,\n2008.\n[20] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng,\nKevin Duh, and Ye-Yi Wang. Representation learn-\ning using multi-task deep neural networks for semantic\nclassiﬁcation and information retrieval. In Proceedings\nof the 2015 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies , pages 912–921, 2015.\n[21] Kristen Masada and Razvan Bunescu. Chord recogni-\ntion in symbolic music using semi-markov conditional\nrandom ﬁelds. In Proceedings of the 18th International\nSociety for Music Information Retrieval Conference ,\npages 23–27, 2017.\n[22] Matthias Mauch. Automatic chord transcription from\naudio using computational models of musical context .\nPhD thesis, Queen Mary University of London, 2010.\n[23] Brian McFee and Juan Pablo Bello. Structured training\nfor large-vocabulary chord recognition. In Proceedings\nof the 18th International Conference on Music Infor-\nmation Retrieval (ISMIR) , pages 188–194, 2017.96 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[24] Yizhao Ni, Matt McVicar, Ra ´ul Santos-Rodriguez, and\nTijl De Bie. An end-to-end machine learning system\nfor harmonic analysis of music. IEEE Transactions on\nAudio, Speech, and Language Processing , 20(6):1771–\n1783, 2012.\n[25] Yizhao Ni, Matt McVicar, Ra ´ul Santos-Rodriguez, and\nTijl De Bie. Understanding effects of subjectivity in\nmeasuring chord estimation accuracy. IEEE Transac-\ntions on Audio, Speech, and Language Processing ,\n21(12):2607–2615, 2013.\n[26] Ali Nikrang, David R. W. Sears, and Gerhard Wid-\nmer. Automatic estimation of harmonic tension by\ndistributed representation of chords. arXiv preprint\narXiv:1707.00972 , 2017.\n[27] H ´el`ene Papadopoulos and Geoffroy Peeters. Local key\nestimation based on harmonic and metric structures. In\nProceedings of the 12th International Conference on\nDigital Audio Effects (DAFx) , pages 1–8, 2009.\n[28] Christopher Raphael and Joshua Stoddard. Functional\nharmonic analysis using probabilistic models. Com-\nputer Music Journal , 28(3):45–52, 2004.\n[29] Sebastian Ruder. An overview of multi-task\nlearning in deep neural networks. arXiv preprint\narXiv:1706.05098 , 2017.\n[30] Siddharth Sigtia, Nicolas Boulanger-Lewandowski,\nand Simon Dixon. Audio chord recognition with a hy-\nbrid recurrent neural network. In Proceedings of the\n16th International Conference on Music Information\nRetrieval (ISMIR) , pages 127–133, 2015.\n[31] David Temperley and Daniel Sleator. Modeling meter\nand harmony: A preference-rule approach. Computer\nMusic Journal , 23(1):10–27, 1999.\n[32] Christopher William White. A corpus-sensitive algo-\nrithm for automated tonal analysis. In Mathematics and\nComputation in Music , pages 115–121. Springer, 2015.\n[33] Christopher William White and Ian Quinn. The yale-\nclassical archives corpus. Empirical Musicology Re-\nview, 11(1), 2016.\n[34] Mu-Heng Yang, Li Su, and Yi-Hsuan Yang. Highlight-\ning root notes in chord recognition using cepstral fea-\ntures and multi-task learning. In Proceedings of the\nAsia-Paciﬁc Signal and Information Processing Asso-\nciation Annual Summit and Conference (APSIPA ASC) ,\npages 1–8. IEEE, 2016.\n[35] Xinquan Zhou and Alexander Lerch. Chord detection\nusing deep learning. In Proceedings of the 16th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR) , pages 52–58, 2015.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 97"
    },
    {
        "title": "A Flexible Approach to Automated Harmonic Analysis: Multiple Annotations of Chorales by Bach and Prætorius.",
        "author": [
            "Nathaniel Condit-Schultz",
            "Yaolong Ju",
            "Ichiro Fujinaga"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492345",
        "url": "https://doi.org/10.5281/zenodo.1492345",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/283_Paper.pdf",
        "abstract": "Despite being a core component of Western music theory, harmonic analysis remains a subjective endeavor, resistant automation. This subjectivity arises from disagreements regarding, among other things, the interpretation of contrapuntal figures, the set of \"legal\" harmonies, and how harmony relates to more abstract features like tonal function. In this paper, we provide a formal specification of harmonic analysis. We then present a novel approach to computational harmonic analysis: rather than computing harmonic analyses based on one specific set of rules, we compute all possible analyses which satisfy only basic, uncontroversial constraints. These myriad interpretations can later be filtered to extract preferred analyses; for instance, to forbid 7th chords or to prefer analyses with fewer non-chord tones. We apply this approach to two concrete musical datasets: existing encodings of 371 chorales by J.S. Bach and new encodings of 200 chorales by M. Prætorius. Through an online API users can filter and download numerous harmonic interpretations of these 571 chorales. This dataset will serve as a useful resource in the study of harmonic/functional progression, voice-leading, and the relationship between melody and harmony, and as a stepping stone towards automated harmonic analysis of more complex music.",
        "zenodo_id": 1492345,
        "dblp_key": "conf/ismir/Condit-SchultzJ18",
        "keywords": [
            "harmonic analysis",
            "subjectivity",
            "contrapuntal figures",
            "legal harmonies",
            "tonal function",
            "formal specification",
            "novel approach",
            "computational harmonic analysis",
            "basic constraints",
            "filtering analyses"
        ],
        "content": "A FLEXIBLE APPROACH TO AUTOMATED HARMONIC ANALYSIS:\nMULTIPLE ANNOTATIONS OF CHORALES BY BACH AND PRÆTORIUS\nNathaniel Condit-Schultz\nMcGill University\nnathaniel.condit-schultz@mcgill.caYaolong Ju\nMcGill University\nyaolong.ju@mail.mcgill.caIchiro Fujinaga\nMcGill University\nichiro.fujinaga@mcgill.ca\nABSTRACT\nDespite being a core component of Western music theory,\nharmonic analysis remains a subjective endeavor, resistant\nautomation. This subjectivity arises from disagreements\nregarding, among other things, the interpretation of con-\ntrapuntal ﬁgures, the set of “legal” harmonies, and how\nharmony relates to more abstract features like tonal func-\ntion. In this paper, we provide a formal speciﬁcation of\nharmonic analysis. We then present a novel approach to\ncomputational harmonic analysis: rather than computing\nharmonic analyses based on one speciﬁc set of rules, we\ncompute all possible analyses which satisfy only basic,\nuncontroversial constraints. These myriad interpretations\ncan later be ﬁltered to extract preferred analyses; for in-\nstance, to forbid 7th chords or to prefer analyses with fewer\nnon-chord tones. We apply this approach to two concrete\nmusical datasets: existing encodings of 371 chorales by\nJ.S. Bach and new encodings of 200 chorales by M. Præto-\nrius. Through an online API users can ﬁlter and download\nnumerous harmonic interpretations of these 571 chorales.\nThis dataset will serve as a useful resource in the study\nof harmonic/functional progression, voice-leading, and the\nrelationship between melody and harmony, and as a step-\nping stone towards automated harmonic analysis of more\ncomplex music.\n1. INTRODUCTION\nBroadly, harmony refers to the simultaneous sounding of\nmultiple pitches [22]. However, harmonic theory involves\nfar more than just pitch collections. Rather, harmonic the-\nory describes an abstract syntactic structure in Western\ntonal music, hierarchically removed from the literal pitches\nof the musical “surface” [22]. Though harmonic theory is a\nfoundational component of basic music theory, the details\nof the theory are vague, and deceptively complex [5]. Har-\nmony intertwines low-level sensory distinctions (conso-\nnance vs dissonance), short-term musical constructs (coun-\nterpoint, voice-leading), and abstract long-range musical\nc\rNathaniel Condit-Schultz, Yaolong Ju, Ichiro Fujinaga.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Nathaniel Condit-Schultz, Yaolong\nJu, Ichiro Fujinaga. “A Flexible Approach to Automated Harmonic Anal-\nysis: Multiple Annotations of Chorales by Bach and Prætorius”, 19th\nInternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.structures (function, form, tonality, etc.), and thus plays a\ncentral role in musical experience. Given this complexity,\nit is no surprise that actual harmonic analysis is highly sub-\njective, and thwarts any attempt to systematize or automate\nit. This paper attempts to clarify the dimensions of har-\nmonic analysis, identifying the import points of disagree-\nment and ambiguity in harmonic theory. We then present\na novel approach to automated harmonic analysis, which\nallows us to generate a variety of consistent harmonic an-\nnotations based on a various assumptions and preferences.\n1.1 Theory and Terminology\nTo avoid confusion with the more general concept of “har-\nmony,” we use the term sonority to refer to pitch-class\ncollections. The most basic sonority is the dyad —pairs\nof pitch classes which form consonant ordissonant in-\ntervals.1Larger sonorities can be seen as combinato-\nrial compositions of dyads, as each new pitch class forms\nan interval with every other pitch class in the sonority.\nHarmonic theory generalizes about various dyad combi-\nnations, reducing a huge variety of possible interval com-\nbinations to a few categories. The central harmonic cat-\negories of Western music are the set of cardinal-three\nsonorities in which all intervals are consonant ( triads ) and\nthe cardinal-four sonorities which include one dissonant\ninterval ( 7th chords ). Other sonorities—the preponder-\nance of possibilities—are unclassiﬁed and considered non-\nsyntactic. Some genres (e.g., jazz, music theatre) employ\nlarger sonorities (9th chords, 13th chords, etc.), which nec-\nessarily contain more dissonant intervals, as well as dis-\nsonant cardinal-three and cardinal-four sonorities (sus4,\nadd9, etc.) [5], but even in these styles the vast majority\nof sonorities are unclassiﬁed.\nTraditionally, dissonant harmonic intervals are only\nused in highly constrained melodic settings: Dissonant\nnotes must “decorate” a neighboring consonant note, typ-\nically by moving to/from the consonance by step—a dis-\nsonance moving to a consonance by step is said to resolve\nto the consonance. Thus, a basic hierarchical distinction\nis introduced into music, as “decorative” dissonances are\nnecessarily subservient to “structural” consonances. Tradi-\ntional theory and pedagogy approaches larger musical tex-\ntures by applying two-part concepts (parallelism, motion\n1Here we only consider generic intervals, and thus generic disso-\nnances. Generically, thirds, ﬁfths, and sixths are consonant, though some\nspeciﬁc versions of these intervals (e.g., diminished ﬁfths, augmented\nthirds) are dissonant.66types, dissonance resolution) to all individual pairs. Un-\nfortunately, larger textures introduce complexities which\ntwo-voice theory cannot address: decorative tones may ap-\npear in multiple voices at different times, at the same time,\nor even staggered such that one voice’s decorative disso-\nnance cooccurs with another’s consonant resolution. As\na result, the consonant harmonies which undergird mu-\nsical syntax may never be explicitly sounded as sonori-\nties. The concrete distinction between consonant and dis-\nsonant intervals gives way to a nebulous distinction be-\ntween chord-tones which instantiate the local harmony and\nnon-chord tones that decorate them [22]. This distinction,\nis the essential task of harmonic analysis. Traditional “ro-\nman numeral” harmonic analysis also requires some in-\nterpretation of higher-level tonal structures, including the\nglobal key and local modulations. Just as the melodic\nsurface elaborates the underlying harmonic progressions,\nharmonic progressions elaborate more abstract functional\n(tonic, subdominant, dominant) progressions and prolon-\ngations, which in turn articulate the key or progressions\nbetween keys. This hierarchy, however, is not clear-cut or\ndiscrete: disentangling surface features from increasingly\nabstract structural progressions is difﬁcult, and the proce-\ndure poorly deﬁned.\n1.2 Literature\nComputational research into harmonic progression and\nfunction has been extensive [8, 12, 22–24, 27, 30]. Many\nresearchers have sought to automate harmonic analysis, ei-\nther using rule-based algorithms [9, 11, 21, 28, 29] or ma-\nchine learning [13, 18, 19, 26]. Impressive performance\nhas been achieved, though proper evaluation is some-\nwhat difﬁcult given that the “correct answer” is not clear\ncut. Even if interpretive leeway is allowed, algorithms in-\nevitably struggle with even mildly idiosyncratic or excep-\ntional passages—devising sufﬁciently complicated rules to\ncover all possibilities is impossible, and such passages\nare too rare to be learned by machine learning. Due to\nthese difﬁculties, many researchers have relied instead on\nmanual annotation by experts, who can make more nu-\nanced decisions and adapt to never-before-seen situations\n[1,3,4,8,20]. However, though human analysts may create\nmore accurate data, manual harmonic annotations—even\nby the same analyst—can be extremely inconsistent [14].\nGiven the subjectivity of harmonic analysis, the consis-\ntency of data annotation may actually more important than\na vaguely-deﬁned “accuracy” [6]—inconsistent answers to\nsimilar or identical musical patterns will inevitably hamper\nlearning, whether human or machine.\nTo account for inconsistency and disagreement between\ntheorists, many studies have employed multiple indepen-\ndent annotators [3, 4]. This approach is appropriate to the\nextent that analytical inconsistency is considered random\nnoise. However, as we will explain, harmonic indetermi-\nnacy is not simply a matter of random error, but rather\nreﬂects fundamental disagreements concerning the nature,\nmeaning, and purpose of harmonic analysis. Thus, anno-\ntation error is not (entirely) stochastic, but rather, is sys-tematic. What’s more, though multiple independent anno-\ntations give us some sense of the scope of disagreement\nbetween analysts, they do little to clarify the root causes of\nthese disagreements. Our view is that is preferable to: A)\nprecisely describe the subjective features of harmonic the-\nory; B) study how different theoretical assumptions result\nin different analyses; and C) evaluate how well different\nassumptions/models explain patterns in music. The goal\nof our project is to facilitate these tasks.\n1.3 Analytical ambiguity\nHarmonic analysis is evidently a useful tool in the descrip-\ntion of musical structure and musical experience, yet in\npractice, harmonic theory is underspeciﬁed with regards to\nmany musical passages. Indeed, many prominent theories\nof music (e.g. Rameau, Riemann, Schenker) differ funda-\nmentally in their approach to harmony. It is often possible\nto interpret the same passage in a number of ways. Further-\nmore, the informative distinctions conveyed by different\ninterpretations is unclear. This ambiguity mainly regards\nfour questions:\n1. Which harmonies are “legal” structural harmonies?\nAre sevenths chords true harmonies, or are they al-\nways decorative?\n2. How do we interpret sonorities that are subsets,\nsupersets, or intersections of each other? Tradi-\ntional harmonic categories like fV;V7;vii ogboth\nshare many pitch classes and have similar musical\nfunction—what, if any, useful information is con-\nveyed by treating them as independent categories?\n3. How do we interpret contrapuntally decorative notes\nwhich are consonant—i.e, can there be consonant\nnon-chord tones? This issue is especially difﬁcult\nwhen multiple voices engage in decorative motion\nat once, creating “passing chords.”\n4. Should harmonic analysis reﬂect only “surface” fea-\ntures (like dissonance resolutions), or should higher-\nlevel structures also play a role? For instance,\nshould, large-scale parallelism inform analyses?—\ni.e., analyzing two parallel passages in a similar way\neven if their surface details differ?\nFigure 1 illustrates a number of these issues in a con-\ncrete musical example. In Figure 1, the three notes col-\nored red form dissonances and therefore must be inter-\npreted as non-chord tones. Notes colored blue are conso-\nnant, but evince melodic contours similar to the dissonant\nnotes. Throughout this paper, we refer to each new sonor-\nity formed whenever any voice articulates a new onset as\na sonority “slice”—in Figure 1, slices are numbered above\nthe grand staff.\nThe consonant passing tones in slices 2 and 8 are espe-\ncially illustrative. If the passing tone in slice 2 is consid-\nered a chord tone, slices 1–2 form the harmonies I!vi6,\nboth tonic function chords. If the passing tone in slice 8 is\ninterpreted as a chord tone, the progression ii!vii6\no\nresults—a transition between two different tonal functionsProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 67(subdominant and dominant). Given these functional dif-\nferences, many analysts would mark slices 1–2 as a single\nIchord but slices 7–8 as ii!vii6\no. This is especially\ntrue since transitions from ii!I(slice 9) are considered\nabnormal, while transitions from vii6\no!Iare norma-\ntive. Several slices illustrate the ambiguity regarding 7th\nchords: Passing tones in slices 6, 18, 20, 22 and 24 might\neach be interpreted as sevenths, or not. For instance, the\nGin slice 11 can be seen as the 7th of a ii6harmony, or\nas a suspension. In Bach’s chorale music, chordal 7ths are\nnearly always treated like dissonances, begging the ques-\ntion: what is the difference between a “chord-tone 7th” and\na “non-chord-tone 7th”?\n2. CURRENT PROJECT\nThis paper describes a new approach to automated har-\nmonic analysis, which remains agnostic regarding many\nof the speciﬁc interpretive complexities discussed so far.\nRather, we base analyses on only a few, basic, uncon-\ntroversial constraints, allowing us to produce numerous\ninterpretations of the same sonorities. Using this ap-\nproach, we have generated a novel form of harmonic anal-\nysis dataset, including numerous harmonic annotations of\nchorales by Michael Prætorius (1571–1621) and Johann\nSebastian Bach (1685–1750). This dataset can serve sev-\neral useful functions:\n1. Researchers can generate speciﬁc, consistent har-\nmonic analyses, conforming to whatever analytical\npreferences/assumptions they prefer, for all music in\nthe corpora. These analyses can be used like any\nother harmonic annotation data—i.e., to study har-\nmonic progression and tonality in general.\n2. The dataset includes a set of late-modal (Prætorius)\nand early-tonal (Bach) music, which are nonetheless\nlargely similar in texture and style. This makes the\ndataset particularly useful for historical research [8].\n3. Finally, by comparing analyses generated with dif-\nferent constraints, we can rigorously explore the\nways in which different harmonic theories ﬁt, or\ndon’t ﬁt, real music.\nChorale music is invaluable for teaching and studying\nharmony, as it features consistent and highly constrained\nmelodic/contrapuntal textures, with few non-chord tones.\nBach’s 371 chorales are mainstays of music theory peda-\ngogy and have been the subject of much music informa-\ntion retrieval research [2, 7, 8, 16, 22–24, 27, 31]. Præto-\nrius’ 200 chorales are music of a somewhat similar texture,\nbut have received relatively little attention. Several sets of\nexpert analyses of Bach’s chorales have been published,\nthough only subsets of the chorales have annotations digi-\ntally aligned with symbolic music data. Other researchers\nhave generated harmonic annotations—or analogous func-\ntional analyses—of the chorales computationally, and used\nthese analyses in research, but have not published their an-\nnotations, nor describe them in detail.3. METHODOLOGY\nThe approach of this project is to calculate all legal har-\nmonic interpretations of a passage, and to only ﬁlter out\nspeciﬁc interpretations at a later stage. Our approach is de-\nsigned speciﬁcally for our dataset, and is thus rather “over\nﬁt” to chorale music, so it will not generalize well to other\nmusic. However, the basic concepts of our approach could\nbe adapted to other tonal music.\nKey to our entire endeavor is establishing “basic” con-\nstraints on harmonic interpretation. In true music the-\nory form, we formulate these constraints as the following\n“rules.” There are two types of rules: harmonic rules and\nmelodic rules. Our harmonic rules are as follows:\n1. Every sonority slice belongs to one and only one har-\nmony.\n2. Every new harmony must be followed by another\nnew harmony on the next stronger metric position—\ni.e., harmonic rhythm cannot be syncopated. (Some\nPrætorius chorales contain exceptions to this rule, as\nthe entire rhythmic texture is syncopated.)\n3. Only triads (major, minor, diminished, or aug-\nmented) and 7th chords (dominant, major, minor,\nhalf-diminished, or fully-diminished) are consid-\nered legal harmonies. However, subsets of legal\nharmonies may also appear in music. Complete\nharmonies are preferred, but cardinal-three subsets\nof seventh chords (Root-3rd-7th or Root-5th-7th),\ndyadic subsets of triads (i.e., consonant intervals),\nand even unisons/octaves are permitted.\nGiven these deﬁnitions of harmony, we can then estab-\nlish which notes do, or do not, belong to the local harmony.\nTo be a non-chord tone, a note must satisfy the following\nmelodic rules—any note that fails any of these rules must\nbe a chord tone:\n1. The antecedent and consequent note of each non-\nchord tone must be consonant (chord tones), except-\ning the special case of Rule 4g (below).\n2. Non-chord tones cannot sustain across metric posi-\ntions that are stronger than their own metric position.\n3. Non-chord tones cannot sustain through changes of\nharmony. A note cannot start as a non-chord tone\nand then become a chord tone (though the opposite\nis possible, in the suspension).\n4. Finally, all non-chord tones must match one of these\ntraditional contrapuntal dissonance models:\n(a)Passing tone : approached and departed by step\nin the same direction.\n(b)Neighbor tone : approached and departed by\nsteps in opposite directions; the antecedent and\nconsequent are the same note.\n(c)Suspension/Retardation : approached by uni-\nson (or sustain); departed by step; stronger\nmetric position than antecedent.68 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 1 . Illustration of “decorative” melodic idioms in a contrived example of four-part counterpoint. Slices (sonorities)\nare numbered above the staff. Notes colored red indicate dissonances. Notes colored blue indicate consonant notes which\nnonetheless articulate decorative melodic idioms, including passing tones (slices 2, 5, 8, 16, 18, 20, 21, 23, 24), neighbor\ntones (slices 6, 17, 18, 19), suspensions (slices 11, 23), a retardation (slice 15), and an anticipation (slice 22). Some of\nthese interpretations are mutually exclusive, as a decorative tone cannot decorate another decorative tone. For instance, if\ntheCin slice 5 is considered a passing tone, then the Bin slice 6 must be a chord tone which resolves the passing tone.\n(d)Appoggiatura : approached by leap; departed\nby step in opposite direction; stronger metric\nposition that antecedent.\n(e)Escape tone : approached by step; left by skip;\nweaker metric position than its antecedent.\n(f)Pedal tone : approached by unison (or sustain);\nleft by unison (or sustain).\n(g)Double passing : two non-chord tones of the\nsame duration, separated by step; approached\nand departed by step in the same direction; the\nﬁrst of the pair must occupy a weaker beat than\nits antecedent.\nAs in all dimensions of harmonic analysis, there is\nnot universal agreement regarding the rules for non-chord\ntones. The rules set out here are an amalgam of the rules\nexplicitly, or implicitly, described in typical music theory\ntext books [15, 17], specialized (through some trial an er-\nror) for our chorale datasets.\n3.1 Data parsing\nSymbolic encodings of the Bach chorales were gathered\nfrom the KernScores repository ( kern.ccarh.org ),\nwhich is maintained by Stanford’s Center for Com-\nputer Assisted Research in the Humanities. The mu-\nsic of 370 four-part chorales, and one ﬁve-part chorale2,\nis encoded in the humdrum **kern representation\n(www.humdrum.org ) [10]. Symbolic encodings of 200\nchorales by Prætorius were recently digitized by members\n2This ﬁve-part chorale was excluded from the dataset available on\nKernscores, but was encoded for the purposes of this studyof McGill University SIMSSA project: Scores were ini-\ntially scanned and interpreted by optical music recogni-\ntion software before being corrected by a human annota-\ntor. This data was originally encoded in musicXML for-\nmat, but was converted to **kern data for this project,\nso as to facilitate alignment with harmonic annotations.\nThe Prætorius data includes 197 four-voice chorales and\nthree ﬁve-voice chorales. In total, the dataset includes 571\nchorales, consisting of 129,568 notes ( +898 rests), which\nform 42,895 sonority slices.\n**kern data was parsed using the Humdrum Toolkit\n[10], before being loaded into R[25] for additional pars-\ning. The analysis workﬂow was also conducted in R. To\nmake the analyses useful as comparisons across the two\ncomposers, (almost) the exact same parsing and analysis\nworkﬂow are applied to each.\nIn addition to pitch and rhythm data, the Bach chorale\ndata contains some phrasing information, in particular, fer-\nmatas. A phrase ending in a Bach chorale was identiﬁed\nwhenever all four voices reach a fermata.3The Præto-\nrius chorale data contains phrasing information, encoded\nas rests in all voices, and both datasets contain metric in-\nformation.4\n3.2 Workﬂow\nOur process has a two-stage workﬂow. The ﬁrst-stage is\nto divide the music exhaustively into contiguous groups\n3Several chorales had notational inconsistencies, wherein fermatas\nwere not encoded on the inner voices. These inconsistencies were ﬁxed\nmanually.\n4Though metric indications in Prætorius’ era are not exactly concep-\ntually equivalent to modern time signatures.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 69Figure 2 . Illustration of contextual windows in Bach’s Chorale 1, Aus meines Herzens Grunde . Slices between dashed red\nlines are analyzed as one window.\nof successive slices: “contextual windows.” The second-\nstage applies an analysis algorithm to each segment.\n3.2.1 Stage 1\nMany sonority slices can be analyzed in isolation. How-\never, many more slices need context to by analyzed. Our\napproach is to parse the music into a single set of contigu-\nous (non-overlapping) windows, identiﬁed using a sim-\nple, rule-based heuristic. A new contextual window begins\nanytime:\n1. All voices attack on a strong beat.\n2. All voices attack and one or more voices did not at-\ntack in the previous slice.\n3. In an offbeat slice, more than two voices attack and\none or more voices sustains into/past the next beat.\n4. After a phrase boundary.\nFigure 2 illustrates the contextual windows derived by this\nheuristic in the ﬁrst seven measures of Bach’s ﬁrst chorale.\nThe aim of this heuristic is to err on the side of larger seg-\nments: unnecessarily large windows can be broken down\ninto separate harmonies at a later stage, but windows that\nare too small will not provide enough context to identify\nall legal interpretations, and in some cases may result in\nwindows that are not parsable.\n3.2.2 Stage 2\nOnce analytical windows are identiﬁed, we apply the fol-\nlowing permutational algorithm to the slices in each win-\ndow.\n1. Identify all ways in which the window can be di-\nvided exhaustively into sub-segments while obeying\nharmonic-rhythm constraints (Harmonic Rule 2).\n2. For each possible segmentation, identify all pitches\nthat can legally be non-chord tones (Melodic Rules\n3–4)—we call these potential non-chord tones.\n3. Compute every combination of potential non-chord\ntones, allowing that some interpretations are mutu-\nally exclusive (detailed explanation below).\n4. For every legal combination of potential non-chord\ntones, remove these non-chord tones and groupthe remaining chord tones into every possible sub-\nsegment.\n5. Discard interpretations which contain (any) illegal\nharmonies.\n6. If any preferred harmonies are present, discard in-\ncomplete harmonies (Harmonic Rule 3).\n7. If the same chord is identiﬁed in two successive\nslices, discard this interpretation (a different sub-\nsegmentation is sure to have found the equivalent).\n8. If a slice is identiﬁed as a dyad/unison, and the pre-\nceding or succeeding slice is a superset of that dyad/\nunison, the slice is subsumed into the superset.\nFigure 3 illustrates the application of this algorithm to\nthe sixth window in the chorale shown in Figure 2. The\nfour slices in this window can be legally divided in six\nsegmentations (Step 1), shown below the staff. Eleven of\nthe twelve notes in the window are potential non-chord\ntones (labeled and enumerated in Figure 3). The algo-\nrithm tests various permutations of these potential non-\nchord tones (algorithm Steps 3–5) as so: First, assume\npotential non-chord #1 isa non-chord tone and all other\nnotes are chord tones: under this assumption, segmentation\n1... forms the illegal sonority fA,B,C,D,E,F#g; seg-\nmentation 1.2. forms the illegal sonorities fB,C,D,Eg\nandfA,B,C,E,F#g; segmentation 1..2 forms the ille-\ngal sonorityfB,C,D,Egand the legal sonority fA,C,F#g;\netc. Repeat this procedure for every other potential non-\nchord tone, every pair of non-chord tones, every triplet\nof non-chord tones, etc., skipping combinations which are\nmutually exclusive—i.e., if #2 is an appoggiatura #4 must\nbe a chord tone (Rule 1). Testing all non-chord tone per-\nmutations across all six segmentations reveals eleven non-\nredundant (Steps 6–8) interpretations with legal chords in\nall segments (Step 5).5Of these eleven, we can “ﬁlter\nout” interpretations involving 7th chords, leaving the three\ntriadic analyses shown in Figure 3.\n5Our actual algorithm incorporates a few additional optimizations to\nlimit the number of permutations which must be tested. The most impor-\ntant involves pitch classes: within a given harmonic segment all instances\nof a single pitch class must be either non-chord tones or chord tones. For\ninstance, it would be meaningless to treat #9 as a passing tone but treat\n#11 as a chord tone. Similarly, #7 (a C) can never actually be a passing\ntone, since the Cin the bass is always a chord tone.70 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 3 . Illustration of the permutational analysis of a\nsingle contextual window (window 6 from Figure 2). Each\nnote in the window is annotated as a potential non-chord\ntone, marked pfor passing tone, nfor neighbor tone, rfor\nretardation, or afor appoggiatura—mutually exclusive po-\ntentials are annotated with arrows. The single unlabeled C\nmust be a chord tone, as it does not match any contrapun-\ntal dissonance model (Melodic Rules 4). Below the staff,\nthe six possible rhythmic segmentations of the window are\nshown. The four possible purely-triadic interpretations of\nthe window are show; the notes which are interpreted as\nnon-chord tones are identiﬁed (by number) beside each\nanalysis.3.3 Edge cases\nChorale music is valued pedagogically for its simplicity\nand consistency. Nonetheless, a handful of chorales con-\ntain unusual features which complicate the batch analysis\nof the corpora. Notable examples in the Bach chorales in-\nclude: an unusual call and response between the soprano\nand the rest of the voices in Chorale 43; dissonant notes\nwhich resolve across phrase boundaries (i.e., through a\nfermata) in Chorales 127, 202, and 234; and suspensions\nwhich resolve indirectly in Chorales 5 and 199. A number\nof Prætorius chorales also contain subsections in which a\nsubset of voices sing while the others rest, confounding our\nwindowing heuristic. Solutions to these special cases, and\na handful others, were hard-coded into the workﬂow.\n4. API\nThe data is hosted at github.com/DDMAL/\nFlexible harmonic chorale annotations .\nThe harmonic permutation data is stored in a rData ﬁle.\nUsers may ﬁlter out speciﬁc harmonic analyses using an\nonline GUI, and download them as a zipped collection\nof text ﬁles encoded in the Humdrum Syntax. Each ﬁle\ncontains the **kern representation of a chorale aligned\nwith one or more harmonic analyses in a **harm repre-\nsentation. Interpretations can be ﬁltered by the following\ncriteria:\n\u000fType of harmonies.\n\u000fNumber of harmonies (per beat/per window).\n\u000fTypes of non-chord tones.\n\u000fNumber of non-chord tones (per slice/per window).\nFor example, one could extract analyses which forbid aug-\nmented triads, appoggiaturas, and \u0007 \u0010)harmonic rhythms.\nUsers may also download the raw data and associated R\nscripts for local use or customization.\n5. CONCLUSION\nThe empirical and computational study of harmony is es-\nsential to furthering our understanding of musical structure\nand perception. However, this research must remain cog-\nnisant of the subtle complexities and controversies of har-\nmonic theory if it is to be fruitful. We have presented a\nnovel approach to automated harmonic analysis which is\nnot limited to one speciﬁc set of theoretical assumptions,\nallowing for just such subtleties to be explored systemati-\ncally. We have also described a new dataset generated via\nthis method. We hope that this dataset will facilitate re-\nsearch into tonality and harmonic progression, especially\nchanges in harmonic practice between the early 1600s and\nthe mid 1700s. However, our grander purpose is to facili-\ntate critical, data-driven, interrogation of harmonic theory\nin general.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 716. REFERENCES\n[1] John Ashley Burgoyne, John Wild, and Ichiro Fuji-\nnaga. An Expert Ground Trught Set for Audio Chord\nRecognition and Music Analysis. In A Klapuri and\nC. Leider, editors, Proceedings of the 12th Interna-\ntional Society for Music Information Retrieval (ISMIR)\nConference , pages 633–638, Miami, FL, 2011.\n[2] Trevor de Clercq. A Model for Scale-Degree Rein-\nterpretation: Melodic Structure, Modulation, and Ca-\ndence Choice in the Chorale Harmonizations of J. S.\nBach. Empirical Musicology Review , 10:188–206, 05\n2015.\n[3] Trevor de Clercq and David Temperley. A Cor-\npus Analysis of Rock Harmony. Popular Music ,\n30(01):47–70, January 2011.\n[4] Johanna Devaney, Claire Arthur, Nathaniel Condit-\nSchultz, and Kirsten Nisula. Theme and Variation En-\ncodings with Roman Numerals: A New Data Set\nfor Symbolic Music Analysis. In Meinard M ¨uller and\nFrans Wiering, editors, Proceedings of the 16th In-\nternational Society for Music Information Retrieval\n(ISMIR) Conference , pages 728–734, Malaga, Spain,\n2015.\n[5] Christopher Doll. Deﬁnitions of ‘Chord’ in the Teach-\ning of Tonal Harmony. Dutch Journal of Music Theory ,\n18(2):91–106, 2013.\n[6] Mark Granroth-Wilding and Mark Steedman. A Robust\nParser-Interpreter for Jazz Chord Sequences. Journal\nof New Music Research , 43(4):355–374, 2014.\n[7] Ga ¨etan Hadjeres and Franc ¸ois Pachet. DeepBach: a\nSteerable Model for Bach Chorales Generation. CoRR ,\n2016.\n[8] Thomas Hedges and Martin Rohrmeier. Exploring\nRameau and Beyond: A Corpus Study of Root Pro-\ngression Theories. In Mathematics and Computation\nin Music: Third International Conference, MCM 2011,\nParis, France, June 15-17, 2011. Proceedings , pages\n334–337, Berlin, Heidelberg, 2011. Springer Berlin\nHeidelberg.\n[9] Tim Hoffman and William P Birmingham. A Con-\nstraint Satisfaction Approach to Tonal Harmonic Anal-\nysis. Electrical Engineering and Computer Science\nDepartment. Technical Report CSE-TR-397-99. Uni-\nversity of Michigan , 2000.\n[10] David Huron. Music Research Using Humdrum: A\nUser’s Guide . Stanford, California: Center for Com-\nputer Assisted Research in the Humanities, 1999.\n[11] Pl ´acido R. Illescas, David Rizo, and Jos ´e M. I ˜nesta.\nHarmonic, Melodic, and Functional Automatic Analy-\nsis. In Proceedings of the International Computer Mu-\nsic Conference , pages 165–168, 2007.[12] Nori Jacoby, Naftali Tishby, and Dmitri Tymoczko. An\nInformation Theoretic Approach to Chord Categoriza-\ntion and Functional Harmony. Journal of New Music\nResearch , 44(3):219–244, 2015.\n[13] Yaolong Ju, Nathaniel Condit-Schultz, and Ichiro Fuji-\nnaga. Non-chord Tone Identiﬁcation Using Deep Neu-\nral Networks. In Proceedings of the Fourth Interna-\ntional Workshop on Digital Libraries for Musicology ,\npages 13–16. ACM, 2017.\n[14] Hendrik Vincent Koops, W. Bas de Hass, John Ashley\nBurgoyne, and Jeroen Bransen. Harmonic Subjectiv-\nity in Popular Music. Technical report, Department of\nInformation and Computing Sciences, Utrecht Univer-\nsity, 2017.\n[15] Stefan Kostka and Dorothy Payne. Tonal Harmony:\nWith an Introduction to Twentieth-Century Music . Mc-\nGraw Hill, 2004.\n[16] Pedro Kr ¨oger, Alexandre Passos, Marcos Sampaio, and\nGivaldo De Cidra. Rameau: a System for Automatic\nHarmonic Analysis. In Proceedings of International\nComputer Music Conference , pages 273–281, 2008.\n[17] Steven G. Laitz. The Complete Musician: an Inte-\ngrated Approach to Tonal Theory, Analysis, and Lis-\ntening . Oxford University Press, 3rd edition, 2012.\n[18] Kristen Masada and Razvan Bunescu. Chord Recog-\nnition in Symbolic Music Using Semi-Markov Condi-\ntional Random Fields. In Proceedings of the 18th Inter-\nnational Society for Music Information Retrieval Con-\nference , pages 272–278, 2017.\n[19] Lesley Mearns. The Computational Analysis of Har-\nmony in Western Art Music. PhD thesis, Queen Mary\nUniversity of London, 2013.\n[20] N ´estor N ´apoles L ´opez. Automatic Harmonic Analy-\nsis of Classical String Quartets from Symbolic Score.\nMaster’s thesis, Universitat Pompeu Fabra, 2017.\n[21] Brian Pardo and William P. Birmingham. The Chordal\nAnalysis of Tonal Music. In The University of Michi-\ngan, Department of Electrical Engineering and Com-\nputer Science Technical Report CSE-TR-439-01 , 2001.\n[22] Ian Quinn. Are Pitch-Class Proﬁles Really Key for\nKey? Gesellschaft f ¨ur Musiktheorie , 7(2):151–163,\n2010.\n[23] Ian Quinn and Panayotis Mavromatis. V oice-Leading\nPrototypes and Harmonic Function in Two Chorale\nCorpora. In Carlos Agon, Moreno Andreatta, G ´erard\nAssayag, Emmanuel Amiot, Jean Bresson, and John\nMandereau, editors, Mathematics and Computation\nin Music , pages 230–240, Berlin, Heidelberg, 2011.\nSpringer Berlin Heidelberg.72 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[24] Ian Quinn and Christopher Wm. White. Expanding No-\ntions of Harmonic Function Through a Corpus Analy-\nsis of the Bach Chorales. In Annual Conference of the\nSociety for Music Theory , Charlotte, NC, 2013.\n[25] R Core Team. R: A Language and Environment for Sta-\ntistical Computing . R Foundation for Statistical Com-\nputing, Vienna, Austria, 2013.\n[26] Christopher Raphael and Joshua Stoddard. Functional\nHarmonic Analysis Using Probabilistic Models. Com-\nputer Music Journal , 28(3):45–52, 2004.\n[27] Martin Rohrmeier and Ian Cross. Statistical Properties\nof Tonal Harmony in Bach’s Chorales. In Proceedings\nof the 10th International Conference on Music Percep-\ntion and Cognition , 2008.\n[28] Craig Stuart Sapp. Computational Chord-Root Identiﬁ-\ncation in Symbolic Musical Data: Rationale, Methods,\nand Applications. Computing in Musicology , 15:99–\n119, 2007.\n[29] David Temperley and Daniel Sleator. Modeling Meter\nand Harmony: A Preference-rule Approach. Computer\nMusic Journal , 23(1):10–27, 1999.\n[30] Christopher Wm. White. Changing Styles, Chang-\ning Corpora, Changing Models. Music Perception ,\n31(3):244–253, 2014.\n[31] Matthew Woolhouse. Probability and Style in the\nChorales of J. S. Bach. Empirical Musicology Review ,\n10(3):207, 2015.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 73"
    },
    {
        "title": "Searching Page-Images of Early Music Scanned with OMR: A Scalable Solution Using Minimal Absent Words.",
        "author": [
            "Tim Crawford",
            "Golnaz Badkobeh",
            "David Lewis 0003"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492391",
        "url": "https://doi.org/10.5281/zenodo.1492391",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/210_Paper.pdf",
        "abstract": "We define three retrieval tasks requiring efficient search of the musical content of a collection of ~32k pageimages of 16th-century music to find: duplicates; pages with the same musical content; pages of related music.  The images are subjected to Optical Music Recognition (OMR), introducing inevitable errors. We encode pages as strings of diatonic pitch intervals, ignoring rests, to reduce the effect of such errors. We extract indices comprising lists of two kinds of 'word'. Approximate matching is done by counting the number of common words between a query page and those in the collection.  The two word-types are (a) normal ngrams and (b) minimal absent words (MAWs). The latter have three important properties for our purpose: they can be built and searched in linear time, the number of MAWs generated tends to be smaller, and they preserve the structure and order of the text, obviating the need for expensive sorting operations.  We show that retrieval performance of MAWs is comparable with ngrams, but with a marked speed improvement. We also show the effect of word length on retrieval. Our results suggest that an index of MAWs of mixed length provides a good method for these tasks which is scalable to larger collections.",
        "zenodo_id": 1492391,
        "dblp_key": "conf/ismir/CrawfordB018",
        "keywords": [
            "Optical Music Recognition",
            "Musical content search",
            "Duplicate detection",
            "Page image collection",
            "Diatonic pitch intervals",
            "OMR errors",
            "Normal ngrams",
            "Minimal absent words (MAWs)",
            "Approximate matching",
            "Linear time building"
        ],
        "content": "SEARCHING PAGE-IMAGES OF EARLY MUSIC SCANNED WITH OMR: A SCALABLE SOLUTION USING MINIMAL ABSENT WORDS Tim Crawford Golnaz Badkobeh David Lewis Goldsmiths, University of London t.crawford@gold.ac.uk Goldsmiths, University of London G.Badkobeh@gold.ac.uk Oxford eResearch Centre david.lewis@oerc.ox.ac.uk ABSTRACT We define three retrieval tasks requiring efficient search of the musical content of a collection of ~32k page-images of 16th-century music to find: duplicates; pages with the same musical content; pages of related music. The images are subjected to Optical Music Recognition (OMR), introducing inevitable errors. We encode pages as strings of diatonic pitch intervals, ignoring rests, to re-duce the effect of such errors. We extract indices com-prising lists of two kinds of ‘word’. Approximate match-ing is done by counting the number of common words between a query page and those in the collection. The two word-types are (a) normal ngrams and (b) minimal absent words (MAWs). The latter have three im-portant properties for our purpose: they can be built and searched in linear time, the number of MAWs generated tends to be smaller, and they preserve the structure and order of the text, obviating the need for expensive sorting operations. We show that retrieval performance of MAWs is com-parable with ngrams, but with a marked speed improve-ment. We also show the effect of word length on retriev-al. Our results suggest that an index of MAWs of mixed length provides a good method for these tasks which is scalable to larger collections.  1. INTRODUCTION The historical repertory of Western classical music is in-creasingly being made publicly available in the form of downloadable (or merely viewable) digital images; these represent pages of the manuscripts or printed books in which they are preserved, and are no different in this re-spect from other typical online library materials such as texts or maps.  Search facilities within the individual library systems are entirely text-based, usually making use of existing or specially commissioned catalogue data. In a few cases, special viewing interfaces are provided to enhance the user-experience, such as the parallel presentation of mul-tiple part-books on the web-site of the Bayerische Staats-bibliothek in Munich.1 However, the data markup neces-sary to achieve this has to be done by human experts, which is impractical in general for large collections. Musicologists need to be able to browse such collec-tions and to search for specific musical parallels within them; librarians need similar facilities for cataloguing purposes (e.g. to identify unknown or unattributed items). This in turn demands fast search methods of adequate ac-curacy as a first step in the research process to reduce the number of items needing to be examined more exhaust-ively. With very few exceptions, music libraries offer online images rather than encoded scores. Providing the latter involves transcription, which can either be done manually by experts, a time-consuming and expensive process, or automatically by OMR, which inevitably introduces er-rors of various kinds. As OMR techniques improve in fu-ture, these errors are likely to diminish, but highly unlike-ly to disappear altogether. For fast searching, we need to extract indexes from the OMR output which enable fast searching at high recall. This depends on the musical data extracted and encoded in the indexes being carefully selected to suit a given use-case. For efficient search of the indexes we can benefit from recent advances in string- and pattern-matching al-gorithms developed for use in bioinformatics for DNA and protein analysis. In this paper we focus on three musicologically-motivated user tasks given a corpus of digital images of 16th-century printed music: finding duplicate images within the collection (called dupl below); finding pages containing substantially the same music as in a query page (same); and identifying pages which have non-identical but related or closely relevant music content, such as in different sections or voice parts than the query (relv). We briefly review earlier work on musical corpus-building, content-based music searching and indexing in section 2. We describe our test collection, relevant as-pects of the OMR process and our music indexing strate-gy in section 3. In section 4, we describe the retrieval tasks and our search method in more detail and our exper-iments and their evaluation in sections 5 & 6. In section 7 we discuss some of the main findings leading to the pro-posals for further work in section 8.                                                              1. E.g.: https://stimmbuecher.digitale-sammlungen.de/view?id=bsb00086863 \n \n © Tim Crawford, Golnaz Badkobeh, David Lewis. Licensed under a Creative Commons Attribution 4.0 International Li-cense (CC BY 4.0). Attribution: Tim Crawford, Golnaz Badkobeh, David Lewis. “Searching Page-images of Early Music Scanned with OMR: A Scalable Solution Using Minimal Absent Words”, 19th Inter-national Society for Music Information Retrieval Conference, Paris, France, 2018. 233   2. PREVIOUS WORK Corpora of historical music For musicologists, the amount of historical material available online has exploded in recent years, in line with the general availability of data of all kinds, including au-dio and video files of performances. This does not mean that their requirements for study and analysis are yet ade-quately met. The sub-discipline of computational, or digi-tal, musicology tends to devote a great deal effort to data-preparation before the powerful tools of MIR, pattern-matching and statistical analysis can be brought to bear. This is because the majority of the data-resources consist of collections of digital images of the source material, rather than files of its musical content. Traditionally, scores, which attempt to represent the overall musical content of the original documents (which is often, as in the case of the music studied in this paper, distributed be-tween multiple part-books), have been made by human experts; this is inevitably a time-consuming and thus ex-pensive process. The translation (automatic or manual) of musical content from documents or their digital-image surrogates into machine-readable ‘texts’ is generally re-ferred to as music encoding. While digital tools such as score-editing programs have made this easier, by ena-bling export to standard formats such as MusicXML2 and MEI,3 the process is in general impractically slow for building large collections. However, there exist some significant and freely avail-able collections of encoded music, such as those main-tained by the Center for Computer Assisted Research in the Humanities at Stanford University,4 which present a wide range of classical music encoded in a number of formats. These encodings permit a variety of ways of searching the data for musical features which are offered by software packages such as Humdrum5 or Music21.6 The online offerings of many digital music libraries in classical music are aggregated in the International Music Score Library Project (IMSLP),7 adding curated metada-ta. The resulting meta-collection (almost nine million pages of music) has rapidly become more-or-less indis-pensable for performers, teachers and students. Searching within IMSLP for most users is done via metadata rather than musical content. An experimental interface for con-tent-based searching, the Peachnote Ngram Viewer,8 works on the output of commercial OMR software run over a large part of the collection; while this is subject to the significant amount of errors introduced in the OMR process, it powerfully demonstrates the potential of effi-cient search over a large collection. In the current work, just as in Peachnote, we are not immediately concerned with an abstract or generalized notion of musical similarity. Rather, we select an encod-ing that represents the musical feature we wish to match.                                                              2. http://www.musicxml.com 3. http://music-encoding.org 4. http://www.musedata.org and http://kern.ccarh.org 5. http://humdrum.ccarh.org 6. http://web.mit.edu/music21/ 7. http://imslp.org 8. http://www.peachnote.com Our aim is to reduce the search space to a manageable number of musical documents which can be compared or analyzed in more detail manually or by a specialized al-gorithm. For large collections this task can best be achieved by searching indexes rather than full encodings of each document. Where the musical features extracted from a document can be represented as some kind of ‘text’, there are many ways of generating useful indexes which can be searched far more quickly than full texts. These have been the sub-ject of information retrieval research for almost half a century, and provide the mechanisms enabling the almost instaneous search familiar to all who use today’s internet. Indexing methods for music – either symbolic or audio – have received less attention, but a number of viable methods have been proposed and/or have found use [1].  For most of the sixteenth and seventeenth centuries, almost all original material comes in the form of separate voice-parts rather than scores. For the purposes of re-trieval these can be treated as linear strings of characters depending on the encoding method. There is a vast litera-ture on string-matching, largely motivated by problems from bioinformatics. Some of the resulting, highly-efficient methods have been proposed for music retrieval. Music retrieval algorithms inspired by bioinformatics A very recent survey of MIR applications for algorithms developed in bioinformatics research is contained in [2], although this does not include the method adopted in this paper. The need for pairwise comparison of potentially extreme-ly long, strings representing the structure of molecules such as DNA or proteins, has been addressed by the de-velopment of algorithms such as FASTA [3] and its de-scendants, such as BLAST,9  which are in common use for DNA analysis. The latter algorithm has found musical uses in the audio [4] and symbolic [5] domains. BLAST has also found use in recent work on audio cover-song recognition in [6], where the major speedup in retrieval it brought was found to compensate for a slight degradation in retrieval accuracy. Most recently, [7] reports on the application to music of methods originally designed for bioinformatics. These include multiple sequence-alignment methods such as MAFFT [8]. The present work uses a method which is finding in-creasing acceptance within bioinformatics, but has not, as far as the authors are aware, previously been applied to music: minimal absent words (MAWs). Here we briefly introduce the concept. A word is an absent word of a sequence if it does not occur in the sequence. An absent word is minimal if all its proper factors occur in the sequence. Absent words are negative information about the sequence. These objects have been extensively studied in combinatorics on words and it is known that although the number of absent words of a sequence is exponential with respect to the size of the sequence, the number of minimal absent words is only linear with respect to the length of the sequence [9].                                                               9. Basic Local Alignment Search Tool; http://blast.ncbi.nlm.nih.gov/ 234 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018   Crochemore et al. in [10] presented a linear-time algo-rithm to compare two documents by considering all their minimal absent words, using a length-weighted index measure.   In recent years, the significance of minimal absent words has been studied in several biological studies. In [11] absent words for four human genomes were comput-ed, and it was shown that intra-species variations in min-imal absent words were lower than inter-species varia-tions.   Furthermore, minimal absent words have been exploit-ed for building phylogenies [12], for measuring dissimi-larities/similarities between bio-sequences [13] and for many other applications [14]. 3. TEST COLLECTION & OMR The collection consists of 31,721 page-images of 16th-century printed music, which have all been subjected to OMR. The music was scanned from archival microfilms, in which the several individual part-books for a given item (usually four but up to as many as 12), one for each voice, follow in sequence as preserved under their single shelfmark. In almost every case they show two facing pages in a single image; these were each separated by us into two single page-images. The collection has associated metadata which gives bibliographical information for each book, but not to the level of musical items; so, for example, while the general sequence of musical items in each book is listed, with ti-tles and original composer ascriptions, the locations of items in the part-books is not recorded. Thus, it is in gen-eral impossible to associate automatically a page image with the music on it. This provides the motivation for the present work, aimed at designing a finding aid for re-searchers or librarians wishing to identify similar or relat-ed music within the collection. The OMR tool we use is Aruspix, a program specifi-cally designed for early printed music.10 While this repre-sents the current state of the art for this repertory [15], recently reported work suggests that significant progress is possible in the near future [16]. However, it is unlikely that 100% accuracy in OMR will ever be consistently achieved for any repertory; for this reason we maintain that fast, error-robust search methods will always be in demand. Aruspix saves its recognized output as MEI (mensural)11 from which we can extract various kinds of musical sequence. (See Fig 1.) Typical errors made by OMR systems can be of dura-tion (wrong/missing time-signatures; wrong/missing note-values) and of pitch (wrong/missing clefs; wrong/missing key-signatures; wrong/missing acciden-tals). The vertical location of symbols such as note-heads on the staff is usually recognized securely; this corre-sponds to diatonic pitch. Changes of clef tend to com-pound this effect as pitches are affected over a span of notes (usually until the next line of music), so it is helpful to use relative pitches, i.e. intervals. We have found se-                                                             10. http://www.aruspix.net 11. http://music-encoding.org/schema/2.1.1/mei-Mensural.rng quences of diatonic intervals to be the most useful for our purposes. We generate a single diatonic-interval string for each page using a simple alphabetic code devised by RISM12 for rapid searching of musical incipits (See Figure 1). Letters in upper case represent ascending intervals, lower case descending; same note is indicated by a hyphen.[17]  A typical item, opening only:  MEI output from Aruspix (opening only, simplified): <clef line=\"3\" shape=\"C\" /> <mensur sign=\"C\" slash=\"1\" /> <note pname=\"e\" oct=\"4\" dur=\"brevis\" /> <note pname=\"d\" oct=\"4\" dur=\"semibrevis\" lig=\"recta\" /> <note pname=\"f\" oct=\"4\" dur=\"semibrevis\" /> <note pname=\"e\" oct=\"4\" dur=\"semibrevis\" /> <dot ploc=\"f\" oloc=\"4\" /> <note pname=\"d\" oct=\"4\" dur=\"semiminima\" /> <note pname=\"c\" oct=\"4\" dur=\"semiminima\" /> <note pname=\"d\" oct=\"4\" dur=\"minima\" /> <custos pname=\"c\" oct=\"4\" />  [Spurious:Note missing!] <note pname=\"e\" oct=\"3\" dur=\"minima\" /> <note pname=\"e\" oct=\"3\" dur=\"minima\" /> <note pname=\"e\" oct=\"4\" dur=\"minima\" />  (NB Because the clef has been mis-recognized, all pitches are a third too low; also, in line 11, a note has been mis-read as a custos.)    Figure 1. The (erroneous) MEI output from Aruspix, and the correct encoding, for a typical item (opening only),13 and the sequences we derive from it. 4. TASKS AND METHOD The three tasks we approach are to recognise page-images which: (a) are duplicates (i.e. different shots/scans of the same page); (b) contain substantially the same mu-sic (which may be distributed differently across adjacent pages); (c) contain related but not identical music (this may be from a different voice-part, from a different sec-tion of the same piece, or from a derivative work). Task (a) involves finding near-identical matches; howev-er, the OMR output, and hence the indexes we extract, are not necessarily exactly the same, owing to recognition errors or small differences in photographic conditions, etc. For task (b), although in principle the encodings on which we base our searches should be largely identical, we cannot be sure that each page of different editions of a pieces of music has exactly the same content; often, the page layout is different, or the music is distributed over multiple pages in one or other copy. Furthermore, there                                                              12. Répértoire Internationale des Sources Musicales; see http://www.rism.info/home.html 13 D. Phinot (c.1510-c.1555), Altus part of ‘Virga Jesse floruit’, from Primus liber cum quatuor vocibus : Mottetti del frutto a quarto (Ven-ice: Gardane, 1539) \nDiatonic pitch sequence: MEI: e4 d4 f4 e4 d4 c4 d4  e3 e3 e4 Correct: g4 g4 a4 g4 f4 e4 f4 g4 g3 g3 g4 Diatonic interval sequence: MEI: -1 +2 -1 -1 -1 +2 -7  0  +7 Correct: 0 +1 -1 -1 -1 +1 +1 -8 0  +8 Encoded diatonic interval sequence: MEI: a B a a a B f  - G Correct: - A a a a A A g - G Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 235   may be extraneous material ‘foreign’ to the query page printed on the same page at the beginning or the end of the piece in question. The ‘related music’ category is best illustrated by exam-ple; all of the following were found as high-ranking matches to the query page (2a) using our methods despite the fact that they tend to diverge after a statement of the opening motif: (a) Query:  (b)   (c)   (d)   (e)   Figure 2. Examples of music ‘related’ to a query (a). (N.B. These matches were based on full pages of music, not just on the incipits displayed here.)  Our query page (2a) was the Superius part of ‘D’amours me plains’, a chanson by Maistre Rogier.14 The following item in the same book is a replicque, or response, to the chanson, with a different text, by Tylman Susato, based on the same musical motifs; this was ranked second. Another piece based on Rogier’s chanson, this time with the same text, by Larcier was in fact ranked first. The third-ranked item was the ‘Agnus Dei’ from Thomas Crequillon’s parody mass on the song, Missa Damours me plains; the ‘Sanctus’ from the same mass was ranked in fourth place. Further examples of ‘related’ music might include sep-arate sections of a work, or arrangements with completely different texts which were catalogued as separate items. In fact, in early testing of our method, we discovered that the Recercar Undecimo by an unidentified composer in a 1593 miscellany,15 is in fact a previously unrecognized instrumental arrangement of a motet, ‘In die tribula-                                                             14 Premier livre des cha[n]so[n]s a quatre parties (Antwerp: Susato, 1543, f. xi 15 Fantasie recercari et contrapunti a tre voci (Venice: Gardane, 1593) tionis,’ by ‘Damianus’, probably Damien Havericq (ac-tive 1538-56), published half a century earlier in 1549.16 At first we extracted ngrams from the page-encodings, i.e. fixed-length substrings of length k extracted sequen-tially starting at each character in the string in turn. These were built into a trie (suffix-tree) structure for efficient searching. We then counted the number of ngrams in common between the query and each page of the collec-tion in turn. Although this worked well enough for task (a), we encountered difficulties with tasks (b) and (c) for two reasons: firstly, this naïve ranking did not take ac-count of the fact that longer pages are more likely to con-tain ngrams which appear in the query by chance, and secondly, we were ignoring the order of locations of the ngrams, which should be the same in query and target documents, for obvious musical reasons. The first difficulty was overcome by using Jaccard dis-tance17 rather than a raw count of coincident ngrams; all results reported here use this measure as a basis for search-result ranking. The second problem can be tackled by including ngram-location in the index and sorting the array of results. However, the process of ensuring an or-dered match from the ngram set adds undesirable compu-tational complexity. Turning to a method that has found wide acceptance in recent bioinformatics, we used minimal absent words (MAWs)18 instead of ngrams. We have found this to be highly successful, both in terms of the reduction of the amount of data that has to be searched and because of the fact that MAWs retain the order and structure of the orig-inal document, avoiding the necessity for the secondary expensive sorting routine. 5. EXPERIMENTS For the purposes of the comparison between retrieval us-ing ngrams and MAWs, we ran experiments based on the three user tasks outlined above using a version of the software implemented in Javascript on a MacBook Pro (2.5 GHz Intel Core i7 with 8GB RAM),  running OS X 10.13.3. The software was run in a standard web browser (Safari) via localhost. While we would not consider this to be a sensible setup for production work, it had the ad-vantage of not requiring network access with consequent latency issues.19 For each task we ran the searches using indexes of dif-ferent word-lengths (3-10 characters) and the two word-types (ngrams and MAWs). In addition (as explained be-low) we used an index of MAWs of mixed length (4-8 characters). For ngrams we did not include the result-sorting rou-tine. We expect that sorted ngram results will give the overall best retrieval performance, but this will come at a significant cost in terms of speed, not evaluated here. In                                                              16 Libro secondo de li motetti a tre voce da diversi (Venice: Scotto, 1549), item XVIII 17. https://en.wikipedia.org/wiki/Jaccard_index 18. http://www.lix.polytechnique.fr/SeminaireDoctorants/AliceHeliouMots Absents.pdf 19. The code and encoded data are accessible at: http://doc.gold.ac.uk/~mas01tc/ISMIR2018/ \n236 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018   fact, we believe that these will find their best use on re-duced result lists after the initial indexed search. Before each experiment, the appropriate full index needs to be loaded into a trie (suffix-tree) structure. This process can take up to a minute or so for the larger index-es which also use a lot of memory. Index loading is not considered as part of our experiments, since the indexes would need to be retained as a persistent service (proba-bly distributed between machines) in a production sys-tem. Each task has its associated query-list derived from the predetermined ground truth (see below). These contain different numbers of queries (48, 107 and 334 for the dupl, relv and same tasks, respectively). Each query, con-sisting of a set of index words, was run by searching in turn for each word on the complete index, counting the words in common between query and target pages, with results sorted by Jaccard distance. Where the number of common words was less than 6 the search was regarded as unsuccessful and no results were returned.20 For cer-tain word lengths, no MAWs were generated for some pages (see Discussion, below); for these cases, too, no results were returned. 6. EVALUATION We had previously gathered ground truth using a web-interface allowing a user to annotate documents in ranked results as (a) a duplicate image of the same page (dupl); (b) a page containing substantially the same music (same); or (c) related or relevant music, such as that be-longing to a different voice-part or section of a work (relv). In the three graphs that follow we present the average rank at which known matches from the ground truth lists for a given word length were retrieved in the three exper-iments. Since we were mainly interested in high-ranking matches, we gave all items falling beneath the rank of 20 a uniform rank of 25.  \n  Figure 3. Average ranks for matches of ‘duplicate’ pages.  In our experiments with our test dataset, retrieval per-formance for the dupl task was found to be similar for                                                              20. This arbitrary number was arrived at in early testing as lower num-bers gave essentially useless results. ngrams and MAWs of length 5 characters. We do not ex-pect, however, that this will remain true for all other col-lections, and it is not the case for the other tasks. For this reason, we also performed all the tasks with a mixed-length index of MAWs (4-8 characters) which gave re-sults almost identical to ngrams in the dupl task, consist-ently high in the ranked results in the case of the same task, and the overall best for the relv task.  \n  Figure 4. Average ranks for matches of ‘same music’ pages  \n  Figure 5. Average ranks for matches of ‘related music’  The experiments are named using ‘ng’ and ‘ma’ to in-dicate the use of ngrams or MAWs. The dashed lines on the graphs represent the average rank for the searches us-ing mixed-length MAWs (4-8 chars); these are not quite as good as the best results for ngrams, but very close, and the speed is much faster.  7. DISCUSSION The usefulness of MAWs is highly data-dependent. Over a length-range of 3 to 10 characters, the number of MAWs generated for each page, while lower than the number of ngrams of those lengths, falls off in a way that means that there is simply not enough data for consistent recognition beyond a certain length.   \nProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 237   \n  Figure 6. Number of minimal absent words per page-image (average, maximum and minimum)  However, a database of MAWs with mixed lengths (4-8) always produces enough data for matching and per-forms almost as well as the best ngram length, but is much faster in operation.  \n  Figure 7. Number of words in the trie structure for the entire collection. NB Log scale!  Finally, we show the average search time per word in the collection for each word-length: \n  Figure 8. Average search time (ms) per word for each word length  Bearing in mind that we need to search for each word in the query page in turn, it can be seen that the lower numbers per page of MAWs compared to ngrams, and the consequently smaller index size brings a significant speed advantage. This is particularly important in a search tool which is to be operated by human researchers, who in-creasingly expect retrieval response comparable to that encountered in everyday web searching.  A possibly interesting finding, whose significance needs further investigation, is that there is a fairly con-sistent range of ngram and MAW lengths (viz. roughly between 4 and 8 characters) that produces useful results - this may relate to the nature of the musical data, i.e. to the ‘language’ or style of the music, but this needs to be test-ed formally with a range of different repertories.  8. FURTHER WORK In future work, we intend to compare the efficacy and performance of MAWs with standard algorithms such as BLAST.  Since our use of ngrams in this research was to provide a benchmark for the efficacy of MAWs, limited attempts have been made to optimise them for retrieval speed. We are confident that with the data that we now have on the most effective ngram lengths, effort can be put into algo-rithmic efficiency for a comparison of the two technolo-gies based on their real-world speed.  In many retrieval tasks, it is sufficient simply to return a ranked list of the k best matches for a query, but in the tasks we investigate here, there is an approximately bina-ry relevance judgement to be made. The number of rele-vant documents can vary from 0 to over 100, so finding an appropriate thresholding value is important. Statistical approaches to thresholding have proved useful in the high-dimensional spaces associated with audio searching [18], and this is a sine qua non in text retrieval. We intend to increase the size of our test collection to investigate how well it scales. In order to achieve this, we hope to establish a consortium of international music li-braries to contribute images and metadata, with the ulti-mate goal of providing a comprehensive search tool for musicologists. This requires further work on system ar-chitecture and management of distributed data and pro-cessing. In principle, there is no reason why similar techniques could not be used on other monophonic repertories, and we hope to widen the scope of our work through our con-tinuing association with projects such as SIMSSA21 and TROMPA.22 MAWs present a valuable new method for music re-search which is scalable to collections a good deal bigger than our test set of 32k pages. The technique is generally applicable to any repertory which is reducible to mono-phonic parts or streams, allowing fast approximate re-trieval of large queries over web-scale collections of noisy data.                                                                 21. Single Interface for Music Score Searching and Analysis (project funded by Social Sciences and Humanities Research Council, Canada)  22. Towards Richer Online Music Public-domain Archives (Horizon 2020 project funded by the EU, 2018-21) \n238 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018   9. ACKNOWLEDGMENTS We gratefully acknowledge the help and advice of Prof. Maxime Crochemore and Dr Jeremy Pickens in the writ-ing of this paper. Our thanks are due to Solon Pissis for help with adapting his MAW-extraction code for our musical purposes. The work was partially funded by the UK AHRC project, Transforming Musicology, AH/L006820/1.   10. REFERENCES [1] M. Schedl, E. Gómez and J. Urbano: “Music Information Retrieval: Recent Developments and Applications,” Foundations and Trends in Information Retrieval, Vol. 8, No. 2-3 127–261, 2014. [2] D. Bountouridis: “Music Information Retrieval Using Biologically-Inspired Techniques,” PhD dissertation, Utrecht University, 2018. [3] W. R. Pearson and D. J. Lipman: “Improved tools for biological sequence comparison”, Proc Natl Acad Sci USA. 85(8), 2444-8, April 1988. [4] R. B. Dannenberg and N. Hu: “Pattern Discovery Techniques for Music Audio”, Proceedings of the International Symposium on Music Information Retrieval, 2002. [5] J. Kilian and H. Hoos: “MusicBLAST — Gapped Sequence Alignment for MIR”, Proceedings of the International Symposium on Music Information Retrieval, 2004. [6] B. Martin, D.G. Brown, P. Hanna and P. Ferraro: “BLAST for Audio Sequences Alignment: A Fast Scalable Cover Identification Tool,” Proceedings of the International Symposium on Music Information Retrieval, 529-534, 2012. [7] D. Bountouridis, D.G Brown, F. Wiering and R.C. Veltkamp: “Melodic similarity and applications using biologically-inspired techniques”, Applied Sciences, Special Issue on Sound and Music Computing, 7. 12, 2017. [8] K. Katoh, K. Misawa, K. Kuma, and T. Miyataa: “MAFFT: a novel method for rapid multiple sequence alignment based on fast Fourier transform,” Nucleic Acids Res. 15; 30 (14), 3059-66, July 2002. [9] M. Beal, F. Mignosi, A. Restivo and M. Sciortino: “Forbidden words in symbolic dynamics,” Advances in Applied Mathematics, Vol 25(2), 163–193, 2000. [10] M. Crochemore, G. Fici, R. Mercas and S. Pissis: “Linear-Time Sequence Comparison Using Minimal Absent Words & Applications,” Proceedings of the Latin American Theoretical Informatics Symposium (LATIN), 334-346, 2016. [11] S.P. Garcia and A.J. Pinho: “Minimal absent words in four human genome assemblies,” PLOS ONE, Vol. 6 (12), 2011. [12] S. Chairungsee and  M. Crochemore: “Using minimal absent words to build phylogeny,” Theoretical Computer Science, Vol. 450, 109–116, 2012. [13] C. Barton, A. Heliou, L. Mouchard and S.P. Pissis: “Linear-time computation of minimal absent words using suffix array,” BMC Bioinformatics Vol. 15 (1), 2014. [14] W.K. Sung, Algorithms in Bioinformatics: A Practical Introduction, CRC Press, London, UK, 2009.   [15] L. Pugin and T. Crawford, “Evaluating OMR on the Early Music Online Collection,” Proceedings of the 14th International Society for Music Information Retrieval Conference, pp. 439–44, 2013. [16] J. Calvo-Zaragoza, J.J. Valero-Mas and A. Pertusa: “End-to-end Optical Music Recognition Using Neural Networks,” Proceedings of the International Symposium on Music Information Retrieval, pp. 472–477, 2017. [17] J. Diet and M. Gerritsen: “Encoding, Searching, and Displaying Music Incipits in the RISM-OPAC,” Music Encoding Conference 2013, Mainz, Germany (unpublished). [18] M. Casey, M. Slaney and C. Rhodes: “Analysis of minimum distances in high-dimensional musical spaces,” IEEE Transactions on Audio, Speech, and Language Processing, Vol. 16 (5), 1015-1028, 2008.  Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 239"
    },
    {
        "title": "Methodologies for Creating Symbolic Corpora of Western Music Before 1600.",
        "author": [
            "Julie Cumming",
            "Cory McKay",
            "Jonathan Stuchbery",
            "Ichiro Fujinaga"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492459",
        "url": "https://doi.org/10.5281/zenodo.1492459",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/46_Paper.pdf",
        "abstract": "The creation of a corpus of compositions in symbolic formats is an essential step for any project in systematic research. There are, however, many potential pitfalls, especially in early music, where scores are edited in different ways: variables include clefs, note values, types of barline, and editorial accidentals. Different score editors and optical music recognition software have their own ways of storing and exporting musical data. Choice of software and file formats, and their various parameters, can thus unintentionally bias data, as can decisions on how to interpret potentially ambiguous markings in original sources. This becomes especially problematic when data from different corpora are combined for computational processing, since observed regularities and irregularities may in fact be linked with inconsistent corpus collection methodologies, internal and external, rather than the underlying music.  This paper proposes guidelines, templates, and workflows for the creation of consistent early music corpora, and for detecting encoding biases in existing corpora. We have assembled a corpus of Renaissance duos as a sample implementation, and present machine learning experiments demonstrating how inconsistent or naïve encoding methodologies for corpus collection can distort results.",
        "zenodo_id": 1492459,
        "dblp_key": "conf/ismir/CummingMSF18",
        "keywords": [
            "corpus",
            "symbolic formats",
            "systematic research",
            "pitfalls",
            "early music",
            "scores",
            "clefs",
            "note values",
            "barlines",
            "editorial accidentals"
        ],
        "content": "METHODOLOGIES FOR CREATING SYMBOLIC CORPORA \nOF WESTERN MUSIC BEFORE 1600  \nJulie E. Cumming  Cory McKay  Jonathan Stuchbery  Ichiro Fujinaga  \nMcGill University  \njulie.cumming  \n@mcgill.ca  Marianopolis College  \ncory.mckay  \n@mail.mcgill.ca  McGill University  \njonathan. stuchbery  \n@mail.mcgill.ca  McGill University  \nichiro.fujinaga\n@mcgill.ca  \nABSTRACT  \nThe creation of a corpus of compositions in symbolic \nformats is an essential step for any project in systematic \nresearch. There are, however, many potential pitfalls, es-\npecial ly in early music, where scores are edited in differ-\nent ways : variables include clefs, note values, types of \nbarline, and editorial accidentals . Different score editors \nand optical music recognition software have their own \nways of storing and exporting mus ical data. Choice of \nsoftware and file formats, and their various parameters, \ncan thus unintentionally bias data, as can decisions on \nhow to interpret potentially ambiguous markings in origi-\nnal sources. This becomes especially problematic when \ndata from di fferent corpora are combined for computa-\ntional processing, since observed regularities and irregu-\nlarities may in fact be linked with inconsistent corpus col-\nlection methodologies, internal and external, rather than \nthe underlying music.  \nThis paper proposes  guidelines, templates , and work-\nflows for the creation of consistent early music corpora, \nand for  detecting encoding biases in existing corpora. We \nhave assembled a corpus of Renaissance duos as a sample \nimplementation, and present machine learning experi-\nments demonstrating how inconsistent or naïve encoding \nmethodologies for corpus collection can distort results.   \n1. INTRODUCTION  \nBecause creating accurate corpora is extremely labour \nintensive, early music researchers often draw on symbolic \nscores already  available online . These collections, how-\never, exhibit many different approaches to encoding \nscores, depending on the choices of the individual who \ndid each encoding , the music editor used, the particular \nsymbolic music  file formats used, and the ways in which  \nthose files were  generated. Even when transcribing music \ndirectly into a music editor, it is important to have clear \nguidelines for many elements of the transcription. A good \ncorpus, therefore, requires a clear set of guidelines and \ntemplates for notation  and file creation. It also requires a \nworkflow that integrates correction , and consistent pro-cesses for generating symbolic files. We describe an ef-\nfective process for encoding a consistent corpus for re-\nsearch projects on  Renaissance music , and use it to create \na publicly -available collection of duos . We end with an \nexperiment involving this dataset  showing how differen t \nor inconsistent encoding methodologies  can distort re-\nsults.   \n1.1.  Related Work \nSeveral collections  of symbolic Renaissance scores exist . \nThe Choral Public Domain Library ( CPDL ) [4] includes \nlarge amounts of Renaissance music, but there is no at-\ntempt at standardization. The original ELVIS database [5] \nalso aim ed for quantity without much curation, but with \nsubstantial metadata. The Josquin Resea rch Project ( JRP) \n[21] is carefully curated  and extremely consistent. Small-\ner collections assembled for specific projects , such as [ 8], \n[12], [13], [19], [20 ], and [22 ], are carefully  curated, but \neach uses a different approach.   \n2. RESEARCH CORPORA IN RENAIS SANCE \nMUSIC : NOTATIONAL CONSIST ENCY  \nIn Renaissance music manuscript s and print s the parts are \nnot aligned in score. Instead t hey are  presented in sepa-\nrate parts (on different parts of the page or in separate \npartbooks).  In order to study this music the par ts must be \ntranscribed and combined  into a score.  Mensuration signs \n(similar to time signatures) indicate the metrical organiza-\ntion, but the parts have no barlines, and ties are never \nused. There are multiple different clefs  (C clefs on any \nline; F clefs  on three lines ; G clef is rare). Performers are \nexpected to add accidentals in specific melodic and con-\ntrapuntal situations without explicit accidentals in  the \nscore (resulting  in debates among  performers and editors \nof early music).  Note values are larger than those of \ncommon Western notation : between 1450 and 1550 the \nbeat normally  falls on the semibreve (whole note).  \nModern editors have a wide variety  of approaches to \ntranscription , as described in in [3] and [14]. Some try to \nmake  the edition look like 1 8th-century music, while oth-\ners try to preserve elements of the original notation , and \neverything in between. There are editions of Renaissance \nmusic scores in original clefs and modern clefs; with bar-\nlines, without barlines, or with mensurstriche  (barline s \nthat only ap pear between the staves ). We can  find scores \nwith original, halved, quartered , and smaller  note value s. \n © Julie E. Cumming, Cory McKay, Jonathan Stuchber y, \nIchiro Fujinaga.  Licensed under a Creative Commons Attribution 4.0 \nInternational License (CC BY 4.0). Attribution:  Julie E. Cumming, \nCory McKay, Jonathan Stuchbery, Ichiro Fujinaga. “Methodologies for \nCreating Symbolic Corpora of Western Music before 1 600” , 19th Inter-\nnational Society for Music Information Retrieval Conference, Paris, \nFrance, 2018.  \n  491  \n \nMost editors introduce editorial accidentals, but there are \nmultiple possibilities, and few agree on every decision. \nEditors often also tr anspose works (for performance by a \nspecific ensemble, or because they believe that the origi-\nnal pitch was higher or lower than the “written” pitch in \nthe original source). The same piece of music  edited by \ndifferent people will look very different  (see Fi gure 1) . \nTranscribing works directly from the original sources is \nextremely time consuming, however, so if a piece is \navailable in modern transcription, we normally start with \nthat, either by transcribing it or by using an OMR pro-\ngram such as PhotoScore, a nd then correct it  manually .  \n \n \n \n \n \n \nFigure 1. Contrasting editions of  Josquin Desprez, Missa \nde beata virgine , Agnu s II, mm. 1 –7. Top:  original note \nvalues with mensurstriche  from [10 ]. Middle : halved note \nvalues with barlines  from [9]. Bottom: our edition, with \noriginal note values, barlines, and time signature  that \nmatch es the measure length.   \n2.1. Problems Resulting from  Inconsistent Notation  \nWhen converting published scores into a symbolic corpus \nfor music research (through OMR or transcription with a \nmusic editor), or when taking symbolic scores from an \nonline repository, it is essential to make the notation of \nthe scores consistent. Inconsistent notation can cause sig-\nnificant errors in computational analysis, as we show in \nthe experiment described in Section 6 below. For exam-\nple, when analysing  counterpoint we normally sample the \nscore at every minim (half note) in the original notation. \nIf we have one score in original note values, and one in \nquartered note values, the half not e will have a complete-\nly different meaning, and the results will not be compara-\nble. The length  of a work can also provide information on \ngenre. If the measures are different lengths, because of \ndifferent editorial decisions , then  this data will be incor-\nrect. When looking at issues of mode we normally check \nfinal and key signature; if a work is transposed, this will \ndistort the data. If the number of beats in a measure does \nnot match the time signature, software such as  music21 \n[7] will not parse the symboli c score correctly.  2.2. Creating and Obtaining Symbolic Scores  \nThe most straightforward way to create a symbolic file is \nto transcribe the piece into a music editor from images of \nthe original source (Renaissance manuscript or print). \nWhile this is time consum ing, especially if the original \nsource is difficult to read, or if there are ambiguities in \nthe notation, it results in a file that is very close to the \noriginal source.  \nAll the other methods involve working with a modern \nedition: t ranscription into a mus ic editor from a modern \neditio n (we do this when the  notation of the edition is not \nsuitable for OMR); obtaining symbolic files from online \nrepositories, including  the CPDL [4] and the JRP  [21]; or \nusing an OMR  program such as PhotoS core on a modern \neditio n. Almost all of these files need  adjustment with \nregard to note values, time signatures, editorial acciden-\ntals, and pitch level. As we constructed our corpus, we \nkept finding additional issues that required decisions, \nwhich we incorporated into guidelines  and templates.  \n2.3. Our Guidelines for Consistency  in Scores of Re-\nnaissance M usic c. 1450 –1550   \nIn order to establish norms it is useful to decide on one \nsource of authority, and to create a clear set of guidelines, \nas well as a template  encapsulating the gui delines. W e \nchose not to follow the standards of a single modern edi-\ntion. Instead , we stayed as close to the original as possi-\nble, given that we are transcribing the pieces into moder n \nnotation in score, with barlines. This means  that we use \nthe original n otated pitch of the work, original note val-\nues, and we do not include editorial accidentals, since \nthese are often a subjective decision of a particular editor  \nand there is rarely complete consensus among experts . \nFor ease of reading we use modern clefs: t reble clef, \ntransposing treble clef, and bass clef  (see Figure 1) . We \nuse time  signatures and ties; most of our time signatures \nuse the whole note as the beat (2/1 or 3/1).  There are no \ntime-signature change unless there is a real change of me-\nter in the pi ece, and  the time signature must match the \nlength of the measure . The traditional f inal long is tran-\nscribed as two breves, tied over the bar. We o nly include \nfermatas found in the original source , and  use a fermata \nsymbol that does not affect the rhythmic value of the \nnote. In general, c orrect and consistent encoding is con-\nsidered more important than the appearance of the score , \nand more important  than graphic features of the modern \nedition or the original notation, such as ligature brackets, \nranges, and or iginal clefs and note shapes . \n \n3. ENCODING EARLY  MUSIC  \nOnce researchers have established notational norms for \nthe corpus, they must also establish norms for encoding. \nWhen using pieces available on line, or when more than \none person is creating symbolic files  for the corpus, there \nare many possible sources of inconsistency: symbolic \nfiles in different formats; the use of different music nota-\ntion software  to generate files ; different  software  ver-\nsions ; and different encoding settings for a given piece of \n492 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018  \n \nsoftwa re. We created a set of basic principles  to address \nsuch problems , and incorporated th em into our  workflow \nand score editor templates.   \n3.1. Encoding Formats  \nWe generate Sibelius, MIDI, Music  XML,  **kern , and \nPDF files for use in several different machine learn ing \nand music analysis contexts . Although it is arguabl y de-\nsirable to use purely open file formats when possible \n(e.g., for long -term compatibility), the ubiquity of a for-\nmat is also an essential consideration, in order to maxim-\nize accessibility. We argue that presenting files in a varie-\nty of formats, open and closed, allows us to find a good \ncompromise between these two concerns.  \nMuch of the detail about encoding described here is \nfocused on MIDI, which is important  because of its ubiq-\nuity and because it r equires that certain data be specified \nrather than left ambiguous (e.g. , the tempo of a piece \ncannot be left undefined, as this will implicitly result in \nthe default MIDI tempo being used). Although there are \noften good musicological reasons for ambiguity,  it can \ncause serious problems for many systematic analysis, \nsearch, display , or feature extraction systems , which may \nuse improper defaults or not work at all when faced with \ncertain kinds of ambiguous data . From the specific per-\nspective of computational music processing, MIDI help-\nfully forces encoders to specify best estimates in cases \nwhere there is ambiguity. The most important reason for \nchoosing MIDI, however, is simply that it can be both \nparsed and produced by almost any software, and follows \na univ ersally accepted and open standard. That being \nsaid, MIDI has many well -publicized imperfections and \nlimitations, so it is always advisable to distribute datasets \nin other formats , as we do .  \n3.2. Basic Principles for Encoding the Corpus  \n Use the same software,  softw are version, operating sys-\ntem, and encoding settings   \n Use a uniform and short file naming convention, and \nonly allow  ASCII characters , as archiving or  moving \nfiles between  computers or network locations can  cause \nproblems with long file names or non-ASCII characters  \n Encode provenance information directly in the files \nthemselves, in case encapsulating databases, etc. are \nlost; use rich character sets when permitted  \n Be consistent with:  \no Instrument names (e.g. , “alto” singer vs. “alto” vio-\nla); be sure  there are no missing instrument names \nthat default to incorrect instrument s \no Dynamics  \no Tempo  \no Time signatures and meter changes  \no Key signatures  \no Voice seg regation  \no Transposing treble clefs  \no Fermatas  o Playback settings, affecting dynamics , varying  tem-\npo, note durati ons, etc.  (disable rubato, swing, and \n“human playback ” settings so that encodings are as \nrhythmically quantized as possible)  \n For MIDI in particular : \no Use MIDI Type 1  \no Conform  to General MIDI instruments  \no Avoid keyboard instruments for non -keyboard parts , \nas keyboard  encodings can sometimes cause individ-\nual voices in a polyphonic work to be collapsed into \none part  \no Standardize to 960 PPQN (Pulse Per Quarter Note ) \no Set tempo to whole note = 80 BPM (quarter note = \n320) \n Avoid:  \no Encoding methodologies that needlessly throw away \ninformation  \no Encoding methodologies that permit ambiguity (e.g. , \nin note durations) in cases where automated feature \nextraction or analysis will be used  \no Format conversions : if they are necessary (e.g. , in \norder to increase accessibility), generate all  alterna-\ntive encodings fr om a single master file   \nWe dealt with consistency issues by building templates \n(blank pieces in the notation software with all the correct \nsettings), into which we copied our pieces. These tem-\nplates  are available at [ 6]. \n3.3. Choice of  Score Editing Software  \nWe chose to use the latest version of Sibelius for compat-\nibility and consistency reasons . It is one of the most \nwidely used score editors, it works well with the  Pho-\ntoScore  OMR software , and it has a scripting language \n(ManuScript) . It is also the only score editor that can be \nused to create MEI files, using the Sibelius MEI plugin \n[15]. Although there are certainly important advantages \nto using  open -source software (e.g. , MuseScore) when \npossible, there are no open -source alternativ es to Sibelius \nthat offer these essential advantages. That being said , Si-\nbelius did initially cause us problems : the transposing  clef \noften did not encode the voice in the lower octave, even \nthough the “8” below the clef showed in the score . This \ndistorts contrapuntal analysis (e.g., conson ant fifths be-\ntween voices turn  into dissonant fourths).  \n4. WORKFLOW  \nIn the process of developing our corpus  we developed a \nworkflow for file creation, including both manual and \nscripted processes that allow ed us to avoid inconsistent \nfile production . This workflow can be use d by  other re-\nsearchers who want to create consistent corpora , and is \navailable in more  detail at [ 6]. It can be summarized \nbriefly as follows:  \n Create or collect symbolic files  \n Copy the corrected symbolic  files into the template   Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 493  \n \n Correct the files in Sibelius, following  the guidelines in \nSection 2. 3 \n Check the files for problems (by looking at the PDFs, \nand comparing the files to original sources), and correct \nthem manually when necessary  \n Save the verified  result as a “master file”  \n Once all the desired master files for the corpus  are as-\nsembled , generate all files in all alternative formats at \nthe same time using a script  \n Check MIDI files for consistency using jSymbolic [18] \n(which  reveal s inconsistent sett ings, including meter \nchanges , dynamics,  and tempo settings)  \n5. THE JOSQUIN / LA RUE  DUOS CORPUS  \nWe used the workflow and templates introduced above to \ncreate a corpus devoted to studying differences in the mu-\nsic of two leading Renaissance composers, Josquin \nDesprez (c. 1450 –1521) and Pierre de la Rue (c. 1 452–\n1518). These two composers are particularly interesting \nbecause i t is difficult to tell the ir music apart, even for \nexperts. They are almost exact contemporaries, and there \nare ten compositions attribute d to both composers in dif-\nferent 16th -century sources. Past attempts to describe dif-\nferences in style are often frustratingly  vague , as in this \ndiscussion of why a La Rue Mass is not by Josquin: “the \nrhythmic motion and continuous repetition of the main \nmelodic motif in mm. 45 –66 lack the vitality characteris-\ntic of Josquin” [11]. \nOur corpus consists of duos (two -voice sections) from \nMasses by these two composers. It is important to com-\npare works in the same genre, since different genres can \nresult in differ ent styles, even for the same composer. Al-\nso, c omposers and improvisers in the Renaissance began \nby learning to work in two voices; this is the purest form \nof Renaissance counterpoint. For this study we inclu ded \nonly duos from Masses securely attributed to  the com-\nposers (i.e. , there is consensus that the Masses are not by \nanother composer). For Josquin , we used the “secure” \ncategories established by Jesse Rodin in the J RP [21]; for \nLa Rue we used the assessments in the La Rue edition  \n[17].  \nMost of the symb olic files  in the corpus  came from the \nJRP [21]. We searched the Masses for du o sections sur-\nrounded by double bars (separate sections of longer Mass \nmovements). We downloaded the Music XML files for \nthe relevant movements, opened them in Sibelius, and \nextracted the duos. Some additional movements were \ntranscribed from the La Rue edition, restoring the original \nnote values.  \nOur final corpus , titled the JLSDD  (Josquin La Rue \nSecure Duos  Dataset) , after systematic cleaning, correc-\ntion, and format translation , consists of 33 secure Josquin \nduos and 44 secure La Rue duos, each available as Sibe-\nlius, Music XML, MIDI, MEI , **kern, and PDF files  at \n[6]. They are distributed with pre -extracted jSymbolic  \n[10] features,  and the Sibelius templates used to build the \ncorpus may also be downloaded from  [6].  6. EXPERIMENTS : JOSQUIN VS. LA RUE  \nWe performed a series of machine learning -based com-\nposer attribution experiments in order to gain empirical \ninsight into the effects of different encoding methodolo-\ngies. For related stud ies on systematic composer classifi-\ncation , see [1], [2], and [ 16]. \n6.1. Datasets Used  \nAll of the experiments described here made use of the 33 \nsecure Josquin duos and 44 secure La Rue duos  intro-\nduced in Section 5 . We generated three different  experi-\nmental  MIDI datasets  from this corpu s:  \n Original:  All 77 secure Josquin and La Rue duos, gen-\nerated from the Sibelius files , as they existed before \nsystematic standards were used to correct, annotate , and \nencode them. These duos used a variety of General \nMIDI instrumen t patches, varying amounts of rubato \nadded by Sibelius, varying amounts of dynamic varia-\ntion added by Sibelius and inconsistent approaches to \nmetrical annotation (e.g. , time signatures of 4/4 and 8/4 \nvs. 2/1). Notably, these differences were distributed \nacross the music of both composers, and were not \nmeaningfully correlated with either of them.  \n Clean:  All 77 secure Josquin and La Rue duos, generat-\ned from the Sibelius files after systematic standardiza-\ntion had been applied. The files were all encoded using \nGeneral MIDI Patch 53 (voic e), all had a tempo of 80 \nwhole -note beats per minute, all had  time signatures \nbased on whole -note beats  and none had added rubato \nor dynamics.  These are , in effect , the clean release ver-\nsion of the duos corpus  described in Secti on 5. \n Simulated:  The 33 secure Josquin duos, generated from \nthe Original Sibelius files using systematic settings that \ndiffered from the settings used when generating the \nClean dataset. This was done in order to allow us to \nsimulate the effects of combin ing datasets acquired \nfrom different sources , where  different encoding stand-\nards were used. In this case, all files were encoded us-\ning General MIDI 1 (piano), a tempo of 120 whole -note \nbeats per minute, no rubato added , and no dynamics \nadded. The choice of a  piano patch had the additional \neffect of causing Sibelius to encode the notes from both \nvoices into a single MIDI channel and track, thereby \nlosing the explicit voice segregation found in the Origi-\nnal and Clean datasets.  \n6.2. Feature Extraction  \nFeatures were e xtracted from each of the Original, Clean , \nand Simulated datasets using the newest version (2.2) of \nthe open -source jSymbolic software [ 18]. jSymbolic  ex-\ntracts 246 unique features from symbolic music files, in-\ncluding a number of multidimensional features, for a total \nof 1497 values. These features can be loosely grouped \ninto the following categories: pitch statistics; melodic fea-\ntures; chords and vertical intervals; rhythm; instrumenta-\ntion; texture; and dynamics. jSymbolic was chosen be-494 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018  \n \ncause it includes far  more features than any other musical \nsymbolic feature extraction software, and its extensive \ndocumentation and relatively easy -to-use interface make \nit particularly accessible to musicological researchers \nwho may have less experience with MIR software.  \nTwo sets of features were  extracted for each experi-\nment:  \n All Features:  All features implemented by jSymbolic \nthat can be extracted from MIDI files.  \n Safe Features:  A subset of the All Features group that \nconsists of just 173 of jSymbolic’s 246 implemented \nfeatures. These features omit all features associated \nwith tempo, dynamics, instrumentation , and meter , \namong other things . The intention of these features is \nthat they can be used even when datasets are in fact \nsystematically biased based on encoding methodo logy \n(since the features that would be sensitive to these bias-\nes are not extracted). All features known to be associat-\ned with these qualities were left out, and then a further \nfeature / class correlation check (see below) was per-\nformed in order to make sur e no bias -sensitive features \nremained. The Safe Features are a good fit with  Renais-\nsance music, in which tempo, dynamics, and instrumen-\ntation are not indicated in the musical sources, and are \nleft to the discretion of the performers.  \nWe further analyzed t he Clean and Original datasets \nby calculating the Pearson correlation coefficient between \neach feature in each dataset we experimented with and \nthe composer class (Josquin or La Rue). For all features \nwith high correlations, we manually checked to see \nwhet her the strong correlation was due to an actual mean-\ningful musical difference or to bias introduced by the en-\ncoding methodology. For example, all the Clean pieces \nhad a tempo of 80 BPM, and all the Simulated pieces had \na tempo of 120 BPM. Thus the tempo fe ature alone was \nperfectly correlated to the class  when the Simulated Jos-\nquin pieces were compared to the Clean La Rue pieces, \nand thus tempo even by itself perfectly distinguished Jos-\nquin from La Rue. Of course, this is in fact due to the ar-\nbitrarily chose n tempos assigned when encoding each of \nthese two datasets, so the perfect classification perfor-\nmance of tempo in this example is clearly due solely to an \nencoding methodology inappropriately correlated to \nclass.  \n6.3. Machine Learning Methodology  \nThe features extracted from the Original, Clean , and \nSimulated datasets were used in several supervised 10 -\nfold cross -validation experiments performed using the \nopen -source Weka machine learning software [2 4]. In \nparticular, Weka’s SMO support vector machine imple-\nmentat ion was used with default hyper -parameter set-\ntings. This particular configuration was chosen because it \nis a relatively quick -and-easy approach to use, while still \nbeing quite effective, and thus simulates what musicolog-ical researchers with only casual ex pertise in machine \nlearning might do  relatively easily . \n6.4. Experimental Results and Analysis  \nTable 1 shows the classification accuracies for each da-\ntaset, averaged across cross -validation folds. In some cas-\nes, the pieces compared for each of the two composers  \ncome from the same dataset (Original, Clean, and Simu-\nlated), in order to explore the internal effectiveness of the \nencoding methodology used in that dataset. In other cas-\nes, the music for one composer was drawn from a differ-\nent dataset than the music for the other composer, in or-\nder to simulate what one might encounter if one were to \nperform experiments using music that had been encoded \nusing different methodologies.  \nWe can see in Row 1 that the SMO algorithm was able \nto use the jSymbolic features to corre ctly distinguish be-\ntween the Josquin and La Rue duos 87.0% of the time \nwhen the Clean dataset was used. This is quite impres-\nsive, given how similar the two composers are, and we \ncan be confident that this result is not inflated by encod-\ning bias  (because of  the systematically consistent way \nthat the Clean data was encoded, and because the features \nwere manually examined to provide additional assurance \nthat no unanticipated bias slipped through ). \nIn Rows 1 and 2  we can see that the Clean data per-\nformed 2.6% b etter than the Original data (87.0% vs. \n84.4%). We can be confident that  neither of  these results \nare artificially inflated by encoding methods correlated \nwith composers, as manual verification to guard against \nthis was performed here as well. There are, notably , some \nimportant  differences in how different pieces were en-\ncoded in the Original data; these differences are just not \ncorrelated with the composer. So, rather than causing \nclassification to improve artificially, these encoding dif-\nferences could instead deflate classification performance \nby injecting noise into the features.  However, it should be \nnoted that the difference in performance between Rows 1 \nand 2 is not large enough to be statistically significant \n(with a p -value of 0.05).  \nIn Rows 4, 5, 9, and 10  we can see that classification \nresults were grossly inflated to 100% when the Simulated \ndata for Josquin was mixed with either the Clean or Orig-\ninal data for La Rue. This is because there were elements \nassociated with instrumentation, tempo, meter , and dy-\nnamics that were strongly based on the encoding  methods  \nused rather than  the underlying music, and the se encod-\nings were correlated with the composers.  This confirms  \nthat, if one is not careful to avoid bias when  encoding da-\nta, then one can achieve re sults that seem impressive but \nare in fact meaningless.  \nWe can see that the Clean / Clean and Original / Orig-\ninal results are quite the same for the All Features  (Rows \n1 and 2)  and Safe Features (Rows 6 and 7) groups. This \nmakes sense, since the Safe Featu res omit all features that \ncould be biased by the encoding differences in the Clean \nand Original groups, and the Clean group has no  internal  Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 495  \n \nbias based on encoding source, while the Original group \nhas no correlation between the different encoding meth-\nodolo gies used and the particular composers.  \n \nRow Feature  \nSet Josquin \nDataset  La Rue \nDataset  CA (%) \n1 All  Clean  Clean  87.0 \n2 All  Original  Original  84.4 \n3 All  Clean  Original  98.7 \n4 All  Simulated  Clean  100.0  \n5 All  Simulated  Original  100.0  \n6 Safe  Clean Clean  87.0 \n7 Safe  Original  Original  84.4 \n8 Safe  Clean  Original  87.0 \n9 Safe  Simulated  Clean  100.0  \n10 Safe  Simulated  Original  100.0  \nTable 1 . Classification accuracies (CA) averaged across \n10 folds for each of the 2 -class composer attribution ex-\nperiments. Each experiment is performed once with all \n246 unique features (“All Features”) and once with a re-\nduced set of 173 features chosen to be less vulnerable to \nencoding bias (“Safe Features”). All experiments include \nthe same 33 secure Josquin duos and  44 secure La Rue \nduos, but the encodings for each vary (“Original,” \n“Clean ,” or “Simulated”).  \nThere is a difference , however, between the All Fea-\ntures and Safe Features performance for the Clean Jos-\nquin vs. Original La Rue experiments: the 98.7% \nachieved by the All Features group (Row 3) was clearly \ninflated, but the 87% achieved by the Safe Features group \n(Row 8) was not (in fact, it was identical to the best real \nresults found in the Clean Josquin vs. Clean La Rue ex-\nperiment). This is because Clean Josqu in vs. Original La \nRue does include some differences in tempo, meter, in-\nstrumentation, rubato and dynamics that are correlated \nwith composer in this case (Clean Josquin is uniform in \nthese paramete rs, but Original La Rue is not).  The All \nFeatures set is  sensitive to these differences, and thus \nproduce s inflated results, but the Safe Features  set filter s \nout these problems by ignoring the composer -correlated \nbiased quantities.  \nIt is also notable that both the Simulated Josquin vs. \nClean La Rue (Row 9) and Si mulated Josquin vs. Origi-\nnal La Rue (Row 10) results w ere clearly inflated (both \n100%), even for the Safe Features. This is because the \nSimulated encoding compressed the two distinct voices in \neach duo into a single voice (as a side effect of using a \npiano  patch rather than a voice patch); although no notes \nwere lost in this process, many features that rely on voice \nsegregation were affected. The Safe Features did not omit \nsuch voice -linked features, so they were affected by the \nencoding bias. This serves a s a good reminder that even \n“safe” features may not always be as safe as one thinks, \nand that cleanly and consistently encoded data is always \nbetter when available.  Of course , a reduced set of “safe” features can still be \nuseful when one has no choice but to use data from dif-\nferent sources that have used different encoding method-\nologies. We could, for example, have made an “Extra \nSafe Features” group that also avoided features linked to \nvoice segregation. The problem with being too cautious \nin this way, how ever, is that one risks omitting features \nthat do in fact reveal musically meaningful  insights. For \nexample, examination of the feature values  shows that \nJosquin and La Rue used voice crossing to different ex-\ntents, so features related to voice crossing distinguish the  \ntwo composers  meaningfully;  if one omits all voice -\nrelated features out of fear of biased results, then such \ninsights will never be revealed. “Safe” feature sets must \nalways strike a balance between security against encoding \nbias on the one ha nd and openness to musically meaning-\nful information on the other.  \n6.5. Summary of Experimental Results  \nUsing consistently and systematically encoded music can \npotentially play an essential role in:  \n Avoiding inflated performances due to encoding biases \ncorrelate d with class  \n Avoiding deflated performance due to feature noise not \ncorrelated with class  \nUsing “safe” features chosen to minimize sensitivity to \nencoding bias is a viable approach if one has no choice \nbut to use data encoded in different ways, but it is inferior \nto using uniformly encoded data because:  \n Overly cautious safe features may eliminate features \nthat would reveal musically meaningful insights  \n Insufficiently cautious safe features may admit unantic-\nipated biases into the feature values if one does n ot per-\nform careful checks to avoid this  \n7. CONCLUSION S \nWe have established that notational consistency and en-\ncoding consistency are essential to reliable computer -\naided research on Renaissance music.  Our experience as-\nsembling corpora with a small team of peop le (including \nundergraduates, graduate students, post -docs, and profes-\nsors) showed that establishing clear guidelines and creat-\ning templates enabled us to reach the desired level of \nconsistency; that consistency then allows us to conduct  \ncompelling researc h. Our corpus , templates  and workflow  \nare available online at  [6]. If other scholars adopt the \nsame conventions for their corpora, large and small, and \nmake them available,  we will be on the path to large -scale \nresearch into Renaissance music; a composite corpus that \nis both varied and consistent.  496 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018  \n \n8. ACKNOWLEDGEMENTS  \nThis work was supported by the Social Sciences and \nHumanities  Research Council of Canada and the Fonds \nde Recherche  du Québec - Société et Culture.  We would \nalso like to thank Laura Beauchamp, Nathaniel Condit -\nSchultz, Néstor Nápoles López , and Ian Lorenz for their \nhelp with multiple aspects of this paper.   \n9. REFERENCES  \n[1] M. W. Beauvois, “A Statistical Analysis of the \nChansons of Arnold and Hugo de Lantins,” Early \nMusic, Vol. 45, No. 4, pp. 527 –543, 2017, \nhttps://doi.org/10.1093/em/cax108 . [Accessed: Jun. 9 , \n2018].  \n[2] A. Brinkman, D. Shanahan and C.  Sapp, “Musical \nStylometry, Machine Learning and Attribution \nStudies: A Semi -Supervised Approach to the Works \nof Josquin,” Proc. of the Biennial Int. Conf. on  \nMusic Perception and Cognition,  pp. 91 –97, 2016.  \n[3] J. Caldwell, Editing Early Music,  Clarendon Press, \nOxford, 1995.  \n[4] Choral Public Domain Library , 2018 . [Online]. \nAvailable: http://www.cpdl.org/wiki/ . [Accessed: \nJun. 7, 2018].  \n[5] J. E. Cumming et al. ELVIS Database , 2016 . \n[Online]. Available: https://database.elvisproject.ca/ . \n[Accessed: Jun. 7, 2018].  \n[6] J. E. Cumming , C. McKay, J. Stuchbery, and I. \nFujinaga , JLSDD (Josquin  La Rue Secure Duos \nDataset) GitHub.com , 2018. [Online ]. Available: \nhttps://github.com/ELVIS -Project/mass -duos -\ncorpus -josquin -larue/tree/Methodologies -for-\nCreating -Symbolic -Music -Corpora . [Accessed: Jun. \n7, 2018].  \n[7] M. S. Cuthbert and C. Ariza,  “music21: A T oolkit \nfor Computer -Aided Musicology and Symbolic \nMusic Data, ” Proc. of ISMIR,  pp. 637 –642, Utrecht, \nNetherlands, 2010.  [Online.] \nhttp://web.mit.edu/music21/ .   [Accessed: Jun. 12, \n2018.}  \n[8] K. Desmond et al., Measuring Polyphony: Digital \nEncodings of Late Me dieval Music , 2018 . [Online]. \nAvailable:  http://measuringpolyphony.org/ . \n[Accessed: Jun. 7 , 2018].  \n[9] J. Desprez, “27 Duos by Josquin Desprez or not, \nedited and adapted for instruments, especially \nrecorders and keyboard instruments or harp ,” A. den \nTeuling, E d. Assen, NL, 2014, p. 11.  \nIMSLP/Petrucci Music Library: Free Public \nDomain Sheet Mus ic. [Online]. Available: \nhttp://imslp.org/wiki/27_Duos_(Josquin_Desprez ). \n[Accessed: Jun. 7, 2019] . [10] J. Desprez, Missa de Beata Virgine: zu 4 und 5 \nStimmen , 2. Aufl., ed. Friedrich Blume, Das \nChorwerk, Heft 42. Wolfenbüttel: Möseler Verlag, \n1951 , p. 48.  [Online]. Available: \nhttp://ks.petruccimusiclibrary.org/files/imglnks/usim\ng/5/56/IMSLP48537 -PMLP102712 -\nDas_Chorwerk_042_ -_Desprez,_Josquin_ -\n_Missa_De_Beata_Virgine.pdf . [A ccessed: Jun. 7 , \n2018].  \n[11] W. Elders, New Josquin Edition,  vol. 4 , Masses \nbased on Gregorian chants 2: Critical Commentary,  \np. 102, Koninklijke  Vereniging voor Nederlandse \nMuziekgeschiedenis, Amsterdam, 2000.  \n[12] R. Freedman , D. Fiala, R. Viglianti,  and V. Besson. \nCitations: The Renaissance Imitation Mass (CRIM) . \n[Online]. Available: https://sites.google.com/  \nhaverford.edu/crim -project/home; \nhttps://www.dropbox.com/sh/lyka868ojjkgz12/AAD\na3dYzGTfqB8YMU48jbIuUa?dl=0 ; \nhttp://159.65.177.99:8000/pieces/ . [Accessed: Jun. 7, \n2018].  \n[13] R. Freedman and P. Vendrix, The Lost Voices \nProject, 2014. [Online]. Available: \nhttp://digitalduchemin.org/ ; \nhttps://www.dropbox.com/sh/f2z4iyks2fk9y1a/AAD\n5qJXwlYQdVC -kuPgBv3Mha?dl=0 ; \nhttp://digitalduch emin.org/mei/DC0407.xml . \n[Accessed: Jun. 7 , 2018].  \n[14] J. Grier, The Critical Editing of Music: History,  \nMethod, and Practice , Cambridge Univ. Press, 1996.  \n[15] A. Hankinson, “Sibelius MEI Plugin ,” GitHub.com , \n2017 . [Online]. Available: https://github.com/music  \n-encoding/sibmei . [Accessed: Jun. 7 , 2018].  \n[16] D. Herremans, D. Martens, and K. Sörensen, \n“Composer Classification Models for Music -Theory \nBuilding,” in Computational Music Analysis , D. \nMeredith, Ed. Cham: Springer International \nPublishing, 2016, pp. 369 –392. [Online]. Available:   \nhttps://www.researchgate.net/ profile/Dorien_Herrem\nans/publication/283321533_Composer_ Classificatio\nn_Models_for_Music -\nTheory_Building/links/5633449c08ae242468db84a9\n/Composer -Classi fication -Models -for-Music -\nTheory -Building.pdf . [Accessed : Jun. 7 , 2018].  \n[17] P. de La Rue, Opera Omnia,  vol. 7, Mass Dubia,  ed. \nN. Davison, J. E. Kreider, T. H. Keahey, American \nInstitute of Musicology, Neuhausen, 1998.  \n[18] C. McKay et al., “jSymbolic 2.2: Extracting features \nfrom symbolic music for use in musicologic al and \nMIR research,” Proc. of the Int. Soc. For Music \nInformation Retrieval Conf., accepted for \npublication, 2018.   Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 497  \n \n[19] E. Parada -Cabaleiro, A. Batliner, A. Baird, and B.  W. \nSchuller, \"The S EILS  Dataset: Symbolically \nEncoded Scores in Modern -Early Notation for \nComputational Musicology,\" Proc. of the 18th \nISMIR,  pp. 575 -481, Souzhou, China, 2017.  “The \nSEILS Dataset,” GitHub.com , 2017. [Online]. \nAvailable: \nhttps://github.com/SEILSdataset/SEILSdataset . \n[Accessed: Jun. 7, 2018].  \n[20] E. Ricciardi and C.  S. Sapp, Tasso in Music Project . \n[Online]. Available: http://www.tassomusic.org/ . \n[Accessed: Jun. 7 , 2018].  \n[21] J. Rodin and C.  S. Sapp, Josquin Research Project . \n[Online]. Available: http://josquin.stanford.edu/ ; \nhttp://josquin.s tanford.edu/about/attribution/ . \n[Accessed: Jun. 7 , 2018].  \n[22] P. Vendrix et al. Gesualdo Online . [Online]. \nAvailable: https://ricercar.gesualdo -online.cesr.univ -\ntours.fr/ . [Accessed: Jun. 7 , 2018].  \n[23] I. H. Witten, E. Frank , and M. A. Hall, Data Mining: \nPractical  Machine Learning Tools and Techniques,  \nMorgan Kaufman, New York, 2011.  498 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Music Mood Detection Based on Audio and Lyrics with Deep Neural Net.",
        "author": [
            "Rémi Delbouys",
            "Romain Hennequin",
            "Francesco Piccoli",
            "Jimena Royo-Letelier",
            "Manuel Moussallam"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492427",
        "url": "https://doi.org/10.5281/zenodo.1492427",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/99_Paper.pdf",
        "abstract": "1.1 Related work We consider the task of multimodal music mood prediction based on the audio signal and the lyrics of a track. We reproduce the implementation of traditional feature engineering based approaches and propose a new model based on deep learning. We compare the performance of both approaches on a database containing 18,000 tracks with associated valence and arousal values and show that our approach outperforms classical models on the arousal detection task, and that both approaches perform equally on the valence prediction task. We also compare the a posteriori fusion with fusion of modalities optimized simultaneously with each unimodal model, and observe a significant improvement of valence prediction. We release part of our database for comparison purposes.",
        "zenodo_id": 1492427,
        "dblp_key": "conf/ismir/DelbouysHPRM18",
        "keywords": [
            "multimodal music mood prediction",
            "audio signal",
            "lyrics",
            "traditional feature engineering",
            "deep learning",
            "database",
            "valence and arousal values",
            "performance comparison",
            "a posteriori fusion",
            "unimodal model"
        ],
        "content": "MUSIC MOOD DETECTION BASED ON AUDIO AND LYRICS WITH\nDEEP NEURAL NET\nR´emi Delbouys Romain Hennequin Francesco Piccoli\nJimena Royo-Letelier Manuel Moussallam\nDeezer, 12 rue d’Ath `enes, 75009 Paris, France\nresearch@deezer.com\nABSTRACT\nWe consider the task of multimodal music mood predic-\ntion based on the audio signal and the lyrics of a track. We\nreproduce the implementation of traditional feature engi-\nneering based approaches and propose a new model based\non deep learning. We compare the performance of both\napproaches on a database containing 18,000 tracks with\nassociated valence and arousal values and show that our\napproach outperforms classical models on the arousal de-\ntection task, and that both approaches perform equally on\nthe valence prediction task. We also compare the a poste-\nriori fusion with fusion of modalities optimized simultane-\nously with each unimodal model, and observe a signiﬁcant\nimprovement of valence prediction. We release part of our\ndatabase for comparison purposes.\n1. INTRODUCTION\nMusic Information Retrieval (MIR) has been an ever grow-\ning ﬁeld of research in recent years, driven by the need to\nautomatically process massive collections of music tracks,\nan important task to, for example, streaming companies.\nIn particular, automatic music mood detection has been an\nactive ﬁeld of research in MIR for the past twenty years.\nIt consists of automatically determining the emotion felt\nwhen listening to a track.1In this work, we focus on\nthe task of multimodal mood detection based on the audio\nsignal and the lyrics of the track. We apply deep learn-\ning techniques to the problem and compare our approach\nto classical feature engineering-based ones on a database\nof 18,000 songs labeled with a continuous arousal/valence\nrepresentation. This database is built on the Million Song\nDataset (MSD) [2] and the Deezer catalog. To our knowl-\nedge this constitutes one of the biggest datasets for multi-\nmodal mood detection ever proposed.\n1We use the words emotion and mood interchangeably, as done in the literature\n(see [15]).\nc\rR´emi Delbouys, Romain Hennequin, Francesco Piccoli, Jimena\nRoyo-Letelier, Manuel Moussallam. Licensed under a Creative Commons Attribu-\ntion 4.0 International License (CC BY 4.0). Attribution: R´emi Delbouys, Romain\nHennequin, Francesco Piccoli, Jimena Royo-Letelier, Manuel Moussallam. “Music\nmood detection based on audio and lyrics with Deep Neural Net”, 19th International\nSociety for Music Information Retrieval Conference, Paris, France, 2018.1.1 Related work\nMusic mood studies appeared in the ﬁrst half of the 20th\ncentury, with the work of Hevner [7]. In this work, the au-\nthor deﬁnes groups of emotions and studies classical music\nworks to unveil correlations between emotions and char-\nacteristics of the music. A ﬁrst indication that music and\nlyrics should be jointly considered when analyzing musical\nmood came from a psychological study exposing indepen-\ndent processing of these modalities by the human brain [3].\nFor the past 15 years, different approaches have been de-\nveloped with a wide range of datasets and features. An\nimportant fraction of them was put together by Kim et al.\nin [15]. Li and Ogihara [18] used signal processing fea-\ntures related to timbre, pitch and rhythm. Tzanetakis et\nal. [28] and Peeters [22] also used classical audio features,\nsuch as Mel-Frequency Cepstral Coefﬁcients (MFCCs), as\ninput to a Support Vector Machine (SVM). Lyrics-based\nmood detection was most often based on feature engineer-\ning. For example, Yang and Lee [31] resorted to a psycho-\nlinguistic lexicon related to emotion. Argamon et al. [1]\nextracted stylistic features from text in an author detec-\ntion task. Multimodal approaches were also studied sev-\neral times. Laurier et al. [16] compared prediction level\nand feature level fusion, referred to as late and early fu-\nsion respectively. In [26], Su et al. developed a sentence\nlevel fusion. An important part of the work based on fea-\nture engineering was compiled into more complete studies,\namong which the one from Hu and Downie [9] is one of\nthe most exhaustive, and compares many of the previously\nintroduced features.\nInﬂuenced by advances in deep learning, notably in\nspeech recognition or machine translation, new models be-\ngan to emerge, based on fewer feature engineering. Re-\ngarding audio-based methods, the Music Information Re-\ntrieval Evaluation eXchange (MIREX) competition [5] has\nmonitored the evolution of the state of the art. In this\nframework, Lidy et al. [19] have shown the promise of\naudio-based deep learning. Recently, Jeon et al. [14] pre-\nsented the ﬁrst multimodal deep learning approach using\na bimodal convolutional recurrent network with a binary\nmood representation. However, they neither compared\ntheir work to classical approaches, nor evaluated the ad-\nvantage of their mid-level fusion against simple late fusion\nof unimodal models. In [12], Huang et al. resorted to deep370Boltzmann machines to unveil early correlations between\naudio and lyrics, but their method was limited by the in-\ncompleteness of their dataset, which made impossible the\nuse of temporally local layers, e.g. recurrent or convolu-\ntional ones. To our knowledge, there is no clear answer\nas to whether feature engineering yields better results than\nmore end-to-end systems for the multimodal task, probably\nbecause of the lack of easily accessible large size datasets.\n1.2 Mood representation\nA variety of mood representations have been used in the\nliterature. They either consist of monolabel tagging with\neither simple tags (e.g. in [9]), clusters of tags (e.g. in\nthe MIREX competition) or continuous representation. In\nthis work, we resort to the latter option. Russell [24] de-\nﬁned a 2-dimensional continuous space of embedding for\nemotions. A point in this space represents the valence\n(from negative to positive mood) and arousal (from calm\nto energetic mood) of an emotion. This representation\nwas used multiple times in the literature [12, 27, 29], and\npresents the advantage of being satisfyingly exhaustive. It\nis worth noting that this representation has been validated\nby embedding emotions in a 2-dimensional space based on\ntheir co-occurrences in a database [10]. Since we choose\nthis representation we formulate mood estimation as a 2-\ndimensional regression problem based on a track’s lyrics\nand/or audio.\n1.3 Contributions of this work\nWe study end-to-end lyrics-based approaches to music\nmood detection and compare their performance with clas-\nsical lyrics-based methods performance, and give insights\non the performing architectures and networks types. We\nshow that lyrics-based networks show promising results\nboth in valence and arousal prediction.\nWe describe our bimodal deep learning model and eval-\nuate the performance of a mid-level fusion, compared to\nunimodal approaches and to late fusion of unimodal pre-\ndictions. We show that arousal is highly correlated to the\naudio source, whereas valence requires both modalities to\nbe predicted signiﬁcantly better. We also see that the lat-\nter task can be notably improved by resorting to mid-level\nfusion.\nFinally, we compare our model to traditional feature en-\ngineering methods and show that deep-learning-based ap-\nproaches outperform classical models, when it comes to\nmultimodal arousal detection, and we show that both sys-\ntems are equally performing on valence prediction. For\nfuture comparison purposes, we also release part of our\ndatabase consisting of valence/arousal labels and corre-\nsponding song identiﬁers.\n2. CLASSICAL FEATURE ENGINEERING-BASED\nAPPROACHES\nWe compare our model to classical approaches based on\nfeature engineering. These methods were iteratively deep-\nened over the years: for audio-based models, a succes-sion of works [18, 22, 28] indicated the top performing\naudio features for mood detection tasks ; for lyrics-based\napproaches, a series of studies [1, 10, 31] investigated a\nwide variety of text-based features. Finally, fusion meth-\nods were also studied multiple times [9, 16, 29]. Hu and\nDownie compiled and deepened these works in a series\nof papers [8–10], which is the most accomplished feature-\nengineering-based approach of the subject. We reimple-\nment this work and compare its performance to ours. This\nmodel consists in the choice of the optimal weighted aver-\nage of the predictions of two unimodal models: an SVM\non top of MFCCs, spectral ﬂux, rolloff and centroid, for\naudio; and an SVM on top of basic, linguistic and stylistic\nfeatures (n-grams, lexicon-based features, etc.) for lyrics.\n3. DEEP LEARNING-BASED APPROACH\nWe ﬁrst explore unimodal deep learning models and then\ncombine them into a multimodal network. In each case,\nthe model simultaneously predicts valence and arousal. In-\nputs are subdivided in several segments for training, so that\neach input has the same length. Output is the average of the\npredictions computed by the model on several segments of\nthe input. For the bimodal models, subdivision of audio\nand lyrics requires synchronization of the modalities.\n3.1 Audio only\nWe use a mel-spectrogram as input, which are 2-\ndimensional. We choose a convolutional neural network\n(ConvNet) [17], the architecture is shown in Fig. 1 (a). It\nis composed of two consecutive 1-dimensional convolution\nlayers (convolutions along the temporal dimension) with\n32and16feature maps of size 8, stride 1, and max pooling\nof size 4and stride 4. We resort to batch normalization [13]\nafter each convolutional layer. We use two fully connected\nlayers as output to the network, the intermediate layer be-\ning of size 64.\n(a) Audio\n (b) Lyrics\n (c) Bimodal\nFigure 1 . Architecture of unimodal and bimodal models\n3.2 Lyrics only\nWe use a word embedding as input to the network, i.e.\neach word is embedded in a continuous space and the vec-\ntors corresponding to each word are stacked, the input be-\ning consequently 2-dimensional. We choose to resort toProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 371Model name Description\nCBOW Continuous bag-of-words: random forest on\ntop of means of input words embedding\nGRU Single Gated Recurrent Unit (GRU) [4], size\n40, dense layers of size 64and2, preceded by\ndropout layers of parameter 0:5\nLSTM Single Long Short-Term Memory (LSTM) [6],\nsize80, dense layers of size 64and2, preceded\nby dropout layers of parameter 0:5\nbiLSTM Single LSTM, size 40, dense layers of size 64\nand2, preceded by dropout layers of parameter\n0:5\n2LSTMs Two LSTM layers, of size 40, dense layers of\nsize 64and2, preceded by dropout layers of\nparameter 0:5\nConvNet+LSTM Convolutional layer with 16features maps of\nsize ( 2,2), stride 1, max-pooling of size 2,\nstride 2, an LSTM layer of size 40and dense\nlayers of size 32and2, preceded by dropout\nlayers of parameter 0:5\n2ConvNets+2LSTMs Two convolutional layers with 16features\nmaps of size ( 2,2), stride 1, max-pooling of\nsize 2, stride 2, two LSTM layers of size 40\nand dense layers of size 32and2, preceded by\ndropout layers of parameter 0:5\nTable 1 . Description of lyrics-based models.\na word2vec [21] embedding trained on 1.6 million lyrics,\nas ﬁrst results seemed to indicate that this specialized em-\nbedding performs better than embedding pretrained on an\nunspecialized, albeit bigger, dataset. We compare several\narchitectures, with recurrent and convolutional layers. One\nof them is shown in Fig. 1 (b). We also compare this ap-\nproach with a simple continuous bag-of-words method that\nacts as a feature-free baseline. The models that were tested\nare described in Table 1.\n3.3 Fusion\nFor the fusion model, we reuse the unimodal architecture\nfrom which we remove the fully connected layers and con-\ncatenate the outputs of each network. On top of this con-\ncatenation, we use two fully connected layers with an inter-\nmediate vector length of size 100. This architecture is pre-\nsented in Fig. 1(c). This allows for detection of more com-\nplex correlations between modalities. We choose to com-\npare this with a simple late fusion, which is a weighted av-\nerage of the outputs of the unimodal models, the weight be-\ning grid-searched. The mid-level fusion model is referred\nto asmiddleDL and the late fusion model as lateDL .\n4. EXPERIMENT\n4.1 Dataset\nThe MSD [2] is a large dataset commonly used for MIR\ntasks. The tracks are associated with tags from LastFM2,\nsome of which are related to mood. We apply the proce-\ndure described by Hu and Downie in [11] to select the tags\nthat are akin to a mood description. We then make use of\nthe dataset published by Warriner et al. [30] which asso-\nciates 14,000 English words with their embedding in Rus-\nsell’s valence/arousal space. We use it for embedding pre-\n2http://www.last.fm/viously selected tags into the valence/arousal space. When\nseveral tags are associated with the same track, we retain\nthe mean of the embedding values. Finally, we normal-\nize the database by centering and reducing valence and\narousal. It would undoubtedly be more accurate to have\ntracks directly labeled with valence/arousal values by hu-\nmans, but no database with sufﬁcient volume exists. An\nadvantage of this procedure is its applicability to differ-\nent mood representations, and thus to different existing\ndatabases.\nThe raw audio signal and lyrics are not provided in the\nMSD. Only features are available, namely MFCCs for\naudio, word-counts for lyrics. For this reason, we use a\nmapping between the MSD and the Deezer catalog using\nthe song metadata (song title, artist name, album title) and\nhave then access to raw audio signals and original lyrics\nfor a part of the songs. As a result, we collected a dataset\nof 18,644 annotated tracks. We note that lyrics and au-\ndio are not synchronized. Automatic synchronization be-\ning outside of the scope of this work, we resort to a simple\nheuristic for audio-lyrics alignment. It consists of aligning\nboth modalities proportionally based on their respective\nlength, i.e. for a certain audio segment, we extract words\nfrom the lyrics that are at the corresponding location rel-\natively to the length of the lyrics. We release the labels,\nalong with Deezer song identiﬁers, MSD identiﬁers, artist\nand track name3. More data can be retrieved using the\nDeezer API4. Unfortunately, we cannot release the lyrics\nand music, due to rights restrictions.\nWe train the models on approximately 60% of the\ndataset, and validate their parameters with another 20%.\nEach model is then tested on the remaining 20%. We re-\nfer to these three sets as training, validation and test set,\nrespectively. We split the dataset randomly, with the con-\nstraint that songs by the same artist must not appear in two\ndifferent sets (since artist and moods may be correlated).\n4.2 Implementation details\nFor audio, we use a mel-spectrogram as input to the net-\nwork, with 40mel-ﬁlters and 1024 sample-long Hann win-\ndow with no overlapping, with a sampling frequency of\n44:1kHz, computed with YAAFE [20]. We use data aug-\nmentation, that was investigated for audio and proven use-\nful in [25], in order to grow our dataset. First, we decide\nto extract 30second long segments from the original track.\nThe input of the network is consequently of size 40*1292 .\nWe choose to sample seven extracts per track: we draw\nthem uniformly from the song. We also use pitch shifting\nand lossy encoding, which are transformations with which\nemotion is invariant, and get three extra segments per orig-\ninal sample. In the end, we get a 28-fold increase in the\nsize of the training set.\nFor lyrics, the input word embedding was computed\nwith gensim’s implementation of word2vec [23] and we\nused 100-dimensional vectors. We use data augmentation\n3https://github.com/deezer/deezer_mood_detection_\ndataset\n4https://developers.deezer.com/api372 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018mode model valence arousal\naudioCA 0.118 0.197\nConvNet 0.179 0.235\nlyricsCA 0.140 0.032\nCBOW 0.080 0.031\nLSTM 0.117 0.027\nGRU 0.106 0.017\nbiLSTM 0.076 0.017\n2LSTMs 0.128 0.024\nConvNet+LSTM 0.134 0.026\n2ConvNets+2LSTMs 0.127 0.022\nbimodalCA 0.219 0.216\nLateDL 0.194 0.235\nmiddleDL 0.219 0.232\nTable 2 .R2scores of the different tested approaches.\nfor lyrics as well by extracting seven 50-word segments\nfrom each track. Consequently, the input of each neural\nnetwork is of size 100*50.\n4.3 Results\nWe present the results and compare in particular deep\nlearning approaches with classical ones. The results are\npresented in Tab. 2 and 3. In the latter, CArefers to classi-\ncal models (described in Sect. 2).\nUnimodal approaches. The results of each unimodal\nmodel are given in Table 2. For lyrics-based ones, we have\ntested several models without feature engineering. The\nhighest performing method, on both validation and test set,\nis based on both recurrent and convolutional layers. In the\nfollowing, we choose this model as the one to be compared\nwith classical models.\nFor both unimodal models, one can see a similar trend\nfor classical and deep learning approaches: lyrics and au-\ndio achieve relatively similar performance on valence de-\ntection, whereas audio clearly outperforms lyrics when\nit comes to arousal prediction. This is unsurprising, as\narousal is closely related to rhythm and energy, which are\nessentially induced by the audio signal. On the contrary,\nvalence is explained by both lyrics and audio, indicating\nthat the positivity of an emotion can be conveyed through\nthe text as well as through the melody, the harmony, the\nrhythm, etc. Similar observations were made by Laurier et\nal. [16], where angry and calm songs were classiﬁed sig-\nniﬁcantly better by audio than by lyrics, and happy and\nsad songs were equally well-classiﬁed by both modalities.\nThis is consistent with our observations, as happy and sad\nemotions can be characterized by high and low valence,\nand angry and calm emotions by high and low arousal.\nWhen looking more closely at the results, one can\nobserve that deep learning approaches are much higher\nperforming than classical ones when it comes to predic-\ntion based on audio. On the contrary, classical lyrics-\nbased models are higher performing than our deep learning\nmodel, in particular when it comes to valence detection,\nwhich is the most informative task for the study on lyrics\nonly (as stated above). The reason can be that classical sys-tems resort to several emotion related lexicons designed by\npsychological studies. On the contrary, classical audio fea-\nture engineering for mood detection does not make use of\nsuch external resources curated by experts.\nLate fusion analysis. As stated earlier, the late fusion\nconsists of a simple optimal weighted average between the\nprediction of both unimodal models. We resort to a grid-\nsearch on the value of the weighting between 0 and 1. The\nresult for the reimplementation of traditional approaches\nand for our model is presented in Table 3. One can ob-\nserve a similar phenomenon for both classical models and\nours. In both cases, the fusion of the modalities does not\nsigniﬁcantly improve arousal detection performance com-\npared to audio-based models. It is as predicted, as we saw\nthat audio-based models perform signiﬁcantly better than\nlyrics-based ones. For deep learning models, using lyrics\nin addition to audio in a late fusion scheme leads to no im-\nprovement, so there is no gain added by using lyrics. When\nit comes to valence detection, both modalities are valu-\nable: in both approaches, the top performing model is a\nrelatively balanced average of unimodal predictions. Here\nalso, these observations generalize to valence/arousal what\nwas observed on the emotions happy, sad, angry and calm\nin [16]. Indeed, based on this study, not only are lyrics\nand audio equally performant for predicting happy and sad\nsongs, but they are also complementary, so that fused mod-\nels can achieve notably better accuracies. However, pre-\ndicting angry and calm songs is not improved when using\nlyrics in addition to audio.\nBimodal approaches comparison. Bimodal method\nperformances are reported in Table 2. Several interest-\ning remarks can be made based on these results. First\nof all, one can notice that if one compares late fusion\nfor both approaches, arousal detection is outperformed by\ndeep learning systems, as the corresponding unimodal ap-\nproach based on audio is more performant, and we have\nseen that lyrics-based arousal detection is in both cases\nperforming poorly. On the contrary, late fusion for valence\ndetection yields better results for classical systems. In this\ncase, the lack of performance of lyrics-based methods re-\nlying on deep learning is not compensated for by a slightly\nimproved audio-based performance.\nHowever, when it comes to mid-level fusion presented\nin paragraph 3.3, there is a clear improvement for valence\ndetection. It seems to indicate that there might be ear-\nlier correlations between both modalities, that our model\nis able to detect. Concerning arousal detection, the capac-\nity of the network to unveil such correlations seems use-\nless: we have seen that our lyrics-based model is not able\nto bring additional information to the audio-based model.\nThis performing fusion, along with more accurately\npredicted valence thanks to audio, is sufﬁcient for achiev-\ning similar performance to classical approaches, without\nthe use of any external data designed by experts. Inter-\nestingly, both models remain useful, as long as they learn\ncomplementary information. For valence detection, an op-\ntimized weighted average of the predictions of both models\nyields the performance presented in Table 4. We can seeProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 373coefﬁcient* 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nFeature engineering approachesvalence 0.133 0.163 0.186 0.201 0.211 0.211 0.207 0.192 0.174 0.147 0.112\narousal 0.034 0.081 0.121 0.152 0.178 0.199 0.211 0.217 0.218 0.212 0.201\nDeep learning approachesvalence 0.118 0.136 0.152 0.165 0.175 0.182 0.186 0.188 0.187 0.183 0.177\narousal 0.025 0.065 0.102 0.135 0.164 0.19 0.212 0.231 0.246 0.257 0.265\nTable 3 .R2scores of the late fusion of unimodal models for classical approaches and deep learning approaches, for\ndifferent values of weighting. *This coefﬁcient is the weight of the audio prediction. The weight of the lyrics prediction is\nits complementary to one.\nmodalities BWC*CA and DLCA DLmean\naudio 0.7 0.193 0.118 0.179\nlyrics 0.5 0.177 0.140 0.134\nfused 0.5 0.243 0.219 0.219\nTable 4 .R2scores of the optimal weighted mean of classi-\ncal and deep learning approaches for valence prediction for\ndifferent modalities. *BWC: best weighting coefﬁcient.\nThis coefﬁcient is the optimal weight of the deep learning-\nbased prediction. CA and DL respectively refers to classi-\ncal approaches and deep learning methods.\na signiﬁcant gain obtained for a balanced average of both\npredictions, indicating that both models have different ap-\nplications, in particular when it comes to lyrics-based va-\nlence detection.\n5. CONCLUSION AND FUTURE WORK\nWe have shown that multimodal mood prediction can go\nwithout feature engineering, as deep learning-based mod-\nels achieve better results than classical approaches on\narousal detection, and both methods perform equally on\nvalence detection. It seems that this gain of performance is\nthe results of the capacity of our model to unveil and use\nmid-level correlations between audio and lyrics, particu-\nlarly when it comes to predicting valence, as we have seen\nthat for this task, both modalities are equally important.\nThe gain of performance obtained when using this fu-\nsion instead of late fusion indicates that further work can be\ndone for understanding correlations between both modal-\nities, and there is no doubt that a database with synchro-\nnized lyrics and audio would be of great help to go further.\nFuture work could also rely on a database with labels indi-\ncating the degree of ambiguity of the mood of a track, as\nwe know that in some cases, there can be signiﬁcant vari-\nability between listeners. Such databases would be partic-\nularly helpful to go further in understanding musical emo-\ntion. Temporally localized label in sufﬁcient volume can\nalso be of particular interest. Future work could also lever-\nage unsupervised pretraining to deep learning models, as\nunlabeled data can be easier to ﬁnd in high volume. We\nalso leave it as a future work to pursue improvements of\nlyrics-based models, with deeper architectures or by op-\ntimizing word embeddings used as input. Studying and\noptimizing in detail ConvNets for music mood detection\noffers the opportunity to temporally localize zones respon-\nsible for the valence and arousal of a track, which could beof paramount importance to understand how music, lyrics\nand mood are correlated. Finally, by learning from feature\nengineering approaches, one could use external resources\ndesigned by psychological studies to improve signiﬁcantly\nthe prediction accuracy, as indicated by the complementar-\nity of both approaches.\n6. ACKNOWLEDGMENTS\nThe authors kindly thank Geoffroy Peeters and Gabriel\nMeseguer Brocal for their insights as well as Matt Mould\nfor his proof-reading. The research leading to this work\nbeneﬁted from the WASABI project supported by the\nFrench National Research Agency (contract ANR-16-\nCE23-0017-01).\n7. REFERENCES\n[1] Shlomo Argamon, Marin ˇSari´c, and Sterling S Stein.\nStyle mining of electronic messages for multiple au-\nthorship discrimination: ﬁrst results. In Proceed-\nings of the ninth ACM SIGKDD international confer-\nence on Knowledge discovery and data mining , pages\n475–480. ACM, 2003.\n[2] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian\nWhitman, and Paul Lamere. The million song\ndataset. In ISMIR , 2011.\n[3] Mireille Besson, Frederique Faita, Isabelle Peretz, A-\nM Bonnel, and Jean Requin. Singing in the brain:\nIndependence of lyrics and tunes. Psychological Sci-\nence, 9(6):494–498, 1998.\n[4] Kyunghyun Cho, Bart Van Merri ¨enboer, Dzmitry\nBahdanau, and Yoshua Bengio. On the properties\nof neural machine translation: Encoder-decoder ap-\nproaches. arXiv preprint arXiv:1409.1259 , 2014.\n[5] J Stephen Downie. The music information re-\ntrieval evaluation exchange (mirex). D-Lib Maga-\nzine, 12(12):795–825, 2006.\n[6] Felix A Gers, J ¨urgen Schmidhuber, and Fred Cum-\nmins. Learning to forget: Continual prediction with\nLSTM. 1999.\n[7] Kate Hevner. Experimental studies of the elements of\nexpression in music. The American Journal of Psy-\nchology , 48(2):246–268, 1936.374 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[8] Xiao Hu, Kahyun Choi, and J Stephen Downie. A\nframework for evaluating multimodal music mood\nclassiﬁcation. Journal of the Association for Infor-\nmation Science and Technology , 2016.\n[9] Xiao Hu and J Stephen Downie. Improving mood\nclassiﬁcation in music digital libraries by combining\nlyrics and audio. In Proceedings of the 10th annual\njoint conference on Digital libraries , pages 159–168.\nACM, 2010.\n[10] Xiao Hu and J Stephen Downie. When lyrics outper-\nform audio for music mood classiﬁcation: A feature\nanalysis. In ISMIR , pages 619–624, 2010.\n[11] Xiao Hu, J Stephen Downie, and Andreas F Ehmann.\nLyric text mining in music mood classiﬁcation.\nAmerican music , 183(5,049):2–209, 2009.\n[12] Moyuan Huang, Wenge Rong, Tom Arjannikov, Nan\nJiang, and Zhang Xiong. Bi-modal deep boltzmann\nmachine based musical emotion classiﬁcation. In\nInternational Conference on Artiﬁcial Neural Net-\nworks , pages 199–207. Springer, 2016.\n[13] Sergey Ioffe and Christian Szegedy. Batch normal-\nization: Accelerating deep network training by reduc-\ning internal covariate shift. In International Confer-\nence on Machine Learning , pages 448–456, 2015.\n[14] Byungsoo Jeon, Chanju Kim, Adrian Kim, Dong-\nwon Kim, Jangyeon Park, and Jung-Woo Ha. Music\nemotion recognition via end-to-end multimodal neu-\nral networks.\n[15] Youngmoo E Kim, Erik M Schmidt, Raymond\nMigneco, Brandon G Morton, Patrick Richardson,\nJeffrey Scott, Jacquelin A Speck, and Douglas Turn-\nbull. Music emotion recognition: A state of the art\nreview. In ISMIR , pages 255–266, 2010.\n[16] Cyril Laurier, Jens Grivolla, and Perfecto Herrera.\nMultimodal music mood classiﬁcation using audio\nand lyrics. In Machine Learning and Applications,\n2008. ICMLA’08. Seventh International Conference\non, pages 688–693. IEEE, 2008.\n[17] Yann LeCun, Koray Kavukcuoglu, and Cl ´ement\nFarabet. Convolutional networks and applications\nin vision. In Circuits and Systems (ISCAS), Pro-\nceedings of 2010 IEEE International Symposium on ,\npages 253–256. IEEE, 2010.\n[18] Tao Li and Mitsunori Ogihara. Detecting emotion\nin music. In ISMIR , pages 239–240. Johns Hopkins\nUniversity, 2003.\n[19] Thomas Lidy and Alexander Schindler. Parallel con-\nvolutional neural networks for music genre and mood\nclassiﬁcation. MIREX , 2016.[20] Benoit Mathieu, Slim Essid, Thomas Fillon, Jacques\nPrado, and Ga ¨el Richard. Yaafe, an easy to use and\nefﬁcient audio feature extraction software. In ISMIR ,\npages 441–446, 2010.\n[21] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. Efﬁcient estimation of word representations in\nvector space. arXiv preprint arXiv:1301.3781 , 2013.\n[22] Geoffroy Peeters. A Generic Training and Classi-\nﬁcation System for MIREX08 Classiﬁcation Tasks:\nAudio Music Mood, Audio Genre, Audio Artist and\nAudio Tag. In MIREX , Philadelphia, United States,\nSeptember 2008.\n[23] Radim ˇReh˚uˇrek and Petr Sojka. Software Framework\nfor Topic Modelling with Large Corpora. In Pro-\nceedings of the LREC 2010 Workshop on New Chal-\nlenges for NLP Frameworks , pages 45–50, Valletta,\nMalta, May 2010. ELRA. http://is.muni.cz/\npublication/884893/en .\n[24] James A Russell. A circumplex model of affect. Jour-\nnal of personality and social psychology , 39(6):1161,\n1980.\n[25] Jan Schluter and Sebastian Bock. Improved musical\nonset detection with convolutional neural networks.\nInAcoustics, speech and signal processing (icassp),\n2014 ieee international conference on , pages 6979–\n6983. IEEE, 2014.\n[26] Feng Su and Hao Xue. Graph-based multimodal mu-\nsic mood classiﬁcation in discriminative latent space.\nInInternational Conference on Multimedia Model-\ning, pages 152–163. Springer, 2017.\n[27] George Trigeorgis, Fabien Ringeval, Raymond\nBrueckner, Erik Marchi, Mihalis A Nicolaou, Bj ¨orn\nSchuller, and Stefanos Zafeiriou. Adieu fea-\ntures? end-to-end speech emotion recognition using\na deep convolutional recurrent network. In Acous-\ntics, Speech and Signal Processing (ICASSP), 2016\nIEEE International Conference on , pages 5200–\n5204. IEEE, 2016.\n[28] George Tzanetakis. Marsyas submissions to MIREX\n2007. In ISMIR , 2007.\n[29] Xing Wang, Xiaoou Chen, Deshun Yang, and Yuqian\nWu. Music emotion classiﬁcation of chinese songs\nbased on lyrics using tf* idf and rhyme. In ISMIR ,\npages 765–770, 2011.\n[30] Amy Beth Warriner, Victor Kuperman, and Marc\nBrysbaert. Norms of valence, arousal, and dominance\nfor 13,915 english lemmas. Behavior research meth-\nods, 45(4):1191–1207, 2013.\n[31] Dan Yang and Won-Sook Lee. Disambiguating music\nemotion using software agents. In ISMIR , volume 4,\npages 218–223, 2004.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 375"
    },
    {
        "title": "Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners.",
        "author": [
            "Andrew M. Demetriou",
            "Andreas Jansson 0001",
            "Aparna Kumar",
            "Rachel M. Bittner"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492465",
        "url": "https://doi.org/10.5281/zenodo.1492465",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/98_Paper.pdf",
        "abstract": "In music information retrieval, we often make assertions about what features of music are important to study, one of which is vocals. While the importance of vocals in music preference is both intuitive and anticipated by psychological theory, we have not found any survey studies that confirm this commonly held assertion. We address two questions: (1) what components of music are most salient to people's musical taste, and (2) how do vocals rank relative to other components of music, in regards to whether people like or dislike a song. Lastly, we explore the aspects of the voice that listeners find important. Two surveys of Spotify users were conducted. The first gathered open-format responses that were then card-sorted into semantic categories by the team of researchers. The second asked respondents to rank the semantic categories derived from the first survey. Responses indicate that vocals were a salient component in the minds of listeners. Further, vocals ranked high as a self-reported factor for a listener liking or disliking a track, among a statistically significant ranking of musical attributes. In addition, we open several new interesting problem areas that have yet to be explored in MIR.",
        "zenodo_id": 1492465,
        "dblp_key": "conf/ismir/DemetriouJKB18",
        "keywords": [
            "music information retrieval",
            "vocals",
            "music preference",
            "salient components",
            "listener liking",
            "MIR",
            "open-format responses",
            "card-sorted",
            "semantic categories",
            "statistically significant ranking"
        ],
        "content": "VOCALS IN MUSIC MATTER: THE RELEVANCE OF VOCALS IN THE\nMINDS OF LISTENERS\nAndrew Demetriou1;2Andreas Jansson2Aparna Kumar2Rachel M. Bittner2\n1Multimedia Computing Group, TU Delft, The Netherlands\n2Spotify Inc., New York City, USA\nABSTRACT\nIn music information retrieval, we often make assertions\nabout what features of music are important to study, one of\nwhich is vocals. While the importance of vocals in music\npreference is both intuitive and anticipated by psychologi-\ncal theory, we have not found any survey studies that con-\nﬁrm this commonly held assertion. We address two ques-\ntions: (1) what components of music are most salient to\npeople’s musical taste, and (2) how do vocals rank relative\nto other components of music, in regards to whether people\nlike or dislike a song. Lastly, we explore the aspects of the\nvoice that listeners ﬁnd important. Two surveys of Spotify\nusers were conducted. The ﬁrst gathered open-format re-\nsponses that were then card-sorted into semantic categories\nby the team of researchers. The second asked respondents\nto rank the semantic categories derived from the ﬁrst sur-\nvey. Responses indicate that vocals were a salient compo-\nnent in the minds of listeners. Further, vocals ranked high\nas a self-reported factor for a listener liking or disliking\na track, among a statistically signiﬁcant ranking of musi-\ncal attributes. In addition, we open several new interesting\nproblem areas that have yet to be explored in MIR.\n1. INTRODUCTION\nThe Music Information Retrieval (MIR) community has\nhistorically focused on content-based understanding of\nmusic. The type of content-based analysis studied over\ntime is typically driven by the data available to the task, or\nthe interests of the speciﬁc researchers. An alternative mo-\ntivator could be to study topics that are salient in the minds\nof listeners, especially with respect to listener’s musical\npreference. Speciﬁcally, understanding which attributes of\nmusic contribute the most to music preference, and their\nrelative weight, could help guide research efforts. One at-\ntribute of music we would expect to be salient in the minds\nof listeners is the singing voice.\nPsychology research anticipates the importance of the\nhuman voice as a salient stimulus, and as a component of\nc\rAndrew Demetriou, Andreas Jansson, Aparna Kumar,\nRachel M. Bittner. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: Andrew Demetriou,\nAndreas Jansson, Aparna Kumar, Rachel M. Bittner. “V ocals in Music\nMatter: the Relevance of V ocals in the Minds of Listeners”, 19th Interna-\ntional Society for Music Information Retrieval Conference, Paris, France,\n2018.music in particular. The human ability to communicate\nexceeds that of any other species studied thus far, with\nboth speech and singing being cultural universals reliant\non vocal production. It is theorized that the advanced hu-\nman ability to communicate, discriminate, and to experi-\nence emotional responses in vocalizations has allowed for\nthe emergence of music [8]. Our emotions are often ac-\ncompanied by involuntary changes in our physiology and\nnonverbal expressions, such as facial expressions and vo-\ncalizations [15]. Our reactions to the emotional content\nexpressed in the vocals in music may have similar effects.\nAs such, much psychological research has focused on the\nsinging voice even more than speech, due to the precision\nrequired to execute and process musical vocalizations [5].\nThis makes musical vocals a well-anticipated candidate for\nstudy as a feature of music, as we would expect people to\nhave a sophisticated ability to deliver, empathize with, and\nprocess vocal communications.\nWe would therefore expect that the vocals in music\nwould be an especially salient component, if not the most\nsalient. While a complete review is beyond the scope of\nthis paper, some research is particularly worth noting. For\nexample, it has been shown that both adults [18] and chil-\ndren [17] recall melodies more correctly when sung with\nthe voice than when played with instruments. Hutchins\nand Moreno [5] review literature that shows relatively pre-\ncise perception of pitch in the human voice, yet fewer no-\nticeable pitch errors in the voice relative to musical in-\nstruments or synthesized voices [6]. Neuroscience stud-\nies show speciﬁc areas of the brain involved in processing\nhuman voices [2]. Although similar regions of the brain\nare involved in processing both music and voices, there is\ndifferential processing of the human voice relative to mu-\nsic [1]. As such, the human voice may be processed as a\nuniquely signiﬁcant sound.\nHowever, while prior research suggests that vocals\nwould be especially relevant to music preference, no study\nto our knowledge has assessed the importance of the voice\nin music, relative to other musical components. To address\nthis gap, we test the hypothesis that the voice is as or more\nimportant than other musical components across implicit\nand explicit datasets, using traditional social science tech-\nniques, as well as data mining techniques. First, we mine\ndata available from Spotify, including playlist titles, search\ndata and artist biographies, to test whether terms related to\nvocals are prevalent. However, we show that the results\nof the data mining are inconclusive as to whether or not514vocals are salient in the minds of listeners. Speciﬁcally, it\nis not clear whether the vocals can be disentangled from\nother factors in playlist titles and search queries, such as\ngenre. For more conclusive results, we gather data from\nusers explicitly. To this aim we conduct two online sur-\nvey studies: the ﬁrst gathered subjective data on the salient\ncomponents of music directly from listener reports, which\nwere separated into semantic categories using card-sorting.\nThe second asked participants to rank the semantic cate-\ngories from the ﬁrst study in terms of importance to their\nmusical preference. We conclude that two aspects related\nto the voice are especially salient, namely the voice itself,\nand the lyrics of the song. Furthermore, we highlight the\nimportance of gathering explicit data to complement im-\nplicit techniques, in situations where factors may not be\neasily disentangled.\n2. VOCALS IN SEMANTIC DATA\nPrior research has shown that semantic descriptors of mu-\nsic may be an appropriate means for users to query music\ndatabases [12]. Given the large amount of semantic data\navailable to Spotify such as playlist titles, search results,\nand artist biographies, one might hypothesize that terms\ndescribing the vocals would commonly appear in this im-\nplicit data.\n2.1 Playlist Tags and Search Queries\nNon-common words or groups of words and emojis ap-\npearing in the titles of a large number of Spotify’s user-\ngenerated playlists were aggregated to create a list of the\n1000 most frequently occurring tags. Each of these 1000\ntags was assigned a category by a professional curator\nbased on the tag itself and information from the tracks\nmost frequently associated with the tag. The categories,\ndetermined by the curator, were Genre (e.g. “K-Pop”),\nMood (e.g. “sad”), Activity (e.g. “gym”), Popularity\n(e.g. “Today’s hits”), Artist (e.g. “Justin Timberlake”), Era\n(e.g. “70’s”), Culture (e.g. “Latin”), Lyrics (e.g. “clean”),\nRhythm (e.g. “groove”), Instrument (e.g. “guitar”), Tempo\n(e.g. “slow”), V oice (e.g. “female singers”), or Other\n(e.g. “favorites”, “Jenna”, “hi”). The percentage of\nplaylists containing each of these tag categories is dis-\nplayed in Figure 1, top.\nSurprisingly, we see that tags explicitly related to vocals\nare not at all common compared to other types of tags, with\nthe most common tags being related to genre, mood, or ac-\ntivity. Playlist titles can be viewed as labels for groups of\nmusic, and this analysis suggests that people do not often\nlabel groups of music based on explicit characteristics of\nthe vocals. However, speciﬁc vocal characteristics (as well\nas many other musical attributes) may be implicit in many\nof the other tag categories, particularly for genre, mood,\nand artist. As vocal delivery style and genre are closely re-\nlated, emotions communicated by the voice and the mood\nof the collection of songs may be related, and as each artist\nhas a unique voice, we conclude that the relative weight of\nvocals may not have been disentangled from other factors.\nGenre\nOther\nMood\nActivity\nPopularity\nArtist\nEra\nCulture\nLyrics\nRhythm\nInstrument\nTempo\nVoice0.00.2% of PlaylistsTop 1000 Playlist Tag TypesGenre\nMood\nActivity\nInstrument\nCulture\nPopularity\nOther\nEra0.000.25% of Queries1 Day of Descriptive Search QueriesInstrument\nStructure\nCulture\nGenre\nOther\nVoice\nArtist\nMood\nMelody\nRhythm\nPopularity\nCategory020tf-idfTerms in Artist BiographiesFigure 1 : (Top) Percentage of Spotify playlists containing\none of the top 1000 tags corresponding to each category.\n(Middle) Percentage of descriptive search queries corre-\nsponding to each tag category, sampled from one day of\nsearch data. (Bottom) tf-idf for each term category in artist\nbiographies compared with Wikipedia term frequencies.\nWe perform a similar analysis on descriptive terms from\none day’s worth of Spotify search queries, and obtained\nsimilarly inconclusive results, shown in Figure 1, middle.\n2.2 Artist Biographies\nFinally, we analyze descriptive terms that occur in 100,000\nprofessionally authored artist biographies on Spotify. We\nuse TF-IDF [16] to retrieve terms that are distinctive to\nmusic writers, by comparing the frequency of terms in\nartist biographies to the frequency of the same terms in\nWikipedia. The 100 most distinctive terms, grouped into\nsemantic categories, are displayed in Figure 1, bottom.\nWhile many terms are much more frequent in music text\n(e.g. “bassist”, “jazz”, “songwriter”), vocals speciﬁcally\nwere not more frequently mentioned than other musical as-\npects. One can hypothesize that the TF-IDF method is in-\nsufﬁcient for this particular task, due to vocals being com-\nmonly discussed outside the context of music, and thus a\nrelatively more common word in Wikipedia.\n2.3 Conclusions\nOur results thus far do not show support for our general hy-\npothesis. It may be the case that the intuitive notion of the\nrelevance of vocals to user preference is misleading. On\nthe other hand, it may also be the case that the importance\nof vocals is implicit in this data, as certain vocal styles are\nindicative of genre or mood. As such, the overlap between\nthe voice and a number of the tags and descriptors ana-\nlyzed prevents us from disentangling the unique effect of\nthe voice from other musical components.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 5153. VOCALS IN SURVEY DATA\nIn order to disentangle the unique effect of the voice among\nother components, we gathered explicit data from users.\nSpeciﬁcally, we conducted two online survey studies in or-\nder to collect self-reported data on 1) the salient compo-\nnents of music, and 2) their relative ranking. Unlike prior\nsurveys, such as [12] that presented users with short musi-\ncal excerpts and groupings of adjectives to rate, we allowed\nthe users to freely enter their responses to the question\n”When you listen to music, what things about the music\ndo you notice?”. This allowed us to assess whether vocals\nwould emerge as a salient component of music. In addi-\ntion, we explored what aspects of the voice users report as\nbeing important to their musical taste.\n3.1 Survey 1: Semantic Components of Music\nThe aim of our ﬁrst survey was to establish an unranked set\nof self-reported salient components of music. While our\nhypothesis was that the vocals would be prominent, it was\ncrucial to avoid biasing respondents as the data collected\nwere explicit. As such, our ﬁrst survey asked participants\nwhat they notice when listening to music that might make\nthem like or dislike a song. We deliberately did not spec-\nify anything further, such as the type of music, or that we\nwere interested in components of music, nor were partici-\npants asked to listen to musical excerpts so as not to bias\nresponses. As an exploratory measure, we then asked par-\nticipants to describe what about vocals speciﬁcally might\nmake them like or dislike a song after the previous open\nended questions, so as not to bias responses. Responses to\nthese two open-response questions were manually sorted\ninto semantic categories by the researchers.\n3.1.1 Recruitment\nA random sample of 50,000 people was drawn from the\ndatabase of Spotify’s Monthly Active Users (MUAs), di-\nvided approximately equally between the United States\nand Canada. 860 individuals responded to the survey, how-\never 224 did not respond to any questions beyond the con-\nsent form, and 9 were removed for giving nonsensical re-\nsponses. 626 individuals — 338 women (average age 33.6\nyears with a standard deviation of 16.1); 288 men (aver-\nage age 30.6 years with a standard deviation of 15.5) —\ncompleted the survey in its entirety.\n3.1.2 Survey\nAn online consent form was ﬁrst presented to respondents.\nWe then asked:\nQ1: When you listen to music, what things about\nthe music do you notice? Please list as many as you\ncan think of here:\nThe respondents were shown a screen with open-\nresponse format ﬁelds to complete, in which they could\ncomplete up to seven ﬁelds. On the following screen, re-\nspondents were presented with a list of their responses in\nrandom order, and asked:\nemotions If it doesn’t feel like there’s emotion behind it, or \nsomehow lacking. emotions When I can either relate or empathize with them \nand when the song projects the emotions onto me. Figure 2 : Survey 1 sample answers for Q3. (Top) Card for\nan answer to Q3a. (Bottom) Card for an answer to Q3b.\nQ2: Please rank how important the aspects you\nlisted are to your musical preference, where 1 is\nthe most important.\nThey were then asked the following two questions about\nthe items they ranked from 1 to 3:\nQ3: (a) What about would make you like a\nsong? (b) What about would make you dislike\na song?\nLastly, to explore what aspects of vocals may be rele-\nvant, participants responded to the following:\nQ4: (Please ignore these questions if you’ve\nalready mentioned the vocals, the voice, the\nsinger/rapper etc.) (a) When would vocals make\nyou like a song? (b) When would vocals make you\ndislike a song?\nThey were then given the opportunity to comment on\nthe survey, and were shown a ﬁnal debrieﬁng screen.\n3.1.3 Semantic Categorization\nA number of partially completed surveys contained re-\nsponses sufﬁciently complete for card sorting. 317 sufﬁ-\ncient responses — 262 from the completed surveys as well\nas 55 sufﬁciently complete partial – were then card-sorted\nby a team of researchers. Card-sorting is a common tech-\nnique used in social sciences and elsewhere to discover\nclusters of related concepts [14]. Traditionally, individuals\nare presented with physical paper “cards” that have terms\nand/or descriptions printed on them, printed pictures, or a\ngroup of objects. They are then asked to group items in a\nway that makes sense, given the research question. Here,\nwe apply card-sorting to derive semantically meaningful\ngroupings of musical components from the freely entered\nwords and phrases that participants entered in each ﬁeld.\nParticipant responses to Q1(i.e. “When you listen to\nmusic, what things do you notice?”) were printed twice,\nonce next to their response to Q3a (“What about would\nmake you like a song?”), and again next to the response\ntoQ3b (“What about would make you dislike a song?”).\nAs such, researchers had respondents’ top 3 terms printed\nout twice, once next to the positive descriptive aspects of\nthe term, and once next to the negative descriptive aspects.\nA term (e.g. “the lyrics”) and its descriptor (e.g. “when\nthey have meaning”) comprised a card. Figure 2 shows\nexamples of positive and negative cards that were used in\ncard sorting.\nAs some responses were unclear (e.g. “the melody” was\nmentioned, but the descriptor clearly focused on the qual-\nity of the singer’s voice), the research team was instructed516 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018to look at both the term and its descriptor when determin-\ning its semantic category. The researchers then reviewed\nthe cards a second time, and deﬁned sub-categories where\nnecessary.\n3.1.4 Results\nThe output of this study was two sets of semantic cate-\ngories: broad semantic categories of music, and vocal-\nspeciﬁc semantic categories. Statistical testing was not\npossible, given the intentionally imprecise nature of the re-\nsponses. However, out of the 626 responses to the ﬁrst\nquestion, 186 (29.7%) mentioned the vocals, the voice, or\nthe singer, 348 (55.6%) mentioned the lyrics, or the words,\nand 101 (16.1%) mentioned both. While this is no indi-\ncation of relative importance, it does demonstrate that the\nvoice and the lyrics were salient musical components to\nour respondents.\nThe broad semantic categories determined by the re-\nsearchers are presented in the left column of Table 1 (note\nthat the other results in Table 1 are from Study 2). The\ncategory of Emotion/mood referred to the ability of a song\nto evoke emotion, whether the emotion was a match or a\nmismatch to the current or desired mood or current activ-\nity, whether the emotion was desirable or undesirable, and\nnostalgia. Voice included genre related terms (e.g. mum-\nble rap, metal, auto-tune, speechiness/rapping), descrip-\ntions of how the voice is used (e.g. unique/novel, scream-\ning, pitch/pitch range, presence or absence of effects,\nintensity/effort/power, emotionality, authenticity, whini-\nness/nasality, melodic-ness), skill, the innate qualities of\nthe voice, liking/disliking, and the mix/blend. The Lyrics\ncategory represented items that indicated whether or not\nlyrics were present, their intelligibility, the presence of\nprofanity, how “well” crafted they were, the “message”,\nthe meaning behind them or general lyrical content and\nhow relatable they are. Beat/Rhythm referred to whether\nit was liked/disliked, whether it “ﬁt” the song, danceabil-\nity, and uniqueness. The Structure/complexity of songs in-\ncluded liking or disliking the hook or chorus, and the song\nlength. Instrumentation referred to drums, bass, and guitar.\nSound referred to audio quality and related concerns. Self-\nexplanatory categories included Tempo/BPM , the mention\nof a Speciﬁc Artist ,Genre ,Harmony ,Chords ,Musician-\nship,Melody , and Popularity/Novelty .\n3.2 Survey 2: Component Ranking\nWhile the ﬁrst study aimed at determining what attributes\nof music were salient in the minds of listeners, the aim\nof the second survey was to determine the relative impor-\ntance of each of the components. Speciﬁcally, we explored\nwhether the voice would be ranked highest among a list of\nmusical attributes. To accomplish this, participants were\nasked to rank a list of attributes derived from the results of\nour ﬁrst survey, thus allowing an assessment of whether or\nnot vocals rank above other components.3.2.1 Recruitment\nA randomized sampling method was employed among the\ndatabase of Spotify’s Monthly Active Users (MAUs) that\nhad not opted-out of email correspondence. An email\nwith a link to an online survey was sent to 50,000 poten-\ntial respondents, approximately equally divided among the\nUnited States and Canada.\nA total of 531 respondents — 263 of which were\nwomen (average age 31.8 years, with a standard deviation\nof 16.5); 268 were men (average age 34.2 years, with a\nstandard deviation of 14.8) — completed the survey in its\nentirety. 429 participants completed the ﬁrst half of the sur-\nvey (broad semantic categories), whereas 360 participants\ncompleted the second half (vocal semantic categories).\n3.2.2 Survey\nAn online consent form was ﬁrst presented to respondents.\nThe derived semantic categories were rephrased to be more\neasily understood (see Table 1, Description). Participants\nwere presented with the new list of descriptions in random\norder, and asked to “Please click all the items below that\nwould make you like or dislike a song.” They were then\npresented with a list of all the items they had clicked, also\nin random order, and asked to rank them.\nAs a continuation of our exploratory study of vocal\ncharacteristics, a second list was then presented, comprised\nof terms derived from the vocal and lyrics semantic cate-\ngories. For clarity, the terms were rephrased as they appear\nin Table 2.\n3.2.3 Analytic Strategy\nResponses were subjected to Borda counting [3] and Ro-\nbust Rank Aggregation [9]. Borda counting is a sim-\nple procedure for aggregating votes by summing ranks.\nThe Borda score Bifor an item iis computed as Bi=PN\np=0(jrpj\u0000rp;i)where Nis the number of participants,\nrp;iis participant p’s rank of item i, starting at zero, and\njrpjis the number of items ranked by p. The Borda method\ndoes not naturally extend to partial lists [4] — we have cho-\nsen to award higher scores to preferred items in long lists.\nTo verify the statistical signiﬁcance of our ﬁndings we\nsupplement the Borda count with Robust Rank Aggrega-\ntion (RRA), in which we compare our survey results to a\nnull hypothesis. Each item receives a score based on its\nobserved position, compared to an expected random order-\ning. Upper bounds to p-values are computed using Bonfer-\nroni correction, with values of 1.0 indicating null ﬁndings.\nIn this work we used the implementation provided by the\nROBUST RANK AGGREG package1.\n3.2.4 Results and Conclusion\nResults can be found in Tables 1 and 2, with categories\nordered by descending Borda count. We are able to show\nstatistical signiﬁcance of both the most salient broad and\nvocal semantic categories. Importantly, our results show\nthat the V ocals and Lyrics ranked second and third among\n1cran.r-project.org/web/packages/\nRobustRankAggregProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 517Broad Semantic Category Description Borda score p-value\nEmotion/mood How it makes you feel - the emotions/mood 4641 <0.001\nV oice V oice/vocals 3688 <0.001\nLyrics Lyrics 3656 <0.001\nBeat/rhythm Beat/rhythm 3460 <0.001\nStructure/Complexity How it’s composed, the hook, the structure 2677 1.000\nMusicianship Skill of the musicians, musicianship 2583 1.000\nMelody The main melody 2577 1.000\nSound The “sound”, or the recording quality 2406 1.000\nSpeciﬁc Artist The speciﬁc artist 2349 1.000\nGenre The speciﬁc genre 2293 1.000\nInstrumentation The musical instruments (e.g. drums, bass, guitar) 2084 1.000\nTempo/BPM How fast or slow the song is 1828 1.000\nHarmony Harmony 1763 1.000\nChords The chords 1086 1.000\nPopularity/Novelty How popular or unique it is 777 1.000\nTable 1 : Broad semantic categories and their clarifying descriptions created during Study 1, ordered by rankings from Study\n2 (see Study 1 results for attribute descriptions). The Borda scores and p-values from Study 2 are reported in columns 3\nand 4. Statistically signiﬁcant p-values are shown in bold. p-values of 1.000 indicate that the ranking is no different from\nrandom.\nthe list of components (Borda scores and RRA agree on the\norder of the ﬁrst four broad categories). This indicates that,\nrelative to other musical components, respondents overall\nindicated the importance of the vocals and lyrics.\n4. NEW A VENUES FOR RESEARCH\nWhile the musical attributes related to the broad musical\ncategories (Table 1) are well studied in MIR, the attributes\nrelated to vocals (Table 2) present a number of exciting and\nunexplored research directions. A limiting factor to study-\ning some of these problems, as is often the case, is the\navailability of data, and we encourage researchers to focus\ndata collection efforts in these areas as well. A further lim-\niting factor is that users of online musical platforms may\ncome from a speciﬁc demographic, e.g. regular internet\nusers typically younger than 35, who engage in music re-\nlated activities in about one third of the online time, have\nhad at least some musical education, and have a preference\nfor pop, rock and classical music [12]. In addition, our\nsample was derived from the U.S. and Canada. As such,\na cross-cultural sample may differ in their relative prefer-\nence for vocals.\nOur exploratory data suggest that there is a vast space of\nresearch in tagging and measuring different qualities of the\nsinging voice, such as whether a singing voice is authentic,\npowerful, natural, melodic, nasal, or emotional (Table 2,\nrows E, H, I, K, M and G). In addition to these categories,\ndetermined by untrained listeners, there are a number of\nother more speciﬁc categories such as modes of phonation\nthat could be explored. Further, in addition to vocal qual-\nities, there are genre-centric vocal styles, such as identify-\ning rap or screaming (Table 2, rows S and O).\nAnother interesting and (as far as we are aware) unex-\nplored research area is to measure whether a voice ﬁts orblends well with the background music (Table 2, row B).\nThis is somewhat related to the problem of determining\n“mashability” in automatic-mashup generation. This is a\nbroad problem that is likely based on many factors, such\nas the style of the vocalist compared to the background,\nthe way the song is mixed, and the overall expectations of\nthe musical genre. We suspect this could be most easily\nstudied when isolated vocals/backgrounds are available in\norder to automatically generate examples of vocals that do\nnot match the background by blending random combina-\ntions.\nThe problem of identifying whether a voice is “unique”\nis likely challenging (Table 2, row F), as it is not necessar-\nily a quality that can be determined in isolation, but rather\nrelative to many other voices. One possible approach to\nthis problem would be to treat the problem as one of out-\nlier detection.\nProduction effects applied to the singing voice are in-\ncreasingly common, especially different types of distor-\ntion or the infamous auto-tune (Table 2, row Q). Auto-\nmatic identiﬁcation of these production effects presents an\ninteresting challenge, and one where data could be auto-\nmatically generated with the help of plugins for generating\neffects and databases with isolated vocals with correspond-\ning backgrounds.\nMeasuring the relatability (Table 2, row J) of a singer\nis a quality that is relative to the listener, rather than abso-\nlute. Factors that could affect a singer’s relatability could\ninclude the age, gender, culture or language of the singer\nrelative to the listener, which might require automatic iden-\ntiﬁcation of each of these attributes of the singer.\nLyric intelligibility (Table 2, row L) has not been well\nstudied, and also presents a novel challenge [7]. This prob-\nlem does not necessarily directly require lyric transcrip-\ntion, and may be able to be determined from qualities of518 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Vocal Semantic Categories Borda score p-value\nA Singing skill 3423 <0.001\nB How well the voice ﬁts or matches the rest of the music 3380 <0.001\nC Lyrical skill / cleverness / wit 3145 <0.001\nD The meaning, or the “message” of the words 3038 0.048\nE Authenticity / “realness” 2884 <0.001\nF Uniqueness 2780 <0.001\nG If the voice is emotional 2771 0.006\nH V oice strength / intensity / effort 2721 1.000\nIIf the voice sounds natural 2480 1.000\nJ Being able to relate 2256 1.000\nK If the voice is melodic 2202 1.000\nL Whether or not you can understand the lyrics 2056 1.000\nM If it’s whiny or nasal 1801 1.000\nN Whether or not there’s screaming 1771 1.000\nO The overall pitch, or the range of the pitch 1400 1.000\nP Whether or not there are lyrics 1250 1.000\nQ Whether it has production effects on it, like autotune 1230 1.000\nR Profanity, explicit lyrics 1086 1.000\nS Whether or not there is rapping 909 1.000\nTable 2 : V ocal-speciﬁc semantic categories from Study 1, ordered by rankings from Study 2. Columns 2 and 3 show the\nBorda scores and p-values. Statistically signiﬁcant p-values are shown in bold. p-values of 1.000 indicate that the ranking\nis no different from random.\nthe audio. Similarly, determining whether a singing voice\ncontains lyrics or is wordless has not been studied (Table 2,\nrow P).\nAutomatic lyric transcription has been studied [11, 13]\nbut is not yet solved, and would power the automatic esti-\nmation of many of these vocal attributes. For lyric-related\nterms, given textual lyrics, while some attributes would be\nrelatively simple to estimate (e.g. whether or not there is\nprofanity), others present interesting NLP challenges, such\nas estimating whether the lyrics are “clever” or are “mean-\ningful” (Table 2, rows R, C, and D).\n5. DISCUSSION AND CONCLUSIONS\nWhile our analyses of playlist titles and search queries\nwere inconclusive, we show evidence that English-\nspeaking respondents from the U.S. and Canada clearly\nindicated that the voice is a salient component of music.\nSpeciﬁcally, Spotify users were asked what they notice\nabout music while listening. Despite the unassuming na-\nture of the question, our results showed that the voice was\nindeed salient among the group of reported musical at-\ntributes. Furthermore, users ranked the voice as the second\nmost important component to their musical preference, af-\nter emotions.\nOur results have a number of implications. With re-\ngards to MIR research speciﬁcally, our results suggest that\nthe voice and lyrics are indeed relevant attributes that war-\nrant further study. While individuals may not necessarily\nwant or know how to describe vocals themselves, i.e. in\ntheir playlists or search queries, surveying listeners di-\nrectly does indicate that they ﬁnd vocals to be important.As such, clarifying how the voice relates to music prefer-\nence is an important topic for future research.\nSecondly, users indicated that the ability of a song to\nevoke emotions was the most important factor. This con-\nﬁrms ﬁndings in prior research of the relevance of emo-\ntional content in music, and how it is linked to musical\npreference, e.g. [10]. Therefore, examining how music af-\nfects the emotions of listeners remains an important theme.\nInterestingly, while genre was the most frequent term used\nto label playlists or search for music, respondents did not\nrank the speciﬁc genre as important relative to the other at-\ntributes. Understanding why this is the case warrants fur-\nther study.\nMore relevant to our hypothesis, is that the vocals and\nthe lyrics of a song were ranked second and third by re-\nspondents who were directly asked what components of\nmusic are important to their preferences. Therefore the\nlink between emotions perceived in the voice and lyrics,\nand the emotions felt in listeners, is very relevant to ques-\ntions of music preference. Clariﬁcation of these links was\nout of scope in these studies, and could be addressed in\nfuture research.\nLastly, we show the relevance of explicitly collected\ndata that might guide future research. While we showed\ninconclusive ﬁndings regarding the prevalence of vocals in\nimplicit data, we did show that the unique effect of vocals\non music preference may be observed using survey data.\nAs such, explicit data-gathering techniques often found in\nthe social sciences, as well as collaborations with social\nscientists, may be of great use to MIR researchers.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 5196. REFERENCES\n[1] Jorge L Armony, William Aub ´e, Arafat Angulo-\nPerkins, Isabelle Peretz, and Luis Concha. The speci-\nﬁcity of neural responses to music and their relation\nto voice processing: An fmri-adaptation study. Neuro-\nscience letters , 593:35–39, 2015.\n[2] Pascal Belin, Robert J Zatorre, Philippe Lafaille, Pierre\nAhad, and Bruce Pike. V oice-selective areas in human\nauditory cortex. Nature , 403(6767):309, 2000.\n[3] Jean C de Borda. M ´emoire sur les ´elections au scrutin.\nHistoire de l’Academie Royale des Sciences , 1781.\n[4] Cynthia Dwork, Ravi Kumar, Moni Naor, and Dan-\ndapani Sivakumar. Rank aggregation methods for the\nweb. In Proceedings of the 10th international confer-\nence on World Wide Web , pages 613–622. ACM, 2001.\n[5] Sean Hutchins and Sylvain Moreno. The linked dual\nrepresentation model of vocal perception and produc-\ntion. Frontiers in psychology , 4:825, 2013.\n[6] Sean Michael Hutchins and Isabelle Peretz. A frog in\nyour throat or in your ear? searching for the causes\nof poor singing. Journal of Experimental Psychology:\nGeneral , 141(1):76, 2012.\n[7] Karim M Ibrahim, David Grunberg, Kat Agres, Chi-\ntralekha Gupta, and Ye Wang. Intelligibility of sung\nlyrics: A pilot study. International Society for Music\nInformation Retrieval Conference, 2017.\n[8] Patrik N. Juslin and Petri Laukka. Communication\nof emotions in vocal expression and musical perfor-\nmance: Different channels, same code? Psychological\nBulletin , 129:770–814, 2003.\n[9] Raivo Kolde, Sven Laur, Priit Adler, and Jaak Vilo.\nRobust rank aggregation for gene list integration and\nmeta-analysis. Bioinformatics , 28(4):573–580, 2012.\n[10] Carol Lynne Krumhansl. Listening niches across a cen-\ntury of popular music. Frontiers in psychology , 8:431,\n2017.\n[11] Anna M Kruspe and IDMT Fraunhofer. Retrieval\nof textual song lyrics from sung inputs. In INTER-\nSPEECH , pages 2140–2144, 2016.\n[12] Micheline Lesaffre, Liesbeth De V oogdt, Marc Leman,\nBernard De Baets, Hans De Meyer, and Jean-Pierre\nMartens. How potential users of music search and re-\ntrieval systems describe the semantic quality of music.\nJournal of the Association for Information Science and\nTechnology , 59(5):695–707, 2008.\n[13] Matt McVicar, Daniel PW Ellis, and Masataka Goto.\nLeveraging repetition for improved automatic lyric\ntranscription in popular music. In Acoustics, Speech\nand Signal Processing (ICASSP), 2014 IEEE Interna-\ntional Conference on , pages 3117–3121. IEEE, 2014.[14] George A Miller. A psychological method to investi-\ngate verbal concepts. Journal of mathematical psychol-\nogy, 6(2):169–191, 1969.\n[15] Stephen W Porges. The polyvagal theory: phylogenetic\nsubstrates of a social nervous system. International\nJournal of Psychophysiology , 42(2):123–146, 2001.\n[16] Karen Sparck Jones. A statistical interpretation of term\nspeciﬁcity and its application in retrieval. Journal of\ndocumentation , 28(1):11–21, 1972.\n[17] Michael W Weiss, E Glenn Schellenberg, Sandra E\nTrehub, and Emily J Dawber. Enhanced processing of\nvocal melodies in childhood. Developmental Psychol-\nogy, 51(3):370, 2015.\n[18] Michael W Weiss, Sandra E Trehub, and E Glenn\nSchellenberg. Something in the way she sings: En-\nhanced memory for vocal melodies. Psychological Sci-\nence, 23(10):1074–1078, 2012.520 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "The NES Music Database: A multi-instrumental dataset with expressive performance attributes.",
        "author": [
            "Chris Donahue",
            "Huanru Henry Mao",
            "Julian J. McAuley"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492455",
        "url": "https://doi.org/10.5281/zenodo.1492455",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/265_Paper.pdf",
        "abstract": "Existing research on music generation focuses on composition, but often ignores the expressive performance characteristics required for plausible renditions of resultant pieces. In this paper, we introduce the Nintendo Entertainment System Music Database (NES-MDB), a large corpus allowing for separate examination of the tasks of composition and performance. NES-MDB contains thousands of multi-instrumental songs composed for playback by the compositionally-constrained NES audio synthesizer. For each song, the dataset contains a musical score for four instrument voices as well as expressive attributes for the dynamics and timbre of each voice. Unlike datasets comprised of General MIDI files, NES-MDB includes all of the information needed to render exact acoustic performances of the original compositions. Alongside the dataset, we provide a tool that renders generated compositions as NESstyle audio by emulating the device's audio processor. Additionally, we establish baselines for the tasks of composition, which consists of learning the semantics of composing for the NES synthesizer, and performance, which involves finding a mapping between a composition and realistic expressive attributes.",
        "zenodo_id": 1492455,
        "dblp_key": "conf/ismir/DonahueMM18",
        "keywords": [
            "Nintendo Entertainment System Music Database (NES-MDB)",
            "large corpus",
            "separate examination",
            "composition and performance",
            "compositionally-constrained NES audio synthesizer",
            "musical score",
            "expressive attributes",
            "exact acoustic performances",
            "General MIDI files",
            "emulating the devices audio processor"
        ],
        "content": "THE NES MUSIC DATABASE: A MULTI-INSTRUMENTAL DATASET\nWITH EXPRESSIVE PERFORMANCE ATTRIBUTES\nChris Donahue\nUC San Diego\ncdonahue@ucsd.eduHuanru Henry Mao\nUC San Diego\nhhmao@ucsd.eduJulian McAuley\nUC San Diego\njmcauley@ucsd.edu\nABSTRACT\nExisting research on music generation focuses on compo-\nsition, but often ignores the expressive performance char-\nacteristics required for plausible renditions of resultant\npieces. In this paper, we introduce the Nintendo Entertain-\nment System Music Database (NES-MDB), a large corpus\nallowing for separate examination of the tasks of composi-\ntion and performance. NES-MDB contains thousands of\nmulti-instrumental songs composed for playback by the\ncompositionally-constrained NES audio synthesizer. For\neach song, the dataset contains a musical score for four\ninstrument voices as well as expressive attributes for the\ndynamics and timbre of each voice. Unlike datasets com-\nprised of General MIDI ﬁles, NES-MDB includes all of the\ninformation needed to render exact acoustic performances\nof the original compositions. Alongside the dataset, we\nprovide a tool that renders generated compositions as NES-\nstyle audio by emulating the device’s audio processor. Ad-\nditionally, we establish baselines for the tasks of compo-\nsition, which consists of learning the semantics of com-\nposing for the NES synthesizer, and performance, which\ninvolves ﬁnding a mapping between a composition and re-\nalistic expressive attributes.\n1. INTRODUCTION\nThe problem of automating music composition is a chal-\nlenging pursuit with the potential for substantial cultural\nimpact. While early systems were hand-crafted by musi-\ncians to encode musical rules and structure [25], recent at-\ntempts view composition as a statistical modeling problem\nusing machine learning [3]. A major challenge to casting\nthis problem in terms of modern machine learning meth-\nods is building representative datasets for training. So far,\nmost datasets only contain information necessary to model\nthe semantics of music composition, and lack details about\nhow to translate these pieces into nuanced performances.\nAs a result, demonstrations of machine learning systems\ntrained on these datasets sound rigid and deadpan. The\ndatasets that do contain expressive performance character-\nc\rChris Donahue, Huanru Henry Mao, Julian McAuley.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Chris Donahue, Huanru Henry\nMao, Julian McAuley. “The NES Music Database: A multi-instrumental\ndataset with expressive performance attributes”, 19th International Soci-\nety for Music Information Retrieval Conference, Paris, France, 2018.istics predominantly focus on solo piano [10,27,32] rather\nthan multi-instrumental music.\nA promising source of multi-instrumental music that\ncontains both compositional and expressive characteris-\ntics is music from early videogames. There are nearly\n14001unique games licensed for the Nintendo Entertain-\nment System (NES), all of which include a musical sound-\ntrack. The technical constraints of the system’s audio pro-\ncessing unit (APU) impose a maximum of four simulta-\nneous monophonic instruments. The machine code for\nthe games preserves the exact expressive characteristics\nneeded to perform each piece of music as intended by the\ncomposer. All of the music was composed in a limited time\nperiod and, as a result, is more stylistically cohesive than\nother large datasets of multi-instrumental music. More-\nover, NES music is celebrated by enthusiasts who continue\nto listen to and compose music for the system [6], appreci-\nating the creativity that arises from resource limitations.\nIn this work, we introduce NES-MDB, and formalize\ntwo primary tasks for which the dataset serves as a large\ntest bed. The ﬁrst task consists of learning the semantics of\ncomposition on a separated score , where individual instru-\nment voices are explicitly represented. This is in contrast\nto the common blended score approach for modeling poly-\nphonic music, which examines reductions of full scores.\nThe second task consists of mapping compositions onto\nsets of expressive performance characteristics. Combining\nstrategies for separated composition and expressive perfor-\nmance yields an effective pipeline for generating NES mu-\nsicde novo . We establish baseline results and reproducible\nevaluation methodology for both tasks. A further contri-\nbution of this work is a library that converts between NES\nmachine code (allowing for realistic playback) and repre-\nsentations suitable for machine learning.2\n2. BACKGROUND AND TASK DESCRIPTIONS\nStatistical modeling of music seeks to learn the distribution\nP(music )from human compositions c\u0018P(music )in\na datasetM. If this distribution could be estimated accu-\nrately, a new piece could be composed simply by sampling.\nSince the space of potential compositions is exponentially\nlarge, to make sampling tractable, one usually assumes a\nfactorized distribution. For monophonic sequences, which\nconsist of no more than one note at a time, the probability\n1Including games released only on the Japanese version of the console\n2https://github.com/chrisdonahue/nesmdb475(a) Blended score (degenerate)\n(b) Separated score (melodic voices top, percussive voice bottom )\n(c) Expressive score (includes dynamics and timbral changes)\nFigure 1 : Three representations (rendered as piano rolls)\nfor a segment of Ending Theme from Abadox (1989) by\ncomposer Kiyohiro Sada. The blended score (Fig. 1a),\nused in prior polyphonic composition research, is degen-\nerate when multiple voices play the same note.\nof a sequence c(length T) might be factorized as\nP(c) =P(n1)\u0001P(n2jn1)\u0001: : :\u0001P(nTjnt<T):(1)\n2.1 Blended composition\nWhile Eq. 1 may be appropriate for modeling compositions\nfor monophonic instruments, in this work we are interested\nin the problem of multi-instrumental polyphonic composi-\ntion, where multiple monophonic instrument voices may\nbe sounding simultaneously. Much of the prior research\non this topic [2, 5, 17] represents music in a blended score\nrepresentation. A blended score Bis a sparse binary ma-\ntrix of size N\u0002T, where Nis the number of possible\nnote values, and B[n; t] = 1 if any voice is playing note\nnat timestep tor0otherwise (Fig. 1a). Often, Nis con-\nstrained to the 88keys on a piano keyboard, and Tis deter-\nmined by some subdivision of the meter, such as sixteenth\nnotes. When polyphonic composition cis represented by\nB, statistical models often factorize the distribution as a\nsequence of chords , the columns Bt:\nP(c) =P(B1)\u0001P(B2jB1)\u0001: : :\u0001P(BTjBt<T):(2)\nThis representation simpliﬁes the probabilistic frame-\nwork of the task, but it is problematic for music with mul-\ntiple instruments (such as the music in NES-MDB). Re-\nsultant systems must provide an additional mechanism for\nassigning notes of a blended score to instrument voices,\nor otherwise render the music on polyphonic instruments\nsuch as the piano.2.2 Separated composition\nGiven the shortcomings of the blended score, we might\nprefer models which operate on a separated score repre-\nsentation (Fig. 1b). A separated score Sis a matrix of size\nV\u0002T, where Vis the number of instrument voices, and\nS[v; t] =n, the note nplayed by voice vat timestep t. In\nother words, the format encodes a monophonic sequence\nfor each instrument voice. Statistical approaches to this\nrepresentation can explicitly model the relationships be-\ntween various instrument voices by\nP(c) =TY\nt=1VY\nv=1P(Sv;tjSv;^t6=t; S^v6=v;8^t): (3)\nThis formulation explicitly models the dependencies\nbetween Sv;t, voice vat time t, and every other note in\nthe score. For this reason, Eq. 3 more closely resem-\nbles the process by which human composers write multi-\ninstrumental music, incorporating temporal and contrapun-\ntal information. Another beneﬁt is that resultant models\ncan be used to harmonize with existing musical material,\nadding voices conditioned on existing ones. However, any\nnon-trivial amount of temporal context introduces high-\ndimensional interdependencies, meaning that such a for-\nmulation would be challenging to sample from. As a con-\nsequence, solutions are often restricted to only take past\ntemporal context into account, allowing for simple and ef-\nﬁcient ancestral sampling (though Gibbs sampling can also\nbe used to sample from Eq. 3 [13, 16]).\nMost existing datasets of multi-instrumental music have\nuninhibited polyphony, causing a separated score represen-\ntation to be inappropriate. However, the hardware con-\nstraints of the NES APU impose a strict limit on the num-\nber of voices, making the format ideal for NES-MDB.\n2.3 Expressive performance\nGiven a piece of a music, a skilled performer will em-\nbellish the piece with expressive characteristics , altering\nthe timing and dynamics to deliver a compelling rendition.\nWhile a few instruments have been augmented to capture\nthis type of information symbolically (e.g. a Disklavier),\nit is rarely available for examination in datasets of multi-\ninstrumental music. Because NES music is comprised of\ninstructions that recreate an exact rendition of each piece,\nexpressive characteristics controlling the velocity and tim-\nbre of each voice are available in NES-MDB (details in\nSection 3.1). Thus, each piece can be represented as an\nexpressive score (Fig. 1c), the union of its separated score\nand expressive characteristics.\nWe consider the task of mapping a composition conto\nexpressive characteristics e. Hence, we would like to\nmodel P(ejc), and the probability of a piece of music\nP(m)can be expressed as P(ejc)\u0001P(c), where P(c)is\nfrom Eq. 3. This allows for a convenient pipeline for music\ngeneration where a piece of music is ﬁrst composed with\nbinary amplitudes and then mapped to realistic dynamics,\nas if interpreted by a performer.476 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018# Games 397\n# Composers 296\n# Songs 5;278\n# Songs w/ length >10s 3;513\n# Notes 2;325;636\nDataset length 46:1hours\nP(Pulse 1 On ) 0:861\nP(Pulse 2 On ) 0:838\nP(Triangle On ) 0:701\nP(Noise On ) 0:390\nAverage polyphony 2:789\nTable 1 : Basic dataset information for NES-MDB.\n2.4 Task summary\nIn summary, we propose three tasks for which NES-MDB\nserves as a large test bed. A pairing of two models that\naddress the second and third tasks can be used to generate\nnovel NES music.\n1. The blended composition task (Eq. 2) models the\nsemantics of blended scores (Fig. 1a). This task is\nmore useful for benchmarking new algorithms than\nfor NES composition.\n2. The separated composition task consists of model-\ning the semantics of separated scores (Fig. 1b) using\nthe factorization from Eq. 3.\n3. The expressive performance task seeks to map sep-\narated scores to expressive characteristics needed to\ngenerate an expressive score (Fig. 1c).\n3. DATASET DESCRIPTION\nThe NES APU consists of ﬁve monophonic instruments:\ntwo pulse wave generators (P1/P2), a triangle wave gen-\nerator (TR), a noise generator (NO), and a sampler which\nallows for playback of audio waveforms stored in mem-\nory. Because the sampler may be used to play melodic or\npercussive sounds, its usage is compositionally ambiguous\nand we exclude it from our dataset.\nIn raw form, music for NES games exists as machine\ncode living in the read-only memory of cartridges, entan-\ngled with the rest of the game logic. An effective method\nfor extracting a musical transcript is to emulate the game\nand log the timing and values of writes to the APU regis-\nters. The video game music (VGM) format3was designed\nfor precisely this purpose, and consists of an ordered list\nof writes to APU registers with 44:1 kHz timing resolu-\ntion. An online repository4contains over 400NES games\nlogged in this format. After removing duplicates, we split\nthese games into distinct training, validation and test sub-\nsets with an 8:1:1ratio, ensuring that no composer appears\nin two of the subsets. Basic statistics of the dataset appear\nin Table 1.\n3http://vgmrips.net/wiki/VGM_Specification\n4http://vgmrips.net/packs/chip/nes-apu3.1 Extracting expressive scores\nGiven the VGM ﬁles, we emulate the functionality of the\nAPU to yield an expressive score (Fig. 1c) at a tempo-\nral discretization of 44:1 kHz . This rate is unnecessarily\nhigh for symbolic music, so we subsequently downsam-\nple the scores.5Because the music has no explicit tempo\nmarkings, we accommodate a variety of implicit tempos by\nchoosing a permissive downsampling rate of 24 Hz . By re-\nmoving dynamics, timbre, and voicing at each timestep, we\nderive separated score (Fig. 1b) and blended score (Fig. 1a)\nversions of the dataset.\nInstrument Note Velocity Timbre\nPulse 1(P1)f0;32; : : : ; 108g [0;15] [0 ;3]\nPulse 2(P2)f0;32; : : : ; 108g [0;15] [0 ;3]\nTriangle (TR)f0;21; : : : ; 108g\nNoise (NO)f0;1; : : : ; 16g [0;15] [0 ;1]\nTable 2 : Dimensionality for each timestep of the expres-\nsive score representation (Fig. 1c) in NES-MDB.\nIn Table 2, we show the dimensionality of the instru-\nment states at each timestep of an expressive score in NES-\nMDB. We constrain the frequency ranges of the melodic\nvoices (pulse and triangle generators) to the MIDI notes\non an 88-key piano keyboard ( 21through 108inclusive,\nthough the pulse generators cannot produce pitches below\nMIDI note 32). The percussive noise voice has 16possible\n“notes” (these do not correspond to MIDI note numbers)\nwhere higher values have more high-frequency noise. For\nall instruments, a note value of 0indicates that the instru-\nment is not sounding (and the corresponding velocity will\nbe0). When sounding, the pulse and noise generators have\n15non-linear velocity values, while the triangle generator\nhas no velocity control beyond on or off.\nAdditionally, the pulse wave generators have 4possi-\nble duty cycles (affecting timbre), and the noise generator\nhas a rarely-used mode where it instead produces metallic\ntones. Unlike for velocity, a timbre value of 0corresponds\nto an actual timbre setting and does not indicate that an in-\nstrument is muted. In total, the pulse, triangle and noise\ngenerators have state spaces of sizes 4621 ,89, and 481\nrespectively—around 40bits of information per timestep\nfor the full ensemble.\n4. EXPERIMENTS AND DISCUSSION\nBelow, we describe our evaluation criteria for experiments\nin separated composition and expressive performance. We\npresent these results only as statistical baselines for com-\nparison; results do not necessarily reﬂect a model’s ability\nto generate compelling musical examples.\nNegative log-likelihood and Accuracy Negative log-\nlikelihood (NLL) is the (log of the) likelihood that a model\nassigns to unseen real data (as per Eq. 3). A low NLL aver-\naged across unseen data may indicate that a model captures\n5We also release NES-MDB in MIDI format with no downsamplingProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 477semantics of the data distribution. Accuracy is deﬁned as\nthe proportion of timesteps where a model’s prediction is\nequal to the actual composition. We report both measures\nfor each voice, as well as aggregations across all voices by\nsumming (for NLL) and averaging (for accuracy).\nPoints of Interest (POI). Unlike other datasets of sym-\nbolic music, NES-MDB is temporally-discretized at a\nhigh, ﬁxed rate ( 24 Hz ), rather than at a variable rate de-\npending on the tempo of the music. As a consequence,\nany given voice has around an 83% chance of playing the\nsame note as that voice at the previous timestep. Accord-\ningly, our primary evaluation criteria focuses on musically-\nsalient points of interest (POIs), timesteps at which a voice\ndeviates from the previous timestep (the beginning or end\nof a note). This evaluation criterion is mostly invariant to\nthe rate of temporal discretization.\n4.1 Separated composition experiments\nFor separated composition, we evaluate the performance\nof several baselines and compare them to a cutting edge\nmethod. Our simplest baselines are unigram and additive-\nsmoothed bigram distributions for each instrument. The\npredictions of such models are trivial; the unigram model\nalways predicts “no note” and the bigram model always\npredicts “last note”. The respective accuracy of these mod-\nels,37% and83%, reﬂect the proportion of the timesteps\nthat are silent (unigram) or identical to the last timestep (bi-\ngram). However, if we evaluate these models only at POIs,\ntheir performance is substantially worse ( 4%and0%).\nWe also measure performance of recurrent neural net-\nworks (RNNs) at modeling the voices independently. We\ntrain a separate RNN (either a basic RNN cell or an\nLSTM cell [15]) on each voice to form our RNN Soloists\nand LSTM Soloists baselines. We compare these to\nLSTM Quartet, a model consisting of a single LSTM that\nprocesses all four voices and outputs an independent soft-\nmax over each note category, giving the model full con-\ntext of the composition in progress. All RNNs have 2\nlayers and 256 units, except for soloists which have 64\nunits each, and we train them with 512steps of unrolling\nfor backpropagation through time. We train all models to\nminimize NLL using the Adam optimizer [19] and employ\nearly stopping based on the NLL of the validation set.\nWhile the DeepBach model [13] was designed for mod-\neling the chorales of J.S. Bach, the four-voice structure of\nthose chorales is shared by NES-MDB, making the model\nappropriate for evaluation in our setting. DeepBach em-\nbeds each timestep of the four-voice score and then pro-\ncesses these embeddings with a bidirectional LSTM to ag-\ngregate past and future musical context. For each voice,\nthe activations of the bidirectional LSTM are concatenated\nwith an embedding of all of the other voices, providing\nthe model with a mechanism to alter its predictions for any\nvoice in context of the others at that timestep. Finally, these\nmerged representations are concatenated to an independent\nsoftmax for each of the four voices. Results for DeepBach\nand our baselines appear in Table 3.\nAs expected, the performance of all models at POIs isworse than the global performance. DeepBach achieves\nsubstantially better performance at POIs than the other\nmodels, likely due to its bidirectional processing which al-\nlows the model to “peek” at future notes. The LSTM Quar-\ntet model is attractive because, unlike DeepBach, it permits\nefﬁcient ancestral sampling. However, we observe qualita-\ntively that samples from this model are musically unsatis-\nfying. While the performance of the soloists is worse than\nthe models which examine all voices, the superior perfor-\nmance of the LSTM Soloists to the RNN Soloists suggests\nthat LSTMs may be beneﬁcial in this context.\nWe also experimented with artiﬁcially emphasizing\nPOIs during training, however we found that resultant\nmodels produced unrealistically sporadic music. Based\non this observation, we recommend that researchers who\nstudy NES-MDB always train models with unbiased em-\nphasis, in order to effectively capture the semantics of the\nparticular temporal discretization.\n4.2 Expressive performance experiments\nThe expressive performance task consists of learning a\nmapping from a separated score to suitable expressive\ncharacteristics. Each timestep of a separated score in NES-\nMDB has note information (random variable N) for the\nfour instrument voices. An expressive score addition-\nally has velocity ( V) and timbre ( T) information for P1,\nP2, and NO but not TR. We can express the distribution\nof performance characteristics given the composition as\nP(V; TjN). Some of our proposed solutions factorize\nthis further into a conditional autoregressive formulationQT\nt=1P(Vt; TtjN; V ^t<t; T^t<t), where the model has ex-\nplicit knowledge of its decisions for velocity and timbre at\nearlier timesteps.\nNotes \nLast velocity \nLast timbre \nLSTM Bidirectional \nLSTM \nDense \nConcatenate \nConcatenate \nFigure 2 : LSTM Note+Auto expressive performance\nmodel that observes both the score and its prior output.\nUnlike for separated composition, there are no well-\nestablished baselines for multi-instrumental expressive\nperformance, and thus we design several approaches.\nFor the autoregressive formulation, our most-sophisticated\nmodel (Fig. 2) uses a bidirectional LSTM to process the\nseparated score, and a forward-directional LSTM for the\nautoregressive expressive characteristics. The represen-478 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Negative log-likelihood Accuracy\nSingle voice Aggregate Single voice Aggregate\nModel P1 P2 TR NO POI All P1 P2 TR NO POI All\nRandom 4:36 4 :36 4 :49 2 :83 16 :04 16 :04:013 :013 :011 :059 :024 :024\nUnigram 4:00 3 :77 3 :01 2 :50 13 :27 11 :53:020 :022 :057 :061 :040 :369\nBigram 4:91 4 :93 4 :15 3 :52 17 :50 3 :63 :000 :000 :000 :000 :000 :831\nRNN Soloists 4:92 4 :90 3 :59 2 :23 15 :64 3 :11 :000 :000 :004 :183 :047 :830\nLSTM Soloists 4:60 4 :30 3 :01 1 :91 13 :82 2 :70 :014 :008 :125 :246 :098 :838\nLSTM Quartet 3:87 3 :71 2 :45 1 :62 11 :65 2 :21 :028 :031 :294 :449 :201 :854\nDeepBach [13] 0:82 1 :01 0 :63 0 :83 3 :28 0 :75 :781 :729 :784 :748 :761 :943\nTable 3 : Results for separated composition experiments. For each instrument, negative log-likelihood and accuracy are cal-\nculated at points of interest (POIs). We also calculate aggregate statistics at POIs and globally (All). While DeepBach [13]\nachieves the best statistical performance, it uses future context and hence is more expensive to sample from.\nNegative log-likelihood Accuracy\nSingle voice Aggregate Single voice Aggregate\nModel VP1 VP2VNO TP1 TP2 POI All VP1 VP2VNO TP1 TP2 POI All\nRandom 2:77 2 :77 2 :77 1 :39 1 :39 11 :09 11 :09:062 :062 :062 :250 :250 :138 :138\nUnigram 2:87 2 :89 3 :04 1 :35 1 :33 11 :47 9 :65 :020 :022 :061 :006 :004 :023 :309\nBigram 2:82 2 :85 2 :78 4 :27 4 :27 17 :00 4 :57 :000 :000 :000 :000 :000 :000 :741\nMultiReg Note 2:74 2 :72 2 :23 1 :27 1 :18 10 :13 8 :49 :106 :122 :292 :406 :507 :287 :359\nMultiReg Note+Auto 2:58 2 :56 2 :04 2 :90 2 :48 12 :56 4 :32 :073 :100 :345 :071 :096 :137 :752\nLSTM Note 2:68 2 :63 2 :09 1 :32 1 :21 9 :94 8 :28 :115 :134 :305 :456 :532 :308 :365\nLSTM Note+Auto 1:93 1 :89 1 :99 2 :23 1 :89 9 :93 3 :42 :305 :321 :386 :241 :432 :337 :774\nTable 4 : Results for expressive performance experiments evaluated at points of interest (POI). Results are broken down by\nexpression category (e.g. VNOis noise velocity, TP1is pulse 1 timbre) and aggregated at POIs and globally (All).\ntations from the composition and autoregressive modules\nare merged and processed by an additional dense layer be-\nfore projecting to six softmaxes, one for each of VP1,VP2,\nVNO,TP1,TP2, and TNO. We compare this model (LSTM\nNote+Auto) to a version which removes the autoregressive\nmodule and only sees the separated score (LSTM Note).\nWe also measure performance of simple multinomial\nregression baselines. The non-autoregressive baseline\n(MultiReg Note) maps the concatenation of NP1,NP2,\nNTR, andNNOdirectly to the six categorical outputs repre-\nsenting velocity and timbre (no temporal context). An au-\ntoregressive version of this model (MultiReg Note+Auto)\ntakes additional inputs consisting of the previous timestep\nfor the six velocity and timbre categories. Additionally, we\nshow results for simple baselines (per-category unigram\nand bigram distributions) which do not consider N. Be-\ncause the noise timbre ﬁeld TNOis so rarely used (less than\n0:2%of all timesteps), we exclude it from our quantitative\nevaluation. Results are shown in Table 4.\nSimilarly to the musical notes in the separated compo-\nsition task (Section 4.1), the high rate of NES-MDB re-\nsults in substantial redundancy across timesteps. Averaged\nacross all velocity and timbre categories, any of these cat-\negories at a given timestep has a 74% chance of having the\nsame value as the previous timestep.\nThe performance of the LSTM Note model is com-\nparable to that of the LSTM Note+Auto model at POIs,\nhowever the global performance of the LSTM Note+Auto\nmodel is substantially better. Intuitively, this suggests that\nthe score is useful for knowing when to change, while\nthe past velocity and timbre values are useful for knowingModel NES-MDB PM NH MD BC\nRandom 61:00 61 :00 61 :00 61 :00 61 :00\nNote 1-gram [2] 8:71 11 :05 10 :25 11 :51 11 :06\nChord 1-gram [2] 8:76 27 :64 5 :94 19 :03 12 :22\nGMM [2] 12:86 15 :84 7 :87 12 :20 11 :90\nNADE [2] 8:53 10 :28 5 :48 10 :06 7 :19\nRNN [2] 3:04 8 :37 4 :46 8 :13 8 :71\nRNN-NADE [2] 2:62 7 :48 2 :91 6 :74 5 :83\nLSTM 2:54 8 :31 3 :49 6 :35 8 :72\nLSTM-NADE [17] 2:48 7 :36 2 :02 5 :02 6 :00\nTable 5 : Negative log-likelihoods for various models on\nthe blended score format (Fig. 1a, Eq. 2) of NES-MDB.\nWe also show results for Piano-midi.de (PM), Nottingham\n(NH), MuseData (MD), and the chorales of J.S. Bach (BC).\nwhat value to output next. Interestingly, the MultiReg Note\nmodel has better performance at POIs than the MultiReg\nNote+Auto model. The latter overﬁt more quickly which\nmay explain its inferior performance despite the fact that it\nsees strictly more information than the note-only model.\n4.3 Blended composition experiments\nIn Table 5, we report the performance of several models\non the blended composition task (Eq. 2). In NES-MDB,\nblended scores consist of 88possible notes with a maxi-\nmum of three simultaneous voices (noise generator is dis-\ncarded). This task, standardized in [2], does not preserve\nthe voicing of the score, and thus it is not immediately\nuseful for generating NES music. Nevertheless, modeling\nblended scores of polyphonic music has become a standard\nbenchmark for sequential models [5, 18], and NES-MDBProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 479may be useful as a larger dataset in the same format.\nIn general, models assign higher likelihood to NES-\nMDB than the four other datasets after training. As with\nour other two tasks, this is likely due to the fact that NES-\nMDB is sampled at a higher temporal rate, and thus the av-\nerage deviation across timesteps is lower. Due to its large\nsize, a beneﬁt of examining NES-MDB in this context is\nthat sequential models tend to take longer to overﬁt the\ndataset than they do for the other four. We note that our im-\nplementations of these models may deviate slightly from\nthose of the original authors, though our models achieve\ncomparable results to those reported in [2,17] when trained\non the original datasets.\n5. RELATED WORK\nThere are several popular datasets commonly used in sta-\ntistical music composition. A dataset consisting of the en-\ntirety of J.S. Bach’s four-voice chorales has been exten-\nsively studied under the lenses of algorithmic composition\nand reharmonization [1, 2, 13, 14]. Like NES-MDB, this\ndataset has a ﬁxed number of voices and can be repre-\nsented as a separated score (Fig. 1b), however it is small in\nsize ( 389chorales) and lacks expressive information. An-\nother popular dataset is Piano-midi.de, a corpus of clas-\nsical piano from various composers [27]. This dataset\nhas expressive timing and dynamics information but has\nheterogeneous time periods and only features solo piano\nmusic. Alongside Bach’s chorales and the Piano-midi.de\ndataset, Boulanger-Lewandowski et al. (2012) standard-\nized the Nottingham collection of folk tunes and MuseData\nlibrary of orchestral and piano classical music into blended\nscore format (Fig. 1a).\nSeveral other symbolic datasets exist containing both\ncompositional and expressive characteristics. The Mag-\naloff Corpus [10] consists of Disklavier recordings of a\nprofessional pianist playing the entirety of Chopin’s solo\npiano works. The Lakh MIDI dataset [28] is the largest\ncorpus of symbolic music assembled to date with nearly\n200k songs. While substantially larger than NES-MDB,\nthe dataset has unconstrained polyphony, inconsistent ex-\npressive characteristics, and encompasses a wide variety of\ngenres, instruments and time periods. Another paper trains\nneural networks on transcriptions of video game music [9],\nthough their dataset only includes a handful of songs.\n5.1 Statistical composition\nWhile most of the early research in algorithmic music\ncomposition focused on expert systems [25], statistical ap-\nproaches have since become the predominant approach.\nMozer (1994) trained RNNs on monophonic melodies us-\ning a formulation similar to Eq. 1, ﬁnding the composed re-\nsults to compare favorably to those from a trigram model.\nOthers have also explored monophonic melody generation\nwith RNNs [8, 26]. Boulanger-Lewandowski et al. (2012)\nstandardize the polyphonic prediction task for blended\nscores (Eq. 2), measuring performance of a multitude of\nclassical baselines against RNNs [30], restricted Boltz-mann machines [34], and NADEs [21] on polyphonic mu-\nsic datasets. Several papers [5, 17, 35] directly compare to\ntheir results. Statistical models of music have also been\nemployed as symbolic priors to assist music transcription\nalgorithms [2, 4, 24].\nProgressing towards models that assist humans in com-\nposition, many researchers study models to create new\nharmonizations for existing musical material. Allan and\nWilliams (2005) train HMMs to create new harmonizations\nfor Bach chorales [1]. Hadjeres et al. (2017) train a bidi-\nrectional RNN model to consider past and future temporal\ncontext (Eq. 3) [13]. Along with [16, 31], they advocate\nfor the usage of Gibbs sampling to generate music from\ncomplex graphical models.\n5.2 Statistical performance\nMusicians perform music expressively by interpreting a\nperformance with appropriate dynamics, timing and ar-\nticulation. Computational models of expressive music\nperformance seek to automatically assign such attributes\nto a score [36]. We point to several extensive surveys\nfor information about the long history of rule-based sys-\ntems [7, 12, 20, 36].\nSeveral statistical models of expressive performance\nhave also been proposed. Raphael (2010) learns a graph-\nical model that automates an accompanying orchestra for\na soloist, operating on acoustic features rather than sym-\nbolic [29]. Flossmann et al. (2013) build a system to con-\ntrol velocity, articulation and timing of piano performances\nby learning a graphical model from a large symbolic cor-\npus of human performances [11]. Xia et al. (2015) model\nthe expressive timing and dynamics of piano duet perfor-\nmances using spectral methods [37]. Two end-to-end sys-\ntems attempt to jointly learn the semantics of composition\nand expressive performance using RNNs [23, 33]. Malik\nand Ek (2017) train an RNN to generate velocity informa-\ntion given a musical score [22]. These approaches differ\nfrom our own in that they focus on piano performances\nrather than multi-instrumental music.\n6. CONCLUSION\nThe NES Music Database is a large corpus for examining\nmulti-instrumental polyphonic composition and expressive\nperformance generation. Compared to existing datasets,\nNES-MDB allows for examination of the “full pipeline”\nof music composition and performance. We parse the\nmachine code of NES music into familiar formats (e.g.\nMIDI), eliminating the need for researchers to understand\nlow-level details of the game system. We also provide\nan open-source tool which converts between the simpler\nformats and machine code, allowing researchers to au-\ndition their generated results as waveforms rendered by\nthe NES. We hope that this dataset will facilitate a new\nparadigm of research on music generation—one that em-\nphasizes the importance of expressive performance. To this\nend, we establish several baselines with reproducible eval-\nuation methodology to encourage further investigation.480 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20187. ACKNOWLEDGEMENTS\nWe would like to thank Louis Pisha for invaluable advice\non the technical details of this project. Additionally, we\nwould like to thank Nicolas Boulanger-Lewandowski, Eu-\nnjeong Stella Koh, Steven Merity, Miller Puckette, and\nCheng-i Wang for helpful conversations throughout this\nwork. This work was supported by UC San Diego’s Chan-\ncellors Research Excellence Scholarship program. GPUs\nused in this research were donated by NVIDIA.\n8. REFERENCES\n[1] Moray Allan and Christopher Williams. Harmonis-\ning chorales by probabilistic inference. In Proc. NIPS ,\n2005.\n[2] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Modeling temporal dependencies in\nhigh-dimensional sequences: Application to poly-\nphonic music generation and transcription. In Proc.\nICML , 2012.\n[3] Jean-Pierre Briot, Ga ¨etan Hadjeres, and Franc ¸ois Pa-\nchet. Deep learning techniques for music generation-a\nsurvey. arXiv:1709.01620 , 2017.\n[4] Ali Taylan Cemgil. Bayesian music transcription. PhD\nthesis, Radboud University Nijmegen , 2004.\n[5] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. Empirical evaluation of gated\nrecurrent neural networks on sequence modeling. In\nNIPS Workshops , 2014.\n[6] Karen Collins. Game sound: an introduction to the\nhistory, theory, and practice of video game music and\nsound design . MIT Press, 2008.\n[7] Miguel Delgado, Waldo Fajardo, and Miguel Molina-\nSolana. A state of the art on computational music per-\nformance. Expert systems with applications , 2011.\n[8] Douglas Eck and J ¨urgen Schmidhuber. Finding tempo-\nral structure in music: Blues improvisation with LSTM\nrecurrent networks. In Proc. Neural Networks for Sig-\nnal Processing , 2002.\n[9] Otto Fabius and Joost R van Amersfoort. Variational\nrecurrent auto-encoders. In ICLR Workshops , 2015.\n[10] Sebastian Flossmann, Werner Goebl, Maarten\nGrachten, Bernhard Niedermayer, and Gerhard Wid-\nmer. The Magaloff project: An interim report. Journal\nof New Music Research , 2010.\n[11] Sebastian Flossmann, Maarten Grachten, and Gerhard\nWidmer. Expressive performance rendering with prob-\nabilistic models. In Guide to Computing for Expressive\nMusic Performance . 2013.[12] Werner Goebl, Simon Dixon, Giovanni De Poli, An-\nders Friberg, Roberto Bresin, and Gerhard Widmer.\nSense in expressive music performance: Data acqui-\nsition, computational studies, and models. 2008.\n[13] Ga ¨etan Hadjeres and Franc ¸ois Pachet. DeepBach: A\nsteerable model for Bach chorales generation. In Proc.\nICML , 2017.\n[14] Hermann Hild, Johannes Feulner, and Wolfram Men-\nzel. Harmonet: A neural net for harmonizing chorales\nin the style of JS Bach. In NIPS , 1992.\n[15] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-\nterm memory. Neural Computation , 1997.\n[16] Cheng-Zhi Anna Huang, Tim Cooijmans, Adam\nRoberts, Aaron Courville, and Douglas Eck. Counter-\npoint by convolution. In Proc. ISMIR , 2017.\n[17] Daniel D Johnson. Generating polyphonic music using\ntied parallel networks. In Proc. International Confer-\nence on Evolutionary and Biologically Inspired Music\nand Art , 2017.\n[18] Rafal Jozefowicz, Wojciech Zaremba, and Ilya\nSutskever. An empirical exploration of recurrent net-\nwork architectures. In Proc. ICML , 2015.\n[19] Diederik P Kingma and Jimmy Ba. Adam: A method\nfor stochastic optimization. arXiv:1412.6980 , 2014.\n[20] Alexis Kirke and Eduardo R Miranda. An overview of\ncomputer systems for expressive music performance.\nInGuide to computing for expressive music perfor-\nmance . 2013.\n[21] Hugo Larochelle and Iain Murray. The neural au-\ntoregressive distribution estimator. In Proc. AISTATS ,\n2011.\n[22] Iman Malik and Carl Henrik Ek. Neural translation of\nmusical style. arXiv:1708.03535 , 2017.\n[23] Huanru Henry Mao, Taylor Shin, and Garrison W.\nCottrell. DeepJ: Style-speciﬁc music generation. In\nProc. International Conference on Semantic Comput-\ning, 2018.\n[24] Juhan Nam, Jiquan Ngiam, Honglak Lee, and Mal-\ncolm Slaney. A classiﬁcation-based polyphonic piano\ntranscription approach using learned feature represen-\ntations. In Proc. ISMIR , 2011.\n[25] Gerhard Nierhaus. Algorithmic composition:\nparadigms of automated music generation . Springer\nScience & Business Media, 2009.\n[26] Jean-Francois Paiement, Samy Bengio, and Douglas\nEck. Probabilistic models for melodic prediction. Ar-\ntiﬁcial Intelligence , 2009.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 481[27] Graham E Poliner and Daniel PW Ellis. A dis-\ncriminative model for polyphonic piano transcription.\nEURASIP Journal on Advances in Signal Processing ,\n2006.\n[28] Colin Raffel. Learning-based methods for comparing\nsequences, with applications to audio-to-midi align-\nment and matching . Columbia University, 2016.\n[29] Christopher Raphael. Music Plus One and machine\nlearning. In Proc. ICML , 2010.\n[30] David E Rumelhart, Geoffrey E Hinton, and Ronald J\nWilliams. Learning internal representations by error\npropagation. Technical report, DTIC Document, 1985.\n[31] Jason Sakellariou, Francesca Tria, Vittorio Loreto, and\nFranc ¸ois Pachet. Maximum entropy model for melodic\npatterns. In ICML Workshops , 2015.\n[32] Craig Stuart Sapp. Comparative analysis of multiple\nmusical performances. In Proc. ISMIR , 2007.\n[33] Ian Simon and Sageev Oore. Performance RNN: Gen-\nerating music with expressive timing and dynamics,\n2017.\n[34] Paul Smolensky. Information processing in dynamical\nsystems: Foundations of harmony theory. Technical re-\nport, DTIC Document, 1986.\n[35] Raunaq V ohra, Kratarth Goel, and JK Sahoo. Modeling\ntemporal dependencies in data using a DBN-LSTM. In\nProc. IEEE Conference on Data Science and Advanced\nAnalytics , 2015.\n[36] Gerhard Widmer and Werner Goebl. Computational\nmodels of expressive music performance: The state of\nthe art. Journal of New Music Research , 2004.\n[37] Guangyu Xia, Yun Wang, Roger B Dannenberg, and\nGeoffrey Gordon. Spectral learning for expressive in-\nteractive ensemble music performance. In Proc. IS-\nMIR, 2015.482 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Convolutional Generative Adversarial Networks with Binary Neurons for Polyphonic Music Generation.",
        "author": [
            "Hao-Wen Dong",
            "Yi-Hsuan Yang"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492377",
        "url": "https://doi.org/10.5281/zenodo.1492377",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/218_Paper.pdf",
        "abstract": "It has been shown recently that deep convolutional generative adversarial networks (GANs) can learn to generate music in the form of piano-rolls, which represent music by binary-valued time-pitch matrices. However, existing models can only generate real-valued piano-rolls and require further post-processing, such as hard thresholding (HT) or Bernoulli sampling (BS), to obtain the final binaryvalued results. In this paper, we study whether we can have a convolutional GAN model that directly creates binaryvalued piano-rolls by using binary neurons. Specifically, we propose to append to the generator an additional refiner network, which uses binary neurons at the output layer. The whole network is trained in two stages. Firstly, the generator and the discriminator are pretrained. Then, the refiner network is trained along with the discriminator to learn to binarize the real-valued piano-rolls the pretrained generator creates. Experimental results show that using binary neurons instead of HT or BS indeed leads to better results in a number of objective measures. Moreover, deterministic binary neurons perform better than stochastic ones in both objective measures and a subjective test. The source code, training data and audio examples of the generated results can be found at https://salu133445. github.io/bmusegan/.",
        "zenodo_id": 1492377,
        "dblp_key": "conf/ismir/DongY18",
        "keywords": [
            "binary neurons",
            "convolutional GAN",
            "binary-valued piano-rolls",
            "pretrained generator",
            "refiner network",
            "objective measures",
            "subjective test",
            "source code",
            "training data",
            "audio examples"
        ],
        "content": "CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS WITH\nBINARY NEURONS FOR POLYPHONIC MUSIC GENERATION\nHao-Wen Dong and Yi-Hsuan Yang\nResearch Center for IT innovation, Academia Sinica, Taipei, Taiwan\nfsalu133445,yang g@citi.sinica.edu.tw\nABSTRACT\nIt has been shown recently that deep convolutional gen-\nerative adversarial networks (GANs) can learn to gener-\nate music in the form of piano-rolls, which represent mu-\nsic by binary-valued time-pitch matrices. However, exist-\ning models can only generate real-valued piano-rolls and\nrequire further post-processing, such as hard thresholding\n(HT) or Bernoulli sampling (BS), to obtain the ﬁnal binary-\nvalued results. In this paper, we study whether we can have\na convolutional GAN model that directly creates binary-\nvalued piano-rolls by using binary neurons. Speciﬁcally,\nwe propose to append to the generator an additional reﬁner\nnetwork, which uses binary neurons at the output layer.\nThe whole network is trained in two stages. Firstly, the\ngenerator and the discriminator are pretrained. Then, the\nreﬁner network is trained along with the discriminator to\nlearn to binarize the real-valued piano-rolls the pretrained\ngenerator creates. Experimental results show that using bi-\nnary neurons instead of HT or BS indeed leads to better\nresults in a number of objective measures. Moreover, de-\nterministic binary neurons perform better than stochastic\nones in both objective measures and a subjective test. The\nsource code, training data and audio examples of the gen-\nerated results can be found at https://salu133445.\ngithub.io/bmusegan/ .\n1. INTRODUCTION\nRecent years have seen increasing research on symbolic-\ndomain music generation and composition using deep neu-\nral networks [7]. Notable progress has been made to gener-\nate monophonic melodies [25,27], lead sheets (i.e., melody\nand chords) [8, 11, 26], or four-part chorales [14]. To add\nsomething new to the table and to increase the polyphony\nand the number of instruments of the generated music, we\nattempt to generate piano-rolls in this paper, a music rep-\nresentation that is more general (e.g., comparing to lead-\nsheets) yet less studied in recent work on music generation.\nAs Figure 1 shows, we can consider an M-track piano-roll\nc\rHao-Wen Dong and Yi-Hsuan Yang. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Hao-Wen Dong and Yi-Hsuan Yang. “Convolutional Gen-\nerative Adversarial Networks with Binary Neurons for Polyphonic Music\nGeneration”, 19th International Society for Music Information Retrieval\nConference, Paris, France, 2018.Dr.\nPi.\nGu.\nBa.\nEn.\nRe.\nS.L.\nS.P.\nDr.\nPi.\nGu.\nBa.\nEn.\nRe.\nS.L.\nS.P.\nFigure 1 . Six examples of eight-track piano-roll of four-\nbar long (each block represents a bar) seen in our training\ndata. The vertical and horizontal axes represent note pitch\nand time, respectively. The eight tracks are Drums, Piano,\nGuitar, Bass, Ensemble, Reed, Synth Lead andSynth Pad .\nas a collection of Mbinary time-pitch matrices indicating\nthe presence of pitches per time step for each track.\nGenerating piano-rolls is challenging because of the\nlarge number of possible active notes per time step and the\ninvolvement of multiple instruments. Unlike a melody or\na chord progression, which can be viewed as a sequence\nof note/chord events and be modeled by a recurrent neural\nnetwork (RNN) [21,24], the musical texture in a piano-roll\nis much more complicated (see Figure 1). While RNNs are\ngood at learning the temporal dependency of music, con-\nvolutional neural networks (CNNs) are usually considered\nbetter at learning local patterns [18].\nFor this reason, in our previous work [10], we used a\nconvolutional generative adversarial network (GAN) [12]\nto learn to generate piano-rolls of ﬁve tracks. We showed\nthat the model generates music that exhibit drum patterns\nand plausible note events. However, musically the gener-\nated result is still far from satisfying to human ears, scoring\naround 3on average on a ﬁve-level Likert scale in overall190quality in a user study [10].1\nThere are several ways to improve upon this prior work.\nThe major topic we are interested in is the introduction of\nthebinary neurons (BNs) [1, 4] to the model. We note\nthat conventional CNN designs, also the one adopted in our\nprevious work [10], can only generate real-valued predic-\ntions and require further postprocessing at test time to ob-\ntain the ﬁnal binary-valued piano-rolls.2This can be done\nby either applying a hard threshold (HT) on the real-valued\npredictions to binarize them (which was done in [10]), or\nby treating the real-valued predictions as probabilities and\nperforming Bernoulli sampling (BS).\nHowever, we note that such na ¨ıve methods for binariz-\ning a piano-roll can easily lead to overly-fragmented notes .\nFor HT, this happens when the original real-valued piano-\nroll has many entries with values close to the threshold. For\nBS, even an entry with low probability can take the value\n1, due to the stochastic nature of probabilistic sampling.\nThe use of BNs can mitigate the aforementioned issue,\nsince the binarization is part of the training process. More-\nover, it has two potential beneﬁts:\n\u000fIn [10], binarization of the output of the generator\nGin GAN is done only at test time not at train-\ning time (see Section 2.1 for a brief introduction of\nGAN). This makes it easy for the discriminator D\nin GAN to distinguish between the generated piano-\nrolls (which are real-valued in this case) and the real\npiano-rolls (which are binary). With BNs, the bina-\nrization is done at training time as well, so Dcan\nfocus on extracting musically relevant features.\n\u000fDue to BNs, the input to the discriminator Din GAN\nat training time is binary instead of real-valued. This\neffectively reduces the model space from <Nto2N,\nwhereNis the product of the number of time steps\nand the number of possible pitches. Training Dmay\nbe easier as the model space is substantially smaller,\nas Figure 2 illustrates.\nSpeciﬁcally, we propose to append to the end of Gare-\nﬁner network Rthat uses either deterministic BNs (DBNs)\nor stocahstic BNs (SBNs) at the output layer. In this way,\nGmakes real-valued predictions and Rbinarizes them. We\ntrain the whole network in two stages: in the ﬁrst stage we\npretrainGandDand then ﬁx G; in the second stage, we\ntrainRand ﬁne-tune D. We use residual blocks [16] in R\nto make this two-stage training feasible (see Section 3.3).\nAs minor contributions, we use a new shared/private de-\nsign ofGandDthat cannot be found in [10]. Moreover,\nwe add toDtwo streams of layers that provide onset/offset\nand chroma information (see Sections 3.2 and 3.4).\nThe proposed model is able to directly generate binary-\nvalued piano-rolls at test time. Our analysis shows that the\n1Another related work on generating piano-rolls, as presented by\nBoulanger-Lewandowski et al. [6], replaced the output layer of an RNN\nwith conditional restricted Boltzmann machines (RBMs) to model high-\ndimensional sequences and applied the model to generate piano-rolls se-\nquentially (i.e. one time step after another).\n2Such binarization is typically not needed for an RNN or an RBM\nin polyphonic music generation, since an RNN is usually used to predict\npre-deﬁned note events [22] and an RBM is often used with binary visible\nand hidden units and sampled by Gibbs sampling [6, 20].\nFigure 2 . An illustration of the decision boundaries (red\ndashed lines) that the discriminator Dhas to learn when\nthe generator Goutputs (left) real values and (right) binary\nvalues. The decision boundaries divide the space into the\nreal class (in blue) and the fake class (in red). The black\nand red dots represent the real data and the fake ones gen-\nerated by the generator, respectively. We can see that the\ndecision boundaries are easier to learn when the generator\noutputs binary values rather than real values.\ngenerated results of our model with DBNs features fewer\noverly-fragmented notes as compared with the result of us-\ning HT or BS. Experimental results also show the effective-\nness of the proposed two-stage training strategy compared\nto either a joint or an end-to-end training strategy.\n2. BACKGROUND\n2.1 Generative Adversarial Networks\nA generative adversarial network (GAN) [12] has two core\ncomponents: a generatorGand a discriminator D. The\nformer takes as input a random vector zsampled from a\nprior distribution pzand generates a fake sample G(z).D\ntakes as input either real data xor fake data generated by\nG. During training time, Dlearns to distinguish the fake\nsamples from the real ones, whereas Glearns to fool D.\nAn alternative form called WGAN was later proposed\nwith the intuition to estimate the Wasserstein distance be-\ntween the real and the model distributions by a deep neural\nnetwork and use it as a critic for the generator [2]. The\nobjective function for WGAN can be formulated as:\nmin\nGmax\nDEx\u0018pd[D(x)]\u0000Ez\u0018pz[D(G(z))];(1)\nwherepddenotes the real data distribution. In order to\nenforce Lipschitz constraints on the discriminator, which\nis required in the training of WGAN, Gulrajani et al. [13]\nproposed to add to the objective function of Dagradient\npenalty (GP) term: E^x\u0018p^x[(r^xk^xk\u00001)2], wherep^xis\ndeﬁned as sampling uniformly along straight lines between\npairs of points sampled from pdand the model distribution\npg. Empirically they found it stabilizes the training and\nalleviates the mode collapse issue, compared to the weight\nclipping strategy used in the original WGAN. Hence, we\nemploy WGAN-GP [13] as our generative framework.\n2.2 Stochastic and Deterministic Binary Neurons\nBinary neurons (BNs) are neurons that output binary-\nvalued predictions. In this work, we consider two types ofProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 191Figure 3 . The generator and the reﬁner. The generator ( Gs\nand several Gi\npcollectively) produces real-valued predic-\ntions. The reﬁner network (several Ri) reﬁnes the outputs\nof the generator into binary ones.\nFigure 4 . The reﬁner network. The tensor size remains the\nsame throughout the network.\nBNs: deterministic binary neurons (DBNs) and stochastic\nbinary neurons (SBNs). DBNs act like neurons with hard\nthresholding functions as their activation functions. We\ndeﬁne the output of a DBN for a real-valued input xas:\nDBN (x) =u(\u001b(x)\u00000:5); (2)\nwhereu(\u0001)denotes the unit step function and \u001b(\u0001)is the\nlogistic sigmoid function. SBNs, in contrast, binarize an\ninputxaccording to a probability, deﬁned as:\nSBN (x) =u(\u001b(x)\u0000v); v\u0018U[0;1]; (3)\nwhereU[0;1]denotes a uniform distribution.\n2.3 Straight-through Estimator\nComputing the exact gradients for either DBNs or SBNs,\nhowever, is intractable. For SBNs, it requires the computa-\ntion of the average loss over all possible binary samplings\nof all the SBNs, which is exponential in the total number\nof SBNs. For DBNs, the threshold function in Eq. (2) is\nnon-differentiable. Therefore, the ﬂow of backpropagation\nused to train parameters of the network would be blocked.\nA few solutions have been proposed to address this is-\nsue [1, 4]. One strategy is to replace the non-differentiable\nfunctions, which are used in the forward pass, by differen-\ntiable functions (usually called the estimators ) in the back-\nward pass. An example is the straight-through (ST) esti-\nmator proposed by Hinton [17]. In the backward pass, ST\nsimply treats BNs as identify functions and ignores their\ngradients. A variant of the ST estimator is the sigmoid-\nadjusted ST estimator [9], which multiplies the gradients\nin the backward pass by the derivative of the sigmoid func-\ntion. Such estimators were originally proposed as regular-\nizers [17] and later found promising for conditional com-\nputation [4]. We use the sigmoid-adjusted ST estimator in\ntraining neural networks with BNs and found it empirically\nworks well for our generation task as well.\nFigure 5 . The discriminator. It consists of three streams:\nthe main stream ( Dm,Dsand severalDi\np; the upper half),\nthe onset/offset stream ( Do) and the chroma stream ( Dc).\nFigure 6 . Residual unit used in the reﬁner network. The\nvalues denote the kernel size and the number of the output\nchannels of the two convolutional layers.\n3. PROPOSED MODEL\n3.1 Data Representation\nFollowing [10], we use the multi-track piano-roll represen-\ntation. A multi-track piano-roll is deﬁned as a set of piano-\nrolls for different tracks (or instruments). Each piano-roll\nis a binary-valued score-like matrix, where its vertical and\nhorizontal axes represent note pitch and time, respectively.\nThe values indicate the presence of notes over different\ntime steps. For the temporal axis, we discard the tempo\ninformation and therefore every beat has the same length\nregardless of tempo.\n3.2 Generator\nAs Figure 3 shows, the generator Gconsists of a “s”hared\nnetworkGsfollowed by M“p”rivate network Gi\np,i=\n1;:::;M , one for each track. The shared generator Gs\nﬁrst produces a high-level representation of the output mu-\nsical segments that is shared by all the tracks. Each pri-\nvate generator Gi\npthen turns such abstraction into the ﬁnal\npiano-roll output for the corresponding track. The intu-\nition is that different tracks have their own musical prop-\nerties (e.g., textures, common-used patterns), while jointly\nthey follow a common, high-level musical idea. The de-\nsign is different from [10] in that the latter does not include\na sharedGsin early layers.\n3.3 Reﬁner\nThe reﬁnerRis composed of Mprivate networks Ri,i=\n1;:::;M , again one for each track. The reﬁner aims to\nreﬁne the real-valued outputs of the generator, ^ x=G(z),\ninto binary ones, ~ x, rather than learning a new mapping192 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Dr.\nPi.\nGu.\nBa.\nEn.\n(a) raw predictions (b) pretrained (+BS) (c) pretrained (+HT) (d) proposed (+SBNs) (d) proposed (+DBNs)\nFigure 7 . Comparison of binarization strategies. (a): the probabilistic, real-valued (raw) predictions of the pretrained G.\n(b), (c): the results of applying post-processing algorithms directly to the raw predictions in (a). (d), (e): the results of the\nproposed models, using an additional reﬁner Rto binarize the real-valued predictions of G. Empty tracks are not shown.\n(We note that in (d), few noises (33 pixels) occur in the Reed andSynth Lead tracks.)\nfromG(z)to the data space. Hence, we draw inspiration\nfrom residual learning and propose to construct the reﬁner\nwith a number of residual units [16], as shown in Figure 4.\nThe output layer (i.e. the ﬁnal layer) of the reﬁner is made\nup of either DBNs or SBNs.\n3.4 Discriminator\nSimilar to the generator, the discriminator Dconsists of\nMprivate network Di\np,i= 1;:::;M , one for each track,\nfollowed by a shared network Ds, as shown in Figure 5.\nEach private network Di\npﬁrst extracts low-level features\nfrom the corresponding track of the input piano-roll. Their\noutputs are concatenated and sent to the shared network Ds\nto extract higher-level abstraction shared by all the tracks.\nThe design differs from [10] in that only one (shared) dis-\ncriminator was used in [10] to evaluate all the tracks col-\nlectively. We intend to evaluate such a new shared/private\ndesign in Section 4.5.\nAs a minor contribution, to help the discriminator ex-\ntract musically-relevant features, we propose to add to the\ndiscriminator two more streams, shown in the lower half\nof Figure 5. In the ﬁrst onset/offset stream , the differences\nbetween adjacent elements in the piano-roll along the time\naxis are ﬁrst computed, and then the resulting matrix is\nsummed along the pitch axis, which is ﬁnally fed to Do.\nIn the second chroma stream , the piano-roll is viewed\nas a sequence of one-beat-long frames. A chroma vector\nis then computed for each frame and jointly form a matrix,\nwhich is then be fed to Dc. Note that all the operations\ninvolved in computing the chroma and onset/offset features\nare differentiable, and thereby we can still train the whole\nnetwork by backpropagation.\nFinally, the features extracted from the three streams are\nconcatenated and fed to Dmto make the ﬁnal prediction.\n3.5 Training\nWe propose to train the model in a two-stage manner:G\nandDare pretrained in the ﬁrst stage; Ris then trained\nalong withD(ﬁxingG) in the second stage. Other training\nstrategies are discussed and compared in Section 4.4.4. ANALYSIS OF THE GENERATED RESULTS\n4.1 Training Data & Implementation Details\nThe Lakh Pianoroll Dataset (LPD) [10]3contains 174,154\nmulti-track piano-rolls derived from the MIDI ﬁles in the\nLakh MIDI Dataset (LMD) [23].4In this paper, we use a\ncleansed subset ( LPD-cleansed ) as the training data, which\ncontains 21,425 multi-track piano-rolls that are in 4/4 time\nand have been matched to distinct entries in Million Song\nDataset (MSD) [5]. To make the training data cleaner, we\nconsider only songs with an alternative tag. We randomly\npick six four-bar phrases from each song, which leads to\nthe ﬁnal training set of 13,746 phrases from 2,291 songs.\nWe set the temporal resolution to 24 time steps per\nbeat to cover common temporal patterns such as triplets\nand 32th notes. An additional one-time-step-long pause is\nadded between two consecutive (i.e. without a pause) notes\nof the same pitch to distinguish them from one single note.\nThe note pitch has 84 possibilities, from C1toB7.\nWe categorize all instruments into drums and sixteen\ninstrument families according to the speciﬁcation of Gen-\neral MIDI Level 1.5We discard the less popular instru-\nment families in LPD and use the following eight tracks:\nDrums, Piano, Guitar, Bass, Ensemble, Reed, Synth Lead\nandSynth Pad . Hence, the size of the target output tensor\nis4(bar)\u000296(time step)\u000284(pitch)\u00028(track).\nBothGandDare implemented as CNNs. The length of\nthe input random vector is 128.Rconsists of two residual\nunits [16] shown in Figure 6. Following [13], we use the\nAdam optimizer [19] and only apply batch normalization\ntoGandR. We apply the slope annealing trick [9] to net-\nworks with BNs, where the slope of the sigmoid function\nin the sigmoid-adjusted ST estimator is multiplied by 1:1\nafter each epoch. The batch size is 16except for the ﬁrst\nstage in the two-stage training setting, where the batch size\nis32. For more details, we refer readers to the online ap-\npendix, which can be found on the project website.6\n3https://salu133445.github.io/\nlakh-pianoroll-dataset/\n4http://colinraffel.com/projects/lmd/\n5https://www.midi.org/specifications/item/\ngm-level-1-sound-set\n6https://salu133445.github.io/bmusegan/Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 193training\ndatapretrained proposed joint end-to-end ablated-I ablated-II\nBS HT SBNs DBNs SBNs DBNs SBNs DBNs BS HT BS HT\nQN 0.88 0.67 0.72 0.42 0.78 0.18 0.55 0.67 0.28 0.61 0.64 0.35 0.37\nPP 0.48 0.20 0.22 0.26 0.45 0.19 0.19 0.16 0.29 0.19 0.20 0.14 0.14\nTD 0.96 0.98 1.00 0.99 0.87 0.95 1.00 1.40 1.10 1.00 1.00 1.30 1.40\n(Underlined and bold font indicate respectively the top and top-three entries with values closest to those shown in the ‘training data’ column.)\nTable 1 . Evaluation results for different models. Values closer to those reported in the ‘training data’ column are better.\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 8 . Closeup of the piano track in Figure 7.\n4.2 Objective Evaluation Metrics\nWe generate 800 samples for each model and use the fol-\nlowing metrics proposed in [10] for evaluation. We con-\nsider a model better if the average metric values of the\ngenerated samples are closer to those computed from the\ntraining data.\n\u000fQualiﬁed note rate (QN) computes the ratio of the\nnumber of the qualiﬁed notes (notes no shorter than\nthree time steps, i.e., a 32th note) to the total number\nof notes. Low QN implies overly-fragmented music.\n\u000fPolyphonicity (PP) is deﬁned as the ratio of the\nnumber of time steps where more than two pitches\nare played to the total number of time steps.\n\u000fTonal distance (TD) measures the distance between\nthe chroma features (one for each beat) of a pair of\ntracks in the tonal space proposed in [15]. In what\nfollows, we only report the TDbetween the piano\nand the guitar, for they are the two most used tracks.\n4.3 Comparison of Binarization Strategies\nWe compare the proposed model with two common test-\ntime binarization strategies: Bernoulli sampling (BS) and\nhard thresholding (HT). Some qualitative results are pro-(a)\n0 20000 40000 60000 80000 100000\nstep0.00.20.40.60.81.0qualified note ratepretrain\nproposed (+DBNs)\nproposed (+SBNs)\njoint (+DBNs)\njoint (+SBNs)\n(b)\n0 20000 40000 60000 80000 100000\nstep0.00.20.40.60.81.0polyphonicitypretrain\nproposed (+DBNs)\nproposed (+SBNs)\njoint (+DBNs)\njoint (+SBNs)\nFigure 9 . (a) Qualiﬁed note rate (QN) and (b) polyphonic-\nity (PP) as a function of training steps for different models.\nThe dashed lines indicate the average QN and PP of the\ntraining data, respectively. (Best viewed in color.)\nvided in Figures 7 and 8. Moreover, we present in Table 1\na quantitative comparison among them.\nBoth qualitative and quantitative results show that the\ntwo test-time binarization strategies can lead to overly-\nfragmented piano-rolls (see the “pretrained” ones). The\nproposed model with DBNs is able to generate piano-rolls\nwith a relatively small number of overly-fragmented notes\n(aQNof0:78; see Table 1) and to better capture the sta-\ntistical properties of the training data in terms of PP. How-\never, the proposed model with SBNs produces a number of\nrandom-noise-like artifacts in the generated piano-rolls, as\ncan be seen in Figure 8(d), leading to a low QNof0:42.\nWe attribute to the stochastic nature of SBNs. Moreover,\nwe can also see from Figure 9 that only the proposed model\nwith DBNs keeps improving after the second-stage train-\ning starts in terms of QNandPP.\n4.4 Comparison of Training Strategies\nWe consider two alternative training strategies:\n\u000fjoint : pretrainGandDin the ﬁrst stage, and then194 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Dr.\nPi.\nGu.\nBa.\nEn.\nRe.\nS.L.\nS.P.\nDr.\nPi.\nGu.\nBa.\nEn.\nFigure 10 . Example generated piano-rolls of the end-to-\nend models with (top) DBNs and (bottom) SBNs. Empty\ntracks are not shown.\ntrainGandR(like viewing Ras part ofG) jointly\nwithDin the second stage.\n\u000fend-to-end : trainG,RandDjointly in one stage.\nAs shown in Table 1, the models with DBNs trained\nusing the joint andend-to-end training strategies receive\nlower scores as compared to the two-stage training strategy\nin terms of QNandPP. We can also see from Figure 9(a)\nthat the model with DBNs trained using the joint training\nstrategy starts to degenerate in terms of QNat about 10,000\nsteps after the second-stage training begins.\nFigure 10 shows some qualitative results for the end-\nto-end models. It seems that the models learn the proper\npitch ranges for different tracks. We also see some chord-\nlike patterns in the generated piano-rolls. From Table 1 and\nFigure 10, in the end-to-end training setting SBNs are not\ninferior to DBNs, unlike the case in the two-stage training.\nAlthough the generated results appear preliminary, to our\nbest knowledge this represents the ﬁrst attempt to generate\nsuch high dimensional data with BNs from scratch.\n4.5 Effects of the Shared/private and Multi-stream\nDesign of the Discriminator\nWe compare the proposed model with two ablated ver-\nsions: the ablated-I model, which removes the onset/offset\nand chroma streams, and the ablated-II model, which uses\nonly a shared discriminator without the shared/private and\nmulti-stream design (i.e., the one adopted in [10]).7Note\nthat the comparison is done by applying either BS or HT\n(not BNs) to the ﬁrst-stage pretrained models.\nAs shown in Table 1, the proposed model (see “pre-\ntrained”) outperforms the two ablated versions in all three\nmetrics. A lower QNfor the proposed model as compared\nto the ablated-I model suggests that the onset/offset stream\ncan alleviate the overly-fragmented note problem. Lower\n7The number of parameters for the proposed, ablated-I and ablated-II\nmodels is 3.7M, 3.4M and 4.6M, respectively.\n10000 20000 30000 40000 50000\nstep0.00.20.40.60.81.0qualified note rateproposed\nablated I (w/o multi-stream design)\nablated II (w/o shared/private & multi-stream design)Figure 11 . Qualiﬁed note rate (QN) as a function of train-\ning steps for different models. The dashed line indicates\nthe average QN of the training data. (Best viewed in color.)\nwith SBNs with DBNs\ncompleteness*0.19 0.81\nharmonicity 0.44 0.56\nrhythmicity 0.56 0.44\noverall rating 0.16 0.84\n*We asked, “Are there many overly-fragmented notes?”\nTable 2 . Result of a user study, averaged over 20 subjects.\nTDfor the proposed and ablated-I models as compared to\nthe ablated-II model indicates that the shared/private de-\nsign better capture the intertrack harmonicity. Figure 11\nalso shows that the proposed and ablated-I models learn\nfaster and better than the ablated-II model in terms of QN.\n4.6 User Study\nFinally, we conduct a user study involving 20 participants\nrecruited from the Internet. In each trial, each subject\nis asked to compare two pieces of four-bar music gener-\nated from scratch by the proposed model using SBNs and\nDBNs, and vote for the better one in four measures. There\nare ﬁve trials in total per subject. We report in Table 2\nthe ratio of votes each model receives. The results show a\npreference to DBNs for the proposed model.\n5. DISCUSSION AND CONCLUSION\nWe have presented a novel convolutional GAN-based\nmodel for generating binary-valued piano-rolls by using\nbinary neurons at the output layer of the generator. We\ntrained the model on an eight-track piano-roll dataset.\nAnalysis showed that the generated results of our model\nwith deterministic binary neurons features fewer overly-\nfragmented notes as compared with existing methods.\nThough the generated results appear preliminary and lack\nmusicality, we showed the potential of adopting binary\nneurons in a music generation system.\nIn future work, we plan to further explore the end-to-\nend models and add recurrent layers to the temporal model.\nIt might also be interesting to use BNs for music transcrip-\ntion [3], where the desired outputs are also binary-valued.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 1956. REFERENCES\n[1] Binary stochastic neurons in tensorﬂow, 2016. Blog\npost on R2RT blog. [Online] https://r2rt.com/\nbinary-stochastic-neurons-in-tensorflow.html .\n[2] Martin Arjovsky, Soumith Chintala, and L ´eon Bottou.\nWasserstein generative adversarial networks. In Proc.\nICML , 2017.\n[3] Emmanouil Benetos, Simon Dixon, Dimitrios Gian-\nnoulis, Holger Kirchhoff, and Anssi Klapuri. Auto-\nmatic music transcription: challenges and future di-\nrections. Journal of Intelligent Information Systems ,\n41(3):407–434, 2013.\n[4] Yoshua Bengio, Nicholas L ´eonard, and Aaron C.\nCourville. Estimating or propagating gradients through\nstochastic neurons for conditional computation. arXiv\npreprint arXiv:1308.3432 , 2013.\n[5] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian\nWhitman, and Paul Lamere. The Million Song Dataset.\nInProc. ISMIR , 2011.\n[6] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Modeling temporal dependencies in\nhigh-dimensional sequences: Application to poly-\nphonic music generation and transcription. In Proc.\nICML , 2012.\n[7] Jean-Pierre Briot, Ga ¨etan Hadjeres, and Franc ¸ois Pa-\nchet. Deep learning techniques for music generation:\nA survey. arXiv preprint arXiv:1709.01620 , 2017.\n[8] Hang Chu, Raquel Urtasun, and Sanja Fidler. Song\nfrom PI: A musically plausible network for pop music\ngeneration. In Proc. ICLR, Workshop Track , 2017.\n[9] Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.\nHierarchical multiscale recurrent neural networks. In\nProc. ICLR , 2017.\n[10] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-\nHsuan Yang. MuseGAN: Symbolic-domain music gen-\neration and accompaniment with multi-track sequential\ngenerative adversarial networks. In Proc. AAAI , 2018.\n[11] Douglas Eck and J ¨urgen Schmidhuber. Finding tempo-\nral structure in music: Blues improvisation with LSTM\nrecurrent networks. In Proc. IEEE Workshop on Neural\nNetworks for Signal Processing , 2002.\n[12] Ian J. Goodfellow et al. Generative adversarial nets. In\nProc. NIPS , 2014.\n[13] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vin-\ncent Dumoulin, and Aaron Courville. Improved train-\ning of Wasserstein GANs. In Proc. NIPS , 2017.\n[14] Ga ¨etan Hadjeres, Franc ¸ois Pachet, and Frank Nielsen.\nDeepBach: A steerable model for Bach chorales gen-\neration. In Proc. ICML , 2017.[15] Christopher Harte, Mark Sandler, and Martin Gasser.\nDetecting harmonic change in musical audio. In Proc.\nACM MM Workshop on Audio and Music Computing\nMultimedia , 2006.\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Identity mappings in deep residual networks. In\nProc. ECCV , 2016.\n[17] Geoffrey Hinton. Neural networks for machine\nlearning—using noise as a regularizer (lecture 9c),\n2012. Coursera, video lectures. [Online] https:\n//www.coursera.org/lecture/neural-networks/\nusing-noise-as-a-regularizer-7-min-wbw7b .\n[18] Cheng-Zhi Anna Huang, Tim Cooijmans, Adam\nRoberts, Aaron Courville, and Douglas Eck. Counter-\npoint by convolution. In Proc. ISMIR , 2017.\n[19] Diederik P. Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[20] Stefan Lattner, Maarten Grachten, and Gerhard Wid-\nmer. Imposing higher-level structure in polyphonic mu-\nsic generation using convolutional restricted Boltz-\nmann machines and constraints. Journal of Creative\nMusic Systems , 3(1), 2018.\n[21] Hyungui Lim, Seungyeon Rhyu, and Kyogu Lee.\nChord generation from symbolic melody using\nBLSTM networks. In Proc. ISMIR , 2017.\n[22] Olof Mogren. C-RNN-GAN: Continuous recurrent\nneural networks with adversarial training. In NIPS\nWorshop on Constructive Machine Learning Work-\nshop , 2016.\n[23] Colin Raffel. Learning-Based Methods for Comparing\nSequences, with Applications to Audio-to-MIDI Align-\nment and Matching . PhD thesis, Columbia University,\n2016.\n[24] Adam Roberts, Jesse Engel, Colin Raffel, Curtis\nHawthorne, and Douglas Eck. A hierarchical latent\nvector model for learning long-term structure in music.\nInProc. ICML , 2018.\n[25] Bob L. Sturm, Jo ˜ao Felipe Santos, Oded Ben-Tal,\nand Iryna Korshunova. Music transcription modelling\nand composition using deep learning. In Proc. CSMS ,\n2016.\n[26] Li-Chia Yang, Szu-Yu Chou, and Yi-Hsuan Yang.\nMidiNet: A convolutional generative adversarial net-\nwork for symbolic-domain music generation. In Proc.\nISMIR , 2017.\n[27] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\nSeqGAN: Sequence generative adversarial nets with\npolicy gradient. In Proc. AAAI , 2017.196 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Learning to Listen, Read, and Follow: Score Following as a Reinforcement Learning Game.",
        "author": [
            "Matthias Dorfer",
            "Florian Henkel",
            "Gerhard Widmer"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492535",
        "url": "https://doi.org/10.5281/zenodo.1492535",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/45_Paper.pdf",
        "abstract": "Score following is the process of tracking a musical performance (audio) with respect to a known symbolic representation (a score). We start this paper by formulating score following as a multimodal Markov Decision Process, the mathematical foundation for sequential decision making. Given this formal definition, we address the score following task with state-of-the-art deep reinforcement learning (RL) algorithms such as synchronous advantage actor critic (A2C). In particular, we design multimodal RL agents that simultaneously learn to listen to music, read the scores from images of sheet music, and follow the audio along in the sheet, in an end-to-end fashion. All this behavior is learned entirely from scratch, based on a weak and potentially delayed reward signal that indicates to the agent how close it is to the correct position in the score. Besides discussing the theoretical advantages of this learning paradigm, we show in experiments that it is in fact superior compared to previously proposed methods for score following in raw sheet music images.",
        "zenodo_id": 1492535,
        "dblp_key": "conf/ismir/DorferHW18",
        "keywords": [
            "multimodal",
            "Markov Decision Process",
            "sequential decision making",
            "deep reinforcement learning",
            "score following",
            "state-of-the-art",
            "sheet music",
            "end-to-end learning",
            "weak reward signal",
            "experiments"
        ],
        "content": "LEARNING TO LISTEN, READ, AND FOLLOW:\nSCORE FOLLOWING AS A REINFORCEMENT LEARNING GAME\nMatthias Dorfer\u0003Florian Henkel\u0003Gerhard Widmer\u0003y\n\u0003Institute of Computational Perception, Johannes Kepler University Linz, Austria\nyThe Austrian Research Institute for Artiﬁcial Intelligence (OFAI), Austria\nmatthias.dorfer@jku.at\nABSTRACT\nScore following is the process of tracking a musical per-\nformance (audio) with respect to a known symbolic rep-\nresentation (a score). We start this paper by formulating\nscore following as a multimodal Markov Decision Process,\nthe mathematical foundation for sequential decision mak-\ning. Given this formal deﬁnition, we address the score fol-\nlowing task with state-of-the-art deep reinforcement learn-\ning (RL) algorithms such as synchronous advantage ac-\ntor critic (A2C). In particular, we design multimodal RL\nagents that simultaneously learn to listen to music, read\nthe scores from images of sheet music, and follow the au-\ndio along in the sheet, in an end-to-end fashion. All this\nbehavior is learned entirely from scratch, based on a weak\nand potentially delayed reward signal that indicates to the\nagent how close it is to the correct position in the score.\nBesides discussing the theoretical advantages of this learn-\ning paradigm, we show in experiments that it is in fact su-\nperior compared to previously proposed methods for score\nfollowing in raw sheet music images.\n1. INTRODUCTION\nThis paper addresses the problem of score following in\nsheet music images. The task of an automatic score follow-\ning system is to follow a musical performance with respect\nto a known symbolical representation, the score (cf. Fig-\nure 1). In contrast to audio-score alignment in general [20],\nall of this takes place in an on-line fashion. Score follow-\ning itself has a long history in Music Information Retrieval\n(MIR) and forms the basis for many subsequent applica-\ntions such as automatic page turning [2], automatic accom-\npaniment [6,23] or the synchronization of visualizations to\nthe live music during concerts [1, 22].\nTraditional approaches to the task depend on a sym-\nbolic, computer-readable representation of the score, such\nas MusicXML or MIDI (see e.g. [1,6,10,14,16,17,21–23]).\nThis representation is created either manually (e.g. via the\nc\rMatthias Dorfer, Florian Henkel, Gerhard Widmer. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Matthias Dorfer, Florian Henkel, Gerhard\nWidmer. “Learning to Listen, Read, and Follow:\nScore Following as a Reinforcement Learning Game”, 19th International\nSociety for Music Information Retrieval Conference, Paris, France, 2018.\nFigure 1 . Sketch of score following in sheet music. Given\nthe incoming audio, the score follower has to track the cor-\nresponding position in the score (image).\ntime-consuming process of (re-)setting the score in a mu-\nsic notation program), or automatically via optical music\nrecognition software [3, 4, 12]. However, automatic meth-\nods are still unreliable and thus of limited use, especially\nfor more complex music like orchestra pieces [26].\nTo avoid these complications, [7] proposes a multi-\nmodal deep neural network that directly learns to match\nsheet music and audio in an end-to-end fashion. Given\nshort excerpts of audio and the corresponding sheet music,\nthe network learns to predict which location in the given\nsheet image best matches the current audio excerpt. In\nthis setup, score following can be formulated as a multi-\nmodal localization task. However, one problem with this\napproach is that successive time steps are treated indepen-\ndently from each other. We will see in our experiments\nthat this causes jumps in the tracking process especially in\nthe presence of repetitive passages. A related approach [8]\ntrains a multimodal neural network to learn a joint embed-\nding space for snippets of sheet music and corresponding\nshort excerpts of audio. The learned embedding allows to\ncompare observations across modalities, e.g., via their co-\nsine distance. This learned cross-modal similarity measure\nis then used to compute an off-line alignment between au-\ndio and sheet music via dynamic time warping.\nOur proposal is inspired by these works, but uses a fun-\ndamentally different machine learning paradigm. The cen-\ntral idea is to interpret score following as a multimodal con-\ntrol problem [9] where the agent has to navigate through\nthe score by adopting its reading speed in reaction to the\ncurrently playing performance. To operationalize this no-\ntion, we formulate score following as a Markov Decision\nProcess (MDP) in Section 3. MDPs are the mathemati-\ncal foundation for sequential decision making and permit784us to address the problem with state-of-the-art Deep Re-\ninforcement Learning (RL) algorithms (Section 4). Based\non the MDP formulation, we design agents that consider\nboth the score and the currently playing music to achieve\nan overall goal, that is to track the correct position in the\nscore for as long as possible. This kind of interaction is\nvery similar to controlling an agent in a video game, which\nis why we term our MDP the score following game ; it is\nin fact inspired by the seminal paper by Mnih et al. [19]\nwhich made a major contribution to the revival of deep\nRL by achieving impressive results in a large variety of\nAtari games. In experiments with monophonic as well as\npolyphonic music (Section 5), we will show that the RL\napproach is indeed superior to previously proposed score\nfollowing methods [7]. The code for the score following\ngame is available at https://github.com/CPJKU/\nscore_following_game .\n2. DESCRIPTION OF DATA\nTo set the stage, we ﬁrst need to describe the kind of\ndata needed for training and evaluating the multimodal RL\nscore following agents. We assume here that we are given\na collection of piano pieces represented as pairs of audio\nrecordings and sheet music images. In order to train our\nmodels and to later quantify the score following error, we\nﬁrst need to establish correspondences between individual\npixel locations of the note heads in a sheet and their respec-\ntive counterparts (note onset events) in the respective audio\nrecordings. This has to be done either in a manual annota-\ntion process or by relying on synthetic training data which\nis generated from digital sheet music formats such as Mus-\nescore orLilypond . As this kind of data representation is\nidentical to the one used in [7, 8] we refer to these works\nfor a detailed description of the entire alignment process.\n3. SCORE FOLLOWING AS A\nMARKOV DECISION PROCESS\nReinforcement learning can be seen as a computational ap-\nproach to learning from interaction to achieve a certain pre-\ndeﬁned goal. In this section, we formulate the task of score\nfollowing as a Markov Decision Process (MDP) , the math-\nematical foundation for reinforcement learning or, more\ngenerally, for the problem of sequential decision making1.\nFigure 2 provides an overview of the components involved\nin the score following MDP.\nThe score following agent (orlearner ) is the active\ncomponent that interacts with its environment , which in\nour case is the score following game. The interaction takes\nplace in a closed loop where the environment confronts\nthe agent with a new situation (a stateSt) and the agent\nhas to respond by making a decision, selecting one out\nof a predeﬁned set of possible actionsAt. After each ac-\ntion taken the agent receives the next state St+1and a nu-\nmerical reward signal Rt+1indicating how well it is do-\ning in achieving the overall goal. Informally, the agent’s\ngoal in our case is to track a performance in the score\n1The notation in this paper follows the book by Barto and Sutton [25]\nactionrewardScore Following\nAgent\nFigure 2 . Sketch of the score following MDP. The agent\nreceives the current state of the environment Stand a scalar\nreward signal Rtfor the action taken in the previous time\nstep. Based on the current state it has to choose an ac-\ntion (e.g. decide whether to increase, keep or decrease its\nspeed in the score) in order to maximize future reward by\ncorrectly following the performance in the score.\nSheet ImageSpectrogram SpectrogramSheet Image\nFigure 3 . Markov state of the score following MDP: the\ncurrent sheet sliding window and spectrogram excerpt. To\ncapture the dynamics of the environment we also add the\none step differences ( \u0001) wrt. the previous time step (state).\nas accurately and robustly as possible; this criterion will\nbe formalized in terms of an appropriate reward signal in\nSection 3.3 below. By running the MDP interaction loop\nwe end up with a sequence of states, actions and rewards\nS0;A0;R1;S1;A1;R2;S2;A2;R3;:::, which is the kind\nof experience a RL agent is learning its behavior from.\nWe will elaborate on different variants of the learning pro-\ncess in Section 4. The remainder of this section speciﬁes\nall components of the score following MDP in detail. In\npractice, our MDP is implemented as an environment in\nOpenAI-Gym2, an open source toolkit for developing and\ncomparing reinforcement learning algorithms.\n3.1 Score Following Markov States\nOur agents need to operate on two different inputs at the\nsame time, which together form the state Stof the MDP:\ninput modality one is a sliding window of the sheet image\nof the current piece, and modality two is an audio spectro-\ngram excerpt of the most recently played music ( \u00182 sec-\nonds). Figure 3 shows an example of this input data for a\npiece by J.S. Bach. Given the audio excerpt as an input the\nagent’s task is to navigate through the global score to con-\nstantly receive sheet windows from the environment that\nmatch the currently playing music. How this interaction\nwith the score takes place is explained in the next subsec-\ntion. The important part for now is to note that score fol-\n2https://gym.openai.com/Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 785lowing embodies dynamics which have to be captured by\nour state formulation, in order for the process to satisfy the\nMarkov property. Therefore, we extend the state represen-\ntation by adding the one step differences ( \u0001) of both the\nscore and the spectrogram. With the \u0001images and spec-\ntrograms a state contains all the information needed by the\nagent to determine where and how fast it is moving along\nin the sheet image.\n3.2 Agents, Actions and Policies\nThe next item in the MDP (Figure 2) is the agent, which\nis the component interacting with the environment by tak-\ning actions as a response to states received. As already\nmentioned, we interpret score following as a multimodal\ncontrol problem where the agent decides how fast it would\nlike to progress in the score. In more precise terms, the\nagent controls its score progression speed vpxlinpixels\nper time step by selecting from a set of actions At2\nf\u0000\u0001vpxl;0;+\u0001vpxlgafter receiving state Stin each time\nstep. Actions\u0006\u0001vpxlincrease or decrease the speed by a\nvalue of \u0001vpxlpixels per time step. Action a1= 0keeps it\nunchanged. To give an example: a pixel speed of vpxl= 14\nwould shift the sliding sheet window 14 pixels forward (to\nthe right) in the global unrolled score.\nFinally, we introduce the concept of a policy\u0019\u0002(ajs)to\ndeﬁne an agent’s behavior. \u0019is a conditional probability\ndistribution over actions conditioned on the current state.\nGiven a state s, it computes an action selection probabil-\nity\u0019\u0002(ajs)for each of the candidate actions a2At. The\nprobabilities are then used for sampling one of the possi-\nble actions. In Section 4 we explain how to use deep neural\nnetworks as function approximators for policy \u0019\u0002by opti-\nmizing the parameters \u0002of a policy network.\n3.3 Goal Deﬁnition: Reward Signal and State Values\nIn order to learn a useful action selection policy, the agent\nneeds feedback . This means that we need to deﬁne how to\nreport back to the agent how well it does in accomplishing\nthe task and, more importantly, what the task actually is.\nThe one component in an MDP that deﬁnes the over-\nall goal is the reward signal Rt2R. It is provided by\nthe environment in form of a scalar, each time the agent\nperforms an action. The sole objective of a RL agent is\nto maximize the cumulative reward over time . Note, that\nachieving this objective requires foresight and planning, as\nactions leading to high instantaneous reward might lead\nto unfavorable situations in the future. To quantify this\nlongterm success, RL introduces the returnGwhich is de-\nﬁned as the discounted cumulative future reward: Gt=\nRt+1+\rRt+2+\r2Rt+3+\u0001\u0001\u0001. The discount rate \r(with\n0:0< \r\u00141:0, in our case 0:9) is a hyper-parameter as-\nsigning less weight to future rewards if smaller than 1:0.\nFigure 4 summarizes the reward computation in our\nscore following MDP. Given annotated training data as de-\nscribed in Section 2, the environment knows, for each on-\nset time in the audio, the true target position xin the score.\nFrom this, and the current position ^xof the agent, we com-\npute the current tracking error as dx= ^x\u0000x, and deﬁne\nFigure 4 . Reward deﬁnition in the score following MDP.\nThe rewardRtdecays linearly (range [0, 1]) depending on\nthe agent’s distance dxto the current true score position x.\nthe reward signal rwithin a predeﬁned tracking window\n[x\u0000b;x+b]around target position xas:r= 1:0\u0000jdxj=b.\nThus, the reward per time step reaches its maximum of 1:0\nwhen the agent’s position is identical to the target posi-\ntion, and decays linearly towards 0:0as the tracking error\nreaches the maximum permitted value bgiven by the win-\ndow size. Whenever the absolute tracking error exceeds\nb(the agent drops out of the window), we reset the score\nfollowing game (back to start of score, ﬁrst audio frame).\nAs an RL agent’s sole objective is to maximize cumulative\nfuture reward, it will learn to match the correct position in\nthe score and to not lose its target by dropping out of the\nwindow. We deﬁne the target onset, corresponding to the\ntarget position in the score, as the rightmost frame in the\nspectrogram excerpt. This allows to run the agents on-line,\nintroducing only the delay required to compute the most\nrecent spectrogram frame. In practice, we linearly inter-\npolate the score positions for spectrogram frames between\ntwo subsequent onsets in order to produce a continuous and\nstronger learning signal for training.\nAs with policy \u0019, we will use function approximation\nto predict the future cumulative reward for a given state\ns, estimating how good the current state actually is. This\nestimated future reward is termed the valueV(s)of state\ns. We will see in the next section how state-of-the-art\nRL algorithms use these value estimates to stabilize the\nvariance-prone process of policy learning.\n4. LEARNING TO FOLLOW\nGiven the formal deﬁnition of score following as an MDP\nwe now describe how to address it with reinforcement\nlearning. Note that there is a large variety of RL algo-\nrithms. We focus on policy gradient methods , in particu-\nlar the class of actor-critic methods , due to their reported\nsuccess in solving control problems [9]. The learners uti-\nlized are REINFORCE with Baseline [27] and Synchronous\nAdvantage Actor Critic (A2C) [18, 28], where the latter is\nconsidered a state-of-the-art approach. As describing the\nmethods in full detail is beyond the scope of this paper, we\nprovide an intuition on how the methods work and refer the\nreader to the respective papers.786 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 5 . Multimodal network architecture used for our\nscore following agents. Given state sthe policy network\npredicts the action selection probability \u0019\u0002(ajs)for the\nallowed action At2 f\u0000 \u0001vpxl;0;+\u0001vpxlg. The value\nnetwork, sharing parameters with the policy network, pro-\nvides a state-value estimate V(s)for the current state.\n4.1 Policy and State-Value Approximation via DNNs\nIn Section 3, we introduced policy\u0019\u0002, determining the be-\nhavior of an agent, and value function V(s), predicting\nhow good a certain state sis with respect to cumulative\nfuture reward. Actor-critic methods make use of both con-\ncepts. The actor is represented by policy \u0019\u0002and is respon-\nsible for selecting the appropriate action in each state. The\ncritic is represented by the value function V(s)and helps\nthe agent to judge how good the selected actions actually\nare. In the context of deep RL both functions are approx-\nimated via a Deep Neural Network (DNN), termed policy\nand value network. We denote the parameters of the policy\nnetwork with \u0002in the following.\nFigure 5 shows a sketch of such a network architec-\nture. As the authors in [7], we use a multimodal convo-\nlutional neural network operating on both sheet music and\naudio at the same time. The input to the network is ex-\nactly the Markov state of the MDP introduced in Section\n3.1. The left part of the network processes sheet images,\nthe right part spectrogram excerpts (including \u0001images).\nAfter low-level representation learning, the two modali-\nties are merged by concatenation and further processed\nusing dense layers. This architecture implies that policy\nand value network share the parameters of the lower lay-\ners, which is a common choice in RL [18]. Finally, there\nare two output layers: the ﬁrst represents our policy and\npredicts the action selection probability \u0019\u0002(ajs). It con-\ntains three output neurons (one for each possible action)\nconverted into a valid probability distribution via soft-max\nactivation. The second output layer consists of one lin-\near output neuron predicting the value V(s)of the current\nstate. Table 1 lists the exact architectures used for our ex-\nperiments. We use exponential linear units for all but the\ntwo output layers [5].Table 1 .Network architecture. DO: Dropout, Conv( 3, stride-1)-\n16: 3\u00023 convolution, 16 feature maps and stride 1.\nAudio (Spectrogram) 78\u000240 Sheet-Image 80\u0002256\nConv( 3, stride-1)- 32 Conv( 5, stride-(1, 2))- 32\nConv( 3, stride-1)- 32 Conv( 3, stride-1)- 32\nConv( 3, stride-2)- 64 Conv( 3, stride-2)- 64\nConv( 3, stride-1)- 64+ DO(0.2) Conv( 3, stride-1)- 64+ DO(0.2)\nConv( 3, stride-2)- 64 Conv( 3, stride-2)- 64\nConv( 3, stride-2)- 96 Conv( 3, stride-2)- 64+ DO(0.2)\nConv( 3, stride-1)- 96 Conv( 3, stride-2)- 96\nConv( 1, stride-1)- 96+ DO(0.2) Conv( 1, stride-1)- 96+ DO(0.2)\nDense(512) Dense(512)\nConcatenation + Dense(512)\nDense(256) + DO(0.2) Dense(512) + DO(0.2)\nDense(3) - Softmax Dense(1) - Linear\n4.2 Learning a Policy via Actor-Critic\nOne of the ﬁrst algorithms proposed for optimiz-\ning a policy was REINFORCE [27], a Monte-Carlo\nalgorithm that learns by generating entire episodes\nS0;A0;R1;S1;A1;R2;S2;A2;:::of states, actions and re-\nwards by following policy \u0019\u0002while interacting with the\nenvironment. Given this sequence it updates the parame-\nters\u0002of the policy network according to the following up-\ndate rule by replaying the episode time step by time step:\n\u0002 \u0002 +\u000bGtr\u0002ln\u0019\u0002(AtjSt;\u0002) (1)\n\u000bis the step size or learning rate and Gtis the true\ndiscounted cumulative future reward (the return) received\nfrom time step tonwards. Gradient r\u0002is the direction in\nparameter space in which to go if we want to maximize the\nselection probability of the respective action. This means\nwhenever the agent did well, achieving a high return Gt,\nwe take larger steps in parameter space towards selecting\nthe responsible actions. By changing the parameters of\nthe policy network, we of course also change our policy\n(behavior) and we will select beneﬁcial actions more fre-\nquently in the future when confronted with similar states.\nREINFORCE and policy optimization are known to\nhave high variance in the gradient estimate [11]. This\nresults in slow learning and poor convergence properties.\nTo address this problem, REINFORCE with Baseline\n(REINFORCE bl) adapts the update rule of Equation (1) by\nsubtracting the estimated state value V(s)(see Section 3.3)\nfrom the actual return Gtreceived:\n\u0002 \u0002 +\u000b(Gt\u0000V(s))r\u0002ln\u0019\u0002(AtjSt;\u0002) (2)\nThis simple adaptation helps to reduce variance and im-\nproves convergence. The value network itself is learned\nby minimizing the mean squared error between the actu-\nally received return and the predicted value estimate of the\nnetwork, (Gt\u0000V(s))2. REINFORCE blwill be the ﬁrst\nlearning algorithm considered in our experiments.\nActor-critic methods are an extension of the baseline\nconcept, allowing agents to learn in an online fashion while\ninteracting with the environment. This avoids the need for\ncreating entire episodes prior to learning. In particular, our\nactor-critic agent will only look into the future a ﬁxed num-\nber oftmax time steps (in our case, 15). This implies that\nwe do not have the actual return Gtavailable for updatingProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 787the value function. The solution is to bootstrap the value\nfunction (i.e., update the value estimate with estimated val-\nues), which is the core characteristic of actor-critic meth-\nods. The authors in [18] propose the Synchronous Ad-\nvantage Actor Critic (A2C) and show that running multi-\nple actors (in our case 16) in parallel on different instances\nof the same kind of environment, further helps to stabilize\ntraining. We will see in our experiments that this also holds\nfor the score following task. For a detailed description of\nthe learning process we refer to the original paper [18].\n5. EXPERIMENTAL RESULTS\nIn this section we experimentally evaluate our RL ap-\nproach to score following and compare it to a previously\nintroduced method [7] that solves the same task. In addi-\ntion to quantitative analysis we also provide a video of our\nagents interacting with the score following environment.3\n5.1 Experimental Setup\nTwo different datasets will be used in our experiments. The\nNottingham Dataset comprises 296 monophonic melodies\nof folk music (training: 187, validation: 63, testing: 46); it\nwas already used in [7] to evaluate score following in sheet\nmusic images. The second dataset contains 479 classical\npieces by various composers such as Beethoven, Mozart\nand Bach, collected from the freely available Mutopia\nProject4(training: 360, validation: 19, testing: 100). It\ncovers polyphonic music and is a substantially harder chal-\nlenge to a score follower. In both cases the sheet music is\ntypeset with Lilypond and the audios are synthesized from\nMIDI using an acoustic piano sound font. This automatic\nrendering process provides the precise audio – sheet mu-\nsic alignments required for training (see Section 2). For\naudio processing we set the computation rate to 20 FPS\nand compute log-frequency spectrograms at a sample rate\nof 22.05kHz. The FFT is computed with a window size\nof 2048 samples and post-processed with a logarithmic ﬁl-\nterbank allowing only frequencies from 60Hz to 6kHz (78\nfrequency bins).\nThe spectrogram context visible to the agents is set to\n40 frames (2 sec. of audio) and the sliding window sheet\nimages cover 160\u0002512pixels and are further downscaled\nby a factor of two before being presented to the network.\nAs optimizer we use the Adam update rule [15] with an ini-\ntial learning rate of 10\u00004and running average coefﬁcients\nof0:5and0:999. We then train the models until there is no\nimprovement in the number of tracked onsets on the val-\nidation set for 50epochs and reduce the learning rate by\nfactor 10three times. The tempo change action \u0001vpxlis\n0:5for Nottingham and 1:0for the polyphonic pieces.\n5.2 Evaluation Measures and Baselines\nRecall from Section 3.3 and Figure 4 that from the agent’s\nposition ^xand the ground truth position x, we compute the\ntracking error dx. This error is the basis for our evaluation\n3score following video: https://youtu.be/COPNciY510g\n4http://www.mutopiaproject.org/\n0 50 100 150 200 250 300 350 4000246optimal speed\n0 50 100 150 200 250 300 350 400\ntime step t5.0\n2.5\n0.02.5optimal action AtFigure 6 . Optimal tempo curve and corresponding opti-\nmal actions Atfor a continuous agent (piece: J. S. Bach,\nBWV994). The Atwould be the target values for training\nan agent with supervised, feed-forward regression.\nmeasures. However, compared to training, we only con-\nsider time steps in our evaluation where there is actually\nan onset present in the audio. While interpolating inter-\nmediate time steps is helpful for creating a stronger learn-\ning signal (Section 3.3), it is not musically meaningful.\nSpeciﬁcally, we will report the evaluation statistics mean\nabsolute tracking error jdxjas well as its standard devi-\nationstd(jdxj)over all test pieces. These two measures\nquantify the accuracy of the score followers. To also mea-\nsure their robustness we compute the ratio Ronof overall\ntracked onsets as well as the ratio of pieces Rtuetracked\nfrom beginning entirely to the end.\nAsbaseline method we consider the approach described\nin [7], which models score following as a multimodal lo-\ncalization task (denoted by MM-Loc in the following).\nAs a second baseline , we also tried to train an agent to\nsolve the score following MDP in a fully supervised fash-\nion. This is theoretically possible, as we know for each\ntime point the exact corresponding position in the score\nimage, which permits us to derive an optimal tempo curve\nand, consequently, an optimal sequence of tempo changes\nfor each of the training pieces. Figure 6 shows such an op-\ntimal tempo curve along with the respective tempo change\nactions for a short Bach piece. The latter would serve as\ntargetsyin a supervised regression problem y=f(x).\nThe network structure used for this experiment is identi-\ncal to the one in Figure 5 except for the output layers. In-\nstead of policy \u0019\u0012and valueVwe only keep a single linear\noutput neuron predicting the value of the optimal tempo\nchange in each time step. However, a closer look at Fig-\nure 6 already reveals the problem inherent in this approach.\nThe optimal tempo change is close to zero most of the time.\nFor the remaining time steps we observe sparse spikes of\nvarying amplitude. When trying to learn to approximate\nthese optimal tempo changes (with a mean squared error\noptimization target), we ended up with a network that pre-\ndicts values very close to zero for all its inputs. We con-\nclude that the relevant tempo change events are too sparse\nfor supervised learning and exclude the method from our\ntables in the following. Besides these technical difﬁculties\nwe will also discuss conceptual advantages of addressing\nscore following as an MDP in Section 6.788 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Method Rtue Ronjdxj std(jdxj)\nNottingham (monophonic, 46 test pieces)\nMM-Loc [7] 0.43 0.65 3.15 13.15\nREINFORCE bl 0.94 0.96 4.21 4.59\nA2C 0.96 0.99 2.17 3.53\nMutopia (polyphonic, 100 test pieces)\nMM-Loc [7] 0.61 0.72 62.34 298.14\nREINFORCE bl 0.20 0.35 48.61 41.99\nA2C 0.74 0.75 19.25 23.23\nTable 2 . Comparison of score following approaches. Best\nresults are marked in bold. For A2C and REINFORCE bl\nwe report the average over 10 evaluation runs.\n5.3 Experimental Results\nTable 2 provides a summary of the experimental results.\nLooking at the Nottingham dataset, we observe large gaps\nin performance between the different approaches. Both\nRL based methods manage to follow almost all of the test\npieces completely to the end. In addition, the mean track-\ning error is lower for A2C and shows a substantially lower\nstandard deviation. The high standard deviation for MM-\nLoc is even more evident in the polyphonic pieces. The\nreason is that MM-Loc is formulated as a localization task,\npredicting a location probability distribution over the score\nimage given the current audio. Musical passages can be\nhighly repetitive, which leads to multiple modes in the lo-\ncation probability distribution, each of which is equally\nprobable. As the MM-Loc tracker follows the mode with\nhighest probability it starts to jump between such ambigu-\nous structures, producing a high standard deviation for the\ntracking error and, in the worst case, loses the target.\nOur MDP formulation of score following addresses this\nissue, as the agent controls its progression speed for navi-\ngating through the sheet image. This restricts the agent as\nit does not allow for large jumps in the score and, in ad-\ndition, is much closer to how music is actually performed\n(e.g. from left to right and top to bottom when excluding\nrepetitions). Our results (especially the ones of A2C) re-\nﬂect this theoretical advantage.\nHowever, in the case of complex polyphonic scores we\nalso observe that the performance of REINFORCE blde-\ngrades completely. The numbers reported are the outcome\nof more than ﬁve days of training. We already mentioned\nin Section 4 that policy optimization is known to have\nhigh variance in the gradient estimate [11], which is ex-\nactly what we observe in our experiments. Even though\nREINFORCE blmanaged to learn a useful policy for the\nNottingham dataset it also took more than ﬁve days to ar-\nrive at that. In contrast, A2C learns a successful policy for\nthe Nottingham dataset in less than six hours and outper-\nforms the baseline method on both datasets. For Mutopia\nit tracks more than 70% of the 100 test pieces entirely to\nthe end without losing the target a single time. This re-sult comes with an average error of only 20 pixels which is\nabout 5mm in a standard A4 page of Western sheet music\n– three times more accurate than the baseline with a mean\nerror of 62 pixels.\nWe also report the results of REINFORCE blto em-\nphasize the potential of RL in this setting. Recall that\nthe underlying MDP is the same for both REINFORCE bl\nand A2C. The only part that changes is a more powerful\nlearner. All other components including network architec-\nture, optimization algorithm and environment remain un-\ntouched. Considering that deep RL is currently one of the\nmost intensively researched areas in machine learning, we\ncan expect further improvement in the score following task\nwhenever there is an advance in RL itself.\n6. DISCUSSION AND CONCLUSION\nWe have proposed a formulation of score following in\nsheet music images as a Markov decision process and\nshowed how to address it with state-of-the-art deep rein-\nforcement learning. Experimental results on monophonic\nand polyphonic piano music show that this is competitive\nwith recently introduced methods [7]. We would like to\nclose with a discussion of some speciﬁc aspects that point\nto interesting future perspectives.\nFirstly, we trained all agents using a continuous reward\nsignal computed by interpolating the target ( ground truth )\nlocation between successive onsets and note heads. Re-\ninforcement learners can, of course, also learn from a de-\nlayed signal (e.g. non-zero rewards only at actual onsets or\neven bar lines or downbeats). This further implies that we\ncould, for example, take one of our models trained on the\nsynthesized audios, annotate a set of real performance au-\ndios at the bar level (which is perfectly feasible), and then\nﬁne-tune the models with the very same algorithms, with\nthe sole difference that for time points without annotation\nthe environment simply returns a neutral reward of zero.\nSecondly, we have already started to experiment with\ncontinuous control agents that directly predict the required\ntempo changes, rather than relying on a discrete set of ac-\ntion. Continuous control has proven to be very successful\nin other domains [9] and would allow for a perfect align-\nment of sheet music and audio (cf. Figure 6).\nA ﬁnal remark concerns RL in general. For many RL\nbenchmarks we are given a simulated environment that the\nagents interact with. These environments are ﬁxed prob-\nlems without a natural split into training, validation and\ntesting situations. This is different in our setting, and one\nof the main challenges is to learn agents, which generalize\nto unseen pieces and audio conditions. While techniques\nsuch as weight-decay, dropout [24] or batch-normalization\n[13] have become a standard tool for regularization in su-\npervised learning they are not researched in the context of\nRL. A broad benchmark of these regularizers in the context\nof RL would be therefore of high relevance.\nWe think that all of this makes the score following MDP\na promising and in our opinion very exciting playground\nfor further research in both music information retrieval and\nreinforcement learning.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 7897. ACKNOWLEDGEMENTS\nThis work is supported by the European Research Council\n(ERC Grant Agreement 670035, project CON ESPRES-\nSIONE).\n8. REFERENCES\n[1] Andreas Arzt, Harald Frostel, Thassilo Gadermaier,\nMartin Gasser, Maarten Grachten, and Gerhard Wid-\nmer. Artiﬁcial intelligence in the concertgebouw. In\nProceedings of the Twenty-Fourth International Joint\nConference on Artiﬁcial Intelligence (IJCAI) , pages\n2424–2430, Buenos Aires, Argentina, 2015.\n[2] Andreas Arzt, Gerhard Widmer, and Simon Dixon. Au-\ntomatic page turning for musicians via real-time ma-\nchine listening. In Proceedings of the 18th European\nConference on Artiﬁcial Intelligence (ECAI) , pages\n241–245, Patras, Greece, 2008.\n[3] Stefan Balke, Sanu Pulimootil Achankunju, and\nMeinard M ¨uller. Matching musical themes based on\nnoisy OCR and OMR input. In Proceedings of the\nIEEE International Conference on Acoustics, Speech,\nand Signal Processing (ICASSP) , pages 703–707, Bris-\nbane, Australia, 2015.\n[4] Donald Byrd and Jakob Grue Simonsen. Towards a\nstandard testbed for optical music recognition: Deﬁni-\ntions, metrics, and page images. Journal of New Music\nResearch , 44(3):169–195, 2015.\n[5] Djork-Arn ´e Clevert, Thomas Unterthiner, and Sepp\nHochreiter. Fast and accurate deep network learn-\ning by exponential linear units (elus). Interna-\ntional Conference on Learning Representations (ICLR)\n(arXiv:1511.07289) , 2015.\n[6] Arshia Cont. A coupled duration-focused architecture\nfor realtime music to score alignment. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence ,\n32(6):837–846, 2009.\n[7] Matthias Dorfer, Andreas Arzt, and Gerhard Widmer.\nTowards score following in sheet music images. In Pro-\nceedings of the 17th International Society for Music\nInformation Retrieval Conference (ISMIR) , pages 789–\n795, New York City, United States, 2016.\n[8] Matthias Dorfer, Andreas Arzt, and Gerhard Widmer.\nLearning audio-sheet music correspondences for score\nidentiﬁcation and ofﬂine alignment. In Proceedings\nof the 18th International Society for Music Informa-\ntion Retrieval Conference (ISMIR) , pages 115–122,\nSuzhou, China, 2017.\n[9] Yan Duan, Xi Chen, Rein Houthooft, John Schulman,\nand Pieter Abbeel. Benchmarking deep reinforcement\nlearning for continuous control. In Proceedings of the\n33nd International Conference on Machine Learning\n(ICML .[10] Zhiyao Duan and Bryan Pardo. A state space model for\non-line polyphonic audio-score alignment. In Proc. of\nthe IEEE Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , Prague, Czech Republic, 2011.\n[11] Evan Greensmith, Peter L. Bartlett, and Jonathan Bax-\nter. Variance reduction techniques for gradient esti-\nmates in reinforcement learning. Journal of Machine\nLearning Research , 5:1471–1530, 2004.\n[12] Jan Haji ˇc jr and Pavel Pecina. The MUSCIMA++\nDataset for Handwritten Optical Music Recognition.\nIn14th International Conference on Document Analy-\nsis and Recognition (ICDAR) , pages 39–46, New York,\nUnited States, 2017.\n[13] Sergey Ioffe and Christian Szegedy. Batch normaliza-\ntion: Accelerating deep network training by reducing\ninternal covariate shift. In Proceedings of the 32nd In-\nternational Conference on Machine Learning, ICML\n2015, Lille, France, 6-11 July 2015 , pages 448–456,\n2015.\n[14] ¨Ozg¨ur Izmirli and Gyanendra Sharma. Bridging\nprinted music and audio through alignment using a\nmid-level score representation. In Proceedings of the\n13th International Society for Music Information Re-\ntrieval Conference (ISMIR) , pages 61–66, Porto, Por-\ntugal, 2012.\n[15] Diederik Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. International Conference on\nLearning Representations (ICLR) (arXiv:1412.6980) ,\n2015.\n[16] Frank Kurth, Meinard M ¨uller, Christian Fremerey,\nYoon-ha Chang, and Michael Clausen. Automated syn-\nchronization of scanned sheet music with audio record-\nings. In Proceedings of the 8th International Confer-\nence on Music Information Retrieval (ISMIR) , pages\n261–266, Vienna, Austria, 2007.\n[17] Marius Miron, Julio Jos ´e Carabias-Orti, and Jordi\nJaner. Audio-to-score alignment at the note level for\norchestral recordings. In Proceedings of the 15th Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR) , pages 125–130, Taipei, Taiwan, 2014.\n[18] V olodymyr Mnih, Adri `a Puigdom `enech Badia, Mehdi\nMirza, Alex Graves, Timothy P. Lillicrap, Tim Harley,\nDavid Silver, and Koray Kavukcuoglu. Asynchronous\nmethods for deep reinforcement learning. In Proceed-\nings of the 33nd International Conference on Machine\nLearning (ICML .\n[19] V olodymyr Mnih, Koray Kavukcuoglu, David Silver,\nAndrei A. Rusu, Joel Veness, Marc G. Bellemare,\nAlex Graves, Martin A. Riedmiller, Andreas Fidjeland,\nGeorg Ostrovski, Stig Petersen, Charles Beattie, Amir\nSadik, Ioannis Antonoglou, Helen King, Dharshan Ku-\nmaran, Daan Wierstra, Shane Legg, and Demis Hass-\nabis. Human-level control through deep reinforcement\nlearning. Nature , 518(7540):529–533, 2015.790 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[20] Meinard M ¨uller. Fundamentals of Music Processing .\nSpringer Verlag, 2015.\n[21] Eita Nakamura, Philippe Cuvillier, Arshia Cont, Nobu-\ntaka Ono, and Shigeki Sagayama. Autoregressive hid-\nden semi-markov model of symbolic music perfor-\nmance for score following. In Proceedings of the 16th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 392–398, M ´alaga, Spain,\n2015.\n[22] Matthew Prockup, David Grunberg, Alex Hrybyk, and\nYoungmoo E. Kim. Orchestral performance compan-\nion: Using real-time audio to score alignment. IEEE\nMultimedia , 20(2):52–60, 2013.\n[23] Christopher Raphael. Music Plus One and machine\nlearning. In Proceedings of the International Confer-\nence on Machine Learning (ICML) , 2010.\n[24] Nitish Srivastava, Geoffrey E. Hinton, Alex\nKrizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov. Dropout: a simple way to prevent neural\nnetworks from overﬁtting. Journal of Machine\nLearning Research , 15(1):1929–1958, 2014.\n[25] Richard S. Sutton and Andrew G. Barto. Reinforcement\nlearning - an introduction . Adaptive computation and\nmachine learning. MIT Press, 1998.\n[26] Verena Thomas, Christian Fremerey, Meinard M ¨uller,\nand Michael Clausen. Linking Sheet Music and Au-\ndio - Challenges and New Approaches. In Meinard\nM¨uller, Masataka Goto, and Markus Schedl, editors,\nMultimodal Music Processing , volume 3 of Dagstuhl\nFollow-Ups , pages 1–22. Schloss Dagstuhl–Leibniz-\nZentrum fuer Informatik, Dagstuhl, Germany, 2012.\n[27] Ronald J. Williams. Simple statistical gradient-\nfollowing algorithms for connectionist reinforcement\nlearning. Machine Learning , 8:229–256, 1992.\n[28] Yuhuai Wu, Elman Mansimov, Shun Liao, Roger B.\nGrosse, and Jimmy Ba. Scalable trust-region method\nfor deep reinforcement learning using kronecker-\nfactored approximation. CoRR , abs/1708.05144, 2017.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 791"
    },
    {
        "title": "Audio-Aligned Jazz Harmony Dataset for Automatic Chord Transcription and Corpus-based Research.",
        "author": [
            "Vsevolod Eremenko",
            "Emir Demirel",
            "Baris Bozkurt",
            "Xavier Serra"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492457",
        "url": "https://doi.org/10.5281/zenodo.1492457",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/206_Paper.pdf",
        "abstract": "In this paper we present a new dataset of time-aligned jazz harmony transcriptions. This dataset is a useful resource for content-based analysis, especially for training and evaluating chord transcription algorithms. Most of the available chord transcription datasets only contain annotations for rock and pop, and the characteristics of jazz, such as the extensive use of seventh chords, are not represented. Our dataset consists of annotations of 113 tracks selected from \"The Smithsonian Collection of Classic Jazz\" and \"Jazz: The Smithsonian Anthology,\" covering a range of performers, subgenres, and historical periods. Annotations were made by a jazz musician and contain information about the meter, structure, and chords for entire audio tracks. We also present evaluation results of this dataset using stateof-the-art chord estimation algorithms that support seventh chords. The dataset is valuable for jazz scholars interested in corpus-based research. To demonstrate this, we extract statistics for symbolic data and chroma features from the audio tracks.",
        "zenodo_id": 1492457,
        "dblp_key": "conf/ismir/EremenkoDBS18",
        "keywords": [
            "time-aligned jazz harmony transcriptions",
            "useful resource for content-based analysis",
            "chord transcription algorithms",
            "jazz dataset",
            "The Smithsonian Collection of Classic Jazz",
            "Jazz: The Smithsonian Anthology",
            "annotations by a jazz musician",
            "evaluation results of chord estimation algorithms",
            "seventh chords",
            "corpus-based research"
        ],
        "content": "AUDIO-ALIGNED JAZZ HARMONY DATASET FOR AUTOMATIC\nCHORD TRANSCRIPTION AND CORPUS-BASED RESEARCH\nVsevolod Eremenko, Emir Demirel, Baris Bozkurt, Xavier Serra\nMusic Technology Group, Universitat Pompeu Fabra, Barcelona\nfirstname.lastname@upf.edu\nABSTRACT\nIn this paper we present a new dataset of time-aligned jazz\nharmony transcriptions. This dataset is a useful resource\nfor content-based analysis, especially for training and eval-\nuating chord transcription algorithms. Most of the avail-\nable chord transcription datasets only contain annotations\nfor rock and pop, and the characteristics of jazz, such as the\nextensive use of seventh chords, are not represented. Our\ndataset consists of annotations of 113 tracks selected from\n“The Smithsonian Collection of Classic Jazz” and “Jazz:\nThe Smithsonian Anthology,” covering a range of perform-\ners, subgenres, and historical periods. Annotations were\nmade by a jazz musician and contain information about\nthe meter, structure, and chords for entire audio tracks. We\nalso present evaluation results of this dataset using state-\nof-the-art chord estimation algorithms that support seventh\nchords. The dataset is valuable for jazz scholars interested\nin corpus-based research. To demonstrate this, we extract\nstatistics for symbolic data and chroma features from the\naudio tracks.\n1. INTRODUCTION\nMusicians in many genres use an abbreviated notation,\nknown as a lead sheet, to represent chord progressions.\nDigitized collections of lead sheets are used for computer-\naided corpus-based musicological research, e.g., [6,13,18,\n31]. Lead sheets do not provide information about how\nspeciﬁc chords are rendered by musicians [21]. To reﬂect\nthis rendering, music information retrieval (MIR) and mu-\nsicology communities have created several datasets of au-\ndio recordings annotated with chord progressions. Such\ncollections are used for training and evaluating various\nMIR algorithms (e.g., Automatic Chord Estimation) and\nfor corpus-based research.\nBecause providing chord annotations for audio is time-\nconsuming and requires qualiﬁed annotators, there are few\nsuch datasets available for MIR research. Of the existing\ndatasets, most are of rock and pop music, with very few\nc\rVsevolod Eremenko, Emir Demirel, Baris Bozkurt,\nXavier Serra. Licensed under a Creative Commons Attribution 4.0 Inter-\nnational License (CC BY 4.0). Attribution: Vsevolod Eremenko, Emir\nDemirel, Baris Bozkurt, Xavier Serra. “ Audio-Aligned Jazz Harmony\nDataset for Automatic Chord Transcription and Corpus-based Research”,\n19th International Society for Music Information Retrieval Conference,\nParis, France, 2018.available for jazz. A balanced and comprehensive corpus\nof jazz audio recordings with chord transcription would be\na useful resource for developing MIR algorithms aimed to\nserve jazz scholars. The particularities of jazz also allow us\nto view the dataset’s format, content selection, and chord\nestimation accuracy evaluation from a different angle.\nThis paper starts with a review of publicly available\ndatasets that contain information about harmony, such as\nchord progressions and structural analysis. Based on this\nreview, we justify the necessity of creating a representa-\ntive, balanced jazz dataset in a new format. We present our\ndataset, which contains lead sheet style chords, beat on-\nsets, and structure annotations for a selection of jazz audio\ntracks, along with full-length annotations for each record-\ning. We explain our track selection principle and transcrip-\ntion methodology, and also provide pre-calculated chroma\nfeatures [27] for the entire dataset. We then discuss how to\nevaluate the performance of Automatic Chord Transcrip-\ntion on jazz recordings. Moreover, baseline evaluation\nscores for two state-of-the-art chord estimation algorithms\nare shown. Dataset is available online1.\n2. RELATED WORKS\n2.1 Chord annotated audio datasets\nHere we review existing datasets with respect to their for-\nmat, content selection principle, annotation methodology,\nand their uses in research. We then discuss some discrep-\nancies in different approaches to chord annotation, as well\nas the advantages and drawbacks of different formats.\n2.1.1 Isophonics family\nIsophonics2is one of the ﬁrst time-aligned chord anno-\ntation datasets, introduced in [17]. Initially, the dataset\nconsisted of twelve studio albums by The Beatles. Harte\njustiﬁed his selection by stating that it is a small but var-\nied corpus (including various styles, recording techniques\nand “complex harmonic progressions” in comparison with\nother popular music artists). These albums are “widely\navailable in most parts of the world” and have had enor-\nmous inﬂuence on the development of pop music. A num-\nber of related theoretical and critical works was also taken\ninto account. Later the corpus was augmented with some\n1http://doi.org/10.5281/zenodo.1290736 . Documen-\ntation: https://mtg.github.io/JAAH\n2Available at http://isophonics.net/content/\nreference-annotations483transcriptions of Carole King, Queen, Michael Jackson,\nand Zweieck.\nThe corpus is organized as a directory of “.lab” ﬁles3.\nEach line describes a chord segment with a start time,\nend time (in seconds), and chord label in the “Harte et\nal.” format [16]. The annotator recorded chord start times\nby tapping keys on a keyboard. The chords were tran-\nscribed using published analyses as a starting point, if pos-\nsible. Notes from the melody line were not included in the\nchords. The resulting chord progression was veriﬁed by\nsynthesizing the audio and playing it alongside the original\ntracks. The dataset has been used for training and testing\nchord evaluation algorithms (e.g., for MIREX4).\nThe same format is used for the “Robbie Williams\ndataset”5announced in [12]; for the chord annotations\nof the RWC and USPop datasets6; and for the datasets by\nDeng: JayChou29, CNPop20, and JazzGuitar997. Deng\npresented this dataset in [11], and it is the only one in the\nfamily which is related to jazz. However, it uses 99 short,\nguitar-only pieces recorded for a study book, and thus does\nnot reﬂect the variety of jazz styles and instrumentations.\n2.1.2 Billboard\nAuthors of the Billboard8dataset argued that both mu-\nsicologists and MIR researchers require a wider range of\ndata [7]. They selected songs randomly from the Billboard\n“Hot 100” chart in the United States between 1958 and\n1991.\nTheir format is close to the traditional lead sheet: it con-\ntains meter, bars, and chord labels for each bar or for par-\nticular beats of a bar. Annotations are time-aligned with\nthe audio by the assignment of a timestamp to the start of\neach “phrase” (usually 4 bars). The “Harte et al.” syntax\nwas used for the chord labels (with a few additions to the\nshorthand system). The authors accompanied the annota-\ntions with pre-extracted NNLS Chroma features [27]. At\nleast three persons were involved in making and reconcil-\ning a singleton annotation for each track. The corpus is\nused for training and testing chord evaluation algorithms\n(e.g., MIREX ACE evaluation) and for musicological re-\nsearch [13].\n2.1.3 Rockcorpus and subjectivity dataset\nRockcorpus9was announced in [9]. The corpus currently\ncontains 200 songs selected from the “500 Greatest Songs\nof All Time” list, which was compiled by the writers of\nRolling Stone magazine, based on polls of 172 “rock stars\nand leading authorities.”\nAs in the Billboard dataset, the authors specify the\nstructure segmentation and assign chords to bars (and to\n3ASCII plain text ﬁles which are used by a variety of popular MIR\ntools, e.g., Sonic Visualizer [8].\n4http://www.music-ir.org/mirex/wiki/MIREX_HOME\n5http://ispg.deib.polimi.it/mir-software.html\n6https://github.com/tmc323/Chord-Annotations\n7http://www.tangkk.net/label\n8http://ddmal.music.mcgill.ca/research/\nbillboard\n9http://rockcorpus.midside.com/2011_paper.htmlbeats if necessary), but not directly to time segments. A\ntimestamp is speciﬁed for each measure bar.\nIn contrast to the previous datasets, authors do not use\n“absolute” chord labels, e.g., C:maj . Instead, they spec-\nify tonal centers for parts of the composition and chords\nas Roman numerals. These show the chord quality and the\nrelation of the chord’s root to the tonic. This approach fa-\ncilitates harmony analysis.\nEach of the two authors provides annotations for each\nrecording. As opposed to the aforementioned examples,\nthe authors do not aim to produce a single \"ground truth\"\nannotation, but keep both versions. Thus it becomes pos-\nsible to study subjectivity in human annotations of chord\nchanges. The Rockcorpus is used for training and testing\nchord evaluation algorithms [19], and for musicological re-\nsearch [9].\nConcerning the study of subjectivity, we should also\nmention the Chordify Annotator Subjectivity Dataset10,\nwhich contains transcriptions of 50 songs from the Bill-\nboard dataset by four different annotators [22]. It uses\nJSON-based JAMS annotation format.\n2.2 Jazz-related datasets\nHere we review datasets which do not have audio-aligned\nchord annotations as their primary purpose, but neverthe-\nless can be useful in the context of jazz harmony studies.\n2.2.1 Weimar Jazz Database\nThe main focus of the Weimar Jazz Database (WJazzD11)\nis jazz soloing. Data is disseminated as a SQLite database\ncontaining transcription and meta information about 456\ninstrumental jazz solos from 343 different recordings\n(more than 132000 beats over 12.5 hours). The database\nincludes: meter, structure segmentation, measures, and\nbeat onsets, along with chord labels in a custom format.\nHowever, as stated by Pﬂeiderer [30], the chords were\ntaken from available lead sheets, “cloned” for all choruses\nof the solo, and only in some cases transcribed from what\nwas actually played by the rhythm section.\nThe database’s metadata includes the MusicBrainz12\nIdentiﬁer, which allows users to link the annotation to\na particular audio recording and fetch meta-information\nabout the track from the MusicBrainz server.\nAlthough WJazzD has signiﬁcant applications for re-\nsearch in the symbolic domain [30], our experience has\nshown that obtaining audio tracks for analysis and aligning\nthem with the annotations is nontrivial: the MusicBrainz\nidentiﬁers are sometimes wrong, and are missing for 8%\nof the tracks. Sometimes WJazzD contains annotations\nof rare or old releases. In different masterings, the tempo\nand therefore the beat positions, differs from modern and\nwidely available releases. We matched 14 tracks from\nWJazzD to tracks in our dataset by the performer’s name\n10https://github.com/chordify/CASD\n11http://jazzomat.hfm-weimar.de\n12A community-supported collection of music recording metadata:\nhttps://musicbrainz.org484 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018and the date of the recording. In three cases the Mu-\nsicBrainz Release is missing, and in three cases rare com-\npilations were used as sources. It took some time to dis-\ncover that three of the tracks (“Embraceable You”, “Lester\nLeaps In”, “Work Song”) are actually alternative takes,\nwhich are ofﬁcially available only on extended reissues.\nBeat positions in the other eleven tracks must be shifted\nand sometimes scaled to match available audio (e.g., for\n“Walking Shoes”). This may be improved by using an in-\nteresting alternative introduced by Balke et al. [3]: a web-\nbased application, “JazzTube,” which matches YouTube\nvideos with WJazzD annotations and provides interactive\neducational visualizations.\n2.2.2 Symbolic datasets\nThe iRb13dataset (announced in [6]) contains chord pro-\ngressions for 1186 jazz standards taken from a popular in-\nternet forum for jazz musicians. It lists the composer, lyri-\ncist, and year of creation. The data are written in the Hum-\ndrum encoding system. The chord data are submitted by\nanonymous enthusiasts and thus provides a rather modern\ninterpretation of jazz standards. Nevertheless, Broze and\nShanahan proved it was useful for corpus-based musicol-\nogy research: see [6] and [31].\n“Charlie Parker’s Omnibook data”14contains chord\nprogressions, themes, and solo scores for 50 recordings by\nCharlie Parker. The dataset is stored in MusicXML and\nintroduced in [10].\nGranroth-Wilding’s “JazzCorpus”15contains 76 chord\nprogressions (approximately 3000 chords) annotated with\nharmonic analyses (i.e., tonal centers and roman numerals\nfor the chords), with the primary goal of training and test-\ning statistical parsing models for determining chord har-\nmonic functions [15].\n2.3 Discussion\n2.3.1 Some discrepancies in chord annotation\napproaches in the context of jazz\nAn article by Harte et al. [16] de facto sets the standard\nfor chord labels in MIR annotations. It describes the basic\nsyntax and a shorthand system. The basic syntax explic-\nitly deﬁnes a chord pitch class set. For example, C:(3,\n5, b7) is interpreted as C, E, G, B [. The shorthand sys-\ntem contains symbols which resemble chord representa-\ntions on lead sheets (e.g., C:7 stands for C dominant sev-\nenth). According to [16], C:7 should be interpreted as\nC:(3, 5, b7) . However, this may not always be the\ncase in jazz. According to theoretical research [25] and ed-\nucational books, e.g., [23], the 5th degree is omitted quite\noften in jazz harmony.\nGenerally speaking, since chord labels emerged in jazz\nand pop music practice in the 1930s, they provide a higher\n13https://musiccog.ohio-state.edu/home/index.\nphp/iRb_Jazz_Corpus\n14https://members.loria.fr/KDeguernel/omnibook/\n15http://jazzparser.granroth-wilding.co.uk/\nJazzCorpus.htmllevel of abstraction than sheet music scores, allowing musi-\ncians to improvise their parts [21]. Similarly, a transcriber\ncan use the single chord label C:7 to mark the whole pas-\nsage containing the walking bass line and comping piano\nphrase, without even noticing, “Is the 5th really played?”\nThus, for jazz corpus annotation, we suggest accepting the\n“Harte et al.” syntax for the purpose of standardization,\nbut sticking to shorthand system and avoiding a literal in-\nterpretation of the labels.\nThere are two different approaches to chord annotation:\n\u000f“Lead sheet style.” Contains a lead sheet [21], which\nhas obvious meaning to musicians practicing the\ncorresponding style (e.g., jazz or rock). It is aligned\nto audio with timestamps for beats or measure bars.\nChords are considered in a rhythmical framework.\nThis style is convenient because the annotation pro-\ncess can be split into two parts: lead sheet transcrip-\ntion done by a qualiﬁed musician, and beats annota-\ntion done by a less skilled person or sometimes even\nautomatically performed.\n\u000f“Isophonics style.” Chord labels are bound to abso-\nlute time segments.\nWe must note that musicians use chord labels for instruct-\ning and describing performance mostly within the lead\nsheet framework. While the lead sheet format and the\nchord-beats relationship is obvious, detecting and inter-\npreting “chord onset” times in jazz is an unclear task. The\nwidely used comping approach to accompaniment [23] as-\nsumes playing phrases instead of long isolated chords, and\na given phrase does not necessarily start with a chord tone.\nFurthermore, individual players in the rhythm section (e.g.,\nbassist and guitarist) may choose different strategies: they\nmay anticipate a new chord, play it on the downbeat, or\ndelay. Thus, before annotating “chord onset” times, we\nshould make sure that it makes musical and perceptual\nsense. All known corpus-based research is based on “lead\nsheet style” annotated datasets. Taking all these considera-\ntions into account, we prefer to use the lead sheet approach\nto chord annotations.\n2.3.2 Criteria for format and dataset for chord annotated\naudio\nBased on the given review and our own hands-on ex-\nperience with chord estimation algorithm evaluation, we\npresent our guidelines and propositions for building an\naudio-aligned chord dataset.\n1. Clearly deﬁne dataset boundaries (e.g., a certain mu-\nsic style or time period). The selection of audio\ntracks should be representative and balanced within\nthese boundaries.\n2. Since sharing audio is restricted by copyright laws,\nuse recent releases and existing compilations to fa-\ncilitate access to dataset audio.\n3. Use the time-aligned lead sheet approach with\n“shorthand” chord labels from [16], but avoid their\nliteral interpretation.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 4854. Annotate entire tracks, but not excerpts. This makes\nit possible to explore structure and self-similarity.\n5. Provide the MusicBrainz identiﬁer to exploit meta-\ninformation from this service. If feasible, add meta-\ninformation to MusicBrainz instead of storing it pri-\nvately within the dataset.\n6. Annotate in a format that is not only machine read-\nable, but convenient for further manual editing and\nveriﬁcation. Relying on plain text ﬁles and speciﬁc\ndirectory structure for storing heterogeneous anno-\ntation is not practical for users. JSON-based JAMS\nformat introduced by Humphrey et al. [20] solves\nthis issue, but currently does not support lead sheet\nchord annotation. It is verbose in order to be com-\nfortably used by the human annotators and supervi-\nsors.\n7. Include pre-extracted chroma features. This makes it\npossible to conduct some MIR experiments without\naccessing the audio. It would be interesting to incor-\nporate chroma features into corpus-based research to\ndemonstrate how a particular chord class is rendered\nin a particular recording.\n3. PROPOSED DATASET\n3.1 Data format and annotation attributes\nTaking into consideration the discussion from the previ-\nous section, we decided to use the JSON format. An ex-\ncerpt from an annotation is shown in Figure 1. We pro-\nvide the track title, artist name, and MusicBrainz ID. The\nstart time, duration of the annotated region, and tuning fre-\nquency estimated automatically by Essentia [5] are shown.\nThe beat onsets array and chord annotations are nested into\nthe “parts” attribute, which in turn could recursively con-\ntain “parts.” This hierarchy represents the structure of the\nmusical piece. Each part has a “name” attribute which de-\nscribes the purpose of the part, such as “intro,” “head,”\n“coda,” “outro,” “interlude,” etc. The inner form of the\nchorus (e.g., AABA, ABAC, blues) and predominant in-\nstrumentation (e.g., ensemble, trumpet solo, vocals female,\netc.) are annotated explicitly. This structural annotation is\nbeneﬁcial for extracting statistical information regarding\nthe type of chorus present in the dataset, as well as other\nmusically important properties. We made chord annota-\ntions in the lead sheet style: each annotation string rep-\nresents a sequence of measure bars, delimited with pipes:\n“|”. A sequence starts and ends with a pipe as well. Chords\nmust be speciﬁed for each beat in a bar (e.g., four chords\nfor 4/4 meter). A simpliﬁcation of this is possible: if a\nchord occupies the whole bar, it could be typed only once;\nand if chords occupy an equal number of beats in a bar\n(e.g., two beats in 4/4 metre), each chord could be speci-\nﬁed only once, e.g., |F G| instead of |F F G G| .\nFor chord labeling, we use the Harte et al. [16] syntax\nfor standardization reasons, but mainly use the shorthand\nsystem and do not assume the literal interpretation of labels\nFigure 1 . An annotation example.\nas pitch class sets. More details on chord label interpreta-\ntion will follow in 4.1.\n3.2 Content selection\nThe community of listeners, musicians, teachers, critics\nand academic scholars deﬁnes the jazz genre, so we de-\ncided to annotate a selection chosen by experts.\nAfter considering several lists of seminal recordings\ncompiled by authorities in jazz history and in musical edu-\ncation [14, 24], we decided to start with “The Smithsonian\nCollection of Classic Jazz” [1] and “Jazz: The Smithsonian\nAnthology” [2].\nThe “Collection” was compiled by Martin Williams and\nﬁrst issued in 1973. Since then, it has been widely used\nfor jazz history education and numerous musicological\nresearch studies draw examples from it [26]. The “An-\nthology” contains more modern material compared to the\n“Collection.” To obtain unbiased and representative selec-\ntion, its curators used a multi-step polling and negotiation\nprocess involving more than 50 “jazz experts, educators,\nauthors, broadcasters, and performers.” Last but not least,\naudio recordings from these lists can be conveniently ob-\ntained: each of the collections are issued in a CD box.\nWe decided to limit the ﬁrst version of our dataset to\njazz styles developed before free jazz and modal jazz, be-\ncause lead sheets with chord labels cannot be used effec-\ntively to instruct or describe performances in these latter\nstyles. We also decided to postpone annotating composi-\ntions which include elements of modern harmonic struc-\ntures (i.e., modal or quartal harmony).\n3.3 Transcription methodology\nWe use the following semi-automatic routine for beat de-\ntection: the DBNBeatTracker algorithm from the madmom\npackage is run [4]; estimated beats are visualized and soni-\nﬁed with Sonic Visualizer; if needed, DBNBeatTracker is\nre-run with a different set of parameters; and ﬁnally beat\nannotations are manually corrected, which is usually nec-\nessary for ritardando or rubato sections in a performance.\nAfter that, chords are transcribed. The annotator aims\nto notate which chords are played by the rhythm section.486 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 2 . Distribution of recordings from the dataset by\nyear.\nIf the chords played by the rhythm section are not clearly\naudible during a solo, chords played in the “head” are repli-\ncated. Useful guidelines on chord transcription in jazz are\ngiven in the introduction of Henry Martin’s book [26]. The\nannotators used existing resources as a starting point, such\nas published transcriptions of a particular performance or\nReal book chord progressions, but the ﬁnal decisions for\neach recording were made by the annotator. We developed\nan automation tool for checking the annotation syntax and\nchord soniﬁcation: chord sounds are generated with Shep-\nard tones and mixed with the original audio track, taking\nits volume into account. If annotation errors are found dur-\ning syntax check or while listening to the soniﬁcation play-\nback, they are corrected and the loop is repeated.\n4. DATASET SUMMARY AND IMPLICATIONS\nFOR CORPUS BASED RESEARCH\nTo date, 113 tracks are annotated with an overall duration\nof almost 7 hours, or 68570 beats. Annotated recordings\nwere made from music created between 1917 and 1989,\nwith the greatest number coming from the formative years\nof jazz: the 1920s-1960s (see Figure 2). Styles vary from\nblues and ragtime to New Orleans, swing, be-bop and hard\nbop with a few examples of gypsy jazz, bossa nova, Afro-\nCuban jazz, cool, and West Coast. Instrumentation varies\nfrom solo piano to jazz combos and to big bands.\n4.1 Classifying chords in the jazz way\nIn total, 59 distinct chord classes appear in the annotations\n(89, if we count chord inversions). To manage such a di-\nversity of chords, we suggest classifying chords as it done\nin jazz pedagogical and theoretical literature. According to\nthe article by Strunk [32], chord inversions are not impor-\ntant in the analysis of jazz performance, perhaps because\nof the improvisational nature of bass lines. Inversions are\nused in lead sheets mainly to emphasize the composed bass\nline (e.g., pedal point or chromaticism). Therefore, we ig-\nnore inversions in our analysis.\nAccording to numerous instructional books, and to the-\noretical work done by Martin [25], there are only ﬁve main\nFigure 3 . Flow chart: how to identify chord class by de-\ngree set.\nChord Beats Beats Duration Duration\nclass Number % (seconds) %\ndom7 29786 43.44 10557 42.23\nmaj 18591 27.11 6606 26.42\nmin 13172 19.21 4681 18.72\ndim 1677 2.45 583 2.33\nhdim7 1280 1.87 511 2.04\nno chord 3986 5.81 2032 8.13\nunclassi- 78 0.11 30 0.12\nﬁed\nTable 1 . Chord classes distribution.\nchord classes in jazz: major (maj), minor (min), dominant\nseventh (dom7), half-diminished seventh (hdim7), and di-\nminished (dim). Seventh chords are more prevalent than\ntriads, although sixth chords are popular in some styles\n(e.g., gypsy jazz). Third, ﬁfth and seventh degrees are used\nto classify chords in a bit of an asymmetric manner: the un-\naltered ﬁfth could be omitted in the major, minor and dom-\ninant seventh (see chapter on three note voicing in [23]);\nthe diminished ﬁfth is required in half-diminished and in\ndiminished chords; and [[7 is characteristic for diminished\nchords. We summarize this classiﬁcation approach in the\nﬂow chart in Figure 3.\nThe frequencies of different chord classes in our cor-\npus are presented in Table 1. The dominant seventh is the\nmost popular chord, followed by major, minor, diminished\nand half-diminished. Chord popularity ranks differ from\nthose calculated in [6] for the iRb corpus: dom7, min, maj,\nhdim, and dim. This could be explained by the fact that our\ndataset is shifted toward the earlier years of jazz develop-\nment, when major keys were more pervasive.\n4.2 Exploring symbolic data\nExploring the distribution of chord transition bigrams and\nn-grams allows us to ﬁnd regularities in chord progres-\nsions. The term bigram for two-chord transitions was de-Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 487Figure 4 . Top ten chord transition n-grams. Each n-gram is expressed as sequence of chord classes (dom, maj, min, hdim7,\ndim) alternated with intervals (e.g., P4 - perfect fourth, M6 - major sixth), separating adjacent chord roots.\nﬁned in [6]. Similarly, we deﬁne an n-gram as a sequence\nofnchord transitions. The ten most frequent n-grams from\nour dataset are presented in Figure 4. The picture pre-\nsented by the plot is what would be expected for a jazz\ncorpus: we see the prevalence of the root movement by the\ncycle of ﬁfths. The famous IIm-V7-I three-chord pattern\n(e.g., [25]) is ranked number 5, which is even higher than\nmost of the shorter two-chord patterns.\n5. CHORD TRANSCRIPTION ALGORITHMS\nBASELINE EVALUATION\nNow we turn to Automatic Chord Estimation (ACE) eval-\nuation for jazz. We adopt the MIREX16approach to eval-\nuating ACE algorithms. The approach supports multiple\nways to match ground truth chord labels with predicted\nlabels, by employing the different chord vocabularies in-\ntroduced by Pauwels [29]. The distinctions between the\nﬁve chord classes deﬁned in 4.1 are crucial for analyzing\njazz performance. More detailed transcriptions (e.g., a dis-\ntinction between maj6 and maj7, detecting extensions of\ndom7, etc.) are also important but secondary to classiﬁca-\ntion into the basic ﬁve classes. To formally implement this\nconcept of chord classiﬁcation, we develop a new vocab-\nulary, called “Jazz5,” which converts chords into the ﬁve\nclasses according to the ﬂowchart in Figure 3.\nFor comparison, we also choose two existing MIREX\nvocabularies: “Sevenths” and “Tetrads,” because they ig-\nnore inversions and can distinguish between major, minor\nand dom7 classes (which together occupy about 90% of\nour dataset). However, these vocabularies penalize differ-\nences within a single basic class (e.g., between a major\ntriad and a major seventh chord). Moreover, the “Sev-\nenths” vocabulary is too basic; it excludes a signiﬁcant\nnumber of chords, such as diminished chords or sixths,\nfrom evaluation.\nWe choose Chordino17, which has been a baseline al-\ngorithm for the MIREX challenge over several years, and\nCREMA18, which was recently introduced in [28]. To\ndate, CREMA is one of the few open-source, state-of-the-\nart algorithms which supports seventh chords.\nResults are provided in the Table 2. “Coverage” signi-\nﬁes the percentage of the dataset which can be evaluated\nusing the given vocabulary. “Accuracy” stands for the per-\n16http://www.music-ir.org/mirex/wiki/2017:\nAudio_Chord_Estimation\n17http://www.isophonics.net/nnls-chroma\n18https://github.com/bmcfee/cremaV ocabulary Coverage Chordino CREMA\n% Accuracy % Accuracy %\n“Jazz5” 99.88 32.68 40.26\nMirexSevenths 86.12 24.57 37.54\nTetrads 99.90 23.10 34.30\nTable 2 . Comparison of coverage and accuracy evaluation\nfor different chord dictionaries and algorithms.\ncentage of the covered dataset for which chords were prop-\nerly predicted, according to the given vocabulary.\nWe see that the accuracy for the jazz dataset is almost\nhalf of the accuracy achieved by the most advanced algo-\nrithms on datasets currently involved in the MIREX chal-\nlenge19(which is roughly 70-80%). Nevertheless, the\nmore recent algorithm (CREMA) performs signiﬁcantly\nbetter than the old one (Chordino) which shows that our\ndataset passes a sanity check: it does not contradict tech-\nnological progress in Automatic Chord Estimation. We see\nfrom this analysis that the “Sevenths” chords vocabulary is\nnot appropriate for a jazz corpus because it ignores almost\n14% of the data. We also note that the “Tetrads” vocabu-\nlary is too punitive: it penalizes up to 9% of predictions.\nHowever, this could potentially be tolerable in the context\nof jazz harmony analysis. We provide code for this evalu-\nation in the project repository.\n6. CONCLUSIONS AND FURTHER WORK\nWe have introduced a dataset of time-aligned jazz har-\nmony transcriptions, which is useful for MIR research and\ncorpus-based musicology. We have demonstrated how the\nparticularities of the jazz genre affect our approach to data\nselection, annotation, and evaluation of chord estimation\nalgorithms.\nFurther work includes growing the dataset by expand-\ning the set of annotated tracks and adding new features.\nFunctional harmony annotation (or local tonal centers) is\nof particular interest, because we could then implement\nchord detection accuracy evaluation based on jazz chord\nsubstitution rules.\n19http://www.music-ir.org/mirex/wiki/2017:\nAudio_Chord_Estimation_Results488 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20187. ACKNOWLEDGMENT\nThe authors would like to thank all the anonymous review-\ners for their valuable comments, which greatly helped to\nimprove the quality of this paper.\n8. REFERENCES\n[1] The Smithsonian Collection of Classic Jazz. Smithso-\nnian Folkways Recording, 1997.\n[2] Jazz: The Smithsonian Anthology. Smithsonian Folk-\nways Recording, 2010.\n[3] Stefan Balke, Christian Dittmar, Jakob Abeßer, Klaus\nFrieler, Martin Pﬂeiderer, and Meinard Müller. Bridg-\ning the Gap: Enriching YouTube Videos with Jazz Mu-\nsic Annotations. Frontiers in Digital Humanities , 5,\n2018.\n[4] Sebastian Böck, Filip Korzeniowski, Jan Schlüter, Flo-\nrian Krebs, and Gerhard Widmer. madmom: a new\nPython Audio and Music Signal Processing Library.\nInProc. of the 24th ACM International Conference on\nMultimedia , pages 1174–1178, 2016.\n[5] Dmitry Bogdanov, Nicolas Wack, Emilia Gómez,\nSankalp Gulati, Perfecto Herrera, O. Mayor, Gerard\nRoma, Justin Salamon, J. R. Zapata, and Xavier Serra.\nEssentia: an audio analysis library for music informa-\ntion retrieval. In Proc. of the of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 493–498, 2013.\n[6] Yuri Broze and Daniel Shanahan. Diachronic Changes\nin Jazz Harmony: A Cognitive Perspective. Music\nPerception: An Interdisciplinary Journal , 3(1):32–45,\n2013.\n[7] John Ashley Burgoyne, Jonathan Wild, and Ichiro Fu-\njinaga. An Expert Ground-Truth Set for Audio Chord\nRecognition and Music Analysis. In Proc. of the of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , number ISMIR, pages 633–638,\n2011.\n[8] Chris Cannam, Christian Landone, Mark Sandler, and\nJuan Pablo Bello. The Sonic Visualiser: A Visualisa-\ntion Platform for Semantic Descriptors from Musical\nSignals. In Proc. of the of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 324–327, 2006.\n[9] Trevor de Clercq and David Temperley. A corpus anal-\nysis of rock harmony. Popular Music , 30(01):47–70,\njan 2011.\n[10] Ken Déguernel, Emmanuel Vincent, and Gérard As-\nsayag. Using Multidimensional Sequences For Impro-\nvisation In The OMax Paradigm. In 13th Sound and\nMusic Computing Conference , 2016.[11] Junqi Deng and Yu-kwong Kwok. A hybrid gaussian-\nhmm-deep-learning approach for automatic chord esti-\nmation with very large vocabulary. In Proc. 17th Inter-\nnational Society for Music Information Retrieval Con-\nference , pages 812–818, 2016.\n[12] Bruno Di Giorgi, Massimiliano Zanoni, Augusto Sarti,\nand Stefano Tubaro. Automatic chord recognition\nbased on the probabilistic modeling of diatonic modal\nharmony. In Proc. of the 8th International Workshop\non Multidimensional Systems (nDS) , pages 1–6, 2013.\n[13] Hubert Léveillé Gauvin. \" The Times They Were A-\nChangin’ \" : A Database-Driven Approach to the Evo-\nlution of Harmonic Syntax in Popular Music from the\n1960s. Empirical Musicology Review , 10(3):215–238,\napr 2015.\n[14] Ted Gioia. The Jazz Standards: A Guide to the Reper-\ntoire. Oxford University Press, 2012.\n[15] Mark Granroth-Wilding. Harmonic analysis of music\nusing combinatory categorial grammar . PhD thesis,\nUniversity of Edinburgh, 2013.\n[16] C Harte, M Sandler, S Abdallah, and E Gómez. Sym-\nbolic representation of musical chords: A proposed\nsyntax for text annotations. In Proc. of the of the Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR) , volume 56, pages 66–71, 2005.\n[17] Christopher Harte. Towards automatic extraction of\nharmony information from music signals . PhD thesis,\nQueen Mary, University of London, 2010.\n[18] Thomas Hedges, Pierre Roy, and François Pachet. Pre-\ndicting the Composer and Style of Jazz Chord Progres-\nsions. Journal of New Music Research , 43(3):276–290,\njul 2014.\n[19] Eric J Humphrey and Juan P Bello. Four timely insights\non automatic chord estimation. In Proc. of the of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2015.\n[20] Eric J Humphrey, Justin Salamon, Oriol Nieto, Jon\nForsyth, Rachel M Bittner, and Juan P Bello. Jams: a\nJson Annotated Music Speciﬁcation for Reproducible\nMir Research. In Proc. of the International Society for\nMusic Information Retrieval (ISMIR) , 2014.\n[21] Barry Dean Kernfeld. The story of fake books : boot-\nlegging songs to musicians . Scarecrow Press, 2006.\n[22] Hendrik Vincent Koops, Bas de Haas, John Ashley\nBurgoyne, Jeroen Bransen, and Anja V olk. Harmonic\nsubjectivity in popular music. Technical Report UU-\nCS-2017-018, Department of Information and Com-\nputing Sciences, Utrecht University, 2017.\n[23] Mark Levine. The Jazz Piano Book . Sher Music, 1989.\n[24] Mark Levine. The Jazz Theory Book . Sher Music,\n2011.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 489[25] Henry Martin. Jazz Harmony: A Syntactic Back-\nground. Annual Review of Jazz Studies , 8:9–30, 1988.\n[26] Henry Martin. Charlie Parker and Thematic Impro-\nvisation . Institute of Jazz Studies, Rutgers–The State\nUniversity of New Jersey, 1996.\n[27] Matthias Mauch and Simon Dixon. Approximate note\ntranscription for the improved identiﬁcation of difﬁ-\ncult chords. In Proc. of the of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\nnumber 1, pages 135–140, 2010.\n[28] Brian Mcfee and Juan Pablo Bello. Structured Train-\ning for Large-V ocabulary Chord Recognition. In Proc.\nof the International Conference on Music Information\nRetrieval (ISMIR) , pages 188–194, 2017.\n[29] Johan Pauwels and Geoffroy Peeters. Evaluating au-\ntomatically estimated chord sequences. In ICASSP ,\nIEEE International Conference on Acoustics, Speech\nand Signal Processing - Proceedings , pages 749–753.\nIEEE, 2013.\n[30] Martin Pﬂeiderer, Klaus Frieler, and Jakob Abeßer. In-\nside the Jazzomat - New perspectives for jazz research .\nSchott Campus, Mainz, Germany, 2017.\n[31] Keith Salley and Daniel T. Shanahan. Phrase Rhythm\nin Standard Jazz Repertoire: A Taxonomy and Corpus\nStudy. Journal of Jazz Studies , 11(1):1, nov 2016.\n[32] Steven Strunk. Harmony (i). The New Grove Dictio-\nnary of Jazz, pp. 485–496, 1994.490 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Bridging Audio Analysis, Perception and Synthesis with Perceptually-regularized Variational Timbre Spaces.",
        "author": [
            "Philippe Esling",
            "Axel Chemla-Romeu-Santos",
            "Adrien Bitton"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492373",
        "url": "https://doi.org/10.5281/zenodo.1492373",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/219_Paper.pdf",
        "abstract": "Generative models aim to understand the properties of data, through the construction of latent spaces that allow classification and generation. However, as the learning is unsupervised, the latent dimensions are not related to perceptual properties. In parallel, music perception research has aimed to understand timbre based on human dissimilarity ratings. These lead to timbre spaces which exhibit perceptual similarities between sounds. However, they do not generalize to novel examples and do not provide an invertible mapping, preventing audio synthesis. Here, we show that Variational Auto-Encoders (VAE) can bridge these lines of research and alleviate their weaknesses by regularizing the latent spaces to match perceptual distances collected from timbre studies. Hence, we propose three types of regularization and show that they lead to spaces that are simultaneously coherent with signal properties and perceptual similarities. We show that these spaces can be used for efficient audio classification. We study how audio descriptors are organized along the latent dimensions and show that even though descriptors behave in a non-linear way across the space, they still exhibit a locally smooth evolution. We also show that, as this space generalizes to novel samples, it can be used to predict perceptual similarities of novel instruments. Finally, we exhibit the generative capabilities of our spaces, that can directly synthesize sounds with continuous evolution of timbre perception.",
        "zenodo_id": 1492373,
        "dblp_key": "conf/ismir/EslingCB18",
        "keywords": [
            "Generative models",
            "latent spaces",
            "classification",
            "generation",
            "unsupervised learning",
            "latent dimensions",
            "timbre spaces",
            "perceptual properties",
            "audio synthesis",
            "Variational Auto-Encoders (VAE)"
        ],
        "content": "BRIDGING AUDIO ANALYSIS, PERCEPTION AND SYNTHESIS WITH\nPERCEPTUALLY-REGULARIZED VARIATIONAL TIMBRE SPACES\nPhilippe Esling, Axel Chemla–Romeu-Santos, Adrien Bitton\nInstitut de Recherche et Coordination Acoustique-Musique (IRCAM)\nCNRS - UMR 9912, UPMC - Sorbonne Universite\n1 Place Igor Stravinsky, F-75004 Paris, France\nfesling, chemla, bitton g@ircam.fr\nABSTRACT\nGenerative models aim to understand the properties of\ndata, through the construction of latent spaces that allow\nclassiﬁcation and generation. However, as the learning is\nunsupervised, the latent dimensions are not related to per-\nceptual properties. In parallel, music perception research\nhas aimed to understand timbre based on human dissimi-\nlarity ratings. These lead to timbre spaces which exhibit\nperceptual similarities between sounds. However, they\ndo not generalize to novel examples and do not provide\nan invertible mapping, preventing audio synthesis. Here,\nwe show that Variational Auto-Encoders (V AE) can bridge\nthese lines of research and alleviate their weaknesses by\nregularizing the latent spaces to match perceptual distances\ncollected from timbre studies. Hence, we propose three\ntypes of regularization and show that they lead to spaces\nthat are simultaneously coherent with signal properties and\nperceptual similarities. We show that these spaces can be\nused for efﬁcient audio classiﬁcation. We study how audio\ndescriptors are organized along the latent dimensions and\nshow that even though descriptors behave in a non-linear\nway across the space, they still exhibit a locally smooth\nevolution. We also show that, as this space generalizes to\nnovel samples, it can be used to predict perceptual similar-\nities of novel instruments. Finally, we exhibit the genera-\ntive capabilities of our spaces, that can directly synthesize\nsounds with continuous evolution of timbre perception.\n1. INTRODUCTION\nGenerative models aim to understand the underlying dis-\ntribution of data based on the observation of examples,\nin order to generate novel content. Recently, audio syn-\nthesis using these models has seen great improvements\nthrough efﬁcient waveform models, such as WaveNet [19]\nandSampleRNN [17]. These models are able to generate\nhigh-quality audio matching the properties of the corpus\nc\rPhilippe Esling, Axel Chemla, Adrien Bitton. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC\nBY 4.0). Attribution: Philippe Esling, Axel Chemla, Adrien Bitton.\n“Bridging audio analysis, perception and synthesis with perceptually-\nregularized variational timbre spaces”, 19th International Society for Mu-\nsic Information Retrieval Conference, Paris, France, 2018.they have been trained on. However, these models give lit-\ntle control over the output or the hidden features it results\nfrom. More recently, NSynth [4] has been proposed to gen-\nerate instrumental notes, while allowing to morph between\nspeciﬁc instruments. However, these models remain highly\ncomplex, requiring very large number of parameters, long\ntraining times and a large number of examples.\nAmongst recent generative models, a key proposal is\ntheVariational Auto-Encoder (VAE) [11]. In these models,\nencoder and decoder networks are jointly trained through\nthe construction of a latent space , that allow both analysis\nand generation. V AEs address all the limitations of con-\ntrol and analysis through this latent space, while remaining\nsimple and fast to learn without requiring large sets of ex-\namples. Furthermore, the V AE seems able to disentangle\nunderlying variation factors by learning independent latent\nvariables [7]. However, these unsupervised dimensions are\nnot related to perceptual properties, which might hamper\nthe control and use of these spaces for analysis and synthe-\nsis. The potential of V AEs for audio applications has only\nbeen scarcely investigated and mostly for speech source\nseparation [13] and transformation [8]. However, the use\nof variational latent spaces speciﬁcally for musical audio\nsynthesis is yet to be investigated.\nIn parallel, music perception research has tried to under-\nstand the mechanisms behind the perception of instrumen-\ntaltimbre . Several studies [15] collected dissimilarity rat-\nings between pairs of instrumental samples. Then, Multi-\nDimensional Scaling (MDS) is applied to these ratings to\nobtain timbre spaces , which exhibit the perceptual similar-\nities between instruments. Although these spaces provide\ninteresting analyses, they are inherently limited by the fact\nthat MDS produces a ﬁxed discrete space, which has to be\nrecomputed for any new sample. Therefore, these spaces\ndo not generalize to novel examples and do not provide an\ninvertible mapping, preventing audio synthesis.\nHere, we show that we can bridge analysis, synthesis\nand perceptual audio research by regularizing the learning\nof latent spaces so that they match the perceptual distances\nfrom timbre studies. Our overall approach is depicted in\nFigure 1. First, we adapt the V AE to analyze musical au-\ndio content, by relying on the Non-Stationary Gabor Trans-\nform (NSGT) with a Constant-Q scale. This transform al-\nlows us to obtain a log-frequency scale while remaining in-\nvertible, which is critical to perform audio synthesis. Even175with a simple model on a small training set, we show that\nthis provides a generative model with an interesting latent\nspace, able to synthesize novel instrumental sounds.\nThen, we propose three regularizations to the learn-\ning objective, aiming to enforce that the latent space ex-\nhibits the same topology as the topology of timbre spaces.\nWe build a model of perceptual relationships by normal-\nizing dissimilarity ratings from ﬁve timbre space studies\n[5, 9, 12, 14, 16]. We show that perceptually-regularized\nlatent spaces are both coherent with perceptual dissimilar-\nities, while being able to reconstruct audio samples with a\nhigh accuracy. Hence, we can drive the learning of the la-\ntent space to match the topology of any given target space.\nWe demonstrate that these spaces can be used for au-\ndio classiﬁcation by training low-capacity classiﬁers on the\nspaces. We obtain high accuracy for family andinstrument\nlabels, but also for the pitch anddynamics , even though the\nmodel had no information on these during training. We ex-\nhibit the generative capabilities of our spaces, by assessing\nthe reconstruction quality of the model on a test dataset.\nWe show that the latent spaces can be directly used to syn-\nthesize sounds with continuous evolution of timbre percep-\ntion. We also show that these spaces generalize to novel\nsamples, by encoding instruments that were not part of the\ntraining set. Therefore, the spaces could be used to predict\nthe perceptual similarities of novel instruments. Finally,\nwe study how audio descriptors behave along the latent di-\nmensions, by generating audio samples on a grid across\nspace. We show that even though descriptors behave in a\nnon-linear way across the space, they still follow a locally\nsmooth evolution. Our source code, audio examples and\nadditional ﬁgures and animations are available online1.\n2. STATE-OF-ART\n2.1 Variational auto-encoders\nGenerative models are a ﬂourishing class of machine learn-\ning approaches, aiming to ﬁnd the underlying probability\ndistribution of the data p(x)[2]. Formally, based on a\nset of examples in x2Rdx, we assume that these fol-\nlow an unknown probability distribution p(x). Further-\nmore, we consider a set of latent variables deﬁned in a\nlower-dimensional space z2Rdz(dz\u001cdx), a higher-\nlevel representation that could have led to generate a given\nexample. These latent variables help govern the distribu-\ntion of the data and enhance the expressivity of the model.\nThe complete model is deﬁned by the joint probability dis-\ntributionp(x;z) =p(xjz)p(z). We could ﬁnd p(x)by\nmarginalizing zfrom the joint probability. However, for\nmost models, this integral can not be found in closed form.\nRecently, variational inference (VI) has been proposed\nto solve this problem through optimization . VI assumes\nthat if the distribution is too complex to ﬁnd, we could\nﬁnd a simpler approximate distribution that still models the\ndata, while trying to minimize its difference to the real dis-\ntribution. VI speciﬁes a family Qof approximate densities,\n1https://github.com/acids-ircam/ismir2018\nVAE\nPerceptual ratingsx\np✓(x|z)q\u0000(z|x)z\nµ(x)⌃(x)R\u0000,✓\u0000z,T\u0000\nMDST\nAnalysis\nSynthesis\nPerception\nTimbre spaces˜x⇠p(x)\n552662\n584462\n771532662531Figure 1 . The V AE models audio samples xby learning an\nencoderq\u001e(zjx)which maps them to a Gaussian N(\u0016(x),\n\u001b(x))in latent space z. The decoder p\u0012(xjz)samples from\nthis Gaussian to generate a reconstruction ~x. Perception\nstudies use dissimilarity ratings to construct a timbre space\nthat exhibits the perceptual distances between instruments.\nHere, we develop regularizations methods R(z;T), to en-\nforce that the variational model ﬁnds a topology of latent\nspace zthat matches the topology of the timbre space T.\nwhere each member q(zjx)2Q is a candidate approxima-\ntion to the exact conditional p(zjx). Hence, the inference\ncan be transformed into an optimization problem by mini-\nmizing the Kullback-Leibler (KL) divergence between the\napproximation and the original density\nq\u0003(zjx) = arg min\nq(zjx)2QDKL\u0002\nq(zjx)kp(zjx)\u0003\n(1)\nBy developing this KL divergence and re-arranging terms\n(the detailed development can be found in [11]), we obtain\nlogp(x)\u0000DKL\u0002\nq(zjx)kp(zjx)\u0003\n=\nEz\u0002\nlogp(xjz)\u0003\n\u0000DKL\u0002\nq(zjx)kp(z)\u0003\n(2)\nThis formulation describes the quantity we want to max-\nimize logp(x)minus the error we make by using an ap-\nproximateqinstead ofp. Therefore, we can optimize\nthis alternative objective, called the evidence lower bound\n(ELBO). Now, to optimize this objective, we will rely on\nparametric distributions q\u001e(z)andp\u0012(z). Optimizing our\ngenerative model will amount to optimizing these parame-\nters\b\n\u0012;\u001e\t\nof these distributions with\nL\u0012;\u001e=E\u0002\nlogp\u0012(xjz)\u0003\n\u0000\f\u0001DKL\u0002\nq\u001e(zjx)kp\u0012(z)\u0003\n(3)\nWe can see that this equation involves q\u001e(zjx)which en-\ncodes the data xinto the latent representation zand a de-\ncoderp(xjz), which allows generating a data xgiven a la-\ntent conﬁguration z. Hence, this structure deﬁnes the Vari-\national Auto-Encoder (V AE), depicted in Figure 1 (Left).176 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018The V AE objective can be interpreted intuitively. The\nﬁrst term increases the likelihood of the data generated\ngiven a conﬁguration of the latent, which amounts to mini-\nmize the reconstruction error . The second term represents\nthe error made by using a simpler distribution q\u001e(zjx)\nrather than the true distribution p\u0012(z). Therefore, this reg-\nularizes the choice of approximation qso that it remains\nclose to the true posterior distribution [11]. Here, we also\nintroduced a weight \fon the KL divergence, which has\nbeen shown to improve the capacity of the model to disen-\ntangle factors of variations in the data [7].\nV AEs are powerful representation learning frameworks,\nwhile remaining simple and fast to learn without requiring\nlarge sets of examples [18]. Their potential for audio appli-\ncations have been only scarcely investigated yet and mostly\nin topics related to speech processing such as blind source\nseparation [13] and speech transformation [8]. However,\nto our best knowledge, the use of V AE to perform musical\naudio analysis and generation has yet to be investigated.\n2.2 Timbre spaces and auditory perception\nFor decades, researchers have tried to understand the\nmechanisms of timbre perception. Timbre is the set of\nproperties that distinguishes two instruments playing the\nsame note at the same intensity. Several studies tried to un-\nderstand this phenomenon by relying on timbre spaces [6],\na model that aims to organize audio samples based on hu-\nman dissimilarity ratings. The experimental protocol con-\nsists of presenting pairs of sounds to subjects. Each subject\nhas to rate the perceptual dissimilarity of all pairs of sam-\nples inside a selected set of instruments. Then, these rat-\nings are compiled into a set of dissimilarity matrices that\nare analyzed with Multi-Dimensional Scaling (MDS). The\nMDS algorithm provides a timbre space that exhibits the\nperceptual distances between different instruments. This\nprocess is depicted in Figure 1 (Right). Here, we brieﬂy\ndetail the studies and redirect the interested readers to the\nfull articles for more details.\nIn his seminal paper, Grey [5] performed a study with\n16 instrumental sound samples in which 22 subjects had to\nrate their dissimilarities on a continuous scale from 0 (most\nsimilar) to 1 (most dissimilar), leading to the ﬁrst construc-\ntion of a timbre space. Following this study, Krumhansl\n[12] used 21 instruments with 9 subjects on a discrete scale\nfrom 1 to 9, Iverson et al. [9] with 16 samples and 10\nsubjects on a continuous scale from 0 to 1, McAdams et\nal. [16] with 18 instruments and 24 subjects on a discrete\nscale from 1 to 16 and, ﬁnally, Lakatos [14] with 17 sub-\njects and different instrument sets on a continuous scale\nfrom 0 to 1. Each of these studies shed light on different\naspects of audio perception, depending on the interpreta-\ntion of the dimensions. However, all studies produced dif-\nferent spaces with different dimensions, preventing a gen-\neralization on the acoustic cues that might correspond to\ntimbre dimensions. Furthermore, these studies are inher-\nently limited by the fact that ordination techniques (e.g.\nMDS) produce ﬁxed spaces that must be recomputed for\nany new data point [16]. Hence, these spaces are unableto generalize nor can we generate data from these as they\ndo not provide an invertible mapping. Here, we show that\nlearning latent spaces, while regularizing their topology to\nﬁt perceptual ratings can alleviate these limitations.\n3. REGULARIZING THE TOPOLOGY OF\nLATENT SPACES\nWe show that we can inﬂuence the learning of the latent\nspace zso that it follows the topology of a given target\nspaceT. Here, we rely on timbre spaces based on percep-\ntual ratings as a target space. However, it should be noted\nthat this idea can be applied to any target space. Here, we\nconsider a set of audio samples xiwhere each have rela-\ntions in both latent space ziand target spaceTi. In order to\nrelate the elements of the audio set to the perceptual space,\nwe consider that each sample is labeled with its instrumen-\ntal classCi, that has an equivalent in the timbre space.\n3.1 Penalty regularization\nFirst, we deﬁne an additive penalty regularizationR(z;T)\nthat imposes that the properties of the latent zare similar\nto that of the target T. Our objective becomes\nE\u0002\nlogp\u0012(xjz)\u0003\n\u0000\fDKL\u0002\nq\u001e(zjx)kp\u0012(z)\u0003\n+\u000bR\u0000\nz;T\u0001\nHence, amongst two otherwise equal solutions, the model\nis pushed to select the one that comply with the penalty.\nThe weight\u000ballows us to control the inﬂuence of this reg-\nularization. In our case, we want the distances between\ninstruments to follow the perceptual distances. There-\nfore, we need to minimize the differences between the dis-\ntances in latent space Dz\ni;j=D(zi;zj)and in target space\nDT\ni;j=D(Ti;Tj). The regularization criterion will mini-\nmize the differences between these sets of distances\nR\u0000\nz;T\u0001\n=X\ni6=jRi;j\u0000\nz;T\u0001\n=X\ni6=jR\u0000\nDz\ni;j;DT\ni;j\u0001\n(4)\nEuclidean . First, we rely on the Euclidean distance to\ncompute the distance between points in both spaces with\nDS\ni;j=kSi\u0000Sjk2and also to compare distance matrices\nRi;j\u0000\nz;T\u0001\n=\r\rDz\ni;j\u0000DT\ni;j\r\r2(5)\nThis regularization provides an incentive to the model to\nobtain the Euclidean metric properties of the target space.\nGaussian . Here, we model the fact that perceptual rat-\nings are subjective assessments. Therefore, we consider\nthat each perceptual rating between instruments iandjis\ndrawn from a univariate Gaussian di;j\u0018 N\u0000\n\u0016i;j;\u001bi;j\u0001\n.\nAs we can see, we deﬁne a different distribution for each\npair of instruments. When evaluating the regularization,\nwe draw a different distance at each iteration for all pairs\n\b\nDT\ni;j\t\nit\u0018N\u0000\n\u0016i;j;\u001bi;j\u0001\nHence, this regularization models the uncertainty present\nin the set of perceptual ratings.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 1773.2 Prior regularization\nIn the V AE objective, we observe that the prior p(z)al-\nready carries information on the organization of the latent\nspace. Therefore, we can inject the desired topology of the\nlatent space inside that term. Here, we propose to intro-\nduce a class-based prior\np(zi) =N(\u0016T(Ci);\u001bT(Ci))\nwhereCiis the class of element i. Therefore, this prior\npushes the V AE to ﬁnd a conﬁguration of the samples in\nlatent space so that they follow the same distribution as\ntheir class in our target timbre space. The computation of\nclass means \u0016T(Ci)and covariances \u001bT(Ci)based on the\nperceptual ratings is detailed in Section 4.1.\n4. EXPERIMENTS\n4.1 Datasets\nTimbre studies. We rely on perceptual dissimilarity rat-\nings collected across ﬁve independent timbre studies [5, 9,\n12, 14, 16], detailed globally in [3, 15]. As discussed ear-\nlier (Section 2.2), even though all studies follow the same\nprotocol, there are some discrepancies in the instruments,\nnumber of participants and rating scales.\nHence, we normalize the dissimilarity ratings so that all\nstudies map to a common scale from 0 to 1. Then, we\ncompute the maximal set of instruments for which we had\npairwise ratings for all pairs by counting co-occurences\nin studies. This leads to a set of 11 instruments (Piano,\nCello, Violin, Flute, Clarinet, Trombone, Horn, Oboe, Sax-\nophone, Trumpet, Tuba). Finally, we extract the set of rat-\nings that corresponds to our selected instruments, amount-\ning to a total of 11845 pairwise ratings. Based on this set of\nratings, we compute an MDS space to obtain the positions\nin target space of each instrument (which also corresponds\nto the mean \u0016T) and to ensure the consistency of our nor-\nmalized perceptual space. For all pairs of instruments, we\nalso ﬁt a Gaussian distribution to the pairwise dissimilar-\nity ratings in order to obtain the mean \u0016i;jand variance\n\u001bi;jof that pair for the Gaussian regularization. We derive\nthe global variance \u001bTfor each instrument, by taking the\nmean of their pairwise variances. Results of this analysis\nare displayed in Figure 2. Even though ratings come from\ndifferent studies, the resulting space appears very coherent\nwith clusters of families and the distances between individ-\nual instruments correlated to previous perceptual studies.\nAudio datasets. In order to learn the distribution of in-\nstrumental audio, we rely on the Studio On Line (SOL)\ndatabase [1]. We selected 2,200 samples to represent the\n11 instruments for which we extracted perceptual ratings.\nThese represent the whole tessitura and dynamics avail-\nable (to remove effects from the pitch and loudness). All\nrecordings were resampled to a sampling rate of 22050Hz.\nFor each audio sample, we compute the Non-Stationary\nGabor Transform (NSGT) mapped on a Constant-Q scale\nof 24 bins per octave. We only keep the magnitude of the\nNSGT to train our models. Then, we perform a corpus-\nwide normalization to preserve the relative intensities of\nPn\nVi\nVcTbn Tp\nSxEh\nFlFh\nClObBn\nXYZ\n-1\n-1-11\n1\n1\nFigure 2 . Multi-dimensional scaling (MDS) applied to the\ncombined normalized set of perceptual dissimilarity rat-\nings (strings in blue, brasses in green and winds in red).\nthe samples and extract a single temporal frame to repre-\nsent the given audio sample. Finally, the dataset is ran-\ndomly split between a training (90%) and test (10%) set.\n4.2 Models\nIn order to evaluate our proposal, we rely on a very sim-\nple V AE architecture to show its efﬁciency. The encoder\nis deﬁned as a 3-layers feed-forward network with ReLU\nactivations and 3000 units per layer. The last layer maps\nto a latent space of 64 dimensions. The decoder is deﬁned\nwith the same architecture, mapping back to the dimen-\nsionality of the input. For learning the model, we use a\nvalue of\f, which is linearly increased from 0 to 2 during\nthe ﬁrst 100 epochs ( warmup procedure [18]). In order to\ntrain the model, we rely on the ADAM optimizer [10] with\nan initial learning rate of 0.00001, and a Xavier weight ini-\ntialization [18]. In a ﬁrst stage, we train the model without\nperceptual regularization ( \u000b= 0) for 5000 epochs. Then,\nwe introduce the perceptual regularization ( \u000b= 1) and\ntrain for another 1000 epochs. This allows the model to\nﬁrst focus on the quality of the reconstruction with its own\nunsupervised regularization, and then to converge towards\na solution with perceptual space properties. This leads to a\ntraining time of one hour on a NVIDIA Titan X GPU.\n5. RESULTS\n5.1 Latent spaces properties\nIn order to visualize the latent spaces, we apply a Principal\nComponent Analysis (PCA) to obtain a 3d representation.\nUsing a PCA ensures that the representation is a linear\ntransform that preserves the distances inside the original\nspace. This also provides an exploitable control space for\naudio synthesis. Results are displayed in Figure 3.\nAs we can see, the V AE without regularization is al-\nready able to dissociate instrumental distributions, while\nproviding almost perfect reconstruction of audio samples\nfrom the low-dimensional space. This conﬁrms that V AEs\ncan provide interesting latent spaces for analysis and syn-\nthesis. However, the relationships between instruments are178 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018(a) (b) (c) (d)\nL \n16.455L \n0.8345L \n0.2161L \n0.4892\n2 2 2 2EH\nFH\nTbn\nTp\nPi\nVi\nVc\nSx\nBn\nCl\nFl\nOb\nTest set NSGT(reconstruction)Violin PianoTrumpet OboeFigure 3 . Comparing the latent spaces for the V AE unregularized (a) or with Prior (b), Euclidean (c) and Gaussian (d)\nregularization. The mean L2differences between latent and timbre spaces distances is indicated on each graph. We show\nunder each space the reconstruction of NSGT distributions from the test set directly from these spaces.\nentirely different from perceptual ratings. Furthermore, the\nlarge variance of the distributions seem to indicate that the\nmodel rather tries to spread the information across the la-\ntent space to help the reconstruction.\nIn the case of all regularizations (b-d), we can clearly\nsee the enhancement on the dissociation of instrumental\ndistributions. Furthermore, the overall distances between\ninstruments match well the distances based on perceptual\nratings (Figure 2). This similarity is particularly striking\nfor theL2regularization (c), which provides the lowest\noverall differences to our combined timbre space. This\nmight come from the fact that MDS spaces have an Eu-\nclidean metric topology. However, this might also indi-\ncate an effect of over-regularization , which might impact\ngeneralization. For all regularized latent spaces, the instru-\nmental distributions are shufﬂed around the space in order\nto comply with the reconstruction objective. However, the\npairwise distances reﬂecting perceptual relations are well\nmatched as indicated by their respective L2differences to\nthe timbre space. Finally, by looking at the reconstructions\nof the NSGT distributions from the test set, we can see that\nenforcing the perceptual topology to the latent spaces does\nnot impact the quality of audio reconstruction (this evalu-\nation is quantiﬁed in Section 5.3). However, we note an\noccasional addition of low-amplitude noise, which might\nindicate that the model focuses on optimizing the partials\nrather than the low-amplitude tail of the distribution.\n5.2 Discriminative capabilities\nWe evaluate the discriminative capabilities of the latent\nspaces through a classiﬁcation task. We use a very low-\ncapacity classiﬁer composed of a single-layer network of\n512 ReLU units with batch normalization and softmax re-\ngression. The low-capacity classiﬁer ensures that the latent\nspace needs to be well organized to obtain a good accu-\nracy. In order to evaluate the impact of our proposal, we\nalso compare these results to a simple PCA with softmax\nregression and an Auto-Encoder (AE) with the same ca-\npacity as the V AE. Results are presented in Table 1.\nWe can see that all models perform an excellent classiﬁ-Method Family Instrument Pitch Dynam.\nPCA 0.790 0.697 0.167 0.527\nAE 0.973 0.957 0.936 0.597\nV AE 0.978 0.993 0.963 0.941\nPrior 0.975 0.991 0.993 0.936\nEuclidean 0.972 0.990 0.990 0.943\nGaussian 0.982 0.991 0.989 0.948\nTable 1 . Discriminative capabilities in classifying family ,\ninstrument ,pitch anddynamics of the test set.\nMethod logp(x)kx\u0000~xk2\nPCA - 2.2570\nAE -1.2008 1.6223\nV AE -2.3443 0.1593\nPrior -2.7143 0.1883\nEuclidean 17.8960 0.1223\nGaussian 0.2894 0.1749\nTable 2 . Generative capabilities evaluated on the log like-\nlihood and reconstruction error over the test set.\ncation of instrumental properties. However, a very interest-\ning observation comes from the vanilla V AE providing the\nbest accuracy on instrument classiﬁcation, even though we\nregularized other models with distances highly relevant to\nthese categories. This might underline the fact that percep-\ntual information could blur discrimination of highly simi-\nlar instruments (such as violin and violoncello). Interest-\ningly, the symmetric results on pitch anddynamics cate-\ngories might indicate that regularized model are pushed to\nfocus on timbre properties. Therefore, they need to more\nclearly separate the variations coming from pitch and loud-\nness to understand the variability of timbre.\n5.3 Generative capabilities\nWe quantify generative capabilities by evaluating recon-\nstructions from the latent space, through the log likelihood\nand mean difference between original and reconstructed\naudio on the test set. The results are presented in Table 2.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 179Centroid\n Flatness Bandwidtha\nb\nc\nd\nEH\nFHTbn TpPi\nVi\nVcSx\nBnClFlOb\nPicc\nHpVaGtr\nCb\nabcd -11-11Prior-regularized\nlatent spaceFigure 4 . (Top) We encode instruments that were not part\nof timbre studies to show the out-of-domain capabilities of\nlatent spaces. (Bottom) Topology of descriptors. We deﬁne\n4 projection planes equally spaced across the xaxis. We\nsample points at these positions on a 50x50 grid and re-\nconstruct their audio distribution to compute their spectral\ncentroid ,ﬂatness andbandwidth .\nOverall, the regularizations do not impact the recon-\nstruction quality of the model. Furthermore, we can now\nsample directly from the spaces to obtain novel sounds that\nremain perceptually relevant, which allows us to turn our\nspaces into generative timbre synthesizers. However, as\npreviously hypothesized, the L2regularization seems to\nhave a too strong effect on the latent space, disrupting the\ngeneralization of the model. We provide generated audio\nclips representing paths between different instruments in\nthe latent space on the supporting repository for subjective\nevaluation of the latent space audio synthesis.\n5.4 Perceptual inference\nThe encoder of our perceptually-regularized spaces is able\nto analyze new instruments that were not part of the origi-\nnal timbre studies. Hence, we could hope that it is able to\npredict perceptual relationships between new instruments,\nto feed further timbre studies. To evaluate this, we ex-\ntracted instruments outside of our perceptual set (Contra-\nbass, Guitar, Harp, Piccolo, Viola) and encode these sam-\nples in the latent space to study the out-of-domain general-\nization capabilities of our model. Results are presented inFigure 4 (Top, only the centroid of distributions are shown\nfor clarity). Here, the Piccolo and Viola seem to group\nin a coherent way with their families. However, the Gui-\ntar and Harp do not provide such straightforward relation-\nships. Therefore, perceptual inference from these spaces\nwould require more extensive perception experiments.\n5.5 The topology of audio descriptors\nWe analyze the behavior of signal descriptors across the\nlatent space in order to study their topology. As the space\nis continuous, we do so by sampling uniformly the PCA\nspace and then using the decoder to generate all audio sam-\nples on this grid. Then, we compute the audio descriptors\nof these samples. In order to provide a visualization here,\nwe select equally-distant planes across the xdimension (at\npositionsf-.75, -.25, .25, .75 g) in Figure 4 for the spec-\ntralﬂatness ,centroid andbandwidth . Videos of continu-\nous traversals of the latent space for different descriptors\nare available on the supporting repository.\nAudio descriptors seem to be organized in a non-linear\nway across our spaces. However, they still exhibit both\nlocally smooth evolution and an overall logical organiza-\ntion. This shows that our model is able to organize audio\nvariations. A very interesting observation comes from the\ntopology of the centroid. Indeed, all perceptual studies un-\nderline its linear correlation to timbre perception, which is\npartly conﬁrmed by our model (see Figure 4). This con-\nﬁrms the perceptual relevance of these latent spaces. How-\never, this also shows that the relation between centroid and\ntimbre perception might not be entirely linear.\n6. CONCLUSION\nWe have shown that V AEs can learn a latent space al-\nlowing for high-level audio analysis and synthesis directly\nfrom these spaces. We proposed different methods for reg-\nularizing these spaces to follow the metric properties of\ntimbre spaces. These regularized models provide a con-\ntrol space from which the generation of perceptually rel-\nevant audio content is straightforward. By analyzing the\nbehavior of audio descriptors across the latent space, we\nhave shown that, while following a non-linear evolution,\nthey still exhibit some locally smooth properties. Future\nworks on these spaces include perceptual experiments to\nconﬁrm their perceptual topology and also to thrive on the\nsmoothness of audio descriptors to develop a descriptor-\nbased synthesizer.\n7. ACKNOWLEDGEMENTS\nThis work was supported by the MAKIMOno project 17-\nCE38-0015-01 funded by the French ANR and the Cana-\ndian Natural Sciences and Engineering Reserch Council\n(STPG 507004-17) and the ACTOR Partnership funded\nby the Canadian Social Sciences and Humanities Research\nCouncil (895-2018-1023).180 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20188. REFERENCES\n[1] Guillaume Ballet, Riccardo Borghesi, Peter Hoffmann,\nand Fabien L ´evy. Studio online 3.0: An internet”\nkiller application” for remote access to ircam sounds\nand processing tools. Journ ´ee dInformatique Musicale\n(JIM) , 1999.\n[2] Christopher M. Bishop and Tom M. Mitchell. Pattern\nrecognition and machine learning. 2014.\n[3] John A. Burgoyne and Stephen McAdams. A meta-\nanalysis of timbre perception using nonlinear exten-\nsions to clascal. In International Symposium on Com-\nputer Music Modeling and Retrieval , pages 181–202.\nSpringer, 2007.\n[4] Jesse Engel, Cinjon Resnick, Adam Roberts, Sander\nDieleman, Douglas Eck, Karen Simonyan, and Mo-\nhammad Norouzi. Neural audio synthesis of musi-\ncal notes with wavenet autoencoders. arXiv preprint\narXiv:1704.01279 , 2017.\n[5] John M Grey. Multidimensional perceptual scaling of\nmusical timbres. the Journal of the Acoustical Society\nof America , 61(5):1270–1277, 1977.\n[6] John M. Grey and John W. Gordon. Perceptual effects\nof spectral modiﬁcations on musical timbres. The Jour-\nnal of the Acoustical Society of America , 63(5):1493–\n1500, 1978.\n[7] Irina Higgins, Loic Matthey, Arka Pal, Christopher\nBurgess, Xavier Glorot, Matthew Botvinick, Shakir\nMohamed, and Alexander Lerchner. beta-vae: Learn-\ning basic visual concepts with a constrained variational\nframework. 2016.\n[8] Wei-Ning Hsu, Yu Zhang, and James Glass. Learning\nlatent representations for speech generation and trans-\nformation. arXiv preprint arXiv:1704.04222 , 2017.\n[9] Paul Iverson and Carol L. Krumhansl. Isolating the dy-\nnamic attributes of musical timbrea. The Journal of\nthe Acoustical Society of America , 94(5):2595–2603,\n1993.\n[10] Diederik P. Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[11] Diederik P. Kingma and Max Welling. Auto-encoding\nvariational bayes. arXiv preprint arXiv:1312.6114 ,\n2013.\n[12] Carol L. Krumhansl. Why is musical timbre so hard to\nunderstand. Structure and perception of electroacous-\ntic sound and music , 9:43–53, 1989.\n[13] Jen-Tzung Kuo and Kuan-Ting Chien. Variational re-\ncurrent neural networks for speech separation. INTER-\nSPEECH 2017 .[14] Stephen Lakatos. A common perceptual space for\nharmonic and percussive timbres. Perception & psy-\nchophysics , 62(7):1426–1439, 2000.\n[15] Stephen McAdams, Bruno L. Giordano, Patrick Susini,\nGeoffroy Peeters, and Vincent Rioux. A meta-analysis\nof acoustic correlates of timbre dimensions. Journal of\nthe Acoustical Society of America , 120(5):3275, 2006.\n[16] Stephen McAdams, Suzanne Winsberg, Sophie Don-\nnadieu, Geert De Soete, and Jochen Krimphoff.\nPerceptual scaling of synthesized musical timbres:\nCommon dimensions, speciﬁcities, and latent subject\nclasses. Psychological research , 58(3):177–192, 1995.\n[17] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani,\nRithesh Kumar, Shubham Jain, Jose Sotelo, Aaron\nCourville, and Yoshua Bengio. Samplernn: An un-\nconditional end-to-end neural audio generation model.\narXiv preprint arXiv:1612.07837 , 2016.\n[18] Casper K. Sønderby, Tapani Raiko, Lars Maaløe,\nSøren K. Sønderby, and Ole Winther. How to train deep\nvariational autoencoders and probabilistic ladder net-\nworks. arXiv preprint arXiv:1602.02282 , 2016.\n[19] Aaron Van Den Oord, Sander Dieleman, Heiga\nZen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray\nKavukcuoglu. Wavenet: A generative model for raw\naudio. arXiv preprint arXiv:1609.03499 , 2016.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 181"
    },
    {
        "title": "Generalized Skipgrams for Pattern Discovery in Polyphonic Streams.",
        "author": [
            "Christoph Finkensiep",
            "Markus Neuwirth",
            "Martin Rohrmeier"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492473",
        "url": "https://doi.org/10.5281/zenodo.1492473",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/202_Paper.pdf",
        "abstract": "The discovery of patterns using a minimal set of assumptions constitutes a central challenge in the modeling of polyphonic music and complex streams in general. Skipgrams have been found to be a powerful model for capturing semi-local dependencies in sequences of entities when dependencies may not be directly adjacent (see, for instance, the problems of modeling sequences of words or letters in computational linguistics). Since common skipgrams define locality based on indices, they can only be applied to a single stream of non-overlapping entities. This paper proposes a generalized skipgram model that allows arbitrary cost functions (defining locality), efficient filtering, recursive application (skipgrams over skipgrams), and memory efficient streaming. Further, a sampling mechanism is proposed that flexibly controls runtime and output size. These generalizations and optimizations make it possible to employ skipgrams for the discovery of repeated patterns of close, nonsimultaneous events or notes. The extensions to the skipgram model provided here do not only apply to musical notes but to any list of entities that is monotonic with respect to a given cost function.",
        "zenodo_id": 1492473,
        "dblp_key": "conf/ismir/FinkensiepNR18",
        "keywords": [
            "polyphonic music",
            "complex streams",
            "minimal set of assumptions",
            "modeling semi-local dependencies",
            "computational linguistics",
            "skipgrams",
            "arbitrary cost functions",
            "efficient filtering",
            "recursive application",
            "memory efficient streaming"
        ],
        "content": "GENERALIZED SKIPGRAMS FOR PATTERN DISCOVERY IN\nPOLYPHONIC STREAMS\nChristoph Finkensiep Markus Neuwirth Martin Rohrmeier\n´Ecole Polytechnique F ´ed´erale de Lausanne\nfchristoph.finkensiep,markus.neuwirth,martin.rohrmeier g@epfl.ch\nABSTRACT\nThe discovery of patterns using a minimal set of assump-\ntions constitutes a central challenge in the modeling of\npolyphonic music and complex streams in general. Skip-\ngrams have been found to be a powerful model for captur-\ning semi-local dependencies in sequences of entities when\ndependencies may not be directly adjacent (see, for in-\nstance, the problems of modeling sequences of words or\nletters in computational linguistics). Since common skip-\ngrams deﬁne locality based on indices, they can only be\napplied to a single stream of non-overlapping entities. This\npaper proposes a generalized skipgram model that allows\narbitrary cost functions (deﬁning locality), efﬁcient ﬁlter-\ning, recursive application (skipgrams over skipgrams), and\nmemory efﬁcient streaming. Further, a sampling mecha-\nnism is proposed that ﬂexibly controls runtime and out-\nput size. These generalizations and optimizations make\nit possible to employ skipgrams for the discovery of re-\npeated patterns of close, nonsimultaneous events or notes.\nThe extensions to the skipgram model provided here do not\nonly apply to musical notes but to any list of entities that is\nmonotonic with respect to a given cost function.\n1. INTRODUCTION\nDiscovering relevant patterns in a given corpus of musical\npieces is a central problem for music modeling and music\ninformation retrieval (MIR) and is crucial for a range of\napplications from search to stylistic modeling. While there\nexist many approaches for modeling monophonic melodies\n[8, 7], polyphony constitutes a persistent challenge due\nto the vast amount of latent structural patterns that occur\non multiple levels. These patterns involve surface orna-\nments and accompaniment ﬁgurations, contrapuntal con-\nﬁgurations, latent polyphony comprising multiple inter-\nleaved voices, and harmonic and voice-leading schemata.\nWhile many of the underlying patterns are themselves\nrelatively simple, identifying these patterns is challenging,\nbecause it involves distinguishing the relevant notes while\nc\rChristoph Finkensiep, Markus Neuwirth, Martin\nRohrmeier. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: Christoph Finkensiep,\nMarkus Neuwirth, Martin Rohrmeier. “Generalized Skipgrams for\nPattern Discovery in Polyphonic Streams”, 19th International Society for\nMusic Information Retrieval Conference, Paris, France, 2018.ignoring others. In addition, many patterns do not spec-\nify notes exactly but leave some ﬂexibility when being in-\nstantiated, especially concerning timing. Finally, multiple\npatterns may co-occur simultaneously or in an interleaved\nmanner.\nWhen modeling the latent structure of polyphony, it is\nimportant to ﬁnd the characteristic properties of the struc-\nture to be modeled. Therefore, it is advantageous to start\nfrom a model with minimal assumptions about the target\nstructure and add assumptions to the basic model until the\ndesired patterns are found. This way, the properties of\nthe modeled structure are always clear and well-separated\nfrom the assumptions inherent in the underlying represen-\ntation.\nThere are a variety of methods for modeling sequential\ndata with minimal assumptions, such as those developed in\ncomputer linguistics, that treat the data as a single stream\nof events. However, these cannot be straightforwardly ap-\nplied to polyphonic data without adding further implicit as-\nsumptions or removing information contained in the orig-\ninal data. Therefore, a generalization of skipgrams [4] is\ndeveloped in this paper that is applicable to a stream of\npolyphonic notes that need not be explicitly presented as\nseparate voices. This model is applied to a musical corpus\nfor the discovery of polyphonic patterns.\nIn the remainder of this paper, we ﬁrst discuss related\nwork in more detail (Section 2); we then describe our ap-\nproach for generalized skipgrams (Section 3); ﬁnally, we\ndescribe and discuss our empirical evaluations (Sections 4\nand 5).\n2. RELATED WORK\nAmong polyphonic structures, voice-leading schemata are\nparticularly prominent in recent research [3]. Schemata\ncan be understood as structural building blocks that can\nbe elaborated in multiple ways. They are deﬁned as ﬁxed\npatterns of two to four voices where the soprano and bass\nconstitute speciﬁc patterns relative to the key of the piece\n(or a segment within that piece) and may be supplemented\nwith one or two middle voices. The core challenge from\nan MIR perspective is that the structural elements in each\nstage of the sequence need not occur simultaneously, ow-\ning to highly ﬂexible note elaborations. Thus, instances of\nschemata in the music are “semi-local”. An example of this\nproblem can be seen in Figure 1. The underlying schema\nconsists of four stages and is elaborated by neighbor and547^5 ^4 ^4 ^3\n#^1 ^2 ^7 ^1\n(a) The “Fonte” schema as characterized by a typical outer-voice\nmovement in scale degrees\n(b) A realization of the Fonte in a piece\nFigure 1 : An example of a voice-leading schema\npassing notes. The ﬁrst stage consists of non-overlapping\nnotes, so there is no point in time where both notes sound\ntogether. The same applies for the third stage.\nThis semi-locality property of schema patterns can be\nmet by formalizing an extension of the skipgram formal-\nism, which has been successfully applied in linguistics to\nsequences of words or letters (for a review see [4]). Skip-\ngrams in the original version can only be applied to “mono-\nphonic” streams like text, melodies or slices of polyphonic\nmusic, as has been done in [10]. The generalized version\nof skipgrams as proposed in the present paper allows not\nonly the application to truly polyphonic streams but also\nrecursive application, which can be used to build nested\nstructures like schema patterns.\nPreviously, polyphonic music was mainly modeled us-\ning slicing techniques, i.e., cutting the piece vertically at\neach note onset or offset. In [10], common, index-based\nskipgrams as well as an onset-time-based variant are ap-\nplied to slices reduced to a “voice-leading type” represen-\ntation, similar to the representation used here (for more de-\ntails see Section 4.2). The approach presented in this pa-\nper takes the idea two steps further by generalizing the cost\nfunction (allowing non-slice representations as input) and\nby building even the vertical structure with this generalized\nskipgram method, in addition to the horizontal structure.\nMultiple viewpoint systems (e.g., [1, 2, 12, 11]) take a\nsequence of slices and derive sequence features, or “view-\npoints”, from it. Polyphonic structure is modeled by in-\ncluding information about the continuation of notes across\nslices. For prediction, n-grams of all lengths are combined\nby comparing all sufﬁxes of a given gram to other grams\nof the corresponding length. However, slicing techniques\nare generally problematic when grouping non-overlapping\nnotes, as these are not contained in any single slice.\nAn alternative to slices is suggested in [6] where poly-\nphonic music is encoded as a set of data points in a mul-\ntidimensional space. Accordingly, patterns are orthogonal\nprojections (i.e., considering only some features, not all) of\nsubsets of the data points that can be translated to a differ-\nent position (in both pitch and time). This translation oper-\n0\n1\n2\n3\n60\n65\n70\n75\nskip within group 1\nskip between groups\n2 and 3\nTime [s]Pitch [semitones]Figure 2 : An example of applying skipgrams to poly-\nphonic music displayed in a piano-roll visualization. The\nhighlighted notes are members of the skipgram, the stages\nare indicated by solid lines between notes belonging to\nthe same stage. The skip cost within and between stages\nis given by inter-onset intervals. This 2\u00024skipgram\nrepresents the same pattern as the polyphonic schema in\nFigure 1.\nation, however, permits only exact matches in the selected\ndimensions and cannot account for temporally varied pat-\nterns.\n3. GRAM-BASED METHODS AND\nGENERALIZED SKIPGRAMS\nGram-based methods extract short sequences of entities\nfrom a longer stream of entities (e.g., words, letters, or\nnotes). The most basic gram model, the n-gram, is just\na consecutive subsequence in the input stream that has n\nelements. Skipgrams extend the n-gram idea by allowing\nnon-adjacent subsequences that “skip” up to kelements\n[4].\nBoth n-grams and skipgrams assume that the distance\nbetween entities is determined by their position in the\nstream . While this assumption might be reasonable for\ntext, it is problematic for other applications that involve\ngeneral temporal streams, in particular streams of musical\nevents such as notes. Therefore, it is desirable to measure\nthe distance between events (notes) based on their timing\ninformation, i.e., onset, offset, and duration. Second, while\nnotes might be simultaneous in a score, they occur sequen-\ntially in a stream or list-of-notes representation, which be-\ncomes problematic if distance is measured by index.\nSears et al. [10] avoid the latter problem by operating\non a slice representation of a piece, in which slices have a\nunique onset and do not overlap. They partially solve the\nﬁrst problem by replacing the maximal number of skips\nwith a maximal inter-onset-interval, i.e., a time-based dis-\ntance measure. Our paper presents a generalization of skip-\ngrams to arbitrary pairwise cost functions for streams that\nare monotonic with respect to the cost function, which al-\nlows both efﬁcient implementation and streams of overlap-\nping entities.\nConsider Figure 2, a piano-roll representation of a poly-548 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018phonic piece of music. The notes that make up a single\nstage of a voice-leading schema might not be simultane-\nous, but should be close together. Given the notes of the\npiece as a list of triples (onset; offset; pitch ), candidates\nfor a schema stage can be found by considering all groups\nof notes (pairs, in the case of two voices) that lie within a\ncertain distance. In the traditional skipgram approach, this\ndistance would be measured by the index of each note in\nthe list. However, in the case of voice-leading schemata, it\nis more meaningful to measure this distance with respect\nto the timing of the notes, e.g., as the distance between on-\nsets or the distance between the offset of the earlier and the\nonset of the latter note.\nSince a voice-leading schema consists of several con-\nsecutive stages, it is natural to apply this more general idea\nof skipgrams again, now to the list possible stages. As\nwith notes, the distance between two stages can be deﬁned\nin several ways, e.g., as the distance from the beginning\nof the ﬁrst stage to the beginning of the second, or the\namount of time between the stages. In the following sec-\ntion, an algorithm that enumerates skipgrams over streams\nof arbitrary objects for arbitrary deﬁnitions of distance is\npresented along with some useful extensions.\n3.1 The Generalized Skipgram Algorithm\nThe basic algorithm for generalized skipgrams is shown in\nAlgorithm 1. It takes a stream of objects (e.g., notes or\nschema stages), an upper bound on the allowed “skip” k,\nthe length of the generated skipgrams n, and a cost func-\ntionc. The cost function is used to represent the distance\nbetween two objects: the combined cost across a skipgram\nmust not be greater than k. While it traverses the input\nstream, a set of preﬁxes (incomplete skipgrams) is main-\ntained. For each preﬁx that can be extended by the current\nelement without increasing the total cost beyond k, the ex-\ntended version is added to the preﬁx set. Preﬁxes of length\nnare added to the output and removed from the set of pre-\nﬁxes. Finally, the current element starts a new preﬁx.\nIn order to keep the set of preﬁxes small, a preﬁx is re-\nmoved as soon as extending it increases the cost beyond k.\nAs long as the stream is sorted in a way that every subse-\nquent element would increase the cost of the preﬁx even\nfurther, this optimization does not discard valid skipgrams.\nThat means the input stream input must satisfy\n8x < y < z2input :c(x; y)\u0014c(x; z);\nwhere x < y denotes that xappears before yininput .\nThe cost function can handle the distance in several\nways. For example, if the distance between the ﬁrst and\nthe last note in a skipgram should be limited, then the cost\nequals the distance between the onsets of two neighboring\nnotes in the skipgram. On the other hand, if the distance\nbetween two neighboring notes is to be limited, the cost\ncan be deﬁned non-continuously as 0if the notes are within\nthe allowed distance and k+ 1. This way, the combined\ncost is 0, except when a single neighbor pair of notes is too\nfar apart, in which case it exceeds k.Algorithm 1 The basic algorithm for enumerating gener-\nalized skipgrams.\n1:function SKIPGRAMS (input ; k; n; c )\n2: pfxs fg\n3: output [ ]\n4: cost(p; x) =Pl\u00001\ni=1c(pi; pi+1) +c(pl; x)\n5: forx input do\n6: open fpjp2pfxs;cost(p; x)\u0014kg\n7: ext fp\u000exjp2openg\n8: append (output ;[pjp2ext;jpj=n])\n9: pfxs open[fpjp2ext;jpj< ng[fxg\n10: end for\n11: return output\n12:end function\nNote that the skipgram algorithm traverses the input\nstream exactly once, so streaming it is straightforward.\nSimilarly, the output can be streamed instead of collected,\neither using some form of concurrency and a channel be-\ntween the output of the skipgram generator and some con-\nsumer, or non-concurrently using an iterator pattern.\n3.2 Early Filtering\nIf the list of generated skipgrams will be ﬁltered for some\nproperty (e.g., only selecting notes that do not overlap or\nthat match a given schema prototype), it is desirable to\nﬁlter out preﬁxes that cannot be completed to satisfy the\npredicate as early as possible, instead of generating all of\nits completions ﬁrst. Therefore, an extension of Algorithm\n1 additionally takes a predicate pred , which it applies to\nevery generated preﬁx. Every preﬁx that does not satisfy\npred is removed. As with the cost function, this predicate\ncan be deﬁned freely.\n3.3 Stable Ordering\nAlgorithm 1 adds its output in the order in which preﬁxes\nare completed. As a consequence, the output stream does\nnot retain the order of the ﬁrst element in each skipgram\nwith respect to the input order. Instead the input order is\nreﬂected in the last element of each skipgram.\nFor some applications, it might be desirable to keep the\norder of the ﬁrst elements intact. For example, when com-\nputing skipgrams of skipgrams (e.g., ﬁrst groups of notes,\nthen sequences of groups), the second skipgram pass ex-\npects its input to be monotonic with respect to the cost\nfunction. In the case of note groups, this will likely depend\non the earliest onset in the group. The ﬁrst skipgram pass\ntakes a list of notes ordered by onset, but the note groups\nit returns will not be ordered by the onset of their ﬁrst (and\ntherefore earliest) note but by the onset of their last note.\nIn general, the appropriate order of skipgrams can be\nobtained by generating the whole list of skipgrams and\nsorting it, but this would destroy the streaming property of\nthe algorithm. As the number of skipgrams grows quickly,\nit might not even be feasible to keep all skipgrams in mem-\nory. Furthermore, as kis intended to limit the range of\nskipgrams, a skipgram can only be displaced as much as k,Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 549so the array will almost be sorted, and sorting can be done\non the ﬂy.\nStable ordering can be ensured efﬁciently by holding\nback completed skipgrams (instead of adding them to the\noutput as soon as they are generated) until no new skip-\ngrams can be generated that should precede them. This\ncan be achieved by using a priority queue to hold the ﬁn-\nished but not yet released skipgrams in the correct order. In\neach iteration, the open preﬁxes are searched for the “old-\nest” ﬁrst element. Then, all skipgrams in the output queue\nstarting with an element not younger than this oldest ini-\ntial preﬁx element are released. If the output needs to be\nordered lexicographically, the queue content must be com-\npared not to the oldest initial element but to the complete\nlexicographically oldest open preﬁx. The queue can be up-\ndated efﬁciently by sorting the newly generated preﬁxes\nand merging the resulting list with the existing queue as in\na merge sort.\n3.4 Sampling Skipgrams\nThe number of generated skipgrams as well as the asymp-\ntotic runtime of the algorithm are difﬁcult to estimate, as\nthey depend on the number of elements within a range of\nkor the number of currently open preﬁxes, respectively, at\nany point in the stream. This, in turn, depends on the com-\nbination of input and cost function, so no general statement\nabout runtime and space complexity can be made without\nknowing both. In the worst case, generalized skipgrams\nconsist of all subsets of length nfrom the list of Lentities,\ngenerating\u0000L\nn\u0001\nskipgrams.\nBecause this amount of skipgrams is costly to enumer-\nate and process, an alternative is to uniformly draw sam-\nples from the list of all skipgrams. For a given probability\np, a biased coin is tossed for each skipgram, which deter-\nmines whether the skipgram is selected or not. If this is\ndone after the skipgram is completely generated, all skip-\ngrams must be enumerated once, so computation time is\nsaved only during consumption but not production. Con-\nversely, one could toss the coin for each new preﬁx of\nlength 1. This way, all extensions of a discarded preﬁx\nneed not be computed, which saves computation time but\nalso removes a whole family of related skipgrams from the\noutput.\nA third approach combines the other two by tossing a\ncoin each time a preﬁx is extended. As this happens n\u00001\ntimes for a preﬁx of length n(not counting the creation of\nthe initial length 1 preﬁx), so the coin is biased not with p\nbut with p0=n\u00001pp. This way, a skipgram is only included\nif all of its preﬁx extensions succeed, i.e., with probability\n(p0)n\u00001=p. Furthermore, computation time can be saved\nby discarding short preﬁxes, while variety is preserved by\nalso discarding preﬁxes in later stages.\nWith this method, it is not possible to uniformly draw\na ﬁxed number of samples. However, the expected num-\nber of samples can be estimated by choosing a small pand\nextrapolating the resulting amount of sampled skipgrams.\nThe total number of skipgrams Ncan be estimated simi-\nlarly, as the number of sampled skipgrams is expected tobeNp.\n4. SKIPGRAMS ON POLYPHONIC MUSIC\n4.1 Dataset\nThe described method is applied to 17 piano sonatas by\nWolfgang Amadeus Mozart in MIDI format.1A schema\nhas 2 or 3 voices and consists of 2 to 4 stages. These\ndimensions of a schema are notated as voices \u0002stages\nornv\u0002ns. and the sampling parameters pvandpsare\nadapted to these dimensions. The skip limit within a stage\nkvis one bar in total, the limit between the stages ksis also\none bar, but per pair of stages.\n4.2 Method\nFor the discovery of musical schemata, three assumptions\nare made. First, schemata are semi-local structures, that\nis, they extend over a limited range of time. This prop-\nerty is inherent in the skipgram formalism with an appro-\npriate cost function. Second, they consist of a horizon-\ntal sequence of pseudo-vertical structures, which consist\nof a ﬁxed number of possibly non-simultaneous or even\nnon-overlapping notes. Third, patterns are characterized\nby their pitch content, not by their temporal properties.\nThis pitch content is subject to certain equivalences, e.g.,\nregarding transposition of the whole pattern or the exact\noctave of each pitch.\nIn order to ﬁnd vertical structures in polyphonic pieces,\nskipgrams can be applied to a stream of notes. Each note\nhas a pitch, an onset and an offset, and the notes are sorted\nby their onsets. For this purpose, the cost function is the\ndifference between the onsets of two notes in the skipgram\ncv(n1; n2) =onset(n2)\u0000onset(n1);\nwhich in sum amounts to the onset difference between the\nearliest and the latest note in the group. This allows notes\nto overlap but restricts their temporal distance. The stable\nvariant of the skipgram algorithm is used to ensure that\nthe output stream is again ordered by (earliest) onset. The\nnumber of notes nvin the ﬁrst pass can be regarded as the\nnumber of voices in the vertical structure.\nA second skipgram pass builds length nssequences of\nvertical structures by taking the output of the ﬁrst pass as\nthe new input. Since these structures should be horizon-\ntally organized, temporal overlap between their stages is\nforbidden (by deﬁning an appropriate pred ). The amount\nof time between the onsets of slices is restricted by a step\nfunction that admits a skip up to ksfor each pair of neigh-\nbors:\ncs(s1; s2) =(\n0 if onset (s2;1)\u0000onset(s1;1)\u0014ks\nks+ 1 otherwise.\nDue to the large amount of skipgrams generated, the sam-\npling parameters are adapted to the number of voices ( pv)\n1Encoded by Craig Sapp in Kern format and converted to\nMIDI. Available at https://github.com/craigsapp/\nmozart-piano-sonatas .550 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018nvnspspv sampled total cov\n2 2 1 :0 1:0 3 :30\u00011081081:0\n2 3 1 :0 0:001 9 :92\u000110710110:99998\n2 4 1 :0 10\u000063:77\u000110710130:30\n3 2 0 :1 1:0 3 :53\u000110810100:9997\nTable 1 : The combinations of parameters used to generate\nthe results. For each combination, the sampled and the\nrough estimated total number of skipgrams, as well as the\ncoverage are given. Coverage is the ratio of the number of\nskipgram classes encountered and the number of possible\nclasses for the given dimensions ( 12nvns\u00001).2\nand stages ( ps). An example of such a nested skipgram is\nshown in Figure 2.\nThe pitch content of the resulting structures is sum-\nmarized by summing the occurrences of skipgrams with\nsimilar pitch content (“skipgram classes”). Pitch combi-\nnations are grouped by sorting the pitches in each stage\nin ascending order, removing octave information (pitch\nclasses), and transposing every pitch class relative to the\nlowest note of the ﬁrst stage. For example, the sequence\n(f; c0; a0)!(e; c0; g0)would be encoded as (0;7;4)!\n(11;7;2). This is similar but not identical to the voice-\nleading type representation used in [10], which addition-\nally removes the order of the pitches and the magnitude\nof each pitch class. Since the focus here is on polyphonic\nvoice-leading schemata, both order and magnitude are re-\ntained. For nvvoices and nsstages, there are 12nvns\u00001\npossible skipgram classes, as the transposition step always\nsets the ﬁrst bass note to 0.\n5. RESULTS AND DISCUSSION\n5.1 Results\nTable 1 gives an overview of the used parameter combina-\ntions. For each set of parameters, it shows the number of\ngenerated skipgrams, the estimated number of total skip-\ngrams, as well as the coverage, which is the ratio of the\nnumber of encountered skipgram classes and the number\nof possible classes for the given dimensions. Good cov-\nerage is important for prediction tasks, where the zero-\nfrequency problem occurs (i.e., where a prediction context\nhas not been encountered at least once, see [10]).\nAs the outer-voice movement is most characteristic of\nvoice-leading schemata, the most general insight can be\nprovided by two-voiced skipgram classes, which also have\na good coverage for reasonable computational effort. Ad-\nditionally, the 3\u00022skipgrams were generated to observe\nthe effect of increasing the number of voices on the found\npatterns. Larger dimensions did not produce a sufﬁcient\ncoverage due to computational constraints. The total num-\nber of skipgrams without sampling can be estimated by\n2Runtimes ranged from a few minutes to several hours. While paral-\nlelization was straightforward by splitting the dataset, memory usage was\nproblematic and prevented generating skipgrams with larger dimensions.class abs rel\n1 (0 ;3)!(0;2)!(10;2)!(10;0) 1190 0 :32\n2 (0 ;3)!(3;3)!(0;3)!(3;3) 1029 0 :27\n3 (0 ;1)!(10;1)!(8;0)!(8;10) 1009 0 :27\n4 (0 ;0)!(5;0)!(0;0)!(5;0) 976 0 :26\n5 (0 ;0)!(9;0)!(5;0)!(0;0) 964 0 :26\n6 (0 ;3)!(3;3)!(8;3)!(3;3) 937 0 :25\n7 (0 ;1)!(10;1)!(0;0)!(8;10) 934 0 :25\n8 (0 ;0)!(9;0)!(0;0)!(9;0) 921 0 :24\n9 (0 ;0)!(10;1)!(10;0)!(8;10) 902 0 :24\n10 (0 ;3)!(0;1)!(10;1)!(10;0) 897 0 :24\nTable 2 : The 10 most frequent 2\u00024skipgram classes that\nhave no repeating stages\nmultiplying the sampled skipgrams with ps(pnsv).pvneeds\nto be exponentiated, as it factors in the output probability\nof a skipgram on each of its stages.\nTable 3 shows the 10 most frequent skipgram classes\nfound for each set of parameters. As the most frequent\npatterns mainly indicate arpeggiation patterns (see Section\n5.2), an additional ﬁlter is applied to the 2\u00024skipgrams,\nwhich forbids the repetition of a stage. The resulting 10\nmost frequent patterns are shown in Table 2.\nIn addition to enumerating (or sampling from) all skip-\ngrams in the corpus, the generalized skipgram algorithm\ncan be used as a pattern matcher or schema ﬁnder. Figure 2\nshows the ﬁrst skipgram matching a two voiced pattern\n(0;6)!(1;4)!(10;4)!(11;3)in the third move-\nment of Mozart’s third piano sonata (from which the exam-\nple in Figure 1 is taken), found by enumerating the pieces\n2\u00024skipgrams. Pattern matching can be performed efﬁ-\nciently by using the early ﬁltering mechanism described in\nSection 3.2 to remove preﬁxes that cannot match.\n5.2 Discussion\nAs Table 1 shows, even with moderate sampling the cov-\nerage is excellent for smaller dimensions. As the dimen-\nsions increase and the sampling probability decreases, the\ncoverage rapidly decreases as well, because the large num-\nber of possible pitch combinations conﬂicts with the in-\ncreased need for reducing the computational effort via\nsampling. For example, for 2 voices and 4 stages, there\nare127\u00193:58\u0001107possible skipgram classes, but the to-\ntal number of sampled skipgrams was only 3:77\u0001107. In\nprinciple, however, the ﬂexibility of nested skipgrams pro-\nvides a good coverage, conﬁrming the results in [10] for\nﬂat skipgrams. For larger problem instances of up to 4\u00024\nskipgrams, the computational problem can be solved by\ngood parallelization and sufﬁcient computation resources.\nBased on frequency, the patterns found in the corpus\nare dominated by interval combinations as they appear in\nmajor and minor triads, followed by simple step-wise rela-\ntions. This cannot be explained by the fact that the mu-\nsic of Mozart is mostly triadic to a large extent, as the\nstages of a single skipgram are strictly non-simultaneous.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 551In music that is triadic but consists of sequences of non-\narpeggiated, non-repeating chords, the stages of a skip-\ngram will be taken from different chords. Thus, the found\npatterns reveal the usage of harmonic surface patterns such\nas Alberti bass. This becomes especially clear from vari-\nants of (0;0)!(5;0)and(0;0)!(9;0), which are\nconsistently ranked very high and indicate a prevalence of\nthe ﬁfth in combination with the third or the root of a major\ntriad, resembling the Alberti bass pattern.\nIn contrast, the ﬁltered patterns shown in Table 2 mainly\nconsist of instances of the typical voice-leading pattern of\ndescending 3-2 suspension sequences. This pattern is used\nas a typical elaboration procedure in several voice-leading\nschemata. This ﬁnding shows that nested skipgrams are\nvery well capable of representing polyphonic structures.\nThe remaining question is how to automatically distinguish\nsurface patterns from patterns on higher structural levels in\nthe generated skipgrams.\n6. CONCLUSION\nThe results clearly show that the generalized skipgram for-\nmalism is capable of modeling streams of events that have\nanon-ﬂat shape, such as streams of notes in polyphonic\nmusic. The monotonicity property explained in Section 3.1\nis a general criterion for the applicability of the presented\nalgorithm. Thus, generalized skipgrams can prove useful\nfor a wide range of problems from various domains other\nthan music that deal with sequential but overlapping data.\nWith respect to polyphonic music, generalized skip-\ngrams provide a powerful mechanism for accessing poly-\nphonic structure, solving the problem of building vertical\nstructures from non-overlapping notes. Hence, a poten-\ntially powerful application of generalized skipgrams is to\nuse them as the basic representation in variety of other\nmethods that provide rich pattern languages, replacing the\ncurrently used sequence-of-slices structure. For example,\nit is possible to apply viewpoint techniques to the skip-\ngrams generated from a polyphonic stream.\nFinally, the skipgram approach requires very little as-\nsumptions on its own, but can easily be extended to ﬁlter\nfor more advanced, theoretically or empirically motivated\nproperties. The discovery of schema-like patterns, for ex-\nample, will require to add appropriate ﬁlters that separate\nsurface from middleground patterns. This helps to advance\nthe understanding of the essential properties of schemata,\nas the added assumptions can be clearly separated from the\nones inherent in the skipgram representation.\n7. ACKNOWLEDGEMENTS\nThe research presented in this paper is generously sup-\nported by the V olkswagen Foundation and Claude Latour.\nWe also thank the anonymous reviewers for their helpful\ncomments.class abs rel\n1 (0 ;0)!(0;0) 3 :3e6 10 :0\n2 (0 ;0)!(5;0) 1 :38e6 4 :18\n3 (0 ;0)!(0;5) 1 :34e6 4 :06\n4 (0 ;0)!(9;0) 1 :34e6 4 :06\n5 (0 ;0)!(7;0) 1 :32e6 4 :01\n6 (0 ;7)!(7;7) 1 :31e6 3 :96\n7 (0 ;5)!(0;0) 1 :28e6 3 :89\n8 (0 ;3)!(3;3) 1 :27e6 3 :86\n9 (0 ;0)!(0;7) 1 :22e6 3 :7\n10 (0 ;0)!(10;0) 1 :21e6 3 :68\n1 (0 ;0)!(0;0)!(0;0) 121073 1 :22\n2 (0 ;0)!(0;0)!(9;0) 44143 0 :44\n3 (0 ;0)!(0;0)!(5;0) 43764 0 :44\n4 (0 ;0)!(5;0)!(0;0) 40859 0 :41\n5 (0 ;3)!(3;3)!(3;3) 40543 0 :40\n6 (0 ;7)!(7;7)!(7;7) 39470 0 :39\n7 (0 ;0)!(9;0)!(0;0) 39056 0 :39\n8 (0 ;0)!(0;0)!(10;0) 34975 0 :35\n9 (0 ;0)!(0;0)!(0;5) 29343 0 :29\n10 (0 ;0)!(0;0)!(7;0) 28667 0 :28\n1 (0 ;0)!(0;0)! 6381 0 :169\n(0;0)!(0;0)\n2 (0 ;0)!(0;0)! 2436 0 :065\n(0;0)!(9;0)\n3 (0 ;0)!(0;0)! 2386 0 :063\n(0;0)!(5;0)\n4 (0 ;0)!(0;0)! 2184 0 :058\n(9;0)!(0;0)\n5 (0 ;0)!(0;0)! 2173 0 :058\n(5;0)!(0;0)\n6 (0 ;0)!(5;0)! 2075 0 :055\n(0;0)!(0;0)\n7 (0 ;3)!(3;3)! 2031 0 :054\n(3;3)!(3;3)\n8 (0 ;0)!(9;0)! 2001 0 :053\n(0;0)!(0;0)\n9 (0 ;7)!(7;7)! 1918 0 :051\n(7;7)!(7;7)\n10 (0 ;0)!(0;0)! 1753 0 :046\n(0;0)!(10;0)\n1 (0 ;0;0)!(0;0;0) 461238 1 :31\n2 (0 ;0;0)!(9;0;0) 214255 0 :61\n3 (0 ;3;3)!(3;3;3) 208562 0 :59\n4 (0 ;0;0)!(5;0;0) 205212 0 :58\n5 (0 ;7;7)!(7;7;7) 200170 0 :57\n6 (0 ;3;3)!(0;3;3) 172370 0 :49\n7 (0 ;0;0)!(10;0;0) 162212 0 :46\n8 (0 ;2;2)!(2;2;2) 133658 0 :38\n9 (0 ;7;7)!(0;7;7) 131357 0 :37\n10 (0 ;0;2)!(0;0;0) 128846 0 :37\nTable 3 : The 10 most frequent 2\u00022,2\u00023,2\u00024, and\n3\u00022skipgram classes. Relative frequencies have been\nscaled by 103and rounded appropriately.552 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20188. REFERENCES\n[1] D. Conklin. “Representation and Discovery of Ver-\ntical Patterns in Music”. In: Music and Artiﬁcial\nIntelligence . Ed. by C. Anagnostopoulou, M. Fer-\nrand, and A. Smaill. Berlin, Heidelberg: Springer\nBerlin Heidelberg, 2002, pp. 32–42. ISBN : 978-3-\n540-45722-0.\n[2] D. Conklin and M. Bergeron. “Discovery of Con-\ntrapuntal Patterns”. In: Proceedings of the 11th In-\nternational Society for Music Information Retrieval\nConference, ISMIR 2010, Utrecht, Netherlands, Au-\ngust 9-13, 2010 . Ed. by J. S. Downie and R. C.\nVeltkamp. International Society for Music Informa-\ntion Retrieval, 2010, pp. 201–206. ISBN : 978-90-\n393-5381-3.\n[3] R. Gjerdingen. Music in the Galant Style . Oxford,\nNew York: Oxford University Press, Oct. 11, 2007.\n528 pp. ISBN : 978-0-19-531371-0.\n[4] D. Guthrie, B. Allison, W. Liu, L. Guthrie, and Y .\nWilks. “A Closer Look at Skip-Gram Modelling”.\nIn:Proc. of the 5th International Conference on\nLanguage Resources and Evaluation (LREC-2006) .\nEuropean Language Ressources Association, 2006,\npp. 1222–1225.\n[5] O. Lartillot. “In-Depth Motivic Analysis Based\non Multiparametric Closed Pattern and Cyclic Se-\nquence Mining”. In: Proceedings of the 15th In-\nternational Society for Music Information Retrieval\nConference, ISMIR 2014, Taipei, Taiwan, October\n27-31, 2014 . Ed. by H.-M. Wang, Y .-H. Yang, and\nJ. H. Lee. 2014, pp. 361–366.\n[6] D. Meredith, K. Lemstr ¨om, and G. A. Wiggins.\n“Algorithms for Discovering Repeated Patterns in\nMultidimensional Representations of Polyphonic\nMusic”. In: Journal of New Music Research 31.4\n(Dec. 1, 2002), pp. 321–345. ISSN : 0929-8215. DOI:\n10.1076/jnmr.31.4.321.14162 .\n[7] M. T. Pearce and G. A. Wiggins. “Expectation in\nMelody: The Inﬂuence of Context and Learning”.\nIn:Music Perception: An Interdisciplinary Journal\n23.5 (July 1, 2006), pp. 377–405. ISSN : 0730-7829,\n1533-8312. DOI:10.1525/mp.2006.23.5.\n377.\n[8] M. Pearce and G. Wiggins. “Improved Methods for\nStatistical Modelling of Monophonic Music”. In:\nJournal of New Music Research 33.4 (Dec. 1, 2004),\npp. 367–385. ISSN : 0929-8215. DOI:10.1080/\n0929821052000343840 .\n[9] P.-Y . Rolland. “Discovering Patterns in Musical Se-\nquences”. In: Journal of New Music Research 28.4\n(Dec. 1, 1999), pp. 334–350. ISSN : 0929-8215. DOI:\n10.1076/0929-8215(199912)28:04;1-\nO;FT334 .[10] D. R. W. Sears, A. Arzt, H. Frostel, R. Sonnleitner,\nand G. Widmer. “Modeling Harmony with Skip-\nGrams”. In: Proceedings of the 18th International\nSociety for Music Information Retrieval Confer-\nence, ISMIR 2017, Suzhou, China, October 23-27,\n2017 . Ed. by S. J. Cunningham, Z. Duan, X. Hu, and\nD. Turnbull. 2017, pp. 332–338. ISBN : 978-981-11-\n5179-8.\n[11] R. P. Whorley, C. Rhodes, G. Wiggins, and M. T.\nPearce. “Harmonising Melodies: Why Do We Add\nthe Bass Line First?” In: International Conference\non Computational Creativity. Sydney, 2013, pp. 79–\n86.\n[12] R. P. Whorley, G. Wiggins, C. Rhodes, and M. T.\nPearce. “Development of Techniques for the Com-\nputational Modelling of Harmony”. In: International\nConference on Computational Creativity. Coimbra,\nPortugal, 2010, pp. 11–15.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 553"
    },
    {
        "title": "Two Web Applications for Exploring Melodic Patterns in Jazz Solos.",
        "author": [
            "Klaus Frieler",
            "Frank Höger",
            "Martin Pfleiderer",
            "Simon Dixon"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492533",
        "url": "https://doi.org/10.5281/zenodo.1492533",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/177_Paper.pdf",
        "abstract": "This paper presents two novel user interfaces for investigating the pattern content in monophonic jazz solos and exemplifies how these interfaces could be used for research on jazz improvisation. In jazz improvisation, patterns are of particular interest for the analysis of improvisation styles, the oral transmission of musical language, the practice of improvisation, and the psychology of creative processes. The ongoing project \"Dig That Lick\" is devoted to addressing these questions with the help of a large database of jazz solo transcriptions generated by automated melody extraction algorithms. To expose these transcriptions to jazz researchers, two prototypes of user interfaces were designed that work currently with the 456 manually transcribed jazz solos of the Weimar Jazz Database. The first one is a Shiny application that allows exploring a set of 653 of the most common patterns by eminent players. The second one is a web interface for a general two-staged pattern search in the Weimar Jazz Database featuring regular expressions. These applications aim on the one hand at an expert audience of jazz researchers to facilitate generating and testing hypotheses about patterns in jazz improvisation, and on the other hand at a wider audience of jazz teachers, students, and fans.",
        "zenodo_id": 1492533,
        "dblp_key": "conf/ismir/FrielerHPD18",
        "keywords": [
            "user interfaces",
            "pattern content",
            "monophonic jazz solos",
            "jazz improvisation",
            "jazz researchers",
            "jazz teachers",
            "students",
            "fans",
            "automated melody extraction algorithms",
            "Weimar Jazz Database"
        ],
        "content": "TWO WEB APPLICATIONS FOR EXPLORING MELODIC PATTERNS IN\nJAZZ SOLOS\nKlaus Frieler1\u0003Frank Höger1Martin Pﬂeiderer1Simon Dixon2\n1Institute for Musicology, University of Music “Franz Liszt” Weimar, Germany\n2Center for Digital Music, Queen Mary University of London, UK\n\u0003Please direct correspondence to: klaus.frieler@hfm-weimar.de\nABSTRACT\nThis paper presents two novel user interfaces for investi-\ngating the pattern content in monophonic jazz solos and\nexempliﬁes how these interfaces could be used for research\non jazz improvisation. In jazz improvisation, patterns are\nof particular interest for the analysis of improvisation styles,\nthe oral transmission of musical language, the practice of\nimprovisation, and the psychology of creative processes.\nThe ongoing project “Dig That Lick” is devoted to address-\ning these questions with the help of a large database of jazz\nsolo transcriptions generated by automated melody extrac-\ntion algorithms. To expose these transcriptions to jazz re-\nsearchers, two prototypes of user interfaces were designed\nthat work currently with the 456 manually transcribed jazz\nsolos of the Weimar Jazz Database. The ﬁrst one is a Shiny\napplication that allows exploring a set of 653 of the most\ncommon patterns by eminent players. The second one is\na web interface for a general two-staged pattern search in\nthe Weimar Jazz Database featuring regular expressions.\nThese applications aim on the one hand at an expert audi-\nence of jazz researchers to facilitate generating and testing\nhypotheses about patterns in jazz improvisation, and on the\nother hand at a wider audience of jazz teachers, students,\nand fans.\n1. INTRODUCTION\nMusic Information Retrieval offers exciting options for mu-\nsicological research, particularly for methodologies which\nare hard (or impossible) to carry out manually, e. g., large\ncorpus studies and measuring acoustical properties. One\nsuch ﬁeld of application is the mining of patterns. Pat-\nterns – and repetitions in general – play an important role\nin nearly all music styles [10] and are thus of interest for\nmany sub-ﬁelds of musicology. In particular, they form a\ncrucial component of jazz improvisation [1, 13, 14]. The\nconcept of ‘pattern’ can be deﬁned in different ways and\nappears under different names in the literature. The more\nc\rKlaus Frieler, Frank Höger, Martin Pﬂeiderer, Simon\nDixon. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Klaus Frieler, Frank Höger, Martin\nPﬂeiderer, Simon Dixon. “Two web applications for exploring melodic\npatterns in jazz solos”, 19th International Society for Music Information\nRetrieval Conference, Paris, France, 2018.formal deﬁnition as ‘repeated sub-sequences’ (over a suit-\nable sequence space) contrasts with more speciﬁc usages\nin jazz theory and practice, where patterns are often called\n‘formulas’, ‘licks’, ‘stock-phrases’, and ‘riffs’. The main\ndifferences between these terms lie in their supposed ori-\ngin, their function, and their musical characteristics .\nA formula is mostly understood as a rather short pattern,\nwhich is well-rehearsed by an improviser. A formula is\ngenerally not musically autonomous, i. e., it can be rhyth-\nmized differently or embedded in other formulas to make\nlonger phrases (e. g., John Coltrane’s solo on “Giant Steps”\n[9]). The term ‘lick’ (or stock-phrase) usually refers to a\nmelodic unit with a distinctive recognizable character. In\nsome cases, licks trace their origin to an individual per-\nformer or even to a single solo. For example, Charlie Parker\ncreated many licks that were used by other jazz musicians\n[14]. In most cases, however, licks cannot be attributed to\na single originator. Thus, they form speciﬁc music vocabu-\nlaries of smaller and wider scope. A riff can be regarded as\na lick which is constantly repeated as an accompaniment\nand has thus a different musical function than a normal\nlick [12]. Other special cases are short quotations of pop-\nular tunes which are often used to humorous effect or for\nthe cultural practice of inter-textuality (‘intermusicality’),\nor ‘signifyin’ [11].\nGiven the signiﬁcance of patterns and licks in jazz, sev-\neral research questions arise. Some concern historical is-\nsues, e. g., the oral tradition of licks and the development\nof a typical jazz language; some are of a more systematic\nnature, e. g., the psychology of the creative process, where\npatterns and formulas can be regarded as necessary to ac-\ncomplish the highly virtuoso feat of modern jazz improvi-\nsation. Some of these research questions are:\n\u000fTo what extent are patterns and licks used to shape\nan improvisation?\n\u000fWhen and by whom are patterns and licks created\nand how are they transmitted between players over\ntime ( pattern archeology )?\n\u000fDoes pattern usage change with time and styles?\n\u000fIs there an inﬂuence of jazz education on pattern us-\nage (e. g., by published pattern collections)?\n\u000fHow are patterns used to build phrases, e. g., to con-\nstruct a typical bebop line?777\u000fWhich role do external musical inﬂuences such as\nquotes and signifying references play in jazz impro-\nvisation?\nIn this paper, two web tools which could help to address\nthese questions are introduced. First, the research back-\nground and some similar tools are discussed (Sect. 2), be-\nfore we describe the tools in detail in Sect. 3, including two\nuse case examples. The web applications are still proto-\ntypes under active development, but they are already help-\nful to make some interesting observations which will be\nreported in Sect. 4. Thoughts on future prospects of these\ntools conclude the paper (Sect. 5).\n2. BACKGROUND\nThe project “Dig That Lick: Analysing Large-Scale Data\nfor Melodic Patterns in Jazz Performances” (DTL) is a\ntwo-year project within the fourth “T-AP Digging Into Data\nChallenge”.1It sets out to investigate some of the afore-\nmentioned research questions using an interdisciplinary ap-\nproach combining musicology, computer science, MIR, and\njazz research. The project aims, ﬁrst, at developing tools\nfor pattern mining on symbolic as well as audio data, and,\nsecond, at understanding psychological and social aspects\nof patterns and licks in jazz. The development of appropri-\nate tools consists of three main pillars:\n1. Automatic transcription of jazz solo improvisations\nfrom audio informed by discographic metadata.\n2. Pattern mining and search in the melody transcrip-\ntions.\n3. Development of suitable user interfaces.\nSince the project is still in its initial phase, we will focus\nin this paper on the second and third issues by describing\ntwo prototypes of user interfaces for pattern mining in the\nWeimar Jazz Database [18] which was created by the Jaz-\nzomat Research Project [19]2.\nIn recent years, several scientiﬁc or commercial web-\nbased melody search engines with interfaces for different\ndatabases of different provenance and quality have been\nimplemented, all of which are scored-based. A web search\nshowed that many of these projects are now defunct or dis-\ncontinued. To name a few: C-Brahms (defunct), Midomi\n(discontinued, but seemingly functional), Hymnar (active,\nonly hymns), Mutopia, Music N-gram Viewer (discontin-\nued, but functional), Best Classical Tunes (outdated, but\nfunctional), and Melody Search (discontinued, functional-\nity unclear due to Flash player issues). The large number\nof abandoned sites suggests that melody search is not very\npopular with a general audience. Some more recent sites,\nthough, aim at musicological experts, e. g., the Troubadour\nMelodies Database, Global Chant, and Cantus Manuscript\nDatabase, which provide simple but efﬁcient search inter-\nfaces to specialized corpora.\n1http://dig-that-lick.eecs.qmul.ac.uk/\n2https://jazzomat.hfm-weimar.deAn older but still functional melody search engine is\nThemeﬁnder3, which interfaces with some large databases\nof folk and classical music in **kern format. The search\nworks well and is fast, though it does not offer metadata\nﬁlters, regular expressions, or a multi-staged search.\nMusipedia4is branded as a “Wikipedia for Music” and\nis based on a user-generated database of melodies. Search\nqueries are given in Lilypond format, but a piano-like user\ninterface to enter queries with the help of the mouse is\nalso provided. The search is based on similarity match-\ning [22] and, hence, always fuzzy; it is not possible to en-\nforce only exact matches. The result set always comprises\nfull melodies without indicating the matching location for\nthe query. The underlying corpus is not clearly speciﬁed,\nbut it seems that some well-known databases such as the\nEssen Folk Song Collection [21] are incorporated.\nFurthermore, Gulati [7] developed a system for melodic\npattern discovery in Indian Art Music based on automati-\ncally extracted pitch contours and a large set of special-\nized methods. A demo for browsing patterns in a large au-\ndio corpus and a visualization of pattern networks includ-\ning audio snippets can be found on the accompanying web\nsite5.\nThe investigation of patterns in jazz has rather different\nrequirements compared to those melody search engines.\nParticularly, the hybrid format of the Weimar Jazz Data-\nbase, which combines transcriptions with audio, as well as\nthe greater length of jazz solos (as compared to, for in-\nstance, incipits, and folk songs) demands ﬁne grained and\ncontrolled access to pattern instances. Furthermore, to as-\nsist users during exploration, providing scores and audio\nsnippets along with more abstract representations is im-\nportant in order to connect to established methodological\nstandards in jazz research and practice.\n3. TWO PATTERN MINING APPLICATIONS\n3.1 The Pattern History Explorer\nThe main goal of the Pattern History Explorer6, an inter-\nactive Shiny web application [2], is to enable the explo-\nration of interval patterns in jazz solos by providing infor-\nmation from many different angles. It provides an overview\nof interval patterns frequently used by a selected subset of\nperformers and traces their usage in the Weimar Jazz Da-\ntabase, allowing for the discovery of cross-artist and cross-\ntemporal relationships.\nCurrently, 653 interval patterns with 11,630 instances\nare included. The pattern corpus was created by mining for\ninterval patterns in solos of eminent performers using the\npartition mode of the melpat module from the MeloSpy-\nSuite [6]. Subsequently, all instances of these patterns were\nsearched for in the Weimar Jazz Database, and the results\nwere included in the application. Since the interval distri-\nbutions of jazz solos are dominated by step-wise motion,\n3http://www.themefinder.org/\n4https://www.musipedia.org/\n5http://compmusic.upf.edu/node/304\n6https://jazzomat.hfm-weimar.de/pattern_\nhistory/778 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018a certain number of patterns can be expected by chance\nalone, even more so by assuming Markov processes of ﬁrst\nor higher order [4]. Therefore, the following restriction to a\nminimum number of instances and sources was imposed to\nensure signiﬁcance of the patterns: interval patterns were\nlimited to be of no fewer than six elements occurring in\nat least three different solos of at least one musician. Ac-\ncording to previous investigations [4], this length seems\nto be a critical point for pattern distributions. The number\nof instances of each pattern depends partly on the amount\nof musical material available. For Bob Berg the criterion\nwas relaxed to patterns of at least seven elements occur-\nring at least twice in two different solos, since from a for-\nmer study [5] it was already known that many interesting\nand highly peculiar patterns occur only twice in the sub-\ncorpus of Berg’s solos in the Weimar Jazz Database. An-\nother exception was Charlie Parker, for whom the source\npatterns were extracted not from the Weimar Jazz Database\nbut from the Omnibook, a collection of 56 transcriptions of\nhis solos. In order to ﬁnd only Parker’s more eminent pat-\nterns, a criterion of at least six elements occurring in at\nleast ten different solos was applied.\nIn general, the user of the Pattern History Explorer se-\nlects a certain interval pattern from the overall set of 653\npatterns. Several options are available in order to ﬁlter the\npattern set or to change the ordering of the patterns accord-\ning to several criteria (e. g., ﬁltering by performer, length,\nintrinsic characteristics such as Huron contour [8] or tonal-\nity type, or content). For the selected pattern, various kinds\nof information can be accessed in the following tabs:\n\u000fListen & See . A sortable list of all instances of the\npattern in the Weimar Jazz Database is displayed. It\nincludes metadata such as name of the performer, ti-\ntle of the solo, year of recording, metrical and start\nposition. In one column the tonal context is displayed\nas a combination of chord context and extended chor-\ndal pitch class values (CDPCX, cf. [6]). Most impor-\ntantly, score snippets and audio links allow for vi-\nsual and aural inspection. Here, and in the following\ntabs, cross-links to the Pattern Search web applica-\ntion (see below) is provided to allow more reﬁned\nsearches.\n\u000fInstances . Further information about the instances\ncan be found here. A rhythmic encoding based on\nabsolute inter-onset interval (IOI) class (very short,\nshort, medium, long, very long), the starting pitch,\nthe chord context, the CDPCX value and a binary\nvector indicating the position of metrical accents in\nthe pattern are displayed. Additionally, some of the\nmetadata from the Listen & See tab are repeated.\nThe ﬁnal column indicates whether the instance is\ncontained in one single phrase. This table is also\nsortable.\n\u000fStats . Musical characteristics and statistical infor-\nmation of the pattern are compiled. Due to space\nlimitations we cover just some of these here, and re-\nfer the interested reader to the online documentation\nWynton MarsalisWoody ShawWayne ShorterSonny RollinsPepper AdamsOrnette ColemanMiles DavisMichael BreckerLee MorganLee KonitzKenny DorhamJoshua RedmanJohn ColtraneJoe LovanoJoe HendersonJ.J. JohnsonHank MobleyGeorge ColemanFreddie HubbardFats NavarroDexter GordonClifford BrownCharlie ShaversCharlie ParkerBranford MarsalisBob Berg\n1940 1960 1980 2000\nRecording YearPerformerFigure 1 . History plot of the pattern [-2, -2, -1, -2, -2, -1,\n-1]. The ﬁrst instance can be found in a solo by Charlie\nShavers on “Limehouse Blues” from 1947. A cluster can\nbe identiﬁed in the years around 1960. The circle radius\nrepresents the frequencies of patterns in each solo in that\nyear.\nfor full details. One important feature is log excess\nprobability , which is deﬁned as log10po\npe, where po\nis the observed frequency of a certain pattern within\nthe database and peis its expected frequency accord-\ning to a Markov model of zeroth order taken from\nthe global interval distribution. Not surprisingly, log\nexcess probability is strongly correlated with pattern\nlength ( r=:82,p < : 001). Another interesting fea-\nture is the number of different starting pitches, as\nthis indicates whether an interval pattern is indeed\na pitch pattern. A pitch pattern with a ﬁxed set of\npitches might be based on a physiological motor pat-\ntern, i. e., tied to a speciﬁc ﬁngering on the instru-\nment. Other interesting information provided here is\nwhich soloists favor this pattern and who played it\nﬁrst. A list of instances by performer can be found\nat the bottom of the page.\n\u000fTimeline . This page contains a visualization of the\ndistribution of pattern instances over time and per-\nformer. See Fig. 1 for a sample plot for the pattern\n[-2, -2, -1, -2, -2, -1, -1] (a descending diatonic line\nwith a chromatic ending). The ﬁrst instance can be\nfound in a solo by Charlie Shavers from 1947 and\nit is favored by Hank Mobley with six instances in\nthree different solos, mostly starting on the super-\ntonic (second scale step) over a ii7-V7transition.\nThis tonal interpretation is preferred in many other\ninstances. Other players who use this pattern are Joe\nHenderson and Freddie Hubbard with ﬁve instances\neach. As the plot shows, this pattern is spread widely\nover jazz history since its ﬁrst occurrence.\nBesides the tabs related to patterns, the tab ‘Info’ pro-\nvides general information on the Pattern History Explorer\nand the ‘Help’ tab contains detailed documentation. The\ntab ‘General Stats’ collects plots pertaining to statistics of\nall included patterns and instances (e. g., Fig. 3).Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 7793.2 Pattern Search\nWhile pre-computing a set of patterns is helpful in regard\nto the exploratory approach of the Pattern History Explorer,\nsearching for instances of arbitrary patterns of any length\nand frequency of occurrence within a database requires a\ndifferent approach. Although it is possible to search the\nWeimar Jazz Database with its accompanying software Me-\nloSpySuite and MeloSpyGUI, our web-based pattern search\ninterface7provides most of the functionality of the melpat\nsearch module while also extending it with audio and score\nsnippets (both as isolated patterns and within their melodic\ncontext) for visual and aural inspection.\nTo execute a basic search, the user has to enter a pat-\ntern as a space or comma separated list of elements and\nchoose a corresponding transformation, that is, a mathe-\nmatical mapping of the basic melodic representation, also\nknown as viewpoints [3]. Currently, ten pitch-related trans-\nformations for primary search are offered (e. g., MIDI pitch,\nsemitone intervals, CDPCX). An additional 18 transforma-\ntions, such as duration, IOI classes and various structural\nmarkers, are supplied for the optional secondary search.\nAdditionally, the search space can be constrained by seven\nmetadata categories, like performer, style, or recording year.\nSearch patterns can be regular expressions (in a speciﬁc\nhybrid syntax depending on the selected transformation)8\nwhich allows searches for variants in a single run. The\nsecondary search can be used to reﬁne the result space,\ne. g., by ﬁltering out certain rhythmic or metrical conﬁgu-\nrations or by conﬁning instances to a single phrase. Since\nthe last constraint is used frequently, there is also a short-\ncut checkbox that ﬁlls in the correct secondary search pat-\ntern (which is based on phrase boundary markers). The\nuser also has the option to request inclusion of up to 20\ntones before and after the actual pattern instance in both\nscore and audio ﬁles. In order to generate these, the cor-\nresponding checkboxes have to be selected, which is the\ndefault. There might be cases though, where the result set\nis very large (e. g., searching for very short intervals), and\none wants to avoid generating all audio and score ﬁles as it\nwould take a considerable amount of time. If both check-\nboxes are disabled, instances will appear in the result table\nafter a few seconds which allows for a ﬁrst examination of\nthe results. The generation of audio and score ﬁles is also\nsuppressed (with a warning being shown) when the result\nset exceeds the (current) limit of 100 instances. Finally, by\nclicking the ‘Display whole phrase’ button, the score of the\nwhole phrase containing the pattern is displayed.\nThe underlying search algorithm is built upon the basic\nPython regular expression module, which is fed with vir-\ntual Unicode strings constructed from the different melodic\nrepresentations (transformations) with different alphabets.\nScores are generated with the help of Lilypond, while au-\ndio snippets are directly extracted from the solo audio ﬁles.\nOn average, a full search including audio and score gener-\n7https://dig-that-lick.hfm-weimar.de/pattern_\nsearch\n8https://jazzomat.hfm-weimar.de/commandline_\ntools/melpat/melpat.html#search-pattern-syntaxation takes several minutes, depending strongly on the size\nof the result set. If the results can be found in the cache,\nthey are returned in a few seconds.\nFinally, a documentation page9is included as well as\na search history which gives a (browser-local) overview of\nall distinct search requests which have been submitted by\nthe user. For each speciﬁc search, a comment can be added\nand by clicking on the ‘Restore Search’ link the result of\nthe corresponding search is displayed.\n3.3 Example 1: A typical Parker lick\nAssume that we want to ﬁnd all occurrences of interval pat-\ntern [-1, -2, -1, -9, 3, 3, -1, -2] which was often played by\nCharlie Parker within various recordings [14]. For this, we\nenter it into the primary pattern ﬁeld and select ‘Semitone\nintervals’ as the primary transformation. By executing the\nsearch we get eight instances, six by Charlie Parker, and\none each by Dexter Gordon and Sonny Stitt, who are both\nknown to be strongly inﬂuenced by Charlie Parker [15].\n3.4 Example 2: Hunting for Coltrane’s “Giant Steps”\npattern\nIn his improvisations on “Giant Steps” recorded in 1959,\nseminal tenor saxophonist John Coltrane repeatedly uses a\nfour-tone pattern that consists of the root, the supertonic,\nthe third and the ﬁfth of the underlying chord [9, 20]. It\nwould be interesting to know whether other jazz musicians\nhave used this simple pattern, and if Coltrane used it on\nother recordings as well. Additionally, it is interesting to\nknow whether the pattern is only played over major chords\nor also with minor chords. While the Pattern History Ex-\nplorer currently covers only patterns with seven tones (six\nintervals) or more, the Pattern Search application offers\nseveral options to search for arbitrary patterns using var-\nious transformations and ﬁlters.\nThe pattern can be expressed, for example, with the\ntransformation ‘Chordal Pitch Class’ (CPC), which maps\npitches to pitch classes starting with the root of the under-\nlying chord. Since the third of a chord can be either major\n(CPC = 4) or minor (CPC = 3), we use regular expression\nsyntax to reﬂect this and the search pattern would be\n0, 2, \"[\", 3, 4, \"]\", 7\nThis translates as “the root (0) followed by the supertonic\n(2) followed by either the minor (3) or the major third (4)\nfollowed by the ﬁfth of the chord (7)”10. Note that special\nsymbols of regular expressions, here the square brackets\nas character set indicators, must be quoted, because of the\nhybrid syntax, where chordal pitch classes are expressed as\nintegers (not characters).\nHitting the search button yields 323 instances. Because\nthe limit of 100 instances is exceeded, no score or audio\nﬁle are generated and only the result list together with a\n9https://dig-that-lick.hfm-weimar.de/pattern_\nsearch/documentation\n10Alternatively, the same search could be carried out using the pattern\n1235 over the extended chordal diatonic pitch class (CDPCX) represen-\ntation.780 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 2 . The ﬁrst four search results for a pattern with\nchordal pitch class transformation using regular expression\n0, 2, \"[\", 3, 4, \"]\", 7 and secondary search pat-\ntern1, 0, \".\", 0 over primary and secondary metrical\naccents.\nwarning is displayed. To prune the result set, one can tick\nthe ‘Within Single Phrase’ box, which results in 297 in-\nstances. Characteristic of Coltrane’s use of the pattern is\nthat it very often starts on a strong beat (ﬁrst or third beat\nin 4/4 time). To express this constraint with a secondary\nsearch, we use the transformation ‘Primary and secondary\nmetrical accent’, which takes on the values ‘1’ for an event\non a beat position and ‘0’ otherwise, and the search pattern\n1,\".{3}\"\nwith the operation ‘Match’. The dot stands for any sym-\nbol (here only ‘0’ or ‘1’), while the number 3 in braces\nis a quantiﬁer meaning “exactly three repetitions”. Again,\nthe quotes are necessary here because of the hybrid syn-\ntax. This leads to all CPC patterns that start on a strong\nbeat, whereby the last three tones can lie on arbitrary met-\nrical positions. This results in 93 instances (by 37 players\nin 58 different solos), which are displayed as a complete\nlist (Fig. 2). There are 12 instances with a minor third and\n83 instances with major. Interestingly, John Coltrane never\nuses the minor version. 26 instances originate from him,\nfrom which 18 can be found in two recordings of “Giant\nSteps”. Michael Brecker, who is said to be heavily inﬂu-\nenced by John Coltrane, accounts for nine instances, all of\nwhich are also major.\nVisual inspection of the results shows, however, that\nsome of the instances are either not over one single chord\nor do not follow an ascending step-wise motion. Similarly,\neven though most of the instances feature a plain motion in\neighth notes, there are a few instances with rather different\nrhythms which still satisfy our metrical constraint. To ﬁl-\nter these cases, one could use other transformations, e. g.,\nsearching for duration patterns. Thus, here and in other sit-\nuations, a tertiary or even quaternary search stage would be\nneeded, which is not available yet but planned for a future\nversion of the Pattern Search application.\n4. SOME OBSERV ATIONS AND A HYPOTHESIS\nIn order to demonstrate the usefulness of the two systems\nin the context of jazz research, we like to report some ﬁrst\nobservations.\n01020\n0.0 0.5 1.0 1.5 2.0\nLog10 FrequencyNumber of starting pitchesFigure 3 . Number of different starting pitches versus log10\nof pattern frequency. The correlation is r=\u0000:92. The ﬁt\nis a quadratic polynomial.\nObservation 1 Pattern usage varies with performers and\nappeared with bebop.\nLooking at the distributions of patterns in the Pattern\nHistory Explorer with respect to performer, it seems clear\nthat different jazz improvisers have different levels of pat-\ntern usage. For example, players from earlier styles (e. g.,\nNew Orleans, Swing) have far fewer long interval patterns\nin their repertoire than later players. This might be partly\ndue to the fact that these performers are seldom playing\nlong lines – a practice that only became widespread with\nthe advent of bebop, and which probably made the usage\nof patterns a necessity.\nObservation 2 The more frequent an interval pattern, the\nmore tonally ﬂexible it is.\nAs shown in Fig. 3, the number of distinct starting pitches\nof a certain pattern generally increases with its frequency\nof occurrence. The relationship is approximately logarith-\nmic in the frequency Np/logfwith a strong correlation\nofr=:92(p < : 001). In other words, the more frequent\n(and shorter) a pattern, the more tonally ﬂexible it is. This\nseems to reﬂect typical rehearsal routines, where shorter\npatterns are more likely to be practiced in all keys whereas\nlong and very long patterns are designed to ﬁt in only one\nor two tonal contexts, e. g., to chord changes of speciﬁc\nsongs. This has to be tested on a larger database, taking\ninto account that not all keys, chords, and chord combina-\ntions are equally likely to occur in jazz.\nObservation 3 Patterns are mostly simple and reﬂect com-\nmon rehearsal routines.\nDiatonic patterns, i. e., step-wise motions, are by far the\nmost frequent pattern type followed by chromatic patterns.\nTogether they account for about 78 % of all patterns in-\ncluded in the Pattern History Explorer. This implies that\nthe main share of patterns is musically rather simple, i. e.,\nbuilt from diatonic scales, chromatic runs, and arpeggios.\nOne can conjecture that this is a result of practice traditions\nin which scales and arpeggios are rehearsed for technicalProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 781Figure 4 . The interval pattern [4, 3, -3, 1, 1, -4, 1, 1, -2, -2,\n-1, -2, -2, -1, 3, 3] of length 16 as found in two solos by\nCharlie Parker.\nﬂuency, but might end up ‘to lie in the ﬁngers’, i. e., as mo-\ntor patterns.\nHypothesis 1 Jazz solos are hierarchically composed of\nadaptive chunks.\nA general problem of pattern mining in jazz solos and\nother melody corpora is, however, to distinguish truly mean-\ningful (i. e., intended) patterns from randomly occurring\npatterns [7]. This is closely connected to the problem of\nﬁnding adequate models for the underlying (random) pro-\ncesses, for which some evidence can be extracted already\nfrom the data.\nSome very long patterns can be found, e. g., a pattern of\n16 tones by Charlie Parker with two instances in two dif-\nferent solos which are also tonally and rhythmically very\nsimilar (Fig. 4). The a priori probability under any Markov\nmodel for this is practically zero. This clearly shows that\nthis pattern was preconceived and then reproduced as a sin-\ngle chunk. In general, it seems very doubtful that jazz solos\ncould be successfully modeled with Markov chains (of any\norder) on the level of single tones or intervals [16, 17].\nThis is also corroborated by the existence of (non-trivial)\ntrill-like patterns (see Fig. 5 for an example), which are\namongst the longest patterns that can be found. These pat-\nterns are somewhat trivial, as they are “oscillations” of\na repeated shorter pattern, but they are also examples of\n“super-patterns”, i. e., long patterns containing shorter sub-\npatterns. This hints at Markov models not working on the\nnote event level but hierarchically on a chunk level.\nTo sum up, these ﬁrst observations suggest that jazz so-\nlos do not follow Markov models of any order on a tone-\nlevel, but are rather created by hierarchical processes with\ninterspersed “islands of high probability”, where patterns\nare reproduced as complete chunks while being adapted\nrhythmically and tonally to the current context. This strat-\negy could be dubbed “mutatis mutandis”, which seems to\nbe basic not only for improvisation but music creation in\ngeneral.\n5. CONCLUSION & OUTLOOK\nBoth applications presented in this paper are already us-\nable interfaces for the Weimar Jazz Database and serve\nas prototypes for applications to explore large databases,\nwhich are going to be automatically extracted from large\ncollections of jazz recordings. Both tools can be primarily\nFigure 5 . The interval pattern [-2, -2, -1, 5, -2, -2, -1, 5,\n-2, -2, -1, 5, -2, -2, -1, 5, -2, -2, -1] of length 19 as found\nin two solos by Bob Berg on the album ‘Enter the Spirit’\nfrom 1993.\nviewed as bespoke interfaces for the speciﬁc needs of jazz\nresearchers, but they could also be of interest to jazz teach-\ners, students and fans, as well as for training courses in\ncomputational music analysis. Compared to the possibili-\nties of the melpat module of the MeloSpySuite, they pro-\nvide superior presentation of results, particularly in their\nprovision of audio and score excerpts.\nThe development of both applications is still ongoing.\nThe Pattern History Explorer will be augmented by more\npatterns in the future. For the Pattern Search application,\nremoving the limit of 100 instances for full searches and\nspeed improvements are already under construction. An-\nother signiﬁcant extension would be the implementation of\narbitrarily many search stages, since the current status of\nonly two stages is often too restrictive. The incorporation\nof other databases, such as the Essen Folk Song Collection,\nwould also be feasible without major modiﬁcations.\nThe current system is based on the Python regular ex-\npression module and requires all melodies to be loaded into\nmemory at once. This is sufﬁciently fast for the Weimar\nJazz Database with 200,000 events. For searching larger\ndata sets, however, a more advanced retrieval technology\nis needed, e. g., distributed NoSQL databases and sophisti-\ncated search algorithms. Moreover, the generation of score\nﬁles could be optimized by switching from Lilypond to\nVexFlow11, not only to speed up score rendering but also\nto allow for score customization.\nSome features in the applications are only possible due\nto the high-quality manual transcriptions in the Weimar\nJazz Database with its comprehensive set of annotations.\nIn the scenario of automatically extracted transcriptions,\nseveral curtailments can be expected, e. g., transformations\nthat depend on such annotations might not be usable. How-\never, it is always possible to use pitch and interval trans-\nformations and to extract audio snippets for aural control,\nproviding also feedback for the transcription algorithm.\nFinally, it is planned to conduct user studies to gather\nfeedback for further improvements of the interface.\n6. ACKNOWLEDGEMENT\nThe “Dig that Lick\" project is supported by the Deutsche\nForschungsgemeinschaft (DFG, PF669/9-1) and the UK\nEconomic and Social Research Council (ES/R004005/1).\n11http://www.vexflow.com/782 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20187. REFERENCES\n[1] Paul F. Berliner. Thinking in jazz. The inﬁnite art of\nimprovisation . University of Chicago Press, Chicago,\n1994.\n[2] Winston Chang, Joe Cheng, J. J. Allaire, Yihui Xie, and\nJonathan McPherson. shiny: Web Application Frame-\nwork for R . 2017.\n[3] Darrell Conklin and Christina Anagnostopoulou. Rep-\nresentation and discovery of multiple viewpoint pat-\nterns. In Proceedings of the 2001 International Com-\nputer Music Conference , San Francisco, 2001. ICMA.\n[4] Klaus Frieler. Pattern usage in monophonic jazz so-\nlos, 2014. Talk given at the International Jazzomat Re-\nsearch Workshop, September, 26–27, 2014.\n[5] Klaus Frieler. Bob Berg’s Solo on “Angles”. In Mar-\ntin Pﬂeiderer, Klaus Frieler, Jakob Abeßer, Wolf-Georg\nZaddach, and Benjamin Burkhart, editors, Inside the\nJazzomat. New Perspectives for Jazz Research. , pages\n41–84. Schott-Campus, Mainz, 2017.\n[6] Klaus Frieler. Computational melody analysis. In Mar-\ntin Pﬂeiderer, Klaus Frieler, Jakob Abeßer, Wolf-Georg\nZaddach, and Benjamin Burkhart, editors, Inside the\nJazzomat. New Perspectives for Jazz Research. , pages\n41–84. Schott-Campus, Mainz, 2017.\n[7] Sankalp Gulati. Computational Approaches for\nMelodic Description in Indian Art Music Corpora .\nPhD thesis, Universitat Pompeu Fabra, Barcelona,\n2016.\n[8] David Huron. The melodic arch in western folksongs.\nComputing in Musicology , 10:3–23, 1996.\n[9] A. C. Lehmann and S. Goldhahn. Duration of playing\nbursts and redundancy of melodic jazz improvisation\nin John Coltranes \"Giant Steps\". Musicae Scientiae ,\n20(3):345–360, September 2016.\n[10] Elizabeth Hellmuth Margulis. On repeat: how music\nplays the mind . Oxford University Press, New York,\nNY , 2014.\n[11] Ingrid Monson. Saying something. Jazz improvisation\nand interaction . University of Chicago Press, Chicago,\n1996.\n[12] Ingrid Monson. Riffs, Repetition, and Theories of\nGlobalization. Ethnomusicology , 43:31–65, 1999.\n[13] Martin Norgaard. How jazz musicians improvise. The\ncentral role of auditory and motor patterns. Music Per-\nception: An Interdisciplinary Journal , 31(3):271–287,\n2014.\n[14] Thomas Owens. Charlie Parker. Techniques of impro-\nvisation . PhD thesis, University of California, Los An-\ngeles, 1974.[15] Thomas Owens. Bebop. The music and its players . Ox-\nford University Press, New York, 1995.\n[16] Francois Pachet. The continuator: Musical interaction\nwith style. Journal of New Music Research , 32(3):333–\n341, 2003.\n[17] François Pachet and Pierre Roy. Markov constraints:\nsteerable generation of Markov sequences. Con-\nstraints , 16(2):148–172, April 2011.\n[18] Martin Pﬂeiderer. The Weimar Jazz Database. In Mar-\ntin Pﬂeiderer, Klaus Frieler, Jakob Abeßer, Wolf-Georg\nZaddach, and Benjamin Burkhart, editors, Inside the\nJazzomat. New Perspectives for Jazz Research . Schott-\nCampus, Mainz, 2017.\n[19] Martin Pﬂeiderer, Klaus Frieler, Jakob Abeßer, Wolf-\nGeorg Zaddach, and Benjamin Burkhart, editors. In-\nside the Jazzomat: New Perspectives for Jazz Research .\nSchott Music GmbH & Co. KG, Mainz, 1 edition,\n2017. OCLC: 1015349144.\n[20] Lewis Porter. John Coltrane, his life and music . Uni-\nversity of Michigan Press, Ann Arbor, 1998.\n[21] H. Schaffrath. The Essen folksong collection. In\nD. Huron, editor, Database containing 6,255 folksong\ntranscriptions in the Kern format and a 34-page re-\nsearch guide . CCARH, Menlo Park, CA, 1995.\n[22] Frans Wiering, Rainer Typke, and Remco C. Veltkamp.\nTransportation Distances and their Application in\nMusic-Notation Retrieval. In Music Query: Methods,\nStrategies, and User Studies , volume 13 of Computing\nin Musicology , pages 113–128. 2004.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 783"
    },
    {
        "title": "Analysis of Common Design Choices in Deep Learning Systems for Downbeat Tracking.",
        "author": [
            "Magdalena Fuentes",
            "Brian McFee",
            "Hélène C. Crayencour",
            "Slim Essid",
            "Juan Pablo Bello"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492355",
        "url": "https://doi.org/10.5281/zenodo.1492355",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/203_Paper.pdf",
        "abstract": "Downbeat tracking consists of annotating a piece of musical audio with the estimated position of the first beat of each bar. In recent years, increasing attention has been paid to applying deep learning models to this task, and various architectures have been proposed, leading to a significant improvement in accuracy. However, there are few insights about the role of the various design choices and the delicate interactions between them. In this paper we offer a systematic investigation of the impact of largely adopted variants. We study the effects of the temporal granularity of the input representation (i.e. beat-level vs tatum-level) and the encoding of the networks outputs. We also investigate the potential of convolutional-recurrent networks, which have not been explored in previous downbeat tracking systems. To this end, we exploit a state-of-the-art recurrent neural network where we introduce those variants, while keeping the training data, network learning parameters and postprocessing stages fixed. We find that temporal granularity has a significant impact on performance, and we analyze its interaction with the encoding of the networks outputs.",
        "zenodo_id": 1492355,
        "dblp_key": "conf/ismir/FuentesMCEB18",
        "keywords": [
            "Downbeat tracking",
            "annotating beat positions",
            "deep learning models",
            "significant improvement",
            "systematic investigation",
            "temporal granularity",
            "encoding of outputs",
            "convolutional-recurrent networks",
            "state-of-the-art recurrent neural network",
            "performance impact"
        ],
        "content": "ANALYSIS OF COMMON DESIGN CHOICES IN DEEP LEARNING\nSYSTEMS FOR DOWNBEAT TRACKING\nMagdalena Fuentes1;2;\u0003, Brian McFee3;4, H´el`ene C. Crayencour1, Slim Essid2, Juan P. Bello3\n1L2S, CNRS-Univ.Paris-Sud-CentraleSup ´elec, France\n2LTCI, T ´el´ecom ParisTech, Univ. Paris-Saclay, France\n3Music and Audio Research Laboratory, New York University, USA\n4Center of Data Science, New York University, USA\nABSTRACT\nDownbeat tracking consists of annotating a piece of mu-\nsical audio with the estimated position of the ﬁrst beat of\neach bar. In recent years, increasing attention has been paid\nto applying deep learning models to this task, and various\narchitectures have been proposed, leading to a signiﬁcant\nimprovement in accuracy. However, there are few insights\nabout the role of the various design choices and the delicate\ninteractions between them. In this paper we offer a system-\natic investigation of the impact of largely adopted variants.\nWe study the effects of the temporal granularity of the in-\nput representation (i.e. beat-level vs tatum-level) and the\nencoding of the networks outputs. We also investigate the\npotential of convolutional-recurrent networks, which have\nnot been explored in previous downbeat tracking systems.\nTo this end, we exploit a state-of-the-art recurrent neural\nnetwork where we introduce those variants, while keeping\nthe training data, network learning parameters and post-\nprocessing stages ﬁxed. We ﬁnd that temporal granularity\nhas a signiﬁcant impact on performance, and we analyze\nits interaction with the encoding of the networks outputs.\n1. INTRODUCTION\nMusical rhythm is organized into hierarchical levels which\ninteract with each other. One of the predominant pulsa-\ntions is the beat, which matches the foot tapping of a per-\nson when listening to a music piece. Tatum is related to\nthe fastest pulsations still perceived by listeners, usually\ntwice to four times faster than beat. Beats of different\naccentuations are grouped in bars. Automatic downbeat\ntracking aims to determine the ﬁrst beat of each bar, being\na key component for the study of the hierarchical metri-\ncal structure. It is an important task in Music Information\nRetrieval (MIR) that represents a useful input for several\napplications, such as automatic music transcription [19],\nstructural segmentation [18] and rhythm similarity [22].\nc\rMagdalena Fuentes, Brian McFee, H ´el`ene C. Crayen-\ncour, Slim Essid, Juan P. Bello. Licensed under a Creative Commons\nAttribution 4.0 International License (CC BY 4.0). Attribution: Mag-\ndalena Fuentes, Brian McFee, H ´el`ene C. Crayencour, Slim Essid, Juan P.\nBello. “Analysis of Common Design Choices in Deep Learning Systems\nfor Downbeat Tracking”, 19th International Society for Music Informa-\ntion Retrieval Conference, Paris, France, 2018.The task of downbeat tracking has received considerable\nattention in recent years. In particular, the introduction\nof deep neural networks provided a major improvement\nin the accuracy of downbeat tracking systems [3, 10, 16],\nand the systems relying on deep learning have become the\nstate-of-the-art. These approaches usually exploit a ﬁrst\nstage of low-level feature computation, where several rep-\nresentations such as chroma [10] or spectral ﬂux [16] have\nbeen adopted. This is usually followed by a stage of fea-\nture learning with neural networks, whose outcome is an\nactivation function that indicates the most likely candi-\ndates for downbeats among the input audio observations.\nThen, a post-processing stage is often used, which con-\nsists of a dynamic model, typically a Dynamic Bayesian\nNetwork (DBN), Hidden Markov Model (HMM) or Con-\nditional Random Field [4, 11, 12].\nAmong the mentioned systems, different design choices\nwere taken at different stages of the processing pipeline,\nsuch as the temporal granularity of the input, low-level\nfeature representations, network architecture, and the post-\nprocessing methods. Additionally, different proposals\nwere evaluated using distinct training data and/or evalu-\nation schemes (e.g., cross-validation vs leave-one-dataset-\nout) [4, 11, 16]. This variability makes it difﬁcult to gain\ninsights about the actual role of each design choice, and\nthe delicate interactions between them.\nIn this paper, we systematically investigate the impact\nof design choices in downbeat tracking models. In par-\nticular, we study the effect of temporal granularity of the\ninput representation (i.e., beat-level vs tatum-level), the\noutput encoding (i.e., the label encoding used to train the\nnetworks), and their interactions with the post-processing\nstage and internal network architecture. This allows for\ngaining fresh understanding into the potential and limita-\ntions of the state-of-the-art approaches, and takes a step\ntoward the systematic design of these systems.\n1.1 Related work\nDurand et al. [11], proposed a system for downbeat track-\ning that consists of an ensemble of models each represent-\ning four different aspects of music: rhythm, harmony, bass\ncontent and melody. The authors developed a Convolu-\ntional Neural Network (CNN) for each musically inspired\nrepresentation, and estimated the downbeat likelihood by106averaging the likelihoods produced by each CNN in the en-\nsemble. Then, the authors turn the soft state assignments\nof the CNN ensemble into hard assignments ( downbeat vs\nno-downbeat ) using an HMM. This approach showed the\npotential of CNNs for downbeat tracking and the comple-\nmentarity of the different musically inspired features.\nIn parallel, B ¨ock et al. [4], presented a system that\njointly tracks beats and downbeats using Bi-Directional\nLong-Short Term Memory networks (Bi-LSTMs). The\nauthors used three different magnitude spectrograms and\ntheir ﬁrst order differences as input representations, in or-\nder to help the networks capture features with sufﬁcient\nresolution in both time and frequency. The input represen-\ntations were fed into a cascade of three fully connected Bi-\nLSTMs, obtaining activation functions for beat and down-\nbeat as output. Subsequently, a highly constrained DBN\nwas used for inferring the metrical structure.\nIn turn, Krebs et al. [16] proposed a downbeat track-\ning system that uses two beat-synchronous features to rep-\nresent the percussive and harmonic content of the audio\nsignal. Those feature representations, based on spectral\nﬂux and chroma, are then fed into two independent Bidi-\nrectional Gated Recurrent Units (Bi-GRUs) [8]. Finally,\nthe downbeat likelihood is obtained by merging the likeli-\nhoods produced by each Bi-GRU. The ﬁnal inference for\ndownbeat candidates relies on a constrained DBN.\nMore recently, combinations of CNNs and Recurrent\nNeural Networks (RNNs) such as GRUs or LSTMs have\nreceived increasing attention. For instance, Convolutional-\nRecurrent Neural Network architectures (CRNNs) have\nbeen proposed in other MIR tasks such as chord recogni-\ntion [20] or drum transcription [23], and they are the state\nof the art in other audio processing domains such as sound\nevent detection [2, 6].\n1.2 Our contributions\nIn this paper we offer a systematic investigation of impor-\ntant system design choices, namely the impact of the in-\nput observations’ temporal granularity, the output encod-\ning, and the post-processing stage. Also, we investigate the\npotential of CRNNs for improving feature learning for the\ntask of downbeat tracking. To perform our experiments,\nwe modify a state-of-the-art RNN-system [16], and study\nthe effect of the different envisaged variations, keeping the\ntraining setup and input features ﬁxed. Our experimental\nresults show that the post-processing stage improves the\nperformance in all cases, whereas the addition of a dense-\nstructured output encoding does not help in the training\nof downbeat tracking systems. The proposed CRNN ar-\nchitecture performs competitively with the state-of-the-art\nRNN system, being even able to improve the reference sys-\ntem’s performance with a proper choice of input’s temporal\ngrid. We also observe that though beat tracking errors tend\nto propagate to the output decisions, the CRNN system is\nable to recover from these errors better than the baseline\nRNN when taking the input observations over a tatum grid\n(as opposed to beat grid).2. ANALYSIS OF COMMON V ARIATIONS\nIn this section we brieﬂy describe the baseline system, the\nmotivation of each studied variation (or design choice) and\nthe experiments related to it. In particular, we study the ef-\nfects and interactions of 4 design choices: the input’s tem-\nporal granularity, the output encoding, the effect of post-\nprocessing and the network architecture.\n2.1 Recurrent neural network baseline\nTo perform our analysis, we implemented the state-of-\nthe-art downbeat tracking system presented by Krebs et\nal. [16]. The architecture of this system consists of two\nconcatenated Bi-GRUs of 25 units each, where each hid-\nden state vector h(t)at time tis mapped by a dense layer\nto a state prediction p(t)using a sigmoid activation. A\ndropout layer is used in training to avoid over-ﬁtting. Two\nseparate networks are trained using different input features\nand the obtained likelihoods are averaged. The low-level\ninput representations comprise two beat-synchronous fea-\nture sets, representing the harmonic andpercussive con-\ntent of the audio signal. The set of features describing\npercussive content, which we will refer to as PCF (Per-\ncussive Content Feature), is based on a multi-band spectral\nﬂux, computed using the short time Fourier transform with\na Hann window, using a hop-size of 10ms and a window\nlength of 2048 samples, with a sampling rate of 44100 Hz.\nThe obtained spectrogram is ﬁltered with a logarithmic ﬁl-\nter bank with 6 bands per octave, covering the range from\n30 to 17 000 Hz. The harmonic content’s representation is\nthe CLP (Chroma-Log-Pitch) [21] with a frame rate of 100\nframes per second. The temporal resolution of the features\nis 4 subdivisions of the beat for the PCF, and 2 subdivi-\nsions for the CLP features. For computational efﬁciency,\nthe authors in [16] assembled in matrices column-wise this\nresolution increment so the CLP feature set is of dimension\n12\u00022and the PCF is 45\u00024, which we maintained in this\nwork. The beats for the beat-synchronous feature mapping\nare obtained using the beat tracker presented in [3], with\nthe DBN introduced in [17].1\nIn our experiments, we have observed that including\nbatch normalization (BN) layers [14] consistently im-\nproves performance. We included two BN layers, one after\nthe input layer, and the other between the Bi-GRUs.\nThe optimization of the model parameters is carried out\nby minimizing the binary-cross-entropy between the esti-\nmated and reference values.\n2.2 Temporal granularity: beat vs tatums\nThe temporal granularity of the input observations (or tem-\nporal grid) relates to important aspects of the design of\ndownbeat tracking systems. It determines the length of the\ncontext taken into account around musical events, which\ncontrols design decisions in the network architecture, such\nas ﬁlter sizes in a CNN, or the length of training sequences\nin an RNN.\n1In particular we used the DBNBeatTracker algorithm of the madmom\npackage version 0.16 [5].Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 107Among the different downbeat systems, several granu-\nlarities have been used. In particular, the latest state-of-the-\nart systems use either musically motivated temporal grids\n(such as tatums or beats) or ﬁxed length frames. Systems\nthat use beat- or tatum-synchronous input depend on reli-\nable beat/tatum estimation upstream, so they are inherently\nmore complex, and prone to error propagation [11, 16].\nOn the other hand, frame-based systems are not subject\nto these problems, but the input dimensionality is much\nhigher due to the increased observation rate [4], which\ncauses difﬁculties when training the models.\nIn this paper, we focus on musically motivated tempo-\nral analysis grids, because they reduce the computational\ncomplexity of the systems considerably. We study the vari-\nations in performance using beat and tatum grids.\nWe compute the tatums by interpolating the beats, with\na resolution of 4 tatums per beat interval.2To study the\nimpact of the temporal grid, we train the networks keep-\ning the input features, architecture, training data, and post-\nprocessing ﬁxed, while changing only the inputs’ temporal\ngranularity. We adapt the sequence length used for training\nthe networks in order to consider the same musical context\nin all cases (as speciﬁed further below). We compare the\ninteraction of the choice of temporal grid with those of the\noutput encoding, the RNN or CRNN architectures and the\npost-processing stage.\n2.3 Output encoding: structured vs unstructured\nAmong the downbeat tracking systems mentioned in Sec-\ntion 1.1, the common choice is to use an one-hot vector\nencoding to indicate the presence or absence of a down-\nbeat at a particular position of the excerpt at training time.\nFor instance, if using temporal analysis grid that is aligned\non beats, a sequence of beats is usually encoded as s=\n[1;0;0;0;1;0;0;0], indicating the presence of a downbeat\nat the ﬁrst and 5th beat positions. We refer to this as un-\nstructured encoding. Here, we also investigate whether a\ndensely structured encoding may help the neural networks\nperform a better downbeat tracking.\n2.3.1 Structured encoding deﬁnition\nWe deﬁne the structured encoding as a set of classes that\nare active within the entire inter-beat interval. This is the\nsetC=f1; : : : ; 13g, where each class indicates the po-\nsition of the beat inside a bar. We consider a maximum\nbar length of 12 beats, and an extra class Xfor labeling an\nobservation in the absence of beats and downbeats, for a\ntotal of 13 classes (K= 13 ). For instance, to label a mu-\nsical piece with time signature 4=4, we use the subset of\nlabelsf1;2;3;4g, and we label consecutive time units cor-\nresponding to the same beat interval with the same class.\nFigure 1 illustrates the difference between the proposed\nand the unstructured encoding. In this strutured class lex-\nicon, the downbeats are represented by the label 1.\nWe train the networks incorporating both the unstruc-\ntured and the structured encoding. In this conﬁguration,\n2This estimation is on the 16th note level, which we assumed as a\ngood compromise to perform downbeat tracking.\nFigure 1 . Audio excerpt in 4=4labeled with the struc-\ntured encoding (top ﬁgure) and the unstructured encoding\n(bottom ﬁgure). The temporal granularity showed is tatum\n(beat quarter-notes). In the structured encoding each tatum\nreceives a label corresponding to its metrical position.\nwe use one dense layer to decode each class lexicon, and\nwe evaluate the performance of the system using the un-\nstructured output. The dense layers are connected so the\ninformation of the beginning of the bar is provided by the\nunstructured dense to the structured one as an extra fea-\nture. We test the effect of the encoding on the different\ntemporal granularities. It is important to note that the un-\nstructured coding has a clear interaction with the temporal\ngrid in terms of the number of 1- vs 0- symbols in the train-\ning data , while the structured coding is consistent (i.e., the\namount of class instances remains proportional) under any\ntemporal granularity.\n2.4 Post-processing: DBN vs thresholding\nThe importance of the post-processing stage has been ad-\ndressed in previous works [11,16]. In this paper, we assess\nthe relative importance of this stage depending on the tem-\nporal granularity and the network architecture. To that end,\nwe use the DBN presented in [16]. This DBN models beats\n(or tatums) as states, forcing the state sequence to always\ntransverse the bar from left to right (i.e., transitions from\nbeat 2 to beat 1 are not allowed), and imposing that time\nsignature changes are unlikely. We consider bar lengths of\n3 and 4 beats (12 and 16 tatums). We invite the interested\nreader to refer to [16] for further information.\n2.5 Architecture: RNNs vs CRNNs\nWe base our CRNN architecture design on previous state-\nof-the-art choices. Particularly, our CNN design is based\non the best CNN of the ensemble in [11], which we com-\nbine with a single Bi-GRU layer [8]. The bi-directional\nversion of GRUs integrates the information across both\ntemporal directions, providing temporal smoothing. CNNs\nare capable of extracting high level features that are in-\nvariant to both spectral and temporal dimensions, whether\nRNNs model longer term dependencies accurately.\nThe architecture that we propose can be seen as an\nencoder-decoder system [7], where the encoder maps the\ninput to an intermediate time series representation that is\nthen mapped to the output space by the decoder. An in-\nteresting advantage of this kind of scheme is that several\ncombinations of encoder-decoder can be explored easily\nas long as they share the intermediate representation.108 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 2 . Encoder architecture: the input representation\nis either a beat/tatum-synchronous chromagram or multi-\nband spectral ﬂux. Each time unit is fed into the encoder\nwith a context window. The CNN outputs a sequence of\ndimension T\u0002Nwhich is fed to the Bi-GRU. Finally, the\nencoder output dimension is T\u0002512.\nFigure 3 . Summary of the CNN architecture.\n2.5.1 Encoder architecture\nThe encoder architecture is depicted in Figure 2. It con-\nsists of a convolutional-recurrent network. Each temporal\nunit (either beat or tatum) is fed into the CNN considering\na ﬁxed-length context window of approximately one bar\n(following [11]). The CNN processes each window inde-\npendently, and outputs a sequence of size T\u0002N(Tbeing\nthe length of the input sequence and Nthe output dimen-\nsion of the CNN) that is fed to the Bi-GRU. In this scheme,\nthe CNN processes the signal locally whereas the recurrent\nnetwork provides temporal consistency.\nThe CNN architecture is based on the harmonic-CNN\npresented in [11]. This network consists of a cascade of\nconvolutional and max-pooling layers, with dropout used\nduring training to avoid over-ﬁtting, to a total of eight lay-\ners. We add batch normalization layers to avoid too large\nor small values within the network that could hurt the en-\ncoder. Additionally, we modify the ﬁlters’ size to adapt to\nthe feature shapes described in Section 2.1. Figure 3 shows\nthe ﬁlter parameters in the case of the tatum grid.\nThe last layer of the CNN differs from the reference\nimplementation in the number of units, which we set to 13\ninstead of 2 to ﬁt features of bigger dimension to the Bi-\nGRU. We also remove the softmax activation of the last\nlayer because the class discrimination is not carried out by\nthe CNN. A summary of the CNN architecture is presented\nin Figure 3. Figure 3 represents the CNN’s 2D ﬁlter sizes\nand the number of units, which is [m\u0002n; u], with mandn\noperating in the spectral and temporal dimensions respec-\ntively, and uthe number of units. The activation (if used) is\nindicated before the CNN description. Max pooling layers\nare notated as [m0\u0002n0]; sindicating frequency and time di-\nmension and stride. The interested reader is referred to [11]\nfor the motivation of network architecture.The local features computed by the CNN are fed into\na Bi-GRU, which consists of two independent GRUs, one\nrunning in each temporal direction. Their hidden state vec-\ntors are concatenated to produce the bi-directional hidden\nstate vector. We set the dimensionality of each GRU to\n256, resulting in a total of 512.\n2.5.2 Decoder architecture\nOur decoder architecture is a fully connected dense layer\nthat maps each hidden state vector to the prediction state\nusing a sigmoid activation, resulting in a downbeat like-\nlihood at each time unit. The optimization of the model\nparameters is carried out by minimizing the binary-cross-\nentropy among the estimated and actual values.\n3. EXPERIMENTS\n3.1 Experimental setup\nModel implementation: The models were implemented\nwith Keras 2.0.6 and TensorFlow 1.2.0 [1, 9]. We use the\nADAM optimizer [15] with default parameters. We stop\ntraining after 10 epochs without changes on the validation\nset, up to a maximum of 100 epochs. The low-level repre-\nsentations were extracted using the madmom library and\nmapped to either the beat or tatum grid (see Section 2.1).\nModel variations: We study the following variations:\nTemporal granularity : using low-level features synchro-\nnized to two temporal granularities (tatum and beat);\nOutput encoding : with and without the addition of the\nstructured encoding during training;\nPost-processing : using either a threshold or a DBN;\nArchitecture : we test RNNs and CRNNs.\nThis results in sixteen different conﬁgurations, which\nwe will refer to as and RorCto indicate the architec-\nture (RNN vs CRNN); SorUto indicate the encoding\n(structured vs unstructured); BorTto indicate temporal\ngranularity (beat vs tatum); and tanddto indicate the\npost-processing method (threshold vs DBN). All models\nare trained using patches of 15 beats or 60 tatums depend-\ning on the temporal grid used. We use mini-batches of 64\npatches per batch and a total of 100 batches per epoch.\nDatasets: We investigate the performance of these conﬁg-\nurations on 8 datasets of Western music, in particular:\nKlapuri which consists of 4h 54m of various genres songs.\nR. Williams which consists of 4h 31m of Pop songs.\nRock which consists of 12h 53m of Pop and Rock songs.\nRWC Pop which consists of 6h 47m of Pop music.\nBeatles which consists of 8h 01m of Beatles songs.\nBallroom which consists of 6h 04m of Ballroom dances.\nHainsworth which contains 3h 19m of various genres.\nRWC Jazz , which consists of 3h 44min of Jazz music.\nEvaluation methods: We perform leave-one-dataset-out\nevaluation and report the F-measure scores as in [11, 16].\n25% of the training data is used for validation. TheProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 109RWC Jazz dataset is only used to illustrate the performance\nof the systems in a challenging scenario where the beat es-\ntimation is less accurate and the music genre differs con-\nsiderably from the training data, it is not used for train-\ning.3Candidates for downbeats are obtained in two dif-\nferent manners. The ﬁrst one is by thresholding the out-\nput activations with a threshold chosen to give the best F-\nmeasure result on the validation set. The second manner\nis to post-process the networks’ outputs by adapting the\nDBN used in [16]. In this way we report the gain of us-\ning the DBN in each case. We use the DBN to model time\nsignatures 3/4 and 4/4 following [16], and modifying it ac-\ncordingly to the temporal grid (i.e., allowing bar lengths of\nf3,4gbeats orf12, 16gtatums).\nMethods are evaluated independently on each dataset\nlisted above for comparison to prior work. We also in-\nclude an evaluation over the union of all datasets (denoted\nALL). To determine statistically signiﬁcant differences, we\nconduct a Friedman test on the ALL-set results, followed\nby post-hoc Conover tests for pairwise differences using\nBonferroni-Holm correction for multiple testing [13].\nAll conﬁgurations are trained with the same input low-\nlevel representations, the same musical context, the same\ntraining parameters and post-processing method. This al-\nlows us to draw conclusions about the performance of the\nmodels in different conditions and to compare the architec-\ntures modularly.\n3.2 Results and discussion\nWe use as baseline two state-of-the-art downbeat tracking\nsystems [11, 16], which reported 78% and 78.6% mean\nF-measure across all datasets.4The performance of the\nmodels presented here across datasets is better than the\nbaselines for all the cases when using the DBN as post-\nprocessing stage. The better results are obtained with\nRUBd (reference implementation, see Section 2.1) and\nCUTd up to 82.4% and 82.8% respectively. A possible\nexplanation for this improvement is the difference in the\nbeat tracking performance, which is 3.3% better than the\none reported in [16]. This is likely to explain the 4.7%\nimprovement in the RUBd model which is our reference\nimplementation. To make a fair comparison, we use the\nRUBd model as a baseline, with the reasonable assumption\nthat it behaves as the state-of-the-art. Figure 4 illustrates\nthe performance of the different model variations across\ndatasets. A detailed analysis is presented in the following.\nThe Friedman test on the ALL set rejected the null\nhypothesis ( p < 1e\u000010). Post-hoc analysis determined\nthat all pair-wise comparisons were signiﬁcantly different\n(p < 1e\u00003), with the following exceptions: RUTt/RSTt,\nRUTd/RSTd, CUTt/CSBt, CUTd/CSBd, CUBd/CSTd,\nand RSBd/CUBd.\nEffect of post-processing: As shown in Figure 4, d vs t\nmodel variants, the DBN post-processing helps in all\n3We kept RWC Jazz out of the training set to be comparable to [16].\n4For datasets that are not evaluated in [11], we report results in [16].cases, being particularly important with the tatum granu-\nlarity and with the RNN models. The gap in performance\nbetween the models with and without post-processing is\nnotable in the case of the Ballroom database, where in\nsome cases is up to 10% F-measure. The DBN increases\nthe performance from RUTt to RUTd by 6.6% in the\ncase of tatum grid across all databases, and 4.1% in the\ncase of the beat grid (RUBt to RUBd). A similar trend is\nobserved with the structured models (RS). The increase\nin the CRNN models performance is smaller, being 3.8%\nfrom CUTt to CUTd and 2.7% for CUBt to CUBd. The\nresults obtained with the thresholding (t models) are more\nconsistent over temporal granularities for the CRNN\nmodels, which suggests that the likelihood estimation of\nthat model is more accurate and consistent over time.\nEffect of the temporal granularity: The temporal grid\nhas an important effect on the performance of the RNN\nmodels, as illustrated in Figure 4 (T vs B variations). When\nusing a tatum grid, the thresholding results (e.g. RUTt) are\nlower in most of the cases, showing that the RNN vari-\nations have more difﬁculty to model the temporal depen-\ndencies in that grid. The post-processing stage with the\nDBN becomes more important in the case of the tatum\ngrid, helping the RNN models up to an extra 2.5% in mean\nF-measure over all datasets compared to the beat grid case.\nBy contrast, the CRNN models appear to be robust to\nthe temporal granularity change. In particular, for the case\nof the thresholding results, the performance of the models\nis similar for the beat and tatum granularities (e.g. CUBt\nvs CUTt), which implies the estimated likelihoods perform\ncomparably. The increase in resolution seems to help the\nCRNN models in most cases, showing a small increase\nfrom beat to tatum grid with the DBN (e.g., CUBd vs\nCUTd). This indicates that the CRNN architecture is likely\nbeing able to take advantage of a ﬁner temporal grid.\nThe impact of the temporal granularity in the RNN and\nCRNN models are in line with the decisions of the authors\nin [11, 16], who in the ﬁrst case decided to use tatums\n(with CNNs), and in the second case decided to use beats\n(with Bi-GRUs).\nEffect of the structural encoding: Regarding the struc-\ntural encoding, the experiments show that it has no im-\npact on the performance across databases (e.g. RU vs RS\nand CU vs CS in Figure 4). We observed some examples\nwhere the performance decreases, and we noticed two sys-\ntematic problems: ﬁrst, in some cases the estimated likeli-\nhoods become sharper and more structured when using the\nencoding, and when the estimation of the networks is not\naccurate the likelihoods consistently maintain the bad esti-\nmation across the whole duration of the audio signal. This\nindicates that the encoding is structuring the likelihood es-\ntimation, but that is not desirable in some cases, especially\nif it prevents the post-processing stages from compensating\nfor these errors. Second, we observed several cases where\nthe attack of the downbeat is not accurately estimated with\nthe addition of the structured encoding. A possible expla-110 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018RUTt\nRUTd\nRUBt\nRUBd\nRSTt\nRSTd\nRSBt\nRSBd\nCUTt\nCUTd\nCUBt\nCUBd\nCSTt\nCSTd\nCSBt\nCSBd\nKlapuri\n Robbie Williams\n Rock\n RWC Pop\n0.60.70.80.9\nF-measureRUTt\nRUTd\nRUBt\nRUBd\nRSTt\nRSTd\nRSBt\nRSBd\nCUTt\nCUTd\nCUBt\nCUBd\nCSTt\nCSTd\nCSBt\nCSBd\nBeatles\n0.60.70.80.9\nF-measure\nBallroom\n0.60.70.80.9\nF-measure\nHainsworth\n0.60.70.80.9\nF-measure\nALLFigure 4 . For each dataset, the estimated mean F-measure for each model under comparison. Error bars correspond to 95%\nconﬁdence intervals under bootstrap sampling ( n= 1000 ).ALL corresponds to the union of all test collections.\nnation is the lack of information about the onset of the\nno-downbeat intervals in the structured encoding, which\ncould be preventing the system to correctly model beats\nand downbeats internally. This could change in a scenario\nwith joint beat and downbeat tracking, where the sparse\nencoding also contains the information of the location of\nbeats. The addition of data augmentation could also con-\ntribute to help the system to learn the encoding properly.\nCRNN vs RNN — difﬁcult scenario: Finally, to see\nthe performance of the systems in a difﬁcult scenario, we\nperformed an experiment on the RWC Jazz dataset, whose\nresults are given in Figure 5. The DBN post-processing is\nused in all cases. The CRNN models are more robust to un-\nseen data, since the jazz genre is different from the genres\nof the training data. The CRNN models have better per-\nformance and less dispersion in the results. Analogously\nto Figure 4, the RNN models show slightly better mean\nperformance in beat grid and the CRNN models in tatum\ngrid.\n4. CONCLUSIONS AND FUTURE WORK\nIn this work we presented a systematic study of com-\nmon decisions in the design of downbeat tracking systems\nbased on deep neural networks. We explored the impact of\ntemporal granularity, output encoding, and post-processing\nstage in two different architectures. The ﬁrst architecture\nis a state-of-the-art RNN, and the second is a CRNN in-\ntroduced in this paper. Experimental results show that\nthe choice of the inputs’ temporal granularity has a sig-\nniﬁcant impact on performance, and that the best conﬁg-\nuration depends on the architecture. The post-processing\nstage improves performance in all cases, with less impact\nin the case of the CRNN models whose likelihood esti-\nRUBd RUTd CUBd CUTd0.20.40.60.81.0F/uni00ADmeasure\nFigure 5 . F-measure scores for the RWC Jazz dataset.\nBoxes show median value and quartiles, whiskers the rest\nof the distribution. Black dots denote mean values. All\nresults are obtained using the DBN post-processing.\nmations are most accurate. We conclude that the addition\nof a densely structured output encoding does not help in\nthe training of downbeat tracking systems. Nevertheless,\nthe interaction of the structured encoding with multi-task\ntraining (beat and downbeat tracking) and data augmenta-\ntion are interesting perspectives for future studies, and will\nbe addressed in future work. The proposed CRNN archi-\ntecture performs as the state-of-the-art, proving robustness\nin a challenging scenario.\nAcknowledgments. The authors would like to thank Si-\nmon Durand and Florian Krebs for sharing the code of their\ndownbeat tracking architectures with us. B.M. is supported\nby the Moore-Sloan Data Science Environment at NYU.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 1115. REFERENCES\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo,\nZ. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,\nM. Devin, et al. Tensorﬂow: Large-scale machine\nlearning on heterogeneous distributed systems. arXiv\npreprint arXiv:1603.04467 , 2016.\n[2] S. Adavanne, P. Pertil, and T. Virtanen. Sound event de-\ntection using spatial features and convolutional recur-\nrent neural network. In 2017 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , 2017.\n[3] S. B ¨ock, F. Krebs, and G. Widmer. A multi-model ap-\nproach to beat tracking considering heterogeneous mu-\nsic styles. In 15th International Society for Music In-\nformation Retrieval Conference (ISMIR) , 2014.\n[4] S. B ¨ock, F. Krebs, and G. Widmer. Joint beat and\ndownbeat tracking with recurrent neural networks. In\n17th International Society for Music Information Re-\ntrieval Conference (ISMIR) , 2016.\n[5] Sebastian B ¨ock, Filip Korzeniowski, Jan Schl ¨uter, Flo-\nrian Krebs, and Gerhard Widmer. madmom: a new\npython audio and music signal processing library. In\nProceedings of the 24th ACM International Conference\non Multimedia (ACMMM , 2016.\n[6] E. C ¸ akr, G. Parascandolo, T. Heittola, H. Huttunen, and\nT. Virtanen. Convolutional recurrent neural networks\nfor polyphonic sound event detection. IEEE/ACM\nTransactions on Audio, Speech, and Language Pro-\ncessing , 25(6):1291–1303, June 2017.\n[7] K. Cho, A. Courville, and Y . Bengio. Describing\nmultimedia content using attention-based encoder-\ndecoder networks. IEEE Transactions on Multimedia ,\n17(11):1875–1886, Nov 2015.\n[8] K. Cho, B. Van Merri ¨enboer, C. Gulcehre, D. Bah-\ndanau, F. Bougares, H. Schwenk, and Y . Bengio.\nLearning phrase representations using rnn encoder-\ndecoder for statistical machine translation. arXiv\npreprint arXiv:1406.1078 , 2014.\n[9] F. Chollet. Keras. https://github.com/\nfchollet/keras , 2015.\n[10] S. Durand, J. P. Bello, B. David, and G. Richard.\nDownbeat tracking with multiple features and deep\nneural networks. In 2015 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , 2015.\n[11] S. Durand, J. P. Bello, B. David, and G. Richard. Ro-\nbust downbeat tracking using an ensemble of convo-\nlutional networks. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing , 25(1):76–89, Jan\n2017.[12] S. Durand and S. Essid. Downbeat detection with con-\nditional random ﬁelds and deep learned features. In\n17th International Society for Music Information Re-\ntrieval Conference (ISMIR) , 2016.\n[13] S. Garcia and F. Herrera. An extension on“statistical\ncomparisons of classiﬁers over multiple data sets” for\nall pairwise comparisons. Journal of Machine Learn-\ning Research , 9:2677–2694, Dec 2008.\n[14] S. Ioffe and C. Szegedy. Batch normalization: accel-\nerating deep network training by reducing internal co-\nvariate shift. In 32nd International Conference on Ma-\nchine Learning (ICML) , 2015.\n[15] D. Kingma and J. Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980 , 2014.\n[16] F. Krebs, S. B ¨ock, M. Dorfer, and G. Widmer. Down-\nbeat tracking using beat synchronous features with re-\ncurrent neural networks. In 17th International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2016.\n[17] F. Krebs, S. B ¨ock, and G. Widmer. An efﬁcient state\nspace model for joint tempo and meter tracking. In 16th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2011.\n[18] N. C. Maddage. Automatic structure detection for pop-\nular music. IEEE MultiMedia , 13(1):65–77, Jan 2006.\n[19] M. Mauch and S. Dixon. Simultaneous estimation of\nchords and musical context from audio. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n18(6):1280–1289, aug 2010.\n[20] B. McFee and J.P. Bello. Structured training for large-\nvocabulary chord recognition. In 18th International\nSociety for Music Information Retrieval Conference\n(ISMIR) , 2017.\n[21] M. M ¨uller and S. Ewert. Chroma toolbox: Matlab im-\nplementations for extracting variants of chroma-based\naudio features. In 12th International Society for Music\nInformation Retrieval Conference (ISMIR) , 2011.\n[22] J. Paulus and A. Klapuri. Measuring the similarity of\nrhythmic patterns. In Michael Fingerhut, editor, Proc.\nof the Third International Conference on Music Infor-\nmation Retrieval (ISMIR) , 2002.\n[23] R. V ogl, M. Dorfer, G. Widmer, and P. Knees. Drum\ntranscription via joint beat and drum modeling using\nconvolutional recurrent neural networks. In 18th Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR) , 2017.112 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "A Confidence Measure For Key Labelling.",
        "author": [
            "Roman B. Gebhardt",
            "Michael Stein",
            "Athanasios Lykartsis"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492333",
        "url": "https://doi.org/10.5281/zenodo.1492333",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/72_Paper.pdf",
        "abstract": "We present a new measure for automatically estimating the confidence of musical key classification. Our approach leverages the degree of harmonic information held within a musical audio signal (its \"keyness\") as well as the steadiness of local key detections across the its duration (its \"stability\"). Using this confidence measure, musical tracks which are likely to be misclassified, i.e. those with low confidence, can then be handled differently from those analysed by standard, fully automatic key detection methods. By means of a listening test, we demonstrate that our developed features significantly correlate with listeners' ratings of harmonic complexity, steadiness and the uniqueness of key. Furthermore, we demonstrate that tracks which are incorrectly labelled using an existing key detection system obtain low confidence values. Finally, we introduce a new method called \"root note heuristics\" for the special treatment of tracks with low confidence. We show that by applying these root note heuristics, key detection results can be improved for minimalistic music.",
        "zenodo_id": 1492333,
        "dblp_key": "conf/ismir/GebhardtSL18",
        "keywords": [
            "confidence",
            "musical key classification",
            "harmonic information",
            "local key detections",
            "stability",
            "keyness",
            "harmonic complexity",
            "steadiness",
            "root note heuristics",
            "minimalistic music"
        ],
        "content": "A CONFIDENCE MEASURE FOR KEY LABELLING\nRoman B. Gebhardt\nAudio Communication Group,\nTU Berlin\nr.gebhardt@campus.\ntu-berlin.deAthanasios Lykartsis\nAudio Communication Group,\nTU Berlin\nathanasios.lykartsis@\ntu-berlin.deMichael Stein\nNative Instruments GmbH\nmichael.stein@\nnative-instruments.de\nABSTRACT\nWe present a new measure for automatically estimat-\ning the conﬁdence of musical key classiﬁcation. Our ap-\nproach leverages the degree of harmonic information held\nwithin a musical audio signal (its “keyness”) as well as the\nsteadiness of local key detections across the its duration\n(its “stability”). Using this conﬁdence measure, musical\ntracks which are likely to be misclassiﬁed, i.e. those with\nlow conﬁdence, can then be handled differently from those\nanalysed by standard, fully automatic key detection meth-\nods. By means of a listening test, we demonstrate that our\ndeveloped features signiﬁcantly correlate with listeners’\nratings of harmonic complexity, steadiness and the unique-\nness of key. Furthermore, we demonstrate that tracks\nwhich are incorrectly labelled using an existing key detec-\ntion system obtain low conﬁdence values. Finally, we in-\ntroduce a new method called “root note heuristics” for the\nspecial treatment of tracks with low conﬁdence. We show\nthat by applying these root note heuristics, key detection\nresults can be improved for minimalistic music.\n1. INTRODUCTION\nA major commercial use case of musical key detection is\nits application in DJ software programs including Native\nInstruments’ Traktor1and Pioneer’s rekordbox2. It rep-\nresents the basis for harmonic music mixing [9], a DJing\ntechnique which is mostly bounded to electronic dance\nmusic (EDM). However, the concept of musical key is\nnot universally applicable to all styles of music, especially\nthose of a minimalistic nature, which is often the case in\n(EDM) [7, 10, 21]. A particular challenge of key detec-\ntion in EDM is that the music often does not follow clas-\nsic Western music standards in terms of its harmonic com-\nposition and progression. This applies to a broad range\nof contemporary EDM music which can be composed in\n1https://www.native-instruments.com/de/\nproducts/traktor/\n2https://rekordbox.com/de/\nc\rRoman B. Gebhardt, Athanasios Lykartsis, Michael\nStein. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Roman B. Gebhardt, Athanasios\nLykartsis, Michael Stein. “A Conﬁdence Measure For Key Labelling”,\n19th International Society for Music Information Retrieval Conference,\nParis, France, 2018.a chromatic space or, if following classic characteristics,\nuses more “exotic” modes such as e.g. Phrygian [19],\nwhich is actually predominant for certain genres such as\nAcid House, Electronic Body Music (EBM) and New Beat,\nwhich, since the 1980s represent a prominent source of in-\nspiration for contemporary EDM. A further difﬁculty is the\ntendency of certain electronic music to be strongly percus-\nsive and very minimalistic in terms of its harmonic content\n[5]. In fact, following pioneering groups like Kraftwerk,\nmelodic minimalism is a main characteristic of techno mu-\nsic [13]. Today, a wide range of EDM productions are\nexclusively percussion-based. The lack of harmonic in-\nformation clearly leads to problems in assigning an unam-\nbiguous key label, which is still the most widely used way\nto describe a track in its harmonic composition [21].\nIn the recent years, conﬁdence measures have gained\ninterested in the ﬁeld of MIR, namely related to tempo es-\ntimation [8,17]. The described scenario motivates to estab-\nlish such measure for key detection tasks. Crucial factors\nto consider are the degree to which a musical audio signal\nconforms to the concept of musical key, and furthermore to\nexplore where a single key persists throughout a recording.\nBeing able to capture this information automatically could\ntherefore serve as an indicator to predict potential misclas-\nsiﬁcations. It may also be used to deﬁne a threshold to\ndecide whether to label a track with a key or alternatively\nsimply with a root note [10], within a genre-speciﬁc frame-\nwork [21] or in spatial coordinates [2,3,12]. Alternatively,\nmultiple key labels could be assigned for tracks contain-\ning key changes [16]. We collate this information to derive\na key detection conﬁdence measure and present an alter-\nnative means for handling music where a traditional key\nassignment is not be possible. The remainder of this paper\nis structured as follows: in Section 2, we present the de-\nvelopment of the conﬁdence features as well as a special\nkey detection method for tracks of a minimalistic nature.\nSection 3 outlines our evaluation of the developed features\nand the special treatment of low conﬁdence scoring tracks.\nFinally, we conclude our work and provide an outlook for\nfuture work in Section 4.\n2. METHOD\nTo establish the conﬁdence measure, we follow two hy-\npotheses and for each we develop a feature: First, there\nmust be sufﬁcient harmonic information within the signal3to reliably determine a key, i.e., it would be inappropriate\nto label a track consisting exclusively of percussive con-\ntent with a meaningful key. Consequently, we denote our\nﬁrst conﬁdence feature as keyness to indicate the amount\nof harmonic content within a musical piece. Second, we\nstate that any local key changes throughout the duration\nof a track will inevitably lead to a discrepancy between a\ngiven global label and at least some regions. Our second\nconﬁdence feature, which measures the steadiness of key\ninformation, will be referred to as stability . The develop-\nment of both features is discussed in the following subsec-\ntions.\n2.1 Keyness\nVarious approaches have been taken to the problem of as-\nsigning a musical key designation based on the information\nretrieved from an audio signal. A straightforward method\nwould be to follow the well-known key template approach\nintroduced by Krumhansl et al. [15], where the correlation\nof an input signal’s chroma distribution with the chosen\nkey’s template could be used as a keyness measure. Of-\nten, these templates are not needed, for instance when the\nkey detection is handled within a tonal space model like\nChew’s Spiral Array [3] or Harte et al.’s Tonal Centroid\nSpace [12]. To avoid the necessity of computing the corre-\nlations and to keep our approach most simple, we bypass\nthis option and retrieve keyness information directly from\nthe chromagram. For this, we use a chromagram represen-\ntation which empahsizes tonal content, based on a percep-\ntually inspired ﬁltering process in [10]. This procedure re-\nmoves energy in the chromagram evoked by noisy and/or\npercussive sounds, which are especially present in EDM.\nWe then apply Chuan et al.’s fuzzy analysis technique [4]\nto further “clean” the chromagram. Figure 1 shows the\nresulting chromagram of an EDM track3with a tempo-\nral resolution of 250 ms and below it, the curve resulting\nfrom the sum of the frame-wise individual chroma energies\nE(c;t)ranging from 0 to 1 for each chroma cat time-frame\nt:\nEc(t) =12X\nc=1E(c;t): (1)\nWe denoteEc(t), the chroma energy . By inspection of\nthe resulting curve, a raw subdivision of the track into\nthree partly recurring harmonic structures can be observed:\nThe ﬁrst with a chroma energy equal (or close to) zero is\npresent in the purely percussive regions which accord to\nour represent regions of lowkeyness. The second structure\ndescribes the G# power chord (where G# is the root and\nD# the ﬁfth), which reaches chroma energy values of 1 to\napproximately 1.75 for Ec(t). The power chord is widely\nused in EDM productions and is ambiguous in terms of\nthe mode of its tonic’s key due to the third missing. Fi-\nnally, the third structure in the middle of the track holds a\n3Praise You 2009 (Fatboy Slim vs. Fedde Le\nGrand Dub): https://www.discogs.com/de/\nFatboy-Slim-vs-Fedde-Le-Grand-Praise-You-2009/\nrelease/1967533\nchromagram\n50 100 150 200 250 300 350CC#/DbDD#/EbEFF#/GbGG#/AbAA#/BbBpitch class\n50 100 150 200 250 300 350\ntime in seconds024chroma energychroma energy curve / local keyness valueFigure 1 . Chromagram (upper plot) and local keyness\ncurve (lower plot) of an EDM track derived from the\nframewise energies of the chromagram.\nchroma energy level of approximately 4 which far exceeds\nthe other regions. In fact, it is the only region that contains\na sufﬁcient number of notes present to use as the basis for\ndetecting the key. As this representative example demon-\nstrates, the straightforward calculation of chroma energy\ncan be informative about how much harmonic information\nis contained in a musical audio signal.\nTo obtain a global keyness measure, we average the\nchroma energy vector Ec(t)over the full duration Tof the\ntrack and obtain the keyness value, K:\nK=1\nT\u0001TX\nt=0Ec(t): (2)\n2.2 Stability\nThe second conﬁdence feature, stability, is derived from\nthe steadiness of key classiﬁcations throughout the full\nduration of the track. For this purpose, we take into ac-\ncount the vector of local key detections using a template-\nbased approach on temporal frames with 250 ms length\nand125 ms hop-size. In DJ software which was the\nframework of our research, the 24 key classes are usu-\nally displayed in the 12-dimensional subspace of so-called\n“Camelot numbers” [6] each of which corresponds to a cer-\ntain “hour” on the circle of ﬁfths. This implies that a major\nkey and its relative minor are considered equivalent. The\nmiddle plot of Figure 2 shows the progression of Camelot\nclassiﬁcations over time. It is important to note that both\nthe vertical axis of the middle plot and the horizontal axis\nof the lower histogram plot are circular i.e. the chroma has\nbeen “wrapped”. In our example, the most frequently de-\ntected Camelot number is 1 (B/G# m) which is followed\nby its direct neighbour one ﬁfth above, 2 (F# / Ebm). The\nright tail of the distribution fades out with small counts\nfor numbers 3 (Db/Bbm) and 4 (Ab/Fm), whereas the left\ntail’s only present value is 11 (A/F# m). For a high de-\ngree of stability, we would expect a low angular spread of\ncamelot detections throughout, which we compute in terms\nof the circular variance V(cam)of the distribution accord-\ning to [1] . In terms of a numeric measure for the stability4 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018C#\nF#\nF#\nG#\nFigure 2 . Local camelot decisions (middle plot) and his-\ntogram of absolute camelot counts (lower plot). The num-\nber describes the “hour” on the circle of ﬁfths.\nof the whole track, we deﬁne the conﬁdence feature of sta-\nbility,S, as:\nS= 1\u0000V(cam); (3)\nwithV(cam)depicting the circular variance of the camelot\nvector. Thus, the stability of a track will be 0 for a uniform\nhistogram and 1 for maximum stability (where only one\ncamelot number is detected throughout). In more com-\nplex compositions in classical music, we can expect key\nchanges throughout musical pieces. However, these key\nchanges are usually small moves on the circle of ﬁfths and\nconsequently small steps on the Camelot wheel (e.g. just\none “hour” for a ﬁfth). When using the circular histogram,\nthese key changes would not have a strong impact on the\nvariance of the distribution and would therefore exert only\na small inﬂuence on the stability feature. In the special\ncase of pop or EDM, key modulation is mostly absent [7].\n2.3 An Overall Conﬁdence Feature\nIn the two previous subsections, we discussed the develop-\nment of two features to measure the keyness andstability of\nmusical audio, both representing independent approaches\nto ﬁnd a quantitative measure for the overall conﬁdence\nof a key detection. As discussed, the two features focus\non different characteristics of the music signal. While the\nkeyness measure describes the amount of harmonic infor-\nmation held by a track, the stability feature focusses on the\nsteadiness of key detections throughout a whole track. Col-\nlectively these features will penalise the presence of key\nchanges within a track as well as “random” labels from\na key classiﬁcation system caused by harmonic structures\nwhich don’t conform to the classic major / minor distri-\nbution. We state that, for a “trustworthy”’ key detectionwhich is informative for harmonic mixing, a given track\nshould score high for both of these features. Thus, we de-\nﬁne an overall conﬁdence feature as the linear combina-\ntion,C, of the subfeatures KandSwith variable weight-\ning parameters \u0014and\u001b. We quantise KandSand dis-\ncretise them individually to evenly distributed percentiles,\nresulting inCkforKandCsforS. As a result, the lowest\npercentile of 1 comprises tracks scoring lower in K(orS\nrespectively) than 99% of the database which is discussed\nin Section 3.2. This is done to ensure an even distribution\nof the subfeature values over all tracks as well as to map\nboth to a range from 1 to 100:\nC=\u0014\u0001Ck+\u001b\u0001Cs\n\u0014+\u001b(4)\nWe consider the choice of \u0014and\u001bto be genre-\ndependent. For minimalistic music such as EDM, where\nwe do not expect highly complex harmonic structure or key\nchanges that would eventually lead to a low score for Cs,\nwe believe greater emphasis should be given to Ckto ﬁlter\ne.g. purely percussive tracks. However, for the analysis\nof classical music, more importance should be attributed\nto the stability feature Cs. Here, we should not expect a\nlack of harmonic information, but frequent and “far” key\nchanges would lead to less clarity about the key the piece\nis composed in. In this paper, we set the values of \u0014= 5\nand\u001b= 2for the evaluation of a database mainly contain-\ning EDM tracks, however we intend to explore the effect\nof modifying these values and genre-speciﬁc parameteri-\nsations in future work.\n2.4 Root Note Heuristics\nWith the proposed conﬁdence feature, C, it is possible to\ndetermine a threshold below which a key detection should\nnot be considered reliable. This raises an important ques-\ntion of how to treat problematic (i.e. low conﬁdence) tracks\nin terms of assigning a key label. One option could be the\nuse of multiple key labels for tracks with low stability [16]\nor to use root note labelling for tracks with low keyness\n[10]. Alternatively, for EDM, minimalistic tracks could be\nlabelled as the root note’s minor key due to the strong bias\ntowards minor mode in this genre [7,14]. We call this pro-\ncedure “root note heuristics” and apply it to tracks whose\nkeyness falls below a certain threshold. For the case of\nroot note detection, we ﬁrst accumulate the chroma ener-\ngiesE(c;t)over time to obtain a global chroma energy\nvectorE(c):\nE(c) =TX\nt=0E(c;t): (5)\nTo detect the most predominant chroma, and hence root\nnote, we apply a simple binary template T(c)in which\nthe referenced chroma and its dominant are given an equal\nweight of 1, with all pitch classes set to 0. Consideration\nof the ﬁfth interval is made to explicitly take power chords\ninto account and allow them to point towards their root. We\nshift this template circularly by one step for each chroma\nvalue accordingly and calculate the inner product per shift.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 5This results in the likelihood R(c)of the chroma cto be\nthe root of the track:\nR(c) =<T(c);E(c)> (6)\nFinally, the minor mode of the chroma with the highest\nvalue ofR(c)is assigned to the track as a whole.\n3. EV ALUATION\nFor an extensive analysis of our developed conﬁdence fea-\ntures, we undertook two separate evaluation procedures.\nFirst, to examine the validity of our subfeatures keyness\nandstability , we conducted a listening test where we asked\nparticipants to rate a set of musical audio examples accord-\ning to three questions concerning their harmonic content.\nSecond, we evaluated the degree to which the calculated\nconﬁdence score for each single track would be associated\nwith a given genre label and whether it was detected cor-\nrectly by a key detection system and - if not - whether\nthe error was close to the ground truth key label or not.\nHence, we would then be able to use the conﬁdence score\nas a prediction measure for the potential rejection of a key\ndecision and eventually the special treatment of the cor-\nresponding tracks. Both approaches are discussed in the\nfollowing subsections.\n3.1 Listening Test for Subfeature Evaluation\nThe listening experiment was performed as an online\nsurvey, in which we presented 12 different representative\nexcerpts4of length 120 s which we considered sufﬁcient\nto allow the perception of any potential key changes.\nThese 12 excerpts could be characterised by the following\nfour properties A - D:\nA: Clear and unique key throughout (Track IDs 1, 8, 12)\nB: Change in key structure (Track IDs 2, 7, 10)\nC: Non-Western melodic content (Track IDs 3, 4, 6)\nD: No or little melodic content (Track IDs 5, 9, 11)\nAfter listening to the audio samples, participants were\nasked to rate them on a 10-point Likert scale in terms of\ntheir harmonic complexity, i.e. whether the tracks fol-\nlowed the major/minor scheme and how clearly they ad-\nhered to one unique key throughout. In order to prevent\nany bias in the participant ratings, no information about the\ndeveloped features was provided. However, a short train-\ning phase was set up before the test to ensure participants\nunderstood the questions they were going to be asked. In\ntotal, we recruited 29 participants (22 male, 7 female) who\nself-reported as musically trained. The participants’ ages\nranged from 23 to 66 with an average of 10 years of mu-\nsical training. In the following sections, the relatedness of\nthe ratings with the computed subfeatures Ck,Csas well\nas the overall conﬁdence Cwill be discussed.\n4A link to the examples will be provided in the camera ready copy.3.1.1 Keyness\nTo assess the subfeature of keyness, we asked participants\nto rate the audio excerpts according to two questions. With\nthe ﬁrst, we aimed to test if the concept of the keyness fea-\nture as a general measure for tonal density or complexity\n(not necessarily relating to a key) would prove appropriate:\nQ1: “To which degree do you ﬁnd the presented audio\nharmonically complex?”\nWe hypothesised a positive correlation between the ratings\nand the computed values of Ck, however we made no\nassumption about the coherence of the ratings with Cs\nas harmonically complex excerpts could still be unstable\nin harmony or key. The mean ratings as well as the\ncorresponding feature values C,CkandCsare displayed\nin the leftmost column of Figure 3. For a measure of\nrelatedness, we calculated Spearman’s rho correlation\nmeasure for the ratings’ means across participants and\nthe feature values. With a choice of \u000b= 0:05as the\nlevel of signiﬁcance, the observed strong positive corre-\nlation ( rs= 0:63; p < 0:05) between the ratings and\ncomputed values for the keyness feature Cksupports\nour initial hypothesis. However, some outliers can be\nidentiﬁed, for which the formulation of the question\nmight have been misleading: Excerpt 7 (second rated\nfrom category B) exhibits strong break beat percussion\nand a rather chaotic melodic progression with a short\nminor mode piano passage, which would contribute to\na low score for Ck. Feedback from some participants\nrevealed the excerpt was considered as rather challenging,\nwhich caused it to be rated high in terms of complexity.\nExcerpt 9 (the highest rated excerpt from category D)\nis also mostly percussive with pitched voice samples\nand sounds. Its relatively unusual composition might\nalso have caused some participants to rate it “complex”.\nThe excerpts from category A consist of quite common,\nrepetetive chord structures which therefore may not have\nbeen perceived as particularly complex in a musical sense.\nHowever, they all feature a high amount of harmonic\ncontent, and therefore represent “complex” musical\nexcerpts in line with our keyness deﬁnition. As discussed\nin 2.1, the keyness feature is derived from the average\namount of tonal information throughout the analysed\nsignal. We argued that in the case of Western music,\na high amount of tonal information usually indicates\nthe presence of a major or minor scheme as harmonic\nlayerings of notes deviating from Western scales rarely\nappear [20] and thus a higher density of tonal information\nshould point towards the clear presence of a musical\nkey. To examine the validity of this assumption, the\nsecond question of the listening test focussed on whether\nthe keyness feature could in fact be used as an indicator\nfor the presence of a major/minor scheme within the audio:\nQ2: “To which degree does the presented audio ﬁt the\nmajor/minor scheme?”6 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20180246810050100Ckr = 0.63\np = 0.03A\nB\nC\nD\nlin. regr.\n0246810050100Csr = 0.00\np = 0.99\n0246810\nratings Q1050100Cr = 0.31\np = 0.320 2 4 6 810050100\nr = 0.90\np = 0.00\n0 2 4 6 810050100\nr = 0.45\np = 0.14\n0246810\nratings Q2050100\nr = 0.75\np = 0.010 2 4 6 810050100\nr = 0.77\np = 0.00\n0 2 4 6 810050100\nr = 0.81\np = 0.00\n0246810\nratings Q3050100\nr = 0.84\np = 0.00Figure 3 . Mean ratings on the questions Q1,Q2andQ3for the 12 stimuli and their corresponding feature values C,Ck\nandCswith the respective Spearman correlation coefﬁcients r.\nAgain, we hypothesised a positive correlation between the\nratings and Ck, but again, not Cs. The results are pre-\nsented in the subplots in the middle column of Figure 3.\nOur hypothesis regarding Ckwas supported with a very\nstrong positive correlation of rs= 0:90; p < 0:01. Re-\nmarkably, it even exceeds the correlation of the stronger\nhypothesis we explored in Q1regarding its relatedness to\nthe complexity ratings, as discussed in 3.1.1: Of the four\noutliers discussed above, namely one excerpt from cate-\ngory D and all the excerpts from category A, all agree\nmuch more strongly with the Ckvalue. As with Q1, no\nsigniﬁcant correlation between the ratings and the values\nofCswas observed.\n3.1.2 Stability\nTo evaluate the stability subfeature Cs, participants were\nasked to rate the stimuli according to the question:\nQ3: “To which certainty does the audio correspond to one\nunique and distinct key?”\nWe expected the ratings for Q3to be correlated with the\ncomputed values of Cs, as key changes should results in\nlower the ratings and stability. In addition, we also hy-\npothesised a positive correlation to Ckas a lack of har-\nmonic information could complicate a clear assignment\nto one unique key. The subplots in the rightmost col-\numn of Figure 3 show the outcomes of the third ques-\ntion. As can be seen, both subfeatures exhibit a signiﬁ-\ncant correlation with the mean ratings. While Ckshows\na strong positive correlation with a Spearman coefﬁcient\nofrs= 0:77; p < 0:01, the correlation of Csis even\nstronger ( rs= 0:81; p < 0:01). The combination of\nboth in the overall conﬁdence feature Cresults in an even\nhigher correlation rs= 0:84; p < 0:01, which fortiﬁes\nour choice to combine both features in order to explain the\ncertainty of a unique key decision and therefore the conﬁ-\ndence of a key assignment.3.2 Evaluation on an annotated dataset\nIn the second part of our evaluation progress, we tested\nhow the computed conﬁdence scores relate to genre la-\nbels and whether a track’s key classiﬁcation was correct\nor not. We based our analysis on a private commercial\ndatabase comprised of 834 tracks consisting mainly of\nEDM (697 total) as well as 137 tracks from Harte’s [11]\nBeatles dataset with key labels forming the ground truth.\nA subset of 101 of the EDM tracks were labelled “Inhar-\nmonic” and represented tracks that were considered am-\nbiguous or unclassiﬁable by musical experts.\n3.2.1 Genre Speciﬁc Differences\nFor a ﬁrst observation, we compare the means of Ckand\nCsfor the three different subsets, namely the Beatles, the\n“Inharmonic” labelled EDM subset, and the remainder of\nthe EDM tracks. According to our model, Ckshould\nbe high for the Beatles dataset, since it contains mostly\nmelodic music. However, we should expect lower values\nfor the EDM set, following the hypothesis that EDM is of-\nten of a more minimalistic melodic nature. For the subset\nof EDM tracks labelled “Inharmonic” we shouldn’t expect\nmuch harmonic information, and hence lows score for Ck.\nAlternatively, a lack of clarity about the label might occur\ndue to the use of a non-Western scale, and would there-\nfore result in a low value for Cs. We hypothesised Csto\nreach higher scores for the remaining EDM tracks as we\nexpected a more stable melodic structure for these than the\nBeatles tracks which inherit a number of key changes and\nsometimes unconventional harmonic content. The results\nin table 1 show that our expectations are conﬁrmed. The\n“Inharmonic” subset scores substantially lower in all (sub)-\nfeatures, while the Beatles dataset scores high in keyness\nwhereas the remainder of the EDM dataset achieves high\nvalues in stability.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 7Subset CkCsC\nEDM 49.4 55.8 51.2\nEDM Inharmonic 14.3 30.6 19.0\nBeatles 82.0 42.1 70.6\nTable 1 . Conﬁdence score means for the different subsets,\nin the range 1 - 100.\n3.2.2 Prediction of Misclassiﬁcation\nWe aimed to assess whether the the conﬁdence feature\nwould be an appropriate indicator of the degree to which\nan automatic key detection could be considered trustwor-\nthy, primilary for the application of harmonic mixing. To\nprovide automatic estimates of musical key, we used a\nkey-template based system built into a state-of-the-art DJ\nsoftware, which was modiﬁed by incorporating the pre-\nprocessing stage as proposed in [10]. Given our equali-\nsation of relative keys to equal Camelot numbers as dis-\ncussed in 3.1.2, we deﬁned three different labelling cat-\negories: Match for key detections matching the ground\ntruth label, Fifth for ﬁfth related errors and thus, one\nCamelot number away from the ground truth and Other\nfor detections greater than one Camelot number apart.\nAcross the 834 tracks, we counted 627 Matches , 117\nFifths and 90 Others . Three hypotheses were put for-\nward: We expected tracks for which our key detection\nresult matched the ground truth to score higher in con-\nﬁdence than those from both other categories. We were\nless sure about the tracks from the Fifth category, but in-\ntuitively expected them to score higher than those from\nOther . Figure 4 shows the distributions of the conﬁdence\nscoresCwithin the three groups. We performed a Welch-\nANOV A which supported this hypothesis with high signif-\nicance, F(2;170:41) = 64 :16; p < : 001. To test the\nmean differences between the three groups, we conducted\na Games-Howell post-hoc analysis which showed signiﬁ-\ncant differences between all three pairs for \u000b= 0:01.\nmatch\n0102030405060708090100050100counts\nfifth\n010203040506070809010001020counts\nother\n0102030405060708090100\nconfidence score C01020counts\nFigure 4 . Distributions of the conﬁdence scores Cwithin\nthe three different labelling categories.\n3.2.3 Root Note Heuristics\nFinally, we evaluated the special treatment of the “root note\nheuristics” introduced in 2.4. For this, we took into consid-\neration the counts of the three labelling categories betweenSubset Match Fifth Other\nEDM 469 / 468 86 /85 41 /43\nEDM Inharmonic 50 / 59 17 /17 34 /25\nBeatles 108 / 108 14 /14 15 /15\nTable 2 . Counts of labelling categories for the three sub-\nsets without / with the application of the root note heuris-\ntics method.\nthe different subsets. As a preliminary investigation, we\napplied the heuristics to the lowest sixth quantile scoring\ntracks. The resulting absolute counts for the labelling cate-\ngories are shown in Table 2. While the Beatles and normal\nEDM subsets are barely affected, a clear improvement is\nachieved within the “Inharmonic” subset. Using the root\nnote heurestics, the number of correctly detected tracks\ncould be increased by 18%. Furthermore, we were able\nto reduce the number of Other classiﬁed errors by 26%.\n4. CONCLUSIONS\nIn this paper, we described the development of a conﬁ-\ndence feature for key labelling, as a means to measure the\nlikelihood of an automatic key classiﬁcation being correct.\nFor this, we developed two subfeatures, keyness and stabil-\nity, to estimate the amount of tonal content of musical au-\ndio as well as the steadiness of key detections throughout\nthe full duration of the track respectively. Both subfeatures\nwere evaluated by means of a listening test. Our analysis\ndemonstrated high correlations for harmonic complexity,\naccordance to the major/minor scheme and the uniqueness\nof one key between the participants’ ratings and the de-\nveloped features. Furthermore, we showed that our con-\nﬁdence feature can be helpful indicator of cases where an\nautomatic estimated key label can be trusted. Our conﬁ-\ndence measure may also be used as a threshold to switch\nbetween different key detection approaches. To this end,\nwe introduced a root note heuristics method that can be\nused as a special key detection approach for tracks of har-\nmonically minimalistic nature, and we showed that the ap-\nplication of this procedure could positively affect key de-\ntection performance. However, the presented root note\nheuristics approach is still at an early stage of develop-\nment, therefore these promising results motivate continued\nresearch towards adjusting the threshold and further devel-\nopment of alternative key detection methods. This work\nhas mostly been focussed on EDM. A major area of future\nwork would therefore be to generalise the key conﬁdence\nconcept for other genres, where it would be neccessary to\nalso take into account relative errors instead of considering\nonly in the Camelot subspace. Also, other possible ways\nto use the developed features can be considered: Since the\nkeyness feature is sequentially analysed over time, this al-\nlows inference about individual segments of a track. In\nthe context of harmonic mixing, this information could be\nextremely useful by allowing a DJ to locate appropriate re-\ngions for executing the transition between two tracks, thus\navoiding harmonic clashes [9, 18].8 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20185. REFERENCES\n[1] P. Berens. Circstat: A MATLAB toolbox for circular\nstatistics. Journal of Statistical Software , 31(10):1–21,\n2009.\n[2] G. Bernandes, D. Cocharro, M. Caetano, and M.E.P.\nDavies. A multi-level tonal interval space for mod-\nelling pitch relatedness and musical consonance. Jour-\nnal of New Music Research , 45(4):281–294, 2016.\n[3] E. Chew. Towards a mathematical model of Tonality .\nPh.D. thesis, MIT, Cambridge, MA, 2000.\n[4] C.-H. Chuan and E. Chew. Fuzzy analysis in pitch\nclass determination for polyphonic audio key ﬁnding.\nInProc. of the 6th International Society for Music In-\nformation Retrieval (ISMIR 2005) Conference , pages\n296–303, 2005.\n[5] G. Dayal and E. Ferrigno. Electronic Dance Music .\nGrove Music Online, Oxford University Press, 2012.\n[6]´A Faraldo. Tonality Estimation in Electronic Dance\nMusic . Ph.D. thesis, UPF, Barcelona, 2017.\n[7]´A Faraldo, E. G ´omez, S. Jord `a, and P. Herrera. Key\nestimation in electronic dance music. In Proc. of the\n38th European Conference on Information Retrieval ,\npages 335–347, 2016.\n[8] F. Font and X. Serra. Tempo estimation for music loops\nand a simple conﬁdence measure. In Proc. of the 17th\nInternational Society for Music Information Retrieval\nConference (ISMIR 2016) , pages 269–275, 2016.\n[9] R.B. Gebhardt, M.E.P. Davies, and B.U. Seeber. Psy-\nchoacoustic approaches for harmonic music mixing.\nApplied Sciences , 6(5):123, 2016.\n[10] R.B. Gebhardt and J. Margraf. Applying psychoacous-\ntics to key detection and root note extraction in EDM.\nInProc. of the 13th International Symp. on CMMR ,\npages 482–492, 2017.\n[11] C. Harte. Towards automatic extraction of harmony\nfrom music signals . Ph.D. thesis, University of London,\nLondon, 2010.\n[12] C. Harte, M. Sandler, and M. Gasser. Detecting har-\nmonic change in musical audio. In Proc. of the 1st ACM\nworkshop on Audio and music computing multimedia ,\npages 21–26, 2006.\n[13] J. Hemming. Methoden der Erforschung popul ¨arer\nMusik . Springer VS, Wiesbaden, 2016. In German.\n[14] P. Knees, ´A. Faraldo, P. Herrera, R. V ogl, S. B ¨ock,\nF. H ¨orschl ¨ager, and M. Le Goff. Two data sets for\ntempo estimation and key detection in electronic dance\nmusic annotated from user corrections. In Proc. of\nthe 16th International Society for Music Information\nRetrieval (ISMIR 2015) Conference , pages 364–370,\n2015.[15] C. Krumhansl, E. Kessler, and J. Edward. Tracing the\ndynamic changes in perceived tonal organization in a\nspatial representation of musical keys. Psychological\nReview , 89(4):334–368, 1982.\n[16] K. Noland and M.B. Sandler. Key estimation using a\nhidden markov model. In Proc. of the 7th International\nSociety for Music Information Retrieval (ISMIR 2006)\nConference , pages 121–126, 2006.\n[17] J. Pauwels, K. O’Hanlon, G. Fazekas, and M.B. San-\ndler. Conﬁdence measures and their applications in\nmusic labelling systems based on hidden markov mod-\nels. In Proc. of the 18th Conference of the International\nSociety for Music Information Retrieval (ISMIR 2017) ,\npages 279–279, 2017.\n[18] M. Spicer. (ac)cumulative form in pop-rock music.\nTwentieth Century Music , 1(1):29 – 64, 2004.\n[19] P. Tagg. From refrain to rave: The decline of ﬁgure and\nthe rise of ground. Popular Music , 13(2):209 – 222,\n1994.\n[20] D. Temperley. Music and Probability . MIT Press,\nCambridge, MA, 2007.\n[21] R. Wooller and A. Brown. A framework for discussing\ntonality in electronic dance music. In Proc. Sound:\nSpace - The Australasian Computer Music Conference ,\npages 91 – 95, 2008.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 9"
    },
    {
        "title": "Jazz Solo Instrument Classification with Convolutional Neural Networks, Source Separation, and Transfer Learning.",
        "author": [
            "Juan S. Gómez",
            "Jakob Abeßer",
            "Estefanía Cano"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492481",
        "url": "https://doi.org/10.5281/zenodo.1492481",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/145_Paper.pdf",
        "abstract": "Predominant instrument recognition in ensemble recordings remains a challenging task, particularly if closelyrelated instruments such as alto and tenor saxophone need to be distinguished. In this paper, we build upon a recentlyproposed instrument recognition algorithm based on a hybrid deep neural network: a combination of convolutional and fully connected layers for learning characteristic spectral-temporal patterns. We systematically evaluate harmonic/percussive and solo/accompaniment source separation algorithms as pre-processing steps to reduce the overlap among multiple instruments prior to the instrument recognition step. For the particular use-case of solo instrument recognition in jazz ensemble recordings, we further apply transfer learning techniques to fine-tune a previously trained instrument recognition model for classifying six jazz solo instruments. Our results indicate that both source separation as pre-processing step as well as transfer learning clearly improve recognition performance, especially for smaller subsets of highly similar instruments.",
        "zenodo_id": 1492481,
        "dblp_key": "conf/ismir/GomezAC18",
        "keywords": [
            "ensemble recordings",
            "instrument recognition",
            "hybrid deep neural network",
            "spectral-temporal patterns",
            "harmonic/percussive separation",
            "solo/accompaniment source separation",
            "transfer learning",
            "jazz solo instruments",
            "pre-processing step",
            "recognition performance"
        ],
        "content": "JAZZ SOLO INSTRUMENT CLASSIFICATION WITH CONVOLUTIONAL\nNEURAL NETWORKS, SOURCE SEPARATION, AND TRANSFER\nLEARNING\nJuan S. G ´omez Jakob Abeßer Estefan ´ıa Cano\nSemantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany\nfgomejn,abr,cano g@idmt.fhg.de\nABSTRACT\nPredominant instrument recognition in ensemble record-\nings remains a challenging task, particularly if closely-\nrelated instruments such as alto and tenor saxophone need\nto be distinguished. In this paper, we build upon a recently-\nproposed instrument recognition algorithm based on a hy-\nbrid deep neural network: a combination of convolu-\ntional and fully connected layers for learning character-\nistic spectral-temporal patterns. We systematically eval-\nuate harmonic/percussive and solo/accompaniment source\nseparation algorithms as pre-processing steps to reduce the\noverlap among multiple instruments prior to the instrument\nrecognition step. For the particular use-case of solo in-\nstrument recognition in jazz ensemble recordings, we fur-\nther apply transfer learning techniques to ﬁne-tune a previ-\nously trained instrument recognition model for classifying\nsix jazz solo instruments. Our results indicate that both\nsource separation as pre-processing step as well as trans-\nfer learning clearly improve recognition performance, es-\npecially for smaller subsets of highly similar instruments.\n1. INTRODUCTION\nAutomatic Instrument Recognition (AIR) is a fundamental\ntask in Music Information Retrieval (MIR) which aims at\nidentifying all participating music instruments in a given\nrecording. This information is valuable for a variety of\ntasks such as automatic music transcription, source separa-\ntion, music similarity computation, and music recommen-\ndation, among others. In general, musical instruments can\nbe categorized based on their underlying sound production\nmechanisms. However, various aspects of human music\nperformance such as dynamics, intonation, or vibrato cre-\nate a large timbral variety that complicate the distinction of\nclosely-related instruments such as a violin and a cello.\nAs part of the ISAD (Informed Sound Activity Detec-\ntion in Music Recordings) research project, we aim at im-\nproving existing methods for timbre description and instru-\nc\rJuan S. G ´omez, Jakob Abeßer, Estefan ´ıa Cano. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Juan S. G ´omez, Jakob Abeßer, Estefan ´ıa\nCano. “Jazz Solo Instrument Classiﬁcation with Convolutional Neural\nNetworks, Source Separation, and Transfer Learning”, 19th International\nSociety for Music Information Retrieval Conference, Paris, France, 2018.ment classiﬁcation in ensemble music recordings. In par-\nticular, this paper focuses on the identiﬁcation of predom-\ninant solo instruments in multitimbral music recordings,\ni. e., the most salient instruments in the audio mixture. This\nassumes that the spectral-temporal envelopes that describe\nthe instrument’s timbre are dominant in the polyphonic\nmixture [11]. As a particular use-case, we focus on the\nclassiﬁcation of solo instruments in jazz ensemble record-\nings. Here, we study the task of instrument recognition\nboth on a class and sub-class level, e. g. between soprano,\nalto, and tenor saxophone. Besides the high timbral sim-\nilarity between different saxophone types, a second chal-\nlenge lies in the large variety of recording conditions that\nheavily inﬂuence the overall sound of a recording [21, 25].\nA system for jazz solo instrument classiﬁcation could be\nused for content-based metadata clean-up and enrichment\nof jazz archives.\nAs the main contributions of this paper, we systemat-\nically evaluate two state-of-the-art source separation al-\ngorithms as pre-processing steps to improve instrument\nrecognition (see Section 3). We extend and improve upon a\nrecently proposed hybrid neural network architecture (see\nFigure 1) that combines convolutional layers for automatic\nlearning of spectral-temporal timbre features, and fully\nconnected layers for classiﬁcation [28]. We further evalu-\nate transfer learning strategies to adapt a given neural net-\nwork model to more speciﬁc classiﬁcation use-cases such\nas jazz solo instrument classiﬁcation, which require a more\ngranular level of detail [13].\n2. RELATED WORK\nThe majority of work towards automatic instrument recog-\nnition has focused on instrument classiﬁcation of isolated\nnote events or monophonic phrases and melodies played\nby single instruments. Considering classiﬁcation scenarios\nwith more than 10 instrument classes, the best-performing\nsystems achieve recognition rates above 90%, as shown for\ninstance in [14, 27].\nIn polyphonic and multitimbral music recordings, how-\never, AIR is a more complicated problem. Traditional ap-\nproaches rely on hand-crafted audio features designed to\ncapture the most discriminative aspects of instrument tim-\nbres. Such features are based on different signal represen-\ntations based on cepstrum [8–10, 29], group delay [5], or\nline spectral frequencies [18]. A classiﬁer ensemble focus-577Figure 1 . Reference model proposed by Han et al. [28]. Time-frequency spectrogram patches are processed by successive\npairs of convolutional layers (Conv) with ReLU activation function (R), max pooling (MaxPool), and global max pooling\n(GlobMaxPool). Dropout (D) is applied for regularization in the feature extractor and classiﬁer. Conv layers have increasing\nnumber of ﬁlters (32, 64, 128, and 256) and output shapes are speciﬁed for each layer.\ning on note-wise, frame-wise, and envelope-wise features\nwas proposed in [14]. We refer the reader to [11] for an\nextensive overview of AIR algorithms that include hand-\ncrafted audio features.\nNovel deep learning algorithms, particularly convolu-\ntional neural networks (CNN), have been widely used for\nvarious image recognition tasks [13]. As a consequence,\nthese methods were successfully adopted to MIR tasks\nsuch as chord recognition [17] and music transcription [1],\nwhere they signiﬁcantly improved upon previous state-of-\nthe-art results. Similarly, the ﬁrst successful AIR methods\nbased on deep learning were recently proposed and de-\nsigned from the combination of convolutional layers for\nfeature learning, and fully-connected layers for classiﬁ-\ncation [24, 28]. Park et al. use a CNN to recognize in-\nstruments using single tone recordings [24]. Han et al.\n[28] propose a similar architecture and evaluate different\nlate-fusion results to obtain clip-wise instrument labels.\nThe authors aim at classifying predominant instruments in\npolyphonic and multitimbral recordings, and improve upon\nprevious state-of-the-art systems by around 0.1 in f-score.\nLi et al. [20] propose to use end-to-end learning, consid-\nering a different network architecture. By these means,\nthey use raw audio data as input without relying on spec-\ntral transformations such as mel spectrograms.\nA variety of pre-processing strategies have been been\napplied MIR tasks such as singing voice detection [19] and\nmelody line estimation [26]. Regarding the AIR task, sev-\neral algorithms include a preceding source separation step.\nIn [2], Bosch et al. evaluate two segregation methods for\nstereo recordings—a simple LRMS (Left/Right-Mid/Side)\nseparation and FASST (Flexible Audio Source Separation\nFramework) developed by Ozerov et al. [22]. The authors\nreport improvements of 19% in f-score using a simple pan-\nning separation, and up to 32% when the model was trained\nwith previously separated audio, taking into account the\ntypical artifacts produced by source separation techniques.\nHeittola et al. [16] propose a system that uses a source-\nﬁlter model for source separation in a non-negative matrix\nfactorization (NMF) scheme. The spectral basis functions\nare constrained to have harmonic spectra with smooth fre-\nquency responses. Using a Gaussian mixture model, theauthors achieved a 59% recognition rate for six polyphonic\nnotes randomly chosen from 19 different instruments.\n3. PROCESSING STEPS\n3.1 Baseline Instrument Recognition Framework\nIn this section, we brieﬂy summarize the instrument recog-\nnition model proposed by Han et al. [28], which we use\nas the starting point for our experiments. As a ﬁrst step,\nmonaural audio signals are processed at a sampling rate\nof 22.05 kHz. A mel spectrogram with a window size of\n1024, a hop size of 512, and 128 mel bands is then com-\nputed. After applying a logarithmic magnitude compres-\nsion, spectral patches one second long are used as input\nto the deep neural network. The resulting time-frequency\npatches have shape xi2R128\u000243.\nThe network architecture is illustrated in Figure 1 and\nconsists of four pairs of convolutional layers with a ﬁlter\nsize of 3\u00023and ReLU activation functions. The input\nof each convolution layer is zero-padded with 1 \u00021, con-\nsidered in the output shape of each layer. The number of\nﬁlters in the conv layer pairs increases from 32 to 256.\nMax pooling over both time and frequency is performed\nbetween successive layer pairs. Dropout of 0.25 is used for\nregularization. An intermediate global max pooling layer\nand ﬂatten layer (F) connect the feature extractor with the\nclassiﬁer. Finally, a fully-connected layer (FC), dropout of\n0.5, and a ﬁnal output layer sigmoid activation (S) with 11\nclasses are used. The model was trained with a learning\nrate of 0:001, a batch size of 128, and the Adam optimizer.\nIn the post-processing stage, Han et al. compare two ag-\ngregation strategies to obtain class predictions on a audio\nﬁle level: ﬁrst, they apply thresholds over averaged and\nnormalized segment-wise class predictions (S1 strategy).\nSecondly, a sliding window of 6 segments and hop-size 3\nsegments is used for local aggregation prior to performing\nS1 strategy (S2 strategy). Refer to [28] for the identiﬁca-\ntion threshold estimation. Apart from the model ensem-\nbling step (which combines different predictors), we were\nable to reproduce the evaluation results reported in [28], in\nterms of recognition performance, intermediate activation\nfunction (ReLU), and the optimal identiﬁcation threshold578 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Micro Averaging Macro Averaging\nMethod Model Ensembling Data setActivation\nFunctionAgg. P R F P R FOpt.\n\u0012\nBaseline system [28] X IRMAS ReLU S2 0.657 0.603 0.629 0.540 0.547 0.517 0.55\nReproduction - IRMAS ReLU S1 0.591 0.548 0.568 0.530 0.477 0.471 0.40\nReLU S2 0.609 0.544 0.574 0.501 0.507 0.475 0.55\nExperiment - MONOTIMBRAL LReLU S1 0.645 0.678 0.661 0.685 0.681 0.657 0.8\nLReLU S2 0.619 0.695 0.655 0.657 0.690 0.649 0.7\nTable 1 . Performance metrics precision (P), recall (R), and F-score (F) from best results reported by [28], its reproduction\nwith the IRMAS data set, and an experiment with the MONOTIMBRAL data set. The displayed results are the best settings\nobtained with respect to ReLU/LReLU activation functions, and S1/S2 aggregation strategies (see Section 3.1).\n\u0012as shown in Table 1. Additionally, an experiment was\nconducted using monotimbral audio as input data to train\nthe neural network. Following [28], we tested different\nintermediate activation functions (ReLU and LReLU) and\nboth aggregation strategies. The monotimbral audio used\nfor this experiment is further explained in Section 4.2.\n3.2 Source Separation\nMotivated by the previous experiment, which showed that\nrecognition performance increases 5-10% by using mono-\ntimbral data as input, we explore the use of sound source\nseparation as a pre-processing stage to musical instrument\nclassiﬁcation. The idea is to evaluate whether isolating the\ndesired instrument from the mixture can improve classi-\nﬁcation performance. This section brieﬂy describes two\nsound separation methods used in our experiments.\n3.2.1 Phase-based Harmonic / Percussive Source\nSeparation\nThe harmonic-percussive separation described in [3] works\nunder the assumption that harmonic music instrument will\nexhibit stable phase contours as the ones obtained by dif-\nferentiating the phase spectrogram in time. In contrast,\ngiven the broadband and transient-like characteristics of\npercussive instruments, this stability in phase cannot be ex-\npected. This system takes advantage of this fundamental\ndistinction between harmonic and percussive instruments,\nand by calculating the expected phase change for a given\nfrequency bin and hop size, a separation mask is created\nto extract harmonic components from the mix. The effects\nof the harmonic-percussive separation can be observed in\nFigure 2, where the spectrogram of the original audio mix-\nture and of the harmonic and percussive components are\ndisplayed.\n3.2.2 Pitch-Informed Solo/Accompaniment Separation\nTo extract solo instruments from multitimbral music, the\nmethod proposed in [4] was also used in our experiments.\nThe system performs separation by ﬁrst extracting pitch in-\nformation from the solo instrument, and then closely track-\ning its harmonic components to create a spectral mask.\nTo extract pitch information, the method proposed in [7]\nis used for main melody extraction. Pitch information is\nextracted by performing a pair-wise evaluation of spectral\npeaks, and by ﬁnding partials with well-deﬁned frequency\nratios. The pitch information extracted is then used to\n050100Mel bandsOriginal Audio\n050100Mel bandsHarmonic Separated Audio\n050100Mel bandsPercussive Separated Audio\n050100Mel bandsSolo Separated Audio\n0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00\nSeconds050100Mel bandsAccompaniment Separated AudioFigure 2 . Mel-spectrograms of the original audio\ntrack, the harmonic/percussive components, and the\nsolo/accompaniment components for a jazz excerpt of a\nsaxophone solo played by John Coltrane. The audio mix-\nture contains the solo saxophone, piano, bass and drums.\ntrack the harmonic components in the separation stage, us-\ning common amplitude modulation, inharmonicity, attack\nlength, and saliency as underlying concepts.\nThe performance of both the pitch detection and the\nseparation stage in this system highly depend on the mu-\nsical instrument to be separated: for musical instruments\nwith clear, stable partials the separation performance can\nbe very good. This is the case of woodwinds and string in-\nstruments such as the violin. However, for musical instru-Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 579ments with a less stable spectral behavior such as the xylo-\nphone, or instruments with strong distortion effects such as\nelectric guitars, separation can be noisy. The effects of the\nsolo/accompaniment separation can be observed in Figure\n2, where the spectrogram of the original audio mixture and\nof the solo and accompaniment components are displayed.\nIt can be seen that starting from 1.50 seconds, the solo in-\nstrument is not detected and hence, no energy is assigned\nto the solo track.\n3.3 Transfer Learning\nFor the special use-case of solo instrument recognition in\njazz ensemble recordings, we aim at training a recognition\nmodel despite the small amount of available training data\n(see the JAZZ data set in Section 4.3). Here, transfer learn-\ning can be applied to ﬁne-tune an existing classiﬁcation\nmodel [13]. We assume that initially learnt feature rep-\nresentations for predominant AIR are highly relevant and\ntherefore transferable for our use-case. Transfer learning\nhas been successfully used in MIR for the task of sound\nevent tagging in [6]. We refer the reader to [23] for a com-\nprehensive overview of transfer learning in classiﬁcation,\nregression, and clustering applications.\n4. DATA SETS\n4.1 IRMAS\nThe IRMAS data set (Instrument Recognition in Music\nAudio Signals) for predominant instrument recognition\nwas ﬁrst introduced by Bosch et al. in [2]. It is partitioned\ninto separate training and test sets. The training set in-\ncludes 6705 stereo audio ﬁles with a duration of 3 seconds\neach, extracted from more than 2000 recordings. All the\nrecordings in the training data set are single-labeled and\nhave a single predominant instrument. The amount of au-\ndio ﬁles per instrument is unevenly distributed and ranges\nfrom 388 to 778. The test set consists of 2874 stereo audio\nﬁles with variable duration ranging from 5 to 20 seconds.\nThese recordings are multi-labeled and cover 1-5 instru-\nment labels per sample. The test set also shows a highly\nuneven instrument distribution with 62 to 1044 audio ﬁles\nper instrument class. As shown in Table 2, the data set con-\ntains 11 musical instruments: cello, clarinet, ﬂute, acoustic\nguitar, electric guitar, organ, piano, saxophone, trumpet,\nviolin, and singing voice. In the experiments described in\nSection 5.2.2, we use a subset denoted as IRMAS-Wind,\nwhich includes all recordings of the wind instruments in\nthe IRMAS data set: ﬂute, clarinet, saxophone, and trum-\npet. The motivation to create this subset is the improved\nperformance of the solo/accompaniment separation algo-\nrithm (see section Section 3.2.1) and its timbral similar-\nity to the JAZZ data set to apply transfer learning strate-\ngies (see Section 4.3). Following [28], training data was\nrandomly split to training (85%) and validation (15%) to\nprevent overﬁtting by implementing early stopping. Test-\ning data was randomly split into development testing data\n(50%) for optimum thresholding in post-processing, andpure testing data (50%) to obtain the ﬁnal performance\nmetrics (see Table 3).\nInstrument IRMAS MONO. JAZZ\nClass Subclass # h # h # h\nCello 499 0.87\nClarinet 567 0.71 26 0.32 31 0.53\nFlute 614 1.17 29 0.42\nAcoustic Guitar 1172 3.08 30 0.38\nElectric Guitar 1702 5.00\nClean 28 0.43\nDistorted 30 0.34\nOrgan 1043 2.25\nHammond Organ 30 0.44\nPiano 1716 5.40 27 0.38\nElectric Piano 29 0.31\nSaxophone 952 2.16 29 0.34\nSoprano 30 0.53\nAlto 29 0.53\nTenor 32 0.53\nTrombone 27 0.53\nTrumpet 744 1.29 29 0.35 36 0.53\nViolin 791 1.56 27 0.47\nV oice 1822 5.38\nFemale 21 0.26\nMale 20 0.26\nDouble Bass 27 0.28\nSynthesizer 30 0.77\nTOTAL 11622 28.87 412 5.75 185 3.18\nTable 2 . Overview of the three data sets IRMAS, MONO-\nTIMBRAL, and JAZZ, which includes various instrument\nclasses and subclasses. Both the number of labels (#) and\nthe total duration in hours (h) is given for each data set.\n4.2 MONOTIMBRAL\nThe MONOTIMBRAL data set includes monotimbral\n(single-labeled) recordings, i. e., monophonic or poly-\nphonic recordings without overlap of other instruments,\nof 15 musical instrument classes: acoustic guitar, clarinet,\ndouble bass, electric guitar clean, electric guitar distorted,\nelectric piano, ﬂute, hammond organ, piano, saxophone,\nfemale singing voice, male singing voice, synthesizer,\ntrumpet, and violin. The data set contains 412 stereo audio\nﬁles with variable duration from 10 to 120 seconds, man-\nually selected from various segments of YouTube videos.\nThe MONOTIMBRAL data set was randomly split equally\ninto a training and test set based on an equal distribution of\naudio ﬁles per instrument class (see Table 3).\n4.3 JAZZ\nAs one speciﬁc use-case, we aim at classifying among the\nsix most popular brass and reed instruments in jazz so-\nlos: trumpet (tp), clarinet (cl), trombone (tb), alto saxo-\nphone (as), tenor saxophone (ts), and soprano saxophone\n(ss). While the number of instruments is smaller com-\npared to the IRMAS and MONOTIMBRAL data sets, they\nhave a higher timbral similarity, considering particularly\nthe three saxophone subclasses. In order to prepare a data\nset, we ﬁrst randomly selected solos from the Weimar Jazz\nDatabase [25] and enriched the data set with additional\njazz solos. While the number of instruments is smaller\ncompared to the IRMAS and MONOTIMBRAL data sets,\nthe audio samples were chosen to maximize diversity of580 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018performing artists. Moreover, examples from each class\nwere randomly selected to have the same duration (see\nTable 2), achieving equal distribution of spectrogram ex-\namples across instrument classes. As with the other data\nsets, the JAZZ data set split randomly as the other data sets\n(see Table 3). Since jazz recordings cover many decades of\nthe 20th century, the instrument recognition task is further\ncomplicated by different recording techniques.\nFor additional information regarding the MONOTIM-\nBRAL and JAZZ data sets, refer to the complimentary\nwebsite for this paper [12].\nTraining Data Set (85/15) Testing Data Set (50/50)\nTrain Validation Development Pure\nIRMAS 17094 3021 48064 48055\nIRMAS-Wind 5486 970 10447 10446\nMonotimbral 8676 1539 10620 10610\nJAZZ 7206 1275 1678 1271\nTable 3 . Number of mel spectrogram examples for each\ndata set split into Train, Validation, Development, Pure\ndata sets.\n5. EV ALUATION\n5.1 Metrics\nFollowing [2, 11, 28], precision, recall, and f-scores were\ncalculated for both micro and macro averages. Micro aver-\naging gives more weight to instrument classes with higher\nappearance in the data distribution. Macro averaging is\ncalculated per label, representing an overall performance\nof the system.\n5.2 Improving Predominant Instrument Recognition\nusing Source Separation\n5.2.1 Harmonic / Percussive Separation\nAfter processing the audio ﬁles with the har-\nmonic/percussive separation introduced in Section 3.2.1,\nwe ﬁrst retrained the baseline model independently on the\nharmonic stream and percussive stream. Furthermore, we\ncreated a two-branch model that processes the harmonic\nand percussive stream in parallel and fuses the results in\nthe ﬁnal fully-connected layers, similar to [15]. As shown\nin Figure 3, using the harmonic stream marginally im-\nproved recognition results for both aggregation strategies\nS1 and S2 by up to 3% in f-score for the multitimbral\nIRMAS data set. In contrast, we did not observe an\nimprovement for the MONOTIMBRAL data set. Using\nthe two-branch model did not improve the performance on\nthe IRMAS data set and worsens the performance on the\nMONOTIMBRAL data set.\n5.2.2 Solo / Accompaniment Separation\nThe aim of performing this separation is to further im-\nprove the quality of the input audio to the classiﬁca-\ntion system. All experiments described in this sec-\ntion were performed on the IRMAS-Wind and the JAZZ\ndata sets (see Section 4), given the performance of the\n(s1, micro)\n(s1, macro)\n(s2, micro)\n(s2, macro)\nstrategy,averaging−0.06−0.04−0.020.000.020.040.060.08score\nprecision\nrecall\nf-scoreFigure 3 . Comparison of the AIR system trained on the\nharmonic stream and the baseline model trained with the\noriginal IRMAS data set. Differences between evaluation\nmetrics are shown for both aggregation strategies S1 and\nS2 (compare Section 3.1) as well as micro and macro av-\neraging (compare Section 5.1).\nsolo/accompaniment algorithm. Both data sets also have\nsimilar timbral characteristics, which represents our tar-\ngeted scenario.\nWe compare AIR models trained on the original au-\ndio tracks with models trained on the solo stream ob-\ntained from the solo/accompaniment separation. As shown\nin Table 4, applying the solo/accompaniment separation\nas pre-processing step improves the AIR performance by\n3.8% in macro f-score for the IRMAS-Wind data set and\n13.4% for the JAZZ data set using the S1 strategy. Ad-\nditionally both micro and macro averages result in similar\nvalues, given the even distribution of examples of the JAZZ\ndata set. The results might also indicate that error propa-\ngation from transcription errors to the source separation\nalgorithm are not critical, since the instrument recognition\nresults are averaged over time and the approximate accu-\nracy of the pitch detection algorithm is 80% [7].\nF-Score\nData set S/A Separation Micro Macro\nIRMAS-Wind - 0.684 0.598\nIRMAS-Wind X 0.713 0.636\nJAZZ - 0.657 0.669\nJAZZ X 0.805 0.803\nTable 4 . Performance metrics obtained by training the\nbaseline model with the IRMAS-Wind and JAZZ data sets.\nBest results were obtained using aggregation strategy S1.\n5.3 Combining Source Separation and Transfer\nLearning for Jazz Solo Instrument Recognition\nFor our ﬁnal use-case of recognizing jazz solo instru-\nments, we aim at combining solo/accompaniment sepa-\nration and transfer learning strategies. We use the mod-\nels trained on the IRMAS-Wind data set (with and with-Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 581out solo/accompaniment separation) as starting point for\nthe transfer learning approach. All models were trained\nfrom scratch following the original parameters from [28].\nThe JAZZ data set includes recordings from trombone and\nthree saxophone subclasses: tenor, alto, and soprano. Ad-\nditionally, the trumpet and the clarinet classes were already\nincluded in the IRMAS-Wind data set. One main challenge\nis that while the characteristics of the predominant melody\ninstruments in the IRMAS and JAZZ data sets are similar,\nthe background instrumentation and recording conditions\nare often very different. We remove the last sigmoid layers\nof models pre-trained with the IRMAS-Wind data set and\nreplace them by a 6-class sigmoid layer, considering the\nJAZZ data set. For testing, we compare two approaches:\n(1) the one-pass method which re-trains the last classiﬁ-\ncation layer using a learning rate of \u000b= 0:01(10 times\nthe original learning rate), while all remaining layers re-\nmain ﬁxed, and (2) the two-pass approach where we further\nre-train all layers in a second training step with a smaller\nlearning rate of \u000b= 0:001. Table 5 shows the classiﬁca-\ntion performance on the JAZZ data set for different system\nconﬁgurations with the one-pass and two-pass strategies,\nas well as with and without the solo/accompaniment sepa-\nration. The best performance was achieved by combining\nsolo/accompaniment separation and the two-pass transfer\nlearning strategy.\nF-score\nS/A Separation Transfer Learning Micro Macro\n- One-pass 0.605 0.621\nX One-pass 0.738 0.748\n- Two-pass 0.583 0.610\nX Two-pass 0.787 0.780\nX - 0.805 0.803\nTable 5 . Performance metrics obtained by combining\nsolo/accompaniment separation with transfer learning on\nthe JAZZ data set. The results obtained by training the\nmodel from scratch (without transfer learning) are also\nshown in the bottom row for reference. Best results were\nobtained using aggregation strategy S1.\nIt can also be observed that the transfer learning\nmodel shows a lower macro f-measure of 0.780 than the\nmodel trained from scratch with 0.803 (see bottom row\nof Table 5). To further understand this behavior, six ad-\nditional 10 s (unseen) jazz solo excerpts1were analyzed.\nFigure 4 shows segment- and clip-wise predictions for\nthese six solo excerpts using solo/accompaniment sepa-\nration. The ﬁgure shows the results for the best transfer\nlearning system and the model trained on the JAZZ data\nset from scratch [12]. A total of 20 predictions were gener-\nated per excerpt on 1 s long windows using a 50 % overlap.\nThese results suggest that transfer learning can improve\ngeneralization of unseen data, but needs further systematic\ninvestigations on a larger testing data set.\n1Ornette Coleman - Ramblin (as), Buddy DeFranco - Autumn Leaves\n(cl), John Coltrane - My Favorite Things (ss), Frank Rossolino - Moon-\nlight in Vermont (tb), Lee Morgan - The Sidewinder (tp), Michael Brecker\n- African Skies (ts)\n0 10 20 30 40 50 600100Mel-bandsMelspectrogram\n0 10 20 30 40 50 60as\nts\nss\ntb\ntp\nclLabelsSegment Predictions with Transfer Learning\n0 10 20 30 40 50 60as\nts\nss\ntb\ntp\nclLabelsAggregated Predictions with Transfer Learning\n0 10 20 30 40 50 60as\nts\nss\ntb\ntp\nclLabelsSegment Predictions without Transfer Learning\n0 10 20 30 40 50 60\nSecondsas\nts\nss\ntb\ntp\nclLabelsAggregated Predictions without Transfer LearningFigure 4 . Mel-spectrogram of 10 second excerpts from\nsix jazz solos covering all solo instruments (top), segment-\nwise and aggregated clip-wise predictions (using strategy\nS1) are shown below for a model trained via transfer learn-\ning (two-pass) and a model trained from scratch. Clip-wise\nground truth is plotted in white rectangles [12].\n6. CONCLUSION\nIn this paper, we investigated two methods to improve upon\na system for AIR on multitimbral ensemble recordings. We\nﬁrst evaluated two state-of-the-art source separation meth-\nods and showed that on multitimbral audio data, analyzing\nthe harmonic and solo streams can be beneﬁcial compared\nto the mixed audio data.\nFor the speciﬁc use-case of jazz solo instrument classi-\nﬁcation, which involves classifying six instruments with\nhigh timbral similarity, combining solo/accompaniment\nsource separation and transfer learning methods seems to\nlead to AIR models with better generalization to unseen\ndata. This must be further investigated by increasing the\nsize of the JAZZ data set. While source separation al-\nlows to narrow the focus on the predominant instrument,\ntransfer learning allows to exploit useful feature represen-\ntations learned from related instruments. In the future,\na deep learning model capable of discriminating highly\nsimilar instruments could potentially be applied in other\ntimbre-related recognition tasks such as performer identi-\nﬁcation [25].\n7. ACKNOWLEDGEMENTS\nThis work has been supported by the German Research\nFoundation (AB 675/2-1).582 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20188. REFERENCES\n[1] Rachel M. Bittner, Brian McFee, Justin Salamon, Pe-\nter Li, and Juan P. Bello. Deep salience representations\nfor f0 estimation in polyphonic music. In Proceedings\nof the International Society of Music Information Re-\ntrieval (ISMIR) , Suzhou, China, October 2017.\n[2] Juan Bosch, Jordi Janer, Ferdinand Fuhrmann, and Per-\nfecto Herrera. A comparison of sound segregation tech-\nniques for predominant instrument recognition in mu-\nsical audio signals. In Proceedings of the International\nSociety for Music Information Retrieval Conference\n(ISMIR) , pages 559–564, Porto, Portugal, 2012.\n[3] Estefan ´ıa Cano, Mark D. Plumbley, and Christian\nDittmar. Phase-based harmonic/percussive separation.\nInProceedings of the Annual Conference of the Inter-\nnational Speech Communication Association (INTER-\nSPEECH) , pages 1628–1632, Singapore, 2014.\n[4] Estefan ´ıa Cano, Gerald Schuller, and Christian\nDittmar. Pitch-informed solo and accompaniment sep-\naration towards its use in music education applications.\nEURASIP Journal on Advances in Signal Processing ,\n23:1–19, 2014.\n[5] Aleksandr Diment, Padmanabhan Rajan, Toni Heit-\ntola, and Tuomas Virtanen. Modiﬁed group delay fea-\nture for musical instrument recognition. In Proceed-\nings of the International Symposium on Computer Mu-\nsic Multidisciplinary Research , pages 431–438, Mar-\nseille, France, 2013.\n[6] Aleksandr Diment and Tuomas Virtanen. Transfer\nlearning of weakly labelled audio. In Proceedings of\nthe IEEE Workshop on Applications of Signal Process-\ning to Audio and Acoustics (WASPAA) , pages 6–10,\nNew Paltz, USA, 2017.\n[7] Karin Dressler. Automatic transcription of the melody\nfrom polyphonic music . PhD thesis, TU Ilmenau, Ger-\nmany, Jul 2017.\n[8] Zhiyao Duan, Bryan Pardo, and Laurent Daudet. A\nnovel cepstral representation for timbre modeling of\nsound sources in polyphonic mixtures. In Proceedings\nof the IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pages 7495–\n7499, Florence, Italy, May 2014.\n[9] Antti Eronen and Anssi Klapuri. Musical instrument\nrecognition using cepstral coefﬁcients and temporal\nfeatures. In Proceedings of the IEEE International\nConference on Acoustics, Speech, and Signal Process-\ning (ICASSP) , pages 753–756, Istanbul, Turkey, 2000.\n[10] Slim Essid, Gael Richard, and Bertrand David. Mu-\nsical instrument recognition on solo performances. In\nProceedings of the European Signal Processing Con-\nference (EUSIPCO) , pages 1289–1292, Vienna, Aus-\ntria, 2004.[11] Ferdinand Fuhrmann. Automatic musical instrument\nrecognition from polyphonic music audio signals . PhD\nthesis, Universitat Pompeu Fabra, 2012.\n[12] Juan S. G ´omez, Jakob Abeßer, and Este-\nfan´ıa Cano. Complementary website. https:\n//github.com/dfg-isad/ismir_2018_\ninstrument_recognition .\n[13] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\nDeep Learning . MIT Press, 2016.\n[14] Mikus Grasis, Jakob Abeßer, Christian Dittmar, and\nHanna Lukashevich. A multiple-expert framework for\ninstrument recognition. In Proceedings of the Inter-\nnational Symposium on Computer Music Multidisci-\nplinary Research (CMMR) , Marseille, France, October\n2013.\n[15] Thomas Grill and Jan Schl ¨uter. Music Boundary De-\ntection Using Neural Networks on Spectrograms and\nSelf-Similarity Lag Matrices. In Proceedings of the\nEuropean Signal Processing Conference (EUSIPCO) ,\nNice, France, 2015.\n[16] Toni Heittola, Anssi Klapuri, and Tuomas Virtanen.\nMusical instrument recognition in polyphonic audio\nusing source-ﬁlter model for sound separation. In Pro-\nceedings of the International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , pages 327–332,\nKobe, Japan, 2009.\n[17] Filip Korzeniowski and Gerhard Widmer. A fully con-\nvolutional deep auditory model for musical chord\nrecognition. In Proceedings of the IEEE International\nWorkshop on Machine Learning for Signal Processing,\nMLSP , pages 1–6, Salerno, Italy, 2016.\n[18] A. G. Krishna and T. V . Sreenivas. Music instrument\nrecognition: from isolated notes to solo phrases. In\nProceedings of the IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\nvolume 4, pages 265–268, Quebec, Canada, 2004.\n[19] Simon Leglaive, Romain Hennequin, and Roland\nBadeau. Singing voice detection with deep recurrent\nneural networks. In Proceedings of the International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , pages 121–125, Brisbane, Australia,\nApril 2015.\n[20] Peter Li, Jiyuan Qian, and Tian Wang. Automatic in-\nstrument recognition in polyphonic music using con-\nvolutional neural networks. CoRR , abs/1511.05520,\n2015.\n[21] Daniel Matz, Estefan ´ıa Cano, and Jakob Abeßer. New\nsonorities for early jazz recordings using sound source\nseparation and automatic mixing tools. In Proceedings\nof the International Society for Music Information Re-\ntrieval (ISMIR) , pages 749–755, Malaga, Spain, 2015.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 583[22] Alexey Ozerov, Emmauel Vincent, and Frederic Bim-\nbot. A general ﬂexible framework for the handling\nof prior information in audio source separation. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 20(4):1118–1133, May 2012.\n[23] S. J. Pan and Q. Yang. A survey on transfer learning.\nIEEE Transactions on Knowledge and Data Engineer-\ning, 22(10):1345–1359, Oct 2010.\n[24] Taejin Park and Taejin Lee. Musical instrument sound\nclassiﬁcation with deep convolutional neural network\nusing feature fusion approach. CoRR , abs/1512.07370,\n2015.\n[25] Martin Pﬂeiderer, Klaus Frieler, Jakob Abeßer, Wolf-\nGeorg Zaddach, and Benjamin Burkhart, editors. In-\nside the Jazzomat - New Perspectives for Jazz Re-\nsearch . Schott Campus, 2017.\n[26] H. Tachibana, T. Ono, N. Ono, and S. Sagayama.\nMelody line estimation in homophonic music au-\ndio signals based on temporal-variability of melodic\nsource. In Proceedings of the International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\npages 425–428, Dallas, Texas, March 2010.\n[27] Steven Tjoa and K. J. Ray Liu. Musical instrument\nrecognition using biologically inspired ﬁltering of tem-\nporal dictionary atoms. In Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 435–440, Utrecht, The Nether-\nlands, 2010.\n[28] Yoonchang Han and Jaehun Kim and Kyogu Lee. Deep\nconvolutional neural networks for predominant instru-\nment recognition in polyphonic music. IEEE/ACM\nTransactions on Audio, Speech, and Language Pro-\ncessing , 25(1):208–221, Jan 2017.\n[29] Li-Fan Yu, Li Su, and Yi-Hsuan Yang. Sparse cepstral\ncodes and power scale for instrument identiﬁcation.\nInProceedings of the IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\npages 7460–7464, Florence, Italy, 2014.584 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Matrix Co-Factorization for Cold-Start Recommendation.",
        "author": [
            "Olivier Gouvert",
            "Thomas Oberlin",
            "Cédric Févotte"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492537",
        "url": "https://doi.org/10.5281/zenodo.1492537",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/142_Paper.pdf",
        "abstract": "Song recommendation from listening counts is now a classical problem, addressed by different kinds of collaborative filtering (CF) techniques. Among them, Poisson matrix factorization (PMF) has raised a lot of interest, since it seems well-suited to the implicit data provided by listening counts. Additionally, it has proven to achieve state-ofthe-art performance while being scalable to big data. Yet, CF suffers from a critical issue, usually called cold-start problem: the system cannot recommend new songs, i.e., songs which have never been listened to. To alleviate this, one should complement the listening counts with another modality. This paper proposes a multi-modal extension of PMF applied to listening counts and tag labels extracted from the Million Song Dataset. In our model, every song is represented by the same activation pattern in each modality but with possibly different scales. As such, the method is not prone to the cold-start problem, i.e., it can learn from a single modality when the other one is not informative. Our model is symmetric (it equally uses both modalities) and we evaluate it on two tasks: new songs recommendation and tag labeling.",
        "zenodo_id": 1492537,
        "dblp_key": "conf/ismir/GouvertOF18",
        "keywords": [
            "cold-start problem",
            "Poisson matrix factorization (PMF)",
            "implicit data",
            "state-of-the-art performance",
            "scalable to big data",
            "multi-modal extension",
            "listening counts",
            "tag labels",
            "Million Song Dataset",
            "symmetric model"
        ],
        "content": "MATRIX CO-FACTORIZATION FOR COLD-START RECOMMENDATION\nOlivier Gouvert1Thomas Oberlin1C´edric F ´evotte1\n1IRIT, Universit ´e de Toulouse, CNRS, France\nfirstname.lastname@irit.fr\nABSTRACT\nSong recommendation from listening counts is now a clas-\nsical problem, addressed by different kinds of collabora-\ntive ﬁltering (CF) techniques. Among them, Poisson ma-\ntrix factorization (PMF) has raised a lot of interest, since\nit seems well-suited to the implicit data provided by listen-\ning counts. Additionally, it has proven to achieve state-of-\nthe-art performance while being scalable to big data. Yet,\nCF suffers from a critical issue, usually called cold-start\nproblem: the system cannot recommend new songs, i.e.,\nsongs which have never been listened to. To alleviate this,\none should complement the listening counts with another\nmodality. This paper proposes a multi-modal extension of\nPMF applied to listening counts and tag labels extracted\nfrom the Million Song Dataset. In our model, every song is\nrepresented by the same activation pattern in each modality\nbut with possibly different scales. As such, the method is\nnot prone to the cold-start problem, i.e., it can learn from a\nsingle modality when the other one is not informative. Our\nmodel is symmetric (it equally uses both modalities) and\nwe evaluate it on two tasks: new songs recommendation\nand tag labeling.\n1. INTRODUCTION\nNew albums and songs are released every day and are in-\nstantly available on streaming platforms. An important is-\nsue for streaming companies is therefore to develop rec-\nommender systems which are able to handle such new\nsongs [13, 20]. More generally, additional information on\nthose songs is needed to enrich the catalog, allowing the\nuser to efﬁciently explore and ﬁnd the songs he might like.\nIn this perspective, tag labeling has proven to be very use-\nful. The labels can be attributed by experts or by the user,\nand algorithms can complement this information with au-\ntomatic labeling [7].\nFor both tasks (song recommendation and tag label-\ning), matrix factorization (MF) techniques [12, 17], and\nin particular Poisson MF (PMF), reach signiﬁcant perfor-\nmance. Unfortunately, these techniques suffer from the\nwell-known cold-start problem: such a recommender sys-\nc\rOlivier Gouvert, Thomas Oberlin, C ´edric F ´evotte. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Olivier Gouvert, Thomas Oberlin, C ´edric\nF´evotte. “Matrix co-factorization for cold-start recommendation”, 19th\nInternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.tem cannot recommend songs which have never been lis-\ntened to, and similarly it cannot labeled untagged songs.\nA joint modeling of both modalities can achieve cold-start\nrecommendation, as soon as at least one modality is ob-\nserved for every song [8, 22].\nIn this paper, we propose a new matrix co-factorization\nmodel based on PMF, which performs those two tasks\njointly. Our model is robust to the cold-start problem for\nboth modalities. It can recommend a song which has never\nbeen listened to, based on its associate tags. And symmet-\nrically, it can associate tags on a song based on who lis-\ntened to it. To do that, we separately model the scale (pop-\nularity) of each song according to each modality, while the\npatterns across the topics are shared.\nThe state of the art of co-factorization techniques is\npresented in Section 2, along with some background on\nPMF. Then, in Section 3 we will present our new model\nand explain its properties. In Section 4, we provide\na majorization-minimization (MM) algorithm for solving\nour optimization problem and underline its scalability. Fi-\nnally, in Section 5, we test our model on songs recommen-\ndation and tag labeling in various settings.\n2. RELATED WORKS\nIn this paper, we will focus on works based on so-called\nhybrid techniques [1] and Poisson matrix factorization.\nNote that recommendation tasks can also be addressed\nwith other techniques such as factorization machines [19].\n2.1 Poisson matrix factorization\nPMF is a non-negative MF (NMF) technique [14]. Let Y\nbe a matrix of size F\u0002I, where each column represent\nan item (song) iaccording to Ffeatures. MF approxi-\nmates the observed matrix Yby a low-rank product of two\nmatrices: Y\u0019WHT, where W2RF\u0002K\n+ represents a\ndictionary matrix, and H2RI\u0002K\n+ represents a matrix of\nattributes (activations), with K\u001cmin(F;I).\nWhen observed data are in the form of counts, i.e.,\nY2NF\u0002I, a classical hypothesis is to assume that each\nobservation is drawn from a Poisson distribution:\nyfi\u0018Poisson([ WHT]fi): (1)\nThe maximum likelihood (ML) estimator of WandH\nis therefore obtained by minimizing the cost function de-792ﬁned by:\nC(W;H) =\u0000logp(YjW;H)\n=DKL(YjWHT) +cst (2)\ns.t.W\u00150;H\u00150;\nwherecstis a constant w.r.t. WandH, and whereDKLis\nthe generalized Kullback-Liebler (KL) divergence deﬁned\nby:\nDKL(YjX) =X\nf;i\u0012\nyfilogyfi\nxfi\u0000yfi+xfi\u0013\n:(3)\nThis low-rank approximation is known as KL non-\nnegative matrix factorization (KL-NMF) [9, 15].\nThe cost function Cis scale invariant, i.e., for any\ndiagonal non-singular matrix \u00032RK\u0002K\n+ , we have\nC(W;H) =C(W\u0003\u00001;H\u0003). To avoid degenerate solu-\ntions, a renormalization such thatP\nfwfk=Fis often\nused, where wfk= [W]fk.\nSeveral extensions based on Bayesian formulations\nhave been proposed in the literature [3,5,6,10,17]. In [10],\nthe authors developed a hierarchical Poisson factorization\n(HPF) by introducing new variables: the popularity of the\nitems and the activity of the users. These variables play a\nsigniﬁcant role in recommendation tasks.\n2.2 Co-factorization\nA way of circumventing the cold-start problem is to intro-\nduce new modalities [8, 11, 16]. Co-factorization frame-\nworks have been developed to jointly factorize two matri-\nces of observations (two modalities): YA\u0019WA(HA)T\nandYB\u0019WB(HB)T, with shared information between\nthe activation matrices: HA\u0019HB.\n2.2.1 Hard co-factorization\nHard co-factorization [8, 21] posits that the link between\nactivations is an equality constraint: HA=HB=H.\nThis is equivalent to concatenate the observations YAand\nYB, and the dictionaries WAandWB:\nDKL(YAjWAHT) +\rDKL(YBjWBHT)\n=DKL\u0012\u0012YA\n\rYB\u0013\nj\u0012WA\n\rWB\u0013\nHT\u0013\n;(4)\nwhere\r2R+is a weighting hyperparameter.\nAs in Section 2.1, scale invariance issues can\nbe solved by a renormalization step such that:P\nuwA\nuk+\rP\nvwB\nvk=U+V.\n2.2.2 Soft co-factorization\nSoft co-factorization [21] relaxes the equality constraint on\nthe activations replacing it by a soft penalty controlled by\nan hyperparameter \u000e2R+:\nDKL(YAjWA(HA)T) +\rDKL(YBjWB(HB)T)\n+\u000ePen(HA;HB):(5)A popular choice for this penalty is the `1-norm:\nPen(HA;HB) =\r\rHA\u0000HB\r\r\n1. It is adapted when both\nmodalities are likely to share the same activations, except\nat some sparse locations where they can differ signiﬁcantly.\n2.2.3 Offset models\nBayesian formulations of the soft co-factorization problem\nhave also been developed through the introduction of an\noffset latent variable [11,22]. The link between activations\nis therefore given by:\nhB\nik=hA\nik+\"ik; (6)\nwhere\"is a latent random variable.\nIn particular in [11], a co-factorization model is devel-\noped based on PMF, with \"ik\u0018Gamma(\u000b;\f). This\nchoice is motivated by the conjugacy propriety of the\ngamma distribution with the Poisson distribution. Never-\ntheless, the model is not symmetric with respect to (w.r.t.)\nthe activations HAandHB, ashB\nik> hA\nikby construc-\ntion. Thus, it can solve the cold-start problem only for the\nmodality A and not for B.\n3. PROPOSED MODEL\n3.1 Notations\nIn this article, we work with two different modalities. The\nﬁrst modality, denoted by A, corresponds to the listening\ncounts ofUusers onIsongs. The second modality, de-\nnoted by B, corresponds to the tags assigned to these I\nsongs, among a set of Vtags.WAandWBthus denote\nthe preferences of users and the atoms of tags across the K\npatterns, respectively.\n3.2 Link between attributes\nWe propose an equality constraint on normalized activa-\ntions. We denote by nA\ni=P\nkhA\nikandnB\ni=P\nkhB\nik, the\nsum of the rows of the activations. We impose, for each\nitemi:\nhA\nik\nnA\ni=hB\nik\nnB\ni=dik; (7)\nwhennA\ni>0andnB\ni>0.\n\u000fTheI\u0002Kmatrix Dwith entries dikcontrols\nthe attributes patterns subject to the constraintP\nkdik= 1. This information is shared by activa-\ntions of both modalities. For example, the Kpat-\nterns can be related to genre information: we ex-\npect that experimental rock songs share the same\npatterns.\n\u000fNA= diag(nA\ni)controls the scale of songs across\nthe modality A. It corresponds to the popularity of\nthe song, in the sense that a lot of people listen to it.\n\u000fNB= diag(nB\ni)controls the scale of songs across\nthe modality B. It corresponds to the fact that a song\ncan have more or less tag labels.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 793Two songs can have the same attributes patterns Dbut\ndifferent scales. For example, a song ican be a very pop-\nular song, known by a large panel of people: nA\ni\u001d0, but\nlack tag labeling: nB\ni\u00190. On the contrary, another song i\ncan be unpopular (because it is new or not well-received):\nnA\ni\u00190, but have a lot of tag information (a set of experts\nmay have labeled the song): nB\ni\u001d0.\nThe counterpart of Equation (2) is the following cost\nfunctionC, which we aim to minimize:\nC(WA;WB;D;NA;NB) (8)\n=DKL(YAjWA(NAD)T)\n+\rDKL(YBjWB(NBD)T)\ns.t.WA\u00150;WB\u00150;D\u00150;\ndiag(NA)\u00150;diag(NB)\u00150:\nWe denote by Z=fWA;WB;NA;NB;Dgthe set of\nvariables to infer.\n3.3 Scale invariance\nLet\u0002 = diag(\u0012i)be a diagonal matrix of size I\u0002Iwith\nnon-negative entries. We have the following scale invari-\nance:\nC(WA;WB;\u0002\u00001D;NA\u0002;NB\u0002)\n=C(WA;WB;D;NA;NB): (9)\nThis scale invariance allows us to impose the constraint on\nD, described in Section 3, by applying a renormalization\nstep (see Section 4.2).\nLet\u0003 = diag(\u0015k)be a diagonal matrix of size K\u0002Kwith\nnon-negative entries, \u0016WA=WA\u0003\u00001,\u0016WB=WB\u0003\u00001\nand\u0016D=D\u0003. We also have the following scale invari-\nance:\nC(\u0016WA;\u0016WB;\u0016D;NA;NB)\n=C(WA;WB;D;NA;NB):(10)\nIn practice, this invariance is not an issue and we do not\napply a renormalization step. However, this kind of invari-\nance plays a role for the scores used in recommendation as\ndiscussed in Section 3.4.\n3.4 Recommendation tasks\nIn recommender systems, a classical problem is to propose\na ranked list of songs, users or tags. We develop how to\nconstruct this list on two tasks: in- and out-prediction.\n3.4.1 In-matrix recommendation\nIn-matrix recommendation is a task of recommendation on\nusers and items which do not suffer from the cold-start\nproblem. For in-matrix recommendation, we propose a\nranked list of songs for each user, based on the score de-\nﬁned by:\nsA\nui=X\nkwA\nukhA\nik: (11)This score and our cost function Chave the same scale\ninvariance described in Eq. 10.\n3.4.2 Cold-start (out-matrix) recommendation\nCold-start (or out-matrix) recommendation is a task of rec-\nommendation on items which suffer from the cold-start\nproblem (on modality A or B). In this section, we take the\nexample of a cold-start problem on modality A, i.e., the\nsong has no information in the modality A (nobody has\nlistened to this song yet) but has tags associated to it. The\nfollowing remark would hold for a cold-start problem on\nmodality B.\nFor cold-start (out-matrix) recommendation the score is\ndeﬁned by:\nsA\nui=X\nkwA\nukdik=X\nkwA\nukhA\nikP\nlhA\nil: (12)\nContrary to in-prediction, we use Dand not HA=NAD\nsince the popularity in the modality A is close to zero for\nsongs with no information, i.e., nA\ni\u00190.\nThis score and the cost function Cdo not have the same\nscale invariance described in Eq. 10. In fact, if we denote\n\u0016wA\nuk=\u0015kwA\nukand\u0016hA\nik=\u0015khA\nik, we have:\n\u0016sA\nui=X\nk\u0016wA\nuk\u0016hA\nikP\nl\u0016hA\nil=sA\nuiP\nkhA\nikP\nk\u0015khA\nik=sA\nuici;(13)\nwhereci=P\nkhA\nikP\nk\u0015khA\nik.\nThis means that, if we want to rank the different scores\nsA\nui, we have to do it for a ﬁxed item. Therefore, to properly\nevaluate the cold-start problem for songs, we will propose\na ranked list of users (or tags), for a given item.\nFor a streaming company, it corresponds to obtaining\na ranked list of users which are likely to listen to this new\nsong, or a ranked list of tags which corresponds to the song.\n4. OPTIMIZATION\n4.1 Auxiliary function\nThe objective function Chas no closed-form minimum\nand is not convex. We use a MM algorithm [9] to reach\na local minimum. The MM algorithms start by design-\ning a majorizing surrogate Gof the objective function\nC(Z)\u0014G(Zj~Z)which is tight at the current value ~Z,\ni.e.,C(~Z) =G(~Zj~Z).\nWe use Jensen inequality on terms of the form\nlog(P\nixi). We deﬁne:\n\u001eA\nuik=~wA\nuk~dikP\nk~wA\nuk~dik; cA\nuik=yA\nui\u001eA\nuik; (14)\n\u001eB\nuik=~wB\nuk~dikP\nk~wB\nuk~dik; cB\nuik=yB\nui\u001eB\nuik: (15)794 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018It leads to the following upper-bound:\nG(Zj~D;~WA;~WB) (16)\n=X\nuik\u0002\n\u0000cA\nuiklog(wA\nuknA\nidik) +wA\nuknA\nidik\u0003\n+\rX\nvik\u0002\n\u0000cB\nviklog(wB\nvknB\nidik) +wB\nvknB\nidik\u0003\n+cst:\n4.2 Updates\nThe auxiliary function Gcan be optimized by using a block\ndescent algorithm. At each iteration, we optimize one la-\ntent variable, keeping all the others ﬁxed. This technique\nleads to four update rules described in the following.\n\u000fVariables WAandWB:\nwA\nuk P\nicA\nuikP\ninA\nidik;wB\nvk P\nicB\nvikP\ninB\nidik(17)\n\u000fVariables NAandNB:\nnA\ni P\nuyA\nuiP\nukwA\nukdik;nB\ni P\nvyB\nviP\nvkwB\nvkdik(18)\n\u000fVariable D:\ndik P\nucA\nuik+\rP\nvcB\nvik\nnA\niP\nuwA\nuk+\rnB\niP\nvwB\nvk(19)\nAs discussed in Section 3.3, we add a renormalization\nstep at the end of each iteration. The update is as follows:\n\u0012i=X\nkdik=I; (20)\nD \u0002\u00001D;NA NA\u0002;NB NB\u0002: (21)\n4.3 Algorithm\nThe complete algorithm is summarized in Algorithm 1.\nNote that the inference only requires browsing the non-\nzero datayA\nui>0andyB\nvi>0, during the update of the\nlocal variables cA\nuikandcB\nuik. Hence, our algorithm has the\nsame scalability as PMF, making it particularly well-suited\nfor processing huge sparse matrices, as it is the case in rec-\nommender systems (see Table 1).\nThe algorithm is stopped when the relative increment of\nthe cost function Cis lower than a chosen parameter \u001c.\n5. EXPERIMENTS\n5.1 Experimental Setup\n5.1.1 Datasets\nWe use two datasets extracted from the Million Song\nDataset (MSD) [2] and merge them on songs:\n\u000fThe Taste Proﬁle dataset provides listening counts of\n1M users on 380k songs [18]. We select a subset of\nthe users and pre-process the data to remove users\nand items with few information [16]. We keep only\nusers who listened to at least 20 songs, and songs\nwhich have been listened to by at last 20 users.Algorithm 1: MM Algorithm\nInput : YA,YB,K,\r\nInitialize: WA;WB;NA;NB;D\nrepeat\nfor each pair (u;i)such thatyA\nui>0: Eq. 14\nfor each pair (v;i)such thatyB\nvi>0: Eq. 15\nfor each user uand tagv: Eq. 17\nfor each item i: Eq. 18-19\nnormalization step: Eq. 21\nuntilCconverges ;\nTaste Proﬁle Last.fm\n#columns (songs) 15;667 15 ;667\n#rows (users or tags) 16;203 620\n#non-zeros 792;761 128 ;652\n%non-zeros 0:31% 1 :32%\nTable 1 . Datasets structure after pre-processing.\n\u000fThe Last.fm dataset provides tag labels for around\n500k songs. These tags were extracted from the\nLast.fm API [4]. Since the tags were collected via\nuser annotation, they are quite noisy. To avoid miss-\nlabeling in the train data, we pre-process it. We\nkeep only the 1000 most used tags in the whole\ndataset. For each couple song-tag, a conﬁdence rat-\ning is given by Last.fm, we keep only couples with\nconﬁdence higher than 10. Finally, we keep only\ntags which appears at least in 20 songs. The top 10\nof the tags in the dataset after the pre-processing are\nshown in Table 2.\nWe binarize the two datasets. Structure of both datasets is\ndescribed in Table 1.\n5.1.2 Evaluation metric: ranking prediction\nIn each experiment, we will propose a ranked list LofN\nitems (which can be songs, tags or users) and evaluate its\nquality w.r.t. a ground-truth relevance. For this, we calcu-\nlate the discounted cumulative gain (DCG) and its normal-\nized version, the NDCG:\nTags Occ. Tags Occ.\nrock 6703 electronic 2413\nalternative 4949 female vocalists 2407\nindie 4151 indie rock 2171\npop 3853 Love 1875\nalternative rock 2854 singer-songwriter 1786\nTable 2 . Occurences (Occ.) of the top tags in the dataset\nafter pre-processing.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 795Experiment OUT-A OUT-B IN-A\nScore NDCG@20 NDCG@200 NDCG@1\u0003NDCG@10 NDCG@100 NDCG\u0003\u0003\nP-coNMF0:0824\n\u00061:48e\u000050:122\n\u00061:33e\u000050:416\n\u00065:85e\u000040:266\n\u00061:59e\u000040:129\n\u00064:24e\u000060:286\n\u00062:82e\u00006\nH-coNMF0:0873\n\u00061:39e\u000050:131\n\u00062:21e\u000050:391\n\u00061:73e\u000040:264\n\u00061:00e\u000040:122\n\u00065:72e\u000060:283\n\u00062:96e\u00006\nKL-NMF . . . .0:163\n\u00065:36e\u000070:313\n\u00061:50e\u00007\nTable 3 . Performance of three models: P-coNMF, H-coNMF, KL-NMF, on three different tasks: out-matrix song recom-\nmendation (OUT-A), tag labeling (OUT-B), in-matrix recommendation (IN-A). Each algorithm is run 5 times, the mean\nand the variance of the NDCG metrics are displayed.\u0003NDCG@1 corresponds to the percentage of success on the ﬁrst\npredicted tag.\u0003\u0003NDCG is not truncated in this column, it is equivalent to chose N=I.\nDCG@N =NX\nn=1rel(n)\nlog2(n+ 1); (22)\nNDCG@N =DCG@N\nIDCG@N; (23)\nwhere rel(n)is the ground-truth relevance of the n-th item\nin the listL. In the following, rel(n) = 1 if the item is\nrelevant and rel(n) = 0 if not.\nThe denominator of the DCG penalizes relevant items\nwhich are at the end of the ranked list. It accounts for\nthe fact that a user will only browse the beginning of the\nlist, and will not pay attention to items which are ranked\nat the end. IDCG is the ideal DCG. It corresponds to the\nDCG score of an oracle which ranks perfectly the list, thus\nscaling the NDCG between 0 and 1.\n5.1.3 Compared methods\nFor each experiment, we will compare the performance of\nour model, proportional co-factorization NMF (P-coNMF)\nwith two other methods:\n\u000fKL-NMF, presented in Section 2.1. It can only be\nused for in-matrix prediction as it suffers from the\ncold-start problem.\n\u000fHard co-factorization (H-coNMF), presented in Sec-\ntion 2.2.1), that use KL-NMF algorithm on concate-\nnated matrix. For out-matrix prediction, we will use\na mask that indicates what columns are missing. The\nobjective function is then:\nC(W;H) =DKL(X\nYjX\nWHT);(24)\nwhere\nis the elementwise multiplication, and X\nis the mask. Note that the masked H-coNMF is ex-\npected to perform as good as soft coNMF with the\n`1\u0000norm, since it does not enforce common activa-\ntion for new songs.\nFor both methods, we chose K= 100 latent factors.\nThe hyperparameter is set such that \r=U\nV, which al-\nlows to compensate for the size difference between the two\ndatasets (V\u001cU).5.2 Cold-start recommendation\nIn this section, we evaluate our algorithm on cold-start rec-\nommendation tasks for both modalities A and B. For this,\nwe artiﬁcially replace columns of YAandYBby columns\nfull of zeros, in order to create the train datasets YA\ntrain\nandYB\ntrain. It leads to 10% of songs with only listening\ncounts information, 10% of songs with only tag informa-\ntion and 80% of songs with both informations. The re-\nmoved columns form the test datasets YA\ntestandYB\ntest.\nFor each song among the never-listened-to songs, we\nwant to ﬁnd a set of users that is likely to listen to it.\nWe train all the algorithms on YA\ntrainandYB\ntrain. For each\nsong, we create a ranked list of users based on the score\ndeﬁned in Section 3.4.2. We evaluate its relevance based\non the NDCG metrics with ground-relevance deﬁned by:\nrel(u;i) =1(yA\ntest;ui>0), where 1(x)is the indicator\nfunction which is equal to 1whenxis true and 0other-\nwise.\nSimilarly, for each song among the untagged songs,\nwe want to ﬁnd a set of tags that can annotate that\nsong. Then we propose a ranked list of tags and calcu-\nlate the NDCG score with ground-relevance deﬁned by:\nrel(v;i) =1(yB\ntest;vi>0).\nThe columns OUT-A and OUT-B of Table 3 present the\nresults of P-coNMF and H-coNMF on the two cold-start\nproblems. For recommending potential listeners (OUT-\nA), H-coNMF seems to be slightly better than our method.\nHowever, P-coNMF outperforms H-coNMF on tag label-\ning task. P-coNMF presents a success rate of 42% on the\nﬁrst predicted tag. This is an acceptable rate since the tag\ndataset is noisy: it has not been labeled by experts but by\nusers and presents some incoherences. For example, the\ntag ’Hip-Hop’ can also be written ’hip hop’. More details\non tag labeling are provided in Section 5.4. Contrary to\nH-coNMF, P-coNMF does not need a mask to know which\ncolumns are missing. Additionally, the scale variables NA\nandNBare able to explain different scalings of the same\nsong in the two datasets. This seems interesting because\nthe amount of listening counts and tags for the same song\nis often highly different.796 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018FACTOR #94 FACTOR #29 FACTOR #30\nTop tagsHip-Hop\nhip hop\nclassic\nrap\nGangsta Rapnew wave\npost-punk\nGuilty Pleasures\nintense\nPost punkexperimental\nExperimental Rock\nAvant-Garde\nnoise\nweird\nTop songs\nbased on HAEminem - “Mockingbird”\nEminem - “Without Me”\nKid Cudi - “Day ’N’ Nite”\nKid Cudi - “Up Up & Away”\nKid Cudi - “Cudi Zone”The Cure - “Boys Don’t Cry”\nThe Smiths - “There Is A Light [...]”\nThe Smiths - “This Charming Man”\nThe Smiths - “What Difference Does It Make?”\nWolfsheim - “Once In A Lifetime”Animal Collective - “Fireworks”\nSigur Ros - “Staralfur”\nSonic Youth - “Youth Against Fascism”\nGrizzly Bear - “Little Brother”\nTV On The Radio - “Crying”\nTop songs\nbased on DDMX - “Where The Hood At”\nLil Jon - “Crunk Juice”\n50 Cent - “Straight To The Bank”\nEminem - “The Kiss”\nThe Notorious B.I.G. - “Respect”New Order - “The Perfect Kiss”\nTalking Heads - “Burning Down The House”\nJoy Division - “Disorder”\nTears For Fears - “Goodnight Song”\nThe Smiths - “Miserable Lie”The Mars V olta - “Tira Me a Las Aranas”\nCocorosie - “Gallows”\nThe Mars V olta - “Concertina”\nThe Mars V olta - “Roulette Dares”\nTV On The Radio - “Golden Age”\nTable 4 . Three examples of factors, with, for both of them, the 5 top tags associated to it, the 5 top songs associated to it,\nwith or without the notion of popularity.\n5.3 In-matrix song recommendation\nWe also evaluate our algorithm on in-matrix prediction.\nThe goal is therefore to predict which songs a user is likely\nto listen. There is no cold-start recommendation here, and\nKL-NMF can be trained.\nWe artiﬁcially split the listening counts dataset in two.\n20% of non-zero values of YAare removed to create the\ntest setYA\ntest. The 80% remaining form the train set YA\ntrain\non which the different models are trained. Each method is\nevaluated with NDCG metric. For each user, a list of songs\nis proposed based on the score deﬁned in Section 3.4.1,\namong the songs he never listened to. The ground-truth\nrelevance is deﬁned by rel(u;i) =1(yA\ntest;ui>0).\nThe results are presented in the third column (IN-A) of\nTable 3. P-coNMF is slightly better than H-coNMF, but\nwe observe that KL-NMF achieves state-of-the-art perfor-\nmance. This is not surprising, since adding information on\nanother modality (tags here) can be viewed as a regulariz-\ning term. We lose in precision in in-matrix recommenda-\ntion task but we solve the cold-start problem. This seems\nan interesting trade-off.\n5.4 Exploratory Analysis\nIn Table 4, we present for each of the three factors\nk2f29;30;94g:\n\u000fin the ﬁrst row, the tags which corresponds to the ﬁve\nhighest values of WB.\n\u000fin the second row, the songs which corresponds to\nthe ﬁve highest values of HA=NAD.\n\u000fin the third row, the songs which corresponds to the\nﬁve highest values of D.\nThe top tags associated to each factor are consistent: for\nexample, genre as ’new wave’ and ’post-punk’ are in the\nsame factor. The model is also robust to the different\nspellings used by the users (’post-punk’ and ’Post punk’\nfor example). Then, we see that the top songs in each fac-\ntor are related with the top tags. Eminem, 50 Cent and TheNotorious B.I.G. are rap artists. The Cure, The Smiths and\nJoy Division are the leading ﬁgures of the new wave. TV\nOn The Radio, The Mars V olta and Animal Collective are\nknown to be experimental rock bands. Finally, we see that\nthe popularity of songs NAhas an important inﬂuence on\nthe diversity of the top songs in each factor. When this no-\ntion is removed (last row of the table), less popular songs\nand bands appear in the top songs.\n6. CONCLUSION\nIn this paper, we proposed a new Poisson matrix co-\nfactorization, in which the attributes of each modality are\nassumed proportional. Contrary to hard and `1-based soft\nco-factorization, in this new model each item may have\ndifferent scaling (or popularity) in each modality. This is\nof particular interest when tackling cold-start recommen-\ndation, in which one scaling is close to zero. The beneﬁts\nof the algorithm over standard co-factorization have been\nillustrated for song recommendation, with emphasis placed\non cold-start situations.\nThis raised interesting short-term perspectives, such as\nthe derivation of more involved Bayesian models, and\ninference or extensions to different, possibly non-binary\ndatasets. Future works should also consider datasets with\nhighly different dimensions or dynamics, by means of a\ntri-factorization.\n7. ACKNOWLEDGMENTS\nThis work has received funding from the European Re-\nsearch Council (ERC) under the European Unions Horizon\n2020 research and innovation program under grant agree-\nment No 681839 (project FACTORY).\n8. REFERENCES\n[1] Gediminas Adomavicius and Alexander Tuzhilin.\nContext-aware recommender systems. In Recom-\nmender Systems Handbook , pages 191–226. Springer,\n2015.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 797[2] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian\nWhitman, and Paul Lamere. The million song dataset.\nInProceedings of the 12th International Conference on\nMusic Information Retrieval (ISMIR) , 2011.\n[3] John Canny. GaP: A factor model for discrete data.\nInProc. International Conference on Research and\nDevelopment in Information Retrieval (SIGIR) , pages\n122–129, 2004.\n[4]`Oscar Celma. Music recommendation. In Music rec-\nommendation and discovery , pages 43–85. Springer,\n2010.\n[5] Ali Taylan Cemgil. Bayesian inference for nonnega-\ntive matrix factorisation models. Computational Intel-\nligence and Neuroscience , 2009.\n[6] O. Dikmen and C. Fevotte. Maximum marginal like-\nlihood estimation for nonnegative dictionary learning\nin the Gamma-Poisson model. IEEE Transactions on\nSignal Processing , 60(10):5163–5175, 2012.\n[7] Douglas Eck, Paul Lamere, Thierry Bertin-Mahieux,\nand Stephen Green. Automatic generation of social\ntags for music recommendation. In Advances in Neural\nInformation Processing Systems (NIPS) , pages 385–\n392, 2008.\n[8] Yi Fang and Luo Si. Matrix co-factorization for rec-\nommendation with rich side information and implicit\nfeedback. In Proc. International Workshop on Informa-\ntion Heterogeneity and Fusion in Recommender Sys-\ntems (HetRec) , pages 65–69, 2011.\n[9] C ´edric F ´evotte and J ´erˆome Idier. Algorithms for non-\nnegative matrix factorization with the \f-divergence.\nNeural computation , 23(9):2421–2456, 2011.\n[10] Prem Gopalan, Jake M. Hofman, and David M. Blei.\nScalable recommendation with hierarchical Poisson\nfactorization. In Proc. Conference on Uncertainty in\nArtiﬁcial Intelligence (UAI) , pages 326–335, 2015.\n[11] Prem K Gopalan, Laurent Charlin, and David Blei.\nContent-based recommendations with Poisson factor-\nization. In Advances in Neural Information Processing\nSystems (NIPS) , pages 3176–3184. 2014.\n[12] Y . Koren, R. Bell, and C. V olinsky. Matrix factoriza-\ntion techniques for recommender systems. Computer ,\n42(8):30–37, 2009.\n[13] Xuan Nhat Lam, Thuc Vu, Trong Duc Le, and Anh Duc\nDuong. Addressing cold-start problem in recommen-\ndation systems. In Proc. International Conference on\nUbiquitous Information Management and Communi-\ncation (IMCOM) , pages 208–211, 2008.\n[14] Daniel D. Lee and H. Sebastian Seung. Learning the\nparts of objects by non-negative matrix factorization.\nNature , 401(6755):788–791, 1999.[15] Daniel D. Lee and H. Sebastian Seung. Algorithms\nfor non-negative matrix factorization. In Advances in\nNeural Information Processing Systems (NIPS) , pages\n556–562. 2001.\n[16] Dawen Liang, Minshu Zhan, and Daniel PW Ellis.\nContent-aware collaborative music recommendation\nusing pre-trained neural networks. In Proc. Interna-\ntional Society for Music Information Retrieval (IS-\nMIR) , pages 295–301, 2015.\n[17] Hao Ma, Chao Liu, Irwin King, and Michael R. Lyu.\nProbabilistic factor models for web site recommenda-\ntion. In Proc. International Conference on Research\nand Development in Information Retrieval (SIGIR) ,\npages 265–274, 2011.\n[18] Brian McFee, Thierry Bertin-Mahieux, Daniel PW El-\nlis, and Gert RG Lanckriet. The million song dataset\nchallenge. In Proc. International Conference on World\nWide Web (WWW) , pages 909–916, 2012.\n[19] S. Rendle. Factorization machines. In Proc. Interna-\ntional Conference on Data Mining (ICDM) , pages\n995–1000, 2010.\n[20] Andrew I. Schein, Alexandrin Popescul, Lyle H. Un-\ngar, and David M. Pennock. Methods and metrics\nfor cold-start recommendations. In Proc. International\nConference on Research and Development in Informa-\ntion Retrieval (SIGIR) , pages 253–260, 2002.\n[21] N. Seichepine, S. Essid, C. F ´evotte, and O. Capp ´e. Soft\nnonnegative matrix co-factorization. IEEE Transac-\ntions on Signal Processing , 62(22):5940–5949, 2014.\n[22] Chong Wang and David M. Blei. Collaborative topic\nmodeling for recommending scientiﬁc articles. In Proc.\nInternational Conference on Knowledge Discovery\nand Data Mining (KDD) , pages 448–456, 2011.798 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Semi-supervised Lyrics and Solo-singing Alignment.",
        "author": [
            "Chitralekha Gupta",
            "Rong Tong",
            "Haizhou Li 0001",
            "Ye Wang 0007"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492487",
        "url": "https://doi.org/10.5281/zenodo.1492487",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/30_Paper.pdf",
        "abstract": "We propose a semi-supervised algorithm to align lyrics to the corresponding singing vocals. The proposed method transcribes and aligns lyrics to solo-singing vocals using the imperfect transcripts from an automatic speech recognition (ASR) system and the published lyrics. The ASR provides time alignment between vocals and hypothesized lyrical content, while the non-aligned published lyrics correct the hypothesized lyrical content. The effectiveness of the proposed method is validated through three experiments. First, a human listening test shows that 73.32% of our automatically aligned sentence-level transcriptions are correct. Second, the automatically aligned sung segments are used for singing acoustic model adaptation, which reduces the word error rate (WER) of automatic transcription of sung lyrics from 72.08% to 37.15% in an open test. Third, another iteration of decoding and model adaptation increases the amount of reliably decoded segments from 44.40% to 91.96% and further reduces the WER to 36.32%. The proposed framework offers an automatic way to generate reliable alignments between lyrics and solosinging. A large-scale solo-singing and lyrics aligned corpus can be derived with the proposed method, which will be beneficial for music and singing voice related research.",
        "zenodo_id": 1492487,
        "dblp_key": "conf/ismir/GuptaT0W18",
        "keywords": [
            "semi-supervised",
            "lyrics",
            "singing",
            "transcription",
            "alignment",
            "ASR",
            "published",
            "lyrics",
            "human",
            "listening"
        ],
        "content": "SEMI-SUPERVISED LYRICS AND SOLO-SINGING ALIGNMENT\nChitralekha Gupta1;2Rong Tong4Haizhou Li3Ye Wang1;2\n1NUS Graduate School for Integrative Sciences and Engineering,2School of Computing,\n3Electrical and Computer Engineering Dept., National University of Singapore, Singapore\n4Alibaba Inc. Singapore R&D Center, Singapore\nchitralekha@u.nus.edu, rong.tong@alibaba-inc.com, haizhou.li@nus.edu.sg,\nwangye@comp.nus.edu.sg\nABSTRACT\nWe propose a semi-supervised algorithm to align lyrics to\nthe corresponding singing vocals. The proposed method\ntranscribes and aligns lyrics to solo-singing vocals using\nthe imperfect transcripts from an automatic speech recog-\nnition (ASR) system and the published lyrics. The ASR\nprovides time alignment between vocals and hypothesized\nlyrical content, while the non-aligned published lyrics cor-\nrect the hypothesized lyrical content. The effectiveness\nof the proposed method is validated through three exper-\niments. First, a human listening test shows that 73.32% of\nour automatically aligned sentence-level transcriptions are\ncorrect. Second, the automatically aligned sung segments\nare used for singing acoustic model adaptation, which re-\nduces the word error rate (WER) of automatic transcrip-\ntion of sung lyrics from 72.08% to 37.15% in an open\ntest. Third, another iteration of decoding and model adap-\ntation increases the amount of reliably decoded segments\nfrom 44.40% to 91.96% and further reduces the WER to\n36.32%. The proposed framework offers an automatic way\nto generate reliable alignments between lyrics and solo-\nsinging. A large-scale solo-singing and lyrics aligned cor-\npus can be derived with the proposed method, which will\nbe beneﬁcial for music and singing voice related research.\n1. INTRODUCTION\nLyrics serve as an important component of music, that of-\nten deﬁnes the mood of the song [2, 4], affects the opin-\nion of a listener about the song [3], and even improves\nthe vocabulary and pronunciation of a foreign language\nlearner [14, 30]. Research in Music Information Retrieval\n(MIR) in the past has explored tasks involving lyrics such\nas automatic lyrics recognition [15, 19, 26, 28] and auto-\nmatic lyrics alignment [5, 11, 27] for various applications\nsuch as karaoke singing, song subtitling, query-by-singing\nas well as acoustic modeling for singing voice. In spite of\nc\rChitralekha Gupta, Rong Tong, Haizhou, Ye Wang. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Chitralekha Gupta, Rong Tong, Haizhou,\nYe Wang. “Semi-supervised lyrics and solo-singing alignment”, 19th In-\nternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.huge advances in speech technology, automatic lyrics tran-\nscription and alignment in singing face challenges due to\nthe differences between sung and spoken voices [11, 26],\nand a lack of transcribed singing data to train phonetic\nmodels for singing [11, 15, 26–28].\nAs singing and speech differ in many ways such as pitch\ndynamics, duration of phonemes, and vibrato [11, 26], the\ndirect use of ASR systems for lyrics alignment or transcrip-\ntion of singing voice will result in erroneous output. There-\nfore, speech acoustic models need to be adapted to singing\nvoice [27]. For training singing-adapted acoustic models,\nlyrics-aligned singing dataset is necessary. Lack of anno-\ntated singing datasets has been a bottleneck for research\nin this ﬁeld. Duan et al. [8] published a small singing\ndataset (1.92 hours) with phone-level annotations, which\nwere done manually that requires a lot of time and effort,\nand is not scalable. One way of getting data for training\nis to force-align the lyrics with singing using speech mod-\nels, and use this aligned singing data for model training\nand adaptation. But due to the differences in speech and\nsinging acoustic characteristics, alignment of lyrics with\nspeech acoustic models will be prone to errors, that will\nresult in badly adapted singing acoustic models.\nWith the increase in popularity of mobile phone karaoke\napplications, singing data collected from such apps are be-\ning made available for research. Smule’s Sing! karaoke\ndataset, called Digital Archive of Mobile Performances\n(DAMP) [33], is one such dataset that contains more than\n34K a capella (solo) singing recordings of 301 songs. But\nit does not have time-aligned lyrics, although the textual\nlyrics are available on Smule’s website. The data also\ncontains inconsistencies in recording conditions, out-of-\nvocabulary words, and incorrectly pronounced words be-\ncause of unfamiliar lyrics or non-native language speakers.\nAlthough the presence of such datasets is a huge boon to\nMIR research, we need tools to further clean up such data\nto make them more usable. There is a need for aligned\nlyrics transcriptions for singing vocals while also eliminat-\ning inconsistent or noisy recordings. To address this need,\nwe propose a simple yet effective solution to produce clean\naudio segments with aligned transcriptions.\nIn this work, we study a strategy to obtain time-aligned\nsung-lyrics dataset with the help of the state-of-the-art\nASR as well as an external resource, i.e. published lyrics.600We use the speech acoustic models to transcribe solo-\nsinging audio segments, and then align this imperfect tran-\nscription with the published lyrics of the song to obtain\na better transcription of the sung segments. We hypoth-\nesize that this strategy will help in correcting the imper-\nfect transcriptions from the ASR module and in cleaning\nup bad audio recordings. We validate our hypothesis by\na human listening experiment. Moreover we show that\na semi-supervised adaptation of speech acoustic models\nwith this cleaned-up annotated dataset results in further\nimprovement in alignment as well as transcription, itera-\ntively. Hence, such an algorithm will potentially automate\nthe labor-intensive process of time aligning lyrics such as\nin karaoke or MTV . Furthermore, it will enable large-scale\nsinging transcription generation, thus increasing the scope\nof research in music information retrieval. We have ap-\nplied our algorithm on a subset of the DAMP dataset, and\nhave published the resulting dataset and code1.\n2. RELATED WORK\nOne of the traditional methods of aligning lyrics to music\nis with the help of the timing information from the musical\nstructure such as chords [17, 24, 25, 35], and chorus [21],\nbut such methods are more suitable for singing in the pres-\nence of background accompaniments. Another study uses\nmusical score to align lyrics [13], but such methods would\nbe applicable for professional singing where the notes are\ncorrectly sung. In karaoke applications, as addressed in\nthis work, correctness of notes is less likely.\nOne of the pioneering studies of applying speech recog-\nnition for lyric alignment was by Mesaros and Virtanen\n[27], who used 49 fragments of songs, 20-30 seconds long,\nalong with their manually acquired transcriptions to adapt\nGaussian Mixture Model-Hidden Markov Model (GMM-\nHMM) speech models for singing in the same way as\nspeaker adaptation is done. They then used these singing-\nadapted speech models to align vocal sections of songs\nwith their manually paired lyrics lines using the Viterbi\nalgorithm. In [28], the authors used the same align-\nment method to automatically obtain the singing-to-lyrics\naligned lines, and then explored multiple model adaptation\ntechniques, to report the best phoneme error rate (PER)\nof 80%. This work has provided a direction for solving the\nproblem of lyrics alignment and recognition in singing, but\nit suffers from manual post-processing and the models are\nbased on a small number of annotated singing samples.\nRecently, with the availability of more singing data, a\nsubset of the DAMP solo-singing dataset was used for the\ntask of sung phoneme recognition by Kruspe [19, 20]. In\nthis work, the author builds new phonetic models trained\nonly on singing data (DAMP data subset) and compares\nit with a pitch-shifted, time-stretched, and vibrato-applied\nversion of a speech dataset called songiﬁed speech data\nTimitM [18]. Their best reported PER was 80%, and\nweighted PER (that gives 0.5 weights to deletions and in-\n1Dataset: https://drive.google.com/open?\nid=1hGuE0Drv3tbN-YNRDzJJMHfzKH6e4O2A ;\nCode: https://github.com/chitralekha18/\nAutomaticSungLyricsAnnotation_ISMIR2018.git\nFigure 1 : The diagram of lyrics to singing vocal alignment\nalgorithm.\nsertions) was 56%, using the DAMP data subset, which\noutperformed the songiﬁed dataset. This work shows an\neffective use of the available (unannotated) singing data to\nbuild improved singing phonetic models. But there is still\nroom for improvement.\nThe ﬁrst step in Kruspe’s work was to obtain aligned\nlyrics annotations of every song, for which the whole lyrics\nof a song was force-aligned with the audio using speech-\ntrained models. These force-aligned sung phonemes were\nthen used to build the new acoustic phonetic models for\nsinging. This approach of forced-alignment of singing\nusing speech acoustic models has also been applied in\nthe earlier attempts of automatic lyrics alignment in a\ncapella singing as well as in singing with background mu-\nsic [11, 16, 17, 35]. But, as noted by Kruspe [19], forced-\nalignment of singing with speech models causes unavoid-\nable errors, because of the mismatch between speech and\nsinging acoustic characteristics [10,23], as well as the mis-\nmatch between the actual lyrics and what the singer sings.\nThus, the lack of appropriate lyrics-aligned song dataset\nand the eventual use of forced-alignment with speech mod-\nels to obtain this annotation is a source of errors.\n3. SEMI-SUPERVISED LYRICS AND SINGING\nVOCALS ALIGNMENT ALGORITHM\nWe propose an algorithm to align lyrics to singing vocals,\nthat consists of two main steps: dividing the singing vo-\ncals into shorter segments (Segmentation), and obtaining\nthe aligned lyrics for each segment (Lyrics Matching). Fig-\nure 1 shows the overview of our algorithm.\n3.1 Segmentation\nOne way to automatically align the published lyrics with\na solo-singing audio is to force-align the lyrics with the\nfull rendition audio (2 to 4 minutes long) using speech\ntrained acoustic models, as discussed in [19]. However,\nthe Viterbi alignment algorithm used in forced-alignment,\nfails to scale well for long audio segments leading to accu-\nmulated alignment errors [29]. In our singing-lyrics tran-\nscription and alignment algorithm, we propose to ﬁrst di-\nvide the audio into shorter segments such that the ASR is\nless prone to the alignment errors. We ﬁnd silent regions\nin the rendition by imposing constraints on the magnitude\nof the short time energy and the silence duration (Algo-\nrithm 1). The center of these silent regions are markedProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 601as boundaries of non-silent sub-segments. Such non-silent\nsub-segments are of varying lengths. So we stitch con-\nsecutive sub-segments together to make segments of \u001810\nseconds duration. We also add silence samples before and\nafter every such segment so that the ASR has some time to\nadapt to the utterance and start recognition in the beginning\nof the utterance, and to avoid abrupt termination at the end\nof the utterance.\nAlgorithm 1 Segmentation algorithm\n1:Calculate short time energy Efor 32 ms window with 16 ms\nhop\n2:ifE > 0:1\u0002mean (E)is true then\n3: non-silent region\n4:else\n5: silent region\n6:end if\n7:ifsilent region duration >=200 ms then\n8: valid silence region\n9: center of this region marks the boundary\n10:else\n11: invalid silent region\n12:end if\n13:sub-segment = boundary-to-boundary region\n14:segment = stitch together such sub-segments for \u001810s dura-\ntion\n15:add 2s silence before and after every segment, to improve\nASR performance\n3.2 Lyrics Matching\nWe would like to obtain the best possible lyrics transcrip-\ntion for these short singing segments. Moreover, to obtain\na clean transcribed dataset of singing vocals, we would also\nlike to reject the noisy audio segments that contain out-\nof-vocabulary, incorrectly pronounced words, and back-\nground noise. We use ASR to decode these segments be-\ncause such ASR transcription ideally suggests words that\nare actually sung and different from the published lyrics.\nThe ASR transcription also help detect erroneous pronun-\nciations, reject noise segments. We understand that the\nthe state-of-the-art ASR is not perfect, and for singing it\nis even more unreliable, as the ASR is trained on speech\nwhile singing is acoustically different from speech. So we\ndesigned an algorithm to overcome these imperfections of\nthe ASR. This algorithm produces time-aligned transcrip-\ntions of clean audio segments with the help of the pub-\nlished lyrics.\nAlgorithm 2 Lyrics Matching algorithm\n1:XN\u00025s.t.xi;j=e\nwhere, X= error matrix,\nN= number of words in published lyrics,\ne= ratio of number of errors obtained from Levenshtein dis-\ntance between ASR output and published lyrics window, to\nthe total number of words in the lyrics window\n2:imin; jmin= argmin X\nwhere imin = minimum distance transcription start index in\nlyrics,\nwhere jmin= minimum distance transcription slack window\nsize\n3:transcription = lyrics[ imin:imin+M+jmin]\nwhere, Mis the number of words in ASR transcription3.2.1 ASR Transcription of Lyrics\nTo obtain the transcription of each of the audio segments,\nwe use the Google speech-to-text API package in python\n[36] that transcribes a given audio segment into a string of\nwords, and gives a set of best possible transcriptions. We\ncompare the top ﬁve of these transcriptions with the pub-\nlished lyrics of the song, and select the one that matches\nthe most, as described in Algorithm 2. The idea is that the\nASR provides a hypothesis of the aligned lyrics although\nimperfect, and the published lyrics helps in checking these\nhypothesized lyrics, and retrieving the correct lyrics. Also,\nwe use the Google ASR to bootstrap, with a plan to im-\nprove our own ASR (as discussed further in Section 4.2).\nDifferent ASR systems have different error patterns, there-\nfore we expect that the Google ASR would boost the per-\nformance of our ASR. We use the Google ASR only for\nbootstrapping, the rest of the experiments use our own\nASR. Below is the description of the lyrics-matching al-\ngorithm.\nFor an ASR output of length M words, we took a lyrics\nwindow of size M, and also empirically decided to provide\na slack of 0 to 4 words, i.e. the lyrics window size could\nbe of length M to M+4. This slack provides room for ac-\ncommodating insertions and deletions in the ASR output,\nthus allowing improvement in the alignment. So, starting\nfrom the ﬁrst word of the published lyrics, we calculate the\nLevenshtein distance [22] between the ASR output and the\nlyrics window of different slack sizes, iterated through the\nentire lyrics by one word shifts. This distance represents\nthe number of errors (substitutions, deletions, insertions)\noccurred in ASR output with respect to the actual lyrics.\nFor the lyrics of a song containing a total of N words,\nwe obtain an error matrix Xof dimensions Nx5, where 5 is\nthe number of slack lyric window sizes ranging from M to\nM+4. Each element eof the matrix is the ratio of the num-\nber of errors obtained from Levenshtein distance between\nthe ASR output and the lyrics window, to the total num-\nber of words in that lyrics window. If (imin; jmin )is the\ncoordinate of the minimum error element of this matrix,\nthenimin is the starting index of the minimum distance\nlyrics transcription, jmin is the slack lyric window size.\nAmongst the top ﬁve ASR outputs, we choose the one that\ngives minimum error e, and select the corresponding lyrics\nwindow from the error matrix to obtain the best lyrics tran-\nscription for that audio segment. We illustrate this with the\nhelp of the following example.\nLet’s assume that the ASR transcription of an audio seg-\nment is “the snow glows on the mountain” , therefore M=6.\nThe slack window size will range from 6 to 10 words. The\nlyrics of this song contains a total of N words, where a\nword sub-sequence is “the snow glows white on the moun-\ntain tonight not a footprint to be seen... ” . The correspond-\ning error matrix Xis shown in Figure 2. The error element\ne1;2is the distance between the ASR transcription and the\nslack lyric window “the snow glows white on the moun-\ntain” which is 1. The error element e2;1is the distance\nbetween the ASR transcription and the slack lyric window\n“snow glows white on the mountain” which is 2, and so on.602 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 2 : Example of an error matrix Xwhere the ASR\ntranscript is “the snow glows on the mountain” , and the\npublished lyrics of this song has N words where a word\nsub-sequence is “the snow glows white on the mountain\ntonight not a footprint to be seen... ” .\nANCHOR SEGMENT ANCHOR SEGMENT NON-ANCHOR SEGMENT Published lyrics ASR output \nFigure 3 : Anchor and non-anchor segments of a song\nbased on sung-lyrics alignment algorithm. Anchor seg-\nments: ASR output and lyrics reliably match; Non-Anchor\nsegments: ASR output and lyrics do not match.\nSo in this example, (imin; jmin )is (1,2), i.e. the best lyrics\ntranscription is “the snow glows white on the mountain”.\n3.2.2 Anchor and Non-Anchor Segments\nFrom our preliminary study, we found that many of the\nASR transcriptions had missing words because either the\naudio contained background noise or there were incor-\nrectly pronounced words or deviation of singing acoustics\nfrom speech. For example, a 10 seconds long non-silent\nsegment from a popular English song would rarely ever\nhave as few as four or ﬁve words. In order to retrieve more\nreliable transcripts, we added a constraint on the number\nof words, as described below.\nTo check the reliability of the lyrics transcriptions, we\nmarked the best lyrics transcriptions of a small subset of\n360 singing segments as correct or incorrect, depending\non whether the transcription matched with the audio. We\nfound that all those segments for which the best lyrics tran-\nscription had less than 10 words were more likely to be in-\ncorrect matches, as shown in Figure 4. The segment tran-\nFigure 4 : The number of audio segments with correct tran-\nscription (blue) or incorrect transcription (cyan) according\nto human judgment on y-axis versus the number of words\nin the transcription of an audio segment on x-axis. We set\n10 words as the minimum threshold for a transcription to\nbe valid for an approximately 10-seconds long segment.scriptions were 94.0% times incorrect (235 incorrect out\nof 250 total number of segments) when they contained less\nthan 10 words, while they were 57.3% times incorrect (63\nout of 110) when they contained more than or equal to 10\nwords. So we empirically set 10 words as the threshold for\nselecting reliable audio segments and transcriptions. By\napplying this constraint, we reject those audio segments\nthat are noisy, or have wrongly pronounced words, or cause\nerrors in transcription because of model mismatch, thus de-\nriving a clean transcribed singing dataset.\nThe audio segments with reliable transcription are la-\nbeled as Anchor segments , and the audio segment(s) be-\ntween two anchor segments that have unreliable transcrip-\ntion, are strung together and labeled as Non-Anchor seg-\nments , as illustrated in Figure 3.\nOne may argue that we could have used the error score\neto evaluate the reliability of a segment. However, if the\nASR output itself is wrong, then this lyrics-matching error\nscore will be misleading. For example, if only 4 words get\ndetected by the ASR, out of 12 words in the audio segment,\nand all the 4 words are correct according to the published\nlyrics, then ewill be zero for this transcription, which is\nincorrect, and also undetectable. Thus we set a threshold\non the number of detected words (i.e. 10 words) as a way to\nmeasure the reliability of the segment and its transcription.\n4. EXPERIMENTS AND RESULTS\nIn order to validate our hypothesis that our algorithm can\nretrieve good quality aligned transcriptions, we conducted\nthree experiments: A) Human veriﬁcation of the quality of\nthe aligned transcriptions through a listening test, B) Semi-\nsupervised adaptation of speech models to singing using\nour aligned sung-lyrics transcriptions for assessing the per-\nformance of automatic lyrics recognition, and C) Second\niteration of alignment, and re-training of acoustic models,\nto check for further improvement in lyrics recognition.\nOur experiments are conducted on 6,000 audio record-\nings from the DAMP dataset that was used by Kruspe [19].\nThe list of recordings used by Kruspe is here [1], how-\never the training and test subsets are not clearly marked.\nTherefore we have deﬁned our training and test datasets,\nand they are subsets of Kruspe’s dataset, as discussed in\nthe following subsections. This data set contains record-\nings of amateur singing of English language pop songs\nwith no background music but different recording condi-\ntions, which were obtained from the Smule Sing! karaoke\napp. Each performance is labeled with metadata such as\nthe gender of the singer, the region of origin, the song title,\netc. We obtained the textual lyrics of the songs from Smule\nSing! website [34]. Since the songs in DAMP dataset were\nsung on Smule Sing! Karaoke app that uses these lyrics, it\nis safe to assume that these were the intended lyrics.\n4.1 Experiment 1: Human Veriﬁcation of the Quality\nof the Aligned-Transcriptions\nIn this experiment, we evaluate the quality of our aligned\ntranscriptions ( segment transcriptions ), by asking partici-\npants to listen to the audio segments and verify if the givenProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 603transcription for the segment is correct or not. As opposed\nto word intelligibility evaluation tasks [6] where partici-\npants are asked to transcribe after listening to the stimuli\nonce, in this task the participants were provided with the\ntranscription and were free to listen to the audio as many\ntimes as they needed. Also the songs were popular English\nsongs, that are less prone to perception errors [7].\n4.1.1 Dataset\nBy applying our lyrics transcription and alignment algo-\nrithm (see Section 3), we obtained 19,873 of anchor seg-\nments ( \u001858 hours) each \u001810 seconds long, out of which\nwe asked humans to validate 5,400 (15 hours) anchor seg-\nment transcriptions through a listening test. The only crite-\nrion to qualify for the listening test was to be proﬁcient in\nEnglish language. 15 university graduate students were the\nhuman listeners. Every listener was given one hour of au-\ndio segments containing 360 anchor segments along with\nthe obtained lyrics transcription. The task was to listen to\neach of the audio segments and compare the given tran-\nscription with the audio. If at least 90% of the words in the\ntranscription match with that in the audio, then the audio\nsegment was marked as correctly transcribed. If not, then\nit was marked as incorrectly transcribed.\nSimilarly, we also tested the quality of the non-anchor\nsegments. Non-anchor segments could be of varying dura-\ntions, greater than or equal to 10 seconds. We conducted\nthe same human validation task for 2 hours (1,262 seg-\nments) of the non-anchor segments of different durations.\n4.1.2 Results and Discussion\nThere were two types of successful segment transcriptions,\none was veriﬁed by humans as correct and also matched\nperfectly with the ASR output, and was labeled as correct\ntranscriptions fully matching ASR . Another was veriﬁed as\ncorrect by humans but did not match with the ASR output\ndue to ASR errors, but our algorithm could successfully re-\ntrieve the correct transcriptions, that we call correct tran-\nscriptions partially matching ASR . And the ones that were\nveriﬁed as wrong by humans are labeled as error transcrip-\ntions due to imperfect ASR or incorrect singing .\nAnchor Segments: Table 1 shows the validation results\nfor the anchor segments. We found that a total of 73.32%\nof the segments were transcribed correctly, where 57.80%\nof the segments were partially matching ASR . This means\nthat our algorithm could successfully retrieve many incor-\nrect ASR transcriptions, which validates our hypothesis\nthat the extra information provided by the published lyrics\ncoupled with ASR decoding produces good aligned tran-\nscriptions. We also found that incorrect singing of lyrics\nand imperfect ASR output resulted in 26.68% erroneous\ntranscriptions. A common error reported by the listeners\nwas many missing words at the trailing end of the incor-\nrectly aligned transcriptions, although the correct words\nwere clearly audible, which is possibly a result of model\nmismatch between singing and speech.\nNon-Anchor Segments: From the human validation of the\nnon-anchor segments, we ﬁnd that 62.07% of the total of\n1,262 non-anchor segments transcriptions are correct. ThisSegment Transcriptions # % Total %\nCorrect transcriptions\nfully matching ASR 838 15.5273.32Correct transcriptions\npartially matching ASR 3,121 57.80\nError transcriptions due to\nimperfect ASR or incorrect singing 1,441 26.68 26.68\nTable 1 : A summary of correct and error transcriptions by\nthe proposed algorithm. Google ASR is used for singing\ntranscription. Total # anchor segments = 5,400 (15 hours).\nsuggests that these segments are relatively less reliable.\nMoreover, these audio segments could be long in duration\n(even more than a minute) that would cause errors in the\nViterbi alignments. Thus in the subsequent experiments,\nwe only use the anchor segments.\n4.2 Experiment 2: Lyrics Transcription with\nSinging-Adapted Acoustic Models\nIn this experiment, we use our automatically generated\naligned-transcriptions of sung audio segments in a semi-\nsupervised adaptation of the speech models for singing.\nWe use these singing-adapted models in an open test to\nvalidate our hypothesis that better aligned transcriptions\nfor training singing acoustic models will result in improve-\nment in automatic lyrics recognition compared to the best\nknown baseline from the literature.\nAdaptation of speech models for singing was previ-\nously attempted by Mesaros et al. [27, 28] who applied\nthe speaker adaptation techniques to transform speech rec-\nognizer to singing voice. To reduce the mismatch be-\ntween singing and speech, they used constrained maxi-\nmum likelihood linear regression (CMLLR) to compute a\nset of transformations to shift the GMM means and vari-\nances of the speech models so that the resulting models\nare more likely to generate the adaptation singing data. In\nour work, we use CMLLR (also known as feature-space\nmaximum likelihood linear regression (fMLLR)) [32] and\nour lyrics-aligned anchor segments to compute transforma-\ntions for a semi-supervised adaptation of the speech mod-\nels to singing. Adaptation can be done with the test dataset\nonly, or the adaptation transformations can be applied at\nthe time of training, called speaker adaptive training (SAT).\nLiterature shows that the use of SAT with fMLLR trans-\nform requires minimum alteration to the standard code for\ntraining [12], and thus is a popular tool for speaker adapta-\ntion that we have used for singing adaptation here.\n4.2.1 Dataset\nThe singing train set consists of 18,176 singing anchor seg-\nments from 2,395 singers while the singing test set con-\nsists of 1,697 singing anchor segments of 331 singers.\nThe training set consists of both human veriﬁed and non-\nveriﬁed anchor segments, while the test set consists of only\nthose anchor segment transcriptions that are veriﬁed as cor-\nrect by humans. All of these anchor segments (training\nand test) are of \u001810 seconds duration. There is no speaker\noverlap between the acoustic model training and test sets.\nA language model is obtained by interpolating a speech\nlanguage model trained from Librispeech [31] text and a604 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Models Adapted by Singing Data %WER %PER\n(1) Baseline (speech acoustic models) 72.08 57.52\n(2) Adapted with test data 47.42 39.34\n(3) Adapted (SAT) with training data 40.25 33.18\n(4) Adapted (SAT+DNN) with training data 37.15 31.20\n(5) Repeat (3) and (4) for 2nd round 36.32 28.49\nTable 2 : The sung word and phone error rate (WER and\nPER) in the lyrics recognition experiments with the speech\nacoustic models (baseline) and the singing-adapted acous-\ntic models, on 1,697 correctly transcribed test singing an-\nchor segments.\nlyric language model trained from lyrics of the 301 songs\nof the DAMP dataset. The same language model is used in\nall the recognition experiments.\n4.2.2 Results and Discussion\nTable 2 reports the automatic lyrics recognition results on\nthe singing test set using different acoustic models to ob-\nserve the effect of adapting speech models for singing us-\ning our sung segments with aligned transcriptions.\nThe baseline speech acoustic model is a tri-phone HMM\nmodel trained on Librispeech corpus using MFCC fea-\ntures. Due to the mismatch between speech and singing\nacoustic characteristics, the WER and PER are high (Ta-\nble 2 (1)). Adapting the baseline model with the singing\ntest data results in a signiﬁcant improvement in the error\nrates (Table 2 (2)). Speaker adaptive training (SAT) fur-\nther improves the recognition accuracy (Table 2 (3)). A\nDNN model [9] is trained on top of the SAT model with\nthe same set of training data. During DNN training, tem-\nporal splicing is applied on each frame with left and right\ncontext window of 4. The SAT+DNN model has 3 hidden\nlayers and 2,976 output targets. With DNN training, the\nWER is reduced by about 7.7% relative to the SAT model\n(Table 2 (4)) and PER is 31.20%.\nMesaros et al. [27] reported the best PER to be 80%\nwith speech models adapted to singing, while Kruspe [19]\nreported the best PER to be 80% and weighted PER to be\n56% with pure singing phonetic models trained on a subset\nof the DAMP dataset. Compared to [19] and [27], our re-\nsults show a signiﬁcant improvement, which is attributed\nto three factors. One is that leveraging on ASR along\nwith the published lyrics as an external resource to validate\nand clean-up the transcriptions has led to better aligned\ntranscriptions for training. Two, our automatic method\nfor generating aligned transcriptions for singing provides\nus with a much larger training dataset. And three, the\nsegment-wise alignment is more accurate than the whole-\nsong forced-aligned with the speech acoustic models.\n4.3 Experiment 3: Alignment with Singing-Adapted\nAcoustic Models and Re-training\nWe would like to test if the singing-adapted acoustic mod-\nels can provide more number of reliably aligned transcrip-\ntions in a second round of alignment. Moreover whether\nre-training the models with this second round of transcrip-\ntions lead to better lyrics recognition.Model # anchor total # segments % anchor\nGoogle ASR 5,400 12,162 44.40\nAdapted (DNN) with\ntraining data 11,184 12,162 91.96\nTable 3 : Comparing the number of anchor segments ob-\ntained from the proposed transcription and alignment algo-\nrithm using Google ASR and the singing-adapted models.\n4.3.1 Dataset\nWe used the singing-adapted models obtained in Experi-\nment 2 to decode 12,162 segments, and then applied our\nlyrics-alignment algorithm to obtain new anchor and non-\nanchor segments. For comparison, we obtained the same\nfrom the Google ASR on the same dataset.\n4.3.2 Results and Discussion\nTable 3 shows that the number of anchor segments with\nthe new models have increased from 44.40% with Google\nASR to 91.96% with the singing-adapted models, which\nmeans that the number of reliable segment transcriptions\nhave increased signiﬁcantly. With the new anchor seg-\nments, we re-train our singing-adapted acoustic models.\nTable 2 (5) shows the free-decoding results after this sec-\nond round of training. The WER and PER have dropped\nfurther to 36.32% and 28.49% respectively .\nThe results of this experiment are promising as they\nshow iterative improvement in the quality of our alignment\nand transcription. This means that we can apply the fol-\nlowing strategy: use only the reliably aligned segments\nfrom the Google ASR to adapt acoustic models for singing,\nand use these models to improve the quality of alignment\nand transcription, and then again use the reliable segments\nfrom the improved alignments for further adaptations.\n5. CONCLUSIONS\nWe propose an algorithm to automatically obtain time-\naligned transcriptions for singing by using the imperfect\ntranscriptions from the state-of-the-art ASR along with the\nnon-aligned published lyrics. Through a human listen-\ning test, we showed that the extra information provided\nby the published lyrics helps to correct many incorrect\nASR transcriptions. Furthermore, using the time-aligned\nlyrics transcriptions for iterative semi-supervised adapta-\ntion of speech acoustic models for singing shows signiﬁ-\ncant improvement in automatic lyrics transcription perfor-\nmance. Thus our strategy to obtain time-aligned transcrip-\ntions for large-scale singing dataset is useful to train im-\nproved acoustic models for singing.\nOur contribution provides an automatic way to obtain\nreliable lyrics transcription for singing, that results in an\nannotated singing dataset. Lack of such datasets has been\na bottleneck in the ﬁeld of singing voice research in MIR.\nThis will not only generate lyrics transcription and align-\nment for karaoke and subtitling applications, but also pro-\nvide reliable data to improve acoustic models for singing,\nthus widening the scope of research in MIR.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 6056. REFERENCES\n[1] MIREX 2017. 2017 Automatic Lyrics-to-Audio Align-\nment. http://www.music-ir.org/mirex/\nwiki/2017:Automatic_Lyrics-to-Audio_\nAlignment . [Online; accessed 15-March-2018].\n[2] S Omar Ali and Zehra F Peynircio ˘glu. Songs and emo-\ntions: are lyrics and melodies equal partners? Psychol-\nogy of Music , 34(4):511–534, 2006.\n[3] Bruce Anderson, David G Berger, R Serge Denisoff,\nK Peter Etzkorn, and Peter Hesbacher. Love negative\nlyrics: Some shifts in stature and alterations in song.\nCommunications , 7(1):3–20, 1981.\n[4] Elvira Brattico, Vinoo Alluri, Brigitte Bogert, Thomas\nJacobsen, Nuutti Vartiainen, Sirke Nieminen, and Mari\nTervaniemi. A functional mri study of happy and sad\nemotions in music with and without lyrics. Frontiers in\npsychology , 2, 2011.\n[5] Yu-Ren Chien, Hsin-Min Wang, and Shyh-Kang Jeng.\nAlignment of lyrics with accompanied singing audio\nbased on acoustic-phonetic vowel likelihood modeling.\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , 24(11):1998–2008, 2016.\n[6] Lauren B Collister and David Huron. Comparison of\nword intelligibility in spoken and sung phrases. 2008.\n[7] Nathaniel Condit-Schultz and David Huron. Catching\nthe lyrics: intelligibility in twelve song genres. Music\nPerception: An Interdisciplinary Journal , 32(5):470–\n483, 2015.\n[8] Zhiyan Duan, Haotian Fang, Bo Li, Khe Chai Sim,\nand Ye Wang. The nus sung and spoken lyrics corpus:\nA quantitative comparison of singing and speech. In\nSignal and Information Processing Association Annual\nSummit and Conference (APSIPA), 2013 Asia-Paciﬁc ,\npages 1–9. IEEE, 2013.\n[9] G. Hinton et al. Deep neural networks for acoustic\nmodeling in speech recognition. In IEEE Signal Pro-\ncessing Magazine , volume 29, pages 82–97, 2012.\n[10] Hiromasa Fujihara and Masataka Goto. Lyrics-to-\naudio alignment and its application. In Dagstuhl\nFollow-Ups , volume 3. Schloss Dagstuhl-Leibniz-\nZentrum fuer Informatik, 2012.\n[11] Hiromasa Fujihara, Masataka Goto, Jun Ogata, and\nHiroshi G Okuno. Lyricsynchronizer: Automatic syn-\nchronization system between musical audio signals and\nlyrics. IEEE Journal of Selected Topics in Signal Pro-\ncessing , 5(6):1252–1261, 2011.\n[12] Mark JF Gales. Maximum likelihood linear transfor-\nmations for hmm-based speech recognition. Computer\nspeech & language , 12(2):75–98, 1998.\n[13] Rong Gong, Philippe Cuvillier, Nicolas Obin, and\nArshia Cont. Real-time audio-to-score alignment of\nsinging voice based on melody and lyric information.\nInInterspeech , 2015.\n[14] Arla J Good, Frank A Russo, and Jennifer Sullivan.The efﬁcacy of singing in foreign-language learning.\nPsychology of Music , 43(5):627–640, 2015.\n[15] Jens Kofod Hansen and IDMT Fraunhofer. Recogni-\ntion of phonemes in a-cappella recordings using tem-\nporal patterns and mel frequency cepstral coefﬁcients.\nIn9th Sound and Music Computing Conference (SMC) ,\npages 494–499, 2012.\n[16] Denny Iskandar, Ye Wang, Min-Yen Kan, and Haizhou\nLi. Syllabic level automatic synchronization of music\nsignals and text lyrics. In Proceedings of the 14th ACM\ninternational conference on Multimedia , pages 659–\n662. ACM, 2006.\n[17] Min-Yen Kan, Ye Wang, Denny Iskandar, Tin Lay\nNwe, and Arun Shenoy. Lyrically: Automatic syn-\nchronization of textual lyrics to acoustic music signals.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 16(2):338–349, 2008.\n[18] Anna M Kruspe. Training phoneme models for singing\nwith “songiﬁed” speech data. In ISMIR , pages 336–\n342, 2015.\n[19] Anna M Kruspe. Bootstrapping a system for phoneme\nrecognition and keyword spotting in unaccompanied\nsinging. In ISMIR , pages 358–364, 2016.\n[20] Anna M Kruspe. Retrieval of textual song lyrics from\nsung inputs. In INTERSPEECH , pages 2140–2144,\n2016.\n[21] Kyogu Lee and Markus Cremer. Segmentation-based\nlyrics-audio alignment using dynamic programming.\nInISMIR , pages 395–400, 2008.\n[22] Vladimir I Levenshtein. Binary codes capable of cor-\nrecting deletions, insertions, and reversals. In Soviet\nphysics doklady , volume 10, pages 707–710, 1966.\n[23] Alex Loscos, Pedro Cano, and Jordi Bonada. Low-\ndelay singing voice alignment to text. In ICMC , 1999.\n[24] Matthias Mauch, Hiromasa Fujihara, and Masataka\nGoto. Lyrics-to-audio alignment and phrase-level seg-\nmentation using incomplete internet-style chord an-\nnotations. In Proceedings of the 7th Sound and Mu-\nsic Computing Conference (SMC 2010) , pages 9–16,\n2010.\n[25] Matthias Mauch, Hiromasa Fujihara, and Masataka\nGoto. Integrating additional chord information into\nhmm-based lyrics-to-audio alignment. IEEE Transac-\ntions on Audio, Speech, and Language Processing ,\n20(1):200–210, 2012.\n[26] Matt McVicar, Daniel PW Ellis, and Masataka Goto.\nLeveraging repetition for improved automatic lyric\ntranscription in popular music. In Acoustics, Speech\nand Signal Processing (ICASSP), 2014 IEEE Interna-\ntional Conference on , pages 3117–3121. IEEE, 2014.\n[27] Annamaria Mesaros and Tuomas Virtanen. Automatic\nalignment of music audio and lyrics. In Proceedings\nof the 11th Int. Conference on Digital Audio Effects\n(DAFx-08) , 2008.606 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[28] Annamaria Mesaros and Tuomas Virtanen. Auto-\nmatic recognition of lyrics in singing. EURASIP\nJournal on Audio, Speech, and Music Processing ,\n2010(1):546047, 2010.\n[29] Pedro J Moreno, Christopher F Joerg, Jean-Manuel\nVan Thong, and Oren Glickman. A recursive algorithm\nfor the forced alignment of very long audio segments.\nInICSLP , volume 98, pages 2711–2714, 1998.\n[30] Hitomi Nakata and Linda Shockey. The effect of\nsinging on improving syllabic pronunciation–vowel\nepenthesis in japanese. In International Conference of\nPhonetic Sciences , 2011.\n[31] Vassil Panayotov, Guoguo Chen, Daniel Povey, and\nSanjeev Khudanpur. Librispeech: an asr corpus based\non public domain audio books. In ICASSP , 2015.\n[32] Daniel Povey and George Saon. Feature and model\nspace speaker adaptation with full covariance gaus-\nsians. In Ninth International Conference on Spoken\nLanguage Processing , 2006.\n[33] Smule. Digital Archive Mobile Performances\n(DAMP). https://ccrma.stanford.edu/\ndamp/ . [Online; accessed 15-March-2018].\n[34] Smule. Digital Archive Mobile Performances\n(DAMP). https://www.smule.com/songs .\n[Online; accessed 15-March-2018].\n[35] Ye Wang, Min-Yen Kan, Tin Lay Nwe, Arun Shenoy,\nand Jun Yin. Lyrically: automatic synchronization of\nacoustic musical signals and textual lyrics. In Proceed-\nings of the 12th annual ACM international conference\non Multimedia , pages 212–219. ACM, 2004.\n[36] A Zhang. Speech Recognition (Version 3.7)\n[Software]. https://github.com/Uberi/\nspeech_recognition#readme , 2017. [Online;\naccessed 14-Oct-2017].Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 607"
    },
    {
        "title": "Instrument Activity Detection in Polyphonic Music using Deep Neural Networks.",
        "author": [
            "Siddharth Gururani",
            "Cameron Summers",
            "Alexander Lerch 0001"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492479",
        "url": "https://doi.org/10.5281/zenodo.1492479",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/275_Paper.pdf",
        "abstract": "Although instrument recognition has been thoroughly research, recognition in polyphonic music still faces challenges. While most research in polyphonic instrument recognition focuses on predicting the predominant instruments in a given audio recording, instrument activity detection represents a generalized problem of detecting the presence or activity of instruments in a track on a fine-grained temporal scale. We present an approach for instrument activity detection in polyphonic music with temporal resolution ranging from one second to the track level. This system allows, for instance, to retrieve specific areas of interest such as guitar solos. Three classes of deep neural networks are trained to detect up to 18 instruments. The architectures investigated in this paper are: multi-layer perceptrons, convolutional neural networks, and convolutional-recurrent neural networks. An in-depth evaluation on publicly available multi-track datasets using methods such as AUC-ROC and Label Ranking Average Precision highlights different aspects of the model performance and indicates the importance of using multiple evaluation metrics. Furthermore, we propose a new visualization to discuss instrument confusion in a multi-label scenario.",
        "zenodo_id": 1492479,
        "dblp_key": "conf/ismir/GururaniSL18",
        "keywords": [
            "instrument recognition",
            "polyphonic music",
            "instrument activity detection",
            "fine-grained temporal scale",
            "deep neural networks",
            "AUC-ROC",
            "Label Ranking Average Precision",
            "multi-track datasets",
            "evaluation metrics",
            "instrument confusion"
        ],
        "content": "INSTRUMENT ACTIVITY DETECTION IN POLYPHONIC MUSIC USING\nDEEP NEURAL NETWORKS\nSiddharth Gururani1Cameron Summers2Alexander Lerch1\n1Center for Music Technology, Georgia Institute of Technology , USA\n2Gracenote, Emeryville, USA\nfsiddgururani, alexander.lerch g@gatech.edu, cameron.summers@nielsen.com\nABSTRACT\nAlthough instrument recognition has been thoroughly\nresearch, recognition in polyphonic music still faces chal-\nlenges. While most research in polyphonic instrument\nrecognition focuses on predicting the predominant instru-\nments in a given audio recording, instrument activity detec-\ntion represents a generalized problem of detecting the pres-\nence or activity of instruments in a track on a ﬁne-grained\ntemporal scale. We present an approach for instrument activ-\nity detection in polyphonic music with temporal resolution\nranging from one second to the track level. This system\nallows, for instance, to retrieve speciﬁc areas of interest\nsuch as guitar solos. Three classes of deep neural networks\nare trained to detect up to 18 instruments. The architec-\ntures investigated in this paper are: multi-layer perceptrons,\nconvolutional neural networks, and convolutional-recurrent\nneural networks. An in-depth evaluation on publicly avail-\nable multi-track datasets using methods such as AUC-ROC\nand Label Ranking Average Precision highlights different\naspects of the model performance and indicates the impor-\ntance of using multiple evaluation metrics. Furthermore, we\npropose a new visualization to discuss instrument confusion\nin a multi-label scenario.\n1. INTRODUCTION\nMusic is an acoustic rendition of musical ideas. In most\ncases, one or more instruments are used for this acoustic\nrendition. As humans, we are easily able to identify the\ninstruments being played in a song after exposure to their\nsound. However, the same cannot be said for computer algo-\nrithms. The task of recognizing musical instruments in an\naudio signal has been an active area of research in the ﬁeld\nof Music Information Retrieval (MIR). While instrument\nrecognition in monophonic audio (only one instrument is\npresent in a signal) is reasonably successful [13], the task\nis much harder in a polyphonic setting. The challenges\nc\rSiddharth Gururani, Cameron Summers, Alexander\nLerch. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Siddharth Gururani, Cameron Sum-\nmers, Alexander Lerch. “Instrument Activity Detection in Polyphonic\nMusic using Deep Neural Networks”, 19th International Society for Music\nInformation Retrieval Conference, Paris, France, 2018.include, among others, the large variance in timbre and per-\nformance style within an instrument class combined with\nperceptual similarity of some instruments and the superpo-\nsition of multiple instruments in time and frequency. Last\nbut not least, the lack of data with relevant annotations for\ndata-driven approaches is also a problem.\nThe identiﬁcation of instruments and their activity in a\nsong is important for music browsing and discovery, such as\nsearching for songs with speciﬁc instruments or identifying\nthe position of lead vocals or a saxophone solo. Instrument\nrecognition can also inform other MIR tasks. For example,\nmusic recommendation systems can beneﬁt from modeling\na user’s afﬁnity towards certain instruments and music genre\nrecognition systems could improve with genre-dependent\ninstrument information. It can also be useful for tasks such\nas automatic music transcription, playing technique detec-\ntion, and source separation in polyphonic music, where\npre-conditioning a model on speciﬁc instruments present\ncould possibly boost its performance.\nAn Instrument Activity Detection (IAD) system takes\nan audio track as input and outputs continuous instrument\nactivity levels along the entire track. These activities may\nbe binary (on/off) or on a continuous scale as likelihood.\nIAD systems may have varying time-resolutions for the\ninstrument activity depending on the use case. For example,\na solo detection use case would have a ﬁner time-resolution\nthat an instrument tagging system which would work on\nthe track level. This paper proposes a deep neural network-\nbased IAD system trained using multi-track datasets. We\nalso address the problem of evaluation of an IAD system.\nThe following section reviews literature in instrument\nrecognition and other related tasks. Section 3 describes the\nproposed IAD system starting with pre-processing the data,\nthe model architectures and post-processing steps. Next,\nSection 4 describes the dataset used, the various experi-\nments, the evaluation metrics and the proposed method to\nvisualize confusion. We report the results for the experi-\nments in terms of the evalution metrics and discuss these\nresults in Section 5. Finally, in Section 6 we conclude\nthe paper enumerating a few possible future directions for\nresearch on IAD.5692. RELATED WORK\nThe task of ‘instrument recognition’ can be divided into two\ndistinct research problems based on the type of data being\nanalyzed: (i) instrument recognition in monophonic audio\nand (ii) instrument recognition in polyphonic audio. This\nsection presents an overview of past literature on instrument\nrecognition as well as related topics such as automatic music\ntagging and sound event detection (SED).\n2.1 Instrument Recognition in Monophonic Music\nIn monophonic music, instrument recognition may be per-\nformed on sounds at the note-level or on continuous audio\nsignals of solo instrument performances. An extensive\nreview of traditional feature extraction and classiﬁcation\napproaches for note-level instrument recognition has been\npublished by Herrera et al. [15]. For solo phrases, Es-\nsid et al. utilize MFCCs as features, Principal Component\nAnalysis (PCA) for dimensionality reduction, and Gaus-\nsian mixture models (GMM) for classifying solo phrases\nof 5 instruments [8]. Krishna and Sreenivas propose the\nso-called Line Spectral Features (LSF). LSFs are used with\na GMM and evaluated for instrument family classiﬁcation\nand 14-class instrument classiﬁcation [19].\nIn addition to extracting established pre-deﬁned features,\nlearned features have also been applied to this task. Yu et al.\nutilize sparse spectral codes and a support vector machine\n(SVM) for classifying single-source and multi-source (poly-\nphonic) audio [31]. Han et al. propose to use sparse coding\nfor learning features from mel-spectrograms extracted from\na dataset of single-note audio clips for 24 instruments. A\nSVM is trained to classify the instruments using the learned\nfeatures achieving a classiﬁcation accuracy of around 95%\nfor 24 instrument classes [13].\n2.2 Instrument Recognition in Polyphonic Music\nRecent work on instrument recognition has focused on poly-\nphonic musical signals. Polyphonic audio synthesized from\ndatasets of individual instrument sounds, such as the RWC\ndataset [10], as well as real-world audio recordings have\nbeen used for this task.\nKitahara et al. extract spectral and temporal features\nalong with PCA and Latent Discriminant Analysis (LDA)\nfor classiﬁcation in duo and trio music [17]. Heittola et al.\ncombine the results of Non-negative Matrix Factorization\n(NMF) with excitations of notes obtained from a multi-pitch\ntracking algorithm [18] to extract harmonic spectra from\na mixture signal. The separated spectra are represented by\nMFCCs and classiﬁed with a GMM [14].\nFuhrmann et al. extract a large set of features repre-\nsenting an audio clip and perform predominant instrument\ndetection in real-world audio signals using one SVM per\ninstrument [9]. The ‘predominant’ instrument is deﬁned\nas one with continuous presence in a snippet of audio and\nis easily audible for a human listener. Bosch et al. extend\nthe work by utilizing source separation to segregate the\npolyphonic audio into streams: ‘bass,’ ‘drums,’ ‘melody,’\nTraining \nSet\nTesting \nTracks Pre-processing \nPre-processing DNN Prediction DNN Training Trained \nModel \nInstrument \nActivity Temporal \nAggregation Figure 1 . Block Diagram for DNN-based IAD System\nand ‘other.’ The segregated audio is subsequently used for\nclassiﬁcation using the aforementioned system [2].\nHan et al. apply deep CNNs for the task of predominant\ninstrument recognition and report a signiﬁcant improvement\nof results over previous approaches [12]. The authors also\nprovide an in-depth discussion of the model parameters and\na qualitative analysis of the CNN models.\n2.3 Related Tasks\nIn music tagging, a track is labeled with a variety of labels\nthat describe it, such as genre, instruments, and mood. IAD\nmay be considered a sub-task in music tagging since the\ntags often include instrumentation. Choi et al. use CNNs\nand CRNNs for the task of automatic tagging [5, 6]. Liu\nand Yang further proposed a method to localize the events\nin music tagging [20] which may be compared to IAD.\nSound Event Detection (SED) aims at detecting envi-\nronmental sounds in a stream of audio. Some examples of\nsound events are gunshots, car horns, baby cries, dog barks,\netc. Cakir et al. explore this task with deep neural networks\non a dataset of environmental sounds [3, 4]. The main dif-\nference between SED and IAD is that in SED the sound\nevents are uncorrelated and thus easier to discriminate while\nmusical sources tend to have higher correlation in popular\nmusic. Music instrument sounds might also have a rich\nharmonic structure absent in most environmental sounds.\n3. METHOD\nA high-level block diagram for the presented IAD system\nis shown in Fig. 1. The individual processing steps are\ndescribed in detail below.\n3.1 Pre-processing\nAll tracks are downsampled to 22.05 kHz, downmixed to\nmono and normalized by the root mean square energy. Each\ntrack is the chunked to 1 s long snippets. Each snippet is\ntransformed into a mel-spectrogram, which is motivated\nby the non-linear frequency resolution of the human audi-\ntory system [22], and has been proven to be a useful input\nrepresentation for multiple MIR tasks such as automatic\ntagging [5], onset detection [25], and feature learning [29].\nThe mel-spectrograms are calculated using Librosa [21]\nwith 96mel bands from 0–11.025 kHz. The block size and\nhop size are 46.4 ms and 11.6 ms, respectively. Decibel\nscaling is applied to the Mel-Spectrogram energies. The\nresult is a matrix of dimension 96\u000286.570 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018CNN CRNN\nConv2D\nk= 3\u00023; d= 64\nMP(p= 2;2)\nConv2D\nk= 3\u00023; d= 128\nMP(p= 2;2)\nConv2D Conv2D\nk= 3\u00023; d= 256 k= 3\u00023; d= 256\nMP(p= 3;3) MP(p= 2;2)\nConv2D Conv2D\n(k= 3\u00023; d= 640) (k= 3\u00023; d= 256)\nMP(p= 3;3) MP(p= 2;2)\nFC(h= 128) GRU (h= 256)\nFC(h= 18)\nTable 1 . Model Architecture. (Conv2D: 2D Convolutional\nLayer, MP: 2D Max-Pooling, k: kernel size, d: ﬁlter depth)\n3.2 Model Architectures\nDeep Neural Networks (DNNs) have consistently outper-\nformed traditional MIR approaches in several tasks such\nas, music transcription [26, 30], onset detection [25], music\ntagging [5]. As this is also true for predominant instrument\nclassiﬁcation (compare Sect. 2), we choose to investigate\nDNNs for the task of IAD. Our architectural choices are\ninﬂuenced by the work of both Choi and Cakir [4–6].\nThe usability of DNNs stems from their ability to ap-\nproximate complex non-linear functions mapping an input\nfeature space to the outputs. This enables researchers to\nprovide raw or minimally processed data to a DNN so that\nit may learn features relevant for the task at hand.\nWe compare the three broad classes of DNNs: multi-\nlayer perceptrons, convolutional neural networks, and —\nsince convolutional networks are useful for acoustic mod-\neling [5]— a convolutional-recurrent network instead of a\ntraditional RNN. The beneﬁt of CRNN lies in the fact that it\nis able to learn both local and temporal features. Note that\nthe model hyperparameters have been chosen so that the\nnumber of parameters for the three models is comparable.\n3.2.1 Multi-Layer Perceptron\nThe input mel-spectrogram matrix is ﬂattened into a vector\nfor the MLP model. A fairly simple architecture is chosen:\n4hidden layers with 256hidden units in each layer and an\noutput layer of 18hidden units. Dropout [28] is used with a\nkeep probability of 0:5at each layer.\n3.2.2 Convolutional Neural Network\nThe CNN architecture is shown in Table 1 (left). Small\nsquare ﬁlters are chosen in order to facilitate hierarchical\nfeature learning from local patches that grow larger in size\nwith network depth. In order to preserve spatial dimensions,\nstride of 1andSame zero-padding scheme is used for all\nthe convolutional layers. Each Conv2D layer is followed by\nbatch-normalization [16] and the Exponential Linear Unit\n(ELU) [7] activation function. The ﬁnal convolution layer’s\noutput is ﬂattened before feeding it to a fully connected\nlayer. Finally, we connect to an output layer of 18 units\nwith a sigmoid activation function.Train Test\nInstrument Abbr. T # T #\ndrum set dru 300 720036 79 15957\nelectric bass bgtr 253 620592 62 13344\nmale singer ms 200 351384 62 10038\ndist. elec. gtr dgtr 171 396204 40 7522\nclean elec. gtr cgtr 119 225456 34 5875\nsynthesizer syn 118 295524 33 5712\nacoustic gtr agtr 91 230556 25 5241\npiano pf 89 187536 24 4063\nvocalists vox 84 154596 12 1895\nfemale singer fs 79 149232 23 3733\nstring section str 24 39444 10 1278\nelec. piano epf 24 52680 14 2075\nelect. organ eorg 22 39516 11 2117\ndouble bass db 21 40116 9 1786\ncello vc 13 22176 9 1623\nviolin vn 10 28452 15 2385\ntabla tab 9 41640 3 806\nﬂute ﬂ 7 9972 7 1171\nTable 2 . Dataset distribution: T denotes tracks and # de-\nnotes 1 s snippets\n3.2.3 Convolutional Recurrent Neural Network\nThe CRNN architecture is shown in Table 1 (right). CRNNs\nhave been applied to tasks such as music tagging [6] and\nsound event detection [4]. We hypothesize that it is a good\nchoice for IAD since we want the model to learn from the\nevolution of spectra over time. The same conﬁguration of\npadding and striding, batch-normalization, and non-linear\nactivation is used for the convolutional modules. Only the\ndepth and height of the ﬁnal Conv layer output is ﬂattened,\nthus preserving the temporal structure of the high-level\nConvNet features. Finally, the last GRU output is connected\nto the output layer consisting of 18 units with a sigmoid\nactivation function.\n3.2.4 Training Procedure\nBinary cross-entropy is used as the loss function for all\nmodels. Stochastic gradient descent with a learning rate\nof0:0001 and momentum of 0:9is used to optimize the\nloss function. The models are trained using batches of 32\ninstances for 20epochs, which is sufﬁcient for the training\nand validation loss to converge for each of the architectures.\n3.3 Temporal Aggregation\nSince the neural networks are trained using 1 s snippets of\naudio, a prediction is made for every 1 s in the test track. For\nexperiments and evaluation with varying time-resolution,\nwe max-pool the predictions and the ground truth over\nnon-overlapping segments according to the desired time-\nresolution. For example, in order to have a 5 s resolution,\nthe maximum across 5continuous predictions for every\ninstrument is chosen as the predicted score for the corre-\nsponding 5 s snippet in the track.\n4. EVALUATION\n4.1 Dataset\nThe dataset used in previous work on predominant instru-\nment detection [2, 9, 12], IRMAS, consists of a training setProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 571with 3 s audio snippets manually annotated with one of 11\npredominant (but non-percussive) instrument labels. These\nsnippets may contain other instruments. The testing set\ncontains audio snippets of variable length with 1or more\npredominant instruments. We believe that training using\npolyphonic audio labeled with a single instrument may not\nbe the ideal strategy for IAD. In this paper, we used multi-\ntrack audio to construct a dataset for IAD. The motivation\nbehind using multi-track datasets is that the annotations for\ninstrument activity can be generated automatically using\nstem energy as opposed to human annotations which may\ncontain more errors. In addition, each snippet may con-\ntain multiple instrument labels, providing the models richer\nground truth.\nTwo publicly available multi-track datasets are used for\ntraining and testing of the models. MedleyDB [1] and Mix-\ning Secrets [11] were combined for this task in order to\nincrease the number of tracks. In a pilot study involving\nonly MedleyDB, we observed a signiﬁcant improvement in\nmodel performance as the amount of training data increased.\nMedleyDB contains 330multi-tracks and Mixing Secrets\ncontains 258multi-tracks. The two datasets combined con-\ntain tracks with approximately 100different instruments.\nFor this paper we consider 18most frequently occurring in-\nstruments. The instruments considered are listed in Table 2 .\nNote that the tracks may contain other instruments that the\nIAD system is not trained to detect.\nEach multi-track in the dataset is associated with a mixed\ntrack. Instrument activation conﬁdence is annotated auto-\nmatically according to the process described in [1]. These\nannotations are computed with time-resolution of 0.0464 s.\nFor our IAD system, however, we deﬁned the minimum\ntime-resolution to be 1 s. The annotations are aggregated by\npicking the maximum value across the time-axis to obtain\none activation value per instrument per snippet. This allows\nfor instruments to have a large activation value in the snip-\npet even if they were active for a small period of time, as\nopposed to a value close to 0if the mean was chosen for\naggregation. Finally, the activations are binarized with a\nﬁxed threshold \u0012= 0:5.\nThe datasets contain tracks where the stems have cross-\ntalk or bleed. For these tracks, stem activations for a certain\ninstrument may contain activity from another instrument.\nTo prevent incorrect annotations, tracks with bleed are not\nconsidered for the IAD dataset, although we make excep-\ntions for rare instruments such as tabla. Additionally, tracks\nwithout a single instrument of interest are not considered.\nSubsequently, the dataset is split into a training and a\ntesting set. We generate a random artist-conditional split to\nprevent the album or artist effect in the testing phase. The\nsplit is chosen such that there is a reasonable number of\ntracks per instrument. Table 2 lists the distribution of the\ndata for the split. The training set consists of 361tracks\nand the testing set consists of 100tracks.1The training set\nis augmented using pitch-shifting: 6semitones lower to 5\nhigher than the original with 1semi-tone increments.\n1The track IDs for the dataset splits used are available at\nhttps://github.com/SiddGururani/ISMIR20184.2 Experimental Setup\nFirst, we preprocess both splits of data as described in\nSect. 3.1 resulting in a time-frequency input representation\nand ground truth pair for each 1 second snippet. Table 2\nlists the distribution of the different instrument classes in\nterms of 1 second snippets. Next, we train each of the DNN\narchitecture as described in Sect. 3.2. Since the models\nwere observed to converge to a solution in 20 epochs, we\ndo not perform any form of early-stopping. In addition,\nwe generate a validation set using a randomly sampled set\nof tracks from both the training and testing set due to lack\nof data. 50tracks from the training and testing splits are\npicked, resulting in a validation set of 100tracks. We use\nthis scheme since we want to validate on unseen data while\nnot using the entire test set. The validation set is used to\nevaluate the models at the end of each epoch. Finally, we\ntest the best performing model for each class of DNNs.\nWe test the models for various time-resolutions of activity\ndetection: 1 s, 5 s, 10 s and track-level aggregation.\n4.3 Evaluation Metrics\nEvaluation of IAD systems, when looked at in detail, poses\nsome challenges. Since each snippet has zero or more in-\nstruments, IAD is a multi-label classiﬁcation problem. The\nsigmoid activation leads to an output between 0and1, de-\nnoting the predicted activity of that instrument. However, as\npointed out by Han et al. [12], binarizing the outputs using a\nﬁxed threshold and evaluating the accuracy depends on the\nselected threshold. Additionally, the dataset is not balanced\nacross the instrument classes, hence stressing the need for\nmetrics robust against unbalanced class distribution.\nPrevious work on predominant instrument recognition\nuses metrics relevant for multi-class classiﬁcation systems\nsuch as precision, recall and f-measure [2,12]. Since IAD is\na multi-label classiﬁcation problem, we use Label Ranking\nAverage Precision (LRAP) and the Area Under Receiver\nOperating Characteristic curve (AUC-ROC).\n4.3.1 Label Ranking Average Precision\nLRAP was proposed in [24] to evaluate multi-label classiﬁ-\ncation systems. Intuitively, the LRAP measures the ability\nof a model to assign better ranks to true labels for an in-\nstance. For example, if all the true labels for an instance\nare ranked higher than other labels in consideration, the\nranking precision for this instance is 1. LRAP measures the\naverage ranking precision across all the instances. In our\nexperiments, we compute LRAP using 2approaches: (i) Mi-\ncro: LRAP computed using the concatenated outputs for\nall testing tracks. (ii) Macro: computed on the track level\nand averaged. This normalizes any effect of track length on\nthe model performance, which could skew the results, for\ninstance, if the model performs well for a particular long\nsong but poorly for shorter songs with fewer snippets.\n4.3.2 Area Under ROC Curve\nThe AUC-ROC or, in short, AUC is computed by ﬁrst plot-\nting the true positive rate and false positive rate on a plane\nfor various classiﬁcation thresholds, which results in a curve.572 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018MLP-micro\nMLP-macro\nCNN-micro\nCNN-macro\nCRNN-micro\nCRNN-macroFigure 2 . LRAP for various time-resolutions\nAUC is the area under this curve. It measures the probability\nthat the model assigns a higher score to a randomly selected\npositive instance than a negative instance. The AUC gives\na summary of the model performance without the need to\nadjust a threshold for binarization.\nSince AUC is usually applied to binary classiﬁcation,\nwe compute it per instrument class. Only the micro AUC\nis computed as not all tracks contain all instruments. We\nreport an average AUC by taking the mean of the micro\nAUC per class.\nThe reason for selecting AUC instead of precision, recall\nor f-measure is that most literature on instrument classiﬁca-\ntion tends to use a common ﬁxed threshold for all classes\nwhich bears the risk of being suboptimal. Han et al. suggest\nthe use of a different threshold per instrument class [12].\nUsing the AUC to summarize model performance alleviates\nthe problem of threshold selection while making it easier to\ndirectly compare model performance.\n4.4 Confusion Visualization\nIn multi-class classiﬁcation, every data sample has only one\npossible prediction and one ground truth label. A confusion\nmatrix visualizes the frequency of confusion between every\npair of predicted class label vs. ground truth class label.\nIn a multi-label classiﬁcation problem such at IAD, every\ninstance has multiple possible predictions and zero or more\nground truth labels. Hence, a traditional confusion matrix\ncannot be computed. However, as a confusion matrix is an\nintuitive way to gain insights into the model, we propose an\nalternative form of confusion visualization computed from\nthe binarized predictions and the ground truths.\nWe hypothesize that an instrument is wrongly detected\ndue to the activity of some instrument present in the audio.\nWe are particularly interested in looking at which instru-\nments were incorrectly missed (false negative) when an\ninstrument was wrongly detected (false positive). For a\nparticular false positive instrument, this is equivalent to\nlooking at the probability of observing false negatives for\nthe other instruments. This probability can be estimated\nusing a histogram of false negatives. Vertically stacking\nthese histograms for each instrument results in a matrix of\ndimensionC\u0002C(C=number of instrument classes). We\n1 s\n5 s\n10 s\nTrackFigure 3 . AUC per instrument for CRNN model\nconvert the histograms to probabilities by normalizing each\nrow of the matrix to a sum of 1.\nNote that unlike a traditional confusion matrix, this is\nnot a symmetric matrix. We only focus on one row at a\ntime in order to compare probabilities of observing false\nnegatives for a given false positive instrument.\n5. RESULTS AND DISCUSSION\nA comparison of model performance is summarized in\nFigure 2 andTable 3 . It can be observed that CNN and\nCRNN outperform MLP in both metrics. This is expected\nsince the convolutional layers allow the model to learn\nhierarchical acoustic features from the time-frequency rep-\nresentation more efﬁciently. However, the CRNN does not\noutperform the CNN, which may be attributed to the fact\nthat only 1 s second snippets are used. The temporal dimen-\nsion of the input is reduced to only 5time steps after the 4\nCNN layers. The beneﬁts of using recurrent layers are more\nnoticeable when longer sequences are involved as in work\nby Choi et al. where they use inputs of length 29 s [6]. In ad-\ndition, the receptive ﬁeld of the deeper layers of the CNN is\nlarge enough for learning temporal features. Another obser-\nvation is that output aggregation tends to improve models’\nlabel ranking performance and mean AUC.\nFigure 3 shows the AUC per instrument of the CRNN\nmodel for the chosen time-resolution aggregation. We ob-\nserve that using output aggregation in time leads to better\nperformance in almost all instrument classes. The model\nachieves high AUC not only for majority instruments in\nthe dataset but also for minority instruments such as ﬂute,\nviolin and cello, suggesting that it not simply predicting the\nmajority. We also observe that the model does not seem to\nperform well for vocals in general. While it does achieve\nhigh AUC for male singers, the AUC for female singers\nand vocalists is low. We investigate this further using the\nMLP CNN CRNN\n1 s 71.28 77.55 77.5\n5 s 70.85 78.35 78.76\n10 s 70.82 78.59 79.22\nTrack 71.1 80.92 80.1\nTable 3 . Mean AUC for various time-resolutionsProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 57300.050.10.15Figure 4 . Distribution of false negative instruments condi-\ntioned on a true positive of a particular instrument\nvisualization method described in Sect. 4.4.\nTo construct the confusion visualization as described in\nSect. 4.4, we pick the CRNN model and use the 1 s time-\nresolution outputs for the test set. Picking the threshold\nfor binarizing the predictions is not straightforward. A\nﬁxed threshold of 0:5, for example, led to 0detections for\nstring section and electronic organ. Therefore, we adjusted\nthe best thresholds for each class. These thresholds are\ndetermined by computing the class-wise f-measure at all\nscore thresholds and selecting the threshold giving the best\nf-measure in the validation set. Figure 4 shows the con-\nstructed visualization. A high value in a row implies that\nthe model may be confusing that particular pair of instru-\nments more often than others. The following observations\ncan be made from the ﬁgure:\n\u000f(bgtr,db): This confusion is possibly due to similar\nfrequency range of the electric bass and double bass.\n\u000f(dgtr,agtr), (dgtr,cgtr), (cgtr,dgtr), and ( agtr,dgtr):\nWhile confusion between acoustic and distorted gui-\ntar is unusual, the confusion between clean and dis-\ntorted guitar is possibly explained by the variety in\ntone for both the clean and distorted guitars. A light\ncrunch or low gain setting may possibly get misclas-\nsiﬁed. This could also explain the poor performance\nfor clean electric guitar.\n\u000f(dru,tab) and ( tab,dru): Both drum set and tabla are\npercussive instruments. In addition, one of the test\ntracks containing tabla has a ‘drum machine’ label\nwhich possibly causes drum false positives.\n\u000f(dgtr,syn) and ( syn,dgtr): This is an interesting case\nsince the variance in sound for both, the distorted\nguitar and synthesizers, is very large. Further investi-\ngation is needed to understand this case.\nNext, we investigate the poor model performance on\nvocal classes. Figure 4 shows confusion between male and\nfemale singers implying that the model might be incorrectly\nclassifying female singers as male. In order to investigate\nthis phenomenon, the three vocal classes were combined for\na follow-up experiment. We max-pool the predictions and1 s 5 s 10 s Track\nAverage AUC\n(ms, fs, vox)0.709 0.725 0.737 0.782\nAUC vocals 0.822 0.96 0.975 0.998\nTable 4 . AUC for different time resolutions comparing\npooled vocals against averaged AUC for vocal classes\nthe ground truth for these three classes, and recompute the\nAUC for this new ‘vocals’ class. Table 4 shows the average\nAUC of the three classes and the AUC of the combined ‘vo-\ncals’ class. The model performs signiﬁcantly better for the\ncombined class conﬁrming our hypothesis that it confuses\nthe vocal classes.\nAnother interesting ﬁnding is that the best threshold\nchosen per instrument for binarization ranges from 0:02\nto0:55with lower thresholds for minority instruments in\ngeneral. We observe a correlation coefﬁcient of 0:9between\nthe thresholds and the training data distribution suggesting\nthat the model has learned biases in the dataset. The impact\nof this ﬁnding requires further experiments.\n6. CONCLUSION\nWe presented a DNN-based IAD system trained using multi-\ntrack datasets to detect 18instruments. The CRNN and\nCNN outperform MLP architectures for the task and per-\nform well for detecting instruments common in popular mu-\nsic, such as drums, electric bass, acoustic guitars, distorted\nguitars and vocals. It also performs well for instruments in\nclassical music such as ﬂute, cello, violin even though they\nwere under-represented in the dataset. We also stress the\nneed for multiple metrics and visualizations for evaluation\nof systems such as IAD which is non-trivial to evaluate.\nAs future work, a few extensions and research directions\nare: (i) pre-training the network using monophonic stems\nfrom the multi-track datasets and subsequently training and\ntesting for IAD, (ii) designing the convolutional network\nfor the CRNN as proposed by Jordi et al. [23] instead of the\ncurrently used 3\u00023ﬁlters as is common in computer vision,\n(iii) converting the proposed monolithic model architecture\nfor IAD to a hierarchical architecture for instrument family\nclassiﬁcation ﬁrst and subsequently instrument classiﬁca-\ntion. While this paper treats the model as a black box and\nfocuses on evaluation and analysis of model outputs, it is\nworth studying the model to understand the internal repre-\nsentations by means of visualization tools such as t-SNE\nand saliency maps [27] as performed by Han et al. [12].\nBy drawing attention to challenges in IAD with this\npaper, we hope to encourage the MIR community to explore\nthis task. IAD is a rewarding avenue for research due to its\nreal-world use cases as well as the potential to augment and\nimprove performance in other tasks in MIR.\n7. ACKNOWLEDGMENTS\nThis work was funded by Gracenote. We thank them for\ntheir generous support. We would also like to thank Nvidia\nfor supporting us with a Titan Xp awarded as part of the\nGPU grant program.574 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20188. REFERENCES\n[1]Rachel M Bittner, Justin Salamon, Mike Tierney,\nMatthias Mauch, Chris Cannam, and Juan Pablo\nBello. Medleydb: A multitrack dataset for annotation-\nintensive mir research. In Proc. of the International\nSociety for Music Information Retrieval Conference (IS-\nMIR) , pages 155–160, 2014.\n[2]Juan J Bosch, Jordi Janer, Ferdinand Fuhrmann, and\nPerfecto Herrera. A comparison of sound segregation\ntechniques for predominant instrument recognition in\nmusical audio signals. In Proc. of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 559–564, 2012.\n[3]Emre Cakir, Toni Heittola, Heikki Huttunen, and Tuo-\nmas Virtanen. Polyphonic sound event detection using\nmulti label deep neural networks. In Proc. International\nJoint Conference on Neural Networks (IJCNN) , pages\n1–7, 2015.\n[4]Emre Cakir, Giambattista Parascandolo, Toni Heittola,\nHeikki Huttunen, Tuomas Virtanen, Emre Cakir, Gi-\nambattista Parascandolo, Toni Heittola, Heikki Hut-\ntunen, and Tuomas Virtanen. Convolutional recurrent\nneural networks for polyphonic sound event detection.\nIEEE/ACM Transactions on Audio, Speech and Lan-\nguage Processing (TASLP) , 25(6):1291–1303, 2017.\n[5]Keunwoo Choi, Gy ¨orgy Fazekas, and Mark Sandler.\nAutomatic tagging using deep convolutional neural net-\nworks. In Proc. of the International Soceity of Music\nInformation Retrieval Conference (ISMIR) , pages 805–\n811, 2016.\n[6]Keunwoo Choi, Gy ¨orgy Fazekas, Mark Sandler, and\nKyunghyun Cho. Convolutional recurrent neural net-\nworks for music classiﬁcation. In Proc. of the Inter-\nnational Conference on Acoustics Speech and Signal\nProcessing (ICASSP) , pages 2392–2396, 2017.\n[7]Djork-Arn ´e Clevert, Thomas Unterthiner, and Sepp\nHochreiter. Fast and accurate deep network learning\nby exponential linear units (elus). International Confer-\nence on Learning Representations (ICLR) , 2015.\n[8]Slim Essid, Ga ¨el Richard, and Bertrand David. Musical\ninstrument recognition on solo performances. In Proc.\nof the 12th European Signal Processing Conference ,\npages 1289–1292, 2004.\n[9]Ferdinand Fuhrmann, Mart ´ın Haro, and Perfecto Her-\nrera. Scalability, generality and temporal aspects in au-\ntomatic recognition of predominant musical instruments\nin polyphonic music. In Proc. of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 321–326, 2009.\n[10] Masataka Goto. Rwc music database: Music genre\ndatabase and musical instrument sound database. In\nProc. International Conference on Music Information\nRetrieval, 2003 , pages 229–230, 2003.[11] Siddharth Gururani and Alexander Lerch. Mixing se-\ncrets: A multitrack dataset for instrument detection in\npolyphonic music. In Late Breaking Demo (Extended\nAbstract), Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\nSuzhou, 2017.\n[12] Yoonchang Han, Jaehun Kim, Kyogu Lee, Yoonchang\nHan, Jaehun Kim, and Kyogu Lee. Deep convolutional\nneural networks for predominant instrument recognition\nin polyphonic music. IEEE/ACM Transactions on Audio,\nSpeech and Language Processing (TASLP) , 25(1):208–\n221, 2017.\n[13] Yoonchang Han, Subin Lee, Juhan Nam, and Kyogu\nLee. Sparse feature learning for instrument identi-\nﬁcation: Effects of sampling and pooling methods.\nThe Journal of the Acoustical Society of America ,\n139(5):2290–2298, 2016.\n[14] Toni Heittola, Anssi Klapuri, and Tuomas Virtanen. Mu-\nsical instrument recognition in polyphonic audio using\nsource-ﬁlter model for sound separation. In Proc. of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 327–332, 2009.\n[15] Perfecto Herrera-Boyer, Geoffroy Peeters, and Shlomo\nDubnov. Automatic classiﬁcation of musical instrument\nsounds. Journal of New Music Research , 32(1):3–21,\n2003.\n[16] Sergey Ioffe and Christian Szegedy. Batch normaliza-\ntion: Accelerating deep network training by reducing in-\nternal covariate shift. In Proc. of the 32nd International\nConference on Machine Learning (ICML) , volume 37,\npages 448–456. PMLR, 2015.\n[17] Tetsuro Kitahara, Masataka Goto, Kazunori Komatani,\nTetsuya Ogata, and Hiroshi G Okuno. Instrument iden-\ntiﬁcation in polyphonic music: Feature weighting to\nminimize inﬂuence of sound overlaps. EURASIP Jour-\nnal on Applied Signal Processing , 2007(1):155–155,\n2007.\n[18] Anssi Klapuri. Multiple fundamental frequency estima-\ntion by summing harmonic amplitudes. In Proc. of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 216–221, 2006.\n[19] AG Krishna and Thippur V Sreenivas. Music instrument\nrecognition: from isolated notes to solo phrases. In Proc.\nof the International Conference on Acoustics Speech\nand Signal Processing (ICASSP) , volume 4, pages iv–iv,\n2004.\n[20] Jen-Yu Liu and Yi-Hsuan Yang. Event localization in\nmusic auto-tagging. In Proc. of the 2016 ACM on Mul-\ntimedia Conference , pages 1048–1057. ACM, 2016.\n[21] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW\nEllis, Matt McVicar, Eric Battenberg, and Oriol Nieto.\nlibrosa: Audio and music signal analysis in python. In\n14th python in science conference , pages 18–25, 2015.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 575[22] Brian CJ Moore. An Introduction to the Psychology of\nHearing . Brill, 2012.\n[23] Jordi Pons, Olga Slizovskaia, Rong Gong, Emilia\nG´omez, and Xavier Serra. Timbre analysis of music\naudio signals with convolutional neural networks. In\nProc. of the 25th European Signal Processing Confer-\nence, Kos island, Greece, 28/08/2017 2017. IEEE.\n[24] Robert E Schapire and Yoram Singer. Boostexter: A\nboosting-based system for text categorization. Machine\nlearning , 39(2-3):135–168, 2000.\n[25] Jan Schluter and Sebastian Bock. Improved musical\nonset detection with convolutional neural networks.\nInProc. of the International Conference on Acoustics\nSpeech and Signal Processing (ICASSP) , pages 6979–\n6983, 2014.\n[26] Siddharth Sigtia, Emmanouil Benetos, and Simon\nDixon. An end-to-end neural network for polyphonic\npiano music transcription. IEEE/ACM Transactions\non Audio, Speech and Language Processing (TASLP) ,\n24(5):927–939, 2016.\n[27] Karen Simonyan, Andrea Vedaldi, and Andrew Zisser-\nman. Deep inside convolutional networks: Visualising\nimage classiﬁcation models and saliency maps. Interna-\ntional Conference on Learning Representations (ICLR) ,\n2013.\n[28] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. Dropout: A\nsimple way to prevent neural networks from overﬁtting.\nThe Journal of Machine Learning Research , 15(1):1929–\n1958, 2014.\n[29] Aaron Van den Oord, Sander Dieleman, and Benjamin\nSchrauwen. Deep content-based music recommenda-\ntion. In Advances in Neural Information Processing\nSystems (NIPS) , pages 2643–2651, 2013.\n[30] Richard V ogl, Matthias Dorfer, and Peter Knees. Drum\ntranscription from polyphonic music with recurrent neu-\nral networks. In Proc. of the International Conference\non Acoustics Speech and Signal Processing (ICASSP) ,\npages 201–205, 2017.\n[31] Li-Fan Yu, Li Su, and Yi-Hsuan Yang. Sparse cepstral\ncodes and power scale for instrument identiﬁcation.\nInProc. of the International Conference on Acoustics\nSpeech and Signal Processing (ICASSP) , pages 7460–\n7464, 2014.576 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Towards Full-Pipeline Handwritten OMR with Musical Symbol Detection by U-Nets.",
        "author": [
            "Jan Hajic Jr.",
            "Matthias Dorfer",
            "Gerhard Widmer",
            "Pavel Pecina"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492389",
        "url": "https://doi.org/10.5281/zenodo.1492389",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/175_Paper.pdf",
        "abstract": "Detecting music notation symbols is the most immediate unsolved subproblem in Optical Music Recognition for musical manuscripts. We show that a U-Net architecture for semantic segmentation combined with a trivial detector already establishes a high baseline for this task, and we propose tricks that further improve detection performance: training against convex hulls of symbol masks, and multichannel output models that enable feature sharing for semantically related symbols. The latter is helpful especially for clefs, which have severe impacts on the overall OMR result. We then integrate the networks into an OMR pipeline by applying a subsequent notation assembly stage, establishing a new baseline result for pitch inference in handwritten music at an f-score of 0.81. Given the automatically inferred pitches we run retrieval experiments on handwritten scores, providing first empirical evidence that utilizing the powerful image processing models brings content-based search in large musical manuscript archives within reach.",
        "zenodo_id": 1492389,
        "dblp_key": "conf/ismir/HajicDWP18",
        "keywords": [
            "Optical Music Recognition",
            "Music notation symbols",
            "U-Net architecture",
            "semantic segmentation",
            "trivial detector",
            "high baseline",
            "tricks",
            "feature sharing",
            "clefs",
            "pitch inference"
        ],
        "content": "TOWARDS FULL-PIPELINE HANDWRITTEN OMR WITH MUSICAL\nSYMBOL DETECTION BY U-NETS\nJan Haji ˇc jr.1Matthias Dorfer2Gerhard Widmer2Pavel Pecina1\n1Institute of Formal and Applied Linguistics, Charles University\n2Institute of Computational Perception, Johannes Kepler University\nhajicj@ufal.mff.cuni.cz\nABSTRACT\nDetecting music notation symbols is the most immediate\nunsolved subproblem in Optical Music Recognition for\nmusical manuscripts. We show that a U-Net architecture\nfor semantic segmentation combined with a trivial detec-\ntor already establishes a high baseline for this task, and\nwe propose tricks that further improve detection perfor-\nmance: training against convex hulls of symbol masks,\nand multichannel output models that enable feature shar-\ning for semantically related symbols. The latter is help-\nful especially for clefs, which have severe impacts on the\noverall OMR result. We then integrate the networks into an\nOMR pipeline by applying a subsequent notation assembly\nstage, establishing a new baseline result for pitch inference\nin handwritten music at an f-score of 0.81. Given the au-\ntomatically inferred pitches we run retrieval experiments\non handwritten scores, providing ﬁrst empirical evidence\nthat utilizing the powerful image processing models brings\ncontent-based search in large musical manuscript archives\nwithin reach.\n1. INTRODUCTION\nOptical Music Recognition (OMR), the ﬁeld of automat-\nically reading music notation from images, has long held\nthe signiﬁcant promise for music information retrieval of\nmaking a great diversity of music available for further\nprocessing. More compositions have probably been writ-\nten than recorded, and more have remained in manuscript\nform rather than being typeset; this is not restricted to the\ntens of thousands of manuscripts from before the age of\nrecordings, but holds also for contemporary music, where\nmany manuscripts have been left unperformed for rea-\nsons unrelated to their musical quality. Making the con-\ntent of such manuscript collections accessible digitally\nand searchable is one of the long-held promises of OMR,\nand at the same time OMR is reported to be the bottle-\nneck there [17]. On printed music or simpler early mu-\nsic notation, this has been attempted by the PROBADO\nc\rJan Haji ˇc jr., Matthias Dorfer, Gerhard Widmer, Pavel\nPecina. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Jan Haji ˇc jr., Matthias Dorfer, Ger-\nhard Widmer, Pavel Pecina. “Towards Full-Pipeline Handwritten OMR\nwith Musical Symbol Detection by U-Nets”, 19th International Society\nfor Music Information Retrieval Conference, Paris, France, 2018.\nFigure 1 . OMR pipeline in this work. Top-down: (1) input\nscore, (3) symbol detection output, (4) notation assembly\noutput. Obtaining MIDI from output of notation assem-\nbly stage (for evaluating pitch accuracy and retrieval per-\nformance) is then deterministic. Our work focuses on the\nsymbol detection step (1) !(3); notation reconstruction is\ndone only with a simple baseline.\n[17, 28] or SIMSSA/Liber Usualis [3] projects. However,\nfor manuscripts, results are not forthcoming.\nThe usual approach to OMR is to break down the prob-\nlem into a four-step pipeline: (1) preprocessing and bina-\nrization, (2) stafﬂine removal, (3) symbol detection (local-\nization and classiﬁcation), and (4) notation reconstruction\n[2]. Once stage (4) is done, the musical content — pitch,\nduration, and onsets — can be inferred, and the score itself\ncan be encoded in a digital format such as MIDI, MEI1\nor MusicXML. We term OMR systems based on explicitly\nmodeling these stages Full-Pipeline OMR .\nBinarization and staff removal have been successfully\ntackled with convolutional neural networks (CNNs) [4,11],\nformulated as semantic segmentation. Symbol classiﬁca-\ntion achieves good results as well [12, 13, 33]. However,\ndetecting the symbols on a full page remains the next ma-\njor bottleneck for handwritten OMR. As CNNs have not\nbeen applied to this task yet, they are a natural choice.\nFull-Pipeline OMR is not necessarily the only viable ap-\nproach: recently, end-to-end OMR systems have been pro-\nposed. [16, 24]. However, they have so far been limited to\nshort excerpts of monophonic music, and it is not clear how\nto generalize their output design from MIDI equivalents to\n1http://music-encoding.org/225lossless structured encoding such as MEI or MusicXML,\nso full-pipeline approaches remain justiﬁed.\nOur work mainly addresses step (3) of the pipeline, ap-\nplied in the context of a baseline full-pipeline system, as\ndepicted in Fig. 1. We skip stage (2): we treat stafﬂines as\nany other object, since we jointly segment and classify and\ndo not therefore have to remove them in order to obtain a\nmore reasonable pre-segmentation. We claim the follow-\ning contributions:\n(1) U-Nets used for musical symbol detection. Apply-\ning fully convolutional networks, speciﬁcally the U-Net ar-\nchitecture [38], for musical symbol segmentation and clas-\nsiﬁcation, without the need for stafﬂine removal. We ap-\nply improvements in the training setup that help overcome\nOMR-speciﬁc issues. The results in Sec. 5 show that the\nimprovements one expects from deep learning in computer\nvision are indeed present.\n(2) Full-Pipeline Handwritten OMR Baseline for\nPitch Accuracy and Retrieval. We combine our stage\n(3) symbol detection results with a baseline stage (4) sys-\ntem for notation assembly and pitch inference. This OMR\nsystem already achieves promising pitch-based retrieval re-\nsults on handwritten music notation; to the best of our\nknowledge, its pitch inference f-score of 0.81 is the ﬁrst\nreported result of its kind, and it is the ﬁrst published full-\npipeline OMR system to demonstrably perform a useful\ntask well on handwritten music.\n2. RELATED WORK\nU-Nets. U-Nets [38] are fully convolutional networks\nshaped like an autoencoder that introduce skip-connections\nbetween corresponding layers of the downsampling and\nupsampling halves of the model (see Fig. 2). For each\npixel, they output a probability of belonging to a speciﬁc\nclass. U-Nets are meant for semantic segmentation, not\ninstance segmentation/object detection, which means that\nthey require an ad-hoc detector on top of the pixel-wise\noutput. On the other hand, this formulation avoids domain-\nspeciﬁc hyperparameters such as choosing R-CNN anchor\nbox sizes, is agnostic towards the shapes of the objects we\nare looking for, and does not assume any implicit priors\non their sizes. This promises that the same hyperparameter\nsettings can be used for all the visually disparate classes\n(the one neuralgic point being the choice of receptive ﬁeld\nsize). Furthermore, U-Nets process the entire image in a\nsingle shot — which is a considerable advantage, as music\nnotation often contains upwards of 500 symbols on a single\npage. A disadvantage of U-Nets (as well as most CNNs)\nis their sensitivity to the training data distribution, includ-\ning the digital imaging process. Because of the variability\nof musical manuscripts, it is likely real-world applications\nwill require case-speciﬁc training data, and data augmen-\ntation would therefore be used to mitigate this sensitivity;\nfortunately, fully convolutional networks are known to re-\nspond well to data augmentation over sheet music [30] as\nwell as over other application scenarios [9, 23]. Therefore,\nwe consider this choice reasonable, at the very least to es-\ntablish a strong baseline for handwritten musical symboldetection with deep learning.\nObject Detection CNNs. A standard architecture for\nobject detection is the Regional CNN (R-CNN) family,\nmost notably Faster R-CNN [40] and Mask R-CNN [26]).\nThese networks output probabilities of an object’s pres-\nence in each one of a pre-deﬁned sets of anchor boxes, and\nmake the bounding box predictions more accurate with re-\ngression. In comparison, the U-Net architecture may have\nan advantage in dealing with musical symbols that have\nsigniﬁcantly varying extents, such as beams or stems, as\nit does not require specifying the appropriate anchor box\nsizes, and it is signiﬁcantly faster, requiring only one pass\nof the network (the detector then requires one connected\ncomponent search). Furthermore, Faster R-CNN does not\noutput pixel masks, which are useful for archival- and\nmusicology-oriented applications downstream of OMR,\nsuch as handwriting-based authorship attribution. Mask\nR-CNN, admittedly, does not have this limitation, but still\nrequires the same bounding box setup.\nAnother option is the YOLO architecture [25], speciﬁ-\ncally the most recent version YOLOv3 [36], which predicts\nbounding boxes and conﬁdence degrees without the need\nto specify anchor boxes. A similar approach was proposed\nin [22], achieveing a notehead detection f-score of 0.97,\nbut only with a post-ﬁltering step.\nConvolutional Networks in OMR. Convolutional net-\nworks have been applied in OMR to symbol classiﬁca-\ntion [33], indicating that they can in principle handle the\nvariability of music notation symbols, but not yet in also\nﬁnding the symbols on the page. Fully convolutional net-\nworks have been successfully applied to staff removal [4],\nand to resolving the document to a background, staff, text,\nand symbol layers [11]. However, these are semantic seg-\nmentation tasks; whereas we need to make decisions about\nindividual symbols. The potential of U-Nets for sym-\nbol detection was preliminarily demonstrated on noteheads\n[22, 31], but compared to other symbol classes, noteheads\nare “easy targets”, as they look different from other ele-\nments, have constant size, and appear only in one pose (as\nopposed to, e.g., beams).\nOMR Symbol Detection. Localizing symbols on the\npage has been previously addressed with heuristics rather\nthan machine learning, e.g. with projections [8, 18],\nKalman Filters [14], Line Adjacency Graphs [37], or other\ncombinations of low-level image features [39]. On hand-\nwritten music, due to its variability, more complex heuris-\ntics such as the algorithm of [1] that consists of 14 interde-\npendent steps have been applied.\nOMR for Content-Based Retrieval. The idea of us-\ning imperfect OMR for retrieval is not new, although orig-\ninally OMR was attempted in the context of transcribing\nindividual scores. In the PROBADO project [17, 28], an\noff-the-shelf OMR system was applied to printed Com-\nmon Western Music Notation (CWMN) scores, allowing\nretrieval and measure-level score following in a database of\n1200 printed scores. The Liber Usualis project at SIMSSA\nis another such project, on square plainchant notation; it\noperates at a more ﬁne-grained level that allows for ex-226 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 2 . Baseline U-Net model architecture.\nample accurate motif retrieval [3]. However, for CWMN\nmanuscripts , we are not aware of similar experiments.\n3. MODEL\nFor all experiments, we use as a basis the same fully con-\nvolutional network architecture [38] as shown in Figure 2.\nThere are three down-sampling blocks and three corre-\nsponding up-sampling blocks. Each down-sampling block\nconsists of two convolutional layers with batch normal-\nization using the same number of ﬁlters; down-sampling\nis done through 2x2 Max Pooling. After each downsiz-\ning step, we use twice the number of ﬁlters. The output\nlayer uses sigmoid activation; otherwise, ELU nonlinear-\nity is used. Additionally, we add element-wise-sum resid-\nual connections between symmetric layers of the encoder\nand decoder part of the network.\nIn the rest of this section, we propose modiﬁcations for\nboth architecture and training strategy for symbol detection\nin handwritten sheet music.\n3.1 Convex Hull Segmentation Targets\nOur ﬁrst proposal is to use the convex hull region of indi-\nvidual symbols as a target for training instead of the orig-\ninal segmentation masks. Figure 3 shows an example of\nthe modiﬁed training targets. This simple adaptation is an\nelegant way of dealing with symbols such as f-clefs or c-\nclefs, which by deﬁnition consist of multiple components.\nAs we employ a connected components detector for recog-\nnizing the symbols in our experiments in Section 4 we cir-\ncumvent the need for treating these symbol classes in any\nspecial way. This advantage also holds “pre-emptively”\nfor complex symbols which for example contain “holes”\nand might break up into multiple components after imper-\nfect automatic segmentation, or may be disconnected due\nto handwriting style (e.g., ﬂats).\n3.2 Multichannel Training\nOur second proposal is to train multichannel U-Nets pre-\ndicting the segmentation simultaneously for multiple sym-\nbol classes. This design choice has two advantages over\nFigure 3 . Training on convex hulls circumvents detection\nproblems for symbols consisting of multiple connected\ncomponents (see f-clef).\ntraining separate detectors for each class. Firstly, at run-\ntime we can predict the segmentation for multiple symbols\nwith a single forward pass of the network. Furthermore, by\nsimultaneously training on multiple symbols at the same\ntime, we allow the model to share low-level feature maps\nfor a certain symbol group (i.e., noteheads, beams, ﬂags\nand stems), and on the other hand force the model to\nlearn upper-layer features that discriminate well between\nthe various symbols, which – because the capacity of the\nmodel stays ﬁxed, and the output layer only uses 1x1 con-\nvolutions – could lead to more descriptive representations\nof the image. In other words, due to the strong correlations\nacross classes induced by music notation syntax, whatever\nfeatures are learned for one output channel will at the same\ntime be relevant for a different channel; the 1x1 convolu-\ntion will simply weigh them differently.\nHowever, this setup presents an optimization problem\ndue to imbalanced classes: both in terms of how many\nforeground pixels there are (i.e. beams vs. duration dots),\nand with respect to how often they occur on an “average”\npage of sheet music (noteheads vs. clefs). We address the\nﬁrst issue by splitting the multichannel model into groups\nof symbols with roughly similar amount of foreground pix-\nels across the dataset. To overcome the second issue, as the\ntraining setup operates on randomly chosen windows of\nthe input image (see Sec. 4), we use oversampling: when\ndrawing the random window when a training batch is be-\ning built, we check whether the window contains at least\none pixel of the target class, and we retry up to ﬁve times if\nthere is none. If no target class pixel is found in ﬁve tries,\nwe concede and use the last sampled window, even though\nno pixel of target class is in it. (As opposed to this over-\nsampling, adjusting the weights of the output channels did\nnot lead to improvements.)\nFurthermore, if model capacity becomes a limiting fac-\ntor, we can opt out of sharing the up-sampling part of the\nmodel and keep a separate “decoder” for each output chan-\nnel. This is a compromise that retains some of the speed,\nspace and feature-sharing advantages, but at the same time\ndoes not so severely restrict the capacity of the model.\n4. EXPERIMENTAL SETUP\nWe restrict ourselves to the subset of symbol classes that\nare necessary for pitch inference and basic duration infer-\nence (we currently do not detect tuplets – detecting hand-\nwritten digits is straightforward enough, the difﬁculty with\ntuplets lies in the notation assembly stage). Already thisProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 227selection contains symbols with heterogeneous appear-\nance: constant-size, trivial shape (speciﬁcally, noteheads,\nledger lines, whole and half rests, duration dots), constant-\nsize, non-trivial shape (clefs, ﬂags, accidentals, quarter-,\n8th- and 16-th rests), and symbols that have simple shapes,\nbut varying extent (stems, beams, barlines).2We assume\nbinary inputs, not least because large-scale OMR symbol\ndetection ground truth is only available for binary images;\nhowever, binarization can be done with the same model.\nDataset. We use the MUSCIMA++ dataset, version 1.0\n[20]. This is the only publicly available dataset of hand-\nwritten music notation with ground truth for symbol de-\ntection at a scale that is feasible for machine learning.\nThe dataset contains over 90 000 manually annotated sym-\nbols with pixel masks. We use the designated writer-\nindependent test set from MUSCIMA++.\nTraining Details. We set the network input size to a\n256\u0002512window and randomly sample crops of this size\nas training samples. We train all our models using the\nAdam update rule with an initial learning rate of 0.001 [27]\nand a batch size of 2 (with the 256\u0002512input window,\nthis is equivalent to batches of a single 512\u0002512image\nof [38]). After there is no update on the validation loss for\n25 epochs, we divide the learning rate by 5 and continue\ntraining from the previously best model. This procedure is\nrepeated two times.\n5. RESULTS\nAs there is no work to which we can compare directly, we\nﬁrst gather at least related OMR solutions, in order to pro-\nvide whatever context we can for the reader. Then, we\nreport results for symbol detection, and evaluate it in con-\ntext of downstream tasks: pitch inference in a baseline full-\npipeline OMR scenario, as well as ﬁrst experiments apply-\ning our models in retrieval settings.\n5.1 Comparison to Existing Systems\nComparison to existing systems is hard, because there are\nfew symbol detection results reported, and even fewer full-\npipeline OMR results. Direct comparison is not possible,\nas the MUSCIMA++ dataset we use has been released only\nvery recently, and previous OMR pipelines (see Sec. 2)\ngenerally do not have publicly available code. Further-\nmore, earlier literature on OMR rarely provides evalua-\ntion scores, most of previous work on OMR has (sensi-\nbly) focused on printed music rather than manuscripts, and\nthere are few established evaluation practices in OMR any-\nway [15, 21]. We do our best to at least gather literature\nwhere some results on related tasks are given, in order to\nprovide context for our work.\nPitch accuracy, printed music. In printed music, re-\nsults for pitch accuracy have been consistently very good,\nwhen reported. Already in [32], the GAMUT system is\nsaid to correctly recover 96 % of pitches in printed music.\n2There are also notation symbols that can have non-trivial shape and\nvarying extent, such as slurs or hairpins; however, these are not required\nfor neither pitch, nor duration inference, and we therefore leave them out.The complex fuzzy system of [39] achieves near-perfect\npitch accuracy (98.7 %). Similarly, the CANTOR system\nevaluated in [5] achieves 98 % semantic accuracy — this\ntime, including polyphonic music. On printed square nota-\ntion, [19] achieves 95 % pitch accuracy. A combination of\nsystems in [42] achieves over 85 % joint pitch and duration\naccuracy.\nSymbol detection, handwritten music. The most ex-\ntensive evaluation of symbol detection in handwritten mu-\nsic has been carried out in [1]. Using a complex combi-\nnation of robust heuristics for segmentation and machine\nlearning for classiﬁcation, they achieve an average sym-\nbol detection f-score of 0.75. These results seem ripe to\nbe surpassed with CNNs: in [31], 98 % handwritten note-\nhead detection accuracy has been reported. For staff detec-\ntion, a similar architecture has been used in [4] with over\n97 % pixel-wise f-score, and similar results are available\nwith a ConvNet pixel classiﬁcation approach for seman-\ntic segmentation into background, text, staffs, and notation\nsymbols [11]. At the same time, [33] reports symbol clas-\nsiﬁcation (without localization) accuracy over 98 %, indi-\ncating that CNNs are well capable of generalizing over the\nvariety of handwritten musical symbols. However, we are\nnotaware of pitch accuracy results reported on handwritten\nCWMN scores.\nOMR for Retrieval. For retrieval, it is even harder to\nﬁnd comparable results, since evaluation metrics for re-\ntrieval depend on the test collection, and there is no such\nestablished collection for OMR. Using the open-source\nAudiveris3OMR software, [7] matches 9803 printed\nmonophonic fragments from A Dictionary of Musical\nThemes to their electronic counterparts, using a compara-\nble DTW alignment that also (mostly) ignores note dura-\ntion, reporting a top-1 accuracy of 0.44; however, the col-\nlection of themes is a difﬁcult one, since it often contains\nvery similar melodies.\nFigure 4 . Results for binary segmentation models for indi-\nvidual symbols. Blue: baseline training with mask output;\ngreen: training with convex hulls.\n5.2 Symbol Detection\nWe report detection f-scores for the chosen subset of sym-\nbols. Aggregating the results is not too meaningful: some\nrare symbols have an outsized impact on downstream pro-\ncessing (clefs). In Fig. 4, we show the baseline results and\ncompare them to the convex hull setup. Training against\n3https://github.com/audiveris/audiveris228 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Method c-clef g-clef f-clef\nsingle channel – no convex hull 0.48 0.58 0.52\nsingle channel 0.70 0.83 0.95\nmulti-channel – all 0.16 0.37 0.49\nmulti-decoder – clefs, oversampling 0.77 0.96 0.93\nTable 1 . Comparison of detection performance (F-score)\nof clefs using different segmentation strategies.\nconvex hulls of objects does address the issue of detecting\notherwise disjoint symbols using connected components;\notherwise it achieves mixed results.\nCompressing the detector with multichannel training\nwithout a loss of performance was possible on corre-\nlated sub-groups of symbols that bypass the class imbal-\nance problem, such as training together noteheads, stems,\nbeams, and ﬂags; the results worsened when all classes\nwere trained at once. The clefs were most affected by all\nthe changes to the model described in Section 3: improved\nby convex hull training, neglected when the multichannel\nmodel was trained to predict all symbol classes at once, and\nthen drastically improved again when trained as a group\nwith separate decoders and the oversampling strategy. Ta-\nble 1 summarizes the results for clef detection. Clefs are\ncritical for useful OMR, since they affect the pitch inferred\nfrom all subsequent noteheads.\n6. APPLICATION SCENARIO: FULL-PIPELINE\nHANDWRITTEN OMR IN RETRIEV AL\nWe now explore the utility of the symbol detectors within\nan OMR pipeline. It is known in OMR that low-level er-\nrors can lead to effects on recognition of wildly different\nmagnitudes [15, 35]; in the presence of detection errors,\none should therefore see how severely they impact down-\nstream applications. We choose a retrieval scenario as the\napplication context for evaluating symbol detection. As\nopposed to applications where we produce the transcribed\nscore [15, 21, 41], this is straightforward to evaluate.\nTo verify that our symbol detection approach can yield\nuseful results in an application context, we add a simple\nnotation assembly and pitch inference system on top of the\nsymbol detection results. We choose retrieval as the most\nfeasible application of handwritten OMR: there are music\nmanuscript archives with thousands of scores that contain\nmanual copies, and matching them cannot be done without\ntheir musical content.\nFor inferring pitch, we must re-introduce stafﬂines.\nHowever, we can safely assume they have been detected\ncorrectly: both [4] and our replication of their experiments\nwith stafﬂines on this dataset exhibit extremely few er-\nrors, and these can be ﬁltered away with a trivial projection\nheuristic such as that of [18].\n6.1 Notation Assembly and Music Inference\nSymbol detection alone is not sufﬁcient for decoding mu-\nsical information: meaningful units are conﬁgurations ofsymbols rather than the symbols themselves [6, 20]. The\nnotation assembly stage is the step where these conﬁgura-\ntions are recovered (step (4) in the OMR pipeline: see 1).\nIn the MUSCIMA++ dataset, they are represented as an\noriented graph; once this graph is recovered, one can per-\nform deterministic pitch inference.4\nSymbol detection outputs vertices of the notation graph;\nwe therefore need to recover graph edges. Replicating the\nbaseline established in [20], we train a binary classiﬁer\nover ordered symbol pairs. While this classiﬁer achieves\nan f-score of 0.92, it makes embarrassing errors: noteheads\nconnected to irrelevant ledger lines in chords, to beams that\nbelong to an entirely different staff, and sometimes to mul-\ntiple adjacent stems. We discard these obviously wrong\nedges using straightforward heuristics. We also discard de-\ntected objects that are entirely contained within another de-\ntected object. The last step is recovering precedence edges:\nwe just order rest and noteheads on each staff left-to-right;\nnoteheads connected to the same stem are considered si-\nmultaneous, but actual polyphony is ignored.\nOnce the pitches, durations, and onsets are inferred for\nthe detected noteheads, we then export them as a MIDI ﬁle.\nMIDI is appropriate for retrieval, since it presents straight-\nforward ways of computing similarity. This ﬁle then can\nserve as both the query and the database key for the given\nscore. To compute the similarity of two MIDI ﬁles, we\nalign them using Dynamic Time Warping [29] (DTW) over\nsequences of time frames that contain onsets. The DTW\nscore function for a pair of frames is 1minus the Dice coef-\nﬁcient of the onset pitch sets in the frames. Then, we match\nindividual pitches within the frame sets that are aligned by\nDTW and measure the f-score of predicted pitches. DTW\nis used as the similarity function in [7]; however, we do not\nreduce polyphonic music to its upper pitch envelope.\n6.2 Results\nWe now report how the full-pipeline baseline on top of the\nobject detection U-Nets predicts pitches, and how it can be\nused to retrieve related scores.\nPitch accuracy. We use the DTW alignment to directly\nevaluate pitch classiﬁcation.5Performing DTW on the\ninference outputs for page images, we achieve a (micro-\n)average F-score of only 0.59. Rather than due to errors in\nsymbol detection, this is mostly due to the polyphony de-\nsynchronization effects of bad duration inference; indeed,\non (mostly) monophonic music, pitch F-score jumps to\n0:78. In order to bypass de-synchronization problems that\nin fact obscure correct pitch recognition, we split the scores\ninto individual staffs (118 in total) and evaluate pitch accu-\nracy on these. The results for the test set staffs are reported\nin Fig. 5. On average, we obtain pitch F-score 0.81, with\n0.83 for monophonic staffs (and ignoring clef errors, 0.88).\nFinally, we evaluate our detector in the context of a\nretrieval application. We run experiments both on gold-\n4A proof-of-concept implementation: https://github.com/\nhajicj/muscima .\n5We could evaluate duration classiﬁcation as well, but due to errors by\nthe notation assembly baseline, this is too low to be worth reporting.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 229Figure 5 . Pitch F-score after DTW alignments on the 118\nindividual staffs in the writer-independent test set, ordered\nby result. Monophonic staffs (darker green) predictably\nscore better than staffs with multiple voices or chords (yel-\nlow). We found no clear relationship between pitch accu-\nracy and handwriting style.\nstandard MIDI retrieval and duplicate score retrieval, us-\ning the predicted scores; since the similarity metric is pitch\nf-score, all retrieval experiments work in both directions.\nExperiments with ground truth MIDI correspond to cross-\nmodal retrieval, where the modalities are a symbolic rep-\nresentation, and the score projected into the MIDI modal-\nity using the OMR system; queries with predictions corre-\nspond to a simpler scenario where we are querying scores\nwith scores, using the OMR system as a hash function.\nRetrieving gold MIDI with scores. Given how small\nthe test set is, retrieving the correct ground truth page —\nand even staff — should be near-perfect. For staff-to-staff\nretrieval, Prec@1 is 0.93; for page-to-page and staff-to-\npage retrieval, this is 1.0, indicating that with our U-Net\nobject detection stage, retrieving gold-standard MIDI us-\ning handwritten scores (and vice versa, as the similarity\nmetric is symmetrical) is feasible.\nRetrieving scores with scores. The next scenario is\nto run retrieval not against the ground truth, but against\nMIDIs predicted from different versions of the test set\nscores. While errors related to differences in handwriting\nget compounded, the rest of the pipeline imposes consis-\ntent limitations on both the database and query recognition\noutputs and may make the same errors on both query and\ndatabase scores, making the task actually easier. There-\nfore, we select a confuse-retrieval subset of 7 scores from\nMUSCIMA++ that are as similar to each other as possi-\nble: mostly monophonic, and with 0 – 2 sharps. Some of\nthese pieces are musically closely related. For these exper-\niments, our database consists of recognition outputs com-\nputed from all confuse-retrieval pages in the training sub-\nset of MUSCIMA++. Queries are taken from predictions\non the writer-independent test set: we use both the 7 entire\npages and individual staffs (34 of those).\nThe system achieves perfect Prec@1 when pages are\nused as queries, and 0.94 when using staff queries (2 staff\nqueries did not return the right piece as the top result). The\nretrieval scores are plotted in Fig. 6. We checked this score\nalso with ground truth queries; this system made only 2 er-\nrors as well, but in different queries, which we take as cir-\ncumstantial evidence that the ground truth MIDI has differ-\nent issues when matching against a predicted MIDI than a\ndifferent prediction. When measuring MAP with the cutoff\nk=6 (as there are 7 versions of each page in MUSCIMA++\nFigure 6 . Pitch f-score between predictions on test set\nstaffs and (predictions on) training set pages. Notice the\npages 07, 09 and 11: these are three movements from\nJ. S. Bach’s Cello suite no. 1, which contain musically\nhighly related material.\nand one of them is used for querying), it drops to 0.86.\n7. DISCUSSION & CONCLUSIONS\nWe consider our work a successful step towards enabling\napplications of hitherto problematic handwritten OMR.\nThe retrieval scenario results are an indication that U-Nets\nare a workable solution to the handwritten symbol detec-\ntion bottleneck in the context of full-pipeline OMR. (Here,\nwe must re-state that these results should notbe interpreted\nas more than supporting evidence that our object detection\nmethod is viable for such scenarios!)\nHowever, U-Nets are still in principle limited by the size\nof the receptive ﬁeld: for instance the middle of a long\nstem looks exactly the same as a barline. We could fur-\nther leverage syntactic properties of music notation: e.g.,\nthe self-attention layer of [34] allows building up the ﬁ-\nnal output from partial recognition results. Fragmenting of\nlong symbols could be overcome with instance segmenta-\ntion embeddings [10].\nTo the best of our knowledge, this is also the ﬁrst time\nOMR was done with a machine-learning method for nota-\ntion assembly. We in fact consider this the most interest-\ning line of follow-up work. Recovering the notation graph\nitself seems like the next bottleneck, especially for dura-\ntion inference. The non-independent nature of the edges\nposes an interesting structured prediction challenge, and\none could also work towards models that jointly detect\nsymbols and recover their relationships.\nDespite their limitations, U-Nets can be used to de-\ntect handwritten music notation symbols. They establish\na new CNN-based baseline for the object detection task,\nand we believe the results in pitch inference and a proof-\nof-concept retrieval scenario indicate that a signiﬁcant step\nhas been taken towards full-pipeline OMR systems, so that\nthe content of musical manuscripts can become accessible\ndigitally.\n8. ACKNOWLEDGMENTS\nJan Haji ˇc jr. and Pavel Pecina acknowledge support by\nthe Czech Science Foundation grant no. P103/12/G084,\nCharles University Grant Agency grants 1444217 and\n170217, and by SVV project 260 453.230 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20189. REFERENCES\n[1] Ana Rebelo. Robust Optical Recognition of Handwrit-\nten Musical Scores based on Domain Knowledge . PhD\nthesis, 2012.\n[2] Ana Rebelo, Ichiro Fujinaga, Filipe Paszkiewicz, An-\ndre R. S. Marcal, Carlos Guedes, and Jaime S. Car-\ndoso. Optical Music Recognition: State-of-the-Art and\nOpen Issues. Int J Multimed Info Retr , 1(3):173–190,\nMar 2012.\n[3] Andrew Hankinson, John Ashley Burgoyne, Gabriel\nVigliensoni, Alastair Porter, Jessica Thompson,\nWendy Liu, Remi Chiu, and Ichiro Fujinaga. Dig-\nital Document Image Retrieval Using Optical Mu-\nsic Recognition. In Proceedings of the 13th Interna-\ntional Society for Music Information Retrieval Confer-\nence, ISMIR 2012, Mosteiro S.Bento Da Vit ´oria, Porto,\nPortugal, October 8-12, 2012 , pages 577–582. FEUP\nEdic ¸ ˜oes, 2012.\n[4] Antonio-Javier Gallego and Jorge Calvo Zaragoza.\nStaff-line removal with selectional auto-encoders. Ex-\npert Systems with Applications , 89:138–148, 2017.\n[5] David Bainbridge. Extensible optical music recogni-\ntion. page 112, 1997.\n[6] David Bainbridge and Tim Bell. A music notation con-\nstruction engine for optical music recognition. Soft-\nware - Practice and Experience , 33(2):173–200, 2003.\n[7] Stefan Balke, Sanu Pulimootil Achankunju, and\nMeinard M ¨uller. Matching Musical Themes based on\nnoisy OCR and OMR input. pages 703–707, 2015.\n[8] P. Bellini, I. Bruno, and P. Nesi. Optical music sheet\nsegmentation. In Proceedings First International Con-\nference on WEB Delivering of Music. WEDELMUSIC\n2001 , pages 183–190. Institute of Electrical & Elec-\ntronics Engineers (IEEE), 2001.\n[9] Avi Ben Cohen, Idit Diamant, Eyal Klang, Michal\nAmitai, and Hayit Greenspan. Fully Convolutional\nNetwork for Liver Segmentation and Lesions Detec-\ntion. In Gustavo Carneiro, Diana Mateus, Lo ¨ıc Peter,\nAndrew Bradley, Jo ˜ao Manuel R. S. Tavares, Vasileios\nBelagiannis, Jo ˜ao Paulo Papa, Jacinto C. Nascimento,\nMarco Loog, Zhi Lu, Jaime S. Cardoso, and Julien\nCornebise, editors, Deep Learning and Data Labeling\nfor Medical Applications , pages 77–85, Cham, 2016.\nSpringer International Publishing.\n[10] Bert De Brabandere, Davy Neven, and Luc Van Gool.\nSemantic Instance Segmentation with a Discriminative\nLoss Function. CoRR , abs/1708.02551, 2017.\n[11] Jorge Calvo Zaragoza, Gabriel Vigliensoni, and Ichiro\nFujinaga. A machine learning framework for the cat-\negorization of elements in images of musical docu-\nments. In Third International Conference on Technolo-\ngies for Music Notation and Representation , A Coru ˜na,\n2017. University of A Coru ˜na.[12] Sukalpa Chanda, Debleena Das, Umapada Pal, and Fu-\nmitaka Kimura. Ofﬂine Hand-Written Musical Symbol\nRecognition. 2014 14th International Conference on\nFrontiers in Handwriting Recognition , pages 405–410,\nsep 2014.\n[13] Cuihong Wen, Jing Zhang, Ana Rebelo, and Fanyong\nCheng. A Directed Acyclic Graph-Large Margin Dis-\ntribution Machine Model for Music Symbol Classiﬁca-\ntion. PLOS ONE , 11(3):e0149688, mar 2016.\n[14] V.P. d’Andecy, J. Camillerapp, and I. Leplumey.\nKalman ﬁltering for segment detection: application to\nmusic scores analysis. In Proceedings of 12th Interna-\ntional Conference on Pattern Recognition . IEEE Com-\nput. Soc. Press, 1994.\n[15] Donald Byrd and Jakob Grue Simonsen. Towards a\nStandard Testbed for Optical Music Recognition: Deﬁ-\nnitions, Metrics, and Page Images. Journal of New Mu-\nsic Research , 44(3):169–195, 2015.\n[16] Eelco van der Wel and Karen Ullrich. Optical\nMusic Recognition with Convolutional Sequence-to-\nSequence Models. CoRR , abs/1707.04877, 2017.\n[17] Christian Fremerey, Meinard M ¨uller, Frank Kurth, and\nMichael Clausen. Automatic mapping of scanned sheet\nmusic to audio recordings. Proceedings of the Inter-\nnational Conference on Music Information Retrieval ,\npages 413–418, 2008.\n[18] Ichlro Fujinaga. Optical Music Recognition using Pro-\njections. Master’s thesis, 1988.\n[19] Gabriel Vigliensoni, John Ashley Burgoyne, Andrew\nHankinson, and Ichiro Fujinaga. Automatic Pitch De-\ntection in Printed Square Notation. In Proceedings of\nthe 12th International Society for Music Information\nRetrieval Conference, ISMIR 2011, Miami, Florida,\nUSA, October 24-28, 2011 , pages 423–428. University\nof Miami, 2011.\n[20] Jan Haji ˇc jr. and Pavel Pecina. The MUSCIMA++\nDataset for Handwritten Optical Music Recognition.\nIn14th International Conference on Document Anal-\nysis and Recognition , pages 39–46, New York, USA,\nNovember 2017. Dept. of Computer Science and Intel-\nligent Systems, Graduate School of Engineering, Os-\naka Prefecture University, IEEE Computer Society.\n[21] Jan Haji ˇc jr., Ji ˇr´ı Novotn ´y, Pavel Pecina, and Jaroslav\nPokorn ´y. Further Steps towards a Standard Testbed for\nOptical Music Recognition. In Proceedings of the 17th\nInternational Society for Music Information Retrieval\nConference , pages 157–163, New York, USA, 2016.\nNew York University, New York University.\n[22] Jan Haji ˇc Jr. and Pavel Pecina. Detecting Noteheads in\nHandwritten Scores with ConvNets and Bounding Box\nRegression. CoRR , abs/1708.01806, 2017.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 231[23] Jesus Munoz Bulnes, Carlos Fernandez, Ignacio Parra,\nDavid Fernandez Llorca, and Miguel A. Sotelo. Deep\nfully convolutional networks with random data aug-\nmentation for enhanced generalization in road detec-\ntion. In 2017 IEEE 20th International Conference on\nIntelligent Transportation Systems (ITSC) . IEEE, oct\n2017.\n[24] Jorge Calvo-Zaragoza, Jose J. Valero-Mas, and An-\ntonio Pertusa. End-to-End Optical Music Recognition\nUsing Neural Networks. In Proceedings of the 18th\nInternational Society for Music Information Retrieval\nConference, ISMIR 2017, Suzhou, China, October 23-\n27, 2017 , pages 472–477, 2017.\n[25] Joseph Redmon, Santosh Kumar Divvala, Ross B. Gir-\nshick, and Ali Farhadi. You Only Look Once: Uniﬁed,\nReal-Time Object Detection. CoRR , abs/1506.02640,\n2015.\n[26] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross\nB. Girshick. Mask R-CNN. CoRR , abs/1703.06870,\n2017.\n[27] Diederik Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. International Conference on\nLearning Representations (ICLR) (arXiv:1412.6980) ,\n2015.\n[28] F. Kurth, M. M ¨uller, C. Fremerey, Y. Chang, and M.\nClausen. Automated synchronization of scanned sheet\nmusic with audio recordings. Proc. ISMIR, Vienna, AT ,\npages 261–266, 2007.\n[29] Lawrence R. Rabiner and Biing-Hwang Juang. Fun-\ndamentals of speech recognition . Prentice Hall signal\nprocessing series. Prentice Hall, 1993.\n[30] Matthias Dorfer, Andreas Arzt, and Gerhard Wid-\nmer. Learning Audio-Sheet Music Correspondences\nfor Score Identiﬁcation and Ofﬂine Alignment. In Pro-\nceedings of the 18th International Society for Music In-\nformation Retrieval Conference, ISMIR 2017, Suzhou,\nChina, October 23-27, 2017 , pages 115–122, 2017.\n[31] Matthias Dorfer, jr. Jan Haji ˇc, and Gerhard Wid-\nmer. On the Potential of Fully Convolutional Neu-\nral Networks for Musical Symbol Detection. In Pro-\nceedings of the 12th IAPR International Workshop on\nGraphics Recognition , pages 53–54, New York, USA,\n2017. IAPR TC10 (Technical Committee on Graphics\nRecognition), IEEE Computer Society.\n[32] Michael Droettboom and Ichiro Fujinaga. Symbol-\nlevel groundtruthing environment for OMR. Proceed-\nings of the 5th International Conference on Music\nInformation Retrieval (ISMIR 2004) , pages 497–500,\n2004.\n[33] Alexander Pacha and Horst Eidenberger. Towards a\nUniversal Music Symbol Classiﬁer. In Proceedings of\nthe 12th International Workshop on Graphics Recogni-\ntion, 2017.[34] N. Parmar, A. Vaswani, J. Uszkoreit, Ł. Kaiser, N.\nShazeer, and A. Ku. Image Transformer. ArXiv e-\nprints , February 2018.\n[35] Pierfrancesco Bellini, Ivan Bruno, and Paolo Nesi. As-\nsessing Optical Music Recognition Tools. Computer\nMusic Journal , 31(1):68–93, Mar 2007.\n[36] Joseph Redmon and Ali Farhadi. YOLOv3: An Incre-\nmental Improvement. Technical report, 2018.\n[37] K. T. Reed and J. R. Parker. Automatic computer\nrecognition of printed music. Proceedings - Interna-\ntional Conference on Pattern Recognition , 3:803–807,\n1996.\n[38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\nU-Net: Convolutional Networks for Biomedical Im-\nage Segmentation. In Medical Image Computing and\nComputer-Assisted Intervention – MICCAI 2015: 18th\nInternational Conference, Munich, Germany, Octo-\nber 5-9, 2015, Proceedings, Part III , pages 234–241,\nCham, 2015. Springer International Publishing.\n[39] Florence Rossant and Isabelle Bloch. Robust and\nAdaptive OMR System Including Fuzzy Modeling, Fu-\nsion of Musical Rules, and Possible Error Detection.\nEURASIP Journal on Advances in Signal Processing ,\n2007(1):081541, 2007.\n[40] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian\nSun. Faster R-CNN: Towards Real-Time Object De-\ntection with Region Proposal Networks. In Advances\nin Neural Information Processing Systems 28: An-\nnual Conference on Neural Information Processing\nSystems 2015, December 7-12, 2015, Montreal, Que-\nbec, Canada , pages 91–99, 2015.\n[41] Mariusz Szwoch. Using MusicXML to Evaluate Accu-\nracy of OMR Systems. Proceedings of the 5th Inter-\nnational Conference on Diagrammatic Representation\nand Inference , pages 419–422, 2008.\n[42] Victor Padilla, Alan Marsden, Alex McLean, and Kia\nNg. Improving OMR for Digital Music Libraries with\nMultiple Recognisers and Multiple Sources. Proceed-\nings of the 1st International Workshop on Digital Li-\nbraries for Musicology - DLfM ’14 , pages 1–8, 2014.232 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "A Generalized Parsing Framework for Generative Models of Harmonic Syntax.",
        "author": [
            "Daniel Harasim",
            "Martin Rohrmeier",
            "Timothy J. O&apos;Donnell"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492367",
        "url": "https://doi.org/10.5281/zenodo.1492367",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/258_Paper.pdf",
        "abstract": "Modeling the structure of musical pieces constitutes a central research problem for music information retrieval, music generation, and musicology. At the present, models of harmonic syntax face challenges on the tasks of detecting local and higher-level modulations (most previous models assume a priori knowledge of key), computing connected parse trees for long sequences, and parsing sequences that do not end with tonic chords, but in turnarounds. This paper addresses those problems by proposing a new generative formalism Probabilistic Abstract Context-Free Grammars (PACFGs) to address these issues, and presents variants of standard parsing algorithms that efficiently enumerate all possible parses of long chord sequences and to estimate their probabilities. PACFGs specifically allow for structured non-terminal symbols in rich and highly flexible feature spaces. The inference procedure moreover takes advantage of these abstractions by sharing probability mass between grammar rules over joint features. The paper presents a model of the harmonic syntax of Jazz using this formalism together with stochastic variational inference to learn the probabilistic parameters of a grammar from a corpus of Jazz-standards. The PACFG model outperforms the standard context-free approach while reducing the number of free parameters and performing key finding on the fly.",
        "zenodo_id": 1492367,
        "dblp_key": "conf/ismir/HarasimRO18",
        "keywords": [
            "harmonic syntax",
            "modulation detection",
            "connected parse trees",
            "long sequences",
            "tonic chords",
            "turnarounds",
            "Probabilistic Abstract Context-Free Grammars (PACFGs)",
            "standard parsing algorithms",
            "joint features",
            "stochastic variational inference"
        ],
        "content": "A GENERALIZED PARSING FRAMEWORK FOR GENERATIVE MODELSOF HARMONIC SYNTAXDaniel Harasim1,2Martin Rohrmeier1,2Timothy J. O’Donnell31Digital and Cognitive Musicology Lab,´Ecole Polytechnique F´ed´erale de Lausanne, Switzerland2Institut f¨ur Kunst- und Musikwissenschaft, TU Dresden, Germany3Department of Linguistics, McGill University, Canadadaniel.harasim@epfl.chABSTRACTModeling the structure of musical pieces constitutes a cen-tral research problem for music information retrieval, mu-sic generation, and musicology. At the present, models ofharmonic syntax face challenges on the tasks of detectinglocal and higher-level modulations (most previous modelsassume a priori knowledge of key), computing connectedparse trees for long sequences, and parsing sequences thatdo not end with tonic chords, but in turnarounds. This pa-per addresses those problems by proposing a new genera-tive formalism Probabilistic Abstract Context-Free Gram-mars (PACFGs) to address these issues, and presents vari-ants of standard parsing algorithms that efﬁciently enumer-ate all possible parses of long chord sequences and to es-timate their probabilities. PACFGs speciﬁcally allow forstructured non-terminal symbols in rich and highly ﬂex-ible feature spaces. The inference procedure moreovertakes advantage of these abstractions by sharing probabil-ity mass between grammar rules over joint features. Thepaper presents a model of the harmonic syntax of Jazzusing this formalism together with stochastic variationalinference to learn the probabilistic parameters of a gram-mar from a corpus of Jazz-standards. The PACFG modeloutperforms the standard context-free approach while re-ducing the number of free parameters and performing keyﬁnding on the ﬂy.1. INTRODUCTIONThe modeling of non-local relations between musical ob-jects such as notes and chords constitutes a central re-search problem for music information retrieval, music gen-eration, and music analysis. Hierarchical models expressthese relations by assuming a latent hierarchical structure[19,22–24,30,31]. Consider for example the Jazz chord se-quence Am7D7G7C4where C4denotes a major-seventhchord. Since the ﬁrst three chords form a II V I sequencec\u0000Daniel Harasim, Martin Rohrmeier, Timothy J.O’Donnell. Licensed under a Creative Commons Attribution 4.0 Inter-national License (CC BY 4.0).Attribution:Daniel Harasim, MartinRohrmeier, Timothy J. O’Donnell. “A Generalized Parsing Frameworkfor Generative Models of Harmonic Syntax”, 19th International Societyfor Music Information Retrieval Conference, Paris, France, 2018.with reference to G7which is the dominant in C major,they form adominant phrase[24]. The dominant phrase asa whole then refers to the tonic chord C4. All four chordstogether thus form atonic phrase.Figure 1 presents a syntactic analysis of the A-partof the Jazz-standardAfternoon in Parisfollowing the ap-proach from [22]. It illustrates the idea of how pieces canbe decomposed into hierarchically-structuredconstituentswhich stand in part-whole relationship with one another.Subdominant, dominant, and tonic phrases are denoted bythe scale degrees II, V, and I, respectively. Note that thesubsequence Cm7F7B[4is both a tonic progression inB[major and a dominant progression in E[major. It formsa dominant phrase in A[major together with B[m7andE[7.ICICIC\nC4VCVCVC\nG7IIC\nDm7VGVDbIAbIA[\nAb4VAbVAbVAbE[7IIAbB[m7VEbIBbIBbB[4VBbVB[F7IIBbCm7IC\nC4Figure 1. Hierarchical analysis of the A-part of the Jazz-standardAfternoon in Paris.Models of harmonic syntax similar to Figure 1 havebeen successfully applied to melody harmonization [16],chord inference from audio [5, 6], and harmonic similarity[7]. There is also some empirical evidence for the psycho-logical reality of hierarchical structures in music [15, 25].While earlier theoretical and psychological work on hierar-152chical models has provided important insight about musi-cal structure, computational implementation of these mod-els to date has been limited to relatively small datasets.Earlier work includes applications to monophonic melodicdata [21], a corpus of 39 blues chord progressions with amaximum of 24 chords per progression [12], or a datasetof 76 chord progressions (avg. length 40) from Jazz-standards that was restricted to subsequences of pieces thatdid not change key [4]. All these earlier approaches as-sume the knowledge of the key of the pieces a priori.In computational linguistics, Context-Free Grammars(CFGs) are a standard way of modeling hierarchical con-stituent structure. They formalize constituent structures us-ingrewrite rulesdenoted by long right arrows. The ruleX\u0000!YZfor example states that the constituentXcon-sists ofthe two constituentsYandZ. The existence ofnatural language treebanks makes it possible to read off thegrammatical rewrite rules including their frequencies fromsyntactical analyses by experts. At present, there are mu-sic databases of simpliﬁed Schenkerian analyses [13], syn-tactic analyses of melodies based on the generative theoryof tonal music [8], and annotated harmonic functions [4].However, to the best of our knowledge there is currentlyno dataset of hierarchically analyzed chord sequences byhuman experts that could serve for the training or the eval-uation of models of harmonic syntax. As a consequence,there exist no comparisons of models of harmonic syntaxagainst expert analyses.In the following, we introduce Abstract Context-FreeGrammars (ACFGs), a generalization of the CFG frame-work designed to account for feature structures charac-teristic of musical categories. A ﬁrst model of Jazz har-mony is proposed in this framework that covers full piecesby incorporating modulations (i.e., changes in key). Wetrain the model in a semi-supervised fashion on a datasetof Jazz-standards and evaluate it on a small set of hand-annotated hierarchical analyses. We further propose a so-lution for handling sequences that do not end with tonicchords, but in turnarounds. Simulations demonstrate thatthe ACFG model is able to outperform a PCFG model ofthe dataset. The implementation of the algorithms devel-oped in this study are publicly available as a package of theJulia programming language [1].12. OVERVIEW OF THE APPROACHWhile the CFG framework has proven invaluable in com-putational linguistics, categories and part-whole relationsbetween musical constituents have properties not pos-sessed by linguistic structures. Musical categories such asscale degrees, for example, are equipped with an arithmeticstructure that corresponds to musical transposition.In the following, we refer to context-free rules of theformX\u0000!YXas apreparationofXbyY. The prepa-ration of the scale degree VB[by IIB[inAfternoon in Paris(see Figure 1) for example is a concrete realization of thegeneral principle that any categoryxkconsisting of a scale1https://github.com/dharasim/GeneralizedChartParsing.jldegreexand a keykcan be prepared by an ascending dia-tonic ﬁfth(x+4mod7)k. [24]. In addition to facts such asthese, a framework for modeling musical structure has toaccount for the fact that the musical categories and rewriterules are grouped into key-independent classes. For exam-ple, bothVB[andVA[are ﬁfth scale degrees. The prob-abilities of the application a rule toVB[andVA[shouldtherefore be related.This paper introduces Abstract Context-free Grammars(ACFGs), a modeling framework with a greater ﬂexibilitythan CFGs. In particular, in ACFGs constituent categoriesare allowed to be of any data type and the rules are general-ized partial functions. Unlike standard context-free rules,ACFG rules can therefore take advantage of the algebraicstructure of categories. Probabilistic ACFGs extend prob-abilistic CFGs with the ability to express a wider range ofprobability distributions over rules.3. ABSTRACT CONTEXT-FREE GRAMMARS3.1 DeﬁnitionsDeﬁnition 1.A(non-probabilistic) Abstract Context-freeGrammar(ACFG)G=(T,C,C0,\u0000)consists of a setTofterminal symbols, a setCofconstituent categories, a setofstart categoriesC0✓C, and a set of partial functions\u0000: ={r|r:C7!(T[C)⇤},calledrewrite rulesorrewrite functions. The arrow7!isused throughout the paper to denote partial functions. Asequence\u00002(T[C)⇤can begenerated in one stepfrom a sequence↵2(T[C)⇤by the application ofa rewrite functionr2\u0000, denoted by↵\u0000!r\u0000, ifthere exist↵1,↵22(T[C)⇤andA2Csuch that↵=↵1A↵2and\u0000=↵1r(A)↵2. A sequence of rewriterulesr1...rnis called aderivationof a sequence of termi-nals↵2T⇤if there exists a start category↵12C0, and↵2,...,↵n2(C[T)⇤such that↵1\u0000!r1↵2\u0000!r2··· \u0000 !rn↵,whereriis always applied to the leftmost category of↵ifori2{1,...,n\u00001}. The set of derivations of↵isdenoted byD(↵). The language of the grammarGis theset of terminal sequences that have a derivation inG.Note that ifCis ﬁnite, the languages that can be de-scribed by ACFGs are exactly the languages that can bedescribed by standard context-free grammars (CFGs). Foreach ACFG with ﬁniteC, a CFG with rule setRcan beconstructed by dividing each rewrite function with domaincardinalitykintokstandard context-free rewrite rules,R:=[r2\u0000{(A, ↵)2C⇥(T[C)⇤|r(A)=↵}.Deﬁnition 2.AProbabilistic Abstract Context-free Gram-mar(PACFG) is an ACFG where each categoryA2Cisassociated with a random variableXAover rewrite func-tionsrsuch thatP(XA=r)is positive if and only ifr(A)Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 153is deﬁned, that isAis in the domain ofr,A2dom(A).The probabilityp(d)of a derivationd=r1...rnof a se-quence of terminal symbols↵2T⇤is deﬁned as the prod-uctQni=1P(XAi=ri)where in each stepriis applied toa categoryAi2C. The probability of↵is then deﬁned asp(↵)=Pd2D(↵)p(d).Note that PACFG categories can share the same proba-bility distribution over rewrite functions without rewritingto exactly the same right-hand sites. This important prop-erty allows us to model the structural relations betweenmusical keys. We use this property in Section 4 to builda model that abstract chords sequences from their concretescale by deﬁning the probability that a rewrite function isapplied to a scale degree independently of its key. Thesharing of probability mass between rules additionally re-duces the number of free parameters of a PACFG model.To illustrate the different learning capabilities of PCFGand PACFG models, consider a toy PCFG with nonter-minal symbolsC={S, A, B}, start symbolS, ter-minal symbolsT={a, b}, and rulesS\u0000!A|B,A\u0000!AA|a, andB\u0000!BB|b. The grammar thusgenerates sequences that solely consist either ofas orbs.In a classical PCFG setting, no probability mass is sharedbetween rules, but each rule has its separate probability.However, in the process of inferring the probabilities ofthe rules from data, it might be desirable to generalize therulesA\u0000!AAandB\u0000!BBto a meta rulex\u0000!xxwherex2{A, B}and to put probability mass on this ab-stract entity. In that way, the grammar can learn somethingaboutA\u0000!AAwhen it observesB\u0000!BBand viceversa. The PACFG version of the PCFG presented aboveaddresses the problem by replacing the classical context-free rules by the partial functionsr1,r2,r3,r4, andr5withr1(S)=A,r2(S)=B,r3(x)=xxforx2{A, B},r4(A)=a, andr5(B)=b. Analogously, a PACFG ofJazz chord sequences can generalize classical rewrite rulesso that their probabilities do not depend on the keys of theirleft-hand sides to model transpositional invariance.3.2 ParsingParsing a sequence of terminal symbols with respect to aformal grammar is the task of computing the distributionof parse trees conditioned on this sequence. Many parsersare based on versions of the CYK algorithm that assumesgrammars to be given in Chomsky normal form. Sincegrammar transformations into Chomsky normal form con-siderably blow up the grammar, the here presented parsertransforms grammars on the ﬂy during parsing, similarto the transformation presented in [18]. Each rule of theformA\u0000!B1...Bkis transformed into a set ofstatessi=B1...Bifor1ik, a transition functiontran :S⇥(T[C)!S,tran(si,Bi+1)=si+1and a completion functioncomp :S!2Csuch that{A}✓comp(sk), whereSdenotes the set of all states.Note that the states and the transition function form asearch trie where the completion function checks if thereitems: edges[s, i, j]fors2Sconstituents[A, i, j]forA2Cfor andi, j2{1,...,|↵|+1}goal items:[A,1,|↵|+ 1]forA2Saxioms:[↵i,i ,i+ 1]fori2{1,...,|↵|}introduce edge:[A, i, j][s, i, j]s=tran(s0,A)complete edge:[s, i, j][A, i, j]A2comp(s)fundamental rule:[s, i, j][A, j, k][s0,i ,k]tran(s, A)=s0Figure 2. Description of the parsing algorithm in the pars-ing as deduction framework. Existing Constituents canstart the parser to read a sequence of terminal symbols andcategories by theintroduce edgerule. Thefundamentalruleis then recursively applied to extend these sequences.Thecomplete edgerule eventually merges sequences tosingle constituents if they are the right-hand side of a gram-mar rule.is a rewrite rule that has a sequence of terminal symbolsand categories as its right-hand side. This trie data struc-ture leads to a compact representation of the forest of alltrees for a given input sequence. More generally, the parsercan handle any transition and completion functions derivedfrom ﬁnite-state automata, see [14].In the following, a generic bottom-up parsing algorithmfor abstract grammars is presented in the parsing as de-duction framework using the above deﬁned transition andcompletion functions [3, 29]. The parsing as deductionframework is a meta-formalism to state and compare dif-ferent parsing algorithms. It views the parses of a sequenceas logical deductions of goals from axioms by using con-stituents as atomic logical formulas. The formula[IB[,2,5]for example states the existence of a constituent with cate-gory IB[that spans over the second, third, and fourth termi-nal symbol. This formula is true in the analysis presentedin Figure 1 because that analysis contains a constituentwith label IB[over the span from the second to the forthleaf chord. The goals are constituents that span the fullsequence and come from the set of start categories. Theaxioms are formulas of the form[ti,i ,i+ 1]for each ter-minal in the input sequencet1...tn. The parsing strategysuch as bottom-up parsing or Earley parsing is encoded inthe deduction rules. These rules are denoted by a set ofatomic formulas over a horizontal line, an atomic formulaunder this line, and an optional side condition (see Figure154 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20182). The formula under the line can be deduced from theformulas above if the side condition holds.The proposed algorithm makes use of two differentkinds of atomic formulas: edges (not yet completed con-stituents) and constituents. A states2Stogether with astart indexiand an end indexjis called anedgeand de-noted by[s, i, j]. Analogously, a categoryA2Ctogetherwith start and end indicesiandjis called aconstituentand denoted by[A, i, j]. Figure 2 shows the axioms, goalitems, and the deduction rules of our algorithm.3.3 Inference of Rule ProbabilitiesIn this section, we give an overview of an inference algo-rithm for the rule probabilitiesP(XA=r). Let\u0000A={r2\u0000|A2dom(r)}be the set of rewrite functionswhose domain contains the constituent categoryA.W eplace a Dirichlet distribution on the probability vector de-scribing the distribution over\u0000A,~✓\u0000A⇠Dirichlet(~↵\u0000A)for pseudocount vector~↵\u0000A. The inference problem is tocompute the posterior distribution over this set of probabil-ity vectors, given the dataDand pseudocounts{~↵\u0000A},p({~✓\u0000A}|D,{~↵\u0000A})/p(D|{~✓\u0000A})p({~✓\u0000A}|{~↵\u0000A}),where{~✓\u0000A}is an abbreviation for{~✓\u0000A}A2C, etc.Varia-tional Bayesian inference(VB) is used to approximate thisposterior distribution [2, 11, 32]. We introduce an approx-imatingvariational distributionq({~✓\u0000A}|{~⌫\u0000A})withvariational parameters{~⌫\u0000A}over our target hidden vari-ables (rule weights) and minimize the Kullback-Leibler di-vergence between this approximation and the true poste-rior,DKL(q({~✓\u0000A}|{~⌫\u0000A})||p({~✓\u0000A}|D,{~↵\u0000A})),by adjusting the variational parameters{~⌫\u0000A}.Following [17], we approximate the distribution overeach probability vector with a Dirichlet distribution~✓\u0000A|~⌫\u0000A⇠Dirichlet(~⌫\u0000A), and make use of themean-ﬁeld approximationq({~✓\u0000A}|{~⌫\u0000A})=YA2Cp(~✓\u0000A|~⌫\u0000A).We minimize the Kullback-Leibler divergence with acoordinate descent algorithm similar to the expectation-maximization algorithm. First, we compute the expec-tation of the counts of rule usages in the data under ourcurrent setting of the variational parameters,Eq[#(r, D)]where#(r, D)is the number of times that rulerwas usedto generate the dataD, and then we update our varia-tional parameters based on these expectations. Since allof our distributions are in the exponential family, it canbe shown that the optimal update is given by the equationˆ~⌫\u0000A=~↵\u0000A+Eq[#(r, D)][2]. In other words, we set thepseudocounts of our variational distributions equal to theexpected number of rule usages plus the pseudocount foreach rule in the prior distribution.Under the standard coordinate-ascent algorithm givenin [17], expected counts must be computed for the wholecorpus before updating using the equation above. Hoff-man et al. [9] propose a stochastic variant of the standardvariational (inspired bystochastic gradient descent) whereupdates are computed with respect to randomly sampledminibatchesof the data. We make use of thisstochasticvariational Bayesalgorithm in the results reported below.4. A GENERATIVE MODEL OF JAZZ HARMONYThis section presents a PACFGG=(T,C,C0,\u0000)thatmodels the syntax of Jazz harmony following the pro-posal in [24]. That work addressed the problem of ﬁnd-ing a restrictive grammar that describes the full variety ofsyntactic relations in the musical idiom of Jazz-standards.The set of terminal symbolsTis a set of pairs describingchords each of which consists of the root of the chord anda string describing the chord form—one of: a major triad,a major-seventh chord, a major sixth chord, a dominant-seventh chord, a minor triad, a minor-seventh chord, a half-diminished-seventh chord, a diminished seventh-chord, anaugmented triad, or a suspended chord.In the following,Zndenotes the ring of integers mod-ulon2N. The categories are modeled as pairs of scaledegrees and keys,C=Z7⇥K, where a key consists ofa pitch class representing its root and a string describingits mode,K=Z12⇥{major,min}. Scale degrees aredenoted by roman numerals from I to VII. All categorieswith scale degree I are start symbols,C0={I}⇥K. Letk2Kdenote an arbitrary key. The set of rewrite functions\u0000consists ofprolongation,PROLONG(hx, ki)=hx, kihx, kiforx2Z7,diatonic preparation,DIAT-PREP(hx, ki)=hx+4mod7,kihx, kiforx2Z7\\{IV},dominant preparation,DOM-PREP(hx, ki)=hV,µ(x, k)ihx, kiforx2Z7\\{I}whereµ(x, k)denotes the modulationfromkinto the key of scale degreex(e.g.µ(II,(0,maj)) =(2,min), the key of the second scale degree of C major isD minor),plagal preparation,PLAGAL-PREP(hI,ki)=hIV,kihI,ki,modulation,MODULATION(hx, ki)=hI,µ(x, k)i,mode change,MODE-CHANGE(hI,(r, m)i)=(hI,(r,min)i,ifm=majhI,(r,maj)i,ifm=min,forr2Z12,m2{maj,min},diatonic substitution,DIAT-SUBST(hx,(r, m)i)=8>>><>>>:hVI,(r, m)i,ifx=I,m=majhIII,(r, m)i,ifx=I,m=minhIV,(r, m)i,ifx=IIhVII,(r, m)i,ifx=VProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 155Figure 3. Parsing the turnaround ofAll of meforx2{I,II,V},r2Z12,m2{maj,min}, anddomi-nant substitution,DOM-SUBSTi(hV,(r, m)i)=hV,(r+imod12,m)iforr2Z12,m2{maj,min}, andi2{3,6,9}.Additionally,\u0000contains appropriate termination rulesC7!Taccording to standard Jazz harmony theory (e.g.seventh-chord-termination(h4,(0,maj)i)=G7, see [20]for further explanation). The distribution ofXhx,kioverrules rewriting the categoryhx, kiis deﬁned as a categori-cal distribution such thatP(Xhx,ki=r)=P(Xhx,k0i=r)for all scale degreesx, rulesr, and keysk,k0that have thesame mode. That is, the probability ofrrewritinghx, kidoes not depend on the root ofkwhich enables the modelto learn the parameters of its probability distributions key-independently.These grammar rules can be grouped into three classes:the prolongation rule, preparation rules, and substitutionrules. Preparation rules create categories that for the lis-tener generate the expectation to hear the prepared chord.Substitution rules substitute chords for other chords thatfulﬁll an equivalent function inside the sequence such astritone substitutions of dominants in Jazz.5. THE TURNAROUND PROBLEMA lead-sheet of a Jazz-standard consists of a melody to-gether with a chord sequence describing the fundamentalharmonic structure of the piece. The chord sequence is re-peated multiple times in a performance. While some lead-sheets end with tonic chords, others include harmonic up-beats to the ﬁrst chord of the piece at the end of the sheet,calledturnarounds. The ﬁnal chord of a performances isnevertheless usually a tonic chord. The lead-sheet of theJazz-standardAll of mestarts for example with a C4chordand ends with the turnaround E[\u00007Dm7G7.The grammar of Jazz harmony proposed above assumesthat pieces end with a tonic chord. Therefore, a simple im-plementation of this grammar would not able to parse lead-sheets that end in turnarounds. We solve this problem bycyclic parsing, meaning that we assume that constituentscan have spans from the end of a piece back to the begin-ning, see Figure 3.\nFigure 4. Tree accuracy plot6. EXPERIMENTS6.1 DatasetThe model is evaluated using theiRealProdataset of Jazz-standards.2This dataset consists of 1173 chord sequenceselectronically-encoded by the Jazz musician communityincluding metadata such as the titles, composers, and keys.The sequences were collected and converted into the Hum-drum format [10] by Daniel Shanahan and Yuri Broze [28],and are available online.3For other research that usesthis dataset see [26, 27]. The chord forms in theiRealProdataset include information about nineths and eleventhsthat are not considered in this study.The subset of 394 Jazz-standards that consist of at most40 chords was considered to train the models. 34.52%(136) of these pieces were parsable using the standardapproach and 90.61% (357) pieces were parsable usingthe cyclic parsing approach described above. Less then55% of the considered Jazz-standards therefore end inturnarounds.6.2 Tree Accuracy EvaluationWe compare four models: (i) the proposed PACFG modelthat uses a representation of rules independent of key,(ii) its PCFG counterpart the rules of which are not in-dependent of key, (iii) a baseline of randomly generatedtrees, and (iv) aright-branching baselinein which all con-stituents split into a constituent on the left and a terminalsymbol on the right.The models are trained on the 357 cyclic parsable se-quences using minibatches of 8 sequences. They are eval-uated on 13 pieces hand-annotated by the authors. We re-port the predicted tree accuracy. That is the precision ofcorrectly predicted spans of internal tree nodes. A span ofa tree node is deﬁned as the start index of its leftmost leaftogether with the end index of its rightmost leaf.Figure 4 shows the means of the tree accuracies includ-ing 95% conﬁdence intervals as error bars. The right-branching baseline performs at an accuracy level under10%. The random baseline performs slightly better at an2https://irealpro.com3https://musiccog.ohio-state.edu/home/index.php/iRbJazzCorpus156 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 5. Predicted tree accuracy for each minibatch up-date. Note that the y-axis displays only values between33% and 50%.accuracy level of 15.35% Under a uniform prior, both thePACFG and the PCFG model perform at an accuracy levelof 36.30% a priori of the data. As opposed to the trainedPCFG model that only improves its performance by about3% (in comparison to the uniform prior) reaching an ac-curacy of 39.43%, the trained PACFG model improves byabout 10% (in comparison to the uniform prior) reachingan accuracy of 45.95%. The PACFG model was thus ableto learn more from the data than the PCFG model. Notethat since the PCFG model does not abstract the grammarrules from the concrete key wherein they are applied, thenumber of free parameters of the PCFG model is approxi-mately 12 times higher than the number of free parametersof the APCFG model.Despite the fact that the PACFG model learns key-independently, it is still much simpler than models thatproduce state-of-the-art parsing results in computationallinguistics. In particular, state-of-the-art models in com-putational linguistics typically make use of conditioninginformation beyond the parent constituent categories usedin the PACFG model—such as larger tree fragments, con-ditioning on heads and/or adjacent elements in the string,state-splitting, and other richer contextual information. Weanticipate that the inclusion of similar structures into mu-sical parsing models will lead to similar improvements inperformance.Figure 5 shows the mean predicted tree accuracies ofthe PACFG and the PCFG models for each minibatch up-date. Note that this ﬁgure is produced using a stochasticalgorithm and is therefore inherently noisy. We see thatthe stochasticity of the inference algorithm leads to ran-dom jumps of the accuracy up to 0.5%. The models appearto do most of their learning in the ﬁrst 10 minibatches.6.3 Performance Diagnosis using Scale DegreeFrequenciesFigure 6 shows the expected frequency of scale-degree usein the whole corpus. The scale degrees VI in major andIII in minor are more frequently used by the model thanexpected. Because these scale degrees are substitutions forthe ﬁrst scale degrees and because they enable modulations\nFigure 6. Expected usage of scale degrees to parse the fulltraining datasetinto the relative key (e.g. from C major to A minor and viceversa), the model may be using them to alternate betweenrelative keys. The prominence of the VII in minor keys isprobably related to the fact that it has a dominant-seventhchord form. The model may be interpreting a I in major asa III in the relative minor key that is then prepared by theVII in minor. For example, the simple chord transition G7C4would in this case be derived byIa\u0000!IIIa\u0000!VIIaIIIa\u0000!G7IIIa\u0000!G7C4.7. CONCLUSION AND FUTURE RESEARCHThe research presented here introduced a new generalgrammar and parsing framework tailored to the needs ofmusic and showed how to perform inference for such amodel.Experiments show that in contrast to standard context-free models, the proposed model is able to learn character-istic structures of the observed data. To the best of ourknowledge, this is the ﬁrst computational approach thatautomatically performs hierarchical analyses of chord se-quences and evaluates them on analyses by human experts.This paper lays the groundwork for more advancedmodels of harmonic syntax. Our future research willin particular focus on expanding the dataset of hand-annotated expert analyses to provide signiﬁcance tests ofthe performance comparison of different models, for ex-ample. Further studies can use the tools developed hereto build models of unsupervised grammar induction, jointmodels of multiple musical levels of musical structure likeharmony and rhythm, and models of musical structure thathave more complex dependencies than those representablein simple tree structures.8. ACKNOWLEDGEMENTSFinancial support for the research presented in this articlehas in part been provided by the Natural Sciences and En-gineering Research Council of Canada (NSERC) and theZukunftskonzeptat TU Dresden funded by theExzellen-zinitiativeof theDeutsche Forschungsgemeinschaft.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 1579. REFERENCES[1]Jeff Bezanson, Alan Edelman, Stefan Karpinski, andViral B. Shah. Julia: A Fresh Approach to NumericalComputing.SIAM review, 59(1):65–98, 2017.[2]David M Blei, Alp Kucukelbir, and Jon D McAuliffe.Variational Inference: A Review for Statisticians.arXiv, (arXiv:1601.00670), 2017.[3]Joshua T Goodman.Parsing inside-out. PhD thesis,1998.[4]Mark Granroth-Wilding and Mark Steedman. A RobustParser-Interpreter for Jazz Chord Sequences.Journalof New Music Research, 43(4):355–374, 10 2014.[5]W Bas De Haas.Music information retrieval based ontonal harmony. PhD thesis, 2012.[6]W Bas De Haas, Jos Pedro Magalh˜aes, and Frans Wier-ing. Improving Audio Chord Transcription by Exploit-ing Harmonic and Metric Knowledge.InternationalSociety for Music Information Retrieval Conference(ISMIR), (Ismir):295–300, 2012.[7]W Bas De Haas, Martin Rohrmeier, and Frans Wier-ing. Modeling Harmonic Similarity using a Genera-tive Grammar of Tonal Harmony.Proceedings of theTenth International Conference on Music InformationRetrieval (ISMIR), 2009.[8]Masatoshi Hamanaka, Keiji Hirata, and Satoshi Tojo.Musical Structural Analysis Database Based on Gttm.InProceedings of the 15th Conference of the Interna-tional Society for Music Information Retrieval, pages325–330, 2014.[9]Matthew D. Hoffman, David M. Blei, Chong Wang,and John Paisley. Stochastic Variational Inference.Journal of Machine Learning Research, 14:1303–1347, 2013.[10]David Brian Huron.The Humdrum Toolkit: ReferenceManual. Center for Computer Assisted Research in theHumanities, 1994.[11]Michael I Jordan, Zoubin Ghahramani, Tommi SJaakola, and Lawrence K Saul. An Introduction toVariational Methods for Graphical Models.MachineLearning, 37:183–233, 1999.[12]Jonah Katz. Harmonic Syntax of the Twelve-Bar BluesForm.Music Perception: An Interdisciplinary Journal,35(2):165–192, 2017.[13]Phillip B Kirlin and David D Jensen. Using SupervisedLearning to Uncover Deep Musical Structure.Proceed-ings of the Twenty-Ninth AAAI Conference on ArtiﬁcialIntelligence, pages 1770–1776, 2015.[14]Dan Klein and Christopher D. Manning. Parsingand Hypergraphs.Proceedings of the 7th Interna-tional Workshop on Parsing Technologies (IWPT-2001), (c):351372, 2001.[15]Stefan Koelsch, Martin Rohrmeier, Renzo Torrecuso,and Sebastian Jentschke. Processing of hierarchicalsyntactic structure in music.Proceedings of the Na-tional Academy of Sciences, 110(38):15443–15448,2013.[16]Hendrik Vincent Koops, Jos Pedro Magalh˜aes, andW. Bas de Haas. A functional approach to automaticmelody harmonisation. InProceedings of the ﬁrst ACMSIGPLAN workshop on Functional art, music, model-ing & design - FARM ’13, page 47. ACM Press, 2013.[17]Kenichi Kurihara and Taisuke Sato. An Applicationof the Variational Bayesian Approach to ProbabilisticContext-Free Grammars. 2004.[18]Martin Lange and Hans Leiß. To CNF or not to CNF- An Efﬁcient Yet Presentable Version of the CYK Al-gorithm.Informatica Didactica, 8:1–21, 2009.[19]Fred Lerdahl and Ray Jackendoff.A Generative Theoryof Tonal Music. Cambridge, MA, 1983.[20]Mark Levine.The jazz theory book. Sher Music, 1995.[21]Eita Nakamura, Masatoshi Hamanaka, Keiji Hirata,and Kazuyoshi Yoshii. Tree-structured probabilisticmodel of monophonic written music based on the gen-erative theory of tonal music. In2016 IEEE Inter-national Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 276–280, 2016.[22]Markus Neuwirth and Martin Rohrmeier. Towars asyntax of the Classical cadence. InWhat is a Cadence,pages 287–338. 2015.[23]Martin Rohrmeier. A generative grammar approach todiatonic harmonic structure.Proceedings SMC’07, 4thSound andMusic Computing Conference, (July):11–13, 2007.[24]Martin Rohrmeier. Towards a generative syntax oftonal harmony.Journal of Mathematics and Music,5(1):35–53, 3 2011.[25]Martin Rohrmeier and Ian Cross. Tacit tonality : Im-plicit learning of context-free harmonic structure. InProceedings of the 7th Triennial Conference of Euro-pean Society for the Cognitive Sciences of Music (ES-COM 2009) Jyv¨askyl¨a, Finland, number Escom, pages443–452, 2009.[26]Keith Salley and Daniel T. Shanahan. Phrase Rhythmin Standard Jazz Repertoire: A Taxonomy and CorpusStudy.Journal of Jazz Studies, 11(1):1, 2016.[27]Daniel Shanahan and Yuri Broze. Diachronic Changesin Jazz Harmony: A Cognitive Perspective.Music Per-ception: An Interdisciplinary Journal, 31(1):32–45,2013.158 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[28]Daniel Shanahan, Yuri Broze, and Richard Rodgers. ADiachronic Analysis of Harmonic Schemata in Jazz. InProceedings of the 12th International Conference onMusic Perception and Cognition and the 8th TriennialConference of the European Society for the CognitiveSciences of Music, pages 909–917, 2012.[29]Stuart M Shieber, Yves Schabes, and Fernando C.N.Pereira. Principles and Implementation of DeductiveParsing.Journal of Logic Programming, 1993.[30]Mark J. Steedman. A Generative Grammar for JazzChord Sequences.Music Perception: An Interdisci-plinary Journal, 2(1):52–77, 1984.[31]Mark J Steedman. The blues and the abstract truth:Music and mental models.Mental models in cognitivescience: essays in honour of Phil Johnson-Laird, pages305–318, 1996.[32]Martin J Wainwright and Michael I Jordan. Graphi-cal models, exponential families, and variational in-ference.Foundations and Trends in Machine Learning,1(1–2):1–305, 2008.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 159"
    },
    {
        "title": "An Energy-based Generative Sequence Model for Testing Sensory Theories of Western Harmony.",
        "author": [
            "Peter M. C. Harrison",
            "Marcus T. Pearce"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492369",
        "url": "https://doi.org/10.5281/zenodo.1492369",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/215_Paper.pdf",
        "abstract": "The relationship between sensory consonance and Western harmony is an important topic in music theory and psychology. We introduce new methods for analysing this relationship, and apply them to large corpora representing three prominent genres of Western music: classical, popular, and jazz music. These methods centre on a generative sequence model with an exponential-family energy-based form that predicts chord sequences from continuous features. We use this model to investigate one aspect of instantaneous consonance (harmonicity) and two aspects of sequential consonance (spectral distance and voice-leading distance). Applied to our three musical genres, the results generally support the relationship between sensory consonance and harmony, but lead us to question the high importance attributed to spectral distance in the psychological literature. We anticipate that our methods will provide a useful platform for future work linking music psychology to music theory.",
        "zenodo_id": 1492369,
        "dblp_key": "conf/ismir/HarrisonP18",
        "keywords": [
            "sensory consonance",
            "Western harmony",
            "music theory",
            "psychology",
            "generative sequence model",
            "exponential-family energy-based form",
            "chord sequences",
            "continuous features",
            "instantaneous consonance",
            "sequential consonance"
        ],
        "content": "AN ENERGY-BASED GENERATIVE SEQUENCE MODEL FOR TESTING\nSENSORY THEORIES OF WESTERN HARMONY\nPeter M. C. Harrison\nQueen Mary University of London\nCognitive Science Research GroupMarcus T. Pearce\nQueen Mary University of London\nCognitive Science Research Group\nABSTRACT\nThe relationship between sensory consonance and Western\nharmony is an important topic in music theory and psy-\nchology. We introduce new methods for analysing this re-\nlationship, and apply them to large corpora representing\nthree prominent genres of Western music: classical, popu-\nlar, and jazz music. These methods centre on a generative\nsequence model with an exponential-family energy-based\nform that predicts chord sequences from continuous fea-\ntures. We use this model to investigate one aspect of in-\nstantaneous consonance (harmonicity) and two aspects of\nsequential consonance (spectral distance and voice-leading\ndistance). Applied to our three musical genres, the results\ngenerally support the relationship between sensory conso-\nnance and harmony, but lead us to question the high impor-\ntance attributed to spectral distance in the psychological\nliterature. We anticipate that our methods will provide a\nuseful platform for future work linking music psychology\nto music theory.\n1. INTRODUCTION\nMusic theorists and psychologists have long sought to un-\nderstand how Western harmony may be shaped by natural\nphenomena universal to all humans [13, 27, 36]. Key to\nthis work is the notion of sensory consonance , describing\na sound’s natural pleasantness [32, 35, 38], and its inverse\nsensory dissonance , describing natural unpleasantness.\nSensory consonance has both instantaneous and se-\nquential aspects. Instantaneous consonance is the conso-\nnance of an individual sound, whereas sequential conso-\nnance is a property of a progression between sounds.\nInstantaneous sensory consonance primarily derives\nfrom roughness andharmonicity . Roughness is an un-\npleasant sensation caused by interactions between spectral\ncomponents in the inner ear [8,41], whereas harmonicity1\nis a pleasant percept elicited by a sound’s resemblance to\nthe harmonic series [4, 20].\n1Related concepts include tonalness [27], toneness [15], fusion [14,\n36],complex sonorousness [29], and multiplicity [29].\nc\rPeter M. C. Harrison, Marcus T. Pearce. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Peter M. C. Harrison, Marcus T. Pearce. “An energy-based\ngenerative sequence model for testing sensory theories of Western har-\nmony”, 19th International Society for Music Information Retrieval Con-\nference, Paris, France, 2018.Sequential sensory consonance is primarily determined\nbyspectral distance andvoice-leading distance . Spec-\ntral distance2describes how much a sound’s acoustic\nspectrum perceptually differs from neighbouring spectra\n[22–24, 27, 29]. V oice-leading distance3describes how\nfar notes in one chord have to move to produce the next\nchord [2, 39, 40]. Consonance is associated with low spec-\ntral and voice-leading distance.\nMany Western harmonic conventions can be rational-\nized as attempts to increase pleasantness by maximizing\nsensory consonance. The major triad maximizes con-\nsonance by minimizing roughness and maximizing har-\nmonicity; the circle of ﬁfths maximizes consonance by\nminimizing spectral distance; tritone substitutions are con-\nsonant through voice-leading efﬁciency [39].\nThis idea – that Western harmony seeks to maximize\nsensory consonance – has a long history in music the-\nory [31]. Its empirical support is surprisingly limited, how-\never. The best evidence comes from research linking sen-\nsory consonance maximization to rules from music the-\nory [15, 27, 39], but this work is constrained by the sub-\njectivity and limited scope of music-theoretic textbooks.\nA better approach is to bypass textbooks and analyse\nmusical scores directly. Usefully, large datasets of digi-\ntised musical scores are now available, as are many com-\nputational models of consonance. However, statistically\nlinking them is non-trivial. One could calculate distribu-\ntions of consonance features, but this would give only lim-\nited causal insight into how these distributions arise. Better\ninsight might be achieved by regressing transition proba-\nbilities against consonance features, but this approach is\nstatistically problematic because of variance heterogeneity\ninduced by the inevitable sparsity of the transition tables.\nThis paper presents a new statistical model developed\nfor tackling this problem. The model is generative and\nfeature-based, deﬁning a probability distribution over sym-\nbolic sequences based on features derived from these se-\nquences. Unlike previous feature-based sequence models,\nit is specialized for continuous features, making it well-\nsuited to consonance modelling. Moreover, the model pa-\nrameters are easily interpretable and have quantiﬁable un-\n2Spectral distance is also known by its antonym spectral similarity\n[23]. Pitch commonality [29] is a similar concept. Psychological models\nof harmony and tonality in the auditory short-term memory (ASTM) tra-\ndition typically rely on some kind of spectral distance measure [1, 7, 17].\n3V oice-leading distance is termed horizontal motion in [2]. Parncutt’s\nnotion of pitch distance [28, 29] is also conceptually similar to voice-\nleading distance.160certainty, enabling error-controlled statistical inference.\nWe use this new model to test sensory theories of har-\nmony as follows. We ﬁt the model to corpora of chord\nsequences from classical, popular, and jazz music, using\npsychological models of sensory consonance as features.\nWe then compute feature importance metrics to quantify\nhow different aspects of consonance constrain harmonic\nmovement. This work constitutes the ﬁrst corpus analysis\ncomprehensively linking sensory consonance to harmonic\npractice.\n2. METHODS\n2.1 Representations\n2.1.1 Input\nChord progressions are represented as sequences of pitch-\nclass sets. Exact chord repetitions are removed, but\nchanges of chord inversion are represented as repeated\npitch-class sets.\n2.1.2 Pitch-Class Spectra\nSome of our features use pitch-class spectra as deﬁned\nin [22, 24]. A pitch-class spectrum is a continuous func-\ntion that describes perceptual weight as a function of pitch\nclass (pc). Perceptual weight is the strength of percep-\ntual evidence for a given pitch class being present. Pitch\nclasses (pc) take values in the interval [0;12)and relate to\nfrequency (f, Hz scale) as follows:\npc=\u0014\n9 + 12 log2\u0012f\n440\u0013\u0015\nmod 12: (1)\nPitch-class sets are transformed to pitch-class spectra\nby expanding each pitch class into its implied harmonics.\nPitch classes are modelled as harmonic complex tones with\n12 harmonics, after [22]. The jth harmonic in a pitch class\nhas levelj\u0000\u001a, where\u001ais the roll-off parameter ( \u001a > 0).\nPartials are represented by Gaussians with mass equal to\npartial level, mean equal to partial pitch class, and standard\ndeviation\u001b. Perceptual weights combine additively.\nFormally,W(pc;X)deﬁnes a pitch-class spectrum, re-\nturning the perceptual weight at pitch-class pcfor an input\npitch-class set X=fx1;x2;:::;xmg:\nW(pc;X) =mX\ni=1T(pc;xi): (2)\nHereiindexes the pitch classes, and T(pc;x)is the contri-\nbution of a harmonic complex tone with fundamental pitch\nclassxto an observation at pitch class pc:\nT(pc;x) =12X\nj=1g\u0000\npc;j\u0000\u001a;h(x;j)\u0001\n: (3)\nNowjindexes the harmonics, g(pc;l;px)is the contribu-\ntion from a harmonic with level land pitch-class pxto an\nobservation at pitch-class pc,g(pc;l;px) =l\n\u001bp\n2\u0019exp \n\u00001\n2\u0012d(pc;px)\n\u001b\u00132!\n;(4)\nd(px;py)is the distance between two pitch classes pxand\npy,\nd(px;py) = min (jpx\u0000pyj;12\u0000jpx\u0000pyj);(5)\nandh(x;j)is the pitch class of the jth partial of a harmonic\ncomplex tone with fundamental pitch class x:\nh(x;j) = (x+ 12 log2j) mod 12: (6)\n\u001aand\u001bare set to 0.75 and 0.0683 after [22].\n2.2 Features\n2.2.1 Spectral Distance\nSpectral distance is operationalised using the psychologi-\ncal model of [22, 24]. The spectral distance between two\npitch-class sets X;Y is deﬁned as 1 minus the continuous\ncosine similarity between the two pitch-class spectra:\nD(X;Y ) = 1\u0000R12\n0W(z;X)W(z;Y)dzqR12\n0W(z;X)2dzqR12\n0W(z;Y)2dz\n(7)\nwithWas deﬁned in Equation 2. The measure takes values\nin the interval [0;1], where 0 indicates maximal similarity\nand 1 indicates maximal divergence.\n2.2.2 Harmonicity\nOur harmonicity model is inspired by the template-\nmatching algorithms of [21] and [29]. The model simulates\nhow listeners search the auditory spectrum for occurrences\nof harmonic spectra. These inferred harmonic spectra are\ntermed virtual pitches . High harmonicity corresponds to a\nstrong virtual pitch percept.\nOur model differs from previous models in two ways.\nFirst, it uses a pitch-class representation, not a pitch repre-\nsentation. This makes it voicing-invariant and hence more\nsuitable for modelling pitch-class sets. Second, it takes\ninto account the strength of all virtual pitches in the spec-\ntrum, not just the strongest virtual pitch.\nThe model works as follows. The virtual pitch-class\nspectrumQdeﬁnes the spectral similarity of the pitch-class\nsetXto a harmonic complex tone with pitch class pc:\nQ(pc;X) =D(pc;X) (8)\nwithDas deﬁned in Equation 7. Normalising Qto unit\nmass produces Q0:\nQ0(pc;X) =Q(pc;X)R12\n0Q(y;X)dy: (9)\nPrevious models compute harmonicity by taking the peak\nof this spectrum. In our experience this works for smallProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 161chords but not for larger chords, where several virtual\npitches need to be accounted for. We therefore instead\ncompute a spectral peakiness measure. Several such mea-\nsures are possible, but here we use Kullback-Leibler diver-\ngence from a uniform distribution. H(X), the harmonicity\nof a pitch-class set X, can therefore be written as follows:\nH(X) =Z12\n0Q0(y;X) log2(12Q0(y;X))dy: (10)\nHarmonicity has a large negative correlation with the\nnumber of notes in a chord. Some correlation is expected,\nbut not to this degree: the harmonicity model considers\na tritone (the least consonant two-note chord) to be more\nconsonant than a major triad (the most consonant three-\nnote chord). We therefore separate the two phenomena by\nadding a ‘chord size’ feature, corresponding to the number\nof notes in a given chord, and rescaling harmonicity to zero\nmean and unit variance across all chords with a given chord\nsize.\n2.2.3 Roughness\nRoughness has traditionally been considered to be an im-\nportant contributor to sensory consonance, though some\nrecent research disputes its importance [20]. We originally\nplanned to include roughness in our model, but then dis-\ncovered that the phenomenon is highly sensitive to chord\nvoicing. Since the voicing of a pitch-class set is unde-\nﬁned, its roughness is therefore unpredictable. Roughness\nis therefore not modelled in the present study.\n2.2.4 Voice-Leading Distance\nAvoice leading connects the individual notes in two pitch-\nclass sets to form simultaneous melodies [39]. Pitch-class\nsets of different sizes can be connected by allowing pitch\nclasses to participate in multiple melodies. Voice-leading\ndistance is an aggregate measure of the resulting melodic\ndistance. We operationalise voice-leading distance using\n[39]’s geometric model.\nConsider two pitch-class sets X=fx1;x2;:::;xmg\nandY=fy1;y2;:::;yng. A voice-leading between X\nandYcan be written A!BwhereA= (a1;a2;:::;aN),\nB= (b1;b2;:::;bN), and the following holds: if x2X\nthenx2A, ify2Ytheny2B, ifa2Athena2X, if\nb2Bthenb2Y, andn\u0014N.\nThe distance of the voice leading A!Bis denoted\nV(A;B)and uses the taxicab norm:\nV(A;B) =NX\ni=1d(ai;bi) (11)\nwithd(ai;bi)as deﬁned in Equation 5.\nThe voice-leading distance between pitch-class sets\nX;Y is then deﬁned as the smallest value of V(A;B)for\nall legalA;B . This minimal distance can be efﬁciently\ncomputed using the algorithm described in [39].2.2.5 Summary\nThis section deﬁned three sensory consonance features.\nThese included one instantaneous measure (harmonicity)\nand two sequential measures (spectral distance, voice-\nleading distance). Harmonicity correlated strongly with\nchord size, which could have confounded our analyses.\nWe therefore controlled for chord size by normalising har-\nmonicity for each chord size and including chord size as a\nfeature.\n2.3 Statistical Model\n2.3.1 Overview\nThe statistical model is generative , deﬁning a probability\ndistribution over chord sequences (e.g. [12, 25, 33]). It\nisfeature-based , using features of the chord and its con-\ntext to predict chord probabilities (e.g. [12]). It is energy-\nbased , deﬁning scalar energies for each feature conﬁgu-\nration which are then transformed and normalised to pro-\nduce the ﬁnal probability distribution (e.g. [3, 10, 30]). It\nisexponential-family in that the energy function is a linear\nfunction of the feature vector (e.g. [10, 30]). Informally,\nthe model might be said to generalise linear regression to\nsymbolic sequences.\n2.3.2 Form\nLetAdenote the set of all possible chords, and let en\n0de-\nnote a chord sequence of length n, wheree0is always a\ngeneric start symbol. Let ei2A denote theith chord and\nej\nithe subsequence (ei;ei+1;:::;ej). Letwbe the weight\nvector that parametrises the model.\nThe probability of a chord sequence is factorised into a\nchain of conditional chord probabilities.\nP(en\n0jw) =nY\ni=1P\u0000\neijei\u00001\n0;w\u0001\n(12)\nThese are given energy-based expressions:\nP\u0000\neijei\u00001\n0;w\u0001\n=exp (\u0000E(ei\u00001\n0;ei;w))\nZ(ei\u00001\n0;w)(13)\nwhereEis the energy function andZis the partition func-\ntion.Znormalises the probability distribution to unit mass:\nZ(ei\u00001\n0;w) =X\nx2Aexp (\u0000E(ei\u00001\n0;x;w)): (14)\nHighEcorresponds to low probability. Eis deﬁned as\na sum of feature functions ,fj, weighted by\u0000w:\nE(ei\u00001\n0;x;w) =\u0000mX\nj=1fj(ei\u00001\n0::x)wj (15)\nwherewjis thejth component of w,mis the dimension-\nality of w, equalling the number of feature functions fj,\nandei\u00001\n0::xis the concatenation of ei\u00001\n0andx2A.\nFeature functions measure a property of the last ele-\nment of a sequence. Our feature functions are chord size,162 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018harmonicity, spectral distance, and voice-leading distance.\nChord size and harmonicity are context-independent,\nwhereas spectral and voice-leading distance relate the last\nchord to the penultimate chord. When the penultimate\nchord is undeﬁned, mean values are imputed for spectral\nand voice-leading distance, with the mean computed over\nall possible chord transitions.\n2.3.3 Estimation\nThe model is parametrised by the weight vector w. This\nweight vector is optimised using maximum-likelihood es-\ntimation on a corpus of sequences, as follows.\nLetenk\n0;kdenote thekth sequence from a corpus of size\nN, wherenkis the sequence’s length. The negative log-\nlikelihood of the weight vector wwith respect to the corpus\nis then\nC(w) =\u0000NX\nk=1nkX\ni=1logP(ei;kjei\u00001\n0;k;w): (16)\nAfter some algebra, the gradient can be written\ndC\ndw=NX\nk=1nkX\ni=1Z0(ei\u00001\n0;k;w)\nZ(ei\u00001\n0;k;w)\u0000f(ei\n0;k) (17)\nwhere\nZ0(ei\u00001\n0;k;w) =X\nx2Af(ei\u00001\n0;k::x) exp (\u0000E(ei\u00001\n0;k;x;w))\n(18)\nandfis the vector of feature functions. This expression can\nbe plugged into a generic optimiser to ﬁnd a weight vector\nminimising the negative log-likelihood. The present work\nused the BFGS optimiser [37].\n2.3.4 Feature Importance\nThis section introduces three complementary feature im-\nportance measures. These are weight ,explained entropy ,\nandunique explained entropy .\nWeight describes a feature’s relationship to chord prob-\nability. The weight for a feature function fjis the parame-\nterwj, corresponding to (minus) the change in the energy\nfunctionEin response to a one-unit change in the fea-\nture function fj(Equation 15). Weight is a signed feature\nimportance measure: the sign dictates whether the model\nprefers high (positive weight) or low (negative weight) fea-\nture values, and the magnitude dictates the strength of pref-\nerence. To aid weight comparability between features, fea-\nture functions are scaled to unit variance over the set of all\npossible chord transitions.\nDividing the cost function (Equation 16) by the num-\nber of chords in the corpus (PN\nk=1nk) gives an estimate\nofcross entropy in units of nats. Cross entropy measures\nchord-wise unpredictability with respect to a given model.\nFrom it we deﬁne two further measures: explained entropy\nandunique explained entropy .\nExplained entropy for a feature fjis computed by com-\nparing cross entropy estimates for two models: a modeltrained using feature fjand a null model trained with no\nfeatures. Explained entropy is the difference between the\ntwo cross entropies. Higher values indicate that the feature\nexplains a lot of structure in the corpus.\nUnique explained entropy for a feature fjis the amount\nthat cross entropy changes when feature fjis removed\nfrom the full feature set. It measures the unique explana-\ntory power of a feature while controlling for other features.\n2.3.5 Related Work\nThe literature contains several alternative approaches for\nfeature-based modelling of chord sequences. One is the\nmultiple viewpoint method [11, 12]. However, this method\nis specialised for discrete features, not the continuous fea-\ntures required for consonance modelling. A second alter-\nnative is the maximum-entropy approach of [10, 30]. This\napproach has some formal similarities with the present\nwork, but its binary feature functions are incompatible with\nour continuous features. A third possibility is the feature-\nbased dynamic networks of [33]; however, these networks\nwould need substantial modiﬁcation to represent the kind\nof feature dependencies required here.\n2.4 Corpora\nOur corpora represent three musical genres: classical mu-\nsic (1,022 movements/pieces), popular music (739 pieces),\nand jazz music (1,186 pieces). The classical corpus was\ncompiled from KernScores [34], including ensemble music\nand keyboard music from several several major composers\nof common-practice tonal music (Bach, Haydn, Mozart,\nBeethoven, Chopin). Chord labels were obtained using the\nalgorithm of [26] with an expanded chord dictionary, and\nwith segment boundaries co-located with metrical beat lo-\ncations as estimated from time signatures. Chord inver-\nsions were identiﬁed as the lowest-pitch chord tone in the\nharmonic segment being analysed. The popular and jazz\ncorpora corresponded to publicly available datasets: the\nMcGill Billboard corpus [6] and the iRBcorpus [5].\n2.5 Efﬁciency\nComputation can be reduced by identifying repeated terms\nin the cost and cost gradient (Equations 16, 17). These\nrepeated terms only need to be evaluated once. Our feature\nfunctions never look back further than the previous chord,\nand they are invariant to chord transposition; this means\nthat repeated terms occur whenever a chord pair is repeated\nat some transposition. Collapsing over these repetitions\nreduces computation by a factor of 20–100 for our corpora.\n2.6 Numeric Integration\nThe features related to pitch-class spectra all use integra-\ntion. These integrals are numerically approximated using\nthe rectangle rule with 1,200 subintervals, after [24].\n2.7 Software\nThe statistical model was implemented in R and C++;\nsource code is available from the authors on request.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 1633. RESULTS\n3.1 Corpus level\nFigure 1 plots feature importances for the three consonance\nmeasures: harmonicity (normalised by chord size), spec-\ntral distance, and voice-leading distance. Analyses are split\nby musical corpus, and conﬁdence intervals are calculated\nusing nonparametric bootstrapping [9].\n3.1.1 Importance by Feature\nAll the consonance features contribute to harmonic struc-\nture in some way. The order of feature importance is\nfairly consistent between genres and importance measures.\nBroadly speaking, voice-leading distance is most impor-\ntant, followed by harmonicity, then spectral distance.\n3.1.2 Importance by Corpus\nHarmonicity is particularly important for popular music,\nless so for classical, and least for jazz. Spectral distance is\nmost important for classical music, less so for popular, and\nunimportant for jazz.\nThe relative importance of voice-leading distance de-\npends on the measure used: it scores highly on explained\nentropy, but less on weight and unique explained entropy.\nThis may be because voice-leading distance and chord\nsize capture some common information: moving from a\nsmall chord to a large chord typically involves a large\nvoice-leading distance. If we wish to assess the unique\neffect of voice-leading distance, we can look at weight\nand unique explained entropy: these measures tell us that\nvoice-leading distance is most important for jazz music,\nless for classical music, and least for popular music.\n3.1.3 Signs of Weights\nThe sign of a feature weight determines whether the model\nprefers positive or negative values of the feature. The ob-\nserved feature signs are all consistent with theory. Har-\nmonicity has a positive weight for all genres, indicating\nthat harmonicity is universally promoted. Spectral distance\nand voice-leading distance both have negative weights, in-\ndicating preference for lower values of these features.\n3.2 Composition Level\nWe also explored the application of these techniques to in-\ndividual compositions (Figure 2). While the composition-\nlevel analyses reﬂect the same trends as the corpus-level\nanalyses (Figure 1), they also reveal substantial overlap be-\ntween the corpora. We assessed the extent of this overlap\nby training a generic machine-learning classiﬁer to predict\ngenre from the complete set of feature importance mea-\nsures. Our classiﬁer was a random forest model trained us-\ning the randomForest package in R [18], with 2,000 trees\nand four variables sampled at each split. Performance was\nassessed using 10-fold cross-validation repeated and aver-\naged over 10 runs, resulting in a classiﬁcation accuracy of\n86% and a kappa statistic of .79. This indicates that genre\ndifferences in sensory consonance are moderately salient,\neven at the composition level.4. CONCLUSION\nThis paper introduces new methods for testing relation-\nships between sensory consonance and Western harmony.\nThe methods centre on a new statistical model that predicts\nsymbolic sequences using continuous features. We demon-\nstrate these methods through application to three corpora\nrepresenting classical, popular, and jazz music.\nThe results strongly support theoretical relationships\nbetween sensory consonance and harmonic structure. The\nthree aspects of sensory consonance tested – harmonic-\nity, spectral distance, and voice-leading distance – all pre-\ndicted harmonic movement. Not all aspects were equally\nimportant, however. Spectral distance performed poorly,\nparticularly in jazz. This is interesting given the high im-\nportance attributed to spectral distance in recent psycho-\nlogical literature [1, 7, 22]. Harmonicity performed well in\npopular music, but less so in classical and jazz. In contrast,\nvoice-leading distance performed consistently well.\nThe corpus analyses deserve further development. It\nwould be worth probing the true universality of sensory\nconsonance by exploring a broader range of styles and us-\ning more reﬁned stylistic categories, possibly at the level of\nthe composer. The validity of the classical analyses could\nalso be improved through more principled sampling [19]\nand manual chord-labelling [16].\nThe three feature importance measures provide useful\ncomplementary perspectives, but it is unnecessary to plot\neach one every time. In future we recommend inspect-\ning the weights to check whether a feature is promoted or\navoided, but then plotting just unique explained entropy.\nUnique explained entropy is preferable to weight because\nits units are well-deﬁned, and preferable to explained en-\ntropy because it controls for other features, thereby provid-\ning a better handle on causality.\nWe focused on interpreting the statistical model through\nfeature importance measures, but an alternative strategy\nwould be to use the model to generate chord sequences for\nsubjective evaluation. This route lacks the objectivity of\nfeature-importance analysis, but it would give a uniquely\nintuitive perspective on what the model has learned.\nThe modelling techniques could be developed further.\nAn important limitation of the current model is the linear-\nity of the energy function, which restricts it to monotonic\nfeature effects. A polynomial energy function would ad-\ndress this problem. It would also be interesting to develop\nthe psychological features further, perhaps adding echoic\nmemory to the spectral distance measure [17], and intro-\nducing an octave-generalised roughness measure.\nDespite these limitations, we believe that the current\nresults have important implications for our understanding\nof Western tonal harmony. In particular, the results im-\nply that voice-leading efﬁciency is a better candidate for\na harmonic universal than spectral similarity. This result\nis important for music psychology, where voice-leading\nefﬁciency is relatively underemphasised compared to har-\nmonicity and spectral similarity (though see [2, 27, 39]).\nFuture psychological work may wish to re-examine the\nrole of voice-leading efﬁciency in harmony perception.164 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Weight Explained entropy Unique explained entropy\n0.00.51.01.52.00.0 0.5 1.0 1.5 0.0 0.2 0.4Voice−leading distanceSpectral distanceHarmonicity\nFeature importanceFeatureCorpus\nClassical\nPopular\nJazzFigure 1 . Measures of feature importance as a function of musical corpus. These measures are calculated from statistical\nmodels trained on the corpus level. Error bars represent 99% conﬁdence intervals estimated by nonparametric bootstrap-\nping [9]. Signs of feature weights are reversed for spectral distance and voice-leading distance, so that positive weights\ncorrespond to consonance maximisation.\nHarmonicity Spectral distance Voice−leading distanceWeight Explained entropy Unique explained entropy\n0.00.51.01.52.00.00.51.01.52.00.00.51.01.52.00.00.51.01.52.0\n024\n0.02.55.07.5\nFeature importanceDensityCorpus\nClassical\nPopular\nJazz\nFigure 2 . Distributions of feature importance measures as calculated for individual compositions within the three corpora.\nDistributions are represented by Epanechnikov kernel density functions. Signs of feature weights are reversed for spectral\ndistance and voice-leading distance, so that positive weights correspond to consonance maximisation.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 1655. ACKNOWLEDGEMENTS\nThe authors would like to thank Emmanouil Benetos and\nMatthew Purver for useful feedback and advice regarding\nthis project. PH is supported by a doctoral studentship\nfrom the EPSRC and AHRC Centre for Doctoral Training\nin Media and Arts Technology (EP/L01632X/1).\n6. REFERENCES\n[1] Emmanuel Bigand, Charles Delb ´e, B´en´edicte Poulin-\nCharronnat, Marc Leman, and Barbara Tillmann. Em-\npirical evidence for musical syntax processing? Com-\nputer simulations reveal the contribution of audi-\ntory short-term memory. Frontiers in Systems Neuro-\nscience , 8, 2014.\n[2] Emmanuel Bigand, Richard Parncutt, and Fred Ler-\ndahl. Perception of musical tension in short chord se-\nquences: The inﬂuence of harmonic function, sensory\ndissonance, horizontal motion, and musical training.\nPerception & Psychophysics , 58(1):124–141, 1996.\n[3] Nicolas Boulanger-Lewandowski, Pascal Vincent, and\nYoshua Bengio. Modeling temporal dependencies in\nhigh-dimensional sequences: Application to poly-\nphonic music generation and transcription. In Proc. of\nthe 29th International Conference on Machine Learn-\ning (ICML-12) , 2012.\n[4] Daniel L. Bowling and Dale Purves. A biological ra-\ntionale for musical consonance. Proceedings of the\nNational Academy of Sciences , 112(36):11155–11160,\n2015.\n[5] Yuri Broze and Daniel Shanahan. Diachronic changes\nin jazz harmony: A cognitive perspective. Music Per-\nception , 31(1):32–45, 2013.\n[6] John Ashley Burgoyne. Stochastic Processes &\nDatabase-Driven Musicology . PhD thesis, McGill\nUniversity, Montr ´eal, Qu ´ebec, Canada, 2011.\n[7] Tom Collins, Barbara Tillmann, Frederick S. Barrett,\nCharles Delb ´e, and Petr Janata. A combined model of\nsensory and cognitive representations underlying tonal\nexpectations in music: From audio signals to behavior.\nPsychological Review , 121(1):33–65, 2014.\n[8] P. Daniel and R. Weber. Psychoacoustical roughness:\nImplementation of an optimized model. Acta Acustica\nunited with Acustica , 83(1):113–123, 1997.\n[9] B. Efron and R. J. Tibshirani. An introduction to the\nbootstrap . Chapman & Hall, Boca Raton, FL, 1993.\n[10] Ga ¨etan Hadjeres, Jason Sakellariou, and Franc ¸ois\nPachet. Style imitation and chord invention\nin polyphonic music with exponential families.\nhttp://arxiv.org/abs/1609.05152, 2016.[11] Peter M. C. Harrison and Marcus T. Pearce. A\nstatistical-learning model of harmony perception. In\nProc. of DMRN+12: Digital Music Research Network\nOne-Day Workshop , page 15, London, UK, 2017.\n[12] Thomas Hedges and Geraint A. Wiggins. The predic-\ntion of merged attributes with multiple viewpoint sys-\ntems. Journal of New Music Research , 45(4):314–332,\n2016.\n[13] Hermann Helmholtz. On the sensations of tone . Dover,\nNew York, NY , 1954. First published in 1863; trans-\nlated by Alexander J. Ellis.\n[14] David Huron. Tonal consonance versus tonal fusion\nin polyphonic sonorities. Music Perception , 9(2):135–\n154, 1991.\n[15] David Huron. Tone and voice: A derivation of the rules\nof voice-leading from perceptual principles. Music Per-\nception , 19(1):1–64, 2001.\n[16] Nori Jacoby, Naftali Tishby, and Dmitri Tymoczko. An\ninformation theoretic approach to chord categorization\nand functional harmony. Journal of New Music Re-\nsearch , 44(3):219–244, 2015.\n[17] Marc Leman. An auditory model of the role of short-\nterm memory in probe-tone ratings. Music Perception ,\n17(4):481–509, 2000.\n[18] Andy Liaw and Matthew Wiener. Classiﬁcation and re-\ngression by randomForest. R news , 2(3):18–22, 2002.\n[19] Justin London. Building a representative corpus of\nclassical music. Music Perception , 31(1):68–90, 2013.\n[20] Josh H. McDermott, Andriana J. Lehr, and An-\ndrew J. Oxenham. Individual differences reveal the\nbasis of consonance. Current Biology , 20(11):1035–\n1041, 2010.\n[21] Andrew J. Milne. A computational model of the cogni-\ntion of tonality . PhD thesis, The Open University, Mil-\nton Keynes, UK, 2013.\n[22] Andrew J. Milne and Simon Holland. Empirically test-\ning Tonnetz, voice-leading, and spectral models of per-\nceived triadic distance. Journal of Mathematics and\nMusic , 10(1):59–85, 2016.\n[23] Andrew J. Milne, Robin Laney, and David Sharp.\nA spectral pitch class model of the probe tone data\nand scalic tonality. Music Perception , 32(4):364–393,\n2015.\n[24] Andrew J. Milne, William A. Sethares, Robin Laney,\nand David B. Sharp. Modelling the similarity of pitch\ncollections with expectation tensors. Journal of Math-\nematics and Music , 5(1):1–20, 2011.\n[25] Jean-Francois Paiement, Douglas Eck, and Samy Ben-\ngio. A probabilistic model for chord progressions. In\nProc. of the 6th International Conference on Music In-\nformation Retrieval , London, UK, 2005.166 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[26] Bryan Pardo and William P. Birmingham. Algo-\nrithms for chordal analysis. Computer Music Journal ,\n26(2):27–49, 2002.\n[27] Richard Parncutt. Harmony: A psychoacoustical ap-\nproach . Springer-Verlag, Berlin, Germany, 1989.\n[28] Richard Parncutt and Graham Hair. Consonance and\ndissonance in music theory and psychology: Disen-\ntangling dissonant dichotomies. Journal of Interdisci-\nplinary Music Studies , 5(2):119–166, 2011.\n[29] Richard Parncutt and Hans Strasburger. Applying psy-\nchoacoustics in composition: “Harmonic” progres-\nsions of “nonharmonic” sonorities. Perspectives of\nNew Music , 32(2):88–129, 1994.\n[30] Jeremy Pickens and Costas Iliopoulos. Markov random\nﬁelds and maximum entropy modeling for music infor-\nmation retrieval. In Proc. of the 6th International Con-\nference on Music Information Retrieval , pages 207–\n214, London, UK, 2005.\n[31] Jean-Philippe Rameau. Treatise on harmony . Jean-\nBaptiste-Christophe Ballard, Paris, France, 1722.\n[32] Pascaline Regnault, Emmanuel Bigand, and Mireille\nBesson. Different brain mechanisms mediate sensitiv-\nity to sensory consonance and harmonic context: Ev-\nidence from auditory event-related brain potentials.\nJournal of Cognitive Neuroscience , 13(2):241–255,\n2001.\n[33] Martin A. Rohrmeier and Thore Graepel. Compar-\ning feature-based models of harmony. In Proc. of the\n9th International Symp. on Computer Music Modeling\nand Retrieval (CMMR) , pages 357–370, London, UK,\n2012.\n[34] C. S. Sapp. Online database of scores in the Humdrum\nﬁle format. In Proc. of the 6th International Society\nfor Music Information Retrieval Conference (ISMIR\n2005) , pages 664–665, 2005.\n[35] E. Glenn Schellenberg and Laurel J. Trainor. Sensory\nconsonance and the perceptual similarity of complex-\ntone harmonic intervals: Tests of adult and infant lis-\nteners. Journal of the Acoustical Society of America ,\n100(5):3321–3328, 1996.\n[36] Carl Stumpf. The Origins of Music . Oxford Univer-\nsity Press, Oxford, UK, 2012. First published in 1911;\ntranslated by David Trippett.\n[37] Wenyu Sun and Ya-Xiang Yuan. Optimization Theory\nand Methods: Nonlinear Programming . Springer Sci-\nence & Business Media, New York, NY , 2006.\n[38] Laurel J. Trainor, Christine D. Tsang, and Vivian\nH. W. Cheung. Preference for sensory consonance in 2-\nand 4-month-old infants. Music Perception , 20(2):187–\n194, 2002.[39] Dmitri Tymoczko. The geometry of musical chords.\nScience , 313(5783):72–74, 2006.\n[40] Dmitri Tymoczko. A Geometry of Music . Oxford Uni-\nversity Press, New York, NY , 2011.\n[41] V ´aclav Vencovsk ´y. Roughness prediction based on a\nmodel of cochlear hydrodynamics. Archives of Acous-\ntics, 41(2):189–201, 2016.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 167"
    },
    {
        "title": "Onsets and Frames: Dual-Objective Piano Transcription.",
        "author": [
            "Curtis Hawthorne",
            "Erich Elsen",
            "Jialin Song",
            "Adam Roberts",
            "Ian Simon",
            "Colin Raffel",
            "Jesse H. Engel",
            "Sageev Oore",
            "Douglas Eck"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492341",
        "url": "https://doi.org/10.5281/zenodo.1492341",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/19_Paper.pdf",
        "abstract": "We advance the state of the art in polyphonic piano music transcription by using a deep convolutional and recurrent neural network which is trained to jointly predict onsets and frames. Our model predicts pitch onset events and then uses those predictions to condition framewise pitch predictions. During inference, we restrict the predictions from the framewise detector by not allowing a new note to start unless the onset detector also agrees that an onset for that pitch is present in the frame. We focus on improving onsets and offsets together instead of either in isolation as we believe this correlates better with human musical perception. Our approach results in over a 100% relative improvement in note F1 score (with offsets) on the MAPS dataset. Furthermore, we extend the model to predict relative velocities of normalized audio which results in more natural-sounding transcriptions.",
        "zenodo_id": 1492341,
        "dblp_key": "conf/ismir/HawthorneESRSRE18",
        "keywords": [
            "polyphonic piano music transcription",
            "deep convolutional and recurrent neural network",
            "jointly predict onsets and frames",
            "pitch onset events",
            "framewise pitch predictions",
            "human musical perception",
            "over 100% relative improvement",
            "note F1 score",
            "normalized audio",
            "more natural-sounding transcriptions"
        ],
        "content": "ONSETS AND FRAMES: DUAL-OBJECTIVE PIANO TRANSCRIPTION\nCurtis Hawthorne?yErich Elsen?Jialin Song?z\nAdam Roberts Ian Simon Colin Raffel Jesse Engel Sageev Oore Douglas Eck\nGoogle Brain Team, Mountain View, CA, USA\nyPlease direct correspondance to: fjord@google.com\nABSTRACT\nWe advance the state of the art in polyphonic piano music\ntranscription by using a deep convolutional and recurrent\nneural network which is trained to jointly predict onsets\nand frames. Our model predicts pitch onset events and\nthen uses those predictions to condition framewise pitch\npredictions. During inference, we restrict the predictions\nfrom the framewise detector by not allowing a new note to\nstart unless the onset detector also agrees that an onset for\nthat pitch is present in the frame. We focus on improving\nonsets andoffsets together instead of either in isolation as\nwe believe this correlates better with human musical per-\nception. Our approach results in over a 100% relative im-\nprovement in note F1 score (with offsets) on the MAPS\ndataset. Furthermore, we extend the model to predict rel-\native velocities of normalized audio which results in more\nnatural-sounding transcriptions.\n1. INTRODUCTION\nAutomatic music transcription (AMT) aims to create a\nsymbolic music representation (e.g., MIDI) from raw au-\ndio. Converting audio recordings of music into a sym-\nbolic form makes many tasks in music information re-\ntrieval (MIR) easier to accomplish, such as searching for\ncommon chord progressions or categorizing musical mo-\ntifs. Making a larger collection of symbolic music avail-\nable also broadens the scope of possible computational\nmusicology studies [8].\nPiano music transcription is a task considered difﬁcult\neven for humans due to its inherent polyphonic nature. Ac-\ncurate note identiﬁcations are further complicated by the\nway note energy decays after an onset, so a transcription\nmodel needs to adapt to a note with varying amplitude and\nharmonics. Nonnegative matrix factorization (NMF) is an\n?Equal contribution.\nzWork done as a Google Brain intern.\nc\rCurtis Hawthorne, Erich Elsen, Jialin Song, Adam\nRoberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev Oore, Douglas\nEck. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Curtis Hawthorne, Erich Elsen,\nJialin Song, Adam Roberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev\nOore, Douglas Eck. “Onsets and Frames: Dual-Objective Piano Tran-\nscription”, 19th International Society for Music Information Retrieval\nConference, Paris, France, 2018.early popular method used in the task of polyphonic mu-\nsic transcription [19]. With recent advancements in deep\nlearning, neural networks have attracted more and more\nattention from the AMT community [13, 18]. In particu-\nlar, the success of convolutional neural networks (CNN)\nfor image classiﬁcation tasks [21] has inspired the use of\nCNNs for AMT because two-dimensional time-frequency\nrepresentations (e.g., constant-Q transform [5]) are com-\nmon input representations for audio. In [13], the authors\ndemonstrated the potential for a single CNN-based acous-\ntic model to accomplish polyphonic piano music transcrip-\ntion. [18] considered an approach inspired by common\nmodels used in speech recognition where a CNN acoustic\nmodel and a Recurrent Neural Network (RNN) language\nmodel are combined. In this paper, we investigate improv-\ning the acoustic model by focusing on note onsets.\nNote onset detection looks for only the very beginning\nof a note. Intuitively, the beginning of a piano note is eas-\nier to identify because the amplitude of that note is at its\npeak. For piano notes, the onset is also percussive and\nhas a distinctive broadband spectrum. Once the model\nhas determined onset events, we can condition framewise\nnote detection tasks on this knowledge. Previously, [6, 27]\ndemonstrated the promise of modeling onset events explic-\nitly in both NMF and CNN frameworks. In this work, we\ndemonstrate that a model conditioned on onsets achieves\nstate of the art performance by a large margin for all com-\nmon metrics measuring transcription quality: frame, note,\nand note-with-offset.\nWe also extend our model to predict the relative veloc-\nity of each onset. Velocity captures the speed with which\na piano key was depressed and is directly related to how\nloud that note sounds. Including velocity information in a\ntranscription is critical for describing the expressivity of\na piano performance and results in much more natural-\nsounding transcriptions.\n2. DATASET AND METRICS\nWe use the MAPS dataset [9] which contains audio and\ncorresponding annotations of isolated notes, chords, and\ncomplete piano pieces. Full piano pieces in the dataset\nconsist of both pieces rendered by software synthesizers\nand recordings of pieces played by a Yamaha Disklavier\nplayer piano. We use the set of synthesized pieces as the\ntraining split and the set of pieces played on the Disklavier\nas the test split, as proposed in [18]. When constructing50these datasets, we also ensured that the same music piece\nwas not present in more than one set. Not including the\nDisklavier recordings, individual notes, or chords in the\ntraining set is closer to a real-world testing environment be-\ncause we often do not have access to recordings of a testing\npiano at training time. Testing on the Disklavier recordings\nis also more realistic because many of the recordings that\nare most interesting to transcribe are ones played on real\npianos.\nWhen processing the MAPS MIDI ﬁles for training\nand evaluation, we ﬁrst translate “sustain pedal” control\nchanges into longer note durations. If a note is active when\nsustain goes on, that note will be extended until either sus-\ntain goes off or the same note is played again. This process\ngives the same note durations as the text ﬁles included with\nthe dataset.\nThe metrics used to evaluate a model are frame-level\nand note-level metrics including precision, recall, and F1\nscore. We use the mireval library [16] to calculate note-\nbased precision, recall, and F1 scores. As is standard, we\ncalculate two versions of note metrics: one requiring that\nonsets be within\u000650ms of ground truth but ignoring off-\nsets and one that also requires offsets resulting in note du-\nrations within 20% of the ground truth or within 50ms,\nwhichever is greater. Frame-based scores are calculated\nusing the standard metrics as deﬁned in [2]. We also intro-\nduce a new note metric for velocity transcription that is fur-\nther described in Section 3.1. Both frame and note scores\nare calculated per piece and the mean of these per-piece\nscores is presented as the ﬁnal metric for a given collection\nof pieces.\nOur goal is to generate piano transcriptions that contain\nall perceptually relevant performance information in an au-\ndio recording without prior information about the record-\ning environment such as characterization of the instrument.\nWe need a numerical measure that correlates with this per-\nceptual goal. Poor quality transcriptions can still result in\nhigh frame scores due to short spurious notes and repeated\nnotes that should be held. Note onsets are important, but\na piece played with only onset information would either\nhave to be entirely staccato or use some kind of heuristic to\ndetermine when to release notes. A high note-with-offset\nscore will correspond to a transcription that sounds good\nbecause it captures the perceptual information from both\nonsets and durations. Adding a velocity requirement to this\nmetric ensures that the dynamics of the piece are captured\nas well. More perceptually accurate metrics may be pos-\nsible and warrant further research. In this work we focus\non improving the note-with-offset score, but also achieve\nstate of the art results for the more common frame and note\nscores and extend the model to transcribe velocity informa-\ntion as well.\n3. MODEL CONFIGURATION\nFramewise piano transcription tasks typically process\nframes of raw audio and produce frames of note activa-\ntions. Previous framewise prediction models [13, 18] have\ntreated frames as both independent and of equal impor-tance, at least prior to being processed by a separate lan-\nguage model. We propose that some frames are more im-\nportant than others, speciﬁcally the onset frame for any\ngiven note. Piano note energy decays starting immediately\nafter the onset, so the onset is both the easiest frame to\nidentify and the most perceptually signiﬁcant.\nWe take advantage of the signiﬁcance of onset frames\nby training a dedicated note onset detector and using the\nraw output of that detector as additional input for the\nframewise note activation detector. We also use the thresh-\nolded output of the onset detector during the inference pro-\ncess, similar to concurrent research described in [24]. An\nactivation from the frame detector is only allowed to start\na note if the onset detector agrees that an onset is present\nin that frame.\nOur onset and frame detectors are built upon the convo-\nlution layer acoustic model architecture presented in [13],\nwith some modiﬁcations. We use librosa [15] to com-\npute the same input data representation of mel-scaled spec-\ntrograms with log amplitude of the input raw audio with\n229 logarithmically-spaced frequency bins, a hop length\nof 512, an FFT window of 2048, and a sample rate of\n16kHz. We present the network with the entire input se-\nquence, which allows us to feed the output of the convolu-\ntional frontend into a recurrent neural network (described\nbelow).\nThe onset detector is composed of the acoustic model,\nfollowed by a bidirectional LSTM [17] with 128 units in\nboth the forward and backward directions, followed by a\nfully connected sigmoid layer with 88 outputs for repre-\nsenting the probability of an onset for each of the 88 piano\nkeys.\nThe frame activation detector is composed of a sepa-\nrate acoustic model, followed by a fully connected sigmoid\nlayer with 88 outputs. Its output is concatenated together\nwith the output of the onset detector and followed by a\nbidirectional LSTM with 128 units in both the forward and\nbackward directions. Finally, the output of that LSTM is\nfollowed by a fully connected sigmoid layer with 88 out-\nputs. During inference, we use a threshold of 0.5 to deter-\nmine whether the onset detector or frame detector is active.\nTraining RNNs over long sequences can require large\namounts of memory and is generally faster with larger\nbatch sizes. To expedite training, we split the training au-\ndio into smaller ﬁles. However, when we do this splitting\nwe do not want to cut the audio during notes because the\nonset detector would miss an onset while the frame de-\ntector would still need to predict the note’s presence. We\nfound that 20 second splits allowed us to achieve a rea-\nsonable batch size during training of at least 8, while also\nforcing splits in only a small number of places where notes\nare active. When notes are active and we must split, we\nchoose a zero-crossing of the audio signal. Inference is\nperformed on the original and un-split audio ﬁle.\nOur ground truth note labels are in continuous time,\nbut the results from audio processing are in spectrogram\nframes. So, we quantize our labels to calculate our train-\ning loss. When quantizing, we use the same frame size asProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 51Log Mel-SpectrogramConv StackConv StackBiLSTMBiLSTMOnset LossFrame LossOnset PredictionsFrame Predictions\nFC SigmoidFC SigmoidFC SigmoidFigure 1 . Diagram of Network Architecture\nthe output of the spectrogram. However, when calculat-\ning metrics, we compare our inference results against the\noriginal, continuous time labels.\nOur loss function is the sum of two cross-entropy\nlosses: one from the onset side and one from the note side.\nLtotal=Lonset +Lframe (1)\nLonset =pmaxX\np=pminTX\nt=0CE(Ionset(p;t);Ponset(p;t))(2)\nwherepmin=max denote the MIDI pitch range of the pi-\nano roll,Tis the number of frames in the example,\nIonset(p;t)is an indicator function that is 1when there is\na ground truth onset at pitch pand framet,Ponset(p;t)\nis the probability output by the model at pitch pand\nframetandCE denotes cross entropy. The labels\nfor the onset loss are created by truncating note lengths\ntomin(notelength;onset length )prior to quantiza-\ntion. We performed a coarse hyperparameter search over\nonsetlength (we tried 16, 32 and 48ms) and found that\n32ms worked best. In hindsight this is not surprising as it\nis also the length of our frames and so almost all onsets\nwill end up spanning exactly two frames. Labeling only\nthe frame that contains the exact beginning of the onset\ndoes not work as well because of possible mis-alignments\nof the audio and labels. We experimented with requiring\na minimum amount of time a note had to be present in a\nframe before it was labeled, but found that the optimum\nvalue was to include any presence.\nIn addition, within the frame-based loss term Lframe ,\nwe apply a weighting to encourage accuracy at the start of\nthe note. A note starts at frame t1, completes its onset at\nt2and ends at frame t3. Because the weight vector assigns\nhigher weights to the early frames of notes, the model is\nincentivized to predict the beginnings of notes accurately,thus preserving the most important musical events of the\npiece. First, we deﬁne a raw frame loss as:\nLframe =pmaxX\np=pminTX\nt=0CE(Iframe (p;t);Pframe (p;t))\n(3)\nwhereIframe (p;t)is1when pitchpis active in the ground\ntruth in frame tandPframe (p;t)is the probability output\nby the model for pitch pbeing active at frame t. Then, we\ndeﬁne the weighted frame loss as:\nLframe (l;p) =8\n><\n>:cL0\nframe (l;p)t1\u0014t\u0014t2\nc\nt\u0000t2L0\nframet2<t\u0014t3\nL0\nframe (l;p)elsewhere(4)\nwherec= 5:0as determined with coarse hyperparameter\nsearch.\n3.1 Velocity Estimation\nWe further extend the model by adding another stack to\nalso predict velocities for each onset. This stack is similar\nto the others and consists of the same layers of convolu-\ntions. This stack does not connect to the other two. The\nvelocity labels are generated by dividing all the velocities\nby the maximum velocity present in the piece. The small-\nest velocity does not go to zero, but rather tovmin\nvmax. The\nstack is trained with the following loss averaged across a\nbatch:\nLvel=pmaxX\np=pminTX\nt=0Ionset(p;t)(vp;t\nlabel\u0000vp;t\npredicted)2(5)\nAt inference time the output is clipped to [0;1]and then\ntransformed to a midi velocity by the following mapping:\nvmidi= 80vpredicted + 10 (6)\nThe ﬁnal mapping is arbitrary, but we found this leads\nto pleasing audio renderings.\nWhile various studies have considered the estimation\nof dynamics (note intensities or velocities) in a record-\ning given the score [10, 22, 26], to our knowledge there\nhas been no work in the literature considering estimation\nof dynamics alongside pitch and timing information. As\na result, as Benetos et al. [3] noted in their review pa-\nper in 2013, “evaluating the performance of current [au-\ntomatic music transcription] systems for the estimation of\nnote dynamics has not yet been addressed.” To evaluate our\nvelocity-aware model, we therefore propose an additional\ncriterion for the note-level precision, recall, and F1 scores.\nEvaluating velocity predictions is not straightforward\nbecause unlike pitch and timing, velocity has no abso-\nlute meaning. For example, if two transcriptions contained\nidentical velocities except that they were offset or scaled\nby a constant factor, they would be effectively equivalent\ndespite reporting completely different velocities for every\nnote. To address these issues, we ﬁrst re-scale all of the\nground-truth velocities in a transcription to be in the range52 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[0;1]. After notes are matched according to their pitch\nand onset/offset timing, we assemble pairs of the reference\n(ground-truth) and estimated velocities for matched notes,\nreferred to as vrandverespectively. We then perform a\nlinear regression to estimate a global scale and offset pa-\nrameter such that the squared difference between pairs of\nreference and estimated velocities is minimized:\nm;b= arg min\nm;bMX\ni=1kvr(i)\u0000(mve(i) +b)k2(7)\nwhereMis the number of matches (i.e. number of entries\ninvrandve). These scalar parameters are used to re-scale\nthe entries of veto obtain\n^ve=fmve(i) +b;i21;:::;Mg (8)\nFinally, a match iis now only considered correct if, in ad-\ndition to having its pitch and timing match, it also satis-\nﬁesj^ve(i)\u0000vr(i)j< \u001c for some threshold \u001c. We used\n\u001c= 0:1in all of our evaluations. The precision, recall,\nand F1 scores are then recomputed as normal based on this\nnewly ﬁltered list of matches.\n4. EXPERIMENTS\nWe trained our onsets and frames model using Tensor-\nFlow [1] on the training dataset described in Section 2 us-\ning a batch size of 8, a learning rate of .0006, and a gradi-\nent clipping L2-norm of 3. A hyperparameter search was\nconducted to ﬁnd the optimal learning rate. We use the\nAdam optimizer [14] and train for 50,000 steps. Train-\ning takes 5 hours on 3 P100 GPUs. The same hyperpa-\nrameters were used to train all models, including those\nfrom the ablation study, except when reproducing the re-\nsults of [18] and [13], where hyperparameters from the\nrespective papers were used. The source code for our\nmodel is available at https://goo.gl/magenta/\nonsets-frames-code .\nFor comparison, we reimplemented the models de-\nscribed in [13, 18] to ensure evaluation consistency. We\nalso compared against the commercial software Melodyne\nversion 4.1.1.0111. We would have liked to compare\nagainst AnthemScore2as described in [25] as well, but\nbecause it produces a MusicXML score with quantized\nnote durations instead of a MIDI ﬁle with millisecond-\nscale timings, an accurate comparison was not possible.\nResults from these evaluations are summarized in Ta-\nble 1. Our onsets and frames model not only produces\nbetter note-based scores (which only take into account on-\nsets), it also produces the best frame-level scores and note-\nbased scores that include offsets.\nAn example input spectrogram, note and onset output\nposteriorgrams, and inferred transcription for a recording\nfrom outside of the training set is shown in Figure 2. The\nimportance of restricting frame activations based on on-\nset predictions during inference is clear: The second-to-\nbottom image (“Estimated Onsets and Notes”) shows the\n1http://www.celemony.com/en/melodyne\n2https://www.lunaverus.com/results from the frame and onset predictors. There are\nseveral examples of notes that either last for only a few\nframes or that reactivate brieﬂy after being active for a\nwhile. Frame results after being restricted by the onset de-\ntector are shown in magenta. Many of the notes that were\nactive for only a few frames did not have a corresponding\nonset detection and were removed, shown in cyan. Cases\nwhere a note brieﬂy reactivated were also removed because\na corresponding second onset was not detected.\nDespite not optimizing for inference speed, our net-\nwork performs 70\u0002faster than real time on a Tesla\nK40c. The MIDI ﬁles resulting from our inference exper-\niments are available at https://goo.gl/magenta/\nonsets-frames-examples .\n5. ABLATION STUDY\nTo understand the individual importance of each piece in\nour model, we conducted an ablation study. We consider\nremoving the onset detector entirely (i.e., using only the\nframe detector) (a), not using the onset information dur-\ning inference (b), making the bi-directional RNNs uni-\ndirectional (c,d), removing the RNN from the onset detec-\ntor entirely (e), pre-training the onset detector rather than\njointly training it with the frame detector (f), weighting all\nframes equally (g), sharing the convolutional features be-\ntween both detectors (h), removing the connection between\nthe onset and frame detectors during training (i), using a\nConstant Q-Transform (CQT) input representation instead\nof mel-scaled spectrograms (j), and ﬁnally removing all the\nLSTMs and sharing the convolutional features (k).\nThese results show the importance of the onset infor-\nmation – not using the onset information during inference\n(b) results in a signiﬁcant 18% relative decrease in the note\nonset score and a 31% relative decrease in the note-with-\noffset score while increasing the frame score slightly. De-\nspite the increased frame score, the output sounds signif-\nicantly worse. To our ears, the decrease in transcription\nquality is best reﬂected by the note-with-offset scores.\nThe model which does not have the onset detector\nat all (a) – consisting of convolutions followed by a bi-\ndirectional RNN followed by a frame-wise loss – does the\nworst on all metrics, although it still outperforms the base-\nline model from [13]. The other ablations indicate a small\nimpact for each component ( <6%). It is encouraging that\nforward-only RNNs have only a small accuracy impact as\nthey can be used for online piano transcription.\nWe tried many other architectures and data augmenta-\ntion strategies not listed in the table, none of which re-\nsulted in any improvement. Signiﬁcantly, augmenting the\ntraining audio by adding normalization, reverb, compres-\nsion, noise, and synthesizing the training MIDI ﬁles with\nother synthesizers made no difference. We believe these\nresults indicate a need for a much larger training dataset of\nreal piano recordings that have fully accurate label align-\nments. These requirements are not satisﬁed by the current\nMAPS dataset because only 60 of its 270 recordings are\nfrom real pianos, and they are also not satisﬁed by Music-\nNet [23] because its alignments are not fully accurate (e.g.,Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 53Frame Note Note w/ offset Note w/ offset & velocity\nP R F1 P R F1 P R F1 P R F1\nSigtia et al., 2016 [18] 71.99 73.32 72.22 44.97 49.55 46.58 17.64 19.71 18.38 — — —\nKelz et al., 2016 [13] 81.18 65.07 71.60 44.27 61.29 50.94 20.13 27.80 23.14 — — —\nMelodyne (decay mode) 71.85 50.39 58.57 62.08 48.53 54.02 21.09 16.56 18.40 10.43 8.15 9.08\nOnsets and Frames 88.53 70.89 78.30 84.24 80.67 82.29 51.32 49.31 50.22 35.52 30.80 35.39\nTable 1 . Precision, Recall, and F1 Results on MAPS conﬁguration 2 test dataset (ENSTDkCl and ENSTDkAm full-length\n.wav ﬁles). Note-based scores calculated by the mireval library, frame-based scores as deﬁned in [2]. Final metric is\nthe mean of scores calculated per piece. MIDI ﬁles used to calculate these scores are available at https://goo.gl/\nmagenta/onsets-frames-examples .\nFrequencyInput Spectrogram\nC3C4C5C6NoteNote Prediction Posteriorgram\nC3C4C5C6NoteOnset Prediction Posteriorgram\nC3C4C5C6NoteEstimated Onsets and Notes\n0 1 2 3 4 5 6\nTime (seconds)C3C4C5C6NoteEstimated and Reference Transcription\nFigure 2 . Inference on 6 seconds of MAPS MUS-mz 3313ENSTDkCl.wav (a recording which is not in the training set).\nFrom top to bottom, we show the log-magnitude mel-frequency spectrogram input, the framewise note probability and onset\nprobability “posteriorgrams” produced by our model, the corresponding estimated onsets and notes after thresholding, and\nﬁnally the resulting estimated transcription produced by our model alongside the reference transcription. In the onset and\nnotes plot (second from the bottom), onset predictions are shown in black. Notes with a corresponding onset prediction are\nshown in magenta and notes which are ﬁltered out because no onset was predicted for the note are shown in cyan. In the\nbottom plot, the estimated transcription is shown in blue and the reference is shown in red. Figure best viewed in color.54 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018there is an audible time difference between piano audio and\nMIDI at 1:24 in sequence 2533). Other approaches, such\nas seq2seq [20] may not require fully accurate alignments.\nF1\nFrame Note Note\nwith offset\nOnset and Frames 78.30 82.29 50.22\n(a) Frame-only LSTM 76.12 62.71 27.89\n(b) No Onset Inference 78.37 67.44 34.15\n(c) Onset forward LSTM 75.98 80.77 46.36\n(d) Frame forward LSTM 76.30 82.27 49.50\n(e) No Onset LSTM 75.90 80.99 46.14\n(f) Pretrain Onsets 75.56 81.95 48.02\n(g) No Weighted Loss 75.54 80.07 48.55\n(h) Shared conv 76.85 81.64 43.61\n(i) Disconnected Detectors 73.91 82.67 44.83\n(j) CQT Input 73.07 76.38 41.14\n(k) No LSTM, shared conv 67.60 75.34 37.03\nTable 2 . Ablation Study Results.\n6. NEED FOR MORE DATA, MORE RIGOROUS\nEVALUATION\nThe most common dataset for evaluation of piano tran-\nscription tasks is the MAPS dataset, in particular the EN-\nSTDkCl and ENSTDkAm renderings of the MUS collec-\ntion of pieces. This set has several desirable properties: the\npieces are real music as opposed to randomly-generated\nsequences, the pieces are played on a real physical piano\nas opposed to a synthesizer, and multiple recording envi-\nronments are available (“close” and “ambient” conﬁgura-\ntions). The main drawback of this dataset is that it contains\nonly 60 recordings. To best measure transcription quality,\nwe believe a new and much larger dataset is needed. How-\never, until that exists, evaluations should make full use of\nthe data that is currently available.\nMany papers, for example [7,12,18,27], further restrict\nthe data used in evaluation by using only the “close” col-\nlection and/or only the ﬁrst 30 seconds or less of each ﬁle.\nWe believe this method results in an evaluation that is not\nrepresentative of real-world transcription tasks. For exam-\nple, evaluating on only the “close” collection raises our\nnote F1 score from 82.29 to 84.34, and evaluating on only\nthe ﬁrst 30 seconds further raises it to 86.38. For compari-\nson, [27] achieved a note F1 score of 80.23 in this setting.\nThe model in [12] is also evaluated using 30s clips from the\n“close” collection, but it was additionally trained on data\nfrom the test piano. This method limits the generalizability\nof the model but produced a note F1 score of 85.06.\nIn addition to the small number of the MAPS Disklavier\nrecordings, we have also noticed several cases where\nthe Disklavier appears to skip some notes played at\nlow velocity. For example, at the beginning of the\nBeethoven Sonata No. 9, 2nd movement, several A [\nnotes played with MIDI velocities in the mid-20s are\nclearly missing from the audio ( https://goo.gl/\nmagenta/onsets-frames-examples ). More anal-\nysis is needed to determine how frequently missed notesoccur, but we have noticed that our model performs partic-\nularly poorly on notes with ground truth velocities below\n30.\nFinally, we believe that more strict metrics should be\nadopted by the community. As discussed in Section 2,\nframe and note onset scores are not enough to determine\nwhether a transcription has captured all musically rele-\nvant information from a performance. We present sev-\neral audio examples at https://goo.gl/magenta/\nonsets-frames-examples to illustrate this point.\nOf the metrics currently available, we believe that the note-\nwith-offset and velocity is the best way to compare models\ngoing forward.\nSimilarly, the current practice of using a 50ms tolerance\nfor note onset correctness allows for too much timing jit-\nter. An audio example illustrating this point is available\nat the above URL. We suggest future work should evalu-\nate models with tighter timing requirements. Much work\nremains to be done here because as observed in [4], achiev-\ning high accuracy is increasingly difﬁcult as timing preci-\nsion is increased, in part due to the limited timing accuracy\nof the datasets currently available [11]. When we trained\nour model at a resolution of 24ms, our scores using the ex-\nisting 50ms metrics were not always as high: Frame 76.87,\nNote F1 82.54, Note-with-offset 49.99. Audio examples of\nthis higher resolution model are also available at the above\nURL. In the examples, the higher time resolution is evi-\ndent, but the model also produces more extraneous notes.\n7. CONCLUSION AND FUTURE WORK\nWe presented a jointly-trained onsets and frames model\nfor transcribing polyphonic piano music which yields sig-\nniﬁcant improvements by using onset information. This\nmodel transfers well between the disparate train and test\ndistributions. The current quality of our model’s output is\non the cusp of enabling downstream applications such as\nsymbolic MIR and automatic music generation. To further\nimprove the results we need to create a new dataset that\nis much larger and more representative of various piano\nrecording environments and music genres for both training\nand evaluation. Combining an improved acoustic model\nwith a language model is a natural next step. Another di-\nrection is to go beyond traditional spectrogram representa-\ntions of audio signals.\nIt is very much worth listening to the examples of tran-\nscription. Consider Mozart Sonata K331, 3rd movement.\nOur system does a good job in terms of capturing harmony,\nmelody, rhythm, and even dynamics. If we compare this\ntranscription to the other systems, the difference is quite\naudible. We have also successfully used the model to tran-\nscribe recordings from the Musopen.org website that are\ncompletely unrelated to our training dataset. The model\neven works surprisingly well transcribing a harpsichord\nrecording. Audio examples are available at https://\ngoo.gl/magenta/onsets-frames-examples .Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 558. ACKNOWLEDGEMENTS\nWe would like to thank Hans Bernhard for helping with\ndata collection and Brian McFee and Justin Salamon for\nconsulting on the velocity metric design.\n9. REFERENCES\n[1] Mart ´ın Abadi, Ashish Agarwal, Paul Barham, Eu-\ngene Brevdo, Zhifeng Chen, Craig Citro, Greg S\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin,\net al. Tensorﬂow: Large-scale machine learning\non heterogeneous distributed systems. arXiv preprint\narXiv:1603.04467 , 2016.\n[2] Mert Bay, Andreas F Ehmann, and J Stephen Downie.\nEvaluation of multiple-f0 estimation and tracking sys-\ntems. In ISMIR , pages 315–320, 2009.\n[3] Emmanouil Benetos, Simon Dixon, Dimitrios Gian-\nnoulis, Holger Kirchhoff, and Anssi Klapuri. Auto-\nmatic music transcription: challenges and future di-\nrections. Journal of Intelligent Information Systems ,\n41(3):407–434, 2013.\n[4] Sebastian B ¨ock and Markus Schedl. Polyphonic pi-\nano note transcription with recurrent neural networks.\nInAcoustics, speech and signal processing (ICASSP),\n2012 ieee international conference on , pages 121–124.\nIEEE, 2012.\n[5] Judith C Brown. Calculation of a constant q spectral\ntransform. The Journal of the Acoustical Society of\nAmerica , 89(1):425–434, 1991.\n[6] Tian Cheng, Matthias Mauch, Emmanouil Benetos, Si-\nmon Dixon, et al. An attack/decay model for piano\ntranscription. In Proceedings of the 17th International\nSociety for Music Information Retrieval Conference,\nISMIR 2016, New York City, United States, August 7-\n11, 2016 . ISMIR, 2016.\n[7] Andrea Cogliati, Zhiyao Duan, and Brendt Wohlberg.\nPiano transcription with convolutional sparse lateral in-\nhibition. IEEE Signal Processing Letters , 24(4):392–\n396, 2017.\n[8] Michael Scott Cuthbert and Christopher Ariza.\nmusic21 : A toolkit for computer-aided musicology\nand symbolic music data. In Proceedings of the 11th\nInternational Society for Music Information Retrieval\nConference , pages 637–642, 2010.\n[9] Valentin Emiya, Roland Badeau, and Bertrand David.\nMultipitch estimation of piano sounds using a new\nprobabilistic spectral smoothness principle. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 18(6):1643–1654, 2010.\n[10] Sebastian Ewert and Meinard M ¨uller. Estimating note\nintensities in music recordings. In IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , pages 385–388, 2011.[11] Sebastian Ewert and Mark Sandler. Piano transcrip-\ntion in the studio using an extensible alternating direc-\ntions framework. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing , 24(11):1983–1997,\n2016.\n[12] Lufei Gao, Li Su, Yi-Hsuan Yang, and Tan Lee. Poly-\nphonic piano note transcription with non-negative ma-\ntrix factorization of differential spectrogram. In Acous-\ntics, Speech and Signal Processing (ICASSP), 2017\nIEEE International Conference on , pages 291–295.\nIEEE, 2017.\n[13] Rainer Kelz, Matthias Dorfer, Filip Korzeniowski, Se-\nbastian B ¨ock, Andreas Arzt, and Gerhard Widmer. On\nthe potential of simple framewise approaches to piano\ntranscription. arXiv preprint arXiv:1612.05153 , 2016.\n[14] Diederik Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[15] Brian McFee. librosa: v0.4.3, May 2016.\n[16] Colin Raffel, Brian McFee, Eric J. Humphrey, Justin\nSalamon, Oriol Nieto, Dawen Liang, and Daniel P. W.\nEllis. mir eval: A transparent implementation of com-\nmon mir metrics. In In Proceedings of the 15th Inter-\nnational Society for Music Information Retrieval Con-\nference, ISMIR . Citeseer, 2014.\n[17] Mike Schuster and Kuldip K Paliwal. Bidirectional re-\ncurrent neural networks. IEEE Transactions on Signal\nProcessing , 45(11):2673–2681, 1997.\n[18] Siddharth Sigtia, Emmanouil Benetos, and Simon\nDixon. An end-to-end neural network for polyphonic\npiano music transcription. IEEE/ACM Transactions on\nAudio, Speech and Language Processing (TASLP) ,\n24(5):927–939, 2016.\n[19] Paris Smaragdis and Judith C Brown. Non-negative\nmatrix factorization for polyphonic music transcrip-\ntion. In Applications of Signal Processing to Audio and\nAcoustics, 2003 IEEE Workshop on. , pages 177–180.\nIEEE, 2003.\n[20] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Se-\nquence to sequence learning with neural networks. In\nAdvances in neural information processing systems ,\npages 3104–3112, 2014.\n[21] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. Rethinking the in-\nception architecture for computer vision. In Proceed-\nings of the IEEE Conference on Computer Vision and\nPattern Recognition , pages 2818–2826, 2016.\n[22] Wai Man Szeto, Kin Hong Wong, and Chi Hang Wong.\nFinding intensities of individual notes in piano music.\nInComputer Music Modeling and Retrieval , 2005.56 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[23] John Thickstun, Zaid Harchaoui, and Sham Kakade.\nLearning features of music from scratch. In In-\nternational Conference on Learning Representations\n(ICLR) , 2017.\n[24] Carl Thom ´e and Sven Ahlb ¨ack. Polyphonic pitch de-\ntection with convolutional recurrent neural networks.\nMusic Information Retrieval Evaluation eXchange\n(MIREX) , 2017.\n[25] Daylin Troxel. Music transcription with a convolu-\ntional neural network 2016. Music Information Re-\ntrieval Evaluation eXchange (MIREX) , 2016.\n[26] Sam Van Herwaarden, Maarten Grachten, and Bas\nDe Haas. Predicting expressive dynamics in piano per-\nformances using neural networks. In Proceedings of\nthe 15th Conference of the International Society for\nMusic Information Retrieval , pages 45–52, 2014.\n[27] Qi Wang, Ruohua Zhou, and Yonghong Yan. A two-\nstage approach to note-level transcription of a speciﬁc\npiano. Applied Sciences , 7(9):901, 2017.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 57"
    },
    {
        "title": "Audio Based Disambiguation of Music Genre Tags.",
        "author": [
            "Romain Hennequin",
            "Jimena Royo-Letelier",
            "Manuel Moussallam"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492499",
        "url": "https://doi.org/10.5281/zenodo.1492499",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/163_Paper.pdf",
        "abstract": "In this paper, we propose to infer music genre embeddings from audio datasets carrying semantic information about genres. We show that such embeddings can be used for disambiguating genre tags (identification of different labels for the same genre, tag translation from a tag system to another, inference of hierarchical taxonomies on these genre tags). These embeddings are built by training a deep convolutional neural network genre classifier with large audio datasets annotated with a flat tag system. We show empirically that they makes it possible to retrieve the original taxonomy of a tag system, spot duplicates tags and translate tags from a tag system to another.",
        "zenodo_id": 1492499,
        "dblp_key": "conf/ismir/HennequinRM18",
        "keywords": [
            "infer",
            "music",
            "genre",
            "embeddings",
            "audio",
            "datasets",
            "semantic",
            "information",
            "genres",
            "disambiguating"
        ],
        "content": "AUDIO BASED DISAMBIGUATION OF MUSIC GENRE TAGS\nRomain Hennequin, Jimena Royo-Letelier, Manuel Moussallam\nDeezer R&D, Paris\nresearch@deezer.com\nABSTRACT\nIn this paper, we propose to infer music genre embeddings\nfrom audio datasets carrying semantic information about\ngenres. We show that such embeddings can be used for dis-\nambiguating genre tags (identiﬁcation of different labels\nfor the same genre, tag translation from a tag system to an-\nother, inference of hierarchical taxonomies on these genre\ntags). These embeddings are built by training a deep con-\nvolutional neural network genre classiﬁer with large audio\ndatasets annotated with a ﬂat tag system. We show em-\npirically that they makes it possible to retrieve the original\ntaxonomy of a tag system, spot duplicates tags and trans-\nlate tags from a tag system to another.\n1. INTRODUCTION\nLarge genre annotated databases have been made avail-\nable lately: the Google Audio Set (GAS) [12], the MuMu\ndataset [20], Discogs [1] or the Free Music Archive (FMA)\ndataset [6] all contain hundreds of genre tags and hundreds\nof thousands multi-label genre track annotations.\nEvery dataset with genre annotations has its own genre\nrepresentation: usually it is a tag set which is sometimes\norganized with a basic taxonomy (Discogs, MuMu, FMA)\nor a basic ontology (GAS).\nHowever these representations usually suffer from am-\nbiguity issues. First, tag deﬁnition may not be explicit:\nfor the same tag name, deﬁnition may not be coherent\nfrom a dataset to another which prevents from doing cor-\nrect translation from one tag set to another with a simple\nstring matching. Second, there may be duplicated tags i.e.\ntag with different names but referring to the exact same\ngenre such as Bossa Nova andBossanova (without space)\nin Discogs. Thirdly, there may be polysemy issues for\nsome tags: it happens that a single tag refers to different\nconcepts. In Discogs, the tag hardcore may refer to hard-\ncore punk or to hardcore electronic music which are quite\ndifferent genres. Finally, while a tag set may be structured\nin a taxonomy or an ontology, those have limitation for\nexpressing all relations between tags: for instance the tag\nBlues Rock in the MuMu taxonomy is a subgenre of Rock\nand is not related to Blues , which makes it as close to Elec-\nc\rRomain Hennequin, Jimena Royo-Letelier, Manuel\nMoussallam. Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: Romain Hennequin, Ji-\nmena Royo-Letelier, Manuel Moussallam. “Audio based disambiguation\nof music genre tags ”, 19th International Society for Music Information\nRetrieval Conference, Paris, France, 2018.tric Blues as to Drum & Bass according to the taxonomy.\nMoreover taxonomy and ontology are generally designed\nwith a particular purpose in mind [21], possibly clarity for\nthe customer for the MuMu taxonomy (which is the Ama-\nzon taxonomy), while it may be musicologic precision for\nDBpedia1, which may result in different meaning for tags\nand different relationship between them.\nBuilding a genre representation from these tag systems\nin order to deal with these ambiguity issues can be done\nusing a top-down approach, using an expert-level ontol-\nogy such as the DBpedia ontology and trying to project the\ntag system into this ontology [7]. Mapping tags to an ex-\nternal expert ontology is not trivial, as a genre can have\nseveral different name and some tags may have several\nmeanings: the tag funk for instance may refer to a genre\nborn in the 60s derived from soul and jazz, or, in Brazil,\nto Funk carioca which is a totally different style inspired\nby gangsta rap music. It also can be done using a bottom\nup approach, inferring relations between entities from data.\nThe latter was mainly done using the genre tag distribution\nof a dataset with Latent Semantic Analysis (LSA) [28] or\nwith a straight use of cooccurrences [26,27] which all rely\non the distributional hypothesis (similar tags are tags that\ncooccur a lot with same other tags). However, it is some-\ntimes not possible to rely only on tag distributions: the\nMuMu dataset has no overlap with the GAS, which pre-\nvents from using tags cooccurrences to infer relationship\nbetween MuMu genre tags and GAS ones.\nSo far, the literature has been mainly focusing on mu-\nsic genre classiﬁcation on ﬂat tag systems from audio\n[4, 8, 9, 23, 24, 30], text such as reviews [13, 19] or lyrics\n[3, 17], album covers [16] or combinations of the previous\nmodalities [18, 20, 25], while rarely addressing the actual\nsemantic relationships that exist between genres. In [29],\nthe authors pointed out that focusing on classiﬁcation met-\nrics was not sufﬁcient and suggested a deeper results anal-\nysis such as explanation of the confusion of the classiﬁers\nin term of musicological aspects. In this paper, we suggest\ngoing deeper in this direction and seeing how the confu-\nsion of the classiﬁer is able to generate a structured genre\nrepresentation: if the classiﬁer is good enough, the con-\nfusion it makes should be able to encode the relation of\nproximity between genres. Showing this property has two\nimplications: it shows in a qualitative way that the clas-\nsiﬁer performs well and allows generation of a structured\nrepresentation of a tag system using audio.\nIn this paper, we thus aim to disambiguate genre tags\n1wiki.dbpedia.org645and relations between them using audio as an alternative\nto the distributional hypothesis: we propose a method able\nto spot inconsistencies, help reducing them and relate tags\nbetween themselves, possibly across different non overlap-\nping datasets with different tag systems. We enforce that\nthe representation is based on audio only information and\nnot on tag distribution using a monolabel learning scheme.\nWhile extracting a semantic representation from audio an-\nnotated with a ﬂat tag representation was already sparsely\naddressed (in [14], basic ontological relations between a\nfew instruments are learnt back from isolated music instru-\nments sounds and in [15] a simple music genre taxonomy\nis learnt with a few genre concepts), in this paper, we pro-\npose to learn representations at a large scale for tag sys-\ntems with several hundreds of genre tags and with datasets\nof several hundreds of thousands of songs.\nIn Section 2, we explain how we compute genre tag\nembeddings using an audio-based genre classiﬁer and use\nthem to deﬁne an audio-based similarity between genre\ntags. In Section 3, we validate the learnt similarity by\nshowing that it performs fairly on two artiﬁcial tasks\n(Discogs taxonomy learning and artiﬁcial deduplication).\nIn Section 4, we show how we can use the learnt similar-\nity to translate tags from a dataset tag system to another.\nFinally, we draw conclusions in Section 5.\n2. LEARNING A GENRE REPRESENTATION\nIn this section, we explain how we build embeddings of\ngenre tags using a genre classiﬁer with audio input. We\nassociate to each genre tag tiin the genre tag set T=\nft1; :::; t Ncgan embedding vector f(ti) =vti2Rn, such\nthatd(vt1;vt2)should correspond to an audio similarity\nbetween genre tag t1and genre tag t2.\n2.1 Datasets\nWe use two large-scale genre annotated datasets for our\nexperiments: The MuMu dataset [20], and the genre pro-\nvided by the Discogs website2. We matched both datasets\nto Deezer track IDs using song metadata (album and artist\nnames, and track titles). We extracted a 30s-long excerpt\nfor each track (the position of the excerpt was sampled at\nrandom between the beginning and the end of the track).\nFor tags with too few occurrences, we extracted several ex-\ncerpts for balancing (as explained in Section 2.2). To avoid\noverlap between datasets we removed the 7260 tracks that\nbelong to both datasets (in order to not affect the transla-\ntion experiment of Section 4).\nWhile each dataset provides a simple genre taxonomy,\nwe do not rely on it in the classiﬁcation stage and consider\nthe genre annotations as ﬂat tag systems with no links be-\ntween tags. The provided taxonomies are used afterwards\nfor evaluation of the built genre representation.\n2.1.1 Discogs\nDiscogs is referred as the “ largest open database contain-\ning explicit crowd-sourced genre annotations ” in [1]. It\n2https://discogs.comcontains genre annotations at the album level for hundreds\nof thousands of albums. Genre tags in Discogs are or-\nganized in a two-level hierarchy: the ﬁrst level, referred\nasgenre , includes generic genre categories ( genre:Rock3,\ngenre:Jazz , etc. . . ) and the second level, referred as style,\ncorresponds to subgenres ( Psychedelic Rock ,Cool Jazz ,\netc. . . ). It contains a total of more than 500 genre/style\ntags. Only the 250most common tags were kept in our\nexperiments ( 235style tags and 15genre tags).\nAfter cleaning, balancing (see Section 2.2) and match-\ning, the Discogs dataset we used contained 418184 tracks.\n2.1.2 MuMu dataset\nThe MuMu dataset [20] has genre annotation based on the\nAmazon 4-level genre taxonomy. It contains genre anno-\ntations at the album level for 31471 albums which contain\na total of 147295 tracks. It contains a total of 446different\ngenres. Only the top 211tags (those with less than 300\nannotated tracks are discarded) are kept. After cleaning,\nbalancing (see Section 2.2) and matching, the ﬁnal MuMu\ndataset we used contained 122014 tracks.\n2.1.3 Dataset split\nWhen training the system described in Section 2.3, we\nsplit the datasets into a training dataset ( 70%), a vali-\ndation dataset ( 10%) used for early stopping, and a test\ndataset ( 20%) used for building genre representations (Sec-\ntion 2.4). The split was done at the artist level meaning two\ntracks by the same artist are in the same part of the split in\norder to avoid overﬁtting on variables such as album or\nartist as advised in [11, 22].\n2.2 Monolabel learning\nThe annotations in a multilabel dataset carry information\nof popularity (through number of occurrences of a tag) and\nof similarity (through cooccurrences of tags). This infor-\nmation was already used in several papers to build genre\ntaxonomies from a ﬂat tag system [26–28] or to build a\ntarget representation to improve classiﬁcation results [20].\nThe goal of the paper is to learn a genre representation\nonly through audio and to avoid using non-audio informa-\ntion such as the one provided by the tag distribution. As\nthis distribution can be easily learnt as a side information\nin the last layer of a neural network, where bias can en-\ncode popularity (higher bias for more popular genre) while\nweights can encode similarity between genres (important\nvalue of dot product between weights corresponding to\nsimilar genres and vice versa), simply training a multilabel\naudio classiﬁer based on a neural net will result in taking\nadvantage of this information, and it may be difﬁcult to as-\nsess at what point the actual audio information is relevant\nin building the representation from this classiﬁer.\nIn order to avoid inﬂuence of these non-audio informa-\ntion in the built genre representation, we propose to turn\nthe multilabel classiﬁcation problem into a monolabel one\nusing the following learning scheme:\n3We preﬁx Discogs genre by ”genre:” to distinguish them from style646 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018\u000fTo remove cooccurrences information, we transform the\nmultilabel dataset into a monolabel one by sampling a\ntag among the multilabel tag annotation of every track.\n\u000fTo remove the popularity information, we balance\nequally all classes using a sampling probability inversely\nproportional to the global popularity of a tag (note, that\nit does not enforce perfect balancing).\nFor instance, if Rock appears 1000 times in the dataset and\nPunk appears 100times, a song with (multi-)labels fRock ,\nPunkgwill get as monolabel Rock with probability 1=11\nandPunk with probability 10=11. This ensures that rare\ngenre tags have a high probability of being drawn, and that\nwe keep the maximum of available information for rare\ntags while discarding somewhat redundant information for\nvery common tags.\nTo enforce balancing, tags with too many occurrences\nare downsampled to keep a maximum of 2000 occurrences\nper tag. Genre with not enough occurrences are upsampled\nto2000 occurrences by duplicating tracks (different 30s\nexcerpts are chosen for each track).\nIn order to avoid ﬁtting independent variables, the sam-\npling is done at the album level, which means that every\ntrack from the same album gets the same label. It also\nensures that different excerpts of the same track have the\nsame label. Using this learning scheme, the confusion be-\ntween genres should result only from similarities in audio.\n2.3 Classiﬁcation system\nWe use a convolutional neural network with a recurrent\nlayer on top of it as a monolabel classiﬁer. We feed it\nwith Mel-spectrograms computed with 1024 samples long\nHann windows without overlap, with 96Mel ﬁlters. Au-\ndio is ﬁrst downsampled to 22050 Hz and stereo channels\nare summed up. Mel-spectrogams were log compressed\nusing the function f(x) = log(1 + Cx)where we chose\nC= 10000 . It results in 646\u000296input matrices.\nThe architecture of the neural network is quite similar to\nthe one used in [4] for automatic tagging, but with half as\nmany ﬁlters in the convolutional layers (we noticed that it\nresulted in less overﬁtting) and a Gated Recurrent Unit [2]\non top of the conv layer (which improved overall classiﬁca-\ntion accuracy). The gated linear unit was used for temporal\npooling (only last temporal output is forwarded to the last\nlayer which removes the time dimension) and was used in\nconjunction with dropout to reduce overﬁtting. The archi-\ntecture is summed up in Table 1.\nThe network was trained with a categorical cross-\nentropy loss with mini-batch stochastic gradient descent\nusing Adadelta [32] and early stopping on the validation\nloss. The system was implemented with Keras [5] using\nthe Tensorﬂow [10] backend.\nAs the main goal of the paper is not to perform in terms\nof classiﬁcation results, we did not try to optimize thor-\noughly the architecture and we just checked that our pro-\nposed system had similar classiﬁcation results as in [20].Layer output shape N param.\nLog-comp Mel-spec 646\u000296\u00021 0\nConv 3\u00023\u000264- MP 2\u00022 323\u000248\u000264 640\nConv 3\u00023\u0002128- MP 3\u00024 107\u000212\u0002128 1280\nConv 3\u00023\u0002256- MP 2\u00023 53\u00024\u0002256 2560\nConv 3\u00023\u0002512- MP 3\u00024 17\u00021\u0002512 5120\nGRU 512 512 1574400\nDense Softmax Nc 512\u0002Nc\nTable 1 . Architecture of the Neural Network. MP stands\nfor Max Pooling.\n2.4 Genre embeddings from classiﬁcation\nThere are several ways of extracting an embedding from\na neural net based classiﬁcation system. We describe the\nthree kinds of genre embeddings we generated from the\naudio classiﬁer in the following subsections. Whereas the\nﬁrst embedding only uses parameters of the classiﬁers, the\nother two make use of the test set.\n2.4.1 Last hidden layer weights\nThe weights of the last hidden layer Ware a 512\u0002Nc\nmatrix. The i-th column of this matrix is then chosen as\nthe embedding of genre tag ti:\nfw(ti) =vti=W:;i: (1)\nThis is a straightforward representation of a genre tag in\nthe network: the output of the last hidden layer for a track\nannotated with some genre should be similar (in terms of\ndot product) to the weight vector of this genre. However, it\nnecessitates retraining to incorporate new genre tags in the\nembedding.\n2.4.2 Columns of output\nWe can also build an embedding using the test dataset:\nfor every track sin the test dataset, we denote Tsthe set\nof tags associated to s. We note the test dataset S=\nfs1; s2: : : sNsgwhere skare the track excerpts. The out-\nput of the network when fed with track excerpt siis a vec-\ntorpk2[0;1]Nc(withPNc\nj=1[pk]j= 1). We note Pthe\nmatrix in RNs\u0002Ncwithpkask-th row. The embedding of\ntagtiis then deﬁned as the i-th column of matrix P:\nfc(ti) =vti=P:;i: (2)\nThis embedding does not require annotation information\nabout the tracks of the test set and the cosine similarity\nmatrix between embeddings of all pairs of genre can be\nunderstood as a normalized confusion matrix and is the au-\ndio counterpart of the occurrence based representation de-\nﬁned in (4). However it has very large dimension (that may\nbe reduced using dimension reduction techniques such as\nLSA) and it is quite difﬁcult to add extra genres without\nretraining the whole system.\n2.4.3 Mean of output\nThis third embedding type also uses the test dataset and\ntakes advantage of the annotations. We note Sti=\nfsk1; sk2: : : skNtgthe set of tracks annotated with genre\ntagti. We then associate to each tithe set of outputs of theProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 647classiﬁerfpkjsk2Stig. Ideally each genre tag tiwould\nbe represented by the distribution of all possible outputs\nfor this genre. In practice, we compute statistics on these\ndistributions. We then deﬁne the third type of genre tag\nembeddings as the mean of the output:\nfm(ti) =vti=1\njStijX\nsk2Stipk: (3)\nAspkis a categorical probability distribution, fm(ti)is\ntoo. Embedding fmmakes it possible to incorporate new\ntags without retraining the whole system, by simply adding\ntracks annotated with the new genre tag in the dataset (the\nonly constraints would be that the classiﬁer was trained\nwith similar genres): this is an important property of the\nembedding since it makes it much easier to incorporate\nnew knowledge from another tag system.\n2.4.4 Occurrence based representation\nIn order to compare the audio-based representation we\nalso deﬁne the following representation which is not based\non audio but on tag distribution only. We note M2\nf0;1gNs\u0002Ncthe multilabel tag occurrence matrix with co-\nefﬁcient Mij= 1iff track siis annotated with tag tj. The\ncoocurrence embedding of tag tiis then deﬁned as the i-th\ncolumn of matrix M:\nfdist(ti) =M:;i: (4)\nThis deﬁnition then shares similarity with the audio-based\nrepresentation fc.\n2.4.5 Similarity measure\nTo compare tags, we use the cosine similarity applied to the\nfour types of genre tag embeddings deﬁned in Equations\n(1), (2), (3) and (4).\n3. MODEL V ALIDATION\nIn this section, we validate that the audio-based similarities\nlearnt in Section 2 have a semantic meaning by showing\nthat the original Discogs taxonomical relations can be in-\nferred from the similarities and that they make it possible\nto spot duplicate tags in a dataset. In order to reproduce the\nresults, we make available the embeddings, the similarity\nmatrices we obtained for the different representation4as\nwell as dataset ﬁles (as lists of Deezer song IDs).\n3.1 Taxonomy Learning\nIn this section, we use similarity obtained from the genre\nembeddings described in Section 2, to infer hierarchical\nlinks between genres. We trained the classiﬁcation system\nwith the Discogs dataset and the purpose of the experiment\nis to infer the genre/style links of the two-level Discogs\ntaxonomy from audio.\nThe cosine similarity computed between genre tag em-\nbeddings provides a measure of similarity between genre\n4github.com/deezer/audio_based_disambiguation_\nof_music_genre_tags.gitfw fc fm fdist\nHR@1 85:1\u00064:6 89:4\u00063:9 87:7\u00065:2 96:2\u00062:5\nHR@2 91:9\u00063:5 98:3\u00061:7 96:2\u00063:2 100:0\u00060\nMAP 90:6\u00062:9 94:2\u00062:2 93:1\u00063:0 98:1\u00061:2\nTable 2 . Average ranking metrics (in %) for the Discogs\ntaxonomy learning task with 95% conﬁdence intervals.\ntags. This can be used to rank for each style the similarity\nwith each of the 15genres. The ground truth is the ac-\ntual genre associated to the style in the Discogs taxonomy\n(note that some rare style are associated to 2music genres,\nsuch as hardcore andnoise which are associated to both\nrock andelectronic ). We measure the quality of this rank-\ning with classic ranking metrics: Hit Rate (HR)@k which\nis the percentage of style for which the associated genre is\nin the top- kaccording to the similiarity score.(HR@1 can\nbe considered as a classiﬁcation accuracy) and Mean Av-\nerage Precision (MAP) as deﬁned in [33]. MAP takes into\naccount the rank of the related genre in the similarity list.\nResults are presented in Table 2. As a reference, we re-\nport results for the occurrence based embedding fdist. As\nstyle tags are always present together with their parent\ngenre tag in the annotations, the performance of the oc-\ncurrence based representation should be interpreted as an\nupper-bound for the results of the other representations, the\nerrors being likely due to incoherences in the Discogs tax-\nonomy (which is conﬁrmed by the perfect HR@2 score of\nfdist). Among the audio-based representations, fcperforms\nbetter than the two others. Despite being smaller than the\noccurrence based representation, we can see that the met-\nrics for the audio representations are quite high, notably\nforfcwhich has a near perfect HR@2. This is noteworthy,\nsince only audio information is used to infer the relations.\nA qualitative analysis of the error shows that most of\nthe “errors” (in the sense that the most similar genre to\na style is not is related genre) actually make sense: for\ninstance blues rock which is a subgenre of genre:rock\nin Discogs taxonomy has the greatest similarity (for fc)\nwith genre:blues which makes as much sense as the\nother (the same phenomena with hybrid subgenre ap-\npears with jazz-funk and genre:funk / soul instead of\ngenre:jazz ,pop rock andgenre:rock instead of genre:pop\nandsoul-jazz andgenre:funk / soul instead of genre:jazz ).\nOther noteworthy examples are bossa nova (subgenre of\ngenre:jazz ) associated with genre:latin ,musique concr `ete\n(subgenre of genre:electronic ) associated to genre:non-\nmusic orrnb/swing (subgenre of genre:hip hop ) associated\ntogenre:funk / soul . These qualitative results conﬁrms that\nmost of the “errors” are actually due to limitations of the\noriginal taxonomy and that HR@2 may be the most re-\nvealing metric. In Figure 1, we plot a 2D t-distributed\nstochastic neighbor embedding (t-SNE) [31] of the learnt\naudio representation fwin order to get visual insights about\nit: most music style tags are gathered in coherent clusters\nand are most of the time close to their related genre tag.\nA noteworthy exception is the style tags related to folk,\nworld, & country that form several clusters, one of which\nbeing next to latin, another one being next to blues and an-648 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 1 .2D t-SNE of fwfor the Discogs tags. Each style\nis colored with the same color as its related genre . Main\ngenres are depicted with bigger circle and black edges.\nother one next to pop. This is pretty coherent since the tag\nfolk, world, & country is supposed to gather several very\ndifferent styles that may be closely related to other genres.\n3.2 Tag deduplication\nIn this section, we show how the audio-based similarities\nlearnt in Section 2 can be used to spot duplicates in a tag\nsystem. To do that we rely on the ability of a classiﬁer\nbased on audio data to discriminate between two genre\ntags. If two genre tags cannot be discriminated, they prob-\nably have some strong relation (even if they have very dis-\nsimilar names). There may be several reasons for two tags\nhaving high confusion similarity: First, they may represent\nthe exact same genre. Second, genre related audio charac-\nteristics may be very similar (the genre may be very similar\nwith respect to audio). Thirdly, there may be differences of\ndistribution in the datasets: datasets are usually an imper-\nfect sample of the set of all music. Some genre may be\nbiased toward a subgenre in a dataset while not in another\none, which may result in strong differences in the meaning\nof some genres. Last, the classiﬁer may not be able to dis-\ntinguish them while there exists difference in some audio\ncharacteristics (that the classiﬁer is not able to handle).\nAs it is very difﬁcult to assess a ground truth for such a\ndeduplication experiment, we propose the following artiﬁ-\ncial tag duplication: we use the Discogs genre dataset. We\nartiﬁcially duplicate every genre tag by creating two dupli-\ncate tags: for instance, Rock is duplicated into Rock1 and\nRock2 , which means that half of the tracks originally an-\nnotated with Rock get the annotation Rock1 instead while\nthe other half get the annotation Rock2 . To avoid learning\nthe similarity through artist speciﬁc characteristics, we per-\nform the split at the artist level, meaning that tracks of the\nsame artist annotated with Rock will get all the same sub-\ntag (either Rock1 orRock2 ). Note that a subtag of group 1\ncannot cooccur with a subtag of group 2, which results in\ntwo separate tag systems (that we will refer as system 1and\nsystem 2), with no overlap. While all tags from system 1\nhaving a semantically equivalent counterpart in system 2is\nquite artiﬁcial, the total separation between the tag systems\nin term of cooccurrences is realistic. There is, for instance,\nno overlap between the GAS and the MuMu dataset which\nmeans we can only rely on audio for linking them.\nIn a similar way as in the experiment of Section 3.1,fw fc fm\nHR@1 92:0\u00062:4 92:8\u00062:3 74:8\u00063:8\nHR@2 95:8\u00061:8 97:0\u00061:5 83:0\u00063:3\nMAP 98:1\u00060:6 98:4\u00060:5 93:3\u00061:1\nTable 3 . Average ranking metrics (in %) for the Discogs\ndeduplication task with 95% conﬁdence intervals.\nwe use the similarity between genre tags embeddings as a\nduplication score . The task is then for each genre tag, to\nretrieve its duplicated tag. Once again, we present quan-\ntitative results in terms of HR@k and MAP in Table 3.\nAs opposed to the taxonomy learning task, it does not\nmake sense to compare the audio based representations\nto the occurrence-based representation since the sampling\nscheme we use avoid a tag of group 1cooccurring with a\ntag of group 2which means that the cosine similarity be-\ntween any tag of group 1with any tag of group 2is0.\nfwandfcperforms similarly, both performing signiﬁcantly\nbetter than fm. Once again the score seems reasonably high\nfor a representation based on audio information only.\nIt is interesting to look at the “errors” (when the most\nsimilar tag is not the actual duplicate) done by the system\nusing fc. Some errors were actual duplicates in Discogs:\nbossa nova was associated to bossanova (without a space)\nwhich is clearly a duplicate issue in Discogs. Other ex-\nample are style:reggae andgenre:reggae (where a style tag\nas the same name as its related genre tag) or thug rap and\ngangsta (considered as the same genre in Wikipedia). This\nshows that the genre similarity computed from the embed-\ndings is able to spot actual duplicates and that HR@2 may\nbe again the most revealing metric. Some errors are match-\ning between quite different concepts but with very sim-\nilar audio, such as ﬁeld recording /musique concr `ete,po-\netry/spoken word ,spoken word /genre:non-music andcon-\nscious /genre:hip hop . Other errors are with very similar\ngenres: bop/hard bop ,honky tonk /country blues ,space\nrock/post rock A few errors are more difﬁcult to explain\nsuch as ragtime /tango which may have some audio simi-\nlarities (the use of piano is quite common in both genres,\nand both are intended for dancing). These errors may come\nfrom the classiﬁcation system we use or from a strong bias\nor annotation noise in the Discogs annotations.\n4. TAGS TRANSLATION\nIn this section, we perform another experiment that aims\nat translating tags from MuMu dataset to Discogs dataset.\nFor sack of clarity Discogs tags are preﬁxed with “ D:” and\nMuMu tags with “ M:”. When there are no or few overlaps\nbetween two datasets, we cannot rely on cooccurrences of\ntags to model relation between the tag systems. The only\nmedia we can rely on is then audio.\nTo train the classiﬁer (see Section 2.3), we used the\nconcatenation of the tags from the MuMu dataset and\nthe Discogs dataset. Tags of each dataset were consid-\nered different even if they had the exact same name: e.g.,\nthere were a M:jazz tag that was considered different from\ntheD:jazz tag. The experiment of translation is then\nvery similar to the deduplication task presented in 3.2:Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 649Audio-based translation fc Cooccurrence-based translation fdist\nMumu tag Discogs tag Mumu tag Discogs tag\nbebop bop irish folk celtic\nmovie scores score contemporary big band big band\nindie & lo-ﬁ lo-ﬁ latin music genre:latin\nelectric blues modern elec. blues rap & hip-hop genre:hip hop\nelectronica leftﬁeld vocal blues ragtime\npunk-pop pop punk dance & electronic genre:electronic\nmodern postbebop genre:jazz today’s country country\nspecial interest avantgarde electric blues genre:blues\nsinger-songwriters folk rock children’s music genre:children’s\nr&b rnb/swing comedy & spoken word comedy\nTable 4 . Top 10most similar tags between MuMu and\nDiscogs according to fc(left columns) and fdist(right\ncolumns), removing string matched tags.\nthe translation task consists of deduplicating the whole\nMuMu/Discogs tag set, focusing on pairs of duplicates for\nwhich the ﬁrst element is a MuMu tag and the second ele-\nment is a Discogs tag.\nThis allows to translate tags from one tag system to an-\nother, but also to spot possible genre deﬁnition differences\nbetween datasets: if two genre tags from two different\ndatasets, with the exact same name can be discriminated\nwith audio, this is probably because they do not carry the\nexact same meaning (provided we can move appart overﬁt-\nting of the audio classiﬁer used to build the representation).\nWe only consider here simple one-to-one tag map-\npings between MuMu and Discogs although it is restrictive\nsince there may exist one-to-many mapping (e.g. between\nM:avant garde & free jazz andD:avant-garde jazz /D:free\njazz) or even more complex relationships.\nAs the Discogs and MuMu datasets have some common\ntracks, we can compare the audio-based similarities with\nthe cooccurrence-based one derived from fdist.\nThere are two aspects that may be qualitatively as-\nsessed: why would two tags with different names be as-\nsociated? and why would two tags with same name have a\nvery low audio similarity.\nIn the two ﬁrst columns of Table 4, we present the 10\nDiscogs tags that are most similar (according to fc) to\nMuMu tags while not having the same normalized name.\nAs can be seen, when the names are different, it can be due\nto the following reasons:\n\u000fTwo different names are used for the exact same\nconcept: M:bebop /D:bop ,M:punk-pop /D:pop punk ,\nM:movie scores /D:score .\n\u000fSome genres were considered sufﬁciently similar to be\ngrouped under the same tag name in one of the tag sys-\ntem while they were not in the other one e.g. M:indie &\nlo-ﬁ/D:lo-ﬁ ,M:r&bD:rnb/swing .\n\u000fOne genre is a subgenre of the other: M:electric\nblues /D:modern electric blues ,M:modern postbe-\nbop/D:genre:jazz\nThe association between M:singer-songwriters (a sub-\ngenre of M:rock ) with D:folk rock (a subgenre of\nD:genre:rock ) seems to link quite similar concepts (which\nseems to be conﬁrmed by the cooccurrence based similar-\nity that is quite high). M:electronica andD:leftﬁeld seem\nto be quite broad electronic genres: the span of the for-\nmer and the lack of precise deﬁnition of the latter while\nboth seem not intended for dancing could explain the asso-ciation. The association M:special interest /D:avantgarde\nremains quite unclear, while the tags are quite vague.\nIn the two last columns of Table 4 are presented top\n10most similar tags between MuMu and Discogs accord-\ning to the cooccurrence based similarity (excluding string\nmatched pairs with basic normalization as in [26]). It can\nbe seen that the top 10for cooccurrences and the top 10\nfor audio similarity contains mostly different tags, with\nthe exception of M:electric blues which is not mapped\nto the same Discogs tag: this tends to show that cooc-\ncurrence similarity is complementary to the audio-based\nsimilarity, and when cooccurrence information is available\n(overlap between dataset), using both similarities should\nprovide the best analysis. This is conﬁrmed with some\nMuMu/Discogs pairs such as M:bebop /D:bop andM:post\nhardcore /D:post-hardcore which seems to be perfect map-\nping and have very high audio similarity but very low (less\nthan0:1) cooccurrence similarity. The low cooccurrence\nsimilarity may be explained by a lack of data for these tags.\nOn the other hand, it is also interesting to check tags\nwith the exact same name in both datasets, but with quite\nlow similarity score: the tags electronic ,instrumental have\nvery low similarity (according to both fcandfdist) from\none database to another. D:electronic refers to a generic\nterm for describing all electronic music while this exact\nsame concept seemed to be carried by M:dance & elec-\ntronic in MuMu. M:electronic is actually a subgenre of\nM:progressive which is a subgenre of M:rock and then\nhas a very different meaning than the one in Discogs. in-\nstrumental (which is not a genre by itself) is considered\na subgenre of M:new age andM:country in the MuMu\ntaxonomy and a subgenre of D:hip hop in Discogs (while\na large number of non-hip-hop songs seems to have the\nD:instrumental tag).\nThus, audio made it possible to spot signiﬁcantly differ-\nent genres that were represented by the exact same string.\nThis highlights that the meaning of some genre may vary\nsigniﬁcantly from a database to another and that string\nmatching can result in wrongly matched concepts.\n5. CONCLUSION\nIn this paper we presented a way of learning genre embed-\ndings from audio and showed that they are able to encode\nsemantic similarities between genre tags: we showed that\nthese embeddings were able to build genre taxonomies, to\nspot duplicates in a dataset or to translate genre from one\ntag set to another one. In future works, we plan to ex-\nplore extraction of structured representation of other tag\ntypes than genre (mood, instruments, country...) from au-\ndio and to exploit other datasets such as the FMA dataset\nor GAS to learn a more global representation. We also plan\nto explore in more details how we can use several sources\n(audio, expert based ontology, string matching, cooccur-\nrences) to build richer representation from ﬂat tag systems.\n6. ACKNOWLEDGEMENTS\nThe authors would like to thank Guillaume Salha for fruit-\nful conversations and Matt Mould for proof-reading.650 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20187. REFERENCES\n[1] Dmitry Bogdanov and Xavier Serra. Quantifying Mu-\nsic Trends and Facts Using Editorial Metadata From\nthe Discogs Database. In International Society for Mu-\nsic Information Retrieval Conference , pages 89–95,\n2017.\n[2] Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. Learning Phrase Repre-\nsentations using RNN Encoder-Decoder for Statistical\nMachine Translation. In Empirical Methods in Natural\nLanguage Processing , 2014.\n[3] Kahyun Choi, Jin Ha Lee, and J. Stephen Downie.\nWhat is this song about anyway?: Automatic classi-\nﬁcation of subject using user interpretations and lyrics.\nInACM/IEEE Joint Conference on Digital Libraries ,\npages 453–454. IEEE, sep 2014.\n[4] Keunwoo Choi, George Fazekas, and Mark Sandler.\nAutomatic tagging using deep convolutional neural\nnetworks. In International Society for Music Informa-\ntion Retrieval Conference , pages 805–811, 2016.\n[5] Franc ¸ois Chollet. Keras: Deep learning library for\ntheano and tensorﬂow, 2015.\n[6] Micha ¨el Defferrard, Kirell Benzi, Pierre Van-\ndergheynst, and Xavier Bresson. FMA: A Dataset For\nMusic Analysis. In International Society for Music\nInformation Retrieval Conference , 2017.\n[7] Dennis Diefenbach, Pierre Ren ´e Lh ´erisson, Fabrice\nMuhlenbach, and Pierre Maret. Computing the seman-\ntic relatedness of music genres using semantic web\ndata. In CEUR Workshop , volume 1695, 2016.\n[8] Sander Dieleman, Phil ´emon Brakel, and Benjamin\nSchrauwen. Audio-based Music Classiﬁcation with a\nPretrained Convolutional Network. In International\nSociety for Music Information Retrieval Conference ,\npages 669–674, 2011.\n[9] Sander Dieleman and Benjamin Schrauwen. End-to-\nend learning for music audio. In IEEE International\nConference on Acoustics, Speech and Signal Process-\ning, pages 6964–6968, 2014.\n[10] Mart ´ın Abadi et al. Tensorﬂow: Large-scale machine\nlearning on heterogeneous distributed systems, 2015.\n[11] Arthur Flexer. a Closer Look on Artist Filters for Mu-\nsical Genre Classiﬁcation. In Ismir 07 , pages 341–344,\n2007.\n[12] Jort F Gemmeke, Daniel P.W. Ellis, Dylan Freedman,\nAren Jansen, Wade Lawrence, R. Channing Moore,\nManoj Plakal, and Marvin Ritter. Audio Set: An on-\ntology and human-labeled dataset for audio events. In\nIEEE International Conference on Acoustics, Speech\nand Signal Processing , pages 776–780, 2017.[13] Xiao Hu, JS Downie, Kris West, and AF Ehmann. Min-\ning Music Reviews: Promising Preliminary Results. In\nInternational Society for Music Information Retrieval\nConference , pages 536–539, 2005.\n[14] S ¸efki Kolozali, Mathieu Barthet, Gy ¨orgy Fazekas, and\nMark Sandler. Automatic ontology generation for mu-\nsical instruments based on audio analysis. IEEE Trans-\nactions on Audio, Speech and Language Processing ,\n21(10):1–14, 2013.\n[15] Tao Li and Midsunori Ogihara. Music genre classiﬁca-\ntion with taxonomy. In Acoustics, Speech, and Signal\nProcessing , pages 197–200, 2005.\n[16] Janis Libeks and Douglas Turnbull. You can judge an\nartist by an album cover: Using images for music an-\nnotation. IEEE Multimedia , 18(4):30–37, apr 2011.\n[17] Rudolf Mayer, Robert Neumayer, and Andreas Rauber.\nRhyme and Style Features for Musical Genre Classiﬁ-\ncation By Song Lyrics. In International Society for Mu-\nsic Information Retrieval Conference , pages 337–342,\n2008.\n[18] Robert Neumayer and Andreas Rauber. Integration of\nText and Audio Features for Genre Classiﬁcation in\nMusic Information Retrieval. In Advances in Informa-\ntion Retrieval , pages 724–727. 2007.\n[19] Sergio Oramas, Luis Espinosa-anke, Aonghus Lawlor,\nXavier Serra, Horacio Saggion, Music Technology\nGroup, Universitat Pompeu Fabra, and Universi-\ntat Pompeu Fabra. Exploring Customer Reviews for\nMusic Genre Classiﬁcation and Evolutionary Studies.\nInInternational Society for Music Information Re-\ntrieval Conference , 2016.\n[20] Sergio Oramas, Oriol Nieto, Francesco Barbieri, and\nXavier Serra. Multi-label Music Genre Classiﬁcation\nfrom Audio, Text, and Images Using Deep Features. In\nInternational Society for Music Information Retrieval\nConference , 2017.\n[21] Franc ¸ois Pachet and Daniel Cazaly. A Taxonomy of\nMusical Genres. In Content-Based Multimedia Infor-\nmation Access Conference , pages 1238–1245, 2000.\n[22] Elias Pampalk, Arthur Flexer, and Gerhard Widmer.\nImprovements of Audio-Based Music Similarity and\nGenre Classiﬁcation. In Ismir , volume 5, pages 634–\n637, 2005.\n[23] Jordi Pons, Thomas Lidy, and Xavier Serra. Experi-\nmenting with musically motivated convolutional neu-\nral networks. In International Workshop on Content-\nBased Multimedia Indexing , volume 2016-June, pages\n1–6. IEEE, jun 2016.\n[24] Chris Sanden and John Z. Zhang. Enhancing Multi-\nlabel Music Genre Classiﬁcation Through Ensemble\nTechniques. In ACM SIGIR Conference on Research\nand Development in Information Retrieval , pages 705–\n714, New York, New York, USA, 2011. ACM Press.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 651[25] Alexander Schindler and Andreas Rauber. An Audio-\nVisual Approach to Music Genre Classiﬁcation\nthrough Affective Color Features. In European Con-\nference on Information Retrieval , pages 61–67, 2015.\n[26] Hendrik Schreiber. Improving genre annotations for\nthe million song dataset. In 16th International Soci-\nety for Music Information Retrieval Conference , pages\n241–247, 2015.\n[27] Hendrik Schreiber. Genre Ontology Learning: Com-\nparing Curated With Crowd-Sourced Ontologies. 17th\nInternational Society for Music Information Retrieval\nConference , pages 400–406, 2016.\n[28] Mohamed Sordo, Oscar Celma, Mart ´ın Blech, and En-\nric Guaus. The Quest for Musical Genres: Do the Ex-\nperts and the Wisdom of Crowds Agree? In 9th Inter-\nnational Conference on Music Information Retrieval ,\npages 255–260, 2008.\n[29] Bob L. Sturm. Aalborg Universitet Classiﬁcation Ac-\ncuracy Is Not Enough Classiﬁcation Accuracy Is Not\nEnough On the Analysis of Music Genre Recognition\nSystems. Journal of Intelligent Information Systems ,\n41(3):371–406, 2013.\n[30] George Tzanetakis and Perry Cook. Musical Genre\nClassiﬁcation of Audio Signals. IEEE Transactions On\nSpeech And Audio Processing , 10(5), 2002.\n[31] Laurens Van Der Maaten and Geoffrey Hinton. Visu-\nalizing high-dimensional data using t-sne. Journal of\nMachine Learning Research , 9:2579–2605, 2008.\n[32] Matthew D Zeiler. ADADELTA: An Adaptive Learn-\ning Rate Method, 2012.\n[33] Mu Zhu. Recall, precision and average precision, 2004.652 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition.",
        "author": [
            "Eric Humphrey",
            "Simon Durand",
            "Brian McFee"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492445",
        "url": "https://doi.org/10.5281/zenodo.1492445",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/248_Paper.pdf",
        "abstract": "Identification of instruments in polyphonic recordings is a challenging, but fundamental problem in music information retrieval. While there has been significant progress in developing predictive models for this and related classification tasks, we as a community lack a common data-set which is large, freely available, diverse, and representative of naturally occurring recordings. This limits our ability to measure the efficacy of computational models. This article describes the construction of a new, open data-set for multi-instrument recognition. The dataset contains 20,000 examples of Creative Commons-licensed music available on the Free Music Archive. Each example is a 10-second excerpt which has been partially labeled for the presence or absence of 20 instrument classes by annotators on a crowd-sourcing platform. We describe in detail how the instrument taxonomy was constructed, how the dataset was sampled and annotated, and compare its characteristics to similar, previous data-sets. Finally, we present experimental results and baseline model performance to motivate future work.",
        "zenodo_id": 1492445,
        "dblp_key": "conf/ismir/HumphreyDM18",
        "keywords": [
            "Identification",
            "polyphonic recordings",
            "challenging",
            "fundamental problem",
            "music information retrieval",
            "significant progress",
            "predictive models",
            "classification tasks",
            "community",
            "common data-set"
        ],
        "content": "OpenMIC-2018: AN OPEN DATASET FOR MULTIPLE INSTRUMENT\nRECOGNITION\nEric J. Humphrey\nSpotify\nejhumprey@spotify.comSimon Durand\nSpotify\ndurand@spotify.comBrian McFee\nNew York University\nbrian.mcfee@nyu.edu\nABSTRACT\nIdentiﬁcation of instruments in polyphonic recordings is a\nchallenging, but fundamental problem in music informa-\ntion retrieval. While there has been signiﬁcant progress in\ndeveloping predictive models for this and related classiﬁ-\ncation tasks, we as a community lack a common data-set\nwhich is large, freely available, diverse, and representative\nof naturally occurring recordings. This limits our ability to\nmeasure the efﬁcacy of computational models.\nThis article describes the construction of a new, open\ndata-set for multi-instrument recognition. The dataset con-\ntains 20,000 examples of Creative Commons-licensed mu-\nsic available on the Free Music Archive. Each example is a\n10-second excerpt which has been partially labeled for the\npresence or absence of 20 instrument classes by annotators\non a crowd-sourcing platform. We describe in detail how\nthe instrument taxonomy was constructed, how the data-\nset was sampled and annotated, and compare its character-\nistics to similar, previous data-sets. Finally, we present ex-\nperimental results and baseline model performance to mo-\ntivate future work.\n1. INTRODUCTION\nMusic information retrieval (MIR) applications often de-\npend on statistical models and machine learning algorithms\nto relate audio content to semantically meaningful repre-\nsentations. The development and evaluation of these meth-\nods, in turn, depends on access to data, typically audio\nrecordings which have been annotated for a particular task\nsuch as chord recognition or tag prediction. Ideally, the\ndata we use to develop and evaluate models should be\nlarge, diverse, and open access, so that we as researchers\nand engineers can diagnose failure modes and propose im-\nprovements. However, because the vast majority of music\nis subject to copyright, this has historically been difﬁcult\nto achieve. This has resulted in a proliferation of de facto\nstandard data-sets which are small, biased, and not freely\navailable, which ultimately impedes scientiﬁc progress.\nc\rEric J. Humphrey, Simon Durand, Brian McFee. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Eric J. Humphrey, Simon Durand, Brian\nMcFee. “OpenMIC-2018: An open dataset for multiple instrument recog-\nnition”, 19th International Society for Music Information Retrieval Con-\nference, Paris, France, 2018.To address this problem, McFee et al. [15] proposed\nan iterative evaluation framework for developing open ac-\ncess data-sets for MIR, with a speciﬁc focus on instrument\nrecognition. While this proposal was apparently met with\nenthusiasm from the community, little progress has been\nmade in the intervening time toward enacting the proposal.\nWe hypothesize that this was primarily due to two factors:\na lack of a conveniently accessible audio data, and the ex-\npense of creating the initial development set. Recently, two\ncomplementary data-sets have been published, which we\ncombine here to resolve both of these issues: the Free Mu-\nsic Archive data-set [8], and AudioSet [11]. The result is a\ndiverse, open access collection of 20,000 audio clips anno-\ntated for the presence of 20 distinct instrument categories,\nwhich we denote as OpenMIC-2018 .\n1.1 Our contributions\nOur primary technical contribution is a new, open dataset\nfor training and evaluating instrument recognition algo-\nrithms. This article describes in detail how the dataset was\nconstructed by using a combination of model transfer from\nprevious datasets and crowd-sourced annotation. Our goals\nin documenting the data construction process are two-fold.\nFirst, it provides transparency around the various decisions\nand compromises made in this speciﬁc dataset. Second, we\ndescribe technical issues and general solutions which may\nbe of interest to future developers of music datasets.\n1.2 Related work\nInstrument recognition, either monophonic or polyphonic,\nis a long-standing problem in MIR, and many datasets for\nevaluating methods have been developed over the years.\nTable 1 lists some of the commonly used datasets, along\nwith various descriptive attributes. Of speciﬁc interest\nare the size of the collections, the number of instrument\nclasses, the duration of each example, the diversity of the\ncollection ( e.g., genre or style), whether the examples are\npolyphonic, the number of instrument labels per example,\nand whether the data is open access.\nBroadly speaking, existing datasets can be broken into\ntwo categories, according to whether samples contain\nnotes played by isolated instruments (RWC [12], Good-\nsounds [1], or NSynth [10]), or recordings of instrument\nensembles. Datasets of isolated instrument recordings are\noften easier to produce and annotate at large scale because\nlong recordings spanning multiple notes can be segmented438Table 1 . A qualitative comparison of different existing datasets for instrument identiﬁcation.\nCollection # Examples # Instruments Duration Diverse Polyphonic Multi-label Open\nRWC [12] 3,544 50 scale\nGood-sounds [1] 6,548 12 note X\nNSynth [10] 305,979 1,006 note X\nMedleyDB [3] 122 80 song X X X\nMusicNet [19] 330 11 song X X X\nIRMAS [4] 6,705 11 3s X X\nOpenMIC-2018 20,000 20 10s X X X X\nto generate examples with a shared label. However, the\nacoustic properties of ensemble recordings differ signiﬁ-\ncantly from those of isolated recordings, so models devel-\noped on single-instrument data often do not generalize to\nthe polyphonic case. Conversely, ensemble recordings are\ntypically difﬁcult to precisely annotate, which results in\neither high-quality collections with a small number of dis-\ntinct tracks (MedleyDB [3] or MusicNet [19]), or in col-\nlections with more tracks but with only partial annotations\n(such as IRMAS [4] with predominant instrument tags for\nshort excerpts). An ideal dataset would be large, diverse,\nstrongly annotated (including both positive and negative\nexamples), and freely available, so that at each instant in\nany recording, full information about all active instruments\nis available. While existing datasets succeed on some of\nthese criteria, none achieves all simultaneously.\n1.3 The Free Music Archive\nThe Free Music Archive1(FMA) is a web-based repos-\nitory of freely available music recordings. Recently, a\nsnapshot of FMA has been released to the research com-\nmunity to facilitate content-based music analysis evalua-\ntion [8]. The FMA snapshot includes 106,574 tracks by\nsome 16,341 artists, along with pre-computed features.\nEach track is annotated with both coarse (16 categories)\nand ﬁne (161 categories) genre tags. Tracks are provided\nunder a small variety of licenses, with the vast majority\nbeing Creative Commons [7]. This allows practitioners\nto archive and redistribute data (with some minor restric-\ntions), which is fundamental to the practice of open and\nreproducible scientiﬁc research.\nWhile previous authors have noted the particular genre\nbiases present on FMA [8], it nonetheless provides a large\npool of realistic musical content which could be used in re-\nsearch applications. Despite the speciﬁc quirks of the FMA\ncollection, using it as a basis for large-scale MIR evalua-\ntion has several beneﬁts. In addition to the obvious beneﬁts\nof being open access, it also facilitates data revision and in-\nclusion of new contributions from the community at large.\nThis in turn makes it easier for corrections to be integrated,\nand the collection to grow over time and not become stale.\n2. CONSTRUCTING OpenMIC-2018\nIn developing OpenMIC-2018, we took inspiration from\nImageNet [9]. ImageNet was constructed by selecting\n1http://freemusicarchive.org/\nInstrument model Candidate construction Selection / annotationInstrumentDNNInstrumentDNNInstrumentDNNInstrumentDNNVGGish featuresAudioSet FMA\nVGGish +\nInstrumentDNN\nClip likelihoodsClip likelihoods\nQuantile sampling  \nOpenMIC  \n2018 \nOpenMIC  \n(audio)CFFigure 1 . A multi-label instrument detector ( Instru-\nmentDNN , section 2.2) is trained on AudioSet data. The\nmodel is used to score each 10s clip in FMA by likeli-\nhood of each instrument (section 2.3). Clips are sorted\ninto quantiles for each instrument, then sub-sampled and\nannotated by CrowdFlower workers (section 3).\nand annotating natural images to represent categories (syn-\nonym sets, or synsets ) drawn from the WordNet ontol-\nogy [16], with a goal of having at least 500 positive ex-\namples for each category. Candidate images were selected\nby querying image search engines for each category term,\nand then labels were veriﬁed by crowd-sourced annotation.\nThe label correction and veriﬁcation step was critical at\nthe time, due to the poor accuracy of image search engines\nwhen the dataset was constructed in 2009.\nWe follow a similar strategy here, with a few notable\nmodiﬁcations. Rather than querying the Internet for candi-\ndate samples, we restrict attention to freely available con-\ntent hosted on the Free Music Archive, and speciﬁcally\nthose with explicit Creative Commons licensing. Addi-\ntionally, instead of the WordNet ontology, we use the re-\ncently published AudioSet concept ontology [11], which\nitself derives from WordNet, but is adapted to acoustically\nmeaningful concepts. Using existing AudioSet data, we\nconstruct a multi-instrument estimator and use this model\nto rank the unlabeled FMA data and provide candidates for\nannotation. The remainder of this section describes the en-\ntire process in detail, which is visualized in Figure 1.\n2.1 AudioSet\nAudioSet is a recently released concept ontology and\nhuman-annotated dataset derived from YouTube videos,\nwith the goal of providing a testbed for identifying acoustic\nevents [11]. The ontology consists of 632 classes, repre-\nsented as a lattice-like graph, rather than hierarchical tree\nstructure, i.e.one low-level class may have two distinct\nparents. The annotated dataset consists of at least 100 pos-\nitive examples of 485 classes, distributed (non-uniformly)\nacross nearly 1.8M video clips of 10 seconds (or less)Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 439drawn from YouTube. Similar in spirit to the work pre-\nsented here, AudioSet is motivated by a lack of large-scale\nannotated audio data for scientiﬁc research purposes.\nWhile the AudioSet ontology includes musical instru-\nments, the audio data does not match our requirements for\nan open music instrument sample. The collection is de-\nrived from YouTube videos, for which there are no guar-\nantees on the legality of licensing, sharing, and archiv-\ning the content. Though abstract features are made avail-\nable via a publicly available acoustic model, an inability to\nmake the source content directly accessible has limited the\nvalue of other large collections, such as the Million Song\nDataset [2]. Furthermore, the content is often quite differ-\nent from musical performances, an important characteristic\nat the root of what makes this task both challenging and in-\nteresting: many of the positively labeled examples are solo\nperformances, which makes it difﬁcult to model and evalu-\nate on realistic, highly correlated ensemble performances.\nThat said, AudioSet serves two important functions in\nthis project. It is impractical to annotate the entire FMA\ncollection of more than 100K recordings outright; how-\never, it is also extremely unlikely that one could draw a\nrandom subsample with sufﬁcient representation across a\nnumber of instruments. The occurrence of musical instru-\nments is heavily biased by popularity, such as voice, gui-\ntar, or piano, and this is especially true in the Free Mu-\nsic Archive. Here, we leverage AudioSet to build a multi-\ninstrument estimator that allows us to sub-sample and more\nefﬁciently use annotation resources. For better or worse,\nwe also leverage the previous work in ontology construc-\ntion, while circumventing the important, but difﬁcult, chal-\nlenge of selecting which instruments to consider: here, we\nare limited to only those with enough signal in AudioSet\non which to build a baseline model.\nWe manually identify the classes that correspond to mu-\nsical instruments, resulting in a set of more than 70 relevant\nclasses. For the sake of coverage, they are merged into “in-\nstruments”, e.g.“Acoustic Guitar”, “Electric Guitar”, and\n“Tapping (guitar technique)” become guitar , while “Cello”\nand “Violin” remain distinct. Note that this class resolu-\ntion is intentionally approximate, as the long-term goals\nof this project include iteratively reﬁning these concepts\nas acoustic models improve. We then ﬁlter the 1.8M clips\nin AudioSet to those containing these classes. Unsurpris-\ningly, the distribution skews toward instruments common\nin Western popular music, such as guitar, violin, or drums,\nand we cut this list at 1500 examples. Additionally, we\nrandomly draw 8000 non-musical examples as negative\ninstances for building the instrument model described in\nsection 2.2. In summary, the resulting instrument subset\nconsists of 206K clips, totalling roughly 2M seconds (570\nhours) of annotated content for 23 instruments.2\n2.2 Multi-instrument modeling\nAudioSet offers no licensing guarantees on the source con-\ntent, and there is no approved mechanism for directly ac-\ncessing the audio data. To make the dataset more gener-\n2https://github.com/cosmir/open-mic-dataally useful, the developers of AudioSet have released both\na pre-trained feature embedding model [13] based on the\nVGG architecture for object detection in images [18],3\nand its outputs over the original AudioSet audio signals.4\nThis model, referred to as “VGGish”, produces a 128-\ndimensional feature vector every 0.96 seconds with an\nequal window size, such that adjacent features capture non-\noverlapping context. VGGish features are ZCA-whitened\nand each coefﬁcient is quantized to 8-bits to reduce the\nfootprint of the dataset.\nUsing the sub-sampling process described above, we ﬁl-\ntered the AudioSet features down to those clips relevant\nfor the instrument ontology considered here. The data are\nconditionally partitioned by YouTube ID into training, val-\nidation, and test splits with a 3 : 1 : 1 ratio. We randomly\ngenerate over 200 unique, fully connected deep network ar-\nchitectures and hyper-parameter conﬁgurations, spanning\ndepth (1–8 layers), width (128 to 2048 units, by powers\nof 2), the application of dropout and batch normalization,\ndifferent optimization algorithms (stochastic gradient de-\nscent, RMSProp, and Adam [14]), as well as various pa-\nrameters for each operation. All models are trained for 50\nepochs of the training data, and the parameter checkpoint\nwith the highest macro- F1(class-averaged) score over the\nvalidation data is taken as the best model.\nOverall, we ﬁnd that roughly 15% of the models be-\nhave with statistical equivalence, achieving a mean macro-\nF1score of 0.514 ( \u001b= 0:0095 ) and a micro- F1(item-\naveraged) score of 0.656 ( \u001b= 0:0056 ) on the test partition.\nThe best conﬁguration is determined to be a 7-layer net-\nwork, with widths of [1024;512;256;1024;256;1024;23],\nbatch-normalization on the ﬁrst four layers, and point-\nwise dropout applied to the inputs of the last ﬁve\n[0:0;0:0;0:25;0:125;0:25;0:25;0:5]. The winning model,\nwhich we refer to as InstrumentDNN , is trained with the\nAdam optimizer in Keras for 8 epochs, with a learning rate\nof0:0001 and a\f1of 0.99. For reproducibility, the train-\ning data and trained model are made publicly available in\nthe source repository.\n2.3 FMA clip sampling\nThe VGGish model is applied to each track in FMA,\nand the resulting ZCA features are processed by Instru-\nmentDNN to produce time-varying instrument likelihoods.\nFull tracks are then divided into candidate clips by per-\nforming maximum-likelihood aggregation over 10 second\nwindows with a 4 second hop size. To account for fram-\ning effects, the maximum likelihood of each instrument is\ntaken over the middle 8 seconds, centered on the frame.\nThis produces over 7M clip candidates.\nWe ultimately want an approximately balanced sample\nthat has good positive representation of each instrument\nclass. Therefore the candidate set is sub-sampled by the\nfollowing process. First, we consider the median like-\n3https://github.com/tensorflow/models/tree/\nmaster/research/audioset\n4https://research.google.com/audioset/\ndownload.html440 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018lihood of each class over all candidates, and sort instru-\nments in ascending order, as a proxy for class occurrence\nin the FMA. Then, proceeding from least to most likely\ninstrument class, the clip candidate set is reordered by de-\nscending conditional class likelihood. We randomly se-\nlectedNinstances from the 99th percentile rank of that\nclass, such that no two clips share a source track, i.e.sam-\npled clips are recording-independent. All remaining clip\ncandidates that also share a common recording with any\nsampled are discarded, and the process is repeated for the\nnext instrument. For Kinstruments, the sampling process\nyieldsN\u0002Kclips from distinct tracks. Initially, we set\nN= 1000;K= 23 , but manual inspection of the results\nrevealed three classes that either InstrumentDNN cannot\nreliably detect, are poorly represented in FMA, or both:\nharp ,bagpipes , and harmonica . We removed these classes,\nleavingK= 20 instruments and 20K clips.\n3. CROWD-SOURCED ANNOTATION\nAt this stage, roughly 25M seconds of audio have been\nsub-sampled to 200K, a 105reduction, while rebalancing\nfor instrument occurrence. Strongly labeling a collection\nof this size is still cost-prohibitive, and we must be prag-\nmatic with our annotation efforts. In tackling this chal-\nlenge, one can think of annotation as a sparse, binary ma-\ntrix completion problem where most of the values in the\ninstrument occurrence matrix will be zero. Therefore an-\nnotation effort is best allocated by ﬂattening this matrix\ninto clip-instrument pairs, and prioritizing likely positives.\nFramed this way, our most likely positives are iden-\ntiﬁed by the clip selection process: each instrument has\n1K potential positive examples that must be validated by\nhuman annotators. We would also like to obtain a num-\nber of strong negatives as well, and draw 500 instances\nper class that fall in the bottom 10thlikelihood percentile\nfrom the space of examples contained in OpenMIC-2018.\nInstrument-wise percentile thresholds are computed over\nthe full space of clip candidates. In contrast to positive\nsampling, negative samples are drawn working from most\nto least likely instruments. This is because the most likely\ninstrument categories will have the fewest potential strong\nnegatives. Additionally, random sampling is constrained\nto draw no more than three strong negatives per clip, so as\nto distribute this information across the collection. Finally,\nto capture potential correlations and confusions, all addi-\ntional likelihoods in the 99thpercentile rank of their re-\nspective instrument classes are added to the pool of binary\nquestions for human annotators. This results in 33,250 po-\ntential positive and 10,000 potential negative binary esti-\nmates for human validation, which makes up roughly 10%\nof all possible clip-instrument judgements.\nHaving identiﬁed the questions worth asking, audio an-\nnotation presents unique design challenges around how to\nbest ask these questions of humans. Unlike images, audio\nclips cannot be scanned in parallel by humans, and must\nbe auditioned sequentially. This encourages annotation de-\nsigns that ask several binary questions about the same ex-\nample. Our ﬁrst attempt to annotate OpenMIC-2018 took\nFigure 2 . An example annotation task, showing the Mel-\nspectrogram visualization, playback, response ﬁeld, and li-\ncensing meta-data.\nthis approach, but we found that annotators struggled with\nthe increased burden of simultaneously judging multiple\ninstrument tags. This resulted in poor agreement, unhappy\nannotators, and an increased level of effort and skill to\ncomplete. Our second attempt used 20 separate annota-\ntion tasks, one per instrument, and annotators were asked\nto determine the presence or absence of a speciﬁc instru-\nment across multiple recordings.\nAnnotation was performed on the CrowdFlower5plat-\nform (CF). In contrast to Amazon Mechanical Turk, CF\nprovides quality controls on sets of questions, collectively\ncalled a “job”. A single contributor can provide at most 50\nresponses (or 10% of the job, whichever is larger), and a\nquestion is ﬁnalized when annotators reach a set agreement\nlevel and number of responses.\nAdditionally, CF makes it easy to include control ques-\ntions for which an answer is already known. These are\nused to “quiz” contributors before they can perform any\n(paid) work on a job, and remove contributors whose accu-\nracy drops below a threshold, e.g.70%. It is important that\ncontrol questions use clear, unambiguous examples. While\nthese can be easily identiﬁed for popular classes, it is difﬁ-\ncult in the rare classes, notably mandolin andclarinet . For\nthese classes, control questions were generated by rank-\ning clips according to the margin between the target instru-\nment’s likelihood and the maximum over other instruments\nfor that clip, which gives preference toward clips where the\ntarget instrument was both present and prominent.\nEach question is a single judgement of an instrument’s\npresence or absence for a given audio clip. As shown in\nFigure 2, we use a radio button interface for the judge-\nment, provide audio playback in the browser, and addition-\nally display an approximately aligned Mel-spectrogram to\nfacilitate the task, inspired by previous audio annotation\nresearch [5]. Finally, we are legally obligated to display\ntrack title, artist, and license information, which may pro-\nvide coincidental information about a given track.\n4. OpenMIC-2018 ANALYSIS\nWe collected over 230K judgements from more than 2,500\nunique contributors across the 20 instrument classes. Fig-\nure 3 summarizes the resulting annotation distributions for\n5http://crowdflower.comProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 4410 250 500 750 1000 1250 1500 1750 2000\n# Clipsaccordion\nbanjo\nbass\ncello\nclarinet\ncymbals\ndrums\nflute\nguitar\nmallet_percussion\nmandolin\norgan\npiano\nsaxophone\nsynthesizer\ntrombone\ntrumpet\nukulele\nviolin\nvoicePresent\nAbsentFigure 3 . Statistics of crowd-sourced annotation for each\ninstrument in OpenMIC-2018.\n0.5 0.6 0.7 0.8 0.9 1.0\nAnnotator agreementaccordion\nbanjo\nbass\ncello\nclarinet\ncymbals\ndrums\nflute\nguitar\nmallet_percussion\nmandolin\norgan\npiano\nsaxophone\nsynthesizer\ntrombone\ntrumpet\nukulele\nviolin\nvoice\nPresent\nAbsent\nFigure 4 . Annotator agreement for each instrument.\neach instrument. For each instrument class, the number\nof conﬁrmed positive and negative clips are plotted sep-\narately. Each class has at least 500 conﬁrmed positives,\nand at least 1500 conﬁrmed positive or negative. Although\nnot every clip is tagged for every instrument, the abun-\ndance of strong negative labels facilitates supervised learn-\ning and strong evaluation. Figure 4 summarizes the inter-\nannotator agreements for each instrument’s presence or ab-\nsence. Some instruments produce more agreement for ab-\nsence than presence ( accordion ,violin ), while the reverse\nis true for others ( synthesizer ). Overall, we observed a high\namount of agreement across all instruments.\nFigure 5 compares InstrumentDNN’s predicted likeli-\nhoods to the annotations for three instrument classes. In-\nstrumentDNN produces a wide range of likelihood values\nonmandolin (ﬁg. 5, left), indicating that the 99th percentile\nlikelihood is well below the threshold for positive detec-\ntion. This is likely due to a combination of model calibra-\ntion errors and poor representation in AudioSet. However,\nthe sampling strategy still produced a large number of val-\nidated positive examples. For more common classes, such\nascymbals (ﬁg. 5, center), there is a clearer distinction be-\ntween the positive and negative selections. For the most\ncommon classes, such as voice (ﬁg. 5, right), the vast ma-\njority of positive selections are validated by the annotatorsas positive, and conversely for the negative selections.\nTo measure the diversity of the annotated subset, ﬁg. 6\ncompares the distribution of genres over both the sample\nand the background population of FMA. While both distri-\nbutions exhibit non-uniform genre distributions, the sam-\nple is fairly representative of FMA. The instrument-based\nsampling does introduce some systematic bias, increasing\nrepresentation of styles with distinctive instrumentation,\nsuch as classical orjazz. This effect can be observed di-\nrectly in ﬁg. 7, which shows the number of clips in each\ngenre that are positively labeled for each instrument. For\nexample, the majority of organ andpiano examples are\ntagged as classical , while synthesizer is drawn primarily\nfrom electronic andexperimental .\n4.1 Experiment: baseline modeling\nTo estimate the expected performance of standard methods\non OpenMIC-2018, we conducted a set of baseline exper-\niments. We trained independent binary classiﬁers for each\ninstrument. We report the accuracy of each of those mod-\nels on 100 splits of the data, randomly selecting 500 test\ninstances, and splitting the resulting training set in 3 folds\nfor hyper-parameter selection. As input representation, we\nuse the mean and standard deviation of VGGish features\nover the clip’s duration.\nWe tested several baseline models, and for simplicity\nreport only the best performing one: a random forest (RF)\nclassiﬁer. The hyper-parameter search is done on the num-\nber of trees (f10;100;1000g) and on the maximum depth\nof the tree (f2;4;8g). We also report the bias point of\neach instrument category, and the performance of Instru-\nmentDNN. This last comparison point gives us a measure\nof how much information is gained by the crowd-sourced\nlabels. This experiment is done with the scikit-learn [17],\nand the code to reproduce will be made available.\nThe results are shown in ﬁg. 8. We see an overall gain\nin accuracy of more than 10 percent point (pp) compared\nto both the bias points and InstrumentDNN. The perfor-\nmance difference can partly be explained by the difference\nin training distributions between RF and InstrumentDNN,\nand because a strong signal can be learned from the dataset.\nThe RF model performance is also more consistent across\ninstruments with only a 20 pp difference between the worst\nand best instrument accuracy, compared to a 34 pp dif-\nference for InstrumentDNN. The gain compared to In-\nstrumentDNN is therefore larger on the more difﬁcult in-\nstruments, such as saxophone, mandolin and ukulele. In\nthat case the crowd-sourced judgments might provide more\nvalue and help build a robust system.\n5. CONCLUSION\nOpenMIC-2018 should prove to be useful for developing\nand evaluating instrument detection models. We note that\nthe dataset is not “complete” in that not every clip has been\nannotated for the presence or absence of every instrument.\nWhile this is true for every instrument dataset—if one con-\nsiders instruments outside its vocabulary—it is usually not442 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20180.0 0.2 0.4 0.6 0.8 1.0\nAudioSet likelihood0.00.20.40.60.81.0CF annotations\nmandolin\n0.0 0.2 0.4 0.6 0.8 1.0\nAudioSet likelihood0.00.20.40.60.81.0CF annotations\ncymbals\n0.0 0.2 0.4 0.6 0.8 1.0\nAudioSet likelihood0.00.20.40.60.81.0CF annotations\nvoiceFigure 5 . The distribution of the initial model likelihood compared to the crowd-sourced annotations for three instruments.\nEach dot represents a clip, the horizontal line indicates the majority vote threshold, and the marginal distributions of model\nlikelihood and crowd agreement are shown as bar plots. Data have been randomly perturbed for clarity of visualization.\n103\n102\n101\nBlues\nClassical\nCountry\nEasy Listening\nElectronic\nExperimental\nFolk\nHip-Hop\nInstrumental\nInternational\nJazz\nOld-Time / Historic\nPop\nRock\nSoul-RnB\nSpokenFMA\nSample\nFigure 6 . The distribution of (top) genres over the selected\nsample clips, and the background population in FMA.\ntaken into consideration as part of the dataset design.\nMore generally, previous datasets have not typically\nbeen designed with a plan for future correction, revi-\nsion, and expansion. We are explicitly planning to ex-\npand and revise the dataset over time, either by additional\ncrowd-sourcing, semi-supervised learning [6], or incre-\nmental evaluation [15]. OpenMIC-2018 will be placed un-\nder version control, archived, and each revision will re-\nceive a unique document object identiﬁer (DOI) via Zen-\nodo.6\nIn addition to supporting corrections and expanded cov-\nerage, we anticipate expanding the vocabulary beyond the\ninitial 20 classes, both in breadth of instrument classes, and\nin depth to provide reﬁnements of classes, such as alto sax-\nophone andtenor saxophone rather than saxophone . Simi-\nlarly, future work could re-use much of the framework de-\nveloped here to annotate the same collection for a variety\nof qualities beyond instrumentation, and facilitate the de-\nvelopment of integrated multi-task models.\nAcknowledgments. B.M. is supported by the Moore-\nSloan Data Science Environment at NYU.\n6http://about.zenodo.org/\nBlues\nClassicalCountry\nEasy ListeningElectronic\nExperimentalFolk\nHip-Hop\nInstrumentalInternationalJazz\nOld-Time / HistoricPopRock\nSoul-RnBSpokenaccordion\nbanjo\nbass\ncello\nclarinet\ncymbals\ndrums\nflute\nguitar\nmallet_percussion\nmandolin\norgan\npiano\nsaxophone\nsynthesizer\ntrombone\ntrumpet\nukulele\nviolin\nvoice04080120160200\nFigure 7 . The distribution of (top) genres over the positive\ncandidate sets for each instrument.\n0.5 0.6 0.7 0.8 0.9\nAccuracyaccordion\nbanjo\nbass\ncello\nclarinet\ncymbals\ndrums\nflute\nguitar\nmandolin\nmallet_percussion\norgan\npiano\nsaxophone\nsynthesizer\ntrombone\ntrumpet\nukulele\nviolin\nvoice\nBaseline\nBias\nInstrumentDNN\nFigure 8 . Accuracy of the Random Forest baseline (red),\nInstrumentDNN (blue), and the dataset bias (black). The\nRandomForest was trained on OpenMIC-2018, while In-\nstrumentDNN was trained on AudioSet.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 4436. REFERENCES\n[1] Giuseppe Bandiera, Oriol Romani Picas, Hiroshi\nTokuda, Wataru Hariya, Koji Oishi, and Xavier Serra.\nGood-sounds. org: A framework to explore goodness\nin instrumental sounds. In Proceedings of the 17th\nInternational Society for Music Information Retrieval\nConference , pages 414–419, 2016.\n[2] Thierry Bertin-Mahieux, Daniel P. W. Ellis, Brian\nWhitman, and Paul Lamere. The Million Song Dataset.\nInProceedings of the 12th International Society for\nMusic Information Retrieval Conference , pages 591–\n596, 2011.\n[3] Rachel M Bittner, Justin Salamon, Mike Tierney,\nMatthias Mauch, Chris Cannam, and Juan Pablo\nBello. MedleyDB: A multitrack dataset for annotation-\nintensive MIR research. In ISMIR , volume 14, pages\n155–160, 2014.\n[4] Juan J Bosch, Jordi Janer, Ferdinand Fuhrmann, and\nPerfecto Herrera. A comparison of sound segregation\ntechniques for predominant instrument recognition in\nmusical audio signals. In ISMIR , pages 559–564, 2012.\n[5] Mark Cartwright, Ayanna Seals, Justin Salamon, Alex\nWilliams, Stefanie Mikloska, Duncan MacConnell,\nE Law, J Bello, and O Nov. Seeing sound: Investigating\nthe effects of visualizations and complexity on crowd-\nsourced audio annotations. Proceedings of the ACM on\nHuman-Computer Interaction , 1(1), 2017.\n[6] Olivier Chapelle, Bernhard Schlkopf, and Alexander\nZien. Semi-Supervised Learning . The MIT Press, 1st\nedition, 2010.\n[7] Creative Commons. About the licenses. 2015.\nhttps://creativecommons.org/about/.\n[8] Micha ¨el Defferrard, Kirell Benzi, Pierre Van-\ndergheynst, and Xavier Bresson. FMA: A dataset\nfor music analysis. In Proceedings of the 18th In-\nternational Society for Music Information Retrieval\nConference, ISMIR 2017, Suzhou, China, October\n23-27, 2017 , pages 316–323, 2017.\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai\nLi, and Li Fei-Fei. Imagenet: A large-scale hierarchi-\ncal image database. In Computer Vision and Pattern\nRecognition, 2009. CVPR 2009. IEEE Conference on ,\npages 248–255. IEEE, 2009.\n[10] Jesse Engel, Cinjon Resnick, Adam Roberts, Sander\nDieleman, Douglas Eck, Karen Simonyan, and Mo-\nhammad Norouzi. Neural audio synthesis of musical\nnotes with wavenet autoencoders. 2017.\n[11] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman,\nAren Jansen, Wade Lawrence, R Channing Moore,\nManoj Plakal, and Marvin Ritter. Audio set: An on-\ntology and human-labeled dataset for audio events. In\nAcoustics, Speech and Signal Processing (ICASSP),2017 IEEE International Conference on , pages 776–\n780. IEEE, 2017.\n[12] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC music database:\nMusic genre database and musical instrument sound\ndatabase. 2003.\n[13] Aren Jansen, Jort F Gemmeke, Daniel PW Ellis,\nXiaofeng Liu, Wade Lawrence, and Dylan Freed-\nman. Large-scale audio event discovery in one mil-\nlion youtube videos. In Acoustics, Speech and Signal\nProcessing (ICASSP), 2017 IEEE International Con-\nference on , pages 786–790. IEEE, 2017.\n[14] Diederik P Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[15] B. McFee, E.J. Humphrey, and J. Urbano. A plan for\nsustainable MIR evaluation. In 17th International So-\nciety for Music Information Retrieval Conference , IS-\nMIR, 2016.\n[16] George A Miller. WordNet: a lexical database for\nenglish. Communications of the ACM , 38(11):39–41,\n1995.\n[17] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duches-\nnay. Scikit-learn: Machine learning in Python. Journal\nof Machine Learning Research , 12:2825–2830, 2011.\n[18] Karen Simonyan and Andrew Zisserman. Very deep\nconvolutional networks for large-scale image recogni-\ntion. In International Conference on Learning Repre-\nsentations , 2016.\n[19] John Thickstun, Zaid Harchaoui, and Sham Kakade.\nLearning features of music from scratch. In Interna-\ntional Conference on Learning Representations , 2017.444 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Frame-level Instrument Recognition by Timbre and Pitch.",
        "author": [
            "Yun-Ning Hung",
            "Yi-Hsuan Yang"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492363",
        "url": "https://doi.org/10.5281/zenodo.1492363",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/55_Paper.pdf",
        "abstract": "Instrument recognition is a fundamental task in music information retrieval, yet little has been done to predict the presence of instruments in multi-instrument music for each time frame. This task is important for not only automatic transcription but also many retrieval problems. In this paper, we use the newly released MusicNet dataset to study this front, by building and evaluating a convolutional neural network for making frame-level instrument prediction. We consider it as a multi-label classification problem for each frame and use frame-level annotations as the supervisory signal in training the network. Moreover, we experiment with different ways to incorporate pitch information to our model, with the premise that doing so informs the model the notes that are active per frame, and also encourages the model to learn relative rates of energy buildup in the harmonic partials of different instruments. Experiments show salient performance improvement over baseline methods. We also report an analysis probing how pitch information helps the instrument prediction task. Code and experiment details can be found at https://biboamy. github.io/instrument-recognition/.",
        "zenodo_id": 1492363,
        "dblp_key": "conf/ismir/HungY18",
        "keywords": [
            "instrument recognition",
            "music information retrieval",
            "multi-instrument music",
            "frame-level instrument prediction",
            "MusicNet dataset",
            "convolutional neural network",
            "multi-label classification",
            "pitch information",
            "energy buildup",
            "harmonic partials"
        ],
        "content": "FRAME-LEVEL INSTRUMENT RECOGNITION BY TIMBRE AND PITCH\nYun-Ning Hung and Yi-Hsuan Yang\nResearch Center for IT Innovation, Academia Sinica, Taipei, Taiwan\nfbiboamy,yangg@citi.sinica.edu.tw\nABSTRACT\nInstrument recognition is a fundamental task in music in-\nformation retrieval, yet little has been done to predict the\npresence of instruments in multi-instrument music for each\ntime frame. This task is important for not only automatic\ntranscription but also many retrieval problems. In this pa-\nper, we use the newly released MusicNet dataset to study\nthis front, by building and evaluating a convolutional neu-\nral network for making frame-level instrument prediction.\nWe consider it as a multi-label classiﬁcation problem for\neach frame and use frame-level annotations as the supervi-\nsory signal in training the network. Moreover, we experi-\nment with different ways to incorporate pitch information\nto our model, with the premise that doing so informs the\nmodel the notes that are active per frame, and also encour-\nages the model to learn relative rates of energy buildup\nin the harmonic partials of different instruments. Exper-\niments show salient performance improvement over base-\nline methods. We also report an analysis probing how pitch\ninformation helps the instrument prediction task. Code and\nexperiment details can be found at https://biboamy.\ngithub.io/instrument-recognition/ .\n1. INTRODUCTION\nProgress in pattern recognition problems usually depends\nhighly on the availability of high-quality labeled data for\nmodel training. For example, in computer vision, the re-\nlease of the ImageNet dataset [11], along with advances in\nalgorithms for training deep neural networks [26], has fu-\neled signiﬁcant progress in image-level object recognition.\nThe subsequent availability of other datasets, such as the\nCOCO dataset [30], provide bounding boxes or even pixel-\nlevel annotations of objects that appear in an image, facil-\nitating research on localizing objects in an image, seman-\ntic segmentation, and instance segmentation [30]. Such a\nmove from image-level to pixel-level prediction opens up\nmany new exciting applications in computer vision [16].\nAnalogously, for many music-related applications, it is\ndesirable to have not only clip-level but also frame-level\npredictions. For example, expert users such as music com-\nposers may want to search for music with certain attributes\nc\rYun-Ning Hung and Yi-Hsuan Yang. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Yun-Ning Hung and Yi-Hsuan Yang. “Frame-level Instru-\nment Recognition by Timbre and Pitch”, 19th International Society for\nMusic Information Retrieval Conference, Paris, France, 2018.and require a system to return not only a list of songs but\nalso indicate the time intervals of the songs that have those\nattributes [3]. Frame-level predictions of music tags can be\nused for visualization and music understanding [31,45]. In\nautomatic music transcription, we want to know the musi-\ncal notes that are active per frame as well as ﬁgure out the\ninstrument that plays each note [13]. V ocal detection [40]\nand guitar solo detection [36] are another two examples\nthat requires frame-level predictions.\nMany of the aforementioned applications are related\nto the classiﬁcation of sound sources, or instrument clas-\nsiﬁcation. However, as labeling the presence of instru-\nments in multi-instrument music for each time frame is\nlabor-intensive and time-consuming, most existing work\non instrument classiﬁcation uses either datasets of solo in-\nstrument recordings (e.g., the ParisTech dataset [24]), or\ndatasets with only clip- or excerpt-level annotations (e.g.,\nthe IRMAS dataset [7]). While it is still possible to train\na model that performs frame-level instrument prediction\nfrom these datasets, it is difﬁcult to evaluate the result due\nto the absence of frame-level annotations.1As a result, to\ndate little work has been done to speciﬁcally study frame-\nlevel instrument recognition, to the best of our knowledge\n(see Section 2 for a brief literature survey).\nThe goal of this paper is to present such a study, by tak-\ning advantage of a recently released dataset called Music-\nNet [44]. The dataset contains 330 freely-licensed classical\nmusic recordings by 10 composers, written for 11 instru-\nments, along with over 1 million annotated labels indicat-\ning the precise time of each note in every recording and\nthe instrument that plays each note. Using the pitch labels\navailable in this dataset, Thickstun et al. [43] built a con-\nvolutional neural network (CNN) model that establishes\na new state-of-the-art in multi-pitch estimation. We pro-\npose that the frame-level instrument labels provided by the\ndataset also represent a valuable information source. And,\nwe try to realize this potential by using the data to train and\nevaluate a frame-level instrument recognition model.\nSpeciﬁcally, we formulate the problem as a multi-label\nclassiﬁcation problem for each frame and use frame-level\nannotations as the supervisory signal in training a CNN\nmodel with three residual blocks [21]. The model learns\n1Moreover, these datasets may not provide high-quality labeled data\nfor frame-level instrument prediction. To name a few reasons: the Paris-\nTech dataset [24] contains only instrument solos and therefore misses the\ncomplexity seen in multi-instrument music; the IRMAS dataset [7] la-\nbels only the “predominant” instrument(s) rather than all the active in-\nstruments in each excerpt; moreover, an instrument may not be always\nactive throughout an excerpt.135to predict instruments from a spectral representation of au-\ndio signals provided by the constant-Q transform (CQT)\n(see Section 4.1 for details). Moreover, as another tech-\nnical contribution, we investigate several ways to incorpo-\nrate pitch information to the instrument recognition model\n(Sections 4.2), with the premise that doing so informs the\nmodel the notes that are active per frame, and also encour-\nages the model to learn the energy distribution of partials\n(i.e., fundamental frequency and overtones) of different in-\nstruments [2,4,14,15]. We experiment with using either the\nground truth pitch labels from MusicNet, or the pitch esti-\nmates provided by the CNN model of Thickstun et al. [43]\n(which is open-source). Although the use of pitch features\nfor music classiﬁcation is not new, to our knowledge few\nattempts have been made to jointly consider timbre and\npitch features in a deep neural network model. We present\nin Section 5 the experimental results and analyze whether\nand how pitch-aware models outperform baseline models\nthat take only CQT as the input.\n2. RELATED WORK\nA great many approaches have been proposed for (clip-\nlevel) instrument recognition. Traditional approaches used\ndomain knowledge to engineer audio feature extraction al-\ngorithms and fed the features to classiﬁers such as support\nvector machine [25, 32]. For example, Diment et al. [12]\ncombined Mel-frequency cepstral coefﬁcients (MFCCs)\nand phase-related features and trained a Gaussian mix-\nture model. Using the instrument solo recordings from the\nRWC dataset [17], they achieved 96.0%, 84.9%, 70.7% ac-\ncuracy in classifying 4, 9, 22 instruments, respectively. Yu\net al. [47] used sparse coding for feature extraction and\nsupport vector machine for classiﬁer training, obtaining\n96% accuracy in 10-instrument classiﬁcation for the solo\nrecordings in the ParisTech dataset [24]. Recently, Yip and\nBittner [46] made open-source a solo instrument classiﬁer\nthat uses MFCCs in tandem with random forests to achieve\n96% frame-level test accuracy in 18-instrument classiﬁca-\ntion using solo recordings from the MedleyDB multi-track\ndataset [5]. Recognizing instruments in multi-instrument\nmusic has been proven more challenging. For example, Yu\net al. [47] achieved 66% F-score in 11-instrument recogni-\ntion using a subset of the IRMAS dataset [7].\nDeep learning has been increasingly used in more recent\nwork. Deep architectures can “learn” features by training\nthe feature extraction module and the classiﬁcation module\nin an end-to-end manner [26], thereby leading to better ac-\ncuracy than traditional approaches. For example, Li et al.\n[27] showed that feeding raw audio waveforms to a CNN\nachieves 72% (clip-level) F-micro score in discriminating\n11 instruments in MedleyDB, which MFCCs and random\nforest only achieves 64%. Han et al. [19] trained a CNN to\nrecognize predominant instrument in IRMAS and achieved\n60% F-micro, which is about 20% higher than a non-\ndeep learning baseline. Park et al. [35] combined multi-\nresolution recurrence plots and spectrogram with CNN to\nachieved 94% accuracy in 20-instrument classiﬁcation us-\ning the UIOWA solo instrument dataset [18].Number of instru- Number of clips Pitch est.\nments used Train set Test set accuracy\n0 3 0 —\n1 172 5 62.9%\n2 33 1 56.2%\n3 95 4 60.5%\n4 15 0 56.6%\n6 2 0 49.6%\nTable 1 : The number of clips in the training and test sets\nof MusicNet [44], divided according to the number of in-\nstruments used (among the seven instruments we consider\nin our experiment) per clip (e.g., a piano trio uses 3 instru-\nments). We also show the average frame-level multi-pitch\nestimation accuracy (using mir eval [38]) achieved by the\nCNN model proposed by Thickstun et al. [43].\nDue to the lack of frame-level instrument labels in many\nexisting datasets, little work has focused on frame-level in-\nstrument recognition. The work presented by Schl ¨uter for\nvocal detection [40] and by Pati and Lerch for guitar solo\ndetection [36] are exceptions, but they each addressed one\nspeciﬁc instrument, rather than general instruments. Liu\nand Yang [31] proposed to use clip-level annotations in a\nweakly-supervised setting to make frame-level predictions,\nbut the model is for general tags. Moreover, due to the\nassumption that CNN can learn high-level features on its\nown, domain knowledge of music has not been much used\nin prior work on deep learning based instrument recogni-\ntion, though there are some exceptions [33, 37].\nOur work differentiates itself from the prior arts in two\naspects. First, we focus on frame-level instrument recog-\nnition. Second, we explicitly employ the result of multi-\npitch estimation [6, 43] as additional inputs to our CNN\nmodel, with a design that is motivated by the observation\nthat instruments have different pitch range and have unique\nenergy distributions in the partials [14].\n3. DATASET\nTraining and evaluating a model for frame-level instrument\nrecognition is possible due to the recent release of the Mu-\nsicNet dataset [44]. It contains 330 freely-licensed music\nrecordings by 10 composers with over 1 million annotated\npitch and instrument labels on 34 hours of chamber mu-\nsic performances. Following [43], we use the pre-deﬁned\nsplit of training and test sets, leading to 320 and 10 clips\nin the training and test sets, respectively. As there are only\nseven different instruments in the test set, we only con-\nsider the recognition of these seven instruments in our ex-\nperiment. They are Piano, Violin, Viola, Cello, Clarinet,\nBassoon andHorn . For the training set, we do not ex-\nclude the sounds from the instruments that are not on the\nlist, but these instruments are not labeled. Different clips\nuse different number of instruments. See Table 1 for some\nstatistics. For convenience, each clip is divided into 3-\nsecond segments. We use these segments as the input to\nour model. We zero-pad (i.e., adding silence) the last seg-136 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018ment of each clip so that it is also 3 seconds. Due to space\nlimit, for details we refer readers to the MusicNet website\n(check reference [44] for the URL) and also our project\nwebsite (see the abstract for the URL).\nWe note that the MedleyDB dataset [5] can also be used\nfor frame-level instrument recognition, but we choose Mu-\nsicNet for two reasons. First, MusicNet is more than three\ntimes larger than MedleyDB in terms of the total duration\nof the clips. Second, MusicNet has pitch labels for each in-\nstrument, while MedleyDB only annotates the melody line.\nHowever, as MusicNet contains only classical music and\nMedleyDB has more Pop and Rock songs, the two datasets\nfeature fairly different instruments and future work can be\ndone to consider they both.\n4. INSTRUMENT RECOGNITION METHOD\n4.1 Basic Network Architectures that Uses CQT\nTo capture the timbral characteristics of each instrument,\nin our basic model we use CQT as the feature represen-\ntation of music audio. CQT is a spectrographic represen-\ntation that has a musically and perceptual motivated fre-\nquency scale [41]. We compute CQT by librosa [34],\nwith sampling rate 44,100 and 512-sample window size.\n88 frequency notes are extracted with 12 bins per octave,\nwhich forms a matrix X2R258\u000288as the input data, for\neach inputting 3-second audio segment.\nWe experiment with two baseline models. The ﬁrst\none is adapted from the CNN model proposed by Liu and\nYang [31], which has been shown effective for music auto-\ntagging. Instead of using 6 feature maps as the input to the\nmodel as they did, we just use CQT as the input. Moreover,\nwe use frame-level annotations as the supervisory signal\nin training the network, instead of training the model in a\nweakly-supervised fashion as they did. A batch normaliza-\ntion layer [23] is added after each convolution layer. Figure\n1a shows the model architecture.\nThe second one is adapted from a more recent CNN\nmodel proposed by Chou et al. [10], which has been shown\neffective for large-scale sound event detection. Its design\nis special in two aspects. First, it uses 1D convolutions\n(along time) instead of 2D convolutions. While 2D con-\nvolutions analyze the input data as a chunk and convolve\non both spectral and temporal dimensions, the 1D convolu-\ntions (along time) might better capture frequency and tim-\nbral information in each time frame [10, 29]. Second, it\nuses the so-called residual (Res) blocks [21,22] to help the\nmodel learn deeper. Speciﬁcally, we employ three Res-\nblocks in between an early convolutional layer and a late\nconvolutional layer. Each Res-block has three convolu-\ntional layers, so the network has a stack of 11 convolutional\nlayers in total. We expect such a deep structure can learn\nwell for a large-scale dataset such as MusicNet. Figure 1b\nshows its model architecture.\n4.2 Adding Pitch\nAlthough people usually expect neural networks can learn\nhigh-level feature such as pitch, onset and melody, our pi-lot study shows that with the basic architecture the network\nstill confuses some instruments (e.g., clarinet, bassoon and\nhorn), and that onset frames for each instrument are not\nnicely located (see the second row of Figure 3). We pro-\npose to remedy this with a pitch-aware model that explic-\nitly takes pitch as input, in a hope that doing so can amplify\nonset and timbre information. We experiment with several\nmethods for inviting pitch to join the model.\n4.2.1 Source of Frame-level Pitch Labels\nWe consider two ways of getting pitch labels in our ex-\nperiment. One is using human-labeled ground truth pitch\nlabels provided by MusicNet. However, in real-word appli-\ncations, it is hard to get 100% correct pitch labels. Hence,\nwe also use pitch estimation predicted by a state-of-the-\nart multi-pitch estimator proposed by Thickstun et al. [43].\nThe author proposed a translation-invariant network which\ncombines traditional ﬁlterbank with a convolutional neu-\nral network. The model shares parameters in the log-\nfrequency domain, which exploits the frequency invariance\nof music to reduce the number of model parameters and to\navoid overﬁtting to the training data. The model reaches\nthe top performance in the 2017 MIREX Multiple Funda-\nmental Frequency Estimation evaluation [1]. The average\npitch estimation accuracy, evaluated using mir eval [38], is\nshown in Table 1.\n4.2.2 Harmonic Series Feature\nFigure 1c depicts the architecture of a proposed pitch-\naware model. In this model, we aim to exploit the observa-\ntion that the energy distribution of the partials constitutes\na key factor in the perception of instrument timbre [14].\nBeing motivated by [6], we propose the harmonic series\nfeature (HSF) to capture the harmonic structure of music\nnotes, calculated as follows. We are given the input pitch\nestimate (or ground truth) P02R258\u000288, which is a ma-\ntrix with the same size as the CQT matrix. The entries in\nP0take the value of either 0 or 1 in the case of ground truth\npitch labels, and the value in [0;1]in the case of estimated\npitches. If the value of an entry is close to 1, we know\nthat likely a music note with the fundamental frequency is\nactive on that time frame.\nFirst, we construct a harmonic map that shifts the active\nentries in P0upwards by a multiple of the corresponding\nfundamental frequency ( f0). That is, the (t;f)-th entry in\nthe resulting harmonic map Pn2R258\u000288is nonzero only\nif that frequency is (n+ 1) times larger than an active f0\nthat frame, i.e., f=f0\u0001(n+ 1) .\nThen, a harmonic series feature up to the (n+1)-th har-\nmonics,2denoted as Hn2R258\u000288, is computed by an\nelement-wise sum of P0,P1,:::up toPn, as illustrated in\nFigure 1c. In that follows, we also refer to Hnas HSF–n.\nWhen using HSF– nas input to the instrument recog-\nnition model, we concatenate CQT XandHnalong the\nchannel dimension, to the effect that emphasizing the par-\ntials in the input audio. The resulting matrix is then used as\nthe input to a CNN model depicted in Figure 1c. The CNN\n2We note that the ﬁrst harmonic is the fundamental frequency.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 137(a) Baseline CNN [31]\n (b) CNN + ResBlocks [10]\n (c) Pitch-aware model (CQT+HSF)\nFigure 1 : Three kinds of model structure used in this instrument recognition experiment.\nmodel used here is also adapted from [10], using 1D con-\nvolutions, ResBlocks, and 11 convolutional layers in total.\nWe call this model ‘ CQT+HSF–n’ hereafter.\n4.2.3 Other Ways of Using Pitch\nWe consider another two methods to use pitch information.\nFirst, instead of stressing the overtones, the matrix P0\nalready contains information regarding which pitches are\nactive per time frame. This information can be impor-\ntant because different instruments (e.g., violin, viola and\ncello) have different pitch ranges. Therefore, a simple way\nof taking pitch information into account is to concatenate\nP0with the input CQT Xalong the frequency dimension\n(which is ﬁne since we use 1D convolutions), leading to a\n258\u0002176matrix, and then feed it to the early convolu-\ntional layer. This method exploits pitch information right\nfrom the beginning of the feature learning process. We call\nit the ‘ CQT+pitch (F) ’ method for short.\nSecond, we can also concatenate P0with the input CQT\nXalong the channel dimension, to allow the pitch informa-\ntion to directly inﬂuence the input CQT X. It can tell us the\npitch note and onset timing, which is critical in instrument\nrecognition. We call this method ‘ CQT+pitch (C) ’.\n4.3 Implementation Details\nAll the networks are trained using stochastic gradient de-\nscend (SGD) with momentum 0.9. The initial learning rate\nis set to 0.01. The weighted cross entropy, as deﬁned be-\nlow, is used as the cost function for model training:\nln=\u0000yn[tn\u0001log\u001b(^yn)+(1\u0000yn)\u0001log(1\u0000\u001b(^yn))];(1)\nwhereynand^ynare the ground truth and predicted la-\nbel for the n-th instrument per time frame, \u001b(\u0001)is the\nsigmoid function to reduce the scale of ^ynto[0;1], and\nwnis a weight computed to emphasize positive labels andcounter class imbalance between the instruments, based on\nthe trick proposed in [39]. Code and model are built with\nthe deep learning framework PyTorch.\nDue to the ﬁnal sigmoid layer, the output of the instru-\nment recognition model is a continuous value in [0;1]for\neach instrument per frame, which can be interpreted as the\nlikelihood of the presence for each instrument. To decide\nthe existence of an instrument, we need to pick a threshold\nto binarize the result. Simply setting the threshold to 0.5\nequally for all the instruments may not work well. Accord-\ningly, we implement a simple threshold picking algorithm\nthat selects the threshold (from 0.01, 0.02, :::to 0.99, in\ntotal 99 candidates) per instrument by maximizing the F1-\nscore on the training set.\nF1-score is the harmonic mean of precision and recall.\nIn our experiments, we compute the F1-score indepen-\ndently (by concatenating the result for all the segments) for\neach instrument and then report the average result across\ninstruments as the performance metric.\nWe do not implement any smoothing algorithm to post-\nprocess the recognition result, though this may help [28].\n5. PERFORMANCE STUDY\nThe evaluation result is shown in Table 2. We ﬁrst examine\nthe result between two models without pitch information.\nFrom the ﬁrst and second rows, we see that adding Res-\nblocks indeed leads to a more accurate model. Therefore,\nwe also use Res-blocks for the pitch-aware models.\nWe then examine the result when we use ground truth\npitch labels to inform the model. From the upper half of\nTable 2, pitch-aware models (i.e., CQT +HSF) indeed out-\nperform the models that only use CQT. While the CQT-\nonly model based on [10] attains 0.887 average F1-score,\nthe best model CQT +HSF-3 reaches 0.933. Salient im-\nprovement is found for Viola ,Clarinet , and Bassoon .138 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018PitchMethod Piano Violin Viola Cello Clarinet Bassoon Horn Avg.source\nnoneCQT only (based on [31]) 0.972 0.934 0.798 0.909 0.854 0.816 0.770 0.865\nCQT only (based on [10]) 0.982 0.956 0.830 0.933 0.894 0.822 0.789 0.887\nCQT+HSF–1 0.999 0.986 0.916 0.972 0.945 0.909 0.776 0.929\ngroud- CQT+HSF–2 0.997 0.984 0.912 0.968 0.941 0.906 0.799 0.930\ntruth CQT+HSF–3 0.997 0.985 0.914 0.971 0.944 0.907 0.810 0.933\npitch CQT+HSF–4 0.997 0.986 0.909 0.969 0.944 0.904 0.815 0.932\nCQT+HSF–5 0.998 0.975 0.902 0.968 0.942 0.912 0.803 0.928\nCQT+HSF–1 0.983 0.955 0.841 0.935 0.901 0.822 0.793 0.890\nCQT+HSF–2 0.983 0.954 0.830 0.933 0.899 0.820 0.800 0.889\nestimated CQT+HSF–3 0.983 0.955 0.829 0.934 0.903 0.818 0.805 0.890\npitch CQT+HSF–4 0.981 0.955 0.833 0.937 0.903 0.831 0.793 0.890\nby [43] CQT+HSF–5 0.984 0.956 0.835 0.935 0.915 0.839 0.805 0.896\nCQT+Pitch (F) 0.983 0.955 0.829 0.936 0.887 0.819 0.791 0.886\nCQT+Pitch (C) 0.982 0.958 0.819 0.921 0.898 0.827 0.794 0.886\nTable 2 : Recognition accuracy (in F1-score) of model with and without pitch information, using either ground truth pitches\nor estimated pitches. We use bold font to highlight the best result per instrument for the three groups of results.\nFigure 2 : Harmonic spectrum of Viola (top left), Violin\n(top right), Bassoon (bottom left) and Horn (bottom right),\ncreated by the software Audacity [42] for real-life record-\nings of instruments playing a single note.\nMoreover, a comparison among the pitch-aware models\nshows that different instruments seem to prefer different\nnumbers of harmonics n.Horn andBassoon achieve best\nF1-score with larger n(i.e., using more partials), while Vi-\nolaandCello achieves best F1-score with smaller n(us-\ning less partials). This is possibly because string instru-\nments have similar amplitudes for the ﬁrst ﬁve overtones,\nas Figure 2 exempliﬁes. Therefore, when more overtones\nare emphasized, it may be hard for the model to detect\nthose trivial difference, and this in turn causes confusion\nbetween similar string instruments. In contrast, there is\nsalient difference in the amplitudes of the ﬁrst ﬁve over-\ntones for Horn andBassoon , making HSF–5 effective.\nFigure 3 shows qualitative result demonstrating the pre-\ndiction result for four clips in the test set. By comparing the\nresult of the ﬁrst two rows and the last row, we see that on-\nset frames are clearly identiﬁed by the HSF-based model.Furthermore, when adding HSF, it seems easier for a model\nto distinguish between similar instruments (e.g., violin ver-\nsus viola). These examples show that adding HSF helps the\nmodel learn onset and timbre information.\nNext, we examine the result when we use pitch esti-\nmation provided by the model of Thickstun et al. [43].\nWe know already from Table 1 that multi-pitch estimation\nis not perfect. Accordingly, as shown in the last part of\nTable 2, the performance of the pitch-aware models de-\ngrades, though still better than the model without pitch\ninformation. The best result is obtained by CQT +HSF–\n5, reaching 0.896 average F1-score. Except for Violin ,\nCQT+HSF–5 outperforms CQT-only for all the instru-\nments. We see salient improvement for Viola ,Clarinet ,\nBassoon andHorn , for which the CQT-only model per-\nforms relatively worse. This shows that HSF helps high-\nlight differences in the spectral patterns of the instruments.\nBesides, similar to the case when using ground truth\npitch labels, when using the estimated pitches, we see that\nViola still prefers using fewer harmonic maps, whereas\nBassoon andHorn prefer more. Given the observation that\ndifferent instruments prefer different number of harmon-\nics, it may be interesting to design an automatic way to dy-\nnamically decide the number of harmonic maps per frame,\nto further improve the result.\nThe fourth row of Figure 3 gives some result for CQT +\nHSF–5 based on estimated pitches. Compared to the re-\nsult of CQT only (second row), we see that CQT +HSF–5\nnicely reduces the confusion between Violin andViola for\nthe solo violin piece, and reinforces the onset timing for\nthe string quartet piece.\nMoving forward, we examine the result of the other two\npitch-based methods, CQT+Pitch (F) and CQT+Pitch (C),\nusing again estimated pitches. From the last two rows of\nTable 2, we see that these two methods do not perform bet-\nter than even the second CQT-only baseline. As these two\npitch-based methods take the pitch estimates directly as\nthe model input, we conjecture that they are more sensitiveProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 139Figure 3 : Prediction results of different methods for four test clips. The ﬁrst row shows the ground truth frame-level\ninstrument labels, where the horizontal axis denotes time. The other rows show the frame-level instrument recognition\nresult for a model that only uses CQT (‘CQT only’; based on [10]) and three pitch-aware models that use either ground\ntruth or estimated pitches. We use black shade to indicate the instrument(s) that are considered active in the labels or in the\nrecognition result in each time frame.\nFigure 4 : Frame-level instrument recognition result for a\npop song, Make You Feel My Love by Adele, using the\nbaseline CNN [31] (top), CNN + Res-blocks [10] (middle)\nand CQT +HSF–5 using estimated pitches (bottom).\nto errors in multi-pitch estimation and accordingly cannot\nperform well. From the recognition result of the string\nquartet clip in the third row of Figure 3, we see that the\nCQT+Pitch (F) method cannot distinguish between similar\ninstruments such as Violin andViola . This suggests that\nHSF might be a better way to exploit pitch information.\nFinally, out of curiosity, we test our models on a famous\npop music (despite that our models are trained on classical\nmusic). Figure 4 shows the prediction result for the song\nMake You Feel My Love by Adele. It is encouraging to seethat our models correctly detect the Piano used throughout\nthe song and the string instruments used in the middle solo\npart. Moreover, they correctly give almost zero estimate\nfor the wind and brass instruments. Moreover, when us-\ning the Res-blocks, the prediction errors on clarinet are re-\nduced. When using the pitch-aware model, the prediction\nerrors on Violin andCello at the beginning of the song are\nreduced. Besides, Piano timbre can also be strengthened\nwhen Piano and the strings play together at the bridge.\n6. CONCLUSION\nIn this paper, we have proposed several methods for frame-\nlevel instrument recognition. Using CQT as the input fea-\nture, our model can achieve 88.7% average F1-score for\nrecognizing seven instruments in the MusicNet dataset.\nEven better result can be obtained by the proposed pitch-\naware models. Among the proposed methods, the HSF-\nbased models achieve the best result, with average F1-\nscore 89.6% and 93.3% respectively when using estimated\nand ground truth pitch information.\nIn future work, we will include MedleyDB to our train-\ning set to cover more instruments and music genres. We\nalso like to explore joint learning frameworks and recur-\nrent models (e.g., [8, 9, 20]) for better accuracy.\n7. ACKNOWLEDGEMENT\nThis work was funded by a project with KKBOX Inc.\n8. REFERENCES\n[1] MIREX multiple fundamental frequency esti-\nmation evaluation result, 2017. [Online] http:140 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018//www.music-ir.org/mirex/wiki/2017:\nMultiple_Fundamental_Frequency_\nEstimation_%26_Tracking_Results_\n-_MIREX_Dataset .\n[2] Giulio Agostini, Maurizio Longari, and Emanuele Pol-\nlastri. Musical instrument timbres classiﬁcation with\nspectral features. EURASIP Journal on Applied Signal\nProcessing , 1:5–14, 2003.\n[3] Kristina Andersen and Peter Knees. Conversations\nwith expert users in music retrieval and research chal-\nlenges for creative MIR. In Proc. Int. Soc. Music Infor-\nmation Retrieval Conf. , pages 122–128, 2016.\n[4] Jayme Garcia Arnal Barbedo and George Tzanetakis.\nMusical instrument classiﬁcation using individual par-\ntials. IEEE Trans. Audio, Speech, and Language Pro-\ncessing , 19(1):111–122, 2011.\n[5] Rachel Bittner, Justin Salamon, Mike Tierney,\nMatthias Mauch, Chris Cannam, and Juan Bello1.\nMedleyDB: A multitrack dataset for annotation-\nintensive MIR research. In Proc. Int. Soc. Music In-\nformation Retrieval Conf. , 2014. [Online] http://\nmedleydb.weebly.com/ .\n[6] Rachel M. Bittner, Brian McFee, Justin Salamon, Peter\nLi, and Juan P. Bello. Deep salience representations for\nf0estimation in polyphonic music. In Proc. Int. Soc.\nMusic Information Retrieval Conf. , pages 63–70, 2017.\n[7] Juan J. Bosch, Jordi Janer, Ferdinand Fuhrmann,\nand Perfecto Herrera. A comparison of sound\nsegregation techniques for predominant instrument\nrecognition in musical audio signals. In Proc.\nInt. Soc. Music Information Retrieval Conf. , pages\n559–564, 2012. [Online] http://mtg.upf.edu/\ndownload/datasets/irmas/ .\n[8] Ning Chen and Shijun Wang. High-level music de-\nscriptor extraction algorithm based on combination of\nmulti-channel CNNs and LSTM. In Proc. Int. Soc. Mu-\nsic Information Retrieval Conf. , pages 509–514, 2017.\n[9] Keunwoo Choi, Gyorgy Fazekas, Mark Sandler, and\nKyunghyun Cho. Convolutional recurrent neural net-\nworks for music classiﬁcation. In Proc. IEEE Int. Conf.\nAcoustics, Speech, and Signal Processing , 2017.\n[10] Szu-Yu Chou, Jyh-Shing Jang, and Yi-Hsuan Yang.\nLearning to recognize transient sound events using at-\ntentional supervision. In Proc. Int. Joint Conf. Artiﬁcial\nIntelligence , 2018.\n[11] Jia Deng et al. ImageNet: A Large-Scale Hierarchical\nImage Database. In Proc. Conf. Computer Vision and\nPattern Recognition , 2009.\n[12] Aleksandr Diment, Padmanabhan Rajan, Toni Heittola,\nand Tuomas Virtanen. Modiﬁed group delay feature\nfor musical instrument recognition. In Proc. Int. Symp.\nComputer Music Multidisciplinary Research , 2013.[13] Zhiyao Duan, Jinyu Han, and Bryan Pardo. Multi-pitch\nstreaming of harmonic sound mixtures. IEEE/ACM\nTrans. Audio, Speech, and Language Processing ,\n22(1):138–150, 2014.\n[14] Zhiyao Duan, Yungang Zhang, Changshui Zhang,\nand Zhenwei Shi. Unsupervised single-channel music\nsource separation by average harmonic structure mod-\neling. IEEE Trans. Audio, Speech, and Language Pro-\ncessing , 16(4):766 – 778, 2008.\n[15] Slim Essid, Ga ¨el Richard, and Bertrand David. Mu-\nsical instrument recognition by pairwise classiﬁcation\nstrategies. IEEE Trans. Audio, Speech, and Language\nProcessing , 14(4):1401–1412, 2006.\n[16] Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu\nOprea, Victor Villena-Martinez, and Jos ´e Garc ´ıa\nRodr ´ıguez. A review on deep learning techniques\napplied to semantic segmentation. arXiv preprint\narXiv:1704.06857 , 2017.\n[17] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC Music Database:\nPopular, classical and jazz music databases. In\nProc. Int. Society of Music Information Re-\ntrieval Conf. , pages 287–288, 2002. [Online]\nhttps://staff.aist.go.jp/m.goto/\nRWC-MDB/rwc-mdb-i.html .\n[18] Matt Hallaron et al. University of Iowa musical in-\nstrument samples. University of Iowa, 1997. [On-\nline] http://theremin.music.uiowa.edu/\nMIS.html .\n[19] Yoonchang Han, Jaehun Kim, and Kyogu Lee. Deep\nconvolutional neural networks for predominant instru-\nment recognition in polyphonic music. IEEE/ACM\nTrans. Audio, Speech, and Language Processing ,\n25(1):208 – 221, 2017.\n[20] Curtis Hawthorne, Erich Elsen adn Jialin Song, Adam\nRoberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev\nOore, and Douglas Eck. Onsets and frames: Dual-\nobjective piano transcription. Proc. Int. Soc. Music In-\nformation Retrieval Conf. , 2018.\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition. In\nProc. IEEE Int. Conf. Computer Vision and Pattern\nRecognition , 2016.\n[22] Shawn Hershey et al. CNN architectures for large-scale\naudio classiﬁcation. In Proc. IEEE Int. Conf. Acoustics,\nSpeech and Signal Processing , 2017.\n[23] Sergey Ioffe and Christian Szegedy. Batch normaliza-\ntion: Accelerating deep network training by reducing\ninternal covariate shift. In Proc. Int. Conf. Machine\nLearning , pages 448–456, 2015.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 141[24] Cyril Joder, Slim Essid, and Ga ¨el Richard. Tempo-\nral integration for audio classiﬁcation with application\nto musical instrument classiﬁcation. IEEE Trans. Au-\ndio, Speech and Language Processing , 17(1):174–186,\n2009.\n[25] Tetsuro Kitahara, Masataka Goto, Kazunori Komatani,\nTetsuya Ogata, and Hiroshi G. Okuno. Instrument\nidentiﬁcation in polyphonic music: Feature weighting\nwith mixed sounds, pitch-dependent timbre modeling,\nand use of musical context. In Proc. Int. Soc. Music\nInformation Retrieval Conf. , pages 558–563, 2005.\n[26] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.\nDeep learning. Nature , 521(7553):436–444, 2015.\n[27] Peter Li, Jiyuan Qian, and Tian Wang. Auto-\nmatic instrument recognition in polyphonic music\nusing convolutional neural networks. arXiv preprint\narXiv:1511.05520 , 2015.\n[28] Dawen Liang, Matthew D. Hoffman, and Gautham J.\nMysore. A generative product-of-ﬁlters model of au-\ndio. In Proc. Int. Conf. Learning Representations ,\n2014.\n[29] Hyungui Lim, Jeongsoo Park, Kyogu Lee, and Yoon-\nchang Han. Rare sound event detection using 1D con-\nvolutional recurrent neural networks. In Proc. Int.\nWorkshop on Detection and Classiﬁcation of Acoustic\nScenes and Events , 2017.\n[30] Tsung-Yi Lin et al. Microsoft COCO: Common objects\nin context. In Proc. European Conf. Computer Vision ,\npages 740–755, 2014.\n[31] Jen-Yu Liu and Yi-Hsuan Yang. Event localization in\nmusic auto-tagging. Proc. ACM Int. Conf. Multimedia ,\npages 1048–1057, 2016.\n[32] Arie Livshin and Xavier Rodet. The signiﬁcance of the\nnon-harmonic “noise” versus the harmonic series for\nmusical instrument recognition. In Proc. Int. Soc. Mu-\nsic Information Retrieval Conf. , 2006.\n[33] Vincent Lostanlen and Carmine-Emanuele Cella. Deep\nconvolutional networks on the pitch spiral for musical\ninstrument recognition. In Proc. Int. Soc. Music Infor-\nmation Retrieval Conf. , pages 612–618, 2016.\n[34] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW\nEllis, Matt McVicar, Eric Battenberg, and Oriol Ni-\neto. librosa: Audio and music signal analysis in\npython. In Proc. Python in Science Conf. , pages 18–\n25, 2015. [Online] https://librosa.github.\nio/librosa/ .\n[35] Taejin Park and Taejin Lee. Musical instrument sound\nclassiﬁcation with deep convolutional neural net-\nwork using feature fusion approach. arXiv preprint\narXiv:1512.07370 , 2015.[36] Kumar Ashis Pati and Alexander Lerch. A dataset and\nmethod for electric guitar solo detection in rock music.\nInProc. Audio Engineering Soc. Conf. , 2017.\n[37] Jordi Pons, Thomas Lidy, and Xavier Serra. Experi-\nmenting with musically motivated convolutional neu-\nral networks. In Proc. Int. Workshop on Content-based\nMultimedia Indexing , 2016.\n[38] Colin Raffel, Brian Mcfee, Eric J. Humphrey, Justin\nSalamon, Oriol Nieto, Dawen Liang, and Daniel P. W.\nEllis. mir eval: a transparent implementation of com-\nmon mir metrics. In Proc. Int. Soc. Music Information\nRetrieval Conf. , 2014. [Online] https://github.\ncom/craffel/mir_eval .\n[39] Rif A. Saurous et al. The story of audioset, 2017.\n[Online] http://www.cs.tut.fi/sgn/\narg/dcase2017/documents/workshop_\npresentations/the_story_of_audioset.\npdf.\n[40] Jan Schl ¨uter. Learning to pinpoint singing voice from\nweakly labeled examples. In Proc. Int. Soc. Music In-\nformation Retrieval Conf. , 2016.\n[41] Christian Schoerkhuber and Anssi Klapuri. Constant-\nQ transform toolbox for music processing. In Proc.\nSound and Music Computing Conf. , 2010.\n[42] Audacity Team. Audacity. https://www.\naudacityteam.org/ , 1999-2018.\n[43] John Thickstun, Zaid Harchaoui, Dean P. Foster, and\nSham M. Kakade. Invariances and data augmentation\nfor supervised music transcription. In Proc. IEEE Int.\nConf. Acoustics, Speech, and Signal Processing , 2018.\n[Online] https://github.com/jthickstun/\nthickstun2018invariances .\n[44] John Thickstun, Zaid Harchaoui, and Sham M.\nKakade. Learning features of music from scratch. In\nProc. Int. Conf. Learning Representations , 2017. [On-\nline] https://homes.cs.washington.edu/\n˜thickstn/musicnet.html .\n[45] Ju-Chiang Wang, Hsin-Min Wang, and Shyh-Kang\nJeng. Playing with tagging: A real-time tagging mu-\nsic player. In Proc. IEEE Int. Conf. Acoustics, Speech\nand Signal Processing , pages 77–80, 2014.\n[46] Hanna Yip and Rachel M. Bittner. An accurate open-\nsource solo musical instrument classiﬁer. In Proc. Int.\nSoc. Music Information Retrieval Conf., Late-Breaking\nDemo Paper , 2017.\n[47] Li-Fan Yu, Li Su, and Yi-Hsuan Yang. Sparse cepstral\ncodes and power scale for instrument identiﬁcation. In\nProc. IEEE Int. Conf. Acoustics, Speech and Signal\nProcessing , pages 7460–7464, 2014.142 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "A Timbre-based Approach to Estimate Key Velocity from Polyphonic Piano Recordings.",
        "author": [
            "Dasaem Jeong",
            "Taegyun Kwon",
            "Juhan Nam"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492359",
        "url": "https://doi.org/10.5281/zenodo.1492359",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/196_Paper.pdf",
        "abstract": "Estimating the key velocity of each note from polyphonic piano music is a highly challenging task. Previous work addressed the problem by estimating note intensity using a polyphonic note model. However, they are limited because the note intensity is vulnerable to various factors in a recording environment. In this paper, we propose a novel method to estimate the key velocity focusing on timbre change which is another cue associated with the key velocity. To this end, we separate individual notes of polyphonic piano music using non-negative matrix factorization (NMF) and feed them into a neural network that is trained to discriminate the timbre change according to the key velocity. Combining the note intensity from the separated notes with the statistics of the neural network prediction, the proposed method estimates the key velocity in the dimension of MIDI note velocity. The evaluation on Saarland Music Data and the MAPS dataset shows promising results in terms of robustness to changes in the recording environment.",
        "zenodo_id": 1492359,
        "dblp_key": "conf/ismir/JeongKN18",
        "keywords": [
            "polyphonic piano music",
            "note intensity",
            "timbre change",
            "key velocity",
            "non-negative matrix factorization",
            "neural network",
            "MIDI note velocity",
            "Saarland Music Data",
            "MAPS dataset",
            "robustness"
        ],
        "content": "A TIMBRE-BASED APPROACH TO ESTIMATE KEY VELOCITY FROM\nPOLYPHONIC PIANO RECORDINGS\nDasaem Jeong, Taegyun Kwon, Juhan Nam\nGraduate School of Culture Technology, KAIST, Korea\nfjdasam, ilcobo2, juhannam g@kaist.ac.kr\nABSTRACT\nEstimating the key velocity of each note from poly-\nphonic piano music is a highly challenging task. Previous\nwork addressed the problem by estimating note intensity\nusing a polyphonic note model. However, they are limited\nbecause the note intensity is vulnerable to various factors\nin a recording environment. In this paper, we propose a\nnovel method to estimate the key velocity focusing on tim-\nbre change which is another cue associated with the key\nvelocity. To this end, we separate individual notes of poly-\nphonic piano music using non-negative matrix factoriza-\ntion (NMF) and feed them into a neural network that is\ntrained to discriminate the timbre change according to the\nkey velocity. Combining the note intensity from the sepa-\nrated notes with the statistics of the neural network predic-\ntion, the proposed method estimates the key velocity in the\ndimension of MIDI note velocity. The evaluation on Saar-\nland Music Data and the MAPS dataset shows promising\nresults in terms of robustness to changes in the recording\nenvironment.\n1. INTRODUCTION\nPolyphonic piano transcription is one of the most active\nresearch topics in automatic music transcription [1]. How-\never, the absolute majority of piano transcription algo-\nrithms so far have been concerned with detecting the pres-\nence of notes in term of pitch (or note number), onset\nand duration, while ignoring note dynamics, which is ex-\npressed by key velocity on piano.\nAlong with tempo, dynamics is a key feature that pro-\nduces a musical “motion” [19]. Previous studies on piano\nperformance analysis employed dynamics as one of two\nmain features of performance characteristics in [22, 25].\nAnother study showed that, if dynamics is estimated for\nindividual notes, a ﬁner analysis is achievable [21].\nThere have been a few works that challenged the task of\nestimating individual note dynamics. To best of our knowl-\nedge, the ﬁrst attempt was made by Ewert and M ¨uller\nwho tackled the problem using a parametric model of\nc\rDasaem Jeong, Taegyun Kwon, Juhan Nam. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC\nBY 4.0). Attribution: Dasaem Jeong, Taegyun Kwon, Juhan Nam. “A\nTimbre-based Approach to Estimate Key Velocity from Polyphonic Piano\nRecordings”, 19th International Society for Music Information Retrieval\nConference, Paris, France, 2018.polyphonic piano notes [7]. Our previous work estimated\nthe note intensity using score-informed non-negative ma-\ntrix factorization (NMF) in various training strategies [15].\nSzeto and Wong used a sinusoidal model to separate chords\ntones into individual piano tones and estimated the note in-\ntensity as part of the source separation task [23].\nAll of them basically estimate individual note dynamics\naccording to energy magnitude or loudness of the notes.\nHowever, this approach has an essential limitation in that\na note produced by a certain key velocity can be recorded\nin different sound levels depending on the recording con-\nditions. For example, a pianissimo note can be recorded\nloudly or a forte note can be quietly, depending on the in-\nput gain of the recording device or the distance from the\nmicrophone.\nIn this paper, we challenged to overcome this limita-\ntion by focusing on differences in timbral characteristics\ncaused by the key velocity. According to previous research,\nloudness and tone of a piano note are uniquely determined\nby the velocity of the hammer at the time it strikes the\nstrings [12]. This implies that the key velocity can be in-\nferred not only from the loudness but also from the tim-\nbre of the note, assuming that the hammer velocity can be\napproximated by the key velocity. This idea was explored\nin [14] where a piano note shows different timbral char-\nacteristics such as a spectral envelope or inharmonicity,\ndepending on the key velocity. While the previous work\nfocused on single notes, we study it for polyphonic music.\nThe proposed system consists of three parts: an NMF\nmodule for note separation and intensity estimation, a neu-\nral network to discriminate key velocity, and intensity-to-\nvelocity calibration using the results from the two mod-\nules. The NMF module is based on score-informed settings\nfrom [15] and [24]. After the decomposition of the audio\nspectrogram, we reproduce the note-separated spectrogram\nfrom the NMF module. The neural network takes the note-\nseparated spectrogram as input and estimates its key ve-\nlocity. The third part obtains proper mapping parameters\nbetween note intensity and key velocity using the distribu-\ntion of velocity estimation from the neural network, and\nﬁnally estimate individual key velocity in the dimension\nof MIDI note velocity. We evaluate the proposed method\non Saarland Music Data and the MAPS dataset and show\npromising results in terms of robustness to changes in the\nrecording environment.\nThe rest of paper is structured as follows. In Section 2\nwe introduce the scope of our work and deﬁne the terms120that represent dynamics of a piano note. Section 3 summa-\nrizes the related works. In Section 4 we explain the NMF\nand neural network framework. The experiment and result\nare explained in Section 5 and 6. Finally, the conclusion is\npresented in Section 7.\n2. BACKGROUND\nTo provide better understanding of the task and scope in\nthis research, we ﬁrst review key terms and deﬁne the prob-\nlem that we attempt to solve.\n2.1 Term Deﬁnitions\nNote intensity is the term that represents the magnitude of\nacoustical energy of a note. It can be deﬁned as sound-\npressure level (SPL) [10] or the sum of spectral energy as\nin [7, 15]. Since the intensity is an acoustical feature, it is\nhighly variable by the recording condition. For example,\nnote intensity can be changed by simple post-processing\nsuch as gain adjustment. Therefore, the intensity of each\nnote is comparable only when the recording conditions are\nconsistent.\nKey velocity refers to the kinetic velocity of the piano\nkey and it is closely connected to the hammer velocity. It\ncan be measured by detecting the elapsed time when the\nhammer shank passes two ﬁxed points [10]. Unlike the\nnote intensity, the key velocity is a feature measured di-\nrectly from the mechanical movement, hence independent\nfrom the acoustic recording environment. If the recording\ncondition is constant and the sympathetic resonance is ig-\nnored, the mapping between key velocity and note inten-\nsity for each pitch is linear [10].\nMIDI velocity is the term that represents the key ve-\nlocity in the MIDI format. It is a one-byte integer value be-\ntween 0 and 127 inclusive in the note messages. Computer-\ncontrolled pianos or MIDI-compatible keyboards have\ntheir own mapping of key velocity to MIDI velocity.\n2.2 Problem Deﬁnition\nThe aim of this study lies in estimating note key velocity\nin terms of MIDI velocity. Although our previous work at-\ntempted to produce the result in MIDI velocity, the method\nrequires an additional data for intensity-to-velocity calibra-\ntion with the same piano and recording condition [15]. In\na real-world situation, however, it is almost impossible to\nobtain such mapping for a target recording. Instead of em-\nploying a target-suited training set, our work aims to learn\na proper intensity-to-velocity mapping directly from a tar-\nget audio recording.\nOne of the obstacles in the task is that most datasets\nrepresent the key velocity with MIDI velocity and the map-\nping between the two varies depending on the piano or key-\nboard model. To focus on the relation between timbre and\nkey velocity in this study, we ﬁx the key-to-MIDI velocity\nmapping by employing only one piano model but differ-\nent recording conditions during the evaluation. However,\nwe evaluate the trained model on recordings with a dif-ferent piano to see how it generalizes. The details will be\nexplained in the evaluation section.\n3. RELATED WORKS\nOur proposed method is based on the NMF framework\nfrom [15] but expand it by employing a recent work by\nWang et al [24]. One of the main limitations in the NMF\nframework is that it is difﬁcult to model the timbre changes\nover time. For example, the NMF model used in [8] and\n[15] assumes the spectral template of each pitch does not\nchange over time. To overcome this limitation, Wang et\nalsuggested using multiple spectral templates per pitch in\nNMF for piano modeling. This NMF model was adopted in\nour proposed system and will be discussed in more detail\nin the next section.\nIdentifying key velocity by its timbre can be compared\nto identiﬁcation of musical instruments. The earlier works\nused various hand-crafted audio features [6, 14]. Recently,\ndeep neural network has become a popular solution for this\ntask [2, 11], which takes spectrograms or mel-frequency\ncepstral coefﬁcients as input. There are a few work inter-\nested in timbral difference by the velocity [4, 14] but they\ndid not aim to distinguish these difference explicitly.\nOur task can also be compared to instrument identiﬁ-\ncation in polyphonic audio. One of typical solutions for\nthis task is using source separation and then handling it\nas monophonic audio sources. Heittola et al. suggested\na framework with NMF-based source separation module\n[13]. Similar to this work, our method also employs NMF-\nbased source separation. But we use the neural networks\ninstead of the Gaussian mixture model to identify the sep-\narated sources.\n4. METHOD\nOur proposed system consists of three parts as shown in the\nFigure 1. The ﬁrst part is score-informed NMF that factor-\nizes the spectrogram of audio recording into note-separated\nspectrogram for every note in the score. This also returns\nthe intensity of each note. The second part is neural net-\nwork (NN) that takes the note-separated spectrogram and\nestimates the key velocity. The third part is intensity-to-\nvelocity calibration which is conducted by comparing the\nestimated velocity from the NN module and the intensity\nfrom the NMF module on their distributions.\n4.1 Note Separation\nThe ﬁrst part of our framework is based on NMF, a matrix\nfactorization for non-negative data which is usually spec-\ntrogram in audio processing domain.\nLet us denote a given spectrogram as V2RF\u0002T\n\u00150, where\nF is the number of frequency bins and T is the number of\ntime frames. With NMF, the spectrogram can be factorized\ninto multiplication of two matrices W2RF\u0002(P\u0001R)\n\u00150 and\nH2R(P\u0001R)\u0002T\n\u00150 where P denotes the number of pitch in\nsemitone and R denotes the number of spectral basis perProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 121Figure 1 . A diagram of the proposed system.\npitch. By doing so we can decompose the input spectro-\ngram with spectral templates bases Wand the activation of\nthe bases over time H.\nTo clarify the relationship between spectral basis and\npitch, we will follow the similar notation presented in\n[24], denoting Wf;p;r :=Wf;(p\u00001)\u0001R+rand Hp;r;t :=\nH(p\u00001)\u0001R+r;tas below:\nVft=X\np;rWf;p;rHp;r;t (1)\nwheref2[1;F],t2[1;T],p2[1;P], andr2[1;R]\nare index of frequency bin, time frame, pitch, and spectral\nbasis in a pitch, respectively.\n4.1.1 NMF Modeling\nWe employ an NMF model that learns multiple time-\nfrequency patterns instead of single spectral templates\n[24], which was applied to the score-informed AMT task.\nThis model captures various timbre of the same pitch and\ntemporal evolution of timbre, which is a necessary part of\nour task. Since the main contribution of our paper lies on\nthe velocity estimation by combining of the NMF and NN\nresults, the following section will mainly explain several\ndifferences in the implementation. The details are found\nin [24].\nConsidering that an NMF model can be conﬁgured\nmainly by the number of basis, initialization method, and\nadditional constraints with corresponding update rules,\nWang et al. ’s model for piano recording [24] is different\nfrom the previous models used in [8,9,15] in three aspects.\nFirst, they suggested multi-basis per pitch so that each\npitch hasRnumber of corresponding bases. The previous\nmodels represent a piano note by the combination of per-\ncussive (onset) and harmonic (sustain) basis for the whole\nnote duration. Since there is only one harmonic basis for\neach pitch, the spectral shape of the note does not change\nover time. This assumes that the most important timbre fea-\nture is constant in the sustain part within the single note\nas well as for different key velocities. But the multi-basis\nmodel can handle this subtle change of timbre by using\nmultiple bases with different activation ratios.\nSecond, employing the multi-basis model requires a dif-\nferent initialization method for matrix WandH. To modeltemporal progression of piano timbre, the r-th basis was\ninitialized to be active after the (r\u00001)-th basis of the\nsame pitch. Since the pitch bases are activated sequentially,\nthey can model temporal evolution of the note tone. As the\npitch bases are differed by their activation initialization,\nthey also have different spectral characteristics. Among R\nbases of a pitch, the ﬁrst basis handles percussive element\nand the the second to the last represent harmonic elements\nin the temporal order. In addition, the harmonic area is set\nto be tapered as the rank index rincreases. This makes the\nearlier bases include more inharmonicity.\nThird, Wang et al.’s model suggested several additional\ncosts for the multi-basis model. They include a soft con-\nstraint, temporal continuity, and energy decay in the tem-\nplate matrix. Among the suggested costs, we did not em-\nploy the decaying cost for W, which encourages smooth\ndecrease of energy in spectral templates in W. We found\nthat our system works better with L1 normalized Wso that\nthe magnitude feature is assigned only to H. We followed\nthe NMF costs and update function strictly except that we\nignore the decaying cost term by assign 0 to \f3.\nFor better intensity estimation, we previously suggested\nusing power spectrogram, instead of linear magnitude\nspectrogram [15]. We also showed that using synthesized\nmonophonic scale tones helps to learn spectral template.\nBased on this observation, our system also uses power\nspectrogram and synthesized piano scale. Another differ-\nence with [24] is post-updating of H. After the update con-\nverges, we set all constraints on Hto zeros and update H\nfor ten times with ﬁxed Wso that our ﬁnal reproduction\ncan resemble the original gain.\nThe NMF module reproduces note-separated spectro-\ngram ˆV(n)for each note nin the score by multiplying\nthe spectral bases of note’s pitch and its activation over\nnote’s duration. The note intensity is deﬁned as the max-\nimum activation of ˆV(n), which can be represented as\nmax(P\nfˆV(n)\nft). Then, we reproduce ˆV(n)again around the\ntime frame of the maximum activation and store it for the\ninput for the neural network. This helps to ﬁx the size of\nNN’s input and maintain the relative position of each ele-\nment in the cropped spectrogram.122 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 2 . Comparison of the intensity-normalized note-\nseparated spectrogram with different MIDI velocities.\nThe spectrogram was reproduced from polyphonic piano\nrecording (SMD). The MIDI note number is 50 and the\nMIDI velocities were 14 and 95, respectively.\n4.2 Velocity Estimation\nThe neural network (NN) model takes the note-separated\nspectrograms from the NMF module as input and estimates\nthe velocity of each note. The note-separated spectrogram\nis converted to a log-frequency spectrogram before it is\nused for the input of the NN module. The frequency res-\nolution is set to 25 cent and the frequency range is from\n27.5 Hz (the lowest pitch of piano) to 16.7 kHz (two oc-\ntave higher than the highest pitch of piano), resulting in\n445 frequency bins. After some preliminary test, we used\n14 frames as input size. The spectrogram magnitude is nor-\nmalized by the maximum value so that every entry in the\nspectrogram lies between 0 and 1 the as shown in the Fig-\nure 2.\nThe neural network consists of 5 fully-connected hid-\nden layers and each layer has 256 nodes. Every hidden\nlayer uses SELUs as an activation function [17]. Applying\nSELUs aims to stabilize the network from internal covari-\nance shifting without any additional complexity.\nThe loss function is set to mean square error of key\nvelocity estimation, approaching the task as a regression\nproblem. We also attempted to use softmax as a classiﬁ-\ncation problem but the result was slightly worse. We used\nAdam optimization [16] with initial learning rate of 1e-4,\nand early stopping on the validation set.\n4.3 Intensity-to-Velocity Calibration\nThe NN module provides an absolute degree of note dy-\nnamics but the relative magnitude between each note from\nthe NMF results is more stable than that from the NN re-\nsults. Therefore, we combine the two results to ﬁnd better\nestimation.\nAs described in Section 1, intensity is affected by both\nkey velocity and recording condition. One cannot distin-\nguish whether the high intensity from the NMF is caused\nby strong strike of hammer or high gain in the recording\ndevice. Therefore each recording condition needs its own\nmapping parameter.Also, the intensity-velocity relation depends on a piano\nor a keyboard model [3]. Our previous study showed that\nthe MIDI velocity of a note can be approximated by a lin-\near relationship with the log value of the intensity Int (n),\nso that Vel (n) =a\u0001log(Int(n))+bfor the Disklavier, which\nwe use for the evaluation [15]. However, we need to know\nintensity-paired velocity in the target recording condition,\nwhich is not available in real-word recordings.\nOur solution is estimating it from the overall velocity\ndistribution of each piece from the NN module. If we as-\nsume the outcome velocity has a distribution with mean\n\u0016Vand standard deviation \u001bVfor each piece, we can ob-\ntain the mapping parameters by comparing it with the dis-\ntribution of log of intensity, \u0016log(I)and\u001blog(I). Then, the\nmapping parameter aandbcorrespond to \u001bV=\u001blog(I)and\n\u0016V\u0000(\u001bV=\u001blog(I))\u0016log(I), respectively, with the assumption\nthat every note has the same mapping parameters. Note\nthat this neglects the note-speciﬁc difference of intensity-\nto-velocity mapping parameter. The error caused by this\nassumption will be also explained in Section 6.\nOur system takes the result of the NN module to es-\ntimate\u0016Vand\u001bVfor each piece. The estimation can be\nalso done by a simple global setting. During the evalua-\ntion, we used this scheme as a baseline to compare with\nour NN model.\n5. EXPERIMENT\n5.1 Experiment I: SMD\nWe used Saarland Music Dataset (SMD) MIDI-Audio Pi-\nano Music [18] for the evaluation. The dataset consists of\nﬁfty pairs of audio and MIDI recordings of performance\non Yamaha Disklavier DCFIIISM4PRO. The MIDI ﬁles of\nSMD contain every movement of piano key and pedal in\nhigh reliability, thus providing the ground truth of note dy-\nnamics in MIDI velocity.\nThe previous work pointed out that the recording condi-\ntion of each piece in SMD is differed by its recording date\n[15]. Therefore, the intensity-to-velocity mapping had to\nbe obtained separately for each subset of pieces that share\nthe same recording condition. The difference in intensity-\nto-velocity mapping in SMD is represented in Figure 3.\nSince the goal of the proposed system is to estimate key\nvelocity robustly against changes in the recording environ-\nment, such different recording conditions are ideal for eval-\nuating this task.\nWe evaluate whether the proposed system can handle\ndifferent recording conditions and estimate correct velocity\ndistributions. We used ﬁfteen pieces recorded in the year of\n2011 as a test set, and other thirty-ﬁve pieces as a training\nset, which was recorded during the year of 2008 and 2010.\nTo evaluate the exact performance and usefulness of the\nNN module, we also present two upper boundary models\nand a baseline model. The ﬁrst upper boundary assumes\nthat the system obtained proper mapping parameters for\nevery individual pitch from other pieces in the same test\nset, as in [15]. The second upper boundary assumes that\nour NN module guessed correct estimation of velocity dis-Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 123Figure 3 . The difference in velocity-intensity mapping be-\ntween two subsets from SMD. Each point represents a sin-\ngle note with MIDI note number 50. The notes recorded in\n2009 show higher intensity compared to the notes recorded\nin 2011 given the same velocity.\ntribution. In this upper boundary, we employed the ground\ntruth of velocity distribution for each piece. The baseline\nis using global mean and standard deviation values. Based\non the statistics of training set, we used \u0016V= 57:87and\n\u001bV= 16:25.\nThe evaluation measure is an absolute error of velocity\nbetween ground truth and estimated value. In MIDI veloc-\nity dimension, absolute error is a more meaningful crite-\nrion than relative error because MIDI velocity is already\na logarithm of the intensity. We used the average of ab-\nsolute velocity error in a piece, which can be represented\nas Err =PN\nnjVGT(n)\u0000VEst(n)j=N, whereVGT(n)and\nVEst(n)are ground truth velocity and estimated velocity of\nthen-th note in a piece, respectively.\n5.2 Experiment II: MAPS\nWe also evaluate our NN module on unseen data to see\nwhether the NN can learn generalized piano timbre from\nthe training set. To this end, we designed another exper-\niment with the MAPS database [5], which was recorded\nwith a different piano and recording conditions.\nFrom the MAPS dataset, we used two subsets per-\nformed by Yamaha Disklavier Mark III (upright) that con-\nsists of 30 recordings. One subset is recorded as “ambient”\nand the other is recorded as “close” condition. We did not\nuse other MAPS dataset for training our NN module. The\nmodel trained from thirty-ﬁve pieces of SMD was used for\nthis test.\nIn this experiment, the evaluation is made only with the\nestimated distribution from the NN module \u0016nnand\u001bnn\nand ground truth \u0016gtand\u001bgt. Since the mapping between\nkey velocity and MIDI velocity in SMD and the MAPS\ndataset is different, we cannot compare these values di-\nrectly. Also, we cannot ﬁgure out how the same key veloc-\nity will be recorded as MIDI velocity in SMD and MAPS\nor which velocity value will make most close reproduction\nof a note in MAPS with the instrument in SMD. What we\ncan assure is that MIDI velocity ranking of notes or piece\nwill be preserved both in SMD and MAPS. Therefore weexamine the Spearman correlation between the NN’s guess\n\u0016nnand\u001bnnand the ground truth MAPS MIDI value \u0016gt\nand\u001bgt.\n5.3 Procedure\nThe experiment procedure is as follows. First, the NMF\nmodule calculates note intensity and reproduces note-\nseparated spectrograms for each pieces in the training set\nand test set. Then, we train the NN module with the note\nspectrograms of the training set from SMD. After the train-\ning, the trained NN estimates the velocity of note spectro-\ngrams of the test set. Combining the distribution of esti-\nmated velocity from the NN and estimated intensity from\nthe NMF as described in section 4.3, we can obtain ﬁnal\nMIDI velocity for each note in the piece. For the Experi-\nment II, the calibration part is omitted. During the exper-\niment, we used STFT with window size 8192, hop size\n2048, and 8 spectral bases per pitch in the NMF module.\n6. RESULTS\n6.1 Experiment I: SMD\nWe present our result on the SMD set recorded in 2011\non Table 1. The ground truth velocity distribution of each\npiece is represented as GT, and the estimated distribution\nfrom the NN module is as NN. The remaining columns on\nthe right are the average errors of four different mapping\nparameter for the same NMF result. UB1 is the ﬁrst upper\nboundary that uses other test pieces to obtain the velocity-\nto-intensity mapping as in [15]. UB2 is the second upper\nboundary that assumes our NN module estimated the cor-\nrect\u0016Vand\u001bV. The proposed method (Prop.) is from the\nNN estimation for \u0016Vand\u001bV. The baseline (Base) always\nguessed\u0016V= 57:87and\u001bV= 16:25. The last column\nshows the error when we directly used the NN estimation\nin note level, instead of combining it with the NMF inten-\nsity.\nThe estimation of the NN module showed high error in\na note level as shown in the NN column. We presume the\nreason for the error is mainly based on the imperfection of\nsource-separation. Also, the different recording condition\nin the test set could make not only intensity difference but\nalso timbral change. This inhomogeneity may also have\nhad a negative impact on the performance of the NN mod-\nule.\nEven though the note-level accuracy was not reliable,\nwe found that the overall distribution of the estimated ve-\nlocity resembles the distribution of ground truth velocity\nas we expected. By employing the estimated velocity dis-\ntribution, the note intensity from the NMF module could\nbe successfully mapped into MIDI velocity as shown in the\nProp. column. The proposed system outperforms the base-\nline estimation in most pieces. While the ﬁxed guess ig-\nnored characteristic of each piece, the NN module success-\nfully estimated a correct distribution from the note spectro-\ngrams.\nThe difference between two upper boundary UB1 and\nUB2 shows the error caused by the assumption that the124 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Composers PieceGround Truth NN Estimation UB1 UB2 Proposed Baseline NN note\nMean STD Mean STD Err Err Err Err Err\nBach BWV 888-1 49.7 12.6 53.3 15.5 3.1 3.9 3.3 6.6 10.4\nBach BWV 888-2 63.3 11.3 62.8 13.3 2.1 3.1 3.5 9.0 10.1\nBartok op. 80-1 68.9 18.2 65.7 15.3 5.9 6.6 8.5 15.0 12.2\nBartok op. 80-2 59.5 23.5 59.5 20.4 5.1 7.2 8.6 10.3 11.3\nBartok op. 80-3 67.4 19.0 64.8 17.3 6.0 7.1 8.9 14.8 13.0\nBrahms op. 5-1 64.8 23.5 62.0 19.6 7.2 8.4 10.0 13.1 13.8\nHaydn HobXVI-52-01 57.9 14.6 58.4 14.7 3.9 5.5 4.6 6.1 11.9\nHaydn HobXVI-52-2 49.8 18.6 53.9 16.6 3.8 4.7 5.6 8.0 11.1\nHaydn HobXVI-52-3 60.4 12.9 59.1 15.5 3.6 5.4 5.5 7.6 12.9\nMozart K. 265 57.5 13.2 57.1 14.4 3.2 6.2 6.2 6.7 10.5\nMozart K. 398 58.6 13.2 57.7 16.5 3.6 5.6 8.5 8.6 11.2\nRachmaninoff op. 36-1 56.5 18.7 54.5 16.9 6.4 6.1 6.9 5.9 11.7\nRachmaninoff op. 36-2 54.7 19.5 50.2 18.1 5.2 5.5 6.4 6.9 11.5\nRachmaninoff op. 36-3 66.3 19.8 66.4 16.0 6.6 9.0 8.5 14.7 12.7\nRavel Jeux d’eau 55.3 17.0 57.8 17.6 5.8 5.5 5.0 5.1 12.5\nAverage 4.83 5.9 6.7 9.2 11.8\nTable 1 . The result of experiment on SMD. The ﬁrst two columns show mean and standard deviation of note velocities\nfrom the ground truth and the estimation by neural network. “Err” stands for absolute mean error of note velocities. UB1 is\nan oracle model that learns key-dependent velocity mapping from other test pieces, and UB2 is another oracle model with\nground-truth velocity mean and variance. The baseline model uses a global mean and variance. NN note represents mean\nerror of velocity estimation of individual notes in the neural network\nintensity-to-velocity mapping is consistent over the key.\nHowever, previous works showed that a piano stroke\nmakes different intensity with the same velocity depend-\ning on the key [20]. This suggests the need of additional\nmethods to compensate the key-dependent mapping in the\nfuture research.\nThe error is notable in Rachmaninoff’s Op. 36-1 . A pos-\nsible reason is that the global setting of velocity distribu-\ntion in the baseline is closer to the ground truth compared\nto the NN estimation. The errors in Ravel’s Jeux d’eau\nis worth mentioning since the two upper boundary meth-\nods made the worse result. We presume that the reason is\nthe frequent use of soft pedal during the performance. Soft\npedal makes intensity lower, thus making our system esti-\nmate it softer than what is expected from its MIDI velocity.\n6.2 Experiment II: MAPS\nFigure 4 shows the correlation between the estimation from\nthe NN module and the ground truth on the MAPS record-\nings. The absolute value of \u0016nnand\u0016gthas an error be-\ncause of different key velocity to MIDI velocity mapping,\nthus cannot be compared directly. However, we can see that\nas the ground truth velocity mean of the piece increases,\nthe estimated mean of NN also tends to catch it up. The\nsame tendency is also found in the standard deviation. The\nSpearman correlation between \u0016GTand\u0016NNis 0.838, and\nthat between \u001bGTand\u001bNNis 0.597.\nFigure 4 also shows that the estimation from the NN\nmodule is not affected much by whether the recording is\nambient or close, indicating that our NN module is robust\nto different piano and recording conditions. We did not ap-\nply the baseline method to MAPS because the estimation\nwould be always constant regardless of the piece.\n7. CONCLUSIONS\nWe presented a system that estimates key velocity from\npolyphonic piano recordings. The main limitation of pre-\nFigure 4 . The test result on the MAPS dataset (Experiment\nII). Each point represents a single piece.\nvious work was the lack of method for calibration be-\ntween intensity and key velocity. To overcome the limi-\ntation, We proposed a neural network module that takes\nnote-separated spectrogram and estimates the key velocity\nof each note. Though the accuracy of individual notes is\nnot reliable, the overall distribution resembles the distribu-\ntion of ground truth velocity for each piece. Our system\nobtains a proper intensity-to-velocity mapping by employ-\ning the estimated velocity distribution, and then estimate\nthe key velocity.\nWe evaluated our system on two different datasets.\nOverall, the evaluation showed a promising result of this\ntimbre-based approach. The velocity estimation from the\nNN module showed a similar distribution with the ground\ntruth velocity distribution despite the different recording\nconditions. Employing this estimated distribution, our sys-\ntem mapped note intensity to MIDI velocity reliably. Also,\nthe result showed that our NN module learns robust fea-\ntures that can be applied to unseen data.\nFor the future work, we plan to apply our solution to\nreal-world recordings with various timbre and recording\nconditions and, by combining other AMT and audio-to-\nscore alignment algorithms, and obtain more full-ﬂedged\nperformance transcription.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 1258. ACKNOWLEDGEMENTS\nThis research was supported/partially supported by Sam-\nsung Research Funding & Incubation Center for Future\nResearch.\n9. REFERENCES\n[1] Emmanouil Benetos, Simon Dixon, Dimitrios Gian-\nnoulis, Holger Kirchhoff, and Anssi Klapuri. Auto-\nmatic music transcription: challenges and future di-\nrections. Journal of Intelligent Information Systems ,\n41(3):407–434, 2013.\n[2] D. G. Bhalke, C. B. Rama Rao, and D. S. Bormane. Au-\ntomatic musical instrument classiﬁcation using frac-\ntional fourier transform based- MFCC features and\ncounter propagation neural network. Journal of Intel-\nligent Information Systems , 46(3):425–446, Jun 2016.\n[3] Roger B Dannenberg. The interpretation of MIDI ve-\nlocity. In Proc. of International Computer Music Con-\nference (ICMC) , pages 193–196, 1996.\n[4] Patrick Joseph Donnelly et al. Learning spectral ﬁl-\nters for single-and multi-label classiﬁcation of musi-\ncal instruments . PhD thesis, Montana State University-\nBozeman, College of Engineering, 2015.\n[5] Valentin Emiya, Roland Badeau, and Bertrand David.\nMultipitch estimation of piano sounds using a new\nprobabilistic spectral smoothness principle. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 18(6):1643–1654, 2010.\n[6] Antti Eronen and Anssi Klapuri. Musical instrument\nrecognition using cepstral coefﬁcients and temporal\nfeatures. In Proc. of IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\npages II753–II756, 2000.\n[7] Sebastian Ewert and Meinard M ¨uller. Estimating note\nintensities in music recordings. In Proc. of 2011 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , pages 385–388, 2011.\n[8] Sebastian Ewert and Meinard M ¨uller. Using score-\ninformed constraints for NMF-based source separa-\ntion. In Proc. of IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\npages 129–132, 2012.\n[9] Sebastian Ewert, Siying Wang, Meinard M ¨uller, and\nM Sandler. Score-informed identiﬁcation of missing\nand extra notes in piano recordings. In Proc. of Inter-\nnational Society of Music Information Retrieval Con-\nference (ISMIR) , pages 30–36, 2016.\n[10] Werner Goebl and Roberto Bresin. Measurement and\nreproduction accuracy of computer-controlled grand\npianos. The Journal of the Acoustical Society of Amer-\nica, 114(4):2273–2283, 2003.[11] Yoonchang Han, Jaehun Kim, and Kyogu Lee. Deep\nconvolutional neural networks for predominant instru-\nment recognition in polyphonic music. IEEE/ACM\nTransactions on Audio, Speech and Language Process-\ning (TASLP) , 25(1):208–221, 2017.\n[12] Harry C Hart, Melville W Fuller, and Walter S Lusby.\nA precision study of piano touch and tone. The Journal\nof the Acoustical Society of America , 6(2):80–94, 1934.\n[13] Toni Heittola, Anssi Klapuri, and Tuomas Virtanen.\nMusical instrument recognition in polyphonic audio\nusing source-ﬁlter model for sound separation. In Proc.\nof International Society for Music Information Re-\ntrieval Conference (ISMIR) , pages 327–332, 2009.\n[14] Kristoffer Jensen. Timbre models of musical sounds .\nPhD thesis, Department of Computer Science, Univer-\nsity of Copenhagen, 1999.\n[15] Dasaem Jeong and Juhan Nam. Note intensity estima-\ntion of piano recordings by score-informed NMF. In\nProc. of Audio Engineering Society Semantic Audio\nConference , 2017.\n[16] Diederik P. Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. Computing Re-\nsearch Repository , abs/1412.6980, 2014.\n[17] G ¨unter Klambauer, Thomas Unterthiner, Andreas\nMayr, and Sepp Hochreiter. Self-normalizing neural\nnetworks. In Advances in Neural Information Process-\ning Systems , pages 972–981, 2017.\n[18] Meinard M ¨uller, Verena Konz, Wolfgang Bogler, and\nVlora Ariﬁ-M ¨uller. Saarland music data (SMD). In\nProc. of the International Society for Music Informa-\ntion Retrieval Conference (ISMIR): Late Breaking ses-\nsion, 2011.\n[19] Bruno H Repp. Music as motion: A synopsis of\nAlexander Truslit’s (1938) Gestaltung und Bewegung\nin der Musik. Psychology of Music , 21(1):48–72, 1993.\n[20] Bruno H Repp. Some empirical observations on sound\nlevel properties of recorded piano tones. The Journal of\nthe Acoustical Society of America , 93(2):1136–1144,\n1993.\n[21] Bruno H Repp. The dynamics of expressive pi-\nano performance: Schumann’s ‘‘Tr ¨aumerei’’revisited.\nThe Journal of the Acoustical Society of America ,\n100(1):641–650, 1996.\n[22] Craig Stuart Sapp. Comparative analysis of multiple\nmusical performances. In Proc. of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 497–500, 2007.\n[23] Wai Man Szeto and Kin Hong Wong. Source separation\nand analysis of piano music signals using instrument-\nspeciﬁc sinusoidal model. In Proc. of 16th Interna-\ntional Conference on Digital Audio Effects (DAFx) ,\n2013.126 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[24] Siying Wang, Sebastian Ewert, and Simon Dixon.\nIdentifying missing and extra notes in piano recordings\nusing score-informed dictionary learning. IEEE/ACM\nTransactions on Audio, Speech, and Language Pro-\ncessing , 25(10):1877–1889, 2017.\n[25] Gerhard Widmer, Simon Dixon, Werner Goebl, Elias\nPampalk, and Asmir Tobudic. In search of the\nHorowitz factor. AI Magazine , 24(3):111, 2003.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 127"
    },
    {
        "title": "Evaluating a Collection of Sound-Tracing Data of Melodic Phrases.",
        "author": [
            "Tejaswinee Kelkar",
            "Udit Roy",
            "Alexander Refsum Jensenius"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492347",
        "url": "https://doi.org/10.5281/zenodo.1492347",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/209_Paper.pdf",
        "abstract": "Melodic contour, the 'shape' of a melody, is a common way to visualize and remember a musical piece. The purpose of this paper is to explore the building blocks of a future 'gesture-based' melody retrieval system. We present a dataset containing 16 melodic phrases from four musical styles and with a large range of contour variability. This is accompanied by full-body motion capture data of 26 participants performing sound-tracing to the melodies. The dataset is analyzed using canonical correlation analysis (CCA), and its neural network variant (Deep CCA), to understand how melodic contours and sound tracings relate to each other. The analyses reveal non-linear relationships between sound and motion. The link between pitch and verticality does not appear strong enough for complex melodies. We also find that descending melodic contours have the least correlation with tracings.",
        "zenodo_id": 1492347,
        "dblp_key": "conf/ismir/KelkarRJ18",
        "keywords": [
            "melodic contour",
            "gesture-based",
            "melody retrieval",
            "dataset",
            "canonical correlation analysis",
            "neural network",
            "pitch and verticality",
            "sound tracings",
            "non-linear relationships",
            "descending melodic contours"
        ],
        "content": "EVALUATING A COLLECTION OF SOUND-TRACING DATA OF\nMELODIC PHRASES\nTejaswinee Kelkar\nRITMO, Dept. of Musicology\nUniversity of Oslo\ntejaswinee.kelkar@imv.uio.noUdit Roy\nIndependent Researcher\nudit.roy@alumni.iiit.ac.inAlexander Refsum Jensenius\nRITMO, Dept. of Musicology\nUniversity of Oslo\na.r.jensenius@imv.uio.no\nABSTRACT\nMelodic contour, the ‘shape’ of a melody, is a common\nway to visualize and remember a musical piece. The pur-\npose of this paper is to explore the building blocks of a fu-\nture ‘gesture-based’ melody retrieval system. We present\na dataset containing 16 melodic phrases from four musi-\ncal styles and with a large range of contour variability.\nThis is accompanied by full-body motion capture data of\n26 participants performing sound-tracing to the melodies.\nThe dataset is analyzed using canonical correlation analy-\nsis (CCA), and its neural network variant (Deep CCA), to\nunderstand how melodic contours and sound tracings re-\nlate to each other. The analyses reveal non-linear relation-\nships between sound and motion. The link between pitch\nand verticality does not appear strong enough for complex\nmelodies. We also ﬁnd that descending melodic contours\nhave the least correlation with tracings.\n1. INTRODUCTION\nCan hand movement be used to retrieve melodies? In this\npaper we use data from a ‘sound-tracing’ experiment (Fig-\nure 1) containing motion capture data to describe music–\nmotion cross-relationships, with the aim of developing a\nretrieval system. Details about the experiment and how\nmotion metaphors come to play a role in the representa-\ntions are presented in [19]. While our earlier analysis was\nfocused on the use of the body and imagining metaphors\nfor tracings [17, 18], in this paper, we will focus on mu-\nsical characteristics and study music–motion correlations.\nThe tracings present a unique opportunity for cross-modal\nretrieval, because a direct correspondence between tracing\nand melodic contour presents an inherent ‘ground-truth.’\nRecent research in neuroscience and psychology has\nshown that action plays an important role in perception. In\nphonology and linguistics, the co-articulation of action and\nsound is also well understood. Theories from embodied\nmusic cognition [22] have been critical to this exploration\nof multimodal correspondences.\nc\rTejaswinee Kelkar, Udit Roy, Alexander Refsum Jense-\nnius. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Tejaswinee Kelkar, Udit Roy,\nAlexander Refsum Jensenius. “Evaluating a collection of Sound-Tracing\nData of Melodic Phrases”, 19th International Society for Music Informa-\ntion Retrieval Conference, Paris, France, 2018.\nFigure 1 . An example of post-processed motion capture\ndata from a sound-tracing study of melodic phrases.\nContour perception is a coarse-level musical ability\nthat we acquire early during childhood [30, 33, 34]. Re-\nsearch suggests that our memory for contour is enhanced\nwhen melodies are tonal, and when tonal accent points of\nmelodies co-occur with strong beats [16], making melodic\nmemory a salient feature in musical perception. More gen-\nerally, it is easier for people to remember the general shape\nof melody rather than precise intervals [14], especially if\nthey are not musical experts. Coarse representations of\nmelodic contour, such as with drawing or moving hands\nin the air may be intuitive to capturing musical moments\nof short time scales [9, 25].\n1.1 Research Questions\nThe inspiration for our work mainly comes from several\nprojects on melodic content retrieval using intuitive and\nmulti-modal representations of musical data. The oldest\nexample of this is the 1975 project titled ‘Directory of\nTunes and Musical Themes,’ where the author uses a sim-\npliﬁed contour notation method, involving letters for de-74noting contour directions, to create a dictionary of musi-\ncal themes where one may look up a tune they remem-\nber [29]. This model is adopted for melodic contour re-\ntrieval in Musipedia.com [15]. Another system is proposed\nin the recent project SoundTracer, in which a user’s mo-\ntion of their mobile phone is used to retrieve tunes from a\nmusic archive [21]. A critical difference between these ap-\nproaches is how they handle mappings between contour in-\nformation and musical information, especially differences\nbetween time-scales and time-representations. Most of\nthese methods do not have ground-truth models of con-\ntours, and instead use one of several ways of mappings,\neach with its own assumptions.\nGodøy et al. has argued for using motion-based, graphi-\ncal, verbal, and other representations of motion data in mu-\nsic retrieval systems [10]. Liem et al. make a case for using\nmultimodal user-centered strategies as a way to navigate\nthe discrepancy between audio similarity and music simi-\nlarity [23], with the former referring to more mathematical\nfeatures, and the latter to more perceptual features. We\nproceed with this as the point of departure for describing\nour dataset and its characteristics, to approach the goal of\nmaking a system for classifying sound-tracings of melodic\nphrases with the following speciﬁc questions:\n1. Are the mappings between melodic contour and mo-\ntion linearly related?\n2. Can we conﬁrm previous ﬁndings regarding correla-\ntion between pitch and the vertical dimension?\n3. What categories of melodic contour are most corre-\nlated for sound-tracing queries?\n2. RELATED WORK\nUnderstanding the close relationship between music and\nmotion is vital to understanding subjective experiences of\nperformers and listeners, [7, 11, 12]. Many empirical ex-\nperiments aimed at investigating music–motion correspon-\ndences deal with stimulus data that is made to explicitly\nobserve certain mappings, for example pitched and non-\npitched sound, vertical dimension and pitch, or player ex-\npertise [5, 20, 27]. This means that the music examples\nthemselves are sorted into types of sound (or types of mo-\ntion). We are more interested in observing how a variety\nof these mapping relationships change in the content of\nmelodic phrases. For this we use multiple labeling strate-\ngies as explained in section 3.4. Another contribution of\nthis work is the use of musical styles from various parts of\nthe world, including those that contain microtonal inﬂec-\ntions.\n2.1 Multi-modal retrieval\nMulti-modal retrieval is the paradigm of information re-\ntrieval used to handle different types of data together. The\nobjective is to learn a set of mapping functions that project\nthe different modalities into a common metric space, to\nbe able to retrieve relevant information in one modalitythrough a query in another. We see that this paradigm is\nused often in the retrieval of image from text and text from\nimage. Canonical Correlation Analysis (CCA) is a com-\nmon tool for investigating linear relationships of two sets\nof variables. In the review paper by Wang et al. for cross\nmodal retrieval [35], several implementations and models\nare analyzed. CCA is also previously used to show music\nand brain imaging cross relationships [3].\nA previous paper analyzing tracings to pitched and\nnon pitched sounds also used CCA to understand music–\nmotion relationships [25], where the authors describe in-\nherent non-linearity in the mappings, despite ﬁnding in-\ntrinsic sound-action relationships. This work was extended\nin [26], in which CCA was used to interpret how different\nfeatures correlate with each other. Pitch and vertical mo-\ntion have linear relationships in this analysis, although it\nis important to note that the sound samples used for this\nstudy were short and synthetic.\nThe biggest reservations in analyzing music–motion\ndata through CCA is that non-linearity cannot be repre-\nsented, and the dependence of the method on time syn-\nchronization is high. The temporal evolution of motion\nand sound remains linear over time [6]. To get around\nthis, kernel-based methods can be used to introduce non-\nlinearity. Ohkushi et al., present a paper that uses Kernel-\nbased CCA methods to analyze motion and music features\ntogether using video sequences from classical ballet, and\noptical ﬂow based clustering. Bozkurt et al. present a CCA\nbased system to analyze and generate speech and arm mo-\ntion for prosody-driven synthesis of the ’beat-gesture’ [4],\nwhich is used for emphasizing prosodically salient points\nin speech. We explore our dataset through CCA due to\nthe previous successes of using this family of methods.\nWe will analyze the same data using Deep CCA, a neural-\nnetwork approximation of CCA, to understand better the\nnon-linear mappings.\n2.2 Canonical Correlation Analysis\nCCA is a statistical method to ﬁnd a linear combina-\ntion of two variables X= (x1;x2;:::;x n)andY=\n(y1;y2;:::;y m)withnandmindependent variables as vec-\ntorsaandbsuch that their correlation \u001a=corr(aX;bY )\nof the transformed variables is maximized. Linear\nvectorsa0andb0can be found such that a0;b0=\nargmax\na;bcorr(aTX;bTY). We can then ﬁnd the second\nset of coefﬁcients which maximize the correlation of the\nvariablesX0=aXandY0=bYwith the additional con-\nstraint to keep (X;X0)and(Y;Y0)uncorrelated. This pro-\ncess can be repeated till d=min(m;n)dimensions.\nThe CCA can be extended to include non-linearity by\nusing a neural network to transform the XandYvariables\nas in the case of Deep CCA [2]. Given the network param-\neters\u00121and\u00122, the objective is to maximize the correla-\ntioncorr(f(X;\u0012 1);f(Y;\u0012 2)). The network is trained by\nfollowing the gradient of the correlation objective as esti-\nmated from the training data.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 753. EXPERIMENT DESCRIPTION\n3.1 Procedure\nThe participants were instructed to move their hands as if\ntheir movement was creating the melody. The use of the\nterm ‘creating,’ instead of ‘representing,’ is purposeful, as\nshown in earlier studies [26,27], to be able to access sound-\nproduction as the tracing intent. The experiment duration\nwas about 10 minutes. All melodies were played at a com-\nfortable listening level through a Genelec 8020 speaker,\nplaced 3m in front of the subjects. Each session consisted\nof an introduction, two example sequences, 32 trials and\na conclusion. Each melody was played twice with a 2s\npause in between. During the ﬁrst presentation, the partic-\nipants were asked to listen to the stimuli, while during the\nsecond presentation, they were asked to trace the melody.\nAll the instructions and required guidelines were recorded\nand played back through the speaker. Their motions are\ntracked using 8 infra-red cameras from Qualisys (7 Oqus\n300 and 1 Oqus 410). We then post-process the data in\nQualisys Track Manager (QTM) ﬁrst by identifying and\nlabeling each marker for each participant. Thereafter, we\ncreate a dataset containing Left and Right hand coordinates\nfor all participants.\nSix participants in the study had to be excluded due to\ntoo many marker dropouts, giving us a ﬁnal dataset con-\ntaining 26 participants tracing 32 melodies: 794 tracings\nfor 16 melodic categories.\n3.2 Subjects\nThe 32 subjects (17 females, 15 males) had a mean age\nof 31 years (SD = 9 years). They were mainly univer-\nsity students and employees, both with and without musi-\ncal training. Their musical experience was quantized using\nthe OMSI (Ollen Musical Sophistication Index) question-\nnaire [28], and they were also asked about the familiarity\nwith the musical genres, and their experience with dancing.\nThe mean of the OMSI score was 694 (SD = 292), indicat-\ning that the general musical proﬁciency in this dataset was\non the higher side. The average familiarity with Western\nclassical music was 4.03 out of a possible 5 points, 3.25 for\njazz music, 1.87 with Sami joik, and 1.71 with Hindustani\nmusic. None of the participants reported having heard any\nof the melodies played to them. All participants provided\ntheir written consent for inclusion before they participated\nin the study, and they were free to withdraw during the ex-\nperiment. The study design was approved by the National\nethics board (NSD).\n3.3 Stimuli\nIn this study, we decided to use melodic phrases from vocal\ngenres that have a tradition of singing without words. V o-\ncal phrases without words were chosen so as to not intro-\nduce lexical meaning as a confounding variable. Leaving\nout instruments also avoids the problem of subjects having\nto choose between different musical layers in their sound-\ntracing. The ﬁnal stimulus set consists of four different\nFigure 2 . Pitch plots of all the 16 melodic phrases used as\nexperiment stimuli, from each genre. The x axis represents\ntime in seconds, and the y axis represents notes. The ex-\ntracted pitches were re-synthesized to create a total of 32\nmelodic phrases used in the experiment.\nmusical genres and four stimuli for each genre. The mu-\nsical genres selected are: (1) Hindustani music, (2) Sami\njoik, (3) jazz scat singing, (4) Western classical vocalise.\nThe melodic fragments are phrases taken from real record-\nings, to retain melodies within their original musical con-\ntext. As can be seen in the pitch plots in Figure 2, the\nmelodies are of varying durations with an average of 4.5 s\n(SD = 1.5 s). The Hindustani and joik phrases are sung by\nmale vocalists, whereas the scat and vocalise phrases are\nsung by female vocalists. This is represented in the pitch\nrange of each phrase as seen in Figure 2.\nSeeger \nSchaeffer \nVarna \nHood xx xy xyy xyx \nImpulsive Iterative Sustained \nAscending Descending Stationary Varying \nArch Bow Tooth Diagonal \nAdams Repetition Recurrence \nFigure 3 . Contour Typologies discussed previously in\nmelodic contour analysis. This ﬁgure is representative,\nmade by the authors.\nMelodic contours are overwhelmingly written about in\nterms of pitch, and so we decided to create a ‘clean’ pitch–\nonly representation of each melody. This was done by\nrunning the sound ﬁles through an autocorrelation algo-\nrithm to create phrases that accurately resemble the pitch\ncontent, but without the vocal, timbral and vowel content\nof the melodic stimulus. These 16 re-synthesized sounds\nwere added to the stimulus set, thus obtaining a total of 32\nsound stimuli.76 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018ID Description\n1All 16 Melodies\n2IJSV 4 Genres\n3ADSC Ascending, Descending,\nSteady or Combined\n4OrigVSyn Original vs Synthesized\n5VibNonVib Vibrato vs No Vibrato\n6MotifNonMotif Motif Repetition Present vs\nNot\nTable 1 . Multiple labellings for melodic categories: we\nrepresent the 16 melodies using 5 different label sets. This\nhelps us analyze which features are best related to which\ncontour classes, genres, or melodic properties.\n3.4 Contour Typology Descriptions\nWe base the selection of melodic excerpts on the descrip-\ntions of melodic contour classes as seen in Figure 3. The\nreference typologies are based on the work of Seeger [32],\nHood [13], Schaeffer [8], Adams [1], and the Hindustani\nclassical Varna system. Through these typologies, we hope\nto cover commonly understood contour shapes and make\nsure that the dataset contains as many of them as possible.\n3.4.1 Multiple labeling\nTo represent the different contour types and categories that\nthese melodies represent, we create multiple labels that ex-\nplain the differences. This enables us to understand how\nthe sound tracings actually map to the different possible\ncategories, and makes it easier to see patterns from the\ndata. We describe these labels as seen in Table 3.4.1. Mul-\ntiple labels allow us to see what categories does the data\ndescribe, and which features or combination of features\ncan help retrieve which labels. Some of these labels are\ncategories, while some are one-versus-rest. Category la-\nbels include individual melodies, genres, and contour cat-\negories, while one-versus-rest correlations are computed\nfor ﬁnding whether vibrato, motivic repetitions exist in the\nmelody, and whether the melodic sample is re-synthesized\nor original.\n4. DATASET CREATION\n4.1 Preprocessing of Motion Data\nWe segment each phrase that is traced by the participants,\nlabel participant and melody numbers, and extract the data\nfor left and right hand markers for this analysis, since the\ninstructions asked people to trace using their hands. To\nanalyze this data, we are more interested in contour fea-\ntures and shape information than time-scales. We therefore\ntime-normalize our datasets so that every melodic sample\nand every motion tracing is the same length. This makes it\neasier to ﬁnd correlations between music and motion data\nusing different features.\nFigure 4 . Feature distribution of melodies for each genre.\nWe make sure that a wide range of variability in the fea-\ntures, as described in Table 2 is present in the dataset.\nFeature Calculated by\n1Pitch Autocorrelation function using\nPRAAT\n2Loudness RMS value of the sound using\nLibrosa\n3Brightness Spectral Centroid using Librosa\n4Number\nof NotesNumber of notes per melody\nTable 2 . Melody features extracted for analysis, and de-\ntails of how they are extracted.\n5. ANALYSIS\n5.1 Music\nSince we are mainly interested in melodic correlations, the\nmost important feature describing melodies is to extract\npitch. For this, we use autocorrelation algorithm avail-\nable in the PRAAT phonetic program. We use Librosa\nv0.5.1 [24] to compute the RMS energy (loudness), and\nthe brightness using Spectral Centroid. We transcribe the\nmelodies to get the number of notes per melody. The dis-\ntribution of these features can be seen for each genre in\nthe stimulus set in Figure 4. We have tried to be true to\nthe musical styles used in this study, most of which do not\nhave written notation as an inherent part of their pedagogy.\n5.2 Motion\nFor tracings, we calculate 9 features that describe vari-\nous characteristics of motion. We record only X and Z\naxes, as maximum motion is found along these directions.\nThe derivatives of motion (velocity, acceleration, jerk) and\nquantity of motion (QoM) which is a cumulative velocity\nquantity are calculated. Distance between hands, cumula-\ntive distance, and symmetry features are calculated as indi-\ncators of contour-supporting features, as found in previous\nstudies.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 77Feature Description\n1X-coordinate (X) Axis corresponding to the\ndirection straight ahead of\nthe participant\n2Z-coordinate (Z) Axis corresponding to the\nupwards direction\n3Velocity (V) First derivative of vertical\nposition\n4Acceleration (A) Second derivative of vertical\nposition\n5Quantity of Mo-\ntionSum of absolute velocities\nfor all markers\n6Distance between\nHandsSample-wise Euclidean dis-\ntance between hand markers\n7Jerk Third derivative of vertical\nposition\n8Cumulative Dis-\ntance TraveledEuclidean distance traveled\nper sample per hand\n9Symmetry Difference between the left\nand right hand in terms of\nvertical position and hori-\nzontal velocity\nTable 3 . Motion features used for analysis. 1-5 are for the\ndominant hand, while 6-9 are features for both hands.\n5.3 Joint Analysis\nIn this section we present our analysis on our dataset with\nthese two feature sets. We analyze the tracings for each\nmelody as well as utilize the multiple label sets to discover\ninteresting patterns in our dataset which are relevant for a\nretrieval application.\n5.3.1 Dynamic Time Warping\nDynamic Time Warping (DTW) is a method to align se-\nquences of different lengths using substitution, addition\nand subtraction costs. It is a non-metric method giving us\nthe distance between two sequences after alignment.\nIn recent research, vertical motion has been shown to\ncorrelate with pitch in the past for simple sounds. Some\nform of non-alignment is also observed between the mo-\ntion and pitch signals. We perform the same analysis on\nour data: compute the correlation between pitch and mo-\ntion in the Z axis before and after alignment with DTW\nfor the 16 melodies and plot their mean and variance in\nFigure 5.\n5.3.2 Longest Run-lengths\nWhile observing the dataset, we ﬁnd that longest ascend-\ning and descending sequences in the melodies are most\noften reliably represented in the motions, although vari-\nances in stationary notes, and ornaments is likely to be\nmuch higher. To exploit this feature in tracings, we use\n“Longest Run-lengths” as a measure. We ﬁnd multiple\nsubsequences following a pattern which can possess dis-\ncriminative qualities. For our analysis, we use the ascend-\ning and descending patterns, thus ﬁnding the subsequences\nFigure 5 . Correlations of pitch with raw data (red) vs after\nDTW-alignment (blue). Although a DTW alignment im-\nproves the correlation, we observe that correlation is still\nlow suggesting that vertical motion and pitch height are\nnot that strongly associated.\nfrom the feature sequence which are purely ascending or\ndescending. We then rank the subsequences and build a\nfeature vector from the lengths of the top Nresults. This\nstep is particularly advantageous when comparing features\nfrom motion and music sequences as it captures the overall\npresence of the pattern in the sequence remaining invariant\nto the mis-alignment or lag between the sequences from\ndifferent modalities. As an example, if we select the Z-\naxis motion of the dominant hand and the melody pitch as\nour sequences and retrieve top 3 ascending subsequence\nlengths. To make the features robust, we do a low pass\nﬁltering of the sequence as a preprocessing step.\nWe analyze our dataset by computing the features for\nfew combinations of motion and music features for ascend-\ning and descending patterns. Thereafter, we perform CCA\nand show the resulting correlation of ﬁrst transformed di-\nmension in Table 4. We utilize the various label categories\ngenerated for the melodies, and show the impact of the fea-\ntures on the labels from each category in Tables 4 and 5.\nWe select the top four run lengths as our feature for each\nmusic–motion feature sequence. For Deep CCA analysis,\nwe use a two layered network (same for both motion and\nmusic features) with 10 and 4 neurons. A ﬁnal round of\nlinear CCA is also performed on the network output.\n6. RESULTS AND DISCUSSION\nFigure 5 shows correlations with raw data and after DTW\nalignment between the vertical motion and pitch for each\nmelody. Overall, the correlation improves after DTW\nalignment, suggesting phase lags and phase differences be-\ntween the timing of melodic peaks and onsets, and those of\nmotion. We see no signiﬁcant differences between genres,\nalthough the improvement in correlations for the vocalize\nexamples is the least pre and post DTW. This could be be-\ncause of the continuous vibrato in these examples, causing\npeople to use more ‘shaky’ representations which are most78 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Motion Music All ADSC IJSV\nAscend Pattern CCA Deep CCA CCA Deep CCA CCA Deep CCA\nZ Pitch 0.19 0.23 0.25 0.16 0.09 0.05 0.24 0.17 0.12 0.13 0.16 -0.13 0.01 0.37 0.19 0.21 0.08 0.36\nZ + V Pitch 0.21 0.27 0.26 0.09 0.15 0.10 0.30 0.03 0.05 0.17 0.22 -0.13 -0.01 0.35 0.24 0.25 0.15 0.34\nAll All 0.33 0.44 0.31 0.14 0.19 0.29 0.44 0.29 0.01 0.36 0.30 0.28 0.23 0.42 0.38 0.43 0.27 0.52\nDescend Pattern\nZ Pitch 0.18 0.21 0.16 -0.11 0.15 0.20 0.17 0.19 0.09 0.19 0.22 0.21 -0.04 0.23 0.22 0.18 0.08 0.28\nZ + V Pitch 0.21 0.31 0.23 0.03 0.14 0.22 0.28 0.28 0.30 0.32 0.26 0.23 0.10 0.24 0.42 0.18 0.34 0.17\nAll All 0.35 0.44 0.39 0.12 0.20 0.25 0.38 0.02 0.37 0.37 0.35 0.25 0.12 0.36 0.40 0.22 0.14 0.52\nTable 4 . Correlations for all samples in the dataset and the two major categorizations of music labels, using ascend and\ndescend patterns as explained in Section 5.3.2, and features from Tables 3 and 2\nMotion Music MotifNonMotif OrgSyn VibNonVib\nAscend Pattern CCA Deep CCA CCA Deep CCA CCA Deep CCA\nZ Pitch 0.05 0.23 0.13 0.26 0.19 0.19 0.22 0.25 0.33 0.07 0.33 0.13\nZ + V Pitch 0.10 0.24 0.17 0.31 0.19 0.22 0.24 0.31 0.33 0.09 0.32 0.20\nAll All 0.29 0.34 0.36 0.47 0.30 0.35 0.42 0.45 0.38 0.29 0.49 0.40\nDescend Pattern\nZ Pitch 0.20 0.17 0.19 0.21 0.20 0.16 0.23 0.18 0.20 0.17 0.24 0.18\nZ + V Pitch 0.22 0.22 0.32 0.29 0.24 0.20 0.35 0.26 0.22 0.22 0.14 0.34\nAll All 0.25 0.40 0.370.45 0.38 0.33 0.45 0.44 0.33 0.35 0.54 0.35\nTable 5 . Correlations for two-class categories, using ascend and descend patterns as explained in Section 5.3.2\nwith features from Tables 3 and 2\nconsistent between participants. The linear mappings of\npitch and vertical motion are limited, making the dataset\nchallenging. This also means that the associations between\npitch and vertical motion, as described in previous stud-\nies, are not that clear for this stimulus set, especially as\nwe use musical samples that are not controlled for being\nisochronous, nor equal tempered.\nThereafter, we conduct CCA and Deep CCA analysis\nas seen in Tables 4, 5. Overall, Deep CCA performs better\nthan its linear counterpart. We ﬁnd better correlation with\nall features from Table 3, as opposed to just using verti-\ncal motion and velocity. With ascending and descending\nlongest run-lengths, we are able to achieve similar results\nfor correlating all melodies with their respective tracings.\nHowever, descending contour classiﬁcation does not have\nsimilar success. There is more general agreement on con-\ntour with some melodies than others, with purely descend-\ning melodies having particularly low correlation. There is\nsome evidence that descending intervals are harder to iden-\ntify than ascending intervals [31], and this could explain a\nlow level of agreement in this study amongst people for de-\nscending melodies. Studying differences between ascend-\ning and descending contours requires further study.\nWhile using genre-labels (IJSV) for correlation, we ﬁnd\nthat scat samples show the least correlation, and the least\nimprovement. Speculatively, this could be related to the\nhigh number of spoken syllables in this style, even though\nthe syllables are not words. Deep CCA also gives an over-\nall correlation of 0.54 for recognizing melodies containing\nvibrato from the dataset. This is an indication that sonictextures are well represented in such a dataset.\nWith all melody and all motion features, we ﬁnd an\noverall correlation of 0.44 with Deep CCA, for both the\nlongest ascend and longest descend features. This supports\nthe view that non-linearity is inherent to tracings.\n7. CONCLUSIONS AND FUTURE WORK\nInterest in cross-modal systems is growing in the context of\nmulti-modal analysis. Previous studies in this area include\nshorter time scales or synthetically generated isochronous\nmusic samples. The strength of this particular study is\nin using musical excerpts as are performed, and that the\nperformed tracings are not iconic or symbolic, but spon-\ntaneous. This makes the dataset a step closer to under-\nstanding contour perception in melodies. We hope that\nthe dataset will prove useful for pattern mining, as it\npresents novel multimodal possibilities for the community\nand could be used for user-centric retrieval interfaces.\nIn the future, we wish to create a system to synthe-\nsize melody–motion pairs based on training a network to\nthis dataset, and conducting a user evaluation study, where\nusers evaluate system generated music–motion pairs in a\nforced–choice paradigm.\n8. ACKNOWLEDGMENTS\nPartially supported by the Research Council of Norway\nthrough its Centres of Excellence scheme (262762 &\n250698), and the Nordic Sound and Music Computing\nNetwork funded by the Nordic Research Council.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 799. REFERENCES\n[1] Charles R Adams. Melodic contour typology. Ethno-\nmusicology , pages 179–215, 1976.\n[2] Galen Andrew, Raman Arora, Jeff Bilmes, and Karen\nLivescu. Deep canonical correlation analysis. In In-\nternational Conference on Machine Learning , pages\n1247–1255, 2013.\n[3] Nick Gang Blair Kaneshiro Jonathan Berger and\nJacek P Dmochowski. Decoding neurally relevant mu-\nsical features using canonical correlation analysis. In\nProceedings of the 18th International Society for Mu-\nsic Information Retrieval Conference, Souzhou, China ,\n2017.\n[4] Elif Bozkurt, Y ¨ucel Yemez, and Engin Erzin. Multi-\nmodal analysis of speech and arm motion for prosody-\ndriven synthesis of beat gestures. Speech Communica-\ntion, 85:29–42, 2016.\n[5] Baptiste Caramiaux, Fr ´ed´eric Bevilacqua, and Norbert\nSchnell. Towards a gesture-sound cross-modal anal-\nysis. In International Gesture Workshop , pages 158–\n170. Springer, 2009.\n[6] Baptiste Caramiaux and Atau Tanaka. Machine learn-\ning of musical gestures. In NIME , pages 513–518,\n2013.\n[7] Martin Clayton and Laura Leante. Embodiment in mu-\nsic performance. 2013.\n[8] Rolf Inge Godøy. Images of sonic objects. Organised\nSound , 15(1):54–62, 2010.\n[9] Rolf Inge Godøy, Egil Haga, and Alexander Refsum\nJensenius. Exploring music-related gestures by sound-\ntracing: A preliminary study. 2006.\n[10] Rolf Inge Godøy and Alexander Refsum Jensenius.\nBody movement in music information retrieval. In 10th\nInternational Society for Music Information Retrieval\nConference , 2009.\n[11] Anthony Gritten and Elaine King. Music and gesture .\nAshgate Publishing, Ltd., 2006.\n[12] Anthony Gritten and Elaine King. New perspectives on\nmusic and gesture . Ashgate Publishing, Ltd., 2011.\n[13] Mantle Hood. The ethnomusicologist , volume 140.\nKent State Univ Pr, 1982.\n[14] David Huron. The melodic arch in western folksongs.\nComputing in Musicology , 10:3–23, 1996.\n[15] K Irwin. Musipedia: The open music encyclopedia.\nReference Reviews , 22(4):45–46, 2008.\n[16] Mari Riess Jones and Peter Q Pfordresher. Tracking\nmusical patterns using joint accent structure. Cana-\ndian Journal of Experimental Psychology/Revue cana-\ndienne de psychologie exp ´erimentale , 51(4):271, 1997.[17] Tejaswinee Kelkar and Alexander Refsum Jensenius.\nExploring melody and motion features in sound-\ntracings. In Proceedings of the SMC Conferences ,\npages 98–103. Aalto University, 2017.\n[18] Tejaswinee Kelkar and Alexander Refsum Jense-\nnius. Representation strategies in two-handed melodic\nsound-tracing. In Proceedings of the 4th International\nConference on Movement Computing , page 11. ACM,\n2017.\n[19] Tejaswinee Kelkar and Alexander Refsum Jense-\nnius. Analyzing free-hand sound-tracings of melodic\nphrases. Applied Sciences , 8(1):135, 2018.\n[20] M Kussner. Creating shapes: musicians and non-\nmusicians visual representations of sound. In Proceed-\nings of 4th Int. Conf. of Students of Systematic Mu-\nsicology, U. Seifert and J. Wewers, Eds. epOs-Music,\nOsnabr ¨uck (Forthcoming) , 2012.\n[21] Olivier Lartilot. Soundtracer, 2018.\n[22] Marc Leman. Embodied music cognition and media-\ntion technology . Mit Press, 2008.\n[23] Cynthia Liem, Meinard M ¨uller, Douglas Eck, George\nTzanetakis, and Alan Hanjalic. The need for music in-\nformation retrieval with user-centered and multimodal\nstrategies. In Proceedings of the 1st international ACM\nworkshop on Music information retrieval with user-\ncentered and multimodal strategies , pages 1–6. ACM,\n2011.\n[24] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW\nEllis, Matt McVicar, Eric Battenberg, and Oriol Nieto.\nlibrosa: Audio and music signal analysis in python.\n2015.\n[25] Kristian Nymoen, Baptiste Caramiaux, Mariusz\nKozak, and Jim Torresen. Analyzing sound tracings:\nA multimodal approach to music information retrieval.\nInProceedings of the 1st International ACM Workshop\non Music Information Retrieval with User-centered\nand Multimodal Strategies , MIRUM ’11, pages 39–44,\nNew York, NY , USA, 2011. ACM.\n[26] Kristian Nymoen, Rolf Inge Godøy, Alexander Ref-\nsum Jensenius, and Jim Torresen. Analyzing corre-\nspondence between sound objects and body motion.\nACM Trans. Appl. Percept. , 10(2):9:1–9:22, June 2013.\n[27] Kristian Nymoen, Jim Torresen, Rolf Godøy, and\nAlexander Refsum Jensenius. A statistical approach\nto analyzing sound tracings. Speech, sound and music\nprocessing: Embracing research in India , pages 120–\n145, 2012.\n[28] Joy E Ollen. A criterion-related validity test of selected\nindicators of musical sophistication using expert rat-\nings. PhD thesis, The Ohio State University, 2006.\n[29] Denys Parsons. The directory of tunes and musical\nthemes . Cambridge, Eng.: S. Brown, 1975.80 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[30] Aniruddh D Patel. Music, language, and the brain . Ox-\nford university press, 2010.\n[31] Art Samplaski. Interval and interval class similarity:\nResults of a confusion study. Psychomusicology: A\nJournal of Research in Music Cognition , 19(1):59,\n2005.\n[32] Charles Seeger. On the moods of a music-logic.\nJournal of the American Musicological Society ,\n13(1/3):224–261, 1960.\n[33] Sandra E Trehub, Judith Becker, and Iain Morley.\nCross-cultural perspectives on music and musicality.\nPhilosophical Transactions of the Royal Society of\nLondon B: Biological Sciences , 370(1664):20140096,\n2015.\n[34] Sandra E Trehub, Dale Bull, and Leigh A Thorpe. In-\nfants’ perception of melodies: The role of melodic con-\ntour. Child development , pages 821–830, 1984.\n[35] Kaiye Wang, Qiyue Yin, Wei Wang, Shu Wu, and\nLiang Wang. A comprehensive survey on cross-modal\nretrieval. arXiv preprint arXiv:1607.06215 , 2016.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 81"
    },
    {
        "title": "Aligned Sub-Hierarchies: A Structure-based Approach to the Cover Song Task.",
        "author": [
            "Katherine M. Kinnaird"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492483",
        "url": "https://doi.org/10.5281/zenodo.1492483",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/81_Paper.pdf",
        "abstract": "Extending previous structure-based approaches to the song comparison tasks such as the fingerprint and cover song tasks, this paper introduces the aligned sub-hierarchies (AsH) representation. Built by applying a post-processing technique to the aligned hierarchies of a song, the AsH representation is the set of unique aligned hierarchies for repeats (called AHR) encoded in the original aligned hierarchies of the whole song. Effectively each AHR within AsH is a section of the aligned hierarchies for the original song. Like aligned hierarchies, the AsH representation can be embedded into a classification space with a natural metric that makes inter-song comparisons based on sections of the songs. Experiments addressing a version of the cover song task on score-based data using AsH as the basis of inter-song comparison demonstrate potential of AsH-based approaches for MIR tasks.",
        "zenodo_id": 1492483,
        "dblp_key": "conf/ismir/Kinnaird18",
        "keywords": [
            "aligned sub-hierarchies",
            "post-processing technique",
            "unique aligned hierarchies",
            "original aligned hierarchies",
            "inter-song comparisons",
            "natural metric",
            "MIR tasks",
            "score-based data",
            "cover song task",
            "classification space"
        ],
        "content": "ALIGNED SUB-HIERARCHIES: A STRUCTURE-BASED APPROACH TO\nTHE COVER SONG TASK\nKatherine M. Kinnaird\nData Sciences Initiative and Division of Applied Mathematics\nBrown University, USA\nkatherine kinnaird@brown.edu\nABSTRACT\nExtending previous structure-based approaches to the song\ncomparison tasks such as the ﬁngerprint and cover song\ntasks, this paper introduces the aligned sub-hierarchies\n(AsH) representation. Built by applying a post-processing\ntechnique to the aligned hierarchies of a song, the AsH\nrepresentation is the set of unique aligned hierarchies for\nrepeats (called AHR) encoded in the original aligned hier-\narchies of the whole song. Effectively each AHRwithin\nAsH is a section of the aligned hierarchies for the original\nsong. Like aligned hierarchies, the AsH representation can\nbe embedded into a classiﬁcation space with a natural met-\nric that makes inter-song comparisons based on sections of\nthe songs. Experiments addressing a version of the cover\nsong task on score-based data using AsH as the basis of\ninter-song comparison demonstrate potential of AsH-based\napproaches for MIR tasks.\n1. INTRODUCTION\nA common starting point in music information retrieval\ntasks is the creation of visualizations and representations\nfor music-based data streams. One of the most inﬂuential\nrepresentations is Foote’s self-similarity matrix (SSM) [4],\nwhich continues to be one of the most recognizable im-\nages in MIR. Much of the work starting with an SSM or\na self-dissimilarity matrix (SDM) like [1, 5, 7, 9–12] cre-\nate post-processing techniques that seek to enhance certain\nproperties. This paper also offers a new post-processing\ntechnique that can be applied to either the SSM or SDM\nand extends the work of [7].\nUnder the music comparison tasks like the ﬁngerprint\ntask and the cover song task, structure-based approaches\nlike [5, 7] seek to compare songs via their whole song rep-\nresentations. While this type of approach has varied suc-\ncess, there can be obvious issues. For example, with more\nrigid comparisons such as [7], whole song comparisons\nmay only be meaningful if the songs have the same number\nof time-steps. Similarly whole song comparisons can fall\nc\rKatherine M. Kinnaird. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution:\nKatherine M. Kinnaird. “Aligned sub-Hierarchies: a structure-based ap-\nproach to the cover song task”, 19th International Society for Music In-\nformation Retrieval Conference, Paris, France, 2018.victim to large artistic choices. For example, [5] created a\nsmoothed image of the thresholded and resampled SDM,\nwhich was then compared using the Euclidean distance.\nWhile effective, this approach stumbled comparing record-\nings of Mazurka Op. 68 No. 4 where Chopin neglected to\ninclude a ﬁnemarking, causing some pianists to play the\npiece twice, while others played the piece once [5].\nThe contrast to the whole song approach is using sec-\ntions of a song. In [2, 3], audio shingles representing sec-\ntions of recordings are compared to address the ﬁngerprint,\ncover song, and remix tasks. In [17], the ﬁngerprint task is\ntackled by comparing sections of recordings’ constellation\nmaps marking their spectrogram peaks. Both approaches\nrequire access to the original audio signal.\nThis work introduces the aligned sub-hierarchies\n(AsH), a structure-based representation that can be used\nto compare songs based on sections of the songs. This\nAsH representation exists between the section-based com-\nparison approaches like [2, 3, 17] and the structure-based\napproaches in [5, 7]. Furthermore, this representation em-\nbeds into a classiﬁcation space with a natural metric that\nobserves the triangle inequality.\nThe paper is organized as follows. Section 2 motivates\nthe necessity of the extension of aligned hierarchies into\nAsH representation in context of MIR tasks. In Section 3,\nwe formalize the deﬁnition of the AsH representation, de-\ntail the construction of AsH, and describe embedding AsH\ninto a classiﬁcation space with a natural metric. In Sec-\ntion 4, we use AsH representations to perform experiments\nfor a version of the cover song task on a set of Mazurka\nscores. We offer future directions for research in Section 5.\n2. MOTIV ATION AND BACKGROUND\nThe aligned sub-hierarchies (AsH) representation is an ex-\ntension of the aligned hierarchies from [7] that is moti-\nvated and inspired by making comparisons based on sec-\ntions of songs or musical scores like those of [2,3,17]. This\nnew representation seeks to combine the strengths of [5,7]\nwhile addressing their limitations. Like their predecessor,\nAsH embeds into a classiﬁcation space with a natural met-\nric, but unlike in [7], the metric for AsH allows comparison\nbetween songs of differing lengths. Inspired particularly\nby the work in [5] stumbling on comparisons where some\nartists chose to repeat a song in its entirety, AsH seeks to\nnote whether two songs share sections of unique structural585decompositions at the original scale of those sections. In\nother words, in contrast to [5], there is no resampling in the\nformulation nor in the comparison of AsH.\nBefore detailing the AsH representation, we will\npresent a summary of the aligned hierarchies introduced\nin [7]. This structure-based visualization catalogues all\nmeaningful repeats present in music-based data streams,\nshowing all possible structural hierarchies aligned on one\ncommon time axis. Each aligned hierarchies represen-\ntationHcomprises of three components: an onset ma-\ntrixBH, a length vector wH, and an annotation vector\n\u000bH. There are as many rows in these three components\nas there are kinds of repetitions present in a music-based\ndata stream, with each row being tied to a particularly type\nof repeat. The binary matrix BHencodes when each re-\npeat begins with a entry of 1 at each starting time step.\nThe vectors wHand\u000bHtogether act as a key for BH,\nwhile the former encodes the lengths of each type of re-\npeat, and where the latter assigns annotations for the types\nof repeats so that types of the same length have different\nannotations. We can visualize the information contained\ninH= (BH;wH;\u000bH)as shown in Figures 1 and 3.1(b).\n(a) Aligned hierarchies visualization\nfor the version of the score observing\nChopin’s repeat signs. One segmen-\ntation of this version is AABABACA.\n(b) Aligned hierarchies vi-\nsualization for the version\nof the score ignoring the\nrepeat signs as if Chopin\nnever wrote them. One seg-\nmentation of this version is\nABACA.\nFigure 1 . Aligned hierarchies for versions of Chopin’s\nMazurka Op. 6, No. 1 score, under threshold T= 0:02,\nwith shingle size 12. Grey blocks denote repeats and rows\ndenote types of repeats. Vertical labels are a subset of re-\npeat lengths, while the horizontal ones mark the time steps.\nThe aligned hierarchies also provide an approach to the\nﬁngerprint task as they can be embedded into a classiﬁ-\ncation space. However, nuanced comparisons can only\nbe made between two aligned hierarchies with the ex-\nact same number of time steps [7]. This rigidity makes\naligned hierarchies-based comparisons for the cover song\ntask inappropriate since two cover songs are unlikely to\nbe the same length. For certain MIR tasks, like the cover\nsong task, we instead would like to compare parts of the\nsongs’ aligned hierarchies to each other, making compar-\nisons based on these smaller aligned hierarchies a more\nsuitable choice. This is the motivation and the inspiration\nfor AsH. Take for example Figure 1 showing two versions\nof Chopin’s Mazurka Op. 6, No. 1 score: Figure 1(a) showsthe aligned hierarchies for the score as Chopin intended\nand Figure 1(b) shows the aligned hierarchies for the score\nwith Chopin’s repeat signs ignored. Under one version of\nthe cover song task, we would like to identify these two\nversions as being based off the same score, and just com-\nparing these two aligned hierarchies, we notice similarities\nbetween the sections of the shown structural hierarchies.\n3. ALIGNED SUB-HIERARCHIES\nThis section introduces both the aligned sub-hierarchies\n(AsH) representation and the classiﬁcation space that AsH\nrepresentations embed into. Starting with the aligned hi-\nerarchies for a song or a musical score, we isolate differ-\nent repeated patterns and ﬁnd the individual aligned hierar-\nchies for each isolated pattern. The collection of the unique\naligned hierarchies for sections of music-based data stream\nis called the aligned sub-hierarchies , abbreviated to AsH.\nAsH is the result of a post-processing technique on\naligned hierarchies that is similar to the result of treating\nsections of a song as songs in their own right and then ﬁnd-\ning each section’s aligned hierarchies. Like other compar-\nison methods like [2, 3, 17] that compare sections of songs\nto each other, the AsH ﬁnds all possible structural hierar-\nchies for sections of each song. By leveraging structural\ninformation already encoded in aligned hierarchies for the\nwhole song, we have already found all the repeated sec-\ntions with smaller repeats within them and potentially keep\nadditional structure information that would have been hid-\nden had we build an aligned hierarchies directly from the\nsong’s section.\n3.1 Deﬁning AsH\nIn this section, we will formally deﬁne the AsH represen-\ntation and consider a motivating example. The below def-\ninition for the aligned hierarchies of a given repeat Rk\ni,\nparticularly the third condition, ensures that we capture all\npossible song sections with their own aligned hierarchies.\nDeﬁnition 3.1. Consider a song and let Hbe the aligned\nhierarchies for the song. Let Rk\nibe a repeat of length k\nbeginning at time step i, encoded in H. Then the set of\nrepeats meeting the following three conditions form the\naligned hierarchies ofRk\nior AHRk\ni:\n1. Encoded in Hthat are of length less than k,\n2. Contained in the set of time steps [i;(i+k\u00001)]\\N,\n3. Have at least one corresponding repeat that is also\ncontained within the time steps [i;(i+k\u00001)]\\N.\nWe encode AHRk\niashRk\ni=\u0010\nBHjRk\ni;wHjRk\ni;\u000bHjRk\ni\u0011\n,\nwhere each component of hRk\niis deﬁned similarly to those\ninH= (BH;wH;\u000bH). Here, the onset matrix BHjRk\nihas\nkcolumns, encoding repeats in hRk\niby by their relative\nposition to the start of Rk\ni. For a general repeat (without a\nspeciﬁed start or length), we say aligned hierarchies of a\nrepeat and shorten to AHR.\nExample 3.1. Consider a song with the thresholded dis-\ntance matrixTshown in Figure 3.1(a). This song has three586 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018kinds of structure: A(which occurs four times), B(which\noccurs ﬁve times), and C(which occurs only once). The\naligned hierarchies for the song is shown in Figure 3.1(b).\nWe can build AHR30\n1from the aligned hierarchies by\nconsidering the blocks contained within the beats 1-30. In\nthis case, those structures are the (ABA )structure on beats\n1-25; the (BAB )structure on beats 11-30; the (AB)struc-\nture on beats 1-15 and 16-30; the (BA)structure on beats\n11-25; theBstructure on beats 11-15 and 26-30; and the\ntwoAstructures on beats 1-10 and 16-25. Given that the\nA,B, and (AB)structures each repeat within beats 1-30,\nthen by Deﬁnition 3.1, AHR30\n1, encoded into hR30\n1, con-\ntains these structures. A visualization for hR30\n1is shown in\nFigure 3.1(c).\nSince the notion of time for AHRk\niis relative to beat i,\nthen for the Example 3.1, we have that AHR30\n36will encode\nthe same information as AHR30\n1. SohR30\n1=hR30\n36.\nDeﬁnition 3.2. LetHbe the aligned hierarchies for a song.\nThe unordered set of unique AHRrepresentations denoted\nfhg=fh1;h2;:::;hmgwhere eachhi2fhgis the AHR\nfor at least one repeat encoded in His the aligned sub-\nhierarchies (or AsH) of the song.\nExample 3.2. Consider Example 3.1 shown in Figure 3.1.\nThe repeats (BAB ),(ABA ),(ABAB )each have an\nAHR. Although each of these occur twice, the AsH rep-\nresentation is comprised of only unique AHRrepresenta-\ntions. Sofhg=\b\nh(BAB );h(ABA );h(ABAB )\t\n.\n3.2 Building AsH from Aligned Hierarchies\nIn this section, we explain crafting fhg, the AsH represen-\ntation of a song, from aligned hierarchies. First we detail\nconstructing AHRk\nifor each repeat encoded in Hand then\nexplain when AHRk\niis added to the AsH representation.\nFor each repeat in H, we note the starting time step i\nand the length of the repeat k, and then form AHRk\nias fol-\nlows. We ﬁrst isolate the rows of BH2Hthat correspond\nto repeats smaller than width k(that is the rows of BHas-\nsociated to the entries of wH<k), and we further restrict\nthis matrix to only the columns ithrough (i+k\u00001). We\ncall the resulting sub-matrix BHjRk\ni. We also form the as-\nsociated vectors wHjRk\niand\u000bHjRk\nias the entries of wH\nand\u000bHthat correspond to the rows of BHjRk\ni.\nTo satisfy Deﬁnition 3.1 as we build AHRk\ni, we remove\nthe rows ofBHjRk\nithat contain fewer than two repeats and\nthen remove the corresponding entries in both wHjRk\niand\n\u000bHjRk\ni. Removing entries in \u000bHjRk\nimay require adjusting\nthe resulting values in \u000bHjRk\niso that for each repeat length\nk, the clusters of repeats of length kstored inBHjRk\niare\nidentiﬁed with integers 1 through \u0014(that is, the number of\nclusters of repeats of length kstored inBHjRk\ni).\nThe resulting triple hRk\ni=\u0010\nBHjRk\ni;wHjRk\ni;\u000bHjRk\ni\u0011\nis the AHRk\nirepresentation. We next check if hRk\niis\nalready infhg, the unordered list of unique AHRk\ni. If\nhRk\ni62fhg, thenhRk\niis added tofhg.A A A A B B B BB C\nt = 1\nt = 11015 253036 4550 606570\n10\n15\n25\n30\n36\n45\n50\n60\n65\n70\n(a)Tfor the toy song with sections marked.\nt = 1 1015 253036 4550 606570B\nA\n(AB )\n(BA )\n(ABA )(BAB )\n(ABAB )\n(b) Visualization for Hof the toy song.\nt = 1 1015 2530B\nA\n(AB )\n(c) Visualization for h(ABAB ), which can be either denoted as hR30\n1\norhR30\n36, as they are equivalent.\nFigure 2 . Visualizations for toy song example with seg-\nmentation ABABCABABB.\nBy construction, a song’s AsH can have AHRof differ-\ning widths. If hi2fhgis the AHRfor repeatRk\ni, thenhi\nis of widthk.\n3.3 Method for Comparing AsH\nComparing two AsH representations to each other requires\nﬁnding the best alignments between collections of AHR.\nThis comparison must also respect that the AsH is an un-\nordered set of AHRrepresentations and thus needs to be\ninvariant to shifts in the ordering of the AHRs. The AsH\nrepresentation can be embedded into a space with a natural\nnotion of distance, and this embedding leverages the em-\nbedding of the aligned hierarchies from Section 3 of [7].\n3.3.1 Embedding AsH\nAs each AHRis an aligned hierarchies representation, we\nstart by embedding each AHRinto(S\u0003)n, the classiﬁcation\nspace for aligned hierarchies. To do this, a sequence ofProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 587binary matrices is created, where the kthmatrix is the rows\nof the aligned hierarchies associated to repeats of length k.\nThe spaceS\u0003is deﬁned asS=\u0018, whereSis the space of\n(m\u0002t)\u0000binary matrices and where \u0018is the equivalence\nrelationship encoding that two matrices are equivalent if\nthey are row permutations of each other.\nIn the case of AsH, we have a collection of AHRs each\nof which can be represented as an element of (S\u0003)n. By\ntreating each hi2 fhgas a column of information, we\nconsiderpcopies of (S\u0003)n. In this sense, we have a prod-\nuct space comprised of (n\u0002p)-copies of the space S\u0003, ar-\nranged intonrows andpcolumns, exactly like the entries\nof a(n\u0002p)-matrix, with each element of AsH occupying\na column of this matrix-like layout.\nSince AsH is an unordered collection of AHRs, we need\nto deﬁne a space that is invariant to the ordering of the ele-\nments of AsH. We deﬁne the relationship \u0018\u0003on(S\u0003)(n\u0002p)\nsuch that two AsH representations are equivalent under \u0018\u0003\nif they are different orderings of the same set of AHRrepre-\nsentations. We can show that \u0018\u0003is an equivalence relation\non(S\u0003)(n\u0002p), which allows us to deﬁne the following:\nDeﬁnition 3.3. LetP be the quotient space\n(S\u0003)(n\u0002p)=\u0018\u0003. ForfAg 2 P , thei-th column is\nAi. We writeAi2fAgand callAiacolumn offAg.\nThe AsH representation fhgof a song can be repre-\nsented as an element of Pwherehi2fhg, thei-th AHR,\ngets placed in the i-th column and sofhg2P . This space\nPencodes the invariance of the ordering of the AHRs in\nan AsH representation, due to the equivalence relation \u0018\u0003,\nmeaning that the AHR(for a given AsH representation) can\nbe placed intoPin any order.\n3.3.2 Metric onP\nTo compare two songs via AsH, we ﬁnd the pairs of the\nAHRfrom the ﬁrst song with those from the second song\nthat minimize the sum of the distances between the pairs.\nWe ﬁrst consider the AHRs within the AsH representations\nthat are of a ﬁxed length k. Then we add all the distances\nfrom the identiﬁed matchings across the possible values of\nk. The resulting sum encodes the total dissimilarity be-\ntween the repeated patterns of all sizes present in all of\nAHRs contained within the two AsH representations.1\nTo ﬁrst compare AHRs of the same length, consider\ntwo AsH representations fAg=fA1;A2;:::;Aqgand\nfBg=fB1;B2;:::;Brg, with all AHRs of length k.\nAssuming that q;r2Z\u00150and thatq\u0015r, and ensuring\nthat we are comparing lists of the same lengths, we append\n(q\u0000r)empty AHRs tofBg, each of which is a row-vector\nofkzeros. We recall that dH: (S\u0003)n\u0002(S\u0003)n!Ris the\ndistance between two aligned hierarchies encoding the to-\ntal dissimilarity between them.\nBelow, we deﬁne fLthat permutes the elements of fBg\nto ﬁnd the optimal matching of AHRs infBgto those in\nfAg. This sum of the distances between the pairs of AHRs\nis the minimum across all possible matchings.\n1The proofs for the material in this section can be found in the author’s\ndoctoral thesis [6].Proposition 3.1. LetfAg;fBg2P . LetSpbe the sym-\nmetric group of degree p. DeﬁnefL:P\u0002P! Ras\nfL(fAg;fBg) = min\n\u001b2SppX\ni=1dH\u0000\nAi;B\u001b(i)\u0001\nThen the function fL:P\u0002P! Ris a distance function.\nProof Sketch : Leveraging properties of the symmetric\ngroup and the fact that dHis a distance function, it is a\nstraight forward check of the four requirements of a dis-\ntance function: non-negativity, observance that the dis-\ntance between two objects is zero if they are equivalent,\nreﬂectivity, and obeying the triangle inequality.2\nWhilefLis a notion of total dissimilarity between two\nsets of AHRs all of the same ﬁxed length, a song’s AsH\nrepresentation likely contains AHRs of differing lengths.\nTo ﬁnd the total dissimilarity across all possible lengths of\nAHRs within AsH representations we add up the fLdis-\ntances found across all values of k. For clarity, we use the\nfollowing deﬁnitions and notation:\nDeﬁnition 3.4. LetfAg2P such that the AHRs offAg\nare not necessarily the same width. Deﬁne fAkgto be the\nAHRs offAgthat are of width k.\nCorollary 1. LetfAg;fBg2P . LetMbe the largest\nwidth of the AHRs infAgorfBg. LetdP:P\u0002P! R\nbe given by:\ndP(fAg;fBg) =MX\ni=1fL\u0000\nfAig;fBig\u0001\n:\nThendPis also a distance function.\nProof Sketch : Using that fLis a distance function, check\nthatdPsatisﬁes the deﬁnition of a distance function.\nWe note that the comparison of two songs using AsH\nrequires that both songs have AsH representations. Due to\nthe fact that we require each AHRto be the aligned hierar-\nchies for a section of the song, it is possible that there ex-\nists aligned hierarchies for a song, but not one AHR. This\nwould happen when all sections within a song do not have\nsmaller repeated structures that repeat within that section.\nIn this case, the song has an empty AsH representation and\nnote that no comparisons can be made between a song with\nan empty AsH representation and any other song.\n4. COVER SONG EXPERIMENTS\nTo test the validity of the using AsH representation and\nits associated metric to approach MIR tasks, we apply the\nAsH-based comparison method to address a version of the\ncover song task for a score-based data set.3Each experi-\nment follows the below procedure:\n2We can further prove that if we ﬁrst ﬁnd and remove exact matches\nfor the AHRs in the two AsHs, then the distance between the remain-\ning AHRs infAgandfBgis the same as the distance between the full\nAsHsfAgandfBg. This fact adds efﬁciency to the computation of\nfL(fAg;fBg). This proof proceeds by induction on the number of un-\nmatched exact matches between fAgandfBg.\n3The code used for these experiments as well as those in [7] can be\nfound at https://github.com/kmkinnaird/ThesisCode/\nreleases/tag/vT.final2588 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20181. Pre-process the songs by building audio shingles us-\ningsconcatenated beat synchronous Chroma feature\nvectors, and then creating and thresholding an SDM\nusing a global threshold T\n2. Construct the aligned hierarchies as in [7]\n3. Extract AsH representations from each aligned hier-\narchies representation\n4. Compute pairwise distances between pairs of AsH\nrepresentations under the dPmetric\n5. Match two songs if they are mutual nearest neigh-\nbors of each other\n6. Evaluate the resulting matchings compared to the\nground truth using precision and recall scores\n4.1 Score-Based Data Set\nFor the experiments in this work, we use a score-based data\nset comprised of 52 Mazurka musical scores by Chopin.\nFor each score, we download two **kern ﬁles posted on the\nKernScore online database4(see [13]). The ﬁrst ﬁle ob-\nserves the repeated signs as marked by Chopin in the score,\nwhile the second ignores these repeat signs as if they are\nnot written at all. If a score has no marked repeat signs, we\ndownload the single **kern ﬁle twice, marking one copy\nas observing the repeat signs and the second copy as hav-\ning the repeat signs ignored. We refer to the versions of the\nscores as songs.\nIn this data set, each time step is in terms of beats with\none time step being equivalent to one beat. We use the\nmusic21 Python library5to extract the Chroma feature\nvectors for each beat in the song. For each time step, we\nencode local information by creating the audio shingles\n(like those in [2, 3]) that are sconcatenated Chroma fea-\nture vectors, for a ﬁxed integer s. As most Mazurkas have 3\nbeats per measure, in these experiments (like those in [7]),\nwe sets= 6ors= 12 . This means that we encode two or\nfour (three beats) bars into each audio shingle.\nWe then createD, the SDM for each song, by comput-\ning cosine dissimilarity measure between all pairs of audio\nshingles. So for audio shingles ai;ajassociated to time\nstepsiandjrespectively, we deﬁne\nDi;j=\u0012\n1\u0000hai;aji\njjaijj2jjajjj2\u0013\n:\nThen each SDM is thresholded based on the chosen global\nthresholdT, which denotes how similar two audio shingles\nmust be to be considered repetitions of each other. In these\nexperiments, we choose global thresholds that are associ-\nated to very small differences between collections of 3 to\n5 notes. To make this choice, we used the framework pre-\nsented in [8] to connect our choice of Tto the number of\nadditions to the C-maj chord one can make and still be con-\nsidered a repeat of the C-maj chord under the threshold T.\nThis thresholding method differs from those in the litera-\nture that choose a threshold based on a ﬁxed percentage of\n4http://kern.humdrum.org/search?s=t&keyword=Chopin\n5http://web.mit.edu/music21/pairwise measures to be selected such as [1,5,11] or based\non a ﬁxed number of nearest neighbors as in [14–16].\nWe complete the processing of each song by extract-\ning the associated aligned hierarchies from the thresholded\nSDM as done in [7]. To ﬁnd the AsH representation for\neach song, we apply the post-processing steps outlined in\nsubsection 3.2. The AsH is the basis for our inter-song\ncomparison in the following experiments.\n4.2 Experimental Setup\nFor this work, we deﬁne the cover song task as matching\nthe score’s **kern ﬁle with the repeat signs observed to\nthe score’s **kern ﬁle that ignore the repeat signs. After\nﬁnding the AsH for each song, we compute the pairwise\ndistances between the songs’ representations using dP, and\nstore the results in a pairwise distance matrix D. We then\nperform a mutual nearest neighbor matching, by treating\neach song as a query track. Therefore each experiment\nhas a maximum of 104 possible matches as each song as\nanother version of its score to match with.\nFor these experiments, the ground truth is the song list\nwith their cover as given by the meta data of each **kern\nﬁle. We compute precision and recall by comparing the\nexperiment’s resulting matches to the ground truth.\n4.3 Results\nTable 1 reports experimental results for s2f6;12gand\nT2 f0:01;0:02;0:03;0:04;0:05g. The 10 experiments\nhave high precision rates but more modest recall rates.\nSince not all songs have an AsH representation for each\nvalue ofsandT, the number of possible matches varies.\nFor each experiment, we note the number of feature\nvectors per audio shingle, s, and the threshold value T\nthat determines when two time steps are said to be repeats\nof each other. The choice of sandTaffects the number\nof non-zero entries in the thresholded SDM, which deter-\nmines whether or not a song has an AsH representation.\nIf a song does not have an AsH associated to it, then we\nremove the row and column in Dassociated to that song\nfrom consideration as well as remove that song from the\nground truth listing. Our computations for precision and\nrecall are based the adjusted ground truth list. In addition\nto reporting the precision and recall values for each exper-\niment, we also report the number of possible matches that\ncould be made based on the choices of sandT.\n4.4 Discussion\nThe above results demonstrate the usability of the AsH rep-\nresentation in approaching MIR tasks. What is more, these\nresults expose both strengths and weaknesses of using AsH\nas the basis for inter-song comparisons.\nIn considering the above results, we note that this ver-\nsion of the cover song task differs from the typical presen-\ntation of the cover song task. Most cover songs follow the\noriginal composer’s intended structure fairly closely. In\nthese experiments, half of our data set closely follows the\ncomposer’s intentions while the other half blatantly makesProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 589sTPossible Precision Recall Empty\nMatches Rate Rate AsHs\n60.01 62 1 0.774 35\n0.02 66 0.962 0.757 33\n0.03 74 0.931 0.730 26\n0.04 78 1 0.692 24\n0.05 88 1 0.727 15\n120.01 34 1 0.882 68\n0.02 44 1 0.818 57\n0.03 46 0.85 0.739 54\n0.04 56 0.84 0.75 45\n0.05 62 0.929 0.839 39\nTable 1 . Results for AsH for score data set on 10 exper-\niments varying s, the width of the audio shingles, and T,\nthe threshold used on the SDM for each song\nlarge changes to the structure of the pieces. It is uncom-\nmon (though not unheard of as discussed in [5]) to see\nsuch large structural changes between two recordings of\nthe same piece, which is what makes this version of the\ncover song task more challenging. Under this version of\nthe cover song task, the AsH-based method was able to\nachieve strong experimental results.\nThe data in these experiments differs from the typical\ndata set for the cover song task. For this score data, there\nare only two natural versions of each score: one with the\nrepeat signs observed and the second with the repeat signs\nignored. This means that every song has exactly one cover\nsong to match with, contrasting from the typical data set\nfor cover song retrieval that has an unknown number of\ncovers for each query song. While a mutual nearest neigh-\nbors matching condition makes sense for this score-based\ndata set (and other similar collections), it does mean that\nthe choice of what each query track matches to is not inde-\npendent from each other. An adjustment to the matching\ncondition would need to be made for a data set of audio\ntracks with varying numbers of cover songs.\nWe also note that for the experiments in [7] nearly every\nsong could be represented by aligned hierarchies, regard-\nless of the shingle size sor threshold value T. In contrast,\nfor the same data set under the same values for sandT, we\nﬁnd several songs without an AsH representationdata set,\nmeaning that those songs cannot be represented by an AsH\nrepresentation or compared to other songs’ AsH represen-\ntations. Songs will lack an AsH representation if none of\nthe repeats in the aligned hierarchies have smaller repeated\nstructures within them. We can add ﬂexibility to our deﬁ-\nnition of what it means for two sections to be repetitions of\neach other by increasing the value of T. Understandably,\nas the value of Trises, so do the number of songs with AsH\nrepresentations, meaning that an appropriate choice of Tis\ncrucial to comparison methods based on AsH representa-\ntions. Even with this caveat, the results from the above ex-\nperiments provide evidence in favor of the usability of AsH\nas a low-dimensional representation for high-dimensional\nsequential data with lots of repeated structure.Finally, the AsH comparison method is based on an ac-\ncumulation of structure-based comparisons between struc-\nture decompositions of sections of the query song (rep-\nresented by the collection AHRfor the query) and the\nstructure decompositions of sections in every other song\nin the data set (also via their collections of AHR). As\nwith the aligned hierarchies, the AsH-based comparisons\nare based on hierarchical structure decompositions of sec-\ntions of songs and are more than just one level or one size\nof structure. What is more, each AHR, like the aligned\nhierarchies, encode not one possible structure hierarchy,\nbut all structure hierarchies that exist within that section\nof the song. Matchings via AsH will occur when two\nsongs have several sections that share hierarchical struc-\nture decompositions. This is a far more nuanced matching\nthan just matching based on one segmentation. This AsH-\nbased comparison method is a starkly different approach\nthan [17] which compares just one section of the query\ntrack to the other songs in the data set. This approach is\nreminiscent of the work in [3] that takes a truncated sum\nof the distances between pairs of audio shingles. The cru-\ncial difference between [3] and this work is that the former\nis based directly on the audio frequencies within a section\nof a song, while the latter is based on the lengths and posi-\ntioning of repeats within sections of the song.\n5. CONCLUSION\nIn this paper, we introduce the aligned sub-hierarchies\n(AsH) representation, an extension of the aligned hierar-\nchies in [7] that allows for structure-based comparisons\nbetween sections of songs. This representation seeks to\naddress limitations of the approach in [5] to the cover song\ntask by creating a collection of unique structure represen-\ntations for sections of each song within a data set. There is\na mathematical framework underpinning AsH as shown by\nembedding AsH representation into a classiﬁcation space\nwith a natural metric. Finally, we address a version of the\ncover song task using AsH-based pairwise song compar-\nisons on a score-based data set. These experiments pro-\nvide a proof of concept for using the AsH representation\nfor highly repetitive, sequential data and offer new insights\ninto structure-based approaches to comparison tasks based\non sections of songs.\nBy existing between music comparisons based on\nwhole song representations like [5, 7] and those based on\npartial song representations like [17], the AsH representa-\ntion opens several new avenues of research. In future work,\nwe plan to explore the impact of relaxing the third condi-\ntion in Deﬁnition 3.1, both from the theory angle of cre-\nating an appropriate metric, like the one in subsection 3.3\nand from the practical angle of being able to efﬁciently ad-\ndress MIR tasks on large data sets. Further exploration of\nthe impact of Tandson AsH is also needed. As the ex-\nperiments presented here were limited to score-based data,\nwe also plan to apply AsH-based comparisons to the cover\nsong task on collections of audio recordings.590 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Acknowledgements\nPart of this work is a portion of the author’s doctoral the-\nsis [6], which was partially funded by the GK-12 Program\nat Dartmouth College (NSF award #0947790). The author\nalso thanks Scott Pauls, Michael Casey, Dan Ellis, Jessica\nThompson, and Brian McFee for their feedback on the ear-\nlier versions of this work.\n6. REFERENCES\n[1] J. Bello. Measuring structural similarity in music.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 19(7):2013–2025, 2011.\n[2] M. Casey, C. Rhodes, and M. Slaney. Analysis of min-\nimum distances in high-dimensional musical spaces.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 16(5):1015 – 1028, 2008.\n[3] M. Casey and M. Slaney. Fast recognition of remixed\naudio. 2007 IEEE International Conference on Audio,\nSpeech and Signal Processing , pages IV – 1425 – IV–\n1428, 2007.\n[4] J. Foote. Visualizing music and audio using self-\nsimilarity. Proc. ACM Multimedia 99 , pages 77–80,\n1999.\n[5] P. Grosche, J. Serr `a, M. M ¨uller, and J.Ll. Arcos.\nStructure-based audio ﬁngerprinting for music re-\ntrieval. Proc. of 13thISMIR Conference , pages 55–60,\n2012.\n[6] K. M. Kinnaird. Aligned Hierarchies for Sequential\nData . PhD thesis, Dartmouth College, 2014.\n[7] K. M. Kinnaird. Aligned hierarchies: A multi-scale\nstructure based representation for music-based data\nstreams. Proc. of 17thISMIR Conference , 2016.\n[8] K. M. Kinnaird. Examining musical meaning in sim-\nilarity thresholds. Proc. of 18thISMIR Conference ,\n2017.\n[9] B. McFee and D. P. W. Ellis. Analyzing song structure\nwith spectral clustering. In Proc. of 15thISMIR Con-\nference , 2014.\n[10] M. M ¨uller. Information Retrieval for Music and Mo-\ntion. Springer Verlag, 2007.\n[11] M. M ¨uller, P. Grosche, and N. Jiang. A segment-based\nﬁtness measure for capturing repetitive structures of\nmusic recordings. Proc. of 12thISMIR Conference ,\npages 615–620, 2011.\n[12] M. M ¨uller and F. Kurth. Enhancing similarity for music\naudio analysis. Proc. of ICASSP , 2006.\n[13] C.S. Sapp. Online database of scores in the humdrum\nﬁle format. Proc. of 6thISMIR Conference , pages 664–\n665, 2005.[14] J. Serr `a, M. M ¨uller, P. Grosche, and J.Ll. Arcos. Unsu-\npervised detection of music boundaries by time series\nstructure features. Proc. of the Twenty-Sixth AAAI Con-\nference on Artiﬁcial Intelligence , 2012.\n[15] J. Serr `a, M. M ¨uller, P. Grosche, and J.Ll. Arcos. Un-\nsupervised music structure annotation by time series\nstructure features and segment similarity. IEEE Trans-\nactions on Multimedia , 16(5), 2014.\n[16] J. Serr `a, X. Serra, and R.G. Andrzejak. Cross recur-\nrence quantiﬁcation for cover song identiﬁcation. New\nJournal of Physics , 11(093017), 2009.\n[17] A. L. Wang. An industrial-strength audio search algo-\nrithm. In Proc. of 4thISMIR Conference , 2003.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 591"
    },
    {
        "title": "Improved Chord Recognition by Combining Duration and Harmonic Language Models.",
        "author": [
            "Filip Korzeniowski",
            "Gerhard Widmer"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492335",
        "url": "https://doi.org/10.5281/zenodo.1492335",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/300_Paper.pdf",
        "abstract": "Chord recognition systems typically comprise an acoustic model that predicts chords for each audio frame, and a temporal model that casts these predictions into labelled chord segments. However, temporal models have been shown to only smooth predictions, without being able to incorporate musical information about chord progressions. Recent research discovered that it might be the low hierarchical level such models have been applied to (directly on audio frames) which prevents learning musical relationships, even for expressive models such as recurrent neural networks (RNNs). However, if applied on the level of chord sequences, RNNs indeed can become powerful chord predictors. In this paper, we disentangle temporal models into a harmonic language model—to be applied on chord sequences—and a chord duration model that connects the chord-level predictions of the language model to the frame-level predictions of the acoustic model. In our experiments, we explore the impact of each model on the chord recognition score, and show that using harmonic language and duration models improves the results.",
        "zenodo_id": 1492335,
        "dblp_key": "conf/ismir/KorzeniowskiW18",
        "keywords": [
            "Chord recognition systems",
            "Acoustic model",
            "Temporal model",
            "Musical information",
            "Expressive models",
            "Recurrent neural networks",
            "Harmonic language model",
            "Chord duration model",
            "Temporal models",
            "Chord sequences"
        ],
        "content": "IMPROVED CHORD RECOGNITION BY COMBINING DURATION AND\nHARMONIC LANGUAGE MODELS\nFilip Korzeniowski and Gerhard Widmer\nInstitute of Computational Perception,\nJohannes Kepler University, Linz, Austria\nﬁlip.korzeniowski@jku.at\nABSTRACT\nChord recognition systems typically comprise an acoustic\nmodel that predicts chords for each audio frame, and a tem-\nporal model that casts these predictions into labelled chord\nsegments. However, temporal models have been shown to\nonly smooth predictions, without being able to incorporate\nmusical information about chord progressions. Recent re-\nsearch discovered that it might be the low hierarchical level\nsuch models have been applied to (directly on audio frames)\nwhich prevents learning musical relationships, even for ex-\npressive models such as recurrent neural networks (RNNs).\nHowever, if applied on the level of chord sequences, RNNs\nindeed can become powerful chord predictors. In this paper,\nwe disentangle temporal models into a harmonic language\nmodel—to be applied on chord sequences—and a chord\nduration model that connects the chord-level predictions of\nthe language model to the frame-level predictions of the\nacoustic model. In our experiments, we explore the impact\nof each model on the chord recognition score, and show that\nusing harmonic language and duration models improves the\nresults.\n1. INTRODUCTION\nChord recognition methods recognise and transcribe mu-\nsical chords from audio recordings. Chords are highly de-\nscriptive harmonic features that form the basis of many\nkinds of applications: theoretical, such as computational\nharmonic analysis of music; practical, such as automatic\nlead-sheet creation for musicians1or music tutoring sys-\ntems2; and ﬁnally, as basis for higher-level tasks such as\ncover song identiﬁcation or key classiﬁcation. Chord recog-\nnition systems face the two key problems of extracting\nmeaningful information from noisy audio, and casting this\ninformation into sensible output. These translate to acoustic\nmodelling (how to predict a chord label for each position or\nframe in the audio), and temporal modelling (how to create\n1https://chordify.net/\n2https://yousician.com\n©Filip Korzeniowski and Gerhard Widmer. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Filip Korzeniowski and Gerhard Widmer. “Improved Chord\nRecognition by Combining Duration and Harmonic Language Models”,\n19th International Society for Music Information Retrieval Conference,\nParis, France, 2018.meaningful segments of chords from these possibly volatile\nframe-wise predictions).\nAcoustic models extract frame-wise chord predictions,\ntypically in the form of a distribution over chord la-\nbels. Originally, these models were hand-crafted and split\ninto feature extraction and pattern matching, where the\nformer computed some form of pitch-class proﬁles (e.g.\n[26, 29, 33]), and the latter used template matching or Gaus-\nsian mixtures [6, 14] to model these features. Recently,\nhowever, neural networks became predominant for acoustic\nmodelling [18, 22, 23, 27]. These models usually compute a\ndistribution over chord labels directly from spectral repre-\nsentations and thus fuse both feature extraction and pattern\nmatching. Due to the discriminative power of deep neural\nnetworks, these models achieve superior results.\nTemporal models process the predictions of an acous-\ntic model and cast them into coherent chord segments.\nSuch models are either task-speciﬁc, such as hand-designed\nBayesian networks [26], or general models learned from\ndata. Here, it is common to use hidden Markov mod-\nels [8] (HMMs), conditional random ﬁelds [23] (CRFs),\nor recurrent neural networks (RNNs) [2, 32]. However,\nexisting models have shown only limited capabilities to\nimprove chord recognition results. First-order models are\nnot capable of learning meaningful musical relations, and\nonly smooth the predictions [4, 7]. More powerful mod-\nels, such as RNNs, do not perform better than their ﬁrst-\norder counterparts [24]. In addition to the fundamental ﬂaw\nof ﬁrst-order models (chord patterns comprise more than\ntwo chords) both approaches are limited by the low hier-\narchical level they are applied on: the temporal model is\nrequired to predict the next symbol for each audio frame.\nThis makes the model focus on short-term smoothing, and\nneglect longer-term musical relations between chords, be-\ncause, most of the time, the chord in the next audio frame\nis the same as in the current one. However, exploiting these\nlonger-term relations is crucial to improve the prediction\nof chords. RNNs, if applied on chord sequences , are capa-\nble of learning these relations, and become powerful chord\npredictors [21].\nOur contributions in this paper are as follows: i) we de-\nscribe a probabilistic model that allows for the integration\nof chord-level language models with frame-level acoustic\nmodels, by connecting the two using chord duration models;\nii) we develop and apply chord language models and chord\nduration models based on RNNs within this framework;10and iii) we explore how these models affect chord recogni-\ntion results, and show that the proposed integrated model\nout-performs existing temporal models.\n2. CHORD SEQUENCE MODELLING\nChord recognition is a sequence labelling task, i.e. we need\nto assign a categorical label yt2Y (a chord from a chord\nalphabet) to each member of the observed sequence xt(an\naudio frame), such that ytis the harmonic interpretation of\nthe music represented by xt. Formally,\n^y1:T= argmax\ny1:TP(y1:Tjx1:T): (1)\nAssuming a generative structure as shown in Fig. 1, the\nprobability distribution factorises as\nP(y1:Tjx1:T)/Y\nt1\nP(yt)PA(ytjxt)PT(ytjy1:t\u00001);\nwherePAis the acoustic model, PTthe temporal model,\nandP(yt)the label prior which we assume to be uniform\nas in [31].\ny1y2y3\u0001\u0001\u0001 yT\nx1 x2 x3 xT\nFigure 1 . Generative chord sequence model. Each chord\nlabelytdepends on all previous labels y1:t\u00001.\nThe temporal model PTpredicts the chord symbol of\neach audio frame. As discussed earlier, this prevents both\nﬁnite-context models (such as HMMs or CRFs) and unre-\nstricted models (such as RNNs) to learn meaningful har-\nmonic relations. To enable this, we disentangle PTinto a\nharmonic language model PLand a duration model PD,\nwhere the former models the harmonic progression of a\npiece, and the latter models the duration of chords.\nThe language model PLis deﬁned as PL(\u0016ykj\u0016y1:k\u00001),\nwhere \u0016y1:k=C(y1:t), andC(\u0001)is a sequence compression\nmapping that removes all consecutive duplicates of a chord\n(e.g.C((C;C;F;F;G )) = (C;F;G )). The frame-wise\nlabelsy1:tare thus reduced to chord changes, and PLcan\nfocus on modelling these.\nThe duration model PDis deﬁned as PD(stjy1:t\u00001),\nwherest2 fc;sgindicates whether the chord changes\n(c) or stays the same (s) at time t.PDthus only predicts\nwhether the chord will change or not, but not which chord\nwill follow—this is left to the language model PL. This\ndeﬁnition allows PDto consider the preceding chord labels\ny1:t\u00001; in practice, we restrict the model to only depend onPD(sjy1:t)PL\u0000\u0016yj\u0016y1:k\u00001\u0001\u0001\nPD(cjy1:t)\nt\u00001 t t+ 1\u0016yk\u00001\u0016yk\u0016yk+1\nAudio FramesChord Sequence\nFigure 2 . Chord-time lattice representing the temporal\nmodelPT, split into a language model PLand duration\nmodelPD. Here, \u0016y1:Krepresents a concrete chord se-\nquence. For each audio frame, we move along the time-axis\nto the right. If the chord changes, we move diagonally to\nthe upper right. This corresponds to the ﬁrst case in Eq. 2.\nIf the chord stays the same, we move only to the right. This\ncorresponds to the second case of the equation.\nthe preceding chord changes , i.e.PD(stjs1:t\u00001). Explor-\ning more complex models of harmonic rhythm is left for\nfuture work.\nUsing these deﬁnitions, the temporal model PTfac-\ntorises as\nPT(ytjy1:t\u00001) = (2)\n(\nPL(\u0016ykj\u0016y1:k\u00001)PD(cjy1:t\u00001)ifyt6=yt\u00001\nPD(sjy1:t\u00001) else:\nThe chord progression can then be interpreted as a path\nthrough a chord-time lattice as shown in Fig. 2.\nThis model cannot be decoded efﬁciently at test-time be-\ncause eachytdepends on all predecessors. We will thus use\neither models that restrict these connections to a ﬁnite past\n(such as higher-order Markov models) or use approximate\ninference methods for other models (such as RNNs).\n3. MODELS\nThe general model described above requires three sub-\nmodels: an acoustic model PAthat predicts a chord distri-\nbution from each audio frame, a duration model PDthat\npredicts when chords change, and a language model PL\nthat predicts the progression of chords in the piece.\n3.1 Acoustic Model\nThe acoustic model we use is a VGG-style convolutional\nneural network, similar to the one presented in [23]. It uses\nthree convolutional blocks: the ﬁrst consists of 4 layers of\n32 3\u00023 ﬁlters (with zero-padding in each layer), followed\nby2\u00021max-pooling in frequency; the second comprises\n2 layers of 64 such ﬁlters followed by the same pooling\nscheme; the third is a single layer of 128 12 \u00029 ﬁlters. Each\nof the blocks is followed by feature-map-wise dropout withProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 11h0 h1 h2\u0001\u0001\u0001 hK\nv(z0) v(z1) v(zK\u00001)P(z1jh1)P(z2jh2) P(zKjhK)\nFigure 3 . Sketch of a RNN used for next step prediction,\nwherezkrefers to an arbitrary categorical input, v(\u0001)is a\n(learnable) input embedding vector, and hkthe hidden state\nat stepk. Arrows denote matrix multiplications followed\nby a non-linear activation function. The input is padded\nwith a dummy input z0in the beginning. The network then\ncomputes the probability distribution for the next symbol.\nprobability 0.2, and each layer is followed by batch normal-\nisation [19] and an ELU activation function [10]. Finally, a\nlinear convolution with 25 1 \u00021 ﬁlters followed by global\naverage pooling and a softmax produces the chord class\nprobabilities PA(ytjxt).\nThe input to the network is a 1.5 s patch of a quarter-\ntone spectrogram computed using a logarithmically spaced\ntriangular ﬁlter bank. Concretely, we process the audio at\na sample rate of 44 100 Hz using the STFT with a frame\nsize of 8192 and a hop size of 4410. Then, we apply to\nthe magnitude of the STFT a triangular ﬁlter bank with 24\nﬁlters per octave between 65 Hz and2 100 Hz . Finally, we\ntake the logarithm of the resulting magnitudes to compress\nthe input range.\nNeural networks tend to produce over-conﬁdent pre-\ndictions, which in further consequence could over-rule\nthe predictions of a temporal model [9]. To mitigate\nthis, we use two techniques: ﬁrst, we train the model\nusing uniform smoothing (i.e. we assign a proportion\nof1\u0000\fto other classes during training); second, dur-\ning inference, we apply the temperature softmax function\n\u001b\u001c(z)j=ezj=\u001c=PK\nk=1ezk=\u001cinstead of the standard softmax\nin the ﬁnal layer. Higher values of \u001cproduce smoother\nprobability distributions. In this paper, we use \f= 0:9and\n\u001c= 1:3, as determined in preliminary experiments.\n3.2 Language Model\nThe language model PLpredicts the next chord, regardless\nof its duration, given the chord sequence it has previously\nseen. As shown in [21], RNN-based models perform bet-\nter than n-gram models at this task. We thus adopt this\napproach, and refer the reader to [21] for details.\nTo give an overview, we follow the set-up introduced\nby [28] and use a recurrent neural network for next-chord\nprediction. The network’s task is to compute a probability\ndistribution over all possible next chord symbols, given the\nchord symbols it has observed before. Figure 3 shows an\nRNN in a general next-step prediction task. In our case, the\ninputszkare the chord symbols given by C(y1:T).\nWe will describe in detail the network’s hyper-\nparameters in Section 4, where we will also evaluate theeffect the language models have on chord recognition.\n3.3 Duration Model\nThe duration model PDpredicts whether the chord will\nchange in the next time step. This corresponds to modelling\nthe duration of chords. Existing temporal models induce\nimplicit duration models: for example, an HMM implies an\nexponential chord duration distribution (if one state is used\nto model a chord), or a negative binomial distribution (if\nmultiple left-to-right states are used per chord). However,\nsuch duration models are simplistic, static, and do not adapt\nto the processed piece.\nAn explicit duration model has been explored in [4],\nwhere beat-synchronised chord durations were stored\nas discrete distributions. Their approach is useful for\nbeat-synchronised models, but impractical for frame-wise\nmodels—the probability tables would become too large,\nand data too sparse to estimate them. Since our approach\navoids the potentially error-prone beat synchronisation, the\napproach of [4] does not work in our case.\nInstead, we opt to use recurrent neural networks to model\nchord durations. These models are able to adapt to charac-\nteristics of the processed data [21], and have shown great\npotential in processing periodic signals [1] (and chords\ndo change periodically within a piece). To train an RNN-\nbased duration model, we set up a next-step-prediction\ntask, identical in principle to the set-up for harmonic lan-\nguage modelling: the network has to compute the proba-\nbility of a chord change in the next time step, given the\nchord changes it has seen in the past. We thus simplify\nPD(stjy1:t\u00001)b=PD(stjs1:t\u00001), as mentioned earlier.\nAgain, see Fig. 3 for an overview (for duration modelling,\nreplacezkwithst).\nIn Section 4, we will describe in detail the hyper-\nparameters of the networks we employed, and compare the\nproperties of various settings to baseline duration models.\nWe will also assess the impact on the duration modelling\nquality on the ﬁnal chord recognition result.\n3.4 Model Integration\nDynamic models such as RNNs have one main advantage\nover their static counter-parts (e.g. n-gram models for\nlanguage modelling or HMMs for duration modelling): they\nconsider all previous observations when predicting the next\none. As a consequence, they are able to adapt to the piece\nthat is currently processed—they assign higher probabilities\nto sub-sequences of chords they have seen earlier [21], or\npredict chord changes according to the harmonic rhythm of\na song (see Sec. 4.3). The ﬂip side of the coin is, however,\nthat this property prohibits the use of dynamic programming\napproaches for efﬁcient decoding. We cannot exactly and\nefﬁciently decode the best chord sequence given the input\naudio.\nHence we have to resort to approximate inference. In par-\nticular, we employ hashed beam search [32] to decode the\nchord sequence. General beam search restricts the search\nspace by keeping only the Nbbest solutions up to the cur-\nrent time step. (In our case, the Nbbest paths through12 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018all possible chord-time lattices, see Fig. 2.) However, as\npointed out in [32], the beam might saturate with almost\nidentical solutions, e.g. the same chord sequence differing\nonly marginally in the times the chords change. Such patho-\nlogical cases may impair the ﬁnal estimate. To mitigate\nthis problem, hashed beam search forces the tracked solu-\ntions to be diverse by pruning similar solutions with lower\nprobability.\nThe similarity of solutions is determined by a task-\nspeciﬁc hash function . For our purpose, we deﬁne the\nhash function of a solution to be the last Nhchord sym-\nbols in the sequence, regardless of their duration; formally,\nthe hash function fh(y1:t) = \u0016y(k\u0000Nh):k. (Recall that\n\u0016y1:k=C(y1:t).) In contrast to the hash function originally\nproposed in [32], which directly uses y(t\u0000Nh):t, our formu-\nlation ensures that sequences that differ only in timing, but\nnot in chord sequence, are considered similar.\nTo summarise, we approximately decode the optimal\nchord transcription as deﬁned in Eq. 1 using hashed beam\nsearch, which at each time step keeps the best Nbsolutions,\nand at mostNssimilar solutions.\n4. EXPERIMENTS\nIn our experiments, we will ﬁrst evaluate harmonic language\nand duration models individually. Here, we will compare\nthe proposed models to common baselines. Then, we will\nintegrate these models into the chord recognition framework\nwe outlined in Section 2, and evaluate how the individual\nparts interact in terms of chord recognition score.\n4.1 Data\nWe use the following datasets in 4-fold cross-validation. Iso-\nphonics3:180 songs by the Beatles, 19 songs by Queen,\nand 18 songs by Zweieck, 10:21 hours of audio; RWC Pop-\nular [15]: 100 songs in the style of American and Japanese\npop music, 6:46 hours of audio; Robbie Williams [13]: 65\nsongs by Robbie Williams, 4:30 of audio; and McGill Bill-\nboard [3]: 742 songs sampled from the American billboard\ncharts between 1958 and 1991, 44:42 hours of audio. The\ncompound dataset thus comprises 1125 unique songs, and\na total of 66:21 hours of audio.\nFurthermore, we used the following data sets (with dupli-\ncate songs removed) as additional data for training the lan-\nguage and duration models: 173 songs from the Rock [11]\ncorpus; a subset of 160 songs from UsPop20024for which\nchord annotations are available5; 291 songs from Weimar\nJazz6, with chord annotations taken from lead sheets of\nJazz standards; and Jay Chou [12], a small collection of 29\nChinese pop songs.\nWe focus on the major/minor chord vocabulary, and\nfollowing [7], map all chords containing a minor third to\nminor, and all others to major. This leaves us with 25\nclasses: 12 root notes\u0002fmajor;minorgand the ‘no- chord’\nclass.\n3http://isophonics.net/datasets\n4https://labrosa.ee.columbia.edu/projects/musicsim/uspop2002.html\n5https://github.com/tmc323/Chord-Annotations\n6http://jazzomat.hfm-weimar.de/dbformat/dboverview.htmlGRU-512 GRU-32 4-gram 2-gram\nlog-P −1.293 −1.576 −1.887 −2.393\nTable 1 . Language model results: average log-probability\nof the correct next chord computed by each model.\n4.2 Language Models\nThe performance of neural networks depends on a good\nchoice of hyper-parameters, such as number of layers, num-\nber of units per layer, or unit type (e.g. vanilla RNN, gated\nrecurrent unit (GRU) [5] or long short-term memory unit\n(LSTM) [17]). The ﬁndings in [21] provide a good start-\ning point for choosing hyper-parameter settings that work\nwell. However, we strive to ﬁnd a simpler model to re-\nduce the computational burden at test time. To this end,\nwe perform a grid search in a restricted search space, us-\ning the validation score of the ﬁrst fold. We search over\nthe following settings: number of layers 2f1;2;3g, num-\nber of units2 f256;512g, unit type2 fGRU;LSTMg,\ninput embedding2fone-hot;R8;R16;R24g, learning rate\n2f0:001;0:005g, and skip connections 2fon;offg. Other\nhyper-parameters were ﬁxed for all trials: we train the net-\nworks for 100 epochs using stochastic gradient descent with\nmini-batches of size 4, employ the Adam update rule [20],\nand starting from epoch 50, linearly anneal the learning rate\nto 0.\nTo increase the diversity in the training data, we use two\ndata augmentation techniques, applied each time we show a\npiece to the network. First, we randomly shift the key of the\npiece; the network can thus learn that harmonic relations\nare independent of the key, as in roman numeral analysis.\nSecond, we select a sub-sequence of random length instead\nof the complete chord sequence; the network thus has to\nlearn to cope with varying context sizes.\nThe best model turned out to be a single-layer network\nof 512 GRUs, with a learnable 16-dimensional input embed-\nding and without skip connections, trained using a learning\nrate of 0.0057. We compare this model and a smaller, but\notherwise identical RNN with 32 units, to two baselines:\na 2-gram model, and a 4-gram model. Both can be used\nfor chord recognition in a higher-order HMM [25]. We\ntrain the n-gram models using maximum likelihood estima-\ntion with Lidstone smoothing as described in [21], using\nthe key-shift data augmentation technique (sub-sequence\ncropping is futile for ﬁnite context models). As evaluation\nmeasure, we use the average log-probability of predicting\nthe correct next chord. Table 1 presents the test results. The\nGRU models predict chord sequences with much higher\nprobability than the baselines.\nWhen we look into the input embedding v(\u0001), which was\nlearned by the RNN during training from a random initiali-\nsation, we observe an interesting positioning of the chord\nsymbols (see Figure 4). We found that similar patterns de-\nvelop for all 1-layer GRUs we tried, and these patterns are\nconsistent for all folds we trained on. We observe i) that\n7Due to space constraints, we cannot present the complete grid search\nresults.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 13NC\nD[D\nE[\nEF\nF]\nGA[\nA\nB[B\nc\nd[d\ne[\nef\nf]\nga[\na\nb[b\nFigure 4 . Chord embedding projected into 2D using PCA\n(left); Tonnetz of triads (right). The “no-chord” class resides\nin the center of the embedding. Major chords are upper-case\nand orange, minor chords lower-case and blue. Clusters in\nthe projected embedding and the corresponding positions in\nthe Tonnetz are marked in color. If projected into 3D (not\nshown here), the chord clusters split into a lower and upper\nhalf of four chords each. The chords in the lower halves are\nshaded in the Tonnetz representation.\nchords form three clusters around the center, in which the\nminor chords are farther from the center than major chords;\nii) that the clusters group major and minor chords with the\nsame root, and the distance between the roots are minor\nthirds (e.g. C, E [, F], A); iii) that clockwise movement\nin the circle of ﬁfths corresponds to clockwise movement\nin the projected embedding; and iv) that the way chords\nare grouped in the embedding corresponds to how they are\nconnected in the Tonnetz.\nAt this time, we cannot provide an explanation for these\nautomatically emerging patterns. However, they warrant a\nfurther investigation to uncover why this speciﬁc arrange-\nment seems to beneﬁt the predictions of the model.\n4.3 Duration Models\nAs for the language model, we performed a grid search\non the ﬁrst fold to ﬁnd good choices for the recurrent unit\ntype2fvanilla RNN ;GRU;LSTMg, and number of recur-\nrent units2f16;32;64;128;256gfor the LSTM and GRU,\nandf128;256;512gfor the vanilla RNN. We use only one\nrecurrent layer for simplicity. We found networks of 256\nGRU units to perform best; although this indicates that even\nbigger models might give better results, for the purposes of\nthis study, we think that this conﬁguration is a good balance\nbetween prediction quality and model complexity.\nThe models were trained for 100 epochs using the Adam\nupdate rule [20] with a learning rate linearly decreasing\nfrom 0.001 to 0. The data was processed in mini-batches of\n10, where the sequences were cut in excerpts of 200 time\nsteps ( 20 s). We also applied gradient clipping at a value of\n0.001 to ensure a smooth learning progress.\nWe compare the best RNN-based duration model with\ntwo baselines. The baselines are selected because both are\nimplicit consequences of using HMMs as temporal model,\nas it is common in chord recognition. We assume a single\nparametrisation for each chord; this ostensible simpliﬁca-\ntion is justiﬁed, because simple temporal models such as\nHMMs do not proﬁt from chord information, as shown55 60 65 70 75 80\nTime[s]0.00.10.20.3PD(stjs1:t\u00001)NegativeBinomial GRU-16 GRU-128\nFigure 5 . Probability of chord change computed by differ-\nent models. Gray vertical dashed lines indicate true chord\nchanges.\nGRU-256 GRU-16 Neg. Binom. Exp.\nlog-P −2.014 −2.868 −3.946 −4.003\nTable 2 . Duration model results: average log-probability of\nchord durations computed by each model.\nby [4, 7]. The ﬁrst baseline we consider is a negative bi-\nnomial distribution . It can be modelled by a HMM us-\ningnstates per chord, connected in a left-to-right manner,\nwith transitions of probability pbetween the states (self-\ntransitions thus have probability 1\u0000p). The second, a\nspecial case of the ﬁrst with n= 1, is an exponential distri-\nbution ; this is the implicit duration distribution used by all\nchord recognition models that employ a simple 1-state-per-\nchord HMM as temporal model. Both baselines are trained\nusing maximum likelihood estimation.\nTo measure the quality of a duration model, we consider\nthe average log-probability it assigns to a chord duration.\nThe results are shown in Table 2. We further added results\nfor the simplest GRU model we tried—using only 16 recur-\nrent units—to indicate the performance of small models of\nthis type. We will also use this simple model when judg-\ning the effect of duration modelling on the ﬁnal result in\nSec. 4.4. As seen in the table, both GRU models clearly\nout-perform the baselines.\nFigure 5 shows the reason why the GRU performs so\nmuch better than the baselines: as a dynamic model, it\ncan adapt to the harmonic rhythm of a piece, while static\nmodels are not capable of doing so. We see that a GRU with\n128 units predicts chord changes with high probability at\nperiods of the harmonic rhythm. It also reliably remembers\nthe period over large gaps in which the chord did not change\n(between seconds 61 and 76). During this time, the peaks\ndecay differently for different multiples of the period, which\nindicates that the network simultaneously tracks multiple\nperiods of varying importance. In contrast, the negative\nbinomial distribution statically yields a higher chord change\nprobability that rises with the number of audio frames since\nthe last chord change. Finally, the smaller GRU model with\nonly 16 units also manages to adapt to the harmonic rhythm;\nhowever, its predictions between the peaks are noisier, and\nit fails to remember the period correctly in the time without\nchord changes.14 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Model Root Maj/Min Seg.\n2-gram / neg. binom. 0.812 0.795 0.804\nGRU-512 / GRU-256 0.821 0.805 0.814\nTable 3 . Results of the standard model (2-gram language\nmodel with negative binomial durations) compared to the\nbest one (GRU language and duration models).\n4.4 Integrated Models\nThe individual results for the language and duration models\nare encouraging, but only meaningful if they translate to\nbetter chord recognition scores. This section will thus eval-\nuate if and how the duration and language models affect the\nperformance of a chord recognition system.\nThe acoustic model used in these experiments was\ntrained for 300 epochs (with 200 parameter updates per\nepoch) using a mini-batch size of 512 and the Adam up-\ndate rule with standard parameters. We linearly decay the\nlearning rate to 0 in the last 100 epochs.\nWe compare all combinations of language and duration\nmodels presented in the previous sections. For language\nmodelling, these are the GRU-512, GRU-32, 4-gram, and\n2-gram models; for duration modelling, these are the GRU-\n256, GRU-16, and negative binomial models. (We leave out\nthe exponential model, because its results differ negligibly\nfrom the negative binomial one). The models are decoded\nusing the Hashed Beam Search algorithm, as described in\nSec. 3.4: we use a beam width of Nb= 25 , where we\ntrack at most Ns= 4 similar solutions as deﬁned by the\nhash function fh, where the number of chords considered\nis set toNh= 5. These values were determined by a small\nnumber of preliminary experiments.\nAdditionally, we evaluate exact decoding results for the\nn-gram language models in combination with the negative\nbinomial duration distribution. This will indicate how much\nthe results suffer due to the approximate beam search.\nAs main evaluation metric, we use the weighted chord\nsymbol recall (WCSR) over the major/minor chord alpha-\nbet, as deﬁned in [30]. We thus compute WCSR =tc=ta,\nwheretcis the total duration of chord segments that have\nbeen recognised correctly, and tais the total duration of\nchord segments annotated with chords from the target al-\nphabet. We also report chord root accuracy and a measure\nof segmentation (see [16], Sec. 8.3). Table 3 compares the\nresults of the standard model (the combination that implic-\nitly emerges in simple HMM-based temporal models) to the\nbest model found in this study. Although the improvements\nare modest, they are consistent, as shown by a paired t-test\n(p<2:487\u000210\u000023for all differences).\nFigure 6 presents the effects of duration and language\nmodels on the WCSR. Better language and duration models\ndirectly improve chord recognition results, as the WCSR\nincreases linearly with higher log-probability of each model.\nAs this relationship does not seem to ﬂatten out, further\nimprovement of each model type can still increase the score.\nWe also observe that the approximate beam search does\nnot impair the result by much compared to exact decoding\n(compare the dotted blue line with the solid one).2.393 1.887 1.576 1.293\nLanguageModelLog-P0.7960.7980.8000.8020.804WCSR(maj/min)DurationModel\nNeg.Binomial,Exact\nNeg.Binomial\nGRU-16\nGRU-2562-Gram 4-Gram GRU-32 GRU-512LanguageModel\n3.946 2.979 2.014\nDurationModelLog-P0.7960.7980.8000.8020.804WCSR(maj/min)LanguageModel\n2-Gram\n4-Gram\nGRU-32GRU-512\n2-Gram,Exact\n4-Gram,ExactNeg.Binomial GRU-16 GRU-256DurationModel\nFigure 6 . Effect of language and duration models on the\nﬁnal result. Both plots show the same results from different\nperspectives.\n5. CONCLUSION AND DISCUSSION\nWe described a probabilistic model that disentangles three\ncomponents of a chord recognition system: the acoustic\nmodel, the duration model, and the language model. We\nthen developed better duration and language models than\nhave been used for chord recognition, and illustrated why\nthe RNN-based duration models perform better and are\nmore meaningful than their static counterparts implicitly\nemployed in HMMs. (For a similar investigation for chord\nlanguage models, see [21].) Finally, we showed that im-\nprovements in each of these models directly inﬂuence chord\nrecognition results.\nWe hope that our contribution facilitates further research\nin harmonic language and duration models for chord recog-\nnition. These aspects have been neglected because they did\nnot show great potential for improving the ﬁnal result [4, 7].\nHowever, we believe (see [24] for some evidence) that this\nwas due to the improper assumption that temporal models\napplied on the time-frame level can appropriately model\nmusical knowledge. The results in this paper indicate that\nchord transitions modelled on the chord level, and con-\nnected to audio frames via strong duration models, indeed\nhave the capability to improve chord recognition results.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 156. ACKNOWLEDGEMENTS\nThis work is supported by the European Research Coun-\ncil (ERC) under the EU’s Horizon 2020 Framework Pro-\ngramme (ERC Grant Agreement number 670035, project\n“Con Espressione”).\n7. REFERENCES\n[1]Sebastian B ¨ock and Markus Schedl. Enhanced Beat\nTracking With Context-Aware Neural Networks. In\n14th International Conference on Digital Audio Effects\n(DAFx-11) , Paris, France, September 2011.\n[2]Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Audio Chord Recognition With Recur-\nrent Neural Networks. In 14th International Society\nfor Music Information Retrieval Conference (ISMIR) ,\nCuritiba, Brazil, November 2013.\n[3]John Ashley Burgoyne, Jonathan Wild, and Ichiro Fu-\njinaga. An Expert Ground Truth Set for Audio Chord\nRecognition and Music Analysis. In 12th International\nSociety for Music Information Retrieval Conference (IS-\nMIR) , Miami, USA, October 2011.\n[4]Ruofeng Chen, Weibin Shen, Ajay Srinivasamurthy,\nand Parag Chordia. Chord Recognition Using Duration-\nExplicit Hidden Markov Models. In 13th International\nSociety for Music Information Retrieval Conference\n(ISMIR) , Porto, Portugal, October 2012.\n[5]Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. On the Properties of Neural\nMachine Translation: Encoder-Decoder Approaches. In\nEighth Workshop on Syntax, Semantics and Structure\nin Statistical Translation (SSST-8), arXiv:1409.1259 ,\nDoha, Qatar, October 2014.\n[6]Taemin Cho. Improved Techniques for Automatic Chord\nRecognition from Music Audio Signals . Dissertation,\nNew York University, New York, 2014.\n[7]Taemin Cho and Juan P. Bello. On the Relative Impor-\ntance of Individual Components of Chord Recognition\nSystems. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing , 22(2):477–492, February\n2014.\n[8]Taemin Cho, Ron J Weiss, and Juan Pablo Bello. Ex-\nploring Common Variations in State Of The Art Chord\nRecognition Systems. In Proceedings of the Sound and\nMusic Computing Conference (SMC) , Barcelona, Spain,\nJuly 2010.\n[9]Jan Chorowski and Navdeep Jaitly. Towards better de-\ncoding and language model integration in sequence to\nsequence models. arXiv:1612.02695 , December 2016.\n[10] Djork-Arn ´e Clevert, Thomas Unterthiner, and Sepp\nHochreiter. Fast and Accurate Deep Network Learn-\ning by Exponential Linear Units (ELUs). In Interna-\ntional Conference on Learning Representations (ICLR),arXiv:1511.07289 , San Juan, Puerto Rico, February\n2016.\n[11] Trevor de Clercq and David Temperley. A corpus anal-\nysis of rock harmony. Popular Music , 30(01):47–70,\nJanuary 2011.\n[12] Junqi Deng and Yu-Kwong Kwok. Automatic Chord es-\ntimation on seventhsbass Chord vocabulary using deep\nneural network. In 2016 IEEE International Conference\non Acoustics Speech and Signal Processing (ICASSP) ,\nShanghai, China, March 2016.\n[13] Bruno Di Giorgi, Massimiliano Zanoni, Augusto Sarti,\nand Stefano Tubaro. Automatic chord recognition based\non the probabilistic modeling of diatonic modal har-\nmony. In Proceedings of the 8th International Work-\nshop on Multidimensional Systems , Erlangen, Germany,\nSeptember 2013.\n[14] Takuya Fujishima. Realtime Chord Recognition of Mu-\nsical Sound: A System Using Common Lisp Music.\nInProceedings of the International Computer Music\nConference (ICMC) , Beijing, China, October 1999.\n[15] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC Music Database:\nPopular, Classical and Jazz Music Databases. In\n3rd International Conference on Music Information\nRetrieval (ISMIR) , Paris, France, 2002.\n[16] Christopher Harte. Towards Automatic Extraction of\nHarmony Information from Music Signals . Dissertation,\nDepartment of Electronic Engineering, Queen Mary,\nUniversity of London, London, United Kingdom, 2010.\n[17] Sepp Hochreiter and J ¨urgen Schmidhuber. Long Short-\nTerm Memory. Neural Computation , 9(8):1735–1780,\nNovember 1997.\n[18] Eric J. Humphrey and Juan P. Bello. Rethinking Au-\ntomatic Chord Recognition with Convolutional Neural\nNetworks. In 11th International Conference on Machine\nLearning and Applications (ICMLA) , Boca Raton, USA,\nDecember 2012.\n[19] Sergey Ioffe and Christian Szegedy. Batch Normaliza-\ntion: Accelerating Deep Network Training by Reduc-\ning Internal Covariate Shift. arXiv:1502.03167 , March\n2015.\n[20] Diederik Kingma and Jimmy Ba. Adam: A Method for\nStochastic Optimization. In International Conference\non Learning Representations (ICLR), arXiv:1412.6980 ,\nSan Diego, USA, May 2015.\n[21] Filip Korzeniowski, David R. W. Sears, and Gerhard\nWidmer. A Large-Scale Study of Language Models for\nChord Prediction. In 2018 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , Calgary, Canada, April 2018.16 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[22] Filip Korzeniowski and Gerhard Widmer. Feature Learn-\ning for Chord Recognition: The Deep Chroma Extractor.\nIn17th International Society for Music Information Re-\ntrieval Conference (ISMIR) , New York, USA, August\n2016.\n[23] Filip Korzeniowski and Gerhard Widmer. A Fully Con-\nvolutional Deep Auditory Model for Musical Chord\nRecognition. In 26th IEEE International Workshop\non Machine Learning for Signal Processing (MLSP) ,\nSalerno, Italy, September 2016.\n[24] Filip Korzeniowski and Gerhard Widmer. On the Futil-\nity of Learning Complex Frame-Level Language Mod-\nels for Chord Recognition. In Proceedings of the AES\nInternational Conference on Semantic Audio , Erlangen,\nGermany, June 2017.\n[25] Filip Korzeniowski and Gerhard Widmer. Automatic\nChord Recognition with Higher-Order Harmonic Lan-\nguage Modelling. In 26th European Signal Processing\nConference (EUSIPCO) , Rome, Italy, September 2018.\n[26] M. Mauch and S. Dixon. Simultaneous Estimation of\nChords and Musical Context From Audio. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n18(6):1280–1289, August 2010.\n[27] Brian McFee and Juan Pablo Bello. Structured Train-\ning for Large-V ocabulary Chord Recognition. In 18th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , Suzhou, China, October 2017.\n[28] Tomas Mikolov, Martin Karaﬁ ´at, Luk ´as Burget, Jan Cer-\nnock ´y, and Sanjeev Khudanpur. Recurrent neural net-\nwork based language model. In INTERSPEECH 2010 ,\npages 1045–1048, Chiba, Japan, September 2010.\n[29] Meinard M ¨uller, Sebastian Ewert, and Sebastian\nKreuzer. Making chroma features more robust to tim-\nbre changes. In 2009 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\nTaipei, Taiwan, April 2009.\n[30] Johan Pauwels and Geoffroy Peeters. Evaluating au-\ntomatically estimated chord sequences. In 2013 IEEE\nInternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP) , May 2013.\n[31] S. Renals, N. Morgan, H. Bourlard, M. Cohen, and\nH. Franco. Connectionist Probability Estimators in\nHMM Speech Recognition. IEEE Transactions on\nSpeech and Audio Processing , 2(1):161–174, January\n1994.\n[32] Siddharth Sigtia, Nicolas Boulanger-Lewandowski, and\nSimon Dixon. Audio Chord Recognition With A Hy-\nbrid Recurrent Neural Network. In 16th International\nSociety for Music Information Retrieval Conference (IS-\nMIR) , M´alaga, Spain, October 2015.[33] Yushi Ueda, Yuki Uchiyama, Takuya Nishimoto, Nobu-\ntaka Ono, and Shigeki Sagayama. HMM-based Ap-\nproach for Automatic Chord Detection Using Reﬁned\nAcoustic Features. In 2010 IEEE International Con-\nference on Acoustics Speech and Signal Processing\n(ICASSP) , Dallas, USA, March 2010.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 17"
    },
    {
        "title": "Genre-Agnostic Key Classification With Convolutional Neural Networks.",
        "author": [
            "Filip Korzeniowski",
            "Gerhard Widmer"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492399",
        "url": "https://doi.org/10.5281/zenodo.1492399",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/7_Paper.pdf",
        "abstract": "We propose modifications to the model structure and training procedure to a recently introduced Convolutional Neural Network for musical key classification. These modifications enable the network to learn a genre-independent model that performs better than models trained for specific music styles, which has not been the case in existing work. We analyse this generalisation capability on three datasets comprising distinct genres. We then evaluate the model on a number of unseen data sets, and show its superior performance compared to the state of the art. Finally, we investigate the model's performance on short excerpts of audio. From these experiments, we conclude that models need to consider the harmonic coherence of the whole piece when classifying the local key of short segments of audio.",
        "zenodo_id": 1492399,
        "dblp_key": "conf/ismir/KorzeniowskiW18a",
        "keywords": [
            "Convolutional Neural Network",
            "genre-independent model",
            "improvement over specific styles",
            "three distinct genres",
            "unseen data sets",
            "superior performance compared to state of the art",
            "harmonic coherence",
            "local key classification",
            "short excerpts of audio",
            "conclusion"
        ],
        "content": "GENRE-AGNOSTIC KEY CLASSIFICATION\nWITH CONVOLUTIONAL NEURAL NETWORKS\nFilip Korzeniowski and Gerhard Widmer\nInstitute of Computational Perception,\nJohannes Kepler University, Linz, Austria\nﬁlip.korzeniowski@jku.at\nABSTRACT\nWe propose modiﬁcations to the model structure and train-\ning procedure to a recently introduced Convolutional Neu-\nral Network for musical key classiﬁcation. These modiﬁ-\ncations enable the network to learn a genre-independent\nmodel that performs better than models trained for speciﬁc\nmusic styles, which has not been the case in existing work.\nWe analyse this generalisation capability on three datasets\ncomprising distinct genres. We then evaluate the model\non a number of unseen data sets, and show its superior\nperformance compared to the state of the art. Finally, we\ninvestigate the model’s performance on short excerpts of\naudio. From these experiments, we conclude that models\nneed to consider the harmonic coherence of the whole piece\nwhen classifying the local key of short segments of audio.\n1. INTRODUCTION\nThe musical key is the highest-level harmonic representa-\ntion in Western tonal music. It thus plays a central role in\nunderstanding the semantic content of a piece. Such under-\nstanding drives not only theoretical analyses of music, but is\nalso relevant for modern music creators, who mix samples\nfrom various different pieces that ﬁt well harmonically into\na new composition. However, deriving the key of a musical\npiece is a demanding task that only experts can perform. It\nis thus impractical to annotate large music collections by\nhand. Therefore, we need computational key classiﬁcation\nsystems.\nMost key classiﬁcation systems (e.g. [9, 16, 17, 21]) con-\nform to the same principle: they extract a time-frequency\nrepresentation of the audio, ﬁlter out nuisances, map this\nrepresentation to chroma vectors, and accumulate them\nover time. The resulting feature vector is then compared to\ntemplate vectors for each key. The drawbacks of such ap-\nproaches include that key templates differ for different mu-\nsical genres [9] and favour one key mode over another [1].\nThis leads to key classiﬁcation systems that perform well\nonly on the musical styles they were designed for. Although\n©Filip Korzeniowski and Gerhard Widmer. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Filip Korzeniowski and Gerhard Widmer. “Genre-Agnostic\nKey Classiﬁcation With Convolutional Neural Networks”, 19th Interna-\ntional Society for Music Information Retrieval Conference, Paris, France,\n2018.there are attempts to address these issues [2], ideally, we\nwould want a model that handles different kinds of input\nautonomously, and does not need human intervention to e.g.\nbalance mode probabilities.\nData-driven methods bear the potential to meet this re-\nquirement. Recently, an end-to-end neural-network-based\nkey classiﬁcation model was introduced [14]. Although it\ngeneralised better across musical genres than hand-crafted\napproaches, it still achieved the best results when tuned\nspeciﬁcally for a musical style. In this paper, we present\nmodiﬁcations to the model structure and its training pro-\ncedure that enable the model to learn a key classiﬁer that\nis agnostic to genre. Not only does it perform better than\nthe model proposed in [14] on all genres the latter is opti-\nmised for; it does so not despite, but because it is trained\non various musical styles, instead of a speciﬁc one (see\nSec. 3.4).\n2. METHOD\nWe build upon the same audio processing pipeline used\nin [14], and input to the network a log-magnitude log-\nfrequency spectrogram (5 frames per second, frame size\n8 192 , sample rate 44 100 Hz , 24 bins per octave). We limit\nthe frequency range to the harmonically most relevant 65 Hz\nto 2 100 Hz, as found in [12].\nThe network structure proposed in [14] was modelled\nafter typical processing pipelines used for key classiﬁcation.\nIt features ﬁve convolutional layers of 5\u00025kernels for\nspectrogram processing, followed by a dense projection\ninto a frame-wise embedding space, which is then averaged\nover time and classiﬁed using a softmax layer. All layers\nexcept the last use the exponential-linear activation func-\ntion [4] (ELU). The architecture, which we name KeyNet ,\nis summarised in Table 1a.\nDuring training, the model is shown the complete spec-\ntrogram of a piece. Its weights are then adapted using\nstochastic gradient descent to minimise the categorical\ncross-entropy between the predicted key distribution and\nthe ground truth. We will refer to the KeyNet architecture,\nwhen trained using full spectrograms, as KeyNet/F .\n2.1 Adaptations of the Training Procedure\nThe outlined training scheme has two drawbacks. First, the\ncomputation of a single update is expensive; the network\nhas to process the full spectrogram (e.g. 600\u0002105values for264a two-minute piece), and keep intermediate results for back-\npropagating the error. Training is thus slow and requires\nmuch memory. Second, it keeps the variety of the data lower\nthan necessary, as the network sees the same spectrograms\nat every epoch.\nTo circumvent these drawbacks, we show the network\nonly short snippets instead of the whole piece at train-\ning time (similar to random cropping in computer vision).\nThese snippets should be as short as possible to reduce com-\nputation time, but have to be long enough to contain the\nrelevant information to determine the key of a piece. From\nour datasets, we found 20 sto be sufﬁcient (with the excep-\ntion of classical music, which we need to treat differently,\ndue to the possibility of extended periods of modulation—\nsee Sec. 3.1 below). Each time the network is presented a\nsong, we cut a random 20 ssnippet from the spectrogram.\nThe network thus sees a different variation of each song\nevery epoch.\nDuring testing, the network processes the whole piece.\nThis gives better results than when using only a snippet.\nSince we do not have to store intermediate results and pro-\ncess each piece many times as in training, memory space\nand run time are not an issue. We will refer to KeyNet\nmodels trained using spectrogram snippets as KeyNet/S .\nWe expect this modiﬁcation to have the following effects.\na) Back-propagation will be faster and require less memory,\nbecause the network sees shorter snippets; we can thus\ntrain faster, and process larger models. b) The network\nwill be less prone to over-ﬁtting, since it almost never sees\nthe same training input; we expect the model to generalise\nbetter. c) The network will be forced to ﬁnd evidence for a\nkey in each excerpt of the training pieces, instead of relying\non parts where the key is more obvious; by asking more of\nthe model, we expect it to pick up more subtle relationships\nbetween the audio and its key.\n2.2 Adaptations of the Model Structure\nThe KeyNet architecture uses a dense layer to project the\nprocessed spectrogram into a key embedding space. In\nits original formulation, which uses an embedding space\nwith 48 dimensions and 8 feature maps in the convolutional\nlayers, this projection accounts for 65 % of the network’s\nparameters. Dense layers are also more prone to over-ﬁtting\nthan convolutional layers.\nWe thus propose to use a network architecture that does\naway with dense layers, and relies on convolutions and\npooling only. At the same time, we move away from mod-\nelling the network based on traditional key classiﬁcation\nmethods—recall that the components of KeyNet were de-\nsigned to correspond to components in typical key classi-\nﬁcation pipelines—and instead use a general network ar-\nchitecture for classiﬁcation, based on the all-convolutional\nnet [19]. The new architecture is summarised in Table 1b,\nand will be referred to as AllConv . As with KeyNet/S, we\nwill train this architecture only with the snippet method.\nWe expect this change to improve results and generalisa-\ntion because a) convolutional layers over-ﬁt less than dense\nlayers; b) given the same number of parameters, deeper(a) KeyNet Architecture\nLayer Type FMaps Params\nInput\nConv-ELU Nf 5\u00025\nConv-ELU Nf 5\u00025\nConv-ELU Nf 5\u00025\nConv-ELU Nf 5\u00025\nConv-ELU Nf 5\u00025\nDense-ELU 2\u0001Nf\nPool-Time Avg.\nDense-Softmax 24(b) AllConv Architecture\nLayer Type FMaps Params\nInput\nConv-ELU Nf 5\u00025\nConv-ELU Nf 3\u00023\nPool-Max 2\u00022\nConv-ELU 2Nf 3\u00023\nConv-ELU 2Nf 3\u00023\nPool-Max 2\u00022\nConv-ELU 4Nf 3\u00023\nConv-ELU 4Nf 3\u00023\nPool-Max 2\u00022\nConv-ELU 8Nf 3\u00023\nConv-ELU 8Nf 3\u00023\nConv-ELU 24 1 \u00021\nPool-Global Avg.\nSoftmax\nTable 1 . Neural Network architectures. Nfis a parameter\nthat controls the model complexity. Horizontal lines denote\ndropout layers [20]. Here, dropout is applied on complete\nfeature maps, not individual units. Each convolution is\nfollowed by batch normalisation [11]. FMaps indicates the\nnumber of feature maps, while Params the parameters of\nthe layer (kernel size, pool size, or number of units).\nnetworks are more expressive than shallower ones [8, 15];\nc) comparable architectures have shown to perform well in\nother audio-related tasks [7, 13].\n3. EXPERIMENTS\nWe ﬁrst evaluate how the proposed modiﬁcations affect\nthe key classiﬁcation performance in Sec. 3.3. Then, we\nanalyse how the number and genre of training data inﬂuence\nresults in Sec. 3.4.\n3.1 Data\nSince we are interested in how well the models generalise\nacross different genres, we use datasets that encompass\nthree distinct musical styles. As in [14], we apply pitch\nshifting in the range of -4 to +7 semitones to increase the\namount of training data.\nElectronic Dance Music: Here, we use songs from the\nGiantSteps MTG Key dataset1, collected by ´Angel\nFaraldo. It comprises 1486 distinct two-minute audio\npreviews from www.beatport.com, with key ground\ntruth for each excerpt. We only use excerpts labelled\nwith a single key and a high conﬁdence (1077 pieces),\nand split them into 80 % training and 20 % validation.\nFor testing, we use the GiantSteps Key Dataset2. It\ncomprises 604 two-minute audio previews from the\nsame source (but distinct from the training set).\n1https://github.com/GiantSteps/giantsteps-mtg-key-dataset\n2https://github.com/GiantSteps/giantsteps-key-datasetProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 265Pop/Rock Music: For this genre, we use the McGill Bill-\nboard dataset [3]3. It consists of 742 unique songs\nsampled from the American Billboard charts between\n1958 and 1991. We split these songs into subsets of\n62.5% for training, 12.5% for validation, and 25% for\ntesting. We determine the global key for each song\nusing the procedure described in [14], which leaves\nus with 625 songs with key annotations in total. The\nexact division and key ground truths are available\nonline4.\nClassical Music: To cover this genre, we collected 1504\n(mostly piano) pieces from our internal database for\nwhich we could derive the key from the piece’s title.\nClassical pieces often modulate their key, but usually\nstart in the key denoted in the title. We thus only\nuse the ﬁrst 30 s of each recording. Tracking key\nmodulations is left for future work. We then select\n81 % for training, 9 %for validation, and 10 % for\ntesting.\n3.2 Metrics\nWe adopt the standard evaluation score for Key Classiﬁca-\ntion as deﬁned in the MIREX evaluation campaign5. It\ngoes beyond simple accuracy, as it considers harmonic sim-\nilarities between key classes. A prediction can fall into one\nof the following categories:\nCorrect: if the tonic and the mode (major/minor) of pre-\ndiction and target correspond.\nFifth: if the tonic of the prediction is the ﬁfth of the target\n(or vice versa), and modes correspond.\nRelative Minor/Major: if modes differ and either a) the\npredicted mode is minor and the predicted tonic is 3\nsemitones below the target, or b) the predicted mode\nis major and the predicted tonic is 3 semitones above\nthe target.\nParallel Minor/Major: if modes differ but the predicted\ntonic matches the target.\nOther: Prediction errors not caught by any category, i.e.\nthe most severe errors.\nThen, a weighted score can be computed as w=rc+ 0:5\u0001\nrf+ 0:3\u0001rr+ 0:2\u0001rp, where rc,rf,rr, and rpare the\nratios of the correct, ﬁfth, relative minor/major, and parallel\nminor/major, respectively. We will use this weighted score\nfor our comparisons.\n3.3 Evaluation of the Adaptations\nTo evaluate the effect of our proposed adaptations, we train\nthe three setups (KeyNet/F, KeyNet/S, AllConv) with the\ncombined data of all datasets. We will consider validation\nresults in the ﬁrst sets of experiments, and show results on\n3http://ddmal.music.mcgill.ca/research/billboard\n4http://www.cp.jku.at/people/korzeniowski/bb.zip\n5http://www.music-ir.org/mirex\n/uni00000087 /uni00000088/uni00000087/uni00000087/uni00000030 /uni00000089/uni00000087/uni00000087/uni00000030 /uni0000008a/uni00000087/uni00000087/uni00000030 /uni0000008b/uni00000087/uni00000087/uni00000030 /uni0000008c/uni00000087/uni00000087/uni00000030 /uni000000a4/uni00000087/uni00000087/uni00000030 /uni0000008d/uni00000087/uni00000087/uni00000030\n/uni00000012/uni00000033/uni00000057/uni00000002/uni00000016/uni00000021/uni00000038/uni00000021/uni00000031/uni00000027/uni0000003b/uni00000027/uni00000038/uni00000039/uni00000087/uni00000057/uni0000008d/uni0000008e/uni00000087/uni00000057/uni000000a5/uni00000087/uni00000087/uni00000057/uni000000a5/uni00000088/uni00000087/uni00000057/uni000000a5/uni00000089/uni00000087/uni00000057/uni000000a5/uni0000008a/uni00000087/uni00000057/uni000000a5/uni0000008b/uni00000087/uni00000057/uni000000a5/uni0000008c/uni00000087/uni00000057/uni000000a5/uni000000a4/uni00000087/uni00000057/uni000000a5/uni0000008d/uni0000001c/uni00000021/uni000000a3/uni0000002d/uni00000026/uni00000021/uni0000003b/uni0000002d/uni00000033/uni00000032/uni00000002/uni00000019/uni00000024/uni00000033/uni00000038/uni00000027\n/uni00000003/uni000000a3/uni000000a3/uni00000006/uni00000033/uni00000032/uni0000003d\n/uni0000000f/uni00000027/uni00000040/uni00000012/uni00000027/uni0000003b/uni00000063/uni00000019\n/uni0000000f/uni00000027/uni00000040/uni00000012/uni00000027/uni0000003bFigure 1 . Average validation score over 10 runs for the\ndifferent model setups. Whiskers represent 95 % conﬁ-\ndence intervals computed by bootstrapping. Transparent\ndots show results of the individual runs. We see that given\nsimilar network sizes, the AllConv model performs best.\nAlso, using snippet training (KeyNet/S) improves results\ncompared to full spectrogram training (KeyNet/F), and en-\nables training larger networks.\nthe testing sets only for our analyses and ﬁnal evaluations.\nThis way, we ensure that the ﬁnal results are unbiased.\nThe capacity of a neural network depends not only on\nthe architecture, but also on its size. For a fair compari-\nson, we evaluate each architecture with varying network\nsizes. For the AllConv architecture, we select the num-\nber of feature maps Nf2 f2;4;8;12;16;20;24g. For\nthe KeyNet architecture, the network size depends on the\nnumber of feature maps in the convolutional layers and\nthe size of the embedding space. For practical reasons,\nwe set the size of the embedding space to be 2Nf, and se-\nlectNf2f8;16;24;32;40g. Note that if we train on full\nspectrograms (KeyNet/F), we could not train networks with\nNf>24due to memory constraints. For each model, we\ntried dropout probabilities of p2f0:0;0:1;0:2g.\nFigure 1 presents the results of the three model conﬁg-\nurations. For each model and model capacity, we select\nthe best dropout probability based on the validation results.\nThe experiments show that both adaptations are beneﬁcial.\nTraining with snippets instead of full spectrograms gives\nbetter results at smaller network capacities and enables train-\ning of larger networks. The AllConv architecture achieves\neven better results, regardless of its size.\nWe can quantify two reasons for this, which are conse-\nquences of the expected beneﬁts of the adaptations: better\ngeneralization through increased data variety and the ab-\nsence of dense layers, and better expressivity through deeper\narchitectures and by training the network on a more difﬁcult\ntask. For the ﬁrst, better generalisation, we compare the\naverage ratio of validation accuracy to training accuracy\nfor each of the models (higher indicates less over-ﬁtting):\n0.945 ,0.969 , and 0.982 for KeyNet/F, KeyNet/S, and All-\nConv, respectively. For the second, model expressiveness ,\nwe compare the model’s capability to ﬁt the training data in\nterms of accuracy: 0.837 ,0.858 , and 0.907 for KeyNet/F,266 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018/uni00000087/uni00000057/uni000000a5/uni00000088/uni00000087/uni00000057/uni000000a5/uni00000089/uni00000087/uni00000057/uni000000a5/uni0000008a/uni00000087/uni00000057/uni000000a5/uni0000008b/uni00000087/uni00000057/uni000000a5/uni0000008c/uni0000001a/uni00000027/uni00000039/uni0000003b/uni00000002/uni00000019/uni00000024/uni00000033/uni00000038/uni00000027\n/uni00000003/uni000000a3/uni000000a3/uni00000006/uni00000033/uni00000032/uni0000003d/uni0000001a/uni00000038/uni00000021/uni0000002d/uni00000032/uni0000002d/uni00000032/uni0000002b/uni00000002/uni00000008/uni00000021/uni0000003b/uni00000021/uni00000039/uni00000027/uni0000003b/uni00000039\n/uni00000005/uni00000023 /uni00000006/uni00000031 /uni0000000b/uni00000039 /uni00000005/uni00000023/uni00000006/uni00000031 /uni0000000b/uni00000039/uni00000005/uni00000023 /uni0000000b/uni00000039/uni00000006/uni00000031 /uni0000000b/uni00000039/uni00000005/uni00000023/uni00000006/uni00000031\n/uni00000011/uni00000024/uni0000000b/uni0000002d/uni000000a3/uni000000a3/uni00000002/uni00000005/uni0000002d/uni000000a3/uni000000a3/uni00000023/uni00000033/uni00000021/uni00000038/uni00000026/uni0000000f/uni00000027/uni00000040/uni00000012/uni00000027/uni0000003b/uni00000063/uni00000019\n/uni00000087/uni00000057/uni0000008e/uni00000088/uni00000087/uni00000057/uni0000008e/uni00000089/uni00000087/uni00000057/uni0000008e/uni0000008a/uni00000087/uni00000057/uni0000008e/uni0000008b/uni00000087/uni00000057/uni0000008e/uni0000008c/uni00000087/uni00000057/uni0000008e/uni000000a4/uni0000001a/uni00000027/uni00000039/uni0000003b/uni00000002/uni00000019/uni00000024/uni00000033/uni00000038/uni00000027\n/uni00000006/uni00000011/uni00000008/uni00000005\n/uni000000a5 /uni00000089/uni00000087\n/uni00000012/uni00000033/uni00000057/uni00000002/uni0000000a/uni0000002d/uni000000a3/uni0000003b/uni00000027/uni00000038/uni00000039/uni00000087/uni00000057/uni0000008d/uni00000089/uni0000008c/uni00000087/uni00000057/uni0000008d/uni0000008a/uni00000087/uni00000087/uni00000057/uni0000008d/uni0000008a/uni0000008c/uni00000087/uni00000057/uni0000008d/uni0000008b/uni00000087/uni0000001a/uni00000027/uni00000039/uni0000003b/uni00000002/uni00000019/uni00000024/uni00000033/uni00000038/uni00000027\n/uni00000088/uni000000a4 /uni0000008b/uni00000087\n/uni00000012/uni00000033/uni00000057/uni00000002/uni0000000a/uni0000002d/uni000000a3/uni0000003b/uni00000027/uni00000038/uni00000039/uni0000000b/uni0000002d/uni00000021/uni00000032/uni0000003b/uni00000019/uni0000003b/uni00000027/uni00000036/uni00000039\nFigure 2 . Average test scores over 10 runs for each architec-\nture (columns), split by dataset (rows). The smaller models\nare on the left of each column. Colors indicate the training\ndata used: Bbstands for the Billboard dataset, Cmfor the\nclassical music dataset, and Gsfor the GiantSteps dataset.\nEach row shows the results of runs where the training set\nalso contained the training data of the respective test set\ngenre (e.g. in the ﬁrst row, we only see runs where McGill\nBillboard data was included in training).\nKeyNet/S, and AllConv, respectively. Stronger models that\ngeneralise better achieve better results.\n3.4 Inﬂuence of Training Data\nWe then want to see how the number and genre of the\ndatasets used for training affects results. To this end, we se-\nlect the hyper-parameter settings for AllConv and KeyNet/S\nthat achieved the best average results in the previous exper-\niment: Nf= 20; p= 0:1for AllConv, Nf= 40; p= 0:1\nfor KeyNet/S. Additionally, we consider smaller models\nof each type, i.e. Nf= 8 for AllConv and Nf= 16\nfor KeyNet/S, both without dropout. Under these settings,\nboth architectures have a comparable number of parame-\nters. We train these models using all possible 1, 2, and\n3-combinations of the datasets, and evaluate them on all\ndata. The results are shown in Fig. 2.\nThe main observations are: a) increasing model capacity\nis more beneﬁcial to the AllConv model than KeyNet/S, re-gardless of dataset; b) adding capacity to the AllConv model\nenables it to better deal with diverse data—the biggest gains\nof additional parameters are achieved if the model is trained\non a combined dataset (pink line)—while this is not always\nthe case for KeyNet/S (see the Billboard results, where it\nseems that adding classical music to the training set impairs\nthe performance of this model); c) given enough capacity\nin the AllConv model, training using the complete data\nperforms better than (or almost equal to) ﬁtting a speciﬁc\ngenre, while the opposite is the case for KeyNet/S, where\nspecialised models outperform the general ones. We thus\nargue that the AllConv model not only copes better with\ndiverse training data, but that it leverages the diversity in\nthe training data to perform as well as it does.\n4. EV ALUATION\nMotivated by the results above, the remainder of our analy-\nsis focuses on the AllConv model. To thoroughly investigate\nits performance and compare it to the state of the art, we\nevaluate it on the following unseen datasets:\nKeyFinder6:1 000 songs from a variety of popular music\ngenres. Unfortunately, we have only the audio for\n998 of the songs available.\nIsophonics7:180 songs by The Beatles, 19songs by\nQueen, and 18songs by Zweieck. Since these songs\ncontain key modulations, we split them into single\nkey segments and retain only segments annotated\nas major or minor keys, as was done for the 2017\nMIREX evaluation campaign8.\nRobbie Williams [6]: 65songs by Robbie Williams,\nwhich we also split into single key segments as out-\nlined above.\nRock9[5]: 200songs taken from Rolling Stone’s “500\nGreatest Songs of All Time” list. As with the McGill\nBillboard dataset, only the tonics are annotated. We\nﬁrst split the songs according to the annotated ton-\nics, and then follow a similar procedure as described\nin [14]: if more than 80 % of the tonic chords are in\neither major or minor, the mode is set accordingly; if\nthere are no tonic chords in a segment, we consider\ndominant chords in the same way.\nWe select the best AllConv model based on the validation\nscore over the compound data of Electronic, Pop/Rock and\nClassical music. On average, models with Nf= 20 and\ndropout probability of 0:1performed best. However, the\nbest single model used Nf= 24 (see Fig. 1), and was\nconsequently chosen as ﬁnal model.\nIn Table 2, we compare this model to other models pro-\nposed in the academic literature. For each dataset, we\nshow the results of the best competing system, if available.\n6http://www.ibrahimshaath.co.uk/keyﬁnder/\n7http://isophonics.net/datasets\n8http://www.music-ir.org/mirex/wiki/2017:Audio Key Detection\nResults\n9http://rockcorpus.midside.com/Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 267Dataset Model Weighted Correct Fifth Relative Parallel Other\nGiantSteps AllConv 74.6 67.9 7.0 8.1 4.1 12.9\nCK1 [14] 74.3 67.9 6.8 7.1 4.3 13.9\nBillboard AllConv 85.1 79.9 5.6 4.2 6.2 4.2\nCK2 [14] 83.9 77.1 9.0 4.9 4.2 4.9\nClassical AllConv 96.6 95.2 1.4 1.4 1.4 0.7\n- - - - - - -\nKeyFinder AllConv 76.1 70.0 5.7 7.4 4.7 12.1\nbgate [10] 72.4 65.0 8.6 6.5 5.4 14.4\nIsophonics AllConv 82.5 76.3 7.6 5.4 3.7 7.1\nBD1 [2] 75.1 66.0 13.6 5.1 3.9 9.2\nR. Williams AllConv 81.2 72.4 10.8 10.3 1.3 5.2\nHS1 [18] 77.1 68.8 10.1 9.0 3.2 9.0\nRock AllConv 74.3 69.3 6.5 1.7 6.0 16.5\n- - - - - - -\nTable 2 . Evaluation results. Best results are in boldface.\n/uni00000011/uni00000024/uni0000000b/uni0000002d/uni000000a3/uni000000a3/uni00000002/uni00000005/uni0000002d/uni000000a3/uni000000a3/uni00000023/uni00000033/uni00000021/uni00000038/uni00000026 /uni00000018/uni00000033/uni00000024/uni00000030\n/uni00000008/uni00000021/uni0000003b/uni00000021/uni00000039/uni00000027/uni0000003b/uni00000087/uni00000088/uni00000087/uni00000087/uni00000089/uni00000087/uni00000087/uni0000008a/uni00000087/uni00000087/uni0000008b/uni00000087/uni00000087/uni0000008c/uni00000087/uni00000087/uni00000016/uni0000002d/uni00000027/uni00000024/uni00000027/uni00000002/uni00000010/uni00000027/uni00000032/uni0000002b/uni0000003b/uni0000002c/uni00000002/uni0000006e/uni00000039/uni0000006f/uni00000016/uni00000038/uni00000027/uni00000026/uni0000002d/uni00000024/uni0000003b/uni0000002d/uni00000033/uni00000032\n/uni0000001d/uni00000038/uni00000033/uni00000032/uni0000002b\n/uni00000006/uni00000033/uni00000038/uni00000038/uni00000027/uni00000024/uni0000003b\nFigure 3 . Distributions of the length of correctly and in-\ncorrectly classiﬁed excerpts depending on the dataset they\ncome from. Densities are estimated using kernel density\nestimation. Horizontal lines with long dashes indicate the\nmedian, those with short dashes the quartiles. The den-\nsities are normalised, i.e. they do not indicate how many\ninstances were classiﬁed correctly (or incorrectly), but only\nthe distribution of except lengths within each group.\nFor the GiantSteps and Billboard datasets, the best com-\npeting systems were variants of the neural-network-based\nmodel from [14]. For the pre-segmented Isophonics and\nRobbie Williams datasets, we use the results available on\nthe MIREX 2017 website8. For the KeyFinder dataset,\nwe report the best results achieved using the open-source\nreference implementation10of the algorithms from [10].\nAs we can see, the proposed model performs best for\nall datasets for which comparisons were possible. Keep\nin mind that the systems we compare to are often speciﬁ-\n10https://github.com/angelfaraldo/edmkeycally tuned for a genre (CK1, CK2, HS1, bgate) or set up\nto favour certain key modes prevalent in a dataset (BD1),\nwhile we use the same, general model for all datasets. For\nexample, CK1 performs badly on the Billboard dataset\n(w= 72:8), BD1 on the GiantSteps ( w= 59:6), and HS1\non the Isophonics dataset ( w= 64:1). In this light, it is re-\nmarkable that the proposed model consistently out-performs\nthe others.\nHowever, the results also point us to a deﬁciency of the\nmodel. Recall that for some datasets (e.g. Rock), we split\nthe ﬁles according to key annotations, and process each\nexcerpt individually. If we compare the results on the Rock\ndataset with those on the Billboard dataset, we see a large\ndiscrepancy, although both sets comprise similar musical\nstyles. As Fig. 3 demonstrates, the duration of a classiﬁed\nexcerpt plays a major role here: for the Billboard set, the\nmedian length of excerpts classiﬁed correctly matches the\none of incorrect classiﬁcations; for the Rock set, however,\nthe median lengths differ greatly: 131 s vs.51 s, for cor-\nrectly and incorrectly classiﬁed excerpts, respectively. The\ndistribution of excerpt lengths that are classiﬁed correctly\nis thus very different from the one of incorrectly classiﬁed\nexcerpts in the Rock set. The shorter an excerpt, the more\nlikely it is classiﬁed incorrectly.\nThis is not surprising per se. Determining the key of a\npiece requires a certain amount of musical context. How-\never, it shows that in order to move beyond global key\nclassiﬁcation, and towards recognising key modulations, it\nwill not sufﬁce to detect key boundaries and apply known\nmethods within these boundaries. To recognise key modu-\nlations, classifying short excerpts individually will reach a\nglass ceiling. Instead, we will need models that consider\nthe hierarchical harmonic coherence of the whole piece.268 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20185. CONCLUSION\nWe have presented a genre-agnostic key classiﬁcation model\nbased on the system developed in [14], with improvements\nof the training procedure and network structure. These im-\nprovements enable faster training, better generalisation, and\ntraining larger and thus more powerful models, which can\nleverage diverse training data instead of being impaired by\nit. The resulting key classiﬁer generalises well over datasets\nof different musical styles, and out-performs systems that\nare specialised for speciﬁc genres (see Table 2).\n6. ACKNOWLEDGEMENTS\nThis work is supported by the European Research Coun-\ncil (ERC) under the EU’s Horizon 2020 Framework Pro-\ngramme (ERC Grant Agreement number 670035, project\n“Con Espressione”).\n7. REFERENCES\n[1]Joshua Albrecht and Daniel Shanahan. The Use of\nLarge Corpora to Train a New Type of Key-Finding\nAlgorithm: An Improved Treatment of the Minor\nMode. Music Perception: An Interdisciplinary Journal ,\n31(1):59–67, 2013.\n[2]Gilberto Bernardes, Matthew E. P. Davies, and Carlos\nGuedes. Automatic musical key estimation with adap-\ntive mode bias. In 2017 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\nNew Orleans, USA, March 2017.\n[3]John Ashley Burgoyne, Jonathan Wild, and Ichiro Fu-\njinaga. An Expert Ground Truth Set for Audio Chord\nRecognition and Music Analysis. In 12th International\nSociety for Music Information Retrieval Conference (IS-\nMIR) , Miami, USA, October 2011.\n[4]Djork-Arn ´e Clevert, Thomas Unterthiner, and Sepp\nHochreiter. Fast and Accurate Deep Network Learn-\ning by Exponential Linear Units (ELUs). In Interna-\ntional Conference on Learning Representations (ICLR),\narXiv:1511.07289 , San Juan, Puerto Rico, February\n2016.\n[5]Trevor de Clercq and David Temperley. A corpus anal-\nysis of rock harmony. Popular Music , 30(01):47–70,\nJanuary 2011.\n[6]Bruno Di Giorgi, Massimiliano Zanoni, Augusto Sarti,\nand Stefano Tubaro. Automatic chord recognition based\non the probabilistic modeling of diatonic modal har-\nmony. In Proceedings of the 8th International Work-\nshop on Multidimensional Systems , Erlangen, Germany,\nSeptember 2013.\n[7]Hamid Eghbal-Zadeh, Bernhard Lehner, Matthias Dor-\nfer, and Gerhard Widmer. CP-JKU Submissions for\nDCASE-2016: A Hybrid Approach Using Binaural I-\nVectors and Deep Convolutional Neural Networks. InDetection and Classiﬁcation of Acoustic Scenes and\nEvents (DCASE) , September 2016.\n[8]Ronen Eldan and Ohad Shamir. The Power of Depth\nfor Feedforward Neural Networks. In Proceedings of\nMachine Learning Research (COLT 2016) , New York,\nUSA, June 2016.\n[9]´Angel Faraldo, Emilia G ´omez, Sergi Jord `a, and Perfecto\nHerrera. Key Estimation in Electronic Dance Music. In\nNicola Ferro, Fabio Crestani, Marie-Francine Moens,\nJosiane Mothe, Fabrizio Silvestri, Giorgio Maria\nDi Nunzio, Claudia Hauff, and Gianmaria Silvello,\neditors, Advances in Information Retrieval , volume\n9626, pages 335–347. Springer International Publishing,\nCham, 2016.\n[10] ´Angel Faraldo, Sergi Jord `a, and Perfecto Herrera. A\nMulti-Proﬁle Method for Key Estimation in EDM. In\nProceedings of the AES International Conference on\nSemantic Audio , Erlangen, Germany, June 2017.\n[11] Sergey Ioffe and Christian Szegedy. Batch Normaliza-\ntion: Accelerating Deep Network Training by Reduc-\ning Internal Covariate Shift. arXiv:1502.03167 , March\n2015.\n[12] Filip Korzeniowski and Gerhard Widmer. Feature Learn-\ning for Chord Recognition: The Deep Chroma Extractor.\nIn17th International Society for Music Information Re-\ntrieval Conference (ISMIR) , New York, USA, August\n2016.\n[13] Filip Korzeniowski and Gerhard Widmer. A Fully Con-\nvolutional Deep Auditory Model for Musical Chord\nRecognition. In 26th IEEE International Workshop\non Machine Learning for Signal Processing (MLSP) ,\nSalerno, Italy, September 2016.\n[14] Filip Korzeniowski and Gerhard Widmer. End-to-End\nMusical Key Estimation Using a Convolutional Neural\nNetwork. In 25th European Signal Processing Confer-\nence (EUSIPCO) , Kos, Greece, August 2017.\n[15] Shiyu Liang and R. Srikant. Why Deep Neural Net-\nworks for Function Approximation? In Interna-\ntional Conference on Learning Representations (ICLR),\narXiv:1610.04161 , Toulon, France, April 2017.\n[16] Katy Noland and Mark Sandler. Signal Processing Pa-\nrameters for Tonality Estimation. In Audio Engineering\nSociety Convention 122 . Audio Engineering Society,\nMay 2007.\n[17] Steffen Pauws. Musical Key Extraction From Audio. In\nProceedings of the 5th International Society for Music\nInformation Retrieval Conference (ISMIR) , Barcelona,\nSpain, October 2004.\n[18] Hendrik Schreiber. MIREX 2017: CNN-Based\nAutomatic Musical Key Detection Submissions\nHS1/HS2/HS3. Technical report, MIREX, 2017.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 269[19] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas\nBrox, and Martin Riedmiller. Striving for Simplicity:\nThe All Convolutional Net. In International Conference\non Learning Representations (ICLR), Workshop Track,\narXiv:1412.6806 , May 2014.\n[20] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. Dropout: A\nSimple Way to Prevent Neural Networks from Over-\nﬁtting. The Journal of Machine Learning Research ,\n15(1):1929–1958, 2014.\n[21] David Temperley. What’s Key for Key? The Krumhansl-\nSchmuckler Key-Finding Algorithm Reconsidered. Mu-\nsic Perception: An Interdisciplinary Journal , 17(1):65–\n100, October 1999.270 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "VenueRank: Identifying Venues that Contribute to Artist Popularity.",
        "author": [
            "Emmanouil Krasanakis",
            "Emmanouil Schinas",
            "Symeon Papadopoulos",
            "Yiannis Kompatsiaris",
            "Pericles A. Mitkas"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492513",
        "url": "https://doi.org/10.5281/zenodo.1492513",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/91_Paper.pdf",
        "abstract": "An important problem in the live music industry is finding venues that help expose artists to wider audiences. However, it is often difficult to obtain live music audience data to tackle this task. In this work, we investigate whether important venues can instead be inferred through social media data. Our approach consists of employing bipartite graph ranking algorithms to help discover important venues in artist-venue graphs mined from Facebook. We use both well-established algorithms, such as BiRank, and a modification of their common iterative scheme that avoids the impact of possibly erroneous heuristics to the ranking, which we call VenueRank. Resulting venue ranks are compared to those obtained from feature extraction for predicting the most listened artists and large listener increments in Spotify. This comparison yields high correlation between venue importance for listener prediction and bipartite graph ranking algorithms, with VenueRank found more robust against overfitting.",
        "zenodo_id": 1492513,
        "dblp_key": "conf/ismir/KrasanakisSPKM18",
        "keywords": [
            "live music industry",
            "finding venues",
            "exposing artists",
            "live music audience data",
            "social media data",
            "artist-venue graphs",
            "Facebook",
            "bipartite graph ranking algorithms",
            "VenueRank",
            "VenueRank"
        ],
        "content": "VENUERANK: IDENTIFYING VENUES THAT CONTRIBUTE TO ARTIST\nPOPULARITY\nEmmanouil Krasanakis1Emmanouil Schinas1;2\nSymeon Papadopoulos1Yiannis Kompatsiaris1Pericles Mitkas2\n1CERTH-ITI, Thessaloniki, Greece - fmaniospas,manosetro,papadop,ikom g@iti.gr\n2AUTH, Thessaloniki, Greece - manosetro@issel.ee.auth.gr,mitkas@auth.gr\nABSTRACT\nAn important problem in the live music industry is ﬁnd-\ning venues that help expose artists to wider audiences.\nHowever, it is often difﬁcult to obtain live music audi-\nence data to tackle this task. In this work, we investigate\nwhether important venues can instead be inferred through\nsocial media data. Our approach consists of employing\nbipartite graph ranking algorithms to help discover impor-\ntant venues in artist-venue graphs mined from Facebook.\nWe use both well-established algorithms, such as BiRank,\nand a modiﬁcation of their common iterative scheme that\navoids the impact of possibly erroneous heuristics to the\nranking, which we call VenueRank. Resulting venue ranks\nare compared to those obtained from feature extraction for\npredicting the most listened artists and large listener incre-\nments in Spotify. This comparison yields high correlation\nbetween venue importance for listener prediction and bi-\npartite graph ranking algorithms, with VenueRank found\nmore robust against overﬁtting.\n1. INTRODUCTION\nIn the music industry, artists aim to present themselves to\nwide audiences. Therefore, it is important for them to gain\nas much exposure as possible from their live performances.\nIn turn, this exposure can inﬂuence their popularity, as ex-\npressed by the size of their audience.\nIn this paper we try to identify which performances of-\nfer higher exposure. Factors such as timing and other re-\ncent events can inﬂuence this. Listener geolocation has\nalso been found to contribute to artist popularity predic-\ntion [3, 21]. Consequently, it is reasonable to hypothesize\nthat performing in certain venues could contribute more\nto artist popularity. Having access to a ranking of venues\nbased on expected exposure could be valuable for artists\nand their agents; when confronted with different options\nregarding their future performances, they could consider\nthese rankings as an important decision criterion.\nc\rEmmanouil Krasanakis, Emmanouil Schinas, Symeon\nPapadopoulos, Yiannis Kompatsiaris, Pericles Mitkas. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Emmanouil Krasanakis, Emmanouil Schinas, Symeon Pa-\npadopoulos, Yiannis Kompatsiaris, Pericles Mitkas. “VenueRank: Iden-\ntifying Venues that Contribute to Artist Popularity”, 19th International\nSociety for Music Information Retrieval Conference, Paris, France, 2018.To rank venue exposure, one could try to predict it using\nmachine learning algorithms. Unfortunately, there are dif-\nﬁculties in quantifying the notion of exposure, not least of\nwhich is that real-life data may misrepresent audience size\nand reactions. For example, participating in a large event\nheld in a well-known venue with many other artists may\ncontribute less to gaining popularity compared to an artist-\nfocused event. A viable alternative to measuring venue ex-\nposure, which we also adopt in this work, is to instead es-\ntimate whether venues contribute to artist popularity (e.g.\nthe number of listeners in music services) from a machine\nlearning perspective. To do so, we can employ feature ex-\ntraction methods to identify the most important venues that\nhelp determine and increase artist popularity.\nHowever, even this formulation depends on obtaining\nlive music audience data required for supervised training.\nSuch data are not necessarily easy to obtain, as they are\ntypically considered conﬁdential. Therefore, in this work\nwe attempt inferring important venues through unsuper-\nvised training, which does not require such data.\nIn particular, given a graph representation, where artists\nare linked with venues they have performed in, we use\ngraph ranking algorithms to rank venues. To validate\nwhether this approach ranks venues based on offered ex-\nposure, we compare the produced ranks with venue impor-\ntances obtained through feature extraction for predicting\npopular artists and artist popularity increments. We ﬁnd\nthat ranking methods can be more informative than raw\nsocial media measures in predicting important venues.\n2. BIPARTITE GRAPH RANKING\n2.1 Motivation for Graph Ranking\nWe can organize data pertaining to artists Aand venuesV\nwhere they have performed as bipartite graphs, i.e. graphs\nin which vertices form two disjoint sets linking only to\neach other. To analyze the importance of venues based\non the structure of such artist-venue graphs, we employ\nranking algorithms, which are used to determine the rel-\native importance of nodes given a graph’s structure [17].\nThese algorithms often operate under the premise that\nnodes linked to a higher number of important nodes are\nalso more important.\nFormulations such as HITS [14] further reﬁne this con-\ncept by recognizing that there can be two types of impor-\ntant nodes; authorities that provide important information702and hubs that point to a lot of information sources. A\nnode’s authority is then derived from its predecessors’ hub\nscores and its hub score is derived from its successors’ au-\nthority scores, forming an iterative process.\nThe distinction between authorities and hubs is of great\ninterest when applied on bipartite graphs, especially if the\nlinks between disjoint set elements represent the same type\nof relation. In this case, we can formulate that one of those\nsets contains only authorities and the other only hubs.\nFor example, in our artist-venue graph setting, where\nvenues always represent locations where artists have per-\nformed, artists can be considered as authorities and venues\nas hubs from which popularity-related authority stems. In-\ntuitively, this means that artists are considered more im-\nportant if they have performed in more important venues,\nwhereas venues are considered more important if more im-\nportant artists have performed there.\n2.2 Ranking based on Prior Ranks\nFormally, bipartite graph ranking algorithms attempt to\nrank nodes in a graph deﬁned by a (weighted) adjacency\nmatrix W:V\u0002A between the disjoint groups A;Vbased\non heuristically estimated prior ranks.\nPrior ranks1are supported by most graph ranking algo-\nrithms and are used to introduce ranking bias that is driven\nby information unrelated to graph structure. For exam-\nple, in web searches [17] prior ranks place more weight\non the pages more similar to the search query. In bipar-\ntite graphs, prior ranks often represent an informed belief\nabout ranks and help attract the solution towards conver-\ngence. However their usage can reduce the robustness of\nextracted structural characteristics (see Subsection 2.3).\nAs demonstrated by He et al. [12], previous bipartite\ngraph ranking algorithms follow similar formulations. In\nparticular, if SandS0are normalizations of Wand its\ntransposition WTrespectively, a0; v0are prior ranks that\ninitially estimate node ranks for the two bipartite graph\ngroups and ra; rvare prior rank elimination parameters,\napproaches follow a common recursive rule for calculating\nbipartite graph group ranks as recursions n!1 :\nan+1= (1\u0000ra)a0+raS0vn (1a)\nvn+1= (1\u0000rv)v0+rbSan (1b)\nIn practice, this iterative process stops when rank differ-\nences converge to a stable set of values. In this work, we\nempirically adopt a simple stopping criterion across algo-\nrithms that stops after rank changes become small enough:\nkan+1\u0000ank2\n2+kvn+1\u0000vnk2\n2<0:1 (2)\nDifferences between bipartite graph ranking algorithms\nlie in the way normalization is performed on the adjacency\nmatrix and its transposition. If DA; DVare two diagonal\nmatrices containing the node degrees of the disjoint sets\nA;V, those algorithms perform normalization as:\nS=D\u0000pv\nvWD\u0000pa\na (3a)\nS0=D\u0000pv\naWTD\u0000pa\nv (3b)\n1Prior ranks have also been referred to as ‘query vectors’ [12].where pa; pvare non-negative constants speciﬁc to each al-\ngorithm (see Table 1). These constants determine whether\ndegree normalization should be performed row-wise or\ncolumn-wise or whether the normalization should produce\na stochastic matrix.\nAlgorithm papv\nHITS [14] 0 0\nCo-HITS [6] 1 0\nBGRM [19] 1 1\nBGER [1] 0 1\nBiRank [12]1\n21\n2\nTable 1 : Different parameters between bipartite graph\nranking algorithms.\nThe advantage of the iterative scheme demonstrated in\nEqn (1) over more general ranking schemes, which do not\ntake the bipartite nature of the graph into account, is that\nthe former converges fast to unique stationary solutions, as\ndemonstrated below.\na)Ifra; rv<1then substituting Eqn (1) into itself as n!\n1yields:\na1= (I\u0000rarvS0S)\u00001[ra(1\u0000rv)S0v0+ (1\u0000ra)a0]\nv1= (I\u0000rarvSS0)\u00001[rv(1\u0000ra)Sa0+ (1\u0000rv)v0]\nAlthough this solution can also help analytically derive\nnode ranks a1; v1, doing so can be computationally in-\ntensive, since it requires matrix inversion. For that reason,\nall previous approaches adopt the iterative scheme, which\nis computationally efﬁcient, especially when Wis sparse.\nb)Ifra=rv= 1then:\na1=S0v1\nv1=Sa1)(S0S\u0000I)a1= 0\n(SS0\u0000I)v1= 0\nTherefore, if S0S; SS0are stochastic matrices, i.e. if\npa+pv= 1 in Eqn (3), their largest eigenvalue is 1\nand thus a1; v1are their principal eigenvectors respec-\ntively. In this case, the iterative scheme resembles the\npower method for calculating the principal eigenvectors.\nHowever, due to absence of vector normalization after each\nstep, large enough ranks may grow uncontrollably and fail\nto converge [16].\n2.3 VenueRank\nThe above formulation of bipartite graph ranking algo-\nrithms relies heavily on the correctness of the prior ranks\na0; v0to produce accurate ranks. If the prior ranks are only\npartially correct (e.g. are only sparsely ﬁlled) ranking al-\ngorithms may converge to much different values. Further-\nmore, structure-related information could be more useful\nfor important venue detection than prior rank heuristics.\nHence, there exist cases where eliminating the effect of\nprior ranks is desirable [15].\nIn such cases, the previous bipartite graph ranking algo-\nrithms eliminate the prior ranks by selecting ra=rv= 1.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 703However, as discussed above, numeric convergence is not\ntheoretically guaranteed for these parameters. For exam-\nple, BiRank fails to converge for these parameters when\nrun on data gathered in Subsection 3.1.\nTherefore, we propose modifying the previous iterative\nprocess to gradually remove the dependency on initial prior\nranks across iterations:\nan+1= (1\u0000ra)an+raS0vn (4a)\nvn+1= (1\u0000rv)vn+rvSan (4b)\nSimilarly to before, as n!1 we obtain a1=S0v1\nandv1=Sa1and thus a1; v1become the principal\neigenvectors of S0SandSS0respectively, as long as the\nlatter are stochastic matrices.\nThis iterative process differs from previous ones in that\nit stabilizes on these eigenvectors for any non-zero param-\netersra; rv. As a result, we can retrieve theoretical guar-\nantees [16] that there exist small enough ra; rvthat make\nit converge. Moreover, we can see that:\nS0S=D\u0000pv\u0000pa\na WTD\u0000pa\u0000pv\nv W\nS0S=D\u0000pa\u0000pv\nv WD\u0000pv\u0000pa\na WT\nTherefore, for constant pa+pv= 1 the eigenvectors of\nthese two matrices remain the same and Eqn (4) converges\nto the same ranks regardless of the type of normalization\ndeﬁned by these two parameters.\nIn short, we have shown that, for the iterative scheme\ndemonstrated in Eqn (4), which we will call VenueRank,\nit sufﬁces to select any pa+pv= 1 and small enough\nra; rvto converge to bipartite graph ranks where the effect\nof prior ranks is eliminated.\n3. EXPERIMENTS\n3.1 Data Collection\nWe collected two types of data for our experiments; data\nfrom Facebook about artist and venue pages and the re-\nspective number of listeners for those artists from Spotify .\nWe use Facebook data to run bipartite graph ranking al-\ngorithms and Spotify data to extract the ground truth with\nwhich to evaluate these algorithms.\nWe started with a collection of 542artists, for which we\nwere granted access to their number of streams and listen-\ners in Spotify Analytics2from 1 January 2015 to 3 May\n2019. We also used the Facebook Graph API3to automat-\nically ﬁnd Facebook pages for those artists and manually\nremoved artists with erroneously matched pages. After this\nstep, the collection comprises 323artists, for whom we can\nretrieve both the monthly number of listeners in Spotify\nand their Facebook page.\nNext, we retrieved the events published in the discov-\nered Facebook pages dating later than 1 January 2014, as\nwell as the venues that hosted them. This process results\nin a tripartite graph with nodes representing artists, events\nand venues (see Figure 1).\n2https://analytics.spotify.com/\n3https://developers.facebook.com/docs/\ngraph-api/\nHOSTSTAKES_PLACE_INTAKES_PLACE_INTAKES_PLACE_IN\nTAKES_PLACE_INHOSTS\nHOSTS\nHOSTS\nHOSTSTAKES_PLACE_IN\nTAKES_PLACE_IN\nTAKES_PLACE_INTAKES_PLACE_IN\nHOSTSHOSTSHOSTS\nHOSTSTAKES_PLACE_IN\nTAKES_PLACE_INHOSTS\nHOSTSTAKES_PLACE_INTAKES_PLACE_IN\nTAKES_PLACE_INTAKES_PLACE_IN\nTAKES_PLACE_INHOSTS\nHOSTSHOSTS\nHOSTS\nHOSTS\nTAKES_PLACE_INTAKES_PLACE_IN\nHOSTSHOSTSTAKES_PLACE_IN\nTAKES_PLACE_INHOSTS\nHOSTSTAKES_PLACE_IN\nTAKES_PLACE_INHOSTS\nHOSTSTAKES_PLACE_IN\nTAKES_PLACE_INTAKES_PLACE_IN\nTAKES_PLACE_INHOSTS\nHOSTSHOSTS\nThe RasmusThe\nRasmus\nDark Matters\nT ourD-A-D /\nBerlin\nColumbia\nTheater BerlinPoets of the\nFall - Berlin,\nGermanyPoets of the\nFall German\nT our - BerlinPoets of the Fall\nD-A-D\nThe\nRasmus at\nThe Garage\n(October\n21, 20…D-A-D /\nLondon\nThe Garage\nSan\nCisco | live\nat The\nGarage [8\nSepte…\nSan\nCisco 'The\nWater' T our -\nThe Garage |\nLondon,\nUKSan CiscoThe\nRasmus at\nUea the\nWaterfront\n(October\n18,The Coral\nat Norwich\nWaterfront\nThe Waterfront\nNorwichThe CoralThe\nRasmus I\nDortmund I\nFZWFZW,\nDortmundFZWPoets of\nthe Fall,\nClearview\nT our at\nFZW\nDortmund,\nGermany -\nPoets of the\nFallPoets of\nthe Fall\nGerman T our\n- Dortmu…\nThe Rasmus\n| E\u0001enaar\nFemMe\nMetal Event E\u0001enaar\nAmberian DawnThe\nRasmus\nDark Matters\nT ourPoets of\nthe Fall,\nClearview\nT our at\nYotaS…\u0002 \u0000 \u0003 \u0004 CLUB\nGREEN\nCONCERT\nThe\nRasmus -\nKöln - sold\nout!José\nGonzález &\nThe String\nTheory ·\nLive in\nLive Music HallJose Gonzalez\nThe\nRasmus\nDark Matters\nT ourLondon, UK\n- Poets of the\nFall\nScala\nJose\nGonzalez in\nLondon, UKJose\nGonzalez in\nLondon, UKFigure 1 : Artist (red) and venue (blue) pages in Facebook\nalongside their associated events (yellow) of the largest\nconnected subgraph of the dataset that contains a Finnish\nrock band called ‘The Rasmus’ (green).\nThis graph contains a total of 105;251events that took\nplace in 4;051venues across 72timezones (see Figure 2).\nUsing the events in that graph as indicators of the appear-\nance of an artist in a venue, we infer a bipartite artist-venue\ngraph, which we use for our analysis. Artists associated\nwith a non-zero number of venues number 224and they\nare associated with a total of 2;392venues.\nFigure 2 : Timezones with more than 20 venues each.\nOur dataset contains the number of Spotify listeners per\nmonth for each artist. From these listeners we procure\nartist popularity based on the total number of listeners for\neach artist, as well as the increase of the number of artist\nlisteners for each month, which yields 8;619 datapoints\nacross all artists. The number of total listeners spans a\nwide range of magnitudes. Hence, denoting the number\nof listeners for month mof an artist as Lm, we deﬁne:\npopularity = log(1 +P\nmLm)\nwhich yields the normal-like artist distribution shown in\nFigure 3. We also quantify the relative increments of\nmonthly listeners as:\nincm= minfLm=Lm\u00001\u00001;1gwhen Lm\u000016= 0704 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 3 : Artist popularities (left) and the distribution of\nrelative listener increments (right).\n3.2 Ground Truth Construction\nIt is difﬁcult to directly extrapolate the exposure granted by\nvenues using only artist popularity andincmdata. How-\never, we can employ a feature extraction scheme to ob-\ntain venue importances reﬂecting their contribution to pre-\ndicting these quantities. Based on the systemic property\nof graph ranking algorithms in Section 2.2, to rank based\non the nodes’ positions on the graph, we can then evalu-\nate the quality of those algorithms by measuring whether\nhigher ranked venues are actually more important for pre-\ndiction. Effectively, this would assert that higher ranks\nrepresent higher exposure. This way, venue importances\nextracted from machine learning on Spotify data can be\nused as ground truth against which to validate ranking al-\ngorithms, which do not utilize such data.\nThe machine learning setting for feature extraction can\nbe either a regression or a classiﬁcation task (popular-vs-\nunpopular). Here, we focus on the latter, since the classiﬁ-\ncation task leads to clearer separation between venues that\nlead to each of the two target classes. Indeed, regression\ntasks using only venues to predict popularity andincm\nvalues yield high error rates, whereas the binary classiﬁers\ndemonstrated below boast high predictive capabilities.\nLabeling and Feature Selection\nTo set up the venue ranking task, we distinguish high\npopularity andincmvalues by performing outlier detec-\ntion [13] and considering outliers residing in the 20% right\ntail of their distributions4as popular and high increment\nones respectively. We then use methods that perform ro-\nbust feature extraction [10] based on these labels.\nTo do so, we consider venues as binary artist features to\npredict popularity , whereas we use exponential decay to\nmodel the decreasing inﬂuence [9] on incmof performing\nin a venue held at month mvasexp\u0000\n\u0000m\u0000mv\n2\u0001\nifmv\u0014m\nand0otherwise. Using these feature values, the feature\nextraction mechanism then identiﬁes which features (i.e.\nvenues) contribute the most to label prediction (i.e. high\nartist popularity and high listener increments).\nUnfortunately, outliers represent a small fraction of all\ndatapoints and hence cause imbalance between label pri-\nors. Imbalance often affects classiﬁcation validity and can\ndistort or bias estimated feature importances. To alleviate\nsuch concerns, we employ SMOTE oversampling [2] to\ngenerate synthetic popular artist proﬁles, so that the num-\nber of popular artists becomes equal to the number of un-\n4To obtain the outliers residing in the right 20% distribution tail, we\nuse the z-score detection threshold 0:84for those greater than the median.popular ones. We prefer an oversampling scheme, because\nthe small number of collected artists prohibits an under-\nsampling one. This process is summarized in Figure 4.\nFigure 4 : Extracting venue importances.\nClassiﬁer\nWe use the random forest classiﬁer of the sklearn Python\npackage [18] with an entropy feature selection crite-\nrion. Compared to other classiﬁcation algorithms, random\nforests calculate feature importances during the training\nprocess and do not require tuning. On the other hand,\nthey can produce lower importances for cross-correlated\nfeatures. To improve the robustness of such features,\nwe instead deploy an ensemble of random forests [20],\nwhich averages importance scores obtained from 10ran-\ndom forests. To avoid erroneously overstating the impor-\ntance of unique venue appearances, we train these ensem-\nbles and produce importances only for venues in our data\nwhere at least 2artists have performed, which number 602.\nValidation\nTo assert the validity of feature importances assigned by\nrandom forest ensembles, we performed leave-one-out\ncross-validation on trained random forests across 13train-\ning repetitions. For predicting high popularity labels we\nobtained 9%false positive error rate (i.e. rate of assign-\ning unpopular artists as popular) and 7%false negative er-\nror rate (i.e. rate of assigning popular artists as unpopu-\nlar), whereas for predicting high incmlabels we obtained\n29% false positive error rate and 33% false negative error\nrate. Since error rates reﬂect informed classiﬁcation, venue\nimportances obtained through this process can indeed be\nconsidered as the ground truth for subsequent experiments.\nThese error rates indicate that popularity importances are\nmore accurate, although from a methodological standpoint\ncausation is better explored by incmimportances.\n3.3 Compared Ranking Algorithms\nIn this section, we explore the performance of unsuper-\nvised ranking algorithms (such as those presented in Sec-\ntion 2) that aim to rank venues using only Facebook data.\nThese algorithms require prior rank estimations, which\nwe heuristically infer through metadata obtained from the\nFacebook Graph API. In particular, we estimate artist and\nvenue prior ranks respectively as:\na0=log(1 +fans +mentions= 2)\nb0=jevents in venuesjProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 705We alternatively tried calculating venue prior ranks by\nsumming of the size of all events hosted in a venue, heuris-\ntically estimated by size = maxf0;log(1 + attending +\ninterested= 2 +maybe= 2\u0000noReply= 2\u0000declined )g.\nHowever, this reduced all BiRank evaluations more than\n30% compared to their currently reported values. We com-\npare the following algorithms:\nRaw : Estimates venue ranks as their prior ranks.\nRFE : Feature extraction using random forest ensembles,\nsimilarly to ground truth construction, but aiming to pre-\ndict high artist prior ranks.\nBiRank : BiRank on the artist-venue bipartite graph ex-\ntracted from Facebook data. Unless stated otherwise, this\nmethod uses parameters ra=rv= 0:85, which are a com-\nmon empirical selection for ranking algorithms [8, 12].\nVenueRank : VenueRank on the artist-venue bipartite\ngraph extracted from Facebook data. As argued above,\nVenueRank eventually removes the effect of prior ranks.\nAlthough inconsequential from a theoretical standpoint,\nwe follow previous conventions and reasoning well-\nestablished for BiRank [12], to select the parameters ra=\nrv= 0:85andpa=pv= 0:5, unless stated otherwise.\nEvaluation Measures\nTo evaluate bipartite venue ranking algorithms, we com-\npare the ranks they produce when applied on Facebook\ndata with the ground truth importances extracted from Spo-\ntify data in Subsection 3.2. Our aim is to ﬁnd whether\nvenues are correctly ranked by unsupervised ranking al-\ngorithms. To this end, we measure rank similarities using\nthe robust Spearman correlation coefﬁcient [5], which is\nis computed as a Pearson correlation between the cardinal\nranks of compared quantities. It must be noted that, due to\nthe possibility of negative exposures being found more im-\nportant, the supremum of Spearman correlation can be less\nthan1. This, however, does not affect the fact that Spear-\nman correlations closer to 1indicate that higher ranked\nvenues are more important and thus boast higher exposure.\nAdditionally, if ranks GTlists venues in a descending\norder of their ground truth importances and ranks Cin de-\nscending order of their calculated ranks, we can deﬁne the\noverlap between the top Nvenues:\noverlap (N) =\f\franks GT[0 :N]\\ranks C[0 :Nj\f\f\nN\nTo evaluate the overall overlap curves across all venues,\nwe also measure their Area Under Curve (AUC) [11],\nwhich is a fair method of curve comparison. To calcu-\nlate this area, we perform numerical trapezoid integration\nof overlaps and normalize the result by dividing it with\nthe width of the horizontal axis. Higher AUC values rep-\nresent better ability to recognize both high-exposure and\nlow-exposure venues.\n3.4 Results\nExperiments are performed under two variants of unsuper-\nvised training on Facebook data: venue ranking on thesame 224artists ( 224A ) (including venues with only one\nperformance) as those used for ground truth construction\nand venue ranking using all 542artists ( 542A ) and their\nrespective venues. Since the latter dataset contains more\nartists and venues, it presents a more challenging setting.\nUsing both variants for evaluation helps identify which al-\ngorithms generalize better and are more robust.\nIn Table 2, we can see that BiRank and VenueRank\nachieve high correlation values with the ground truth in\nthe 224A dataset. However, BiRank heavily relies on ac-\ncurate prior ranks to do so and does not perform well in the\nlarger 542A dataset, where it produces worse estimations\ncompared to even its prior ranks. This implies that BiRank\nexhibits overﬁtting characteristics. On the other hand, both\nRFE and VenueRank boast great robustness in that they are\nless affected by the transition to the larger 542A dataset.\nConsequently, VenueRank exhibits high performance and\nis more suited to real-world applications, since it is more\nrobust to artist-venue graph changes.\nSince there exist -to the best of our knowledge- no\nprevious studies that can serve as comparison for venue\ncorrelations, common guidelines [7] suggest that we can\nresort to the Cohen convention [4] to classify extracted\nvenue ranks as strongly correlated between VenueRank\nand ground truth importances across all experiments.\npopularity incm\nAlgorithm 224A 542A 224A 542A\nRaw 36% 38% 44% 42%\nBiRank 70% 33% 76% 28%\nRFE 57% 51% 64% 49%\nVenueRank 71% 63% 69% 60%\nTable 2 : Spearman correlation coefﬁcient between ground\ntruth venue rankings and rankings produced by algorithms.\nFeature importances forming the popularity andincm\nground truths are themselves signiﬁcantly correlated, with\n58% Spearman correlation coefﬁcient. This indicates that\nvenues characterizing popular artists also tend to charac-\nterize higher listener increments and conversely.\nFigure 5 shows the overlap between various algorithms\nand the ground truth for different numbers of top venues.\nWe can see that, for a small number of top venues (i.e. less\nthan 200), ranking methods do not produce high overlap\nwith ground truth venues. However, for a greater number\nof venues, they rank highly a large portion of important\nvenues. A curve over a larger area is more important than\nonly identifying top venues, because ranking methods may\nbe used to compare middle-ranked or low-ranked venues\nto the majority of artists instead of only the most popular\nones. AUC results corroborate the previous ones. In par-\nticular, BiRank again performs better than other methods\nunder perfect information, whereas VenueRank performs\nbetter than other methods and is thus more robust in the\ncase of the more challenging 542A dataset variant.706 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018(a) 224A for popularity\n (b) 542A for popularity\n(c) 224A for incm\n (d) 542A for incm\nFigure 5 : Curves and AUCs of high-ranked venue overlap\nbetween ranking algorithms and the ground truth.\n3.5 Convergence when Ignoring Prior Ranks\nIn Figure 6 we present the convergence time of BiRank\nwith respect to its iterative scheme parameters ra; rb. We\ncan see that execution time increases asymptotically to in-\nﬁnity as prior ranks are ignored, i.e. ra=rb!1. Instead,\nVenueRank exhibits similar behavior for these parameters\nconvergence-wise, and it always converges to the same sta-\ntionary solution, as long as these parameters are not close\nenough to 1to cause numeric errors. Furthermore, that so-\nlution is the same as BiRank when the effect of the prior\nranks is completely eliminated. Hence, when the effect\nof prior ranks is undesireable, it is preferable to employ\nVenueRank instead of selecting BiRank parameters close\nto1, if we want to achieve faster convergence.\nFigure 6 : Convergence time of BiRank (green solid line)\nand VenueRank (dashed black line). VenueRank always\nhas the stationary solution of BiRank with ra=rb!1.\n3.6 Case Study\nFinally, we conduct a case study, where we try to ﬁnd im-\nportant venues in the city of Stockholm, Sweden through\nvenue ranking algorithms. To this end, we used two onlinearticles5;6to gather a total of 10 highly recommended\nvenues and ﬁnd their rankings obtained from running the\nprevious algorithms on all 542artists and 5;041venues.\nIn Table 3 we show their rank within the ordered list of all\n635Stockholm venues in our dataset (rank of the highest-\nranked venue is 1).\nVenueRank places 8 of the 10 venues in the top 50\nranks, whereas other methods place at most 5 of the 10\nvenues in the top 50 ranks. VenueRank’s performance is\ncommendable, given that our dataset also includes popu-\nlar parks and hotels often used for live music acts, which\nwould not be recommended in the above articles, and that\nworse ranks often stem from incomplete data (e.g. the\ndataset contains only two events hosted in ‘Nalen’).\nRaw RFE BiRank VenueRank\nAnnexet 95 105 147 40\nBerwaldhallen 48 94 96 46\nCirkus 67 61 193 23\nDebaser Medis 9 29 9 11\nDebaser Rest. 3 8 4 1\nFasching 55 50 125 33\nNalen 195 97 172 113\nPet Sounds Bar 81 19 343 49\nSodra Teatern 39 1 24 2\nStallet 11 63 13 68\nTable 3 : Rank cardinality for recommended venues com-\npared to other Stockholm venues.\n4. CONCLUSIONS AND FUTURE WORK\nIn this work, we introduce VenueRank as a modiﬁcation\nof the common iterative scheme of bipartite graph ranking\nalgorithms that removes dependence on prior ranks while\nensuring convergence. We then explore ranking algorithms\nthat help identify which venues help predict artist popular-\nity. Experiments on real-life data show that VenueRank ap-\nplied on a Facebook artist-venue graph can robustly iden-\ntify which venues are correlated with more popular artists\nand actively contribute to increasing their Spotify listeners.\nIn particular, in a setting with partially inaccurate informa-\ntion, VenueRank yields substantial improvement compared\nto other unsupervised ranking algorithms.\nIn part, this shows that graph structure can be more im-\nportant than rough social network metrics when predicting\nhigh-exposure venues. Furthermore, it demonstrates that\nthere exists a clear link between graph structure and venue\nexposure that increases artist popularity.\nIn the future, we plan to carry out more detailed exper-\niments on larger datasets. Furthermore, from a theoretical\nperspective, the VenueRank iterative scheme can also be\ncombined with BiRank to produce more robust solutions\nacross the whole parameter space. Finally, we propose im-\nproving venues ranks by taking into account how they con-\ntribute to the exposure of lower popularity artists.\n5https://theculturetrip.com/europe/sweden/articles/\nthe-6-best-live-music-venues-in-stockholm\n6https://scandinaviantraveler.com/en/places/\n7-best-music-venues-in-stockholmProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 7075. ACKNOWLEDGEMENTS\nThe authors would like to thank Playground Music Scan-\ndinavia for providing artist data. This work is partially\nfunded by the European Commission under the contract\nnumber H2020-761634 FuturePulse.\n6. REFERENCES\n[1] Lei Cao, Jiafeng Guo, and Xueqi Cheng. Bipartite\ngraph based entity ranking for related entity ﬁnding.\nInProceedings of the 2011 IEEE/WIC/ACM Interna-\ntional Conferences on Web Intelligence and Intelligent\nAgent Technology-Volume 01 , pages 130–137. IEEE\nComputer Society, 2011.\n[2] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall,\nand W Philip Kegelmeyer. Smote: synthetic minority\nover-sampling technique. Journal of artiﬁcial intelli-\ngence research , 16:321–357, 2002.\n[3] Zhiyong Cheng and Jialie Shen. Just-for-me: An adap-\ntive personalization system for location-aware social\nmusic recommendation. In Proceedings of interna-\ntional conference on multimedia retrieval , page 185.\nACM, 2014.\n[4] Jacob Cohen. Statistical power analysis for the behav-\nioral sciences 2nd edn, 1988.\n[5] Christophe Croux and Catherine Dehon. Inﬂuence\nfunctions of the spearman and kendall correlation mea-\nsures. Statistical methods & applications , 19(4):497–\n515, 2010.\n[6] Hongbo Deng, Michael R Lyu, and Irwin King. A gen-\neralized co-hits algorithm and its application to bipar-\ntite graphs. In Proceedings of the 15th ACM SIGKDD\ninternational conference on Knowledge discovery and\ndata mining , pages 239–248. ACM, 2009.\n[7] Joseph A Durlak. How to select, calculate, and in-\nterpret effect sizes. Journal of pediatric psychology ,\n34(9):917–928, 2009.\n[8] Nadav Eiron, Kevin S McCurley, and John A Tomlin.\nRanking the web frontier. In Proceedings of the 13th\ninternational conference on World Wide Web , pages\n309–318. ACM, 2004.\n[9] Amit Goyal, Francesco Bonchi, and Laks VS Laksh-\nmanan. Learning inﬂuence probabilities in social net-\nworks. In Proceedings of the third ACM international\nconference on Web search and data mining , pages 241–\n250. ACM, 2010.\n[10] Isabelle Guyon and Andr ´e Elisseeff. An introduction\nto feature extraction. In Feature extraction , pages 1–\n25. Springer, 2006.[11] James A Hanley and Barbara J McNeil. The meaning\nand use of the area under a receiver operating charac-\nteristic (roc) curve. Radiology , 143(1):29–36, 1982.\n[12] Xiangnan He, Ming Gao, Min-Yen Kan, and Dingxian\nWang. Birank: Towards ranking on bipartite graphs.\nIEEE Transactions on Knowledge and Data Engineer-\ning, 29(1):57–71, 2017.\n[13] Boris Iglewicz and David Hoaglin. Volume 16: how\nto detect and handle outliers, The ASQC basic refer-\nences in quality control: statistical techniques, Edward\nF . Mykytka . PhD thesis, Ph. D., Editor, 1993.\n[14] Jon M Kleinberg. Authoritative sources in a hyper-\nlinked environment. Journal of the ACM (JACM) ,\n46(5):604–632, 1999.\n[15] Joel C Miller, Gregory Rae, Fred Schaefer, Lesley A\nWard, Thomas LoFaro, and Ayman Farahat. Modiﬁ-\ncations of kleinberg’s hits algorithm using matrix ex-\nponentiation and web log records. In Proceedings of\nthe 24th annual international ACM SIGIR conference\non Research and development in information retrieval ,\npages 444–445. ACM, 2001.\n[16] Erkki Oja and Juha Karhunen. On stochastic approxi-\nmation of the eigenvectors and eigenvalues of the ex-\npectation of a random matrix. Journal of mathematical\nanalysis and applications , 106(1):69–84, 1985.\n[17] Lawrence Page, Sergey Brin, Rajeev Motwani, and\nTerry Winograd. The pagerank citation ranking: Bring-\ning order to the web. Technical report, Stanford Info-\nLab, 1999.\n[18] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duches-\nnay. Scikit-learn: Machine learning in Python. Journal\nof Machine Learning Research , 12:2825–2830, 2011.\n[19] Xiaoguang Rui, Mingjing Li, Zhiwei Li, Wei-Ying Ma,\nand Nenghai Yu. Bipartite graph reinforcement model\nfor web image annotation. In Proceedings of the 15th\nACM international conference on Multimedia , pages\n585–594. ACM, 2007.\n[20] Yvan Saeys, Thomas Abeel, and Yves Van de Peer. Ro-\nbust feature selection using ensemble feature selection\ntechniques. In Joint European Conference on Machine\nLearning and Knowledge Discovery in Databases ,\npages 313–325. Springer, 2008.\n[21] Markus Schedl and Dominik Schnitzer. Location-\naware music artist recommendation. In International\nConference on Multimedia Modeling , pages 205–213.\nSpringer, 2014.708 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Visualization of Audio Data Using Stacked Graphs.",
        "author": [
            "Mathieu Lagrange",
            "Mathias Rossignol",
            "Grégoire Lafay"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492531",
        "url": "https://doi.org/10.5281/zenodo.1492531",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/96_Paper.pdf",
        "abstract": "In this paper, we study the benefit of considering stacked graphs to display audio data. Thanks to a careful use of layering of the spectral information, the resulting display is both concise and intuitive. Compared to the spectrogram display, it allows the reader to focus more on the temporal aspect of the time/frequency decomposition while keeping an abstract view of the spectral information. The use of such a display is validated using two perceptual experiments that demonstrate the potential of the approach. The first considers the proposed display to perform an identification task of the musical instrument and the second considers the proposed display to evaluate the technical level of a musical performer. Both experiments show the potential of the display and potential applications scenarios in musical training are discussed.",
        "zenodo_id": 1492531,
        "dblp_key": "conf/ismir/LagrangeRL18",
        "keywords": [
            "stacked graphs",
            "audio data",
            "layering spectral information",
            "concise and intuitive",
            "temporal aspect",
            "spectral information",
            "perceptual experiments",
            "identification task",
            "technical level",
            "musical training"
        ],
        "content": "VISUALIZATION OF AUDIO DATA USING STACKED GRAPHS\nMathieu Lagrange\u0003, Mathias Rossignol, Gr ´egoire Lafay\u0003\n\u0003LS2N, CNRS, ´Ecole Centrale de Nantes\nmathieu.lagrange@cnrs.fr\nABSTRACT\nIn this paper, we study the beneﬁt of considering\nstacked graphs to display audio data. Thanks to a care-\nful use of layering of the spectral information, the result-\ning display is both concise and intuitive. Compared to the\nspectrogram display, it allows the reader to focus more on\nthe temporal aspect of the time/frequency decomposition\nwhile keeping an abstract view of the spectral information.\nThe use of such a display is validated using two per-\nceptual experiments that demonstrate the potential of the\napproach. The ﬁrst considers the proposed display to per-\nform an identiﬁcation task of the musical instrument and\nthe second considers the proposed display to evaluate the\ntechnical level of a musical performer. Both experiments\nshow the potential of the display and potential applications\nscenarios in musical training are discussed.\n1. INTRODUCTION\nThe visual display of quantitative information [13] is at the\ncore of the growth of human knowledge as it allows human\nbeings to go beyond the limitation of natural languages in\nterms of precision and scale.\nDeﬁning what is the essence of a good visual display of\nquantitative data is non trivial and usually domain speciﬁc.\nThat said, in most scientiﬁc ﬁelds, such displays serve two\nmajors goals: 1) the routine interaction of the researcher\nwith the data or the physical phenomenon and 2) the need\nof the researcher to motivate its claim to its peers. Both\ntasks require the display to fulﬁll the simplicity rule both\nin terms of production and design. First, the display shall\nbe computed and adapted according to the need of the re-\nsearcher very efﬁciently in order to allow an effective ex-\nploration of the data. Second, the display shall be able to\nconvey at the ﬁrst glance an important qualitative aspect\nabout the data.\nThis paper is about the visualization of audio data, and\naudio data is originally made to be listened to. There-\nfore, we shall keep in mind that ”all visual projections of\nsounds are arbitrary and ﬁctitious” [11]. That said, even\nif recorded versions of sounds can now be played back\nc\rMathieu Lagrange\u0003, Mathias Rossignol, Gr ´egoire\nLafay\u0003. Licensed under a Creative Commons Attribution 4.0 Interna-\ntional License (CC BY 4.0). Attribution: Mathieu Lagrange\u0003, Mathias\nRossignol, Gr ´egoire Lafay\u0003. “Visualization of audio data using stacked\ngraphs”, 19th International Society for Music Information Retrieval Con-\nference, Paris, France, 2018.at convenience, it is still useful to represent them graph-\nically as listening depends on time. On contrary, the vi-\nsual display allows the reader to grasp a global view of\nthe waveform at a glance. Also, the eye is less subject to\nstimulation fatigue and the visual display is very powerful\nto convey evidence as we are still fully into the print cul-\nture that since the Gutenberg invention gives an ”uncritical\nacceptance [to] visual metaphors and models” [8].\nWe propose in this paper a display of audio data that is\nintuitive and gives information about the main dimensions\nof sound in a compact manner using stacked graphs [3].\nThe display can be computed easily and efﬁciently1. In\norder to put this display into context, an overview of the\nroutinely used type of displays is given, respectively from\nthe perspective of the musician composer in Section 2 and\nthe physicist in Section 3. We shall argue that the proposed\ndisplay fully described in Section 4 can be thought of as the\nphysicist’s counterpart to a notational system introduced\nby Schafer [11].\nThe display is then evaluated and compared to the com-\nmonly used waveform and spectrogram displays with two\nperceptual experiments. In the ﬁrst experiment, the sub-\njects have to distinguish between tones of different musi-\ncal instruments by listening to the sounds or by consider-\ning the visual displays under evaluation. The protocol and\nthe results for this experiment are presented in Section 5.\nIn the second experiment, the subjects are asked to distin-\nguish between saxophone performances of different level\nof instrumental expertise by listening to the sound and by\nconsidering the displays under evaluation. The protocol\nand the results for this experiment are presented in Sec-\ntion 6.\n2. ABOUT NOTATION\nFrom the phonetic alphabet for speech to the musical score\nfor music, notation consists in putting together on a one or\ntwo-dimensional space symbols describing speciﬁc sound\nevents. In a manner probably inherited from writing, time\nsequencing is usually depicted from left to right in the\nWestern musical culture. Speciﬁc to the musical score is\nthe use of the vertical axis to depict the pitch. A musi-\ncal tone is therefore solely described in terms of time of\nappearance, duration, pitch and sometimes intensity. As\nsuch, the score is largely prescriptive and gives a tremen-\ndous amount of freedom to the musical performer in terms\n1A reference implementation as well as all the data discussed in\nthis paper is available at https://bitbucket.org/mlagrange/\npaperaudiostackgraph771Attack Body Decay\nDuration moderate non-existent slow\nFrequency steady low\nFluctuations transient steady-state\nDynamics loud to soft\nDuration  \u00003 seconds\u0000!\nFigure 1 : Annotation of a church bell from Schafer [11].\nof interpretation.\nIn an intent to provide a more descriptive notation of\nmusical objects, Schaeffer [10] designed a ”solf `ege des\nobject musicaux” that extensively apprehend the descrip-\ntion of any kind of sound object. Perhaps because of its\ncomplexity this notation is hardly used today. In an ef-\nfort to simplify this notation, Schafer proposed a notational\nsystem that can be considered for describing any kind of\nsound, be it a unique event or any kind of compound. The\nmain rationale is to split the temporal axis from left to right\ninto 3 parts corresponding to the attack ,sustain anddecay .\nFor each part, its duration, frequency (related to the notion\nof mass as introduced by Schaeffer), ﬂuctuations (related\nto the notion of grain as introduced by Schaeffer) and dy-\nnamics are displayed from top to bottom. Except for the\nfrequency content that is depicted as a rough spectrogram\ncontour, the other dimensions are described according to\na speciﬁc alphabet of a few symbols. An example taken\nfrom [11] of such annotation is given on Figure 1 for the\nsound of a church bell.\n3. ABOUT MEASURE\nWhen dealing with sound as a physicist, one wants to quan-\ntify mechanical properties and display them precisely. As\nin notation, the main aspect that is commonly looked for is\nthe distribution of energy across frequency and time. The\ndistribution of energy as a function of the modulation rate\nand the frequency scale of observations are less considered\nin the signal processing literature [2,4] but are shown to be\nperceptually important [5, 14].\nTherefore, in order to display a sound on a two-\ndimensional plane, one has to resort to a choice or a com-\npromise. Either timing is emphasized and frequency ne-\nglected as in the waveform display 2a or frequency is\nemphasized and timing neglected as in the display of the\nFourier spectrum 2b. A compromise is made by consider-\ning time and frequency respectively as horizontal and ver-\ntical axes of the two-dimensional plane as with the popu-\nlar spectrogram magnitude of the short term Fourier trans-\nform , see Figure 2c. In such display, the use of a color code\nconveys information about energy.\nThat said, we believe that the spectrogram display still\nfavors frequency over time. Spectral structure can be ana-\nlyzed precisely, for example harmonicity, modulations, etc.\nConversely, temporal dynamics and structure are harder to\nappreciate, as the way energy ﬂuctuates in each sub bands\nhas to be reconstructed from the color code.\nThe spectrogram is a display that is thus in our opinion\nvery powerful for close inspection of a sound event that is(a) Waveform\n(b) Spectrum\n(c) Spectrogram\nFigure 2 : Standard displays of the sound of a church bell.\nactive over a short period of time. Indeed, enlarging the\ntime resolution quickly blurs the frequency resolution and\nmay lead to a completely non informative display.\n4. VISUALIZING SPECTRAL CONTENT USING\nSTACKED GRAPH\nWith those limitations in mind, we propose in this paper\nto take a compromise that conversely favors time over fre-\nquency. In such display, the plane is therefore organized\nwith time and energy as the horizontal and vertical axes\nrespectively. The frequency is displayed as stacked layers\ndisplaying the level of energy across frequency sub bands\nof growing frequency range. Those layers can have colors\nassigned.\nWe seek a display that depicts information that is per-772 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Mel scaled\nmagnitude\nspectrogramStackingAudio spack\nFigure 3 : Processing chain of the spack display.\nFigure 4 : Spectral stack display (spack) of the sound of a church bell. The color code conveys nicely the modulation within\neach frequency band and the overall disappearance of the high frequency range.\nceptually meaningful. Therefore, we consider spectral data\nprojected on a Mel-scale [12].\nIn order to improve legibility, colors are assigned to\nfrequency layers according to their ranges with a color\ncode ranging from blue (low frequency) to yellow (high\nfrequency). The blue color is often associated with large\nphenomena, with the following adjectives: celestial, calm,\ndeep, whereas the yellow color is often associated with\ntransient phenomena that are highly energetic. Kandinsky\nin [7] states that ”Blue is comparable to low pitched organ\nsounds. Yellow becomes high pitched and can not be very\ndeep”. The color code is then chosen to be a linear gradi-\nent from blue (low frequency range) through green (middle\nfrequency range) to yellow (high frequency range). In this\npaper, the gradient follows the LCH color model speciﬁed\nby the Commission Internationale de l’ ´Eclairage (CIE) so\nthat the perceived brightness appears to change uniformly\nacross the gradient while maintaining the color saturation.\nWe argue that this display, termed spectral stack\n(spack), convey useful information about the sound. In\nparticular, it conveys nicely, aside of ﬁne details, the im-\nportant dimensions retained by Schafer, see Figure 1.\nTo compute the spack display, a mel-scaled magnitude\nspectrogram is computed from the audio, see Figure 3. To\neach mel spectral band is assigned a given color code from\ndark blue (low frequency) to yellow (high frequency). At\neach time frame, the spack display is a stacking of the mag-\nnitude values of each mel frequency band, see Figure 4.\n5. TASK 1: IDENTIFYING THE MUSICAL\nINSTRUMENT\nThe identiﬁcation of the musical instrument used to play\na tone rely largely on 2 factors, the spectral envelope and\nthe attack [1, 6]. The spack display shall be able to conve-\nniently display those factors. Indeed, the spectral envelope,\ni.e.the distribution of the energy across frequency is en-\ncoded using the stacking axis and color code. The attack is\nFigure 5 : Classiﬁcation performance of the different dis-\nplays on Task 1 (identifying the musical instrument):\nsound (S) waveform (W), spectrogram (Spe) and spack\n(Spa). The star shows the average performance and the\nlength of the vertical line is twice the standard deviation.\nalso well displayed as the spack focuses on the display of\nenergy through time.\n5.1 Protocol\nSeveral tones played by four musical instruments: piano,\nviolin, trumpet, and ﬂute are considered as stimuli. Each\ninstrument is played mezzo forte at 5 different pitches: C,\nD, E, F and G. For each sound, three visual representa-\ntions are evaluated: waveform (W), spectrogram (Spe) and\nspack (Spa). For reference, the sound (S) is also consid-\nered2.\nThe test is a forced-choice categorization task. The\nsounds are displayed by gray dots on a 2 dimensional plane\ndisplayed on a computer screen. The dots can be moved\n2The sounds and the visual displays are available on the companion\nwebsiteProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 773freely within this plane and colored using 4 different col-\nors, each corresponding to a given instrument. The corre-\nspondence is given to the subjects at the beginning of the\nexperiment by the instructor: piano (black), violin (red),\ntrumpet (magenta), and ﬂute (green). If the sound modal-\nity is tested, the sound is played when the dot is clicked.\nIf a visual modality is tested, the corresponding display is\nshown when the dot is clicked using the mouse.\nEight subjects, studying at the Engineering school\n”Ecole Centrale de Nantes”, aged from 24 to 26 years,\nperformed the test. Each subject reported normal hearing.\nThey performed the test at the same time in a quiet en-\nvironment using headphones. The sound level was set to\na comfortable level before the experiment. A short intro-\nduction was given by the instructor for each display with a\nfocus on the meaning of the axes and the color code. The\nsubjects performed the evaluation using the sound modal-\nity ﬁrst. The order of the three remaining modalities are\nordered randomly among subjects to reduce the impact of\nprecedence. The test is over when the subjects have as-\nsigned a color to each dot, this for all the evaluated modal-\nities.\n5.2 Results\nClassiﬁcation performance is evaluated as the number of\ncouple of sounds played by the same instrument that have\nbeen assigned the same color divided by the number of\ncouples. As can be seen on Figure 5, the task is trivial\nwhen listening to the sound, as the subjects achieve per-\nfect classiﬁcation. On overall, the classiﬁcation is quite\ngood for each of the graphical displays with a higher av-\nerage performance for the spack display. Subjects verbally\nreported ease of use for the spack display.\n6. TASK 2: ASSESSING THE LEVEL OF A\nSAXOPHONE PERFORMANCE\nThe control of the breath while playing the saxophone is\ncrucial and can be monitored to assess the technical level\nof a saxophone player [9]. For example, playing a single\ntone with sharp attack and constant amplitude during the\nsteady state is non trivial and requires years of practice.\nProfessional players typically practice such exercises on\na daily basis as warm-ups and perform them with a trainer\nto get criticisms in order to improve their skills. Using\ngraphical displays of their performance could be useful for\nthem to spot during or after the performance. In order to be\nefﬁcient, such display shall be intuitive with a few degrees\nof freedom in order to be easy to understand.\nThe validation of the spack display for such pedagog-\nical needs is out of the scope of this paper. Nonetheless,\nwe designed here a task that can demonstrate how several\nmeaningful characteristics of the saxophone performance\ncan be identiﬁed only by considering the graphical displays\nunder evaluation.\nIn this kind of training, it could be useful for the trainer\nto have some kind of display of its performance. As the\ncrucial part is to be able to control the air ﬂow while play-\ning in order to keep a stable amplitude and timbre, we hy-(a) Waveform\n(b) Spectrogram\n(c) Spack\nFigure 6 : Graphical displays of forte B tone. Several per-\nformance issues can be observed: lack of airﬂow control at\nthe attack, change of pitch and loudness at 3 seconds and\nlack of steady airﬂow during the whole performance.\npothesize that the spack display may be a good candidate\nfor such a task.\n6.1 Protocol\nThe stimuli considered in this experiment are recorded per-\nformances of four saxophone players with a technical level\nassumed to be high or low (2 low, 2 high). Each player\nplayed several tones at pitch B and G. They were asked\nto play each note in three different ways: piano ,forte and\ncrescendo decrescendo3.\nThe test follows a XXY structure, where three perfor-\nmances are shown to the subject, one is at a given level\n(high or low) and the other two of the other level (low\nor high). The subject is then asked, based solely on the\nmodality at hand, to select the one that is different from\nthe two others. 24 triplets are randomly selected from the\n3The sounds and the visual displays are available on the companion\nwebsite774 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018(a) Waveform\n(b) Spectrogram\n(c) Spack\nFigure 7 : Graphical displays of another forte B tone. Sev-\neral performance issues can be observed, for example: lack\nof sharpness at the attack, change of timbre and loudness\nat 5 seconds.\nvalid combinations of the above described stimuli.\n16 subjects, studying at the Engineering school ”Ecole\nCentrale de Nantes”, aged from 24 to 28 years, performed\nthe test in two sessions, 9 for the ﬁrst session, and 7 for\nthe second session. Each subject reported normal hear-\ning. For each session, they performed the test at the same\ntime in a quiet environment using headphones. The sound\nlevel was set to a comfortable level before the experiment.\nA short introduction was given by the instructor for each\ndisplay with a focus on the meaning of the axis and the\ncolor code. The subjects performed the evaluation using\nthe sound modality ﬁrst. The order of the three remain-\ning modalities are ordered randomly among subjects to re-\nduce the impact of precedence. The test is over when the\nsubjects have examined the 24 triplets for the 4 evaluated\nmodalities.\nFigure 8 : Boxplot display of the differentiation perfor-\nmance of the different displays on Task 2 (detecting the\nlevel of the saxophone player): sound (S), waveform (W),\nspectrogram (Spe) and spack (Spa).\nTable 1 : Results of the repeated measure ANOV A evalu-\nating the effect of the type of display on the performance.\nsum sq. df mean sq. F p-value\nType 0.13 3 0.045 5.3 0.003\nError 0.38 45 0.008\n6.2 Results\nFor each modality, the number of correct selection is aver-\naged among the 24 triplets and then averaged among sub-\njects. As can be seen on Figure 8, the task is more complex\nthan task 1, as the score achieved using the sound modality\nis lower than task 1. This might be due to the fact that the\ntask is less explicit than task 1. For the visual displays, the\nsame ranking as task 1 is observed with a larger difference\nbetween each modality.\nA repeated measure ANOV A is used to test the poten-\ntial signiﬁcance of the type of display on the differentia-\ntion performance. A mauchly test reveals that the default\nof sphericity is not signiﬁcant, thus no correction of the\ndegrees of freedom of the Fisher test is needed. Table 1\npresents the results of the Fisher test showing that the ef-\nfect of the representation is signiﬁcant p= 0:003. In\naddition, a multiple comparison test shows that the only\nsigniﬁcant differences are between Waveform and Spack\np= 0:03and Waveform and Sound p= 0:003. No signiﬁ-\ncant difference is found between the remaining modalities:\nthe Sound, the Spectrogram and the Spack displays.\nThus, if considering the graphical displays solely, only\nthe Spack displays signiﬁcantly improves upon the Wave-\nform display. As can be seen on Figure 8, The spectro-\ngram display have the largest dispersion of correct answer\nrate, i.e.the ratio of correct responses over the number of\npossible responses, termed p(c) in the following. Consid-\nering the distribution of p(c) for the spectrogram display\nshown on Figure 9, two modes can be observed contrary\nto the one of the spack display. Even though each sub-Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 775jects have been given the same introduction to each of the\ngraphical displays, their familiarity with the standard dis-\nplays may vary since some subjects had previous training\nin signal processing courses. This may explain the higher\nmode in the distribution of the spectrogram display. Even\nif this observation shall be considered with care due to the\nrather low number of subjects, this can lead us to conjec-\nture about the inﬂuence of the familiarity of the subjects\nwith the spectrogram display on the reported performance.\nThe spack display does not exhibit the same distribution\nproﬁle and prior familiarity cannot be assumed as the dis-\nplay was equally new to all subjects.\nFigure 9 : Histogram of the classiﬁcation performance for\nthe spectrogram (Spe) and the spack (Spa) displays. Only\nthe spectrogram display exhibit two modes, suggesting dif-\nferent levels of expertise of the subjects.\n7. CONCLUSIONS\nIn this paper, we proposed a display based on the stacking\nof the envelopes of logarithmically spaced band pass ﬁl-\nters. We have shown qualitatively that this kind of display\nmay have some potential as it conveys nicely the distri-\nbution of the energy across time and frequency in a way\nthat is an alternative to the one taken when considering the\nspectrogram.\nWhen considering two evaluation tasks: 1) identifying\nthe type of instrument played, and 2) identifying at which\nskill level a saxophone tone is played, the spack display\ncompares favorably to more conventional displays, such as\nthe waveform and spectrogram displays. Subjects reported\nease of understanding and quick access to important as-\npects of the sounds.\nFuture work will focus on the design of validation tasks\nfor the spack display using a wider range of audio data,\nnamely speech and environmental data.\nAs the spack display is both compact and intuitive, it\ncan be considered as an inspection tool while practicing a\nmusical instrument in order to monitor the control of the\nnuance and the timbre while playing. Evaluation of the\nspack display in such a training use case would thus be of\ninterest.8. ACKNOWLEDGMENTS\nThe authors would like to acknowledge support for this\nproject from ANR project Houle (grant ANR-11-JS03-\n005-01) and ANR project Cense (grant ANR-16-CE22-\n0012).\n9. REFERENCES\n[1] Trevor R Agus, Clara Suied, Simon J Thorpe, and\nDaniel Pressnitzer. Fast recognition of musical sounds\nbased on timbre. The Journal of the Acoustical Society\nof America , 131(5):4124–4133, 2012.\n[2] Joachim Anden and Stephane Mallat. Multiscale Scat-\ntering for Audio Classiﬁcation. In ISMIR , 2011.\n[3] L Byron and M Wattenberg. Stacked Graphs-Geometry\n& Aesthetics. IEEE Trans. Vis. Comput. Graph. , 2008.\n[4] Taishih Chi, Powen Ru, and Shihab Shamma. Multires-\nolution spectrotemporal analysis of complex sounds.\nThe Journal of the Acoustical Society of America ,\n118(2):887, 2005.\n[5] Torsten Dau, Birger Kollmeier, and Armin Kohlrausch.\nModeling auditory processing of amplitude modula-\ntion. i. detection and masking with narrow-band carri-\ners.The Journal of the Acoustical Society of America ,\n102(5):2892–2905, 1997.\n[6] John M Grey. Multidimensional perceptual scaling of\nmusical timbres. the Journal of the Acoustical Society\nof America , 61(5):1270–1277, 1977.\n[7] W. Kandinsky. Concerning the spiritual in art . Dover\npublications, 1954.\n[8] M McLuhan. The Gutenberg Galaxy . University of\nToronto Press, 1963.\n[9] Matthias Robine and Mathieu Lagrange. Evaluation of\nthe technical leval of saxophone performers by consid-\nering the evolution of spectral parameters of the sound.\nInISMIR , pages 79–84, 2006.\n[10] P Schaeffer. Trait ´e des objets musicaux .´Editions Du\nSeuil, 1966.\n[11] RM Schafer. The soundscape: Our sonic environment\nand the tuning of the world . Destiny books, Rochester,\nVermont, 1977.\n[12] SS Stevens, J. V olkmann, and E. B. Newman. A scale\nfor the measurement of the psychological magnitude\npitch. The Journal of the Acoustical Society of Amer-\nica, 185(8), 1937.\n[13] E.R. Tufte. The Visual Display of Quantitative Infor-\nmation , volume 7. Graphics press Cheshire, CT, 1983.\n[14] Xiaowei Yang, Kuansan Wang, and Shihab A Shamma.\nAuditory representations of acoustic signals. IEEE\ntransactions on information theory , 38(2):824–839,\n1992.776 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "A Predictive Model for Music based on Learned Interval Representations.",
        "author": [
            "Stefan Lattner",
            "Maarten Grachten",
            "Gerhard Widmer"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492331",
        "url": "https://doi.org/10.5281/zenodo.1492331",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/179_Paper.pdf",
        "abstract": "Connectionist sequence models (e.g., RNNs) applied to musical sequences suffer from two known problems: First, they have strictly \"absolute pitch perception\". Therefore, they fail to generalize over musical concepts which are commonly perceived in terms of relative distances between pitches (e.g., melodies, scale types, modes, cadences, or chord types). Second, they fall short of capturing the concepts of repetition and musical form. In this paper we introduce the recurrent gated autoencoder (RGAE), a recurrent neural network which learns and operates on interval representations of musical sequences. The relative pitch modeling increases generalization and reduces sparsity in the input data. Furthermore, it can learn sequences of copy-and-shift operations (i.e. chromatically transposed copies of musical fragments)—a promising capability for learning musical repetition structure. We show that the RGAE improves the state of the art for general connectionist sequence models in learning to predict monophonic melodies, and that ensembles of relative and absolute music processing models improve the results appreciably. Furthermore, we show that the relative pitch processing of the RGAE naturally facilitates the learning and the generation of sequences of copy-and-shift operations, wherefore the RGAE greatly outperforms a common absolute pitch recurrent neural network on this task.",
        "zenodo_id": 1492331,
        "dblp_key": "conf/ismir/LattnerGW18",
        "keywords": [
            "Connectionist sequence models",
            "Absolute pitch perception",
            "Generalization over musical concepts",
            "Relative pitch modeling",
            "Sparsity in input data",
            "Learning repetition structure",
            "Chromatic transposed copies",
            "Improving state of the art",
            "Ensembles of models",
            "Copy-and-shift operations"
        ],
        "content": "A PREDICTIVE MODEL FOR MUSIC BASED ON LEARNED INTERVAL\nREPRESENTATIONS\nStefan Lattner1;2, Maarten Grachten1;2, Gerhard Widmer1\n1Institute of Computational Perception, JKU Linz\n2Sony Computer Science Laboratories (CSL), Paris, France\nABSTRACT\nConnectionist sequence models (e.g., RNNs) applied to\nmusical sequences suffer from two known problems: First,\nthey have strictly “absolute pitch perception”. Therefore,\nthey fail to generalize over musical concepts which are\ncommonly perceived in terms of relative distances between\npitches (e.g., melodies, scale types, modes, cadences, or\nchord types). Second, they fall short of capturing the con-\ncepts of repetition and musical form. In this paper we\nintroduce the recurrent gated autoencoder (RGAE), a re-\ncurrent neural network which learns and operates on in-\nterval representations of musical sequences. The relative\npitch modeling increases generalization and reduces spar-\nsity in the input data. Furthermore, it can learn sequences\nof copy-and-shift operations (i.e. chromatically transposed\ncopies of musical fragments)—a promising capability for\nlearning musical repetition structure. We show that the\nRGAE improves the state of the art for general connec-\ntionist sequence models in learning to predict monophonic\nmelodies, and that ensembles of relative and absolute mu-\nsic processing models improve the results appreciably. Fur-\nthermore, we show that the relative pitch processing of the\nRGAE naturally facilitates the learning and the generation\nof sequences of copy-and-shift operations, wherefore the\nRGAE greatly outperforms a common absolute pitch re-\ncurrent neural network on this task.\n1. INTRODUCTION\nThe objective of sequence models for music prediction is\nto predict (the probability of) musical events at the next\ntime step, given some prior musical context. In the (most\ncommon) case of predicting note events, this task involves\nﬁnding relationships between past and future occurrences\nof absolute pitches. However, many music theoretical con-\nstructs that might help to ﬁnd such relationships are de-\nﬁned in relative terms, such as diatonic scale steps, and\ncadences. The discrepancy between the relative nature of\nmany regularities in music and the absolute pitch represen-\ntation is problematic for modeling tasks, because it leads to\nc\rStefan Lattner, Maarten Grachten, Gerhard Widmer. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Stefan Lattner, Maarten Grachten, Gerhard\nWidmer. “A predictive model for music based on learned interval rep-\nresentations”, 19th International Society for Music Information Retrieval\nConference, Paris, France, 2018.high sparsity in the input data, increased model sizes, and\naltogether reduced generalization in music modeling.\nTo remedy these problems, musical input sequences can\nbe transposed to a common key before training, augmented\nby random transpositions during training, or, in case of\nsymbolic monophonic music, transformed into interval rep-\nresentations before training. In this work, we propose a\nsequence model which learns both interval representations\nfrom absolute pitch sequences and temporal dependencies\nbetween these intervals. By learning not only the inter-\nvals between two successive notes, but all intervals within\na window of npitches, the model is more robust to dia-\ntonic transposition and can also learn repetition structure.\nMore precisely, a recurrent neural network (RNN) is em-\nployed on top of a gated autoencoder (GAE), which we re-\nfer to as recurrent gated autoencoder (RGAE). The GAE\nportion learns the intervals between its input and its target\npitches and represents them in its latent space. The RNN\nportion operates on these interval representations, to learn\ntheir temporal dependencies. The implicit transformation\nto intervals allows this architecture to operate directly on\nabsolute musical textures, without the need for data pre-\nprocessing. Besides, relative pitch modeling reduces the\nsparsity in the data and the representations learned by the\nGAE are transposition-invariant. Therefore, the RGAE\nrequires less temporal connections than a common RNN\nwhile achieving higher prediction accuracy.\nAlso, operating on the intervals of input sequences brings\nadded value to sequence modeling. By allowing the model\nto relate its prediction with events using speciﬁc time lags,\nit can learn copy-and-shift operations. In the space of inter-\nvals, such operations are performed by repeatedly applying\na constant interval to events occurring a constant time lag\nin the past. Moreover, the RNN portion of the architec-\nture can learn sequences of such copy-and-shift operations\n(i.e., “structure schemes”), which can then be realized as\nmusical notes by the GAE.\nThis ability is promising for music modeling, where\nmusical form deﬁnes the self-similarity within a piece, and\nrepeated sections often occur as a transposed (i.e., shifted\nin the pitch dimension) version of the initial section. Mu-\nsical form is challenging to learn with common sequence\nmodels, like RNNs. They are specialized in learning the\nstatistics of musical textures and are “blind” towards simi-\nlarity and (transposed) repetition (i.e., there is no content-\nindependent “repetition neuron”). As a result, when sam-\npling music using such models, repeated fragments occur26either due to chance or as a phenomenon of an entangle-\nment with a learned texture. In contrast, the ability of\nRGAEs to learn copy-and-shift operations may allow to\nrepresent musical form explicitly, and to realize learned\nschemes as musical textures in music prediction and music\ngeneration tasks.\nWe show that the RGAE is competitive with state-of-\nthe-art models in a music sequence learning task. Fur-\nthermore, we demonstrate that the RGAE, due to its rel-\native pitch processing, is complementary to absolute pitch\nmodels, by combining their predictions to obtain improved\naccuracy. Lastly, we show that the RGAE is particularly\nsuited for learning sequences of copy-and-shift operations.\nIt can learn to recognize and continue pre-deﬁned “struc-\nture schemes”, abstracted from the actual texture, with\nwhich the scheme is realized.\nIn Section 2, we provide an overview of related mod-\nels and related publications. In Section 3, the GAE and\nthe proposed extensions to the RGAE are described, as\nwell as the baseline RNN used for comparison and com-\nbined prediction. General training details concerning the\nGAE are given in Section 4. The two experiments con-\nducted, including the data used, training details and dis-\ncussion for each experiment separately, are presented in\nSection 5. Section 6 concludes the paper and provides fur-\nther directions.\n2. RELATED WORK\nGAEs are bi-linear models utilizing multiplicative interac-\ntions to learn correlations between or within data instances.\nThey were introduced by [15] as a derivative of the gated\nBoltzmann machines (GBMs) [17, 18], as standard learn-\ning criteria became applicable through the development of\ndenoising autoencoders [28]. In music, bi-linear models\nwere applied to learn co-variances within spectrogram data\nfor music similarity estimation [25], and for learning mu-\nsical transformations in the symbolic domain [11].\nThe GAE was utilized for learning the derivatives of se-\nquences in [16] (between subsequent frames in movies of\nrotated 3D objects), and to predict accelerated motion by\nstacking two layers to learn second-order derivatives [19].\nThis method is very similar to the one proposed here, but\nwe use different dimensionalities between input and out-\nput, and we do not assume constant transformations but\nrather learn sequences of transformations using an RNN.\nProbabilistic n-gram models, specialized on learning to\npredict monophonic pitch sequences include IDyOM [23],\nand [10], both employ multiple features of the musical sur-\nface. In this paper, we do not compare the RGAE with\nthese models, as they are more specialized on the musi-\ncal domain, by explicit selection of (computed) features.\nWe compare the RGAE to the currently best performing\ngeneral connectionist sequence model, the RTDRBM [1].\nIts architecture is similar to the well-known RTRBM pro-\nposed in [27], but it employs a different cost function.\nFor structured sequence generation, Markov chains to-\ngether with pre-deﬁned repetition structure schemes were\nemployed in [4], where speciﬁc methods for handling tran-sitions between repeating segments were proposed; in [20],\nwhere an approach to a controlled creation of variations\nwas introduced; in [5], where chords were generated, obey-\ning a pre-deﬁned repetition structure. In [12], a convolu-\ntional restricted Boltzmann machine was employed, and\ndifferent structural properties were imposed using differ-\nentiable soft-constraints and gradient descent optimization.\nA constrained variable neighborhood search to generate\npolyphonic music obeying a tension proﬁle and the repe-\ntition structure from a template piece was proposed in [7].\nIn [6], Markov chains and evolutionary algorithms were\nused to generate repetition structure for Electronic Dance\nMusic.\n3. MODELS\n3.1 Gated Autoencoder\nA GAE learns ﬁrst-order derivatives between its input and\nits output. In musical sequences, this amounts to learning\npitch intervals, which are represented as distinct codes in\nits latent space. In reconstruction, it applies learned inter-\nval codes to pitches in order to transpose them. Its ability to\nlearn and to perform musical transformations is, however,\nnot limited to single intervals. For example, it was shown\nin [11], that more complex musical transformations like di-\natonic transposition can be learned by a GAE and can be\napplied to an unseen material. Intervals are encoded in the\nlatent space of the GAE, denoted as mappings\nmt+1=\u001bq(Wm(Qxt\nt\u0000n\u0001Vxt+1)); (1)\nwhere xt+1is a binary vector encoding active notes at time\nstept+1as on-bits, xt\nt\u0000ncontain the concatenated vectors\nof the lastntime steps, Q;VandWmare weight matri-\nces, and\u001bqis the softplus non-linearity. The operator \u0001\n(indicated as a triangle in Figure 1) depicts the Hadamard\nproduct of the ﬁlter responses Qxt\nt\u0000nandVxt+1, denoted\nasfactors . This operation allows the model to relate its\ninputs, making it possible to learn interval representations.\nGAEs are often trained by minimizing the symmetric\nerror when reconstructing the output from the input and\nvice versa. In the proposed RGAE architecture, we use\npredictive training and just learn to reconstruct the target\nxt+1from the input xt\nt\u0000nand the mapping mt+1as\n~ xt+1=\u001bg(V>(W>\nmmt+1\u0001Qxt\nt\u0000n)); (2)\nwhere\u001bgis the sigmoid non-linearity. The GAE portion of\nthe RGAE is pre-trained by minimizing the binary cross-\nentropy loss of the reconstruction as\nL(x;~ x) =\u00001\nNNX\nn=1\u0014\nxnlog2~xn+(1\u0000xn) log2(1\u0000~xn)\u0015\n:\n(3)\n3.2 Recurrent Gated Autoencoder\nThe proposed model is a combination of a gated autoen-\ncoder (GAE) and a recurrent neural network (RNN) as de-\npicted in Figure 1. The GAE learns relative pitch (i.e., in-\nterval) representations of the musical surface, and the RNN\nlearns their temporal dependencies.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 27RNN GAEFigure 1 : Schematic illustration of the proposed recurrent\ngated autoencoder architecture. Arrows represent weight\nmatrices, rounded rectangles represent vectors. The trian-\ngles depict the Hadamard product. The speciﬁcs of the\ngated recurrent unit are omitted for better clarity.\nWe use gated recurrent units (GRUs) [2] for the RNN\nportion of the RGAE. This type of units have been shown\nto be often as efﬁcient as long short-term memory units\n(LSTMs, [9]) while being conceptually simpler [3]. It is\nintuitively clear that any RNN variant can be potentially\nattached on a GAE. The input to the RNN at time t is the\nGAE’s mapping mt, resulting in the following speciﬁca-\ntion:\nzt=\u001bg(Wzmt+Uzht\u00001+bz); (4)\nrt=\u001bg(Wrmt+Urht\u00001+br); (5)\nht=zt\u0001ht\u00001+(1\u0000zt)\u0001\u001bh(Whmt+Uh(rt\u0001ht\u00001)+bh);\n(6)\nwhere htis the hidden state at time t,ztis the update gate\nvector, rtis the reset gate vector, and W,Uandbare pa-\nrameter matrices and vectors. The RNN predicts the next\nmapping of the GAE as\nemt+1=\u001bq(Uoht); (7)\nwhich is used to reconstruct the target conﬁguration at t+1\nas\n~ xt+1=\u001bs(V>(W>\nmemt+1\u0001Qxt\nt\u0000n)): (8)\nHere, we use the softmax non-linearity \u001bs, as the data\nthe RGAE is trained on is monophonic. The full architec-\nture is trained with Backpropagation through time (BPTT)\nto minimize the categorical cross-entropy loss for the re-\nconstructed target as\nL(x;~ x) =\u00001\nNNX\nn=1xnlog2~xn: (9)\nWhen the RGAE is applied to polyphonic music, in\nEquation 8 the sigmoid non-linearity, together with the bi-\nnary cross-entropy loss (cf. Equation 3) has to be used.\n3.3 Baseline RNN\nAs a baseline, we employ an RNN with GRUs to directly\noperate on the data. Accordingly, Equations 4, 5, and 6 areadapted to take xtinstead of mtas input. Consequently,\nthe prediction of the baseline RNN amounts to\n~ xt+1=\u001bs(Uoht); (10)\nwhere the softmax non-linearity is applied, making the cat-\negorical cross-entropy loss (cf. Equation 9) applicable in\ntraining.\n4. GATED AUTOENCODER PRE-TRAINING\nDue to the relatively high number of parameters in its GAE\nportion, the RGAE is prone to overﬁtting. To circumvent\nthis, and to establish robust interval representations, we\npre-train the GAE ﬁrst, using the cross-entropy of the re-\nconstruction as the cost function (cf. Equation 3). In the\nsecond training iteration, we train the RNN portion of the\nGAE to minimize the cross-entropy error of the architec-\nture’s prediction (cf. Equation 9). The datasets may differ\nbetween the training iterations as long as the included rela-\ntions are identical (e.g. “intervals of western tonal music”).\nConsequently, the GAE parameters trained on one dataset\ncan be used for prediction tasks on several datasets. Fine-\ntuning the whole architecture in the last few epochs of pre-\ndictive training can make up for possible bias.\nIn the following, we describe how the GAE is pre-trained\nin our experiments. Details varying between the experi-\nments are given later in the experiments section (cf. Sec-\ntion 5).\n4.0.1 Enforcing Transposition-Invariance\nA property of interval representations in music is trans-\nposition invariance (i.e., transposing the melody does not\nchange the representation). Although training the GAE as\ndescribed in Section 3.1 naturally tends to lead to similar\nmapping codes for input target pairs that have the same\ninterval relationships, the training does not explicitly en-\nforce such similarities and consequently the mappings may\nnot be maximally transposition invariant. Therefore, when\npre-training the GAE, we explicitly support the learning of\ntransposition-invariant codes. First, we deﬁne a transposi-\ntion function shift(x;\u000e), which shifts the bits of a vector x\nof lengthMby\u000epitches:\nshift(x;\u000e) = (x(0+\u000e) modM;:::;x (M\u00001+\u000e) modM)>;\n(11)\nwhere shift(xt\nt\u0000n;\u000e)denotes the transposition of each sin-\ngle time step vector before concatenation and linearization.\nThe altered training is then as follows: First, the map-\nping code mt+1of an input/target pair is inferred as shown\nin Equation 1. Then, mt+1is used to reconstruct a trans-\nposed version of the target from an equally transposed in-\nput (modifying Equation 2) as\n~ x0\nt+1=\u001bg(V>(W>\nmmt+1\u0001Qshift(xt\nt\u0000n;\u000e)));(12)\nwith\u000e2[\u000030;30]. Finally, we penalize the error between\nthe reconstruction of the transposed target and the actual\ntransposed target (i.e., employing Equation 3) as\nL(shift(xt+1;\u000e);~ x0\nt+1): (13)28 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018The transposition distance \u000eis randomly chosen for each\ntraining batch. This method amounts to both, a form of\nguided training and data augmentation.\n4.0.2 Pre-training and Architecture\nWe use 512 units in the factor layer and 64units in the\nmapping layer of the GAE. On the latter, sparsity regular-\nization [14] is applied. The deviation of the norms of the\ncolumns of both weight matrices UandVfrom their av-\nerage norm is penalized. Furthermore, we restrict these\nnorms to a maximum value. The learning rate is reduced\nfrom 0:001to0during training, and RMSProp [8] is used.\n5. EXPERIMENTS\n5.1 Experiment 1: Folk Song Prediction\nWe test the RGAE and RNN in a sequence learning task\nusing the data described in Section 5.1.1. In order to make\nthe results comparable, we use the same experiment setup\nas in [1, 22].\n5.1.1 Data\nThe EFSC subset (comprising a total of 54,308 note events)\nof the Essen Folk Song Collection (EFSC) [24] constitutes\nthe data for the actual training and evaluation. It consists\nof 119 Yugoslavian folk songs, 91 Alsatian folk songs, 93\nSwiss folk songs, 104 Austrian folk songs, the German\nsubset kinder (213 songs), and 237 songs of the Chinese\nsubset shanxi . The melodies are represented as series of\npitches ignoring note durations.\nFor pre-training the GAE portion of the RGAE, we use a\npolyphonic Mozart piano music dataset ( [29], comprising\n13 piano sonatas with more than 106,000 notes) in piano-\nroll representation (i.e., using a regular time grid of 1/8th\nnote resolution, and an active note can span several time\nsteps). We pre-train on that data because polyphonic music\nacts as a better regularizer for learning interval representa-\ntions than monophonic music.\n5.1.2 Training and Architecture\nWe use only 16hidden units in the RNN portion of the\nRGAE. The look-back window of the GAE is n= 8pitches,\nand we apply 50% dropout on the input in pre-training\nand when training the whole architecture. We pre-train the\nGAE for 250 epochs on the Mozart piano pieces (cf. Sec-\ntion 5.1.1). Subsequently, the RNN portion is trained for\n110 epochs on the interval representations (i.e., mappings\nprovided by the GAE) of the EFSC datasets. In the last 10\nepochs the whole architecture is ﬁne-tuned.\nThe baseline RNN with 50hidden units is trained for\n70 epochs on the EFSC data. The learning rate scheme is\nadopted from that described in Section 4.0.2 for all models.\n5.1.3 Combining Model Predictions\nWe hypothesize that the RNN and the RGAE are comple-\nmentary in how they process musical sequences. For ex-\nample, the RNN may have better stability in remembering\nabsolute reference pitches, like the tonic of a piece, andis superior in modeling prior probabilities, to keep predic-\ntions in a plausible pitch range. In contrast, the RGAE can\nmake use of structural cues indicating repetitions and can\ngeneralize better due to relative pitch processing. There are\nseveral possibilities to combine the predictions of statisti-\ncal models. Next to the ad-hoc approach of merely aver-\naging their outputs, we can also use information about the\ncertainty of the models and weight their outputs accord-\ningly. A measure for the certainty of a prediction is given\nby the Shannon entropy [26]:\nH(p) =\u0000X\na2Ap(a) log2p(a); (14)\nwherep(a2A) =P(X=a)is a probability mass func-\ntion over a discrete alphabet A. The method which worked\nbest in our experiments is calculating the entropy-weighted\ngeometric mean of both predictions, as proposed in [21]:\np(t) =1\nRY\nm2Mpm(t)wm; (15)\nwherepm(t)is the predicted distribution of model mat\ntimet,wm=Hrelative (pm)\u0000bis the weight of model m,\nnon-linearly scaled using a bias b(set to 0:5in our exper-\niments), and Ris a normalization constant. The relative\nentropyHrelative (pm)for modelmis given by\nHrelative (pm) =H(pm)\nHmax(pm); (16)\nwhereHmax(pm)>0is the entropy of the probability mass\nuniformly distributed over the alphabet (indicating maxi-\nmal uncertainty of the model).\n5.1.4 Evaluation\nSince the datasets are rather small, a ﬁxed training/test set\nsplit would lead to a poor estimation of the performance of\nthe models. Therefore, and in accordance with [1, 22], a\n10-fold cross validation is performed for each dataset and\nthe categorical cross-entropy loss (cf. Equation 9) is re-\nported.\n5.1.5 Results and Discussion\nThe results are shown in Table 1. The current state-of-\nthe-art results for general connectionist sequence models\non the datasets are achieved by the RTDRBM model in-\ntroduced in [1]. The results show that the RGAE slightly\noutperforms the RTDRBM and is clearly superior to the\nbaseline RNN. Note that the RGAE only has 16units for\nlearning temporal dependencies (the GAE portion mainly\ntransforms absolute pitch input to relative pitch represen-\ntations). This compactness suggests that the relative pro-\ncessing of music indeed supports generalization by reduc-\ning the sparsity in the data.\nWhen combining the predictions of the RGAE with an\nabsolute pitch model (i.e., RNN or RTDRBM) based on\nthe entropy-weighted geometric mean (cf. Section 5.1.3), a\nmore substantial improvement is achieved than when com-\nbining the two absolute pitch models. This result showsProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 29RNN RTDRBM [1] RGAE RNN + RNN + RTDRBM +\nData (GRU) RTDRBM RGAE RGAE\nAlsatian folk songs 2:890 2 :897 2 :872 2 :844 2 :788 2:771\nYugoslavian folk songs 2:717 2 :655 2 :676 2 :617 2 :586 2:530\nSwiss folk songs 2:954 2 :932 2 :895 2 :851 2 :831 2:769\nAustrian folk songs 3:185 3 :259 3 :171 3 :163 3:070 3:085\nGerman folk songs 2:358 2 :301 2 :305 2 :257 2 :233 2:184\nChinese folk songs 2:725 2 :685 2 :752 2 :612 2 :650 2:595\nAverage 2:805 2 :788 2 :779 2 :724 2 :693 2:656\nTable 1 : Cross-Entropies of the 10-fold cross validation in the prediction task for different data sets and different models.\nWhen combining the RGAE with an absolute pitch model (i.e., RNN, RTDRBM), results improve substantially. The\nresults suggest that absolute and relative pitch models are complementary in the aspects they learn about music and can be\neffectively used in an ensemble method.\nthat absolute and relative processing of music are comple-\nmentary and can, therefore, be effectively used together in\nan ensemble method.\n5.2 Experiment 2: Copy-and-Shift Operations\nThis experiment shall be seen as a proof-of-concept for the\nRGAEs ability to learning sequences of copy-and-shift op-\nerations (i.e., structure schemes). We oppose our model to\nan RNN with GRUs, which is known to have difﬁculties to\nlearn tasks in the form “whatever has been generated be-\nfore, now create a (shifted) copy of it”. The hypothesis is\nthat the RGAE, due to its modeling of intervals, is superior\nin solving this task. It has shown in previous studies that\nit can learn content-invariant transformations between data\ninstances [16], a necessary capability for learning content-\ninvariant structure schemes.\n5.2.1 Data\nIn order to obtain a controlled setup for testing the model\nperformances, we construct data obeying different recur-\nring (chromatic) transposition patterns. To this end, the\nEFSC dataset is transformed into a piano-roll representa-\ntion with a resolution of 1/8th note. From that, short frag-\nments of length 4,8, and 16(\u0014the length of the recep-\ntive ﬁeld of the input to the models) are randomly sampled\n(rests are omitted). It is necessary that the RGAE has ac-\ncess to all past events with which the prediction should be\nrelated. Choosing longer fragment lengths than the lengths\nof the receptive ﬁelds yields considerably worse results,\nalso for the baseline RNN, which already performs weakly\nin this setup. The fragments are copied and transposed ac-\ncording to some pre-deﬁned transposition schemes (cf. Ta-\nble 2). For each of the 10schemes and fragment lengths,\n26 sequences (512 time steps each, resulting in 133 120\ntime steps) are generated, where 20 sequences are used for\ntraining, 5 sequences are used for testing and 1 for evalu-\nation. This results in a total of 600sequences for training,\n150sequences for testing and 30 sequences for evaluation.\n5.2.2 Training and Architecture\nThe lookback window of the RGAE is n= 16 time steps,\nthe RNN portion has 64 units, and we do not use dropout\non the input. For the baseline RNN, we also input the 16\npreceding time steps, as this supports copy operations byTransposition Schemes\nf+5 ;+5 ;+5 ; : : :g\nf+7 ;+7 ;+7 ; : : :g\nf\u00005;\u00005;\u00005; : : :g\nf\u00007;\u00007;\u00007; : : :g\nf+12 ;\u000012;+12 ; : : :g\nf+3 ;\u00003;+3 ; : : :g\nf+4 ;\u00004;+4 ; : : :g\nf+9 ;\u00009;+9 ; : : :g\nf+4 ;\u00008;+4 ;\u00008; : : :g\nf\u00004;+8 ;\u00004;+8 ; : : :g\nTable 2 : The different relative transposition schemes used\nin Experiment 2.\nfreeing up memory in the hidden units. The baseline RNN\nmodel size (512 units) is selected by starting from 64 units\nand always doubling that number until no substantial im-\nprovement occurs on the evaluation set.\nThe GAE portion of the RGAE is pretrained for 50 ep-\nochs on the structured sequences described above. Sub-\nsequently, the RGAE is trained for 50epochs, holding the\nparameters of the GAE ﬁxed. As the data of the pretraining\ndoes not differ from the sequences in the prediction task,\nﬁnetuning is not necessary.\nThe baseline RNN is trained for 60 epochs. Again, for\nboth models the learning rate scheme described in Sec-\ntion 4.0.2 is employed. Note that in this task, we always\nrandomly transpose the input to the models in all training\nphases. Therefore, we need no dropout on the input of the\nRGAE, and the baseline RNN does not overﬁt, despite its\nhigh number of parameters.\n5.2.3 Evaluation\nThe models have to learn to continue sequences from the\ntest set after exposition to the ﬁrst 64time steps of each\nsequence. The experiment is different to typical prediction\ntasks in that possibly incorrect predictions are fed back to\nthe models, causing errors to accumulate. To obtain more\nstable continuations, we do not sample from the predicted\ndistributions of the models, but instead, treat the exper-\niment as a classiﬁcation task and choose the pitch with\nthe highest predicted probability. Accordingly, the preci-\nsion is merely the percentage of correctly predicted pitches\nover time. In addition, we quantify how many sequences\nare correctly continued until the end by considering all se-30 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Model Pr (%) >99% CE # Params\nRNN 41:38 6:67 10:10\u00182 300 000\nRGAE 99:43 92:00 0:16\u0018600 000\nTable 3 : Results of the structure learning task. Average\nprecision (Pr), percentage of continuations above 99% pre-\ncision, cross-entropy (CE) and number of parameters of the\nrespective model.\nRGAE RNN020406080100Precision (%)\nPrecisions vs. Model\nFigure 2 : Distribution of precisions for continuation of\nsequential copy-and-shift operations in the test set of size\n150. The median is marked with a orange line, the boxes\nindicate the interquartile range, and circles indicate out-\nliers.\nquences with an overall precision above 99% as correctly\ncontinued. Furthermore, like in Experiment 1, the categor-\nical cross-entropy loss (cf. Equation 9) is computed.\n5.2.4 Results and Discussion\nTable 3 shows the quantitative results of the experiment,\nand Figure 2 shows a box plot comparing the precisions\nof the two models. With an average precision of 99:43%\npercent, where 92% of all examples are ﬂawlessly contin-\nued, the RGAE shows remarkable stability in continuing\nthe structure scheme realizations. The cross-entropy of the\nRGAE is about two orders of magnitude lower than that\nof the RNN. In Figure 3, a speciﬁc example of this se-\nquence continuation task is depicted. Note that the hid-\nden unit activations of the RGAE are more regular because\nthey only represent copy-and-shift operations instead of\nthe musical texture itself (as it is the case for the RNN).\nThe most challenging part for the RGAE is counting, in\norder to change the copy operation (i.e., transposition dis-\ntance) at the right time (in fact, at most of the incorrectly\ncontinued sequences, the RGAE miscounted by one time\nstep). It is important to note that the hidden unit activations\nof the RNN portion are identical for identical schemes,\nbecause they operate on transformations between events,\nrather than on the events themselves (i.e., they are largely\ncontent-invariant).\n6. CONCLUSION AND FUTURE WORK\nThe principle of modeling sequences of ﬁrst-order deriva-\ntives in music is a compelling concept with the potential\nto solve two persistent problems in MIR: Learning trans-\nposition-invariant interval representations, and learning rep-\nresentations of (chromatically transposed) repetition struc-\nture. The proposed model is conceptually simple and can\nbe trained as a generative model in sequence learning tasks.\nFigure 3 : Generated structure schemes and hidden unit\nactivations of the RGAE and the RNN models after input\nof a primer indicating the f\u00004;+8;\u00004;+8;:::gscheme,\nrealized with melodies of length 16 not contained in the\ntrain set. Black notes indicate correct continuation, green\nnotes indicate false negatives, red notes indicate false pos-\nitives. Hidden units activations of the RNN are pruned due\nto space limitation.\nMoreover, the RGAE can act as a building block for\nmore complex architectures, in order to extend its capabili-\nties. For example, the temporal lookback window could be\ngreatly extended by employing the RGAE on top of a (di-\nlated) convolutional network, enabling it to learn higher-\nlevel repetition structure. In another variant, an RGAE\ncould be employed on top of an RNN. Applied to music,\nthe RNN would provide the RGAE with representations of\nimportant, absolute reference pitches (e.g., the tonic of a\nscale, or the root note of a chord), and the RGAE could\nlearn sequences of intervals in relation to them. Another\ninteresting architecture would involve stacking more than\none RGAE on top of one another to learn higher-order\nderivatives, for example, variations between mutually trans-\nposed parts in music.\nThe RGAE, however, is not limited to the symbolic,\nmonophonic, domain of music. We show in [13] that a\nGAE can also operate in the spectral domain of audio and\nin polyphonic symbolic music. Finally, we note that the\nRGAE is general enough to be applicable to other domains\nwhere the derivatives of functions are of higher importance\nthan their absolute course. Possible applications include\nmodeling temporal progressions of changes in loudness,\ntempo, mood, information density curves, and other musi-\ncal properties, modeling moving or rotating objects, cam-\nera movements in video recordings, and signals in the time\ndomain.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 317. ACKNOWLEDGMENTS\nThis research was supported by the EU FP7 (project\nLrn2Cre8, FET grant number 610859), and the European\nResearch Council (project CON ESPRESSIONE, ERC grant\nnumber 670035). We thank Srikanth Cherla for providing\nus with the source code of the RTDRBM model [1].\n8. REFERENCES\n[1] Srikanth Cherla. Neural Probabilistic Models for\nMelody Prediction, Sequence Labelling and Classiﬁ-\ncation . PhD thesis, City, University of London, 2016.\n[2] Kyunghyun Cho, Bart van Merri ¨enboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. On the properties of neu-\nral machine translation: Encoder–decoder approaches.\nSyntax, Semantics and Structure in Statistical Transla-\ntion, page 103, 2014.\n[3] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho,\nand Yoshua Bengio. Empirical evaluation of gated re-\ncurrent neural networks on sequence modeling. arXiv\npreprint arXiv:1412.3555 , 2014.\n[4] Tom Collins, Robin C. Laney, Alistair Willis, and\nPaul H. Garthwaite. Developing and evaluating compu-\ntational models of musical style. Artiﬁcial Intelligence\nfor Engineering Design, Analysis and Manufacturing ,\n30(1):16–43, 2016.\n[5] Darrell Conklin. Chord sequence generation with semi-\notic patterns. Journal of Mathematics and Music ,\n10(2):92–106, 2016.\n[6] Arne Eigenfeldt and Philippe Pasquier. Evolving struc-\ntures for electronic dance music. In Genetic and Evo-\nlutionary Computation Conference, GECCO ’13, Ams-\nterdam, The Netherlands, July 6-10, 2013 , pages 319–\n326. ACM, 2013.\n[7] Dorien Herremans and Elaine Chew. MorpheuS: Au-\ntomatic music generation with recurrent pattern con-\nstraints and tension proﬁles. In Proceedings of the\nIEEE Region 10 Conference (TENCON), Singapore,\nNovember 22-25, 2016 , pages 282–285. IEEE, 2016.\n[8] Geoffrey Hinton, Nitish Srivastava, and Kevin Swer-\nsky. Neural networks for machine learning lecture 6a\noverview of mini-batch gradient descent, 2012.\n[9] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-\nterm memory. Neural Computation , 9(8):1735–1780,\n1997.\n[10] Jonas Langhabel, Robert Lieck, Marc Toussaint, and\nMartin Rohrmeier. Feature discovery for sequential\nprediction of monophonic music. In Sally Jo Cunning-\nham, Zhiyao Duan, Xiao Hu, and Douglas Turnbull,\neditors, Proceedings of the 18th International Soci-\nety for Music Information Retrieval Conference, IS-\nMIR 2017, Suzhou, China, October 23-27, 2017 , pages\n649–656, 2017.[11] Stefan Lattner and Maarten Grachten. Learning trans-\nformations of musical material using gated autoen-\ncoders. In Proceedings of the 2nd Conference on Com-\nputer Simulation of Musical Creativity, CSMC 2017,\nMilton Keynes, UK, September 11-13, 2017 , 2017.\n[12] Stefan Lattner, Maarten Grachten, and Gerhard Wid-\nmer. Imposing higher-level structure in polyphonic mu-\nsic generation using convolutional restricted Boltz-\nmann machines and constraints. Journal of Creative\nMusic Systems , 3(1), 2018.\n[13] Stefan Lattner, Maarten Grachten, and Gerhard Wid-\nmer. Learning transposition-invariant interval features\nfrom symbolic music and audio. In Proceedings of\nthe 19th International Society for Music Information\nRetrieval Conference, ISMIR 2018, Paris, France,\nSeptember 23-27 , 2018.\n[14] Honglak Lee, Chaitanya Ekanadham, and Andrew Y .\nNg. Sparse deep belief net model for visual area\nV2. In John C. Platt, Daphne Koller, Yoram Singer,\nand Sam T. Roweis, editors, Proceedings of the\nTwenty-First Annual Conference on Neural Informa-\ntion Processing Systems, Vancouver, British Columbia,\nCanada, December 3-6, 2007 , pages 873–880. Curran\nAssociates, Inc., 2007.\n[15] Roland Memisevic. Gradient-based learning of higher-\norder image features. In IEEE International Confer-\nence on Computer Vision (ICCV), 2011 , pages 1591–\n1598. IEEE, 2011.\n[16] Roland Memisevic and Georgios Exarchakis. Learning\ninvariant features by harnessing the aperture problem.\nInICML (3) , pages 100–108, 2013.\n[17] Roland Memisevic and Geoffrey Hinton. Unsupervised\nlearning of image transformations. In IEEE Conference\non Computer Vision and Pattern Recognition, 2007.\nCVPR. , pages 1–8. IEEE, 2007.\n[18] Roland Memisevic and Geoffrey E Hinton. Learn-\ning to represent spatial transformations with factored\nhigher-order Boltzmann machines. Neural Computa-\ntion, 22(6):1473–1492, 2010.\n[19] Vincent Michalski, Roland Memisevic, and Kishore\nKonda. ”modeling deep temporal dependencies with\nrecurrent grammar cells”. In Advances in neural infor-\nmation processing systems , pages 1925–1933, 2014.\n[20] Franc ¸ois Pachet, Sony CSL Paris, Alexandre Pa-\npadopoulos, and Pierre Roy. Sampling variations of se-\nquences for structured music generation. In Proceed-\nings of the 18th International Society for Music Infor-\nmation Retrieval Conference , pages 167–173, 2017.\n[21] Marcus Pearce, Darrell Conklin, and Geraint Wiggins.\nMethods for combining statistical models of music. In\nInternational Symposium on Computer Music Model-\ning and Retrieval , pages 295–312. Springer, 2004.32 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[22] Marcus Pearce and Geraint Wiggins. Improved meth-\nods for statistical modelling of monophonic music.\nJournal of New Music Research , 33(4):367–385, 2004.\n[23] Marcus Thomas Pearce. The construction and evalua-\ntion of statistical models of melodic structure in music\nperception and composition . PhD thesis, City Univer-\nsity London, 2005.\n[24] Helmut Schaffrath. The Essen Folksong Collection in\nKern Format. In David Huron, editor, Database con-\ntaining , folksong transcriptions in the Kern format\nand a -page research guide computer database . Menlo\nPark, CA, 1995.\n[25] Jan Schlueter and Christian Osendorfer. Music simi-\nlarity estimation with the mean-covariance restricted\nBoltzmann machine. In 10th International Conference\non Machine Learning and Applications and Workshops\n(ICMLA), 2011 , volume 2, pages 118–123. IEEE,\n2011.\n[26] Claude Elwood Shannon. A mathematical theory\nof communication. Bell System Technical Journal ,\n27:379–423, 623–656, July 1948.\n[27] Ilya Sutskever, Geoffrey E. Hinton, and Graham W.\nTaylor. The recurrent temporal restricted Boltzmann\nmachine. In Daphne Koller, Dale Schuurmans, Yoshua\nBengio, and L ´eon Bottou, editors, Advances in Neural\nInformation Processing Systems 21, Proceedings of the\nTwenty-Second Annual Conference on Neural Informa-\ntion Processing Systems, Vancouver, British Columbia,\nCanada, December 8-11, 2008 , pages 1601–1608.\nCurran Associates, Inc., 2008.\n[28] Pascal Vincent, Hugo Larochelle, Isabelle La-\njoie, Yoshua Bengio, and Pierre-Antoine Manzagol.\nStacked denoising autoencoders: Learning useful rep-\nresentations in a deep network with a local denois-\ning criterion. Journal of Machine Learning Research ,\n11(Dec):3371–3408, 2010.\n[29] Gerhard Widmer. Discovering simple rules in com-\nplex data: A meta-learning algorithm and some\nsurprising musical discoveries. Artiﬁcial Intelligence ,\n146(2):129–148, 2003.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 33"
    },
    {
        "title": "Learning Interval Representations from Polyphonic Music Sequences.",
        "author": [
            "Stefan Lattner",
            "Maarten Grachten",
            "Gerhard Widmer"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492503",
        "url": "https://doi.org/10.5281/zenodo.1492503",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/172_Paper.pdf",
        "abstract": "Many music theoretical constructs (such as scale types, modes, cadences, and chord types) are defined in terms of pitch intervals—relative distances between pitches. Therefore, when computer models are employed in music tasks, it can be useful to operate on interval representations rather than on the raw musical surface. Moreover, interval representations are transposition-invariant, valuable for tasks like audio alignment, cover song detection and music structure analysis. We employ a gated autoencoder to learn fixed-length, invertible and transposition-invariant interval representations from polyphonic music in the symbolic domain and in audio. An unsupervised training method is proposed yielding an organization of intervals in the representation space which is musically plausible. Based on the representations, a transposition-invariant self-similarity matrix is constructed and used to determine repeated sections in symbolic music and in audio, yielding competitive results in the MIREX task \"Discovery of Repeated Themes and Sections\".",
        "zenodo_id": 1492503,
        "dblp_key": "conf/ismir/LattnerGW18a",
        "keywords": [
            "pitch intervals",
            "scale types",
            "modes",
            "cadences",
            "chord types",
            "transposition-invariant",
            "interval representations",
            "polyphonic music",
            "symbolic domain",
            "unsupervised training"
        ],
        "content": "LEARNING TRANSPOSITION-INV ARIANT INTERV AL FEATURES FROM\nSYMBOLIC MUSIC AND AUDIO\nStefan Lattner1;2, Maarten Grachten1;2, Gerhard Widmer1\n1Institute of Computational Perception, JKU Linz\n2Sony Computer Science Laboratories (CSL), Paris, France\nABSTRACT\nMany music theoretical constructs (such as scale types,\nmodes, cadences, and chord types) are deﬁned in terms of\npitch intervals—relative distances between pitches. There-\nfore, when computer models are employed in music tasks,\nit can be useful to operate on interval representations rather\nthan on the raw musical surface. Moreover, interval rep-\nresentations are transposition-invariant, valuable for tasks\nlike audio alignment, cover song detection and music struc-\nture analysis. We employ a gated autoencoder to learn\nﬁxed-length, invertible and transposition-invariant interval\nrepresentations from polyphonic music in the symbolic do-\nmain and in audio. An unsupervised training method is\nproposed yielding an organization of intervals in the repre-\nsentation space which is musically plausible. Based on the\nrepresentations, a transposition-invariant self-similarity ma-\ntrix is constructed and used to determine repeated sections\nin symbolic music and in audio, yielding competitive re-\nsults in the MIREX task ”Discovery of Repeated Themes\nand Sections”.\n1. INTRODUCTION\nThe notion of relative pitch is important in music under-\nstanding. Many music theoretical concepts, such as scale\ntypes, modes, chord types and cadences, are deﬁned in\nterms of relations between pitches or pitch classes. But\nrelative pitch is not only a music theoretical construct. It\nis common for people to perceive and memorize melodies\nin terms of pitch intervals (or in terms of contours , the\nupward or downward direction of pitch intervals) rather\nthan sequences of absolute pitches. This characteristic of\nmusic perception also has ramiﬁcations for the perception\nof form in musical works, since it implies that transposi-\ntion of some musical fragment along the pitch dimension\n(such that the relative distances between pitches remain the\nsame) does not alter the perceived identity of the musical\nmaterial, or at least establishes a sense of similarity be-\ntween the original and the transposed material. As such,\nadequate detection of musical form in terms of (approxi-\nc\rStefan Lattner, Maarten Grachten, Gerhard Widmer. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Stefan Lattner, Maarten Grachten, Gerhard\nWidmer. “Learning transposition-invariant interval features from sym-\nbolic music and audio”, 19th International Society for Music Information\nRetrieval Conference, Paris, France, 2018.mately) repeated structures presupposes the ability to ac-\ncount for pitch transposition—one of the most common\ntypes of transformations found in music.\nRelative pitch perception in humans is currently not well-\nunderstood [13]. For example there are no established the-\nories on how the human brain derives a relative representa-\ntion of pitch from the tonotopic representations formed in\nthe cochlea, neither is it clear whether there is a connection\nbetween the perception of pitch relations in simultaneous\nversus consecutive pitches.\nComputational approaches to address tasks of music un-\nderstanding (such as detecting patterns and form in music)\noften circumvent this issue by representing musical stim-\nuli as sequences of monophonic pitches, after which sim-\nply differencing consecutive pitches yields a relative pitch\nrepresentation. This approach also works for polyphonic\nmusic, to the extent that the music can be meaningfully\nsegregated into monophonic pitch streams. A drawback\nof this approach is that it presupposes the ability to segre-\ngate musical streams, which is often far from trivial due to\nthe ambiguity of musical contexts. To take an analogous\napproach on acoustical representations of musical stimuli\nis even more challenging, since it further depends on the\nability to detect pitches and onsets in sound.\nIn this paper we take a different approach altogether.\nWe train a neural network model to learn representations\nthat represent the relation between the music at some time\npointtand the preceding musical context. During train-\ning, these representations are adapted to minimize the re-\nconstruction error of the music at tgiven the preceding\ncontext and the representation itself.\nA crucial aspect of the model is its bilinear architec-\nture (more speciﬁcally, a gated autoencoder , or GAE ar-\nchitecture) involving multiplicative connections, which fa-\ncilitates the formation of relative pitch representations. We\nstimulate such representations more explicitly using an al-\ntered training procedure in which we transpose the training\ndata using arbitrary transpositions.\nThe result are two models (for symbolic music and au-\ndio) that can map both monophonic and polyphonic music\nto a sequence of points in a vector space—the mapping\nspace —in a way that is invariant to pitch transpositions.\nThis means that a musical fragment will be projected to\nthe same mapping space trajectory independently of how\nit is transposed.\nWe validate our approach experimentally in several ways.\nFirst we show that musical fragments that are nearest neigh-661bors in the mapping space have many pitch intervals in\ncommon (as opposed to nearest neighbors in the input space).\nThen we show that the topology of the learned mapping\nspace reﬂects musically meaningful relations between in-\ntervals (such as the tritone being dissimilar to other in-\ntervals). Lastly we use mapping space representations to\ndetect musical form both for symbolic and audio repre-\nsentations of music, showing that it yields competitive re-\nsults, and in the case of audio even improves the state of\nthe art. A re-implementation of the transposition-invariant\nGAE for audio is publicly available1.\nThe paper is structured as follows. Section 2 provides\nan overview of relation learning using GAEs, and reviews\nwork on creating interval representations from music. In\nSection 3, the used architecture is described and in Sec-\ntion 4, data is introduced on which the GAE is trained.\nThe training procedure, including the novel method to sup-\nport the emergence of transposition-invariance, is proposed\nin Section 5. The experiments conducted to examine the\nproperties of learned mappings are described in Section 6,\nand results are presented and discussed in Section 7. Sec-\ntion 8 wraps the paper up with conclusions and prospects\nof future work.\n2. RELATED WORK\nGAEs utilize multiplicative interactions to learn correla-\ntions between or within data instances. The method was\ninspired by the correlation theory of the brain [32], where it\nwas pointed out that some cognitive phenomena cannot be\nexplained with the conventional brain theory and an exten-\nsion was proposed which involves the correlation of neural\npatterns.\nIn machine learning, this principle was deployed in bi-\nlinear models, for example to separate person and pose\nin face images [30]. Bi-linear models, like the GAE, are\ntwo-factor models whose outputs are linear in either fac-\ntor when the other is held constant. [26] proposed another\nvariant of a bi-linear model in order to learn objects and\ntheir optical ﬂow. Due to its similar architecture, the gated\nBoltzmann machine (GBM) [17,18] can be seen as a direct\npredecessor of the GAE. The GAE was introduced by [14]\nas a derivative of the GBM, as standard learning criteria\nbecame applicable through the development of denoising\nautoencoders [31].\nGAEs have been further used to learn transformation-\ninvariant representations for classiﬁcation tasks [15], for\nparent-offspring resemblance [5], for learning to negate\nadjectives in linguistics [27], for activity recognition with\nthe Kinekt sensor [22], in robotics to learn to write num-\nbers [6], and for learning multi-modal mappings between\naction, sound, and visual stimuli [7].\nIn music, bi-linear models have been applied to learn\nco-variances within spectrogram data for music similarity\nestimation [28], and for learning musical transformations\nin the symbolic domain [9]. In sequence modeling, the\nGAE has been utilized to learn co-variances between sub-\n1seehttps://github.com/SonyCSLParis/cgae-invarsequent frames in movies of rotated 3D objects [16] and\nto predict accelerated motion by stacking more layers in\norder to learn higher-order derivatives [21], which uses a\nmethod similar to the one proposed here.\nTransposition-invariance in music is achieved in [20]\nby transforming symbolic pitch–time representations into\npoint-sets, in which translatable patterns are identiﬁed. An-\nother method in the symbolic domain is that in [2], where\na general interval representation for polyphonic music is\nput forward, in [24], where speciﬁc pitch-class intervals in\npolyphonic music are used for characterizing music styles\nand in [23] where transposition-invariant self-similarity ma-\ntrices are computed. In [12], an approach to calculating\ntransposition-invariant mid-level representations from au-\ndio is introduced, based on the 2-D power spectrum of\nmelodic fragments. Similarly, a method to calculate inter-\npretable interval representations from audio is proposed in\n[33], where chromagrams that are close in time are cross-\ncorrelated to obtain local pitch-invariance.\n3. MODEL\nLetxjbe a vector representing pitches of currently sound-\ning notes (in the symbolic domain) or the energy distributed\nover frequency bands (in the audio domain), in a ﬁxed-\nlength time interval. Given a temporal context xt\nt\u0000n=\nxt\u0000n:::xtas the input and the next time step xt+1as the\ntarget, the goal is to learn a mapping mtwhich does not\nchange when shifting xt+1\nt\u0000nup- or downwards in the pitch\ndimension. A gated autoencoder (GAE, depicted in Fig-\nure 1) is well-suited for this task, modeling the intervals\nbetween reference pitches in the input and pitches in the\ntarget, encoded in the latent variables of the GAE as map-\nping codes mj. Unlike in common prediction tasks, the\ntargets are known when training a GAE. The goal of the\ntraining is to ﬁnd a mapping mjfor any input/target pair\nwhich transforms the input into the given target. The map-\nping at time tis calculated as\nmt=\u001bh(W1\u001bh(W0(Uxt\nt\u0000n\u0001Vxt+1))); (1)\nwhere U;VandWkare weight matrices, \u001bhis the hyper-\nbolic tangent non-linearity, and we will refer to the learnt\nmappings mjas the mapping space of the input/target pairs.\nThe operator\u0001(depicted as a triangle in Figure 1) depicts\nthe Hadamard (or element-wise) product of the ﬁlter re-\nsponses Uxt\nt\u0000nandVxt+1, denoted as factors . This op-\neration allows the model to relate its inputs, making it pos-\nsible to learn interval representations.\nThe target of the GAE can be reconstructed as a func-\ntion of the input xt\nt\u0000nand a mapping mt:\n~ xt+1=\u001bg(V>(W>\n0W>\n1mt\u0001Uxt\nt\u0000n)); (2)\nwhere\u001bgis the sigmoid non-linearity for binary input and\nthe identity function for real-valued input.\nThe cost function is deﬁned to penalize the error of re-\nconstructing the target xt+1given the input xt\nt\u0000nand the662 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 1 : Schematic illustration of the gated autoencoder\narchitecture used in the experiments.\nmapping mtas\nLc=c(xt+1;~ xt+1); (3)\nwherec(\u0001)is the mean-square error for real-valued sequen-\nces and the cross-entropy loss for binary sequences.\n4. DATA\nWe train the model both on symbolic music representations\nand on audio spectrograms. For the symbolic data, the\nMozart/Batik data set [35] is used, consisting of 13 piano\nsonatas containing more than 106,000 notes. The dataset is\nencoded as successive 60dimensional binary vectors (en-\ncoding MIDI note number 36to96), each representing a\nsingle time step of 1/16th note duration. The pitch of an\nactive note is encoded as a corresponding on-bit, and as\nmultiple voices are encoded simultaneously, a vector may\nhave multiple active bits. The result is a pianoroll-like rep-\nresentation.\nThe audio dataset consists of 100 random piano pieces\nof the MAPS dataset [8] (subset MUS), at a sampling rate\nof 22.05 kHz. We choose a constant-Q transformed spec-\ntrogram using a hop size of 1984 , and Hann windows with\ndifferent sizes depending on the frequency bin. The range\ncomprises 120frequency bins (24 per octave), starting from\na minimal frequency of 65:4Hz. Each time step is contrast-\nnormalized to zero mean and unit variance.\n5. TRAINING\nThe model is trained with stochastic gradient descent in\norder to minimize the cost function (cf. Equation 3) us-\ning the data described in Section 4. However, rather than\nusing the data as is, we use data-augmentation in combina-\ntion with an altered training procedure to explicitly aim at\ntransposition invariance of the mapping codes.\n5.1 Enforcing Transposition-Invariance\nAs described in Section 3 the classical GAE training pro-\ncedure derives a mapping code from an input/target pair,and subsequently penalizes the reconstruction error of the\ntarget given the input and the derived mapping code. Al-\nthough this procedure naturally tends to lead to similar\nmapping codes for input target pairs that have the same in-\nterval relationships, the training does not explicitly enforce\nsuch similarities and consequently the mappings may not\nbe maximally transposition invariant.\nUnder ideal transposition invariance, by deﬁnition the\nmappings would be identical across different pitch\ntranspositions of an input/target pair. Suppose that a pair\n(xt\nt\u0000n;xt+1)leads to a mapping m(by Equation 1). Trans-\nposition invariance implies that reconstructing a target x0\nt+1\nfrom the pair (x0t\nt\u0000n;m)should be as successful as recon-\nstructing xt+1from the pair (xt\nt\u0000n;m)when (x0t\nt\u0000n;x0\nt+1)\ncan be obtained from (xt\nt\u0000n;xt+1)by a single pitch trans-\nposition.\nOur altered training procedure explicitly aims to achieve\nthis characteristic of the mapping codes by penalizing the\nreconstruction error using mappings obtained from trans-\nposed input/target pairs. More formally, we deﬁne a trans-\nposition function shift(x;\u000e), shifting the values of a vector\nxof lengthMby\u000esteps (MIDI note numbers and CQT\nfrequency bins for symbolic and audio data, respectively):\nshift(x;\u000e) = (x(0+\u000e) modM;:::;x (M\u00001+\u000e) modM)>;\n(4)\nandshift(xt\nt\u0000n;\u000e)denotes the transposition of each single\ntime step vector before concatenation and linearization.\nThe training procedure is then as follows. First, the\nmapping code mtof an input/target pair is inferred as shown\nin Equation 1. Then, mtis used to reconstruct a trans-\nposed version of the target, from an equally transposed in-\nput (modifying Equation 2) as\n~ x0\nt+1=\u001bg(V>(W>\n0W>\n1mt\u0001Ushift(xt\nt\u0000n;\u000e)));(5)\nwith\u000e2[\u000030;30]for the symbolic, and \u000e2[\u000060;60]\nfor the audio data. Finally, we penalize the error between\nthe reconstruction of the transposed target and the actual\ntransposed target (i.e., employing Equation 3) as\nL(shift(xt+1;\u000e);~ x0\nt+1): (6)\nThe transposition distance \u000eis randomly chosen for each\ntraining batch. This method amounts to both, a form of\nguided training and data augmentation. Some weights (i.e.,\nﬁlters) in UandVresulting from that training are depicted\nin Figure 2.\n5.2 Architecture and Training Details\nThe architecture and training details of the GAE are as fol-\nlows: A temporal context length of n= 8 is used (the\nchoice ofn>1leads to higher robustness of the mapping\ncodes to diatonic transposition). The factor layer has 1024\nunits for the symbolic data, and 512 units for the spec-\ntrogram data. Furthermore, for all datasets, there are 128\nneurons in the ﬁrst mapping layer and 64neurons in the\nsecond mapping layer (resulting in mt2R64).Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 663Figure 2 : Some ﬁlter pairs2fU;Vgof a GAE trained on\npolyphonic Mozart piano pieces.\nL2 weight regularization for weights UandVis ap-\nplied, as well as sparsity regularization [11] on the top-\nmost mapping layer. The deviation of the norms of the\ncolumns of both weight matrices UandVfrom their av-\nerage norm is penalized. Furthermore, we restrict these\nnorms to a maximum value. We apply 50% dropout on\nthe input and no dropout on the target, as proposed in [14].\nThe learning rate (1e-3) is gradually decremented to zero\nover the course of training.\n6. EXPERIMENTS\nIn this Section we describe several experimental analyses\nto validate the proposed approach. They are intended to\ntest the degree of transposition-invariance of the learned\nmappings, as well as assess their musical relevance (Sec-\ntions 6.1 and 6.3). Finally, we put the learned represen-\ntations to practice in a repeated section discovery task for\nsymbolic music and audio (Section 6.2).\n6.1 Classiﬁcation and Cluster Analysis\nOur hypothesis is that the model learns relative pitch rep-\nresentations (i.e. intervals) from polyphonic absolute pitch\nsequences. In order to test this hypothesis, we conduct two\nexperiments using the symbolic data.\nIn the ﬁrst experiment a ten-fold k-nn classiﬁcation of\nintervals is performed (where k = 10), where the task is to\nidentify all pitch intervals between notes in the input and\nthe target of an input/target pair. If the learned mappings\nactually represent intervals, the classiﬁer will perform sub-\nstantially better on the mappings than on the input space.\nAs intervals in music are transposition-invariant, the inter-\nval labels do not change when performing transposition in\nthe input space. Thus, we perform the classiﬁcation on\nthe mappings of the original data and of randomly trans-\nposed data, to test if the mappings are indeed transposition-\ninvariant.\nWe label the symbolic train data input/target pairs ac-\ncording to all intervals which occur between them, inde-\npendent of the temporal distance of the notes exhibiting\nthe intervals. Thus, each pair can have multiple labels. For\neach pair in the test set the k-nn classiﬁer predicts the set\nof interval labels that are present in the k neighbors of that\npair. The classiﬁcation is performed in the input space (us-\ning concatenated pairs) and in the mapping space. Using\nthese predictions we determine the precision, recall, andData Precision Recall F1\nOriginal input\nMapping space 91.27 70.25 76.66\nInput space 65.58 46.05 50.59\nTransposed input\nMapping space 90.78 71.44 77.31\nInput space 51.81 32.99 37.43\nAll 26.40 100.0 40.05\nNone 0.0 0.0 0.0\nTable 1 : Results of the k-nn classiﬁcation in the map-\nping space and in the input space for the original symbolic\ndata and data randomly transposed by [\u000024;24]semi-\ntones. “All” is a lower bound (always predict all intervals),\n“None” returns the empty set.\nF-score over the test set (cf. Table 1). For example, when a\npair contains 6 intervals and the classiﬁer estimate yield 4\ntrue-positive and 4 false-positive interval occurrences, that\npair is assigned a precision of 0.5 and a recall of 0.67.\nIn the second part of the experiment, the cluster cen-\nters of all intervals in the mapping space are determined.\nAgain, each pair projected into the mapping space accounts\nfor all intervals it exhibits and can therefore participate in\nmore than one cluster. The mutual Euclidean distances be-\ntween all cluster centers are displayed as a matrix (cf. Fig-\nure 3). An interpretation of the results follows in Section 7.\n6.2 Discovery of Repeated Themes and Sections\nThe MIREX Task for Discovery of Repeated Themes and\nSections for Symbolic Music and Audio2tests algorithms\nfor their ability to identify repeated patterns in music. The\ncommonly used JKUPDD dataset [3] contains 26 motifs,\nthemes, and repeated sections annotated in 5 pieces by J.\nS. Bach, L. v. Beethoven, F. Chopin, O. Gibbons and W.\nA. Mozart. We use the MIDI and the audio versions of the\ndataset and preprocess them as described in Section 4.\nWe calculate the reciprocal of the Euclidean distances\nbetween all representations mtof a song, resulting in a\ntransposition-invariant similarity matrix X. Then, the val-\nues of the main diagonal are set to the minimal value of the\nmatrix. Subsequently, the matrix is normalized and con-\nvolved with an identity matrix of size 15\u000215to empha-\nsize and smooth diagonals (Figure 4 shows a resulting ma-\ntrix). The method used to determine repeated parts based\non diagonals of high values in the self-similarity matrix is\nadopted from [25], with a different method to identify di-\nagonals, as described below.\nThe function\ns(i;j;N ) =NX\nk=N\u0000mX(i+k;j+k)wk\nm(7)\nreturns the score for a diagonal starting at X(i;j)with\n2http://www.music-ir.org/mirex/wiki/2017:\nDiscovery_of_Repeated_Themes_&_Sections664 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 3 : Distance matrix of cluster centers of intervals\nrepresented in mapping space. Darker cells indicate higher\ndistances between respective clusters, brighter cells indi-\ncate closeness.\nlengthN, and diagonals with high score are considered to\nbe repeated sections. For each i;j, we iteratively evaluate\nthe score with Nincreasing from 1in integer steps, until\nthe score undercuts a threshold \r. Only the last mvalues,\nm= min(10;N), of the diagonal are taken into account,\nbecause those values indicate when to stop tracing. The\nfactor\nwk=1 +k+m\u0000N\nm(8)\nlinearly weights the last mvalues of the diagonal so that\nlater values have more impact on the overall score.\nThree empirically determined parameters inﬂuence the\nfunctioning of the method: (1) from the diagonals found,\nwe only keep those spanning more than 2whole notes ,\n(2) all sections whose common boundaries start and end\nwithin the length of a half note are considered to be repe-\ntitions of each other, (3) the thresholds \rdetermining if a\ndiagonal should be considered a repetition in the symbolic\nand the audio data are set to 0:9and0:81, respectively. The\nresults are shown in Table 2 and are discussed in Section 7.\n6.3 Sensitivity Analysis\nThe sensitivity of the model to speciﬁc context informa-\ntion provides important insights into the functioning of the\nmodel. A common way of determining a networks sen-\nsitivity is by calculating the absolute value of the gradi-\nents of the networks predictions with respect to the input,\nholding the network parameters ﬁxed [29]. Figure 5 shows\nthe sensitivity of the model with respect to the temporal\ncontext. The model is particularly sensitive to note oc-\ncurrences at t2f0;\u00003;\u00007g. This shows that the most\ninformative notes for a prediction are direct predecessors\n(t= 0), and notes which occur a quarter ( t=\u00003) and a\nFigure 4 : Symbolic music and corresponding self-\nsimilarity matrix calculated from transposition-invariant\nmapping codes. Warmer colors indicate similarity, colder\ncolors indicate dissimilarity.\nhalf note (t=\u00007, i.e., eight sixteenth notes) before the\nprediction.\n7. RESULTS AND DISCUSSION\nThe results of the k-nn classiﬁcation on the raw data and on\nrepresentations learnt by the model are shown in Table 1.\nClassiﬁcation in the mapping space appreciably outper-\nforms classiﬁcation in the input space, and obtains similar\nvalues for mappings of the original data and the randomly\ntransposed data. In contrast, when performing classiﬁca-\ntion in the input space the results deteriorate for the ran-\ndomly transposed input and do not exceed the theoretical\nlower bound (i.e, always predict all intervals). As the reg-\nister and keys of the original data are limited, correlations\nbetween absolute and relative pitch exist. When transpos-\ning the input, the classiﬁer cannot make use of these abso-\nlute cues for relative pitch any more and performs weakly\nin the input space.\nFigure 3 indicates which intervals are close to each other\nin the mapping space. An obvious regularity are the slightly\nbrighter k-diagonals (i.e. parallels to the main diagonal)\nwithk2f\u0000 24;\u000012;12;24g, showing that two pitch in-\ntervals lead to similar mapping codes when they result in\nthe same pitch class, such as the intervals +8 and -4 semi-\ntones, or -7 and -19 semitones. This is an indication thatProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 665Algorithm Fest Pest Rest Fo(.5) Po(.5) Ro(.5) Fo(.75) Po(.75) Ro(.75) F3 P3 R3 Time (s)\nSymbolic\nGAE intervals (ours) 59.07 77.60 58.30 68.92 80.24 67.46 77.51 91.38 73.29 50.44 60.36 53.23 127\nVMO symbolic [34] 60.79 74.57 56.94 71.92 79.54 68.78 75.98 75.98 75.99 56.68 68.98 53.56 4333\nSIARCT-CFP [4] 33.70 21.50 78.00 76.50 78.30 74.70 - - - - - - -\nCOSIATEC [19] 50.20 43.60 63.80 63.20 57.00 71.60 68.40 65.40 76.40 44.20 40.40 54.40 7297\nAudio\nGAE intervals (ours) 57.67 67.46 59.52 58.85 61.89 56.54 68.44 72.62 64.86 51.61 59.60 55.13 194\nVMO deadpan [34] 56.15 66.80 57.83 67.78 72.93 64.30 70.58 72.81 68.66 50.60 61.36 52.25 96\nSIARCT-CFP [4] 23.94 14.90 60.90 56.87 62.90 51.90 - - - - - - -\nNieto [25] 49.80 54.96 51.73 38.73 34.98 45.17 31.79 37.58 27.61 32.01 35.12 35.28 454\nTable 2 : Different precision, recall and f-scores (adopted from [34], details on the measures are given in [3]) of different\nmethods in the Discovery of Repeated Themes and Sections MIREX task, for symbolic music and audio. The F3score\nconstitutes a summarization of all measures.\n, sixteenth notes\nFigure 5 : Absolute sensitivity of the model when look-\ning backwards on the temporal context, averaged over the\nwhole dataset.\nthe model learns the phenomenon of octave equivalence,\neven if the input to the model represents only absolute\npitch. Another distinct feature is the stripe which is orthog-\nonal to the main diagonal (i.e. where y=\u0000x). This indi-\ncates that the model develops some notion of relative dis-\ntances, by positioning intervals of the same distance (but\ndifferent signs) close to each other.\nNote also that the mappings of certain intervals, notably\n6and\u00006, are distant to those of most other intervals (dark\nhorizontal and vertical lines). This likely reﬂects the fact\nthat tritone intervals are rare in diatonic music, and is fur-\nther evidence of the musical signiﬁcance of the learned\nmappings.\nTable 2 shows results of the repeated themes and sec-\ntion discovery task, where the F3score is a good indi-\ncator for the overall performance of the models (see [3]\nfor a thorough explanation on the respective measures).\nFor the audio data, the current state-of-the-art F3score\nwas raised from 50:60to51:61by our proposed method.\nThe method performs slightly worse on the symbolic data,\nwhich is counterintuitive at ﬁrst sight, given that results\nof other models suggest that this task is easier. Our hy-\npothesis is that for discovery of repeated sections, approx-imate matching leads to better results than exact compar-\nison, simply because musical variation goes beyond chro-\nmatic transposition (towards which our model is invariant).\nFor approximate matching, a spectrogram representation\nis better suited than symbolic vectors, as notes are blurred\nover more than one frequency bin, and harmonics may pro-\nvide additional cues for a similarity estimation. The pro-\nposed approach is computationally efﬁcient, because the\ndiagonal detector (cf. Equations 7 and 8) is rather sim-\nple and the transposition-invariance of the representations\ndoes not require explicit comparison of mutually trans-\nposed musical textures.\n8. CONCLUSION AND FUTURE WORK\nIn this paper we have presented a computational approach\nto deriving (pitch) transposition-invariant vector space rep-\nresentations of music both in the symbolic and the audio\ndomain. The representations encode pitch intervals that\noccur in the music in a musically meaningful way, with tri-\ntone intervals—a rare interval in diatonic music—leading\nto more distinct representations, and octaves leading to\nmore similar representations. Furthermore, the temporal\nsensitivity of the model reveals a beat pattern that shows in-\ncreased sensitivity to pitch intervals occurring at beat mul-\ntiples of each other.\nThe transposition-invariance of the representations\nmakes it possible to detect transposed repetitions of mu-\nsical sections in the symbolic and in the spectral domain of\naudio. We have demonstrated that this is beneﬁcial in tasks\nsuch as the MIREX task Discovery of Repeated Themes\nand Sections . A simple diagonal ﬁnding approach on a\ntransposition-invariant self-similarity matrix produced by\nour model is sufﬁcient to outperform the state of the art in\nthe audio version of the task.\nWe believe it is worthwhile to further explore the utility\nof transposition-invariant music representations for other\napplications, including speech recognition, music summa-\nrization, music classiﬁcation, transposition-invariant mu-\nsic alignment (including a cappella voices with pitch drift),\nquery by humming, fast melody-based retrieval in large au-\ndio collections, and music generation. First results show\nthat the proposed representations are useful for audio-to-\nscore alignment [1] and for music prediction tasks [10].666 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20189. ACKNOWLEDGMENTS\nThis research was supported by the EU FP7 (project\nLrn2Cre8, FET grant number 610859), and the European\nResearch Council (project CON ESPRESSIONE, ERC grant\nnumber 670035). We thank Oriol Nieto for providing us\nwith the source code of his experiments [25].\n10. REFERENCES\n[1] Andreas Arzt and Stefan Lattner. Audio-to-score align-\nment using transposition-invariant features. In Pro-\nceedings of the 19th International Society for Music\nInformation Retrieval Conference, ISMIR 2018, Paris,\nFrance, September 23-27 , 2018.\n[2] Emilios Cambouropoulos. A general pitch interval rep-\nresentation: Theory and applications. Journal of New\nMusic Research , 25(3):231–251, 1996.\n[3] Tom Collins. Discovery of repeated themes and\nsections. http://www.music-ir.org/mirex/\nwiki/2017:Discovery_of_Repeated_\nThemes_%26_Sections , 2017.\n[4] Tom Collins, Andreas Arzt, Sebastian Flossmann, and\nGerhard Widmer. Siarct-cfp: Improving precision and\nthe discovery of inexact musical patterns in point-set\nrepresentations. In ISMIR , pages 549–554, 2013.\n[5] Afshin Dehghan, Enrique G Ortiz, Ruben Villegas,\nand Mubarak Shah. Who do i look like? determining\nparent-offspring resemblance via gated autoencoders.\nInProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , pages 1757–1764,\n2014.\n[6] Alain Droniou, Serena Ivaldi, and Olivier Sigaud.\nLearning a repertoire of actions with deep neural net-\nworks. In IEEE International Joint Conferences on\nDevelopment and Learning and Epigenetic Robotics\n(ICDL-Epirob) , pages 229–234. IEEE, 2014.\n[7] Alain Droniou, Serena Ivaldi, and Olivier Sigaud.\nDeep unsupervised network for multimodal percep-\ntion, representation and classiﬁcation. Robotics and\nAutonomous Systems , 71:83–98, 2015.\n[8] Valentin Emiya, Roland Badeau, and Bertrand David.\nMultipitch estimation of piano sounds using a new\nprobabilistic spectral smoothness principle. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 18(6):1643–1654, 2010.\n[9] Stefan Lattner and Maarten Grachten. Learning trans-\nformations of musical material using gated autoen-\ncoders. In Proceedings of the 2nd Conference on Com-\nputer Simulation of Musical Creativity, CSMC 2017,\nMilton Keynes, UK, September 11-13, 2017 , 2017.\n[10] Stefan Lattner, Maarten Grachten, and Gerhard Wid-\nmer. A predictive model for music based on learned rel-\native pitch representations. In Proceedings of the 19thInternational Society for Music Information Retrieval\nConference, ISMIR 2018, Paris, France, September 23-\n27, 2018.\n[11] Honglak Lee, Chaitanya Ekanadham, and Andrew Y .\nNg. Sparse deep belief net model for visual area\nV2. In John C. Platt, Daphne Koller, Yoram Singer,\nand Sam T. Roweis, editors, Proceedings of the\nTwenty-First Annual Conference on Neural Informa-\ntion Processing Systems, Vancouver, British Columbia,\nCanada, December 3-6, 2007 , pages 873–880. Curran\nAssociates, Inc., 2007.\n[12] Matija Marolt. A mid-level representation for melody-\nbased retrieval in audio collections. IEEE Transactions\non Multimedia , 10(8):1617–1625, 2008.\n[13] Josh McDermott and Andrew Oxenham. Music per-\nception, pitch, and the auditory system. Current Opin-\nion in Neurobiology , 18:1–12, 2008.\n[14] Roland Memisevic. Gradient-based learning of higher-\norder image features. In IEEE International Confer-\nence on Computer Vision (ICCV), 2011 , pages 1591–\n1598. IEEE, 2011.\n[15] Roland Memisevic. On multi-view feature learning. In\nJohn Langford and Joelle Pineau, editors, Proceed-\nings of the 29th International Conference on Machine\nLearning (ICML-12) , ICML ’12, pages 161–168, New\nYork, NY , USA, July 2012. Omnipress.\n[16] Roland Memisevic and Georgios Exarchakis. Learning\ninvariant features by harnessing the aperture problem.\nInICML (3) , pages 100–108, 2013.\n[17] Roland Memisevic and Geoffrey Hinton. Unsupervised\nlearning of image transformations. In IEEE Conference\non Computer Vision and Pattern Recognition, 2007.\nCVPR. , pages 1–8. IEEE, 2007.\n[18] Roland Memisevic and Geoffrey E Hinton. Learn-\ning to represent spatial transformations with factored\nhigher-order Boltzmann machines. Neural Computa-\ntion, 22(6):1473–1492, 2010.\n[19] David Meredith. Cosiatec and siateccompress: Pattern\ndiscovery by geometric compression. In International\nSociety for Music Information Retrieval Conference ,\n2013.\n[20] David Meredith, Kjell Lemstr ¨om, and Geraint A Wig-\ngins. Algorithms for discovering repeated patterns in\nmultidimensional representations of polyphonic music.\nJournal of New Music Research , 31(4):321–345, 2002.\n[21] Vincent Michalski, Roland Memisevic, and Kishore\nKonda. ”modeling deep temporal dependencies with\nrecurrent grammar cells”. In Advances in neural infor-\nmation processing systems , pages 1925–1933, 2014.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 667[22] Decebal Constantin Mocanu, Haitham Bou Ammar,\nDietwig Lowet, Kurt Driessens, Antonio Liotta, Ger-\nhard Weiss, and Karl Tuyls. Factored four way con-\nditional restricted Boltzmann machines for activity\nrecognition. Pattern Recognition Letters , 66:100–108,\n2015.\n[23] Meinard M ¨uller and Michael Clausen. Transposition-\ninvariant self-similarity matrices. In Simon Dixon,\nDavid Bainbridge, and Rainer Typke, editors, Pro-\nceedings of the 8th International Conference on Music\nInformation Retrieval, ISMIR 2007, Vienna, Austria,\nSeptember 23-27, 2007 , pages 47–50. Austrian Com-\nputer Society, 2007.\n[24] Eita Nakamura and Shinji Takaki. Characteristics of\npolyphonic music style and markov model of pitch-\nclass intervals. In Tom Collins, David Meredith, and\nAnja V olk, editors, Mathematics and Computation in\nMusic - 5th International Conference, MCM 2015,\nLondon, UK, June 22-25, 2015, Proceedings , volume\n9110 of Lecture Notes in Computer Science , pages\n109–114. Springer, 2015.\n[25] Oriol Nieto and Morwaread M Farbood. Identifying\npolyphonic patterns from audio recordings using music\nsegmentation techniques. In Proc. of the 15th Interna-\ntional Society for Music Information Retrieval Confer-\nence, pages 411–416, 2014.\n[26] Bruno A Olshausen, Charles Cadieu, Jack Culpep-\nper, and David K Warland. Bilinear models of natural\nimages. In Electronic Imaging 2007 , pages 649206–\n649206. International Society for Optics and Photon-\nics, 2007.\n[27] Laura Rimell, Amandla Mabona, Luana Bulat, and\nDouwe Kiela. Learning to negate adjectives with bi-\nlinear models. EACL 2017 , page 71, 2017.\n[28] Jan Schlueter and Christian Osendorfer. Music simi-\nlarity estimation with the mean-covariance restricted\nBoltzmann machine. In 10th International Conference\non Machine Learning and Applications and Workshops\n(ICMLA), 2011 , volume 2, pages 118–123. IEEE,\n2011.\n[29] Karen Simonyan, Andrea Vedaldi, and Andrew Zisser-\nman. Deep inside convolutional networks: Visualising\nimage classiﬁcation models and saliency maps. arXiv\npreprint arXiv:1312.6034 , 2013.\n[30] Joshua B Tenenbaum and William T Freeman. Sepa-\nrating style and content with bilinear models. Neural\nComputation , 12(6):1247–1283, 2000.\n[31] Pascal Vincent, Hugo Larochelle, Isabelle La-\njoie, Yoshua Bengio, and Pierre-Antoine Manzagol.\nStacked denoising autoencoders: Learning useful rep-\nresentations in a deep network with a local denois-\ning criterion. Journal of Machine Learning Research ,\n11(Dec):3371–3408, 2010.[32] C V on der Malsburg. The correlation theory of brain\nfunction reprinted in e. domani, jl van hemmen and k.\nschulten (eds.), models of neural networks ii, 1981.\n[33] Thomas C Walters, David A Ross, and Richard F\nLyon. The intervalgram: an audio feature for large-\nscale melody recognition. In Proc. of the 9th Interna-\ntional Symposium on Computer Music Modeling and\nRetrieval (CMMR) . Citeseer, 2012.\n[34] Cheng-i Wang, Jennifer Hsu, and Shlomo Dubnov.\nMusic pattern discovery with variable markov oracle:\nA uniﬁed approach to symbolic and audio represen-\ntations. In Meinard M ¨uller and Frans Wiering, edi-\ntors, Proceedings of the 16th International Society for\nMusic Information Retrieval Conference, ISMIR 2015,\nM´alaga, Spain, October 26-30, 2015 , pages 176–182,\n2015.\n[35] Gerhard Widmer. Discovering simple rules in com-\nplex data: A meta-learning algorithm and some\nsurprising musical discoveries. Artiﬁcial Intelligence ,\n146(2):129–148, 2003.668 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Revisiting Singing Voice Detection: A quantitative review and the future outlook.",
        "author": [
            "Kyungyun Lee",
            "Keunwoo Choi",
            "Juhan Nam"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492463",
        "url": "https://doi.org/10.5281/zenodo.1492463",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/38_Paper.pdf",
        "abstract": "Since the vocal component plays a crucial role in popular music, singing voice detection has been an active research topic in music information retrieval. Although several proposed algorithms have shown high performances, we argue that there is still room for improving the singing voice detection system. In order to identify the area of improvement, we first perform an error analysis on three recent singing voice detection systems. Based on the analysis, we design novel methods to test the systems on multiple sets of internally curated and generated data to further examine the pitfalls, which are not clearly revealed with the currently available datasets. From the experiment results, we also propose several directions towards building a more robust singing voice detector.",
        "zenodo_id": 1492463,
        "dblp_key": "conf/ismir/LeeCN18",
        "keywords": [
            "vocal component",
            "music information retrieval",
            "singing voice detection",
            "active research topic",
            "crucial role",
            "high performances",
            "room for improvement",
            "error analysis",
            "internally curated",
            "generated data"
        ],
        "content": "REVISITING SINGING VOICE DETECTION:\nA QUANTITATIVE REVIEW AND THE FUTURE OUTLOOK\nKyungyun Lee1Keunwoo Choi2Juhan Nam3\n1School of Computing, KAIST\n2Spotify Inc., USA\n3Graduate School of Culture Technology, KAIST\nkyungyun.lee@kaist.ac.kr, keunwooc@spotify.com, juhannam@kaist.ac.kr\nABSTRACT\nSince the vocal component plays a crucial role in popular\nmusic, singing voice detection has been an active research\ntopic in music information retrieval. Although several pro-\nposed algorithms have shown high performances, we ar-\ngue that there is still room for improving the singing voice\ndetection system. In order to identify the area of improve-\nment, we ﬁrst perform an error analysis on three recent\nsinging voice detection systems. Based on the analysis,\nwe design novel methods to test the systems on multiple\nsets of internally curated and generated data to further ex-\namine the pitfalls, which are not clearly revealed with the\ncurrently available datasets. From the experiment results,\nwe also propose several directions towards building a more\nrobust singing voice detector.\n1. INTRODUCTION\nSinging voice detection (or VD, vocal detection) is a music\ninformation retrieval (MIR) task to identify vocal segments\nin a song. The length of each segment is typically at a\nframe level, for example, 100 ms. Since singing voice is\none of the key components in popular music, VD can be\napplied to music discovery and recommendation as well as\nvarious MIR tasks such as melody extraction [7], audio-\nlyrics alignment [31], and artist recognition [2].\nExisting VD methods can be categorized into three dif-\nferent classes. First , the early approaches focused on the\nacoustic similarity between singing voice and speech, uti-\nlizing cepstral coefﬁcients [1] and linear predictive cod-\ning [10]. The second class would be the majority of ex-\nisting methods, where the systems take advantages of ma-\nchine learning classiﬁers such as support vector machines\nor hidden Markov models, combined with large sets of au-\ndio descriptors (e.g., spectral ﬂatness) as well as dedicated\nnew features such as the Fluctogram [14]. Lastly , there\nis a recent trend towards feature learning using deep neu-\nral networks, with which the VD systems learn optimized\nc\rKyungyun Lee, Keunwoo Choi, Juhan Nam. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Kyungyun Lee, Keunwoo Choi, Juhan Nam. “Revis-\niting Singing V oice Detection: A quantitative review and the future out-\nlook”, 19th International Society for Music Information Retrieval Con-\nference, Paris, France, 2018.features for the task using a convolutional neural network\n(CNN) [27] and a recurrent neural network (RNN) [11].\nThey have achieved state-of-the-art performances on com-\nmonly used datasets with over 90% of the true positive rate\n(recall) and accuracy.\nWe hypothesize that there are common problems in ex-\nisting VD methods in spite of such well-performing met-\nrics that have been reported. Our scope primarily includes\nmethods in the second and third classes since they signif-\nicantly outperform those in the ﬁrst class. Our hypothe-\nsis was inspired by inspecting the assumptions in the ex-\nisting algorithms. The most common one, for example,\nhas been made on the spectro-temporal characteristics of\nsinging voices; that they include frequency modulation (or\nvibrato) [15, 24], which leads to our analysis on whether\nthere are any problems by pursuing to be a vibrato detector.\nWe can also raise similar questions on the behavior of the\nsystems in the third class, the deep learning-based systems,\nby examining on their assumptions and results. Based on\nthe analysis, we invent a set of empirical analysis methods\nand use them to reveal the exact types of problems in the\ncurrent VD systems.\nOur contributions are as follows :\n\u000fA quantitative analysis to clarify and classify common\nerrors of three recent VD systems (Section 4)\n\u000fAn analysis using curated and generated audio con-\ntents that exploit the discovered weakness of the systems\n(Section 5)\n\u000fSuggestions on future research directions (Section 6)\nIn addition, we review previous VD systems in Section 3\nand summarize the paper in Section 7.\n2. BACKGROUND\n2.1 Problem deﬁnition\nSinging voice detection is usually deﬁned as a binary clas-\nsiﬁcation task about whether a short audio segment in-\nput includes singing voice. However, the details have\nbeen rather empirically decided. By ‘short’, the segment\nlength for prediction is often 100 ms or 200 ms. ‘Audio’\ncan be provided as stereo, although they are frequently\ndownmixed to mono. More importantly, singing voice is\nnot clearly deﬁned, for example, leaving the question that506Size Annotations Past VD papers Notes\nJamendo Corpus 93 tracks (443 mins) V ocal activation[13], [27], [26][11], [24], [12],Train/valid/test split from [22]\nRWC Popular Music 100 tracks (407 mins)instrument annotationV ocal activation,\n[13][26], [27], [14]VD annotation by [16]\nMIR-1K 100 short clips (113 mins)pitch contoursV ocal activation,[9]providedRegular speech ﬁles\nMedleyDB 122 tracks (437 mins)pitch annotationMelody annotation,[26] Multitrack\nTable 1 : A summary of public datasets relevant to singing voice detection\nbackground vocals should be regarded as singing voice or\nnot. In previous works, this problem has been neglected\nsince the majority of songs in datasets do not include back-\nground vocals that are independent of the main vocals.\nThese will be further discussed in Section 6.\n2.2 Public Datasets\nIn Table 1, four public datasets for evaluating VD systems\nare summarized. Three of them are well described by\nLehner et al. [12]: Jamendo Corpus [22], RWC Popular\nMusic Database [4] and MIR-1K Corpus [8]. In addition,\nwe add MedleyDB [3], which is a multitrack dataset, com-\nposed of raw mono recordings for each instrument as well\nas processed stereo mix tracks. Although it does not pro-\nvide annotations for vocal/non-vocal segments, we utilize\nthe annotations for the instrument activation, which con-\nsiders vocals as one of the instruments. There can be more\nbeneﬁts by using the multitrack dataset for VD research,\nwhich will be discussed in Section 6.\n2.3 Audio Representation\nIn this section, we present the properties as well as the un-\nderlying assumptions of various audio representations in\nthe context of VD. Previous works have used a combina-\ntion of numerous audio features, seeking easier ways for\nthe algorithm to detect the singing voice. They range from\naudio representations such as short-time Fourier transform\n(STFT) to high-level features such as onsets and pitch es-\ntimations.\n\u000fSTFT provides a 2-dimensional representation of au-\ndio, decomposing the frequency components. STFT is\nprobably the most basic (or ‘raw’) representation in VD,\nbased on which some other representations are either\ndesigned and computed, or learned using deep learning\nmethods.\n\u000fMel-spectrogram is a mel-scaled frequency representa-\ntion and usually more compressive than STFTs and orig-\ninally inspired by the human perception of speech. Be-\ning closely related to speech provides a good motivation\nto be used in VD, therefore mel-spectrogram has been\nactively used as an input representation of CNNs [27]\nand RNNs [11]. When deep learning methods are used,\nmel-spectrogram is often preferred due to its efﬁciency\ncompared to STFT.\u000fSpectral Features such as spectral centroid and spec-\ntral roll-off are statistics of a spectral distribution of\na single frame of time-frequency representations (e.g.,\nSTFT). A particular and most noteworthy example\nisMel-Frequency Cepstral Coefﬁcients (MFCCs).\nMFCCs have originally been designed for automatic\nspeech recognition and take advantages of mel-scale\nand Fourier analysis for providing approximately pitch-\ninvariant timbre-related information. They are often\n(assumed to be) relevant to MIR tasks including VD\n[12, 25]. Spectral features, in general, are not robust to\nadditive noise, which means that they would be heavily\naffected by the instrumental part of the music when used\nfor VD.\n3. MODELS\nIn this section, we introduce three recent and distinctive\nVD systems that have improved the state-of-the-art perfor-\nmances along with the details of our re-implementation of\nthem.1They are brieﬂy illustrated in Figure 1, where x\nandyindicate the input audio signal and the output predic-\ntion, respectively.\n3.1 Lehner et al. [14] ( FE-VD )\nThis feature engineering (FE) method, FE-VD is based on\nthe Fluctogram, spectral ﬂatness, vocal variance and other\nhand-engineered audio features. We select this model for\nits rich and task-speciﬁc feature extraction process to com-\npare with the other models. Although the features are ul-\ntimately computed frame-wise, context from the adjacent\nframes are taken into account, supposedly enabling the sys-\ntem to use dynamic aspect of the features. The features are\naimed to reduce the false positive rate caused by the confu-\nsion between singing voice and pitch-varying instruments\nsuch as woodwinds and strings. Random forest classiﬁer\nwas adopted as a classiﬁer, achieving an accuracy of 88.2%\non the Jamendo dataset. While their methods have shown\nreduction in the false positive rates on strings, Lehner et al.\nmentions woodwinds such as pan ﬂutes and saxophones\nstill show high error rate.\nFollowing [14], we extract 6 different audio features\n(the Fluctogram, spectral ﬂatness, spectral contraction, vo-\ncal variances, MFCCs and delta MFCCs), resulting in 116-\ndimensional features per frame. We use input size of\n1http://github.com/kyungyunlee/ismir2018-revisiting-svdProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 507Figure 1 : Block diagrams for three VD systems – (a)\nFE-VD [14], (b) CNN-VD [27], and (c) RNN-VD [11]. x\nandyfor input audio signal and output prediction (prob-\nability of singing voice). Rounded and gray blocks are\ntrainable classiﬁers or layers. The details of the features\nin (a) are explained in [14]. In (c), ‘+’ indicates frequency-\naxis concatenation and ‘h’ and ‘p’ are the separated har-\nmonic/percussive components.\n1.1 seconds as the input to the random forest classiﬁer,\nwhere we perform grid search to ﬁnd optimal parameters.\nAs a post-processing step, we apply the median ﬁlter of\n800 ms on the output predictions.\n3.2 Schl ¨uter et al. [27] ( CNN-VD )\nRecently, VD systems using deep learning models have\nshown the state-of-the-art results [11, 26, 27]. These sys-\ntems often use basic audio representations such as STFT\nas an input to the models such as CNN and RNN, expect-\ning the relevant features are learned by the model. We ﬁrst\nintroduce a CNN-based system [27].\nSchl¨uter et al. suggested a deep CNN architecture with\n4 3-by-3 2D convolution layers. We name the CNN model\nCNN-VD . As a result, the system extracts trained, rele-\nvant local time-frequency patterns from its input, a mel-\nspectrogram. During training, they apply data augmenta-\ntion such as pitch shifting and time stretching on the audio\nrepresentation. They reported that it reduces the error rate\nfrom 9.4% to 7.7% on the Jamendo dataset.\nOur CNN architecture is identical to the original one\nand uses an input size of 115 frames (1.6 sec). However,\nwe do not perform data augmentation or threshold opti-\nmization for a fair comparison with other models. Thus,\nwe use 0.5 as the threshold value for the prediction. Here,\nwe also apply median ﬁlter of 800 ms for smoothing.\n3.3 Leglaive et al. [11] ( RNN-VD )\nAs another deep learning-based system, Leglaive et al. [11]\nproposed a recurrent neural network with bi-directional\nlong short-term memory units (Bi-LSTMs) [6], with an\nassumption that temporal information of music can pro-\nvide valuable information for detecting vocal segments.\nWe name this system RNN-VD . For the classiﬁer input, the\nsystem performs double-stage harmonic-percussion source\nseparation (HPSS) [20] on the audio signal to extract sig-\nnals relevant to the singing voice. For each frame, Mel-\nspectrograms of the obtained harmonic and percussive\ncomponents are concatenated as an input for the classi-\nﬁer. Several recurrent layers followed by a shared densely-FE-VD CNN-VD RNN-VD\nAcc.(%) 87.9 86.8 87.5\nRecall(%) 91.7 89.1 87.2\nPrecision(%) 83.8 83.7 86.1\nF-measure(%) 87.6 86.3 86.6\nFPR(%) 15.3 15.1 12.2\nFNR(%) 8.3 10.9 12.8\nTable 2 : Results of our implementations on the Jamendo\ntest set. FPR and FNR refer to false positive rate and false\nnegative rate, respectively.\nconnected layer (also known as time-distributed dense\nlayer) yield the output predictions for each input frame.\nThis model achieves the state-of-the-art result without data\naugmentation, showing accuracy of 91.5% on the Jamendo\ndataset. From this result, although the contributions from\nadditional preprocessing vs. recurrent layers may be com-\nbined, we can assume that past and future temporal context\nhelp to identify vocal segments.\nFor our RNN architecture, we use the best performing\nmodel from the original article [11], one with three hidden\nlayers of size 30, 20 and 40. The input to the model is 218\nframes (3.5 seconds) and the threshold value of 0.5 is used\nto predict the presence of singing voice as done in [11].\n4. EXPERIMENT I: ERROR CATEGORIZATION\nThe purpose of this experiment is to identify common er-\nrors in the VD systems through our implementation of\nmodels from Section 3. The results and observations lead\nto the motivation of experiments in Section 5. Librosa [18]\nis used in audio processing and feature extraction stages.\n4.1 Data and Methods\nThree systems ( FE-VD ,CNN-VD , and RNN-VD ) are\ntrained on the Jamendo dataset with the suggested split of\n61, 16 and 16 for training, validation and test sets [22], re-\nspectively. They are primarily tested on the Jamendo test\nset. For qualitative analysis, we also utilize MedleyDB.\n4.2 Results\nThe test results of our implementation are shown in\nTable 2. We did not focus on ﬁne-tuning individual models\nbecause three systems altogether are used as a tool to get\na generalized view of the recent VD systems, thus show-\ning slightly lower performances compared to the results in\noriginal papers. Overall, FE-VD ,CNN-VD andRNN-VD\nshow a negligible difference on the test scores. We ob-\nserve trends that are similar to the original papers in terms\nof performance and the precision/recall ratio.\nUpon listening to the misclassiﬁed segments, we cat-\negorize the source of errors into three classes – pitch-\nﬂuctuating instruments, low signal-to-noise ratio of the\nsinging voice, and non-melodic sounds.508 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Song Title Confusing inst FE-VD CNN-VD RNN-VD\nLIrlandaise Woodwind, Synth 46.6 29.5 22.0\nCastaway Elec. Guitar 62.5 56.5 24.2\nSay me Good Bye N/A 2.8 3.0 2.5\nInside N/A 5.9 6.7 5.0\nTable 3 : False positive rate (%) of each system for 4 songs\nfrom the Jamendo test set. The top 2 songs are the ones\nranked within the top 5 lowest accuracy and the bottom 2\nsongs are the ones ranked within the top 5 highest accura-\ncies at song level across all three systems.\n4.2.1 Pitch-ﬂuctuating instruments\nClasses of instruments such as strings, woodwinds and\nbrass exhibit similar characteristics as the singing voice,\nwhich we refer to as being ‘voice-like’ [28]. By ‘voice-\nlike’, we consider three aspects of the signal, namely,\npitch range, harmonic structure, and temporal dynamics\n(vibrato). Especially, we ﬁnd temporal dynamics as im-\nportant attributes that are recognized by the VD systems to\nidentify vocal segments.\nFrequency modulation, also known as vibrato, resem-\nbles the modulation created from the vowel component of\nsinging voice. This is illustrated in Figure 2, where mel-\nspectrograms of both female vocalist and an electric guitar\nshow curved lines. We observe that this similarity causes\nfurther confusion in the system.\nIn Table 3, we list two songs found among the top 5\nleast/most accurately predicted songs in the test set of\nall three systems. The woodwind in ‘05 - Llrlandaise’\ncauses high false positives, which may be due to the pres-\nence of vibrato and the similarity in pitch range to that of\nsoprano singers (above 220 Hz). FE-VD andCNN-VD\nshow poor performance on woodwinds, probably because\nthe Fluctogram of FE-VD and small 2D convolution ker-\nnels of CNN-VD are speciﬁcally designed to detect vibrato\nas one of the features for identifying singing voice. In the\nsame song, all three systems show confusion with the syn-\nthesizer. Synthesizers mimicking pitch-ﬂuctuating instru-\nments are particularly challenging as it is difﬁcult to char-\nacterize them as a speciﬁc instrument type.\nIn addition, electric guitars are one of the most fre-\nquently found sources of false positives, as can be seen\nfrom ‘03 - castaway’, mostly caused by the recognizable\nvibrato patterns. We ﬁnd the confusion worse when the\nguitar is played with effects such as wah-wah, which imi-\ntates the vowel sound of the human. Lastly, we note that\nsome of the other problematic instruments in our test sets\ninclude saxophones, trombones and cellos, which are well-\nknown ‘voice-like’ instruments.\nThis observation, regarding the system pitfalls on vi-\nbrato patterns, is further investigated in Section 5.1.\n4.2.2 Signal-to-noise ratio and the performance\nLastly, we note that all the three systems are affected by\nthe signal-to-noise ratio (SNR), or the relative gain of vo-\ncal component, as one can easily expect. All of the three\n95.5 96.5\ntime (second)51210242048frequency (Hz)\n154.5 155.5\ntime (second)Figure 2 : Excerpts of mel-spectrograms from MedleyDB:\n‘Handel TornamiA Vagheggiar’ with female vocalist (left)\nand ‘PurlingHiss Lolita’ with electric guitar (right) (see\nSection 4.2.1.)\nsystems exhibit high false negative rate when the vocal sig-\nnal is relatively at a low level.\nIn systems such as FE-VD , where audio features such\nas MFCCs or spectral ﬂatness are used, the performance\nvaries by SNR because the features are statistics of the\nwhole bandwidth which includes not only the target signal\n(vocal) but also additive noise (instrumental). VD systems\nwith deep neural networks are also not free from this issue\nsince the low-level operation in the layers of deep neural\nnetworks may end up being a simple pattern matching by\ncomputing correlation.\nThis is a common phenomenon in other tasks as well,\ne.g., speech recognition, and we continue the discussion to\na follow-up experiment in Section 5.2 and ﬁnally a sugges-\ntion on the problem deﬁnition and dataset composition in\nSection 6.\n4.2.3 Non-melodic Sources\nAlthough the interest of most VD systems appears to lie\nmainly in the melodic component of the song, we ex-\npected the system to learn percussive nature of the singing\nvoice as well, which is exhibited by consonants from the\nsingers. Therefore, our hypothesis is whether the system\nis confused by the consonants of singing voice and percus-\nsive instruments, resulting in either i)missing consonant\nparts (false negative) or ii)mis-classifying percussive in-\nstruments (false positive).\nFrom our test results, we encounter false positive seg-\nments containing snare drums and hi-hats, but the exact\ncause of this misclassiﬁcation is unclear. We further tested\nthe system with drum set solos for potential false positives\nand with a collection of consonant sounds such as plosives\nand fricatives from the human voice for potential false neg-\natives, but we did not observe a clear pattern in misclassiﬁ-\ncation. Although we do not conduct further experiment on\nthis, it suggests a deeper analysis, which may also lead to\na clear understanding of preprocessing strategies including\nHPSS.\n5. EXPERIMENT II: STRESS TESTING\n5.1 Testing with artiﬁcial vibrato\nBased on the confusion between ‘voice-like’ instruments\nand singing voice, we hypothesize that the current VD sys-Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 509tems use vibrato patterns as one of the main tools for vocal\nsegment detection. We explore the degree of confusion for\neach VD system by testing them on synthetic vibratos with\nvarying rate, extent and formant frequencies.\n5.1.1 Data Preparation\nWe create a set of synthetic vibratos with low pass-ﬁltered\nsawtooth waveforms with f0=220 Hz. We vary the modu-\nlation rate and frequency deviation ( f\u0001) to investigate their\neffects. Furthermore, we apply 5 bi-quad ﬁlters at the cor-\nresponding formant frequencies (3 for each) to synthesize\nso that they would sound like the basic vowel sounds, ‘a’,\n‘e’, ‘i’, ‘o’, ‘u’ [29]. The modulation rate ranges in f0.5,\n1, 2, 4, 6, 8, 10 Hz gand the frequency deviation ranges in\nf0.01, 0.1, 0.3, 0.6, 1, 2, 4, 8 semitones gwith respect to its\nf0). As a result, the set consists of 7(rates)\u00028(f\u0001’s)\u00026\n(5 formants + 1 unﬁltered) = 336 variations.\n5.1.2 Results\nFigure 3 shows the result of the prediction by the three VD\nsystems on the synthetic vibratos. The accuracy of 1.0 in-\ndicates that the system does not confuse the artiﬁcial vi-\nbratos with singing voice. Here, we observe the perfor-\nmance difference of each model, which were not visible\nfrom looking at the scores in Table 2. In general, confu-\nsion areas tend to be concentrated on the bottom left to the\ncenter area of the graph. The extent and rate of the artiﬁ-\ncial tones that are highly misclassiﬁed seem to be around\nthe range of vibratos of singers, which is said to be around\n0.6 to 2 semitone with rate around 5.5 to 8 Hz [30]. We\nalso observe a within-system difference, i.e., the presence\nand the type of formants affect the models. For instance,\nvibratos mimicking the vowel ‘a’ cause higher misclassiﬁ-\ncation in all three models.\nFE-VD performs much better than the latter two sys-\ntems. Note that FE-VD is a feature engineering model,\nwhere unique features, such as the Fluctogram and vocal\nvariance, are mostly adapted from the ones used in speech\nrecognition task. As these features were intentionally de-\nsigned to reduce false positives from pitch-varying instru-\nments, it appears to signiﬁcantly reduce error rate on vi-\nbratos with rate and extent that are beyond the range of\nhuman singers.\nCNN-VD confuses slightly wider range of vibratos.\nThis is expected to some extent since the model promi-\nnently uses 3\u00023 ﬁlters on mel-spectrogram to detect local\nfeatures, which can be regarded as a local pattern detector.\nIn other words, the locality of CNN results in a system that\nis easily confused by frequency modulation regardless of\nthe non-singing voice aspects of the signal. This implies\nthat the model may beneﬁt from looking at a varying range\nof time and frequency to learn vocal-speciﬁc characteris-\ntics such as timbre [21].\nLastly, RNN-VD performs better than the CNN-VD ,\nthough worse than FE-VD . On detecting vocal and non-\nvocal segments, it seems natural, even for humans, that\npast and future temporal context help. Also, we presume\nthat the preprocessing of double stage HPSS contributes to\nunfiltered\n a\n e\n i\n o\n u\n       f (semitones)\n0.512468100.01\n0.1\n0.3\n0.6\n1\n2\n4\n8\nrate (Hz)\nFE\nCNN\nRNN\n0.0 0.2 0.4 0.6 0.8 1.0\nAccuracyFigure 3 : Heat-maps of the accuracies of the vibrato\nexperiment result. Each row corresponds to VD sys-\ntems ( FE-VD ,CNN-VD ,RNN-VD ) and each column cor-\nresponds to the formant (unﬁltered, ’a’, ’e’, ’i’, ’o’, ’u’).\nWithin each heat map, x- and y-axes correspond to the\nvibrato rate and frequency deviation as annotated on the\nlower-left subplot (see Section 5.1)\nthe robustness of the system against vibrato. Again, this\nobservation leaves a question of separating the contribu-\ntions from preprocessing and model structure.\n5.2 Testing with SNR\nIn this experiment, VD systems are tested with vocal gain\nadjusted tracks to further explore the behavior of the sys-\ntems on various scenarios, which can reﬂect the real-world\naudio settings of live recordings and radios, for example.\n5.2.1 Data preparation\nWe create a modiﬁed test set using 61 vocal-containing\ntracks provided by MedleyDB. We use the ﬁrst 30 seconds\nof the songs to build a pair of (vocal, instrumental) tracks.\nV ocal tracks are modiﬁed with SNR of f+12 dB, -12 dB,\n+6 dB, -6 dB, 0 dB g.\n5.2.2 Results\nThe results of the energy level robustness test are presented\nin Figure 4 with false positive rate, false negative rate, and\noverall error rate. We see a consistent trend across the per-\nformance of all three VD systems, which is once again an\nexpected pattern as aforementioned in Section 4.2.2 – that\nincreasing SNR helps to reduce false negatives. Overall er-\nror rate also exhibits a noticeable decrease in common with\nhigher SNRs. In practice, one could take advantage of data\naugmentation with changing SNR to build a more robust\nsystem. More importantly, it can be part of the evaluation\nprocedure for VD, as we discuss in Section 6.\nWhile the VD systems behave similarly on all test\ncases, we note that FE-VD , owing to its additional fea-\ntures, shows lowest variance and lowest value for the false\npositive rate. Also, our assumption that the double-stage\nHPSS, which ﬁlters out vocal-related signals, would make\nRNN-VD more robust against SNR is observed to be not\nnecessarily true as we clearly see performance differences\nacross the varying SNR test cases.510 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018-12 -60 (REF) +6 +12\nVocal Level Adjustments (dB)020406080100FPR (%) FE CNN RNN\n-12 -60 (REF) +6 +12\nVocal Level Adjustments (dB)020406080100FNR (%) FE CNN RNN\n-12 -60 (REF) +6 +12\nVocal Level Adjustments (dB)020406080100Error Rate (%)FE CNN RNNFigure 4 : False positive rates, false negative rates, and overall error rates for the three systems in the stress testing with\ncontrolling SNR (see Section 5.2).\n6. DIRECTIONS TO IMPROVE\n6.1 Deﬁning the problem and the datasets\n6.1.1 Deﬁning singing voice\nBy using the annotations in datasets such as Jamendo,\nmany VD systems implicitly assume that the target\n‘singing voice’ is deﬁned as vocal components that corre-\nspond to the main melody . Other voice-related components\nsuch as backing vocal, narration, humming, and breathing\nare not clearly deﬁned to be singing voice or not.\nIn some applications, however, they can be of interest.\nFor example, a system may want to ﬁnd purely instrumen-\ntal tracks, avoiding tracks with backing vocal. In this case,\nthe method should consider backing vocal as singing voice.\nHowever, for Karaoke applications, only the singing voice\nof the main melody would matter.\nTherefore, an improvement can be made on deﬁning the\nVD problem and creating datasets. For the annotation, a\nhierarchy among the voice-related components can be use-\nful for both structured training and evaluation of a sys-\ntem [17, 23]. For the audio input, we see a great beneﬁt\nof multitracks, where main vocal melody, backing vocal,\nand other components are provided separately.\n6.1.2 Varying-SNR scenarios\nFor a long while, varying SNR had been one of the com-\nmon ways to evaluate speech recognition or enhance-\nment using dataset such as Aurora [5]. As observed in\nSection 4.2.2, it can be used as a ‘test-set augmentation’\nto measure the performance of a system more precisely.\nAlso, it can be an additional data augmentation method\nalong with the ones in [27] to build a VD system more\nrobust to various audio settings, such as audios from user\ngenerated videos. These can both be easily achieved with\na multitrack dataset in practice.\n6.1.3 Measuring dataset noise\nHuman annotators are neither perfect or identical, thus\ncausing annotation noise and disagreement. Since VD is\na binary classiﬁcation problem, we may remain optimistic\nby assuming that the annotation noise is a matter of tem-\nporal precision, which is arbitrary and not agreed among\nmany datasets so far. For example, in RWC Popular Music\n[16], “short background segments of less than 0.5-second\nduration were merged with the preceding region” and the\nannotations have 8 decimal digits (in second), while in Ja-\nmendo, they are 3 decimal digits. The optimal precisionmay depend on human perception of sound which is often\nsaid around 10 ms in general [19]. Although it would re-\nquire a deeper investigation, the current temporal precision\nmay be too high, leading to evaluate the systems with an\noverly precise annotation.\n6.2 Learning from human perception\nThe characteristic of voice was the main motivation in the\nvery early works exploiting speech-related features [1,10].\nClearly, however, those approaches that solely relied on\nspeech features showed limited performances. While fol-\nlowing works has improved the performance, as our exper-\niments have demonstrated through this paper, the systems\ndo not completely take advantage of the cues that human is\nprobably using, e.g., the global formants, linguistic infor-\nmation, musical knowledge, etc.\n6.3 Preprocessing\nA light-weight VD system was introduced in [12] where\nonly MFCCs were used to achieve an accuracy of 84.8%\non the Jamendo dataset. This implies that there is a possi-\nbility to achieve better performance by optimizing the pre-\nprocessing stage. One of the unanswered questions is the\neffect of the preprocessing stage in RNN-VD [11] as well\nas whether similar processing could lead to better perfor-\nmance with other systems, e.g., CNN [27].\n7. CONCLUSIONS\nIn this paper, we suggested that there still are several areas\nto improve for the current singing voice detectors. In the\nﬁrst set of experiments, we identiﬁed the common errors\nthrough error analysis on three recent systems. Our obser-\nvations that the main sources of error are pitch-ﬂuctuating\ninstruments and low signal-to-noise ratios of the singing\nvoice motivated us to further perform stress tests. Test-\ning with synthetic vibratos revealed that some systems\n(FE-VD ) are more robust to non-vocal vibratos than others\n(CNN-VD andRNN-VD ). SNR-varying test showed that\nSNR manipulation greatly affects the current VD systems,\nthus it can potentially be used to strengthen the VD sys-\ntems to become invariant to a wider range of audio settings.\nAs we propose several directions for a more robust singing\nvoice detector, we note that deﬁning the VD problem is\ndependent on the goal of the system, thus using multitrack\ndatasets can be beneﬁcial. Our future interest is to further\ninvestigate on SNR to extend VD systems on uncontrolled\naudio settings and to examine different components of in-\ndividual systems, including the preprocessing stage.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 5118. ACKNOWLEDGEMENTS\nWe thank Bernhard Lehner and Simon Leglaive for ac-\ntive discussion and code, Jeongsoo Park for sharing Ono’s\ncode. This work was supported by the National Research\nFoundation of Korea (Project 2015R1C1A1A02036962).\n9. REFERENCES\n[1] Adam L Berenzweig and Daniel PW Ellis. Locating\nsinging voice segments within music signals. In Appli-\ncations of Signal Processing to Audio and Acoustics,\n2001 IEEE Workshop on the , pages 119–122. IEEE,\n2001.\n[2] Adam L Berenzweig, Daniel PW Ellis, and Steve\nLawrence. Using voice segments to improve artist clas-\nsiﬁcation of music. In Audio Engineering Society Con-\nference: 22nd International Conference: Virtual, Syn-\nthetic, and Entertainment Audio . Audio Engineering\nSociety, 2002.\n[3] Rachel M Bittner, Justin Salamon, Mike Tierney,\nMatthias Mauch, Chris Cannam, and Juan Pablo\nBello. MedleyDB: A multitrack dataset for annotation-\nintensive mir research. In ISMIR , volume 14, pages\n155–160, 2014.\n[4] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC music database:\nPopular, classical and jazz music databases. In Proc.\nof the 3rd International Society for Music Information\nRetrieval Conference (ISMIR) , volume 2, pages\n287–288, 2002.\n[5] Hans-G ¨unter Hirsch and David Pearce. The aurora\nexperimental framework for the performance evalua-\ntion of speech recognition systems under noisy con-\nditions. In ASR2000-Automatic Speech Recognition:\nChallenges for the new Millenium ISCA Tutorial and\nResearch Workshop (ITRW) , 2000.\n[6] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-\nterm memory. Neural computation , 9(8):1735–1780,\n1997.\n[7] Chao-Ling Hsu, Liang-Yu Chen, Jyh-Shing Roger\nJang, and Hsing-Ji Li. Singing pitch extraction from\nmonaural polyphonic songs by contextual audio mod-\neling and singing harmonic enhancement. In Proc. of\nthe 10th International Society for Music Information\nRetrieval Conference (ISMIR) , pages 201–206, 2009.\n[8] Chao-Ling Hsu and Jyh-Shing Roger Jang. On the\nimprovement of singing voice separation for monau-\nral recordings using the MIR-1K dataset. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n18(2):310–319, 2010.\n[9] Chao-Ling Hsu, DeLiang Wang, Jyh-Shing Roger\nJang, and Ke Hu. A tandem algorithm for singing pitchextraction and voice separation from music accompa-\nniment. IEEE Transactions on Audio, Speech, and Lan-\nguage Processing , 20(5):1482–1491, 2012.\n[10] Youngmoo E Kim and Brian Whitman. Singer identiﬁ-\ncation in popular music recordings using voice coding\nfeatures. In Proc. of the 3rd International Conference\non Music Information Retrieval (ISMIR) , volume 13,\npage 17, 2002.\n[11] Simon Leglaive, Romain Hennequin, and Roland\nBadeau. Singing voice detection with deep recurrent\nneural networks. In Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 2015 IEEE International Confer-\nence on , pages 121–125. IEEE, 2015.\n[12] Bernhard Lehner, Reinhard Sonnleitner, and Ger-\nhard Widmer. Towards light-weight, real-time-capable\nsinging voice detection. In Proc. of the 14th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 53–58, 2013.\n[13] Bernhard Lehner, Gerhard Widmer, and Sebastian\nB¨ock. A low-latency, real-time-capable singing voice\ndetection method with lstm recurrent neural networks.\nInSignal Processing Conference (EUSIPCO), 2015\n23rd European , pages 21–25. IEEE, 2015.\n[14] Bernhard Lehner, Gerhard Widmer, and Reinhard\nSonnleitner. On the reduction of false positives in\nsinging voice detection. In Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2014 IEEE International\nConference on , pages 7480–7484. IEEE, 2014.\n[15] Maria E Markaki, Andr ´e Holzapfel, and Yannis\nStylianou. Singing voice detection using modulation\nfrequency feature. In SAPA@ INTERSPEECH , pages\n7–10, 2008.\n[16] Matthias Mauch, Hiromasa Fujihara, Kazuyoshi\nYoshii, and Masataka Goto. Timbre and melody fea-\ntures for the recognition of vocal activity and instru-\nmental solos in polyphonic music. In Proc. of the 12th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 233–238, 2011.\n[17] Brian McFee and Juan Pablo Bello. Structured training\nfor large-vocabulary chord recognition. In Proc. of the\n18th International Society for Music Information Re-\ntrieval Conference (ISMIR) , 2017.\n[18] Brian McFee, Matt McVicar, Oriol Nieto, Stefan\nBalke, Carl Thome, Dawen Liang, Eric Battenberg,\nJosh Moore, Rachel Bittner, Ryuichi Yamamoto, et al.\nlibrosa 0.5. 0, 2017.\n[19] Brian CJ Moore. An introduction to the psychology of\nhearing . Brill, 2012.\n[20] Nobutaka Ono, Kenichi Miyamoto, Jonathan Le Roux,\nHirokazu Kameoka, and Shigeki Sagayama. Sep-\naration of a monaural audio signal into har-\nmonic/percussive components by complementary dif-512 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018fusion on spectrogram. In Signal Processing Confer-\nence, 2008 16th European , pages 1–4. IEEE, 2008.\n[21] Jordi Pons, Olga Slizovskaia, Rong Gong, Emilia\nG´omez, and Xavier Serra. Timbre analysis of music\naudio signals with convolutional neural networks. In\nSignal Processing Conference (EUSIPCO), 2017 25th\nEuropean , pages 2744–2748. IEEE, 2017.\n[22] Mathieu Ramona, Ga ¨el Richard, and Bertrand David.\nV ocal detection in music with support vector ma-\nchines. In Acoustics, Speech and Signal Processing,\n2008. ICASSP 2008. IEEE International Conference\non, pages 1885–1888. IEEE, 2008.\n[23] Joseph Redmon and Ali Farhadi. YOLO9000: better,\nfaster, stronger. In 2017 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2017, Hon-\nolulu, HI, USA, July 21-26, 2017 , pages 6517–6525,\n2017.\n[24] Lise Regnier and Geoffroy Peeters. Singing voice de-\ntection in music tracks using direct voice vibrato de-\ntection. In Acoustics, Speech and Signal Processing,\n2009. ICASSP 2009. IEEE International Conference\non, pages 1685–1688. IEEE, 2009.\n[25] Martın Rocamora and Perfecto Herrera. Comparing au-\ndio descriptors for singing voice detection in music au-\ndio ﬁles. In Brazilian symposium on computer music,\n11th. san pablo, brazil , volume 26, page 27, 2007.\n[26] Jan Schl ¨uter. Learning to pinpoint singing voice from\nweakly labeled examples. In Proc. of the 17th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 44–50, 2016.\n[27] Jan Schl ¨uter and Thomas Grill. Exploring data aug-\nmentation for improved singing voice detection with\nneural networks. In Proc. of the 16th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 121–126, 2015.\n[28] Emery Schubert and Joe Wolfe. V oicelikeness of musi-\ncal instruments: A literature review of acoustical, psy-\nchological and expressiveness perspectives. Musicae\nScientiae , 20(2):248–262, 2016.\n[29] Julius Orion Smith. Introduction to digital ﬁlters: with\naudio applications , volume 2. Julius Smith, 2007.\n[30] Renee Timmers and Peter Desain. Vibrato: Questions\nand answers from musicians and science. In Proc. Int.\nConf. on Music Perception and Cognition , volume 2,\n2000.\n[31] Ye Wang, Min-Yen Kan, Tin Lay Nwe, Arun Shenoy,\nand Jun Yin. Lyrically: automatic synchronization of\nacoustic musical signals and textual lyrics. In Proc. of\nthe 12th annual ACM international conference on Mul-\ntimedia , pages 212–219. ACM, 2004.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 513"
    },
    {
        "title": "Skeleton Plays Piano: Online Generation of Pianist Body Movements from MIDI Performance.",
        "author": [
            "Bochen Li",
            "Akira Maezawa",
            "Zhiyao Duan"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492387",
        "url": "https://doi.org/10.5281/zenodo.1492387",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/109_Paper.pdf",
        "abstract": "Generating expressive body movements of a pianist for a given symbolic sequence of key depressions is important for music interaction, but most existing methods cannot incorporate musical context information and generate movements of body joints that are further away from the fingers such as head and shoulders. This paper addresses such limitations by directly training a deep neural network system to map a MIDI note stream and additional metric structures to a skeleton sequence of a pianist playing a keyboard instrument in an online fashion. Experiments show that (a) incorporation of metric information yields in 4% smaller error, (b) the model is capable of learning the motion behavior of a specific player, and (c) no significant difference between the generated and real human movements is observed by human subjects in 75% of the pieces.",
        "zenodo_id": 1492387,
        "dblp_key": "conf/ismir/LiMD18",
        "keywords": [
            "expressive body movements",
            "pianist",
            "symbolic sequence of key depressions",
            "music interaction",
            "musical context information",
            "joint movements",
            "head and shoulders",
            "online fashion",
            "MIDI note stream",
            "metric structures"
        ],
        "content": "SKELETON PLAYS PIANO: ONLINE GENERATION OF PIANIST BODY\nMOVEMENTS FROM MIDI PERFORMANCE\nBochen Li1Akira Maezawa2Zhiyao Duan1\n1University of Rochester, USA2Yamaha Corporation, Japan\nfbochen.li, zhiyao.duan g@rochester.edu, akira.maezawa@music.yamaha.com\nABSTRACT\nGenerating expressive body movements of a pianist for\na given symbolic sequence of key depressions is important\nfor music interaction, but most existing methods cannot in-\ncorporate musical context information and generate move-\nments of body joints that are further away from the ﬁngers\nsuch as head and shoulders. This paper addresses such lim-\nitations by directly training a deep neural network system\nto map a MIDI note stream and additional metric structures\nto a skeleton sequence of a pianist playing a keyboard in-\nstrument in an online fashion. Experiments show that (a)\nincorporation of metric information yields in 4% smaller\nerror, (b) the model is capable of learning the motion be-\nhavior of a speciﬁc player, and (c) no signiﬁcant difference\nbetween the generated and real human movements is ob-\nserved by human subjects in 75% of the pieces.\n1. INTRODUCTION\nMusic performance is a multimodal art form. Visual ex-\npression is critical for conveying musical expression and\nideas to the audience [4,5]. Furthermore, visual expression\nis critical for communicating musical ideas among musi-\ncians in a music ensemble, such as predicting the leader-\nfollower relationship in an ensemble [15].\nDespite the importance of body motion in music perfor-\nmance, much work in automatic music performance gen-\neration has focused on synthesizing expressive audio data\nfrom a corresponding symbolic representation of the music\nperformance (e.g., a MIDI ﬁle). We believe that, however,\nbody motion generation is a critical component that opens\ndoor to multiple applications. For educational purposes,\nfor example, replicating the visual performance character-\nistics of well-known musicians can serve as demonstra-\ntions for instrument beginners to learn from. Musicol-\nogists can apply this framework to analyze the role of\ngesture and motion in music performance and perception.\nFor entertainment purposes, rendering visual performances\nalong with music audio enables a more immersive music\nenjoyment experience as in live concerts. For automatic\nc\rBochen Li, Akira Maezawa, Zhiyao Duan. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Bochen Li, Akira Maezawa, Zhiyao Duan. “Skele-\nton plays piano: online generation of pianist body movements from MIDI\nperformance”, 19th International Society for Music Information Retrieval\nConference, Paris, France, 2018.\n࣮൙࢈๽ӌٰ֖҇൙ܘ\nIPSJ SIG Technical Report\nTime\nInput\nOutputMIDI stream\nPose sequeneTime\nPitch\n+ Metric info.Beat in\nthe bar4 1 3 4 1 3 41 2 23\nऺ1චࠬ൥ʍҩ๗Ɠ⾁ಁС৭ʍз⾁࣮൙ػߢ໑ɪʨƒܤҾΦપʍߢ\nػ໑ʱॲ२ɸʪƓ\n˴˙˽ʱӌࡌɸʪɲʇʆƒ௰ίʍС৭ࠖʊɩɰʪஞݴʍற\n१ʱƒ௰ίʍԪছʊ੆ɶʅ౩ϿɴɺʪɲʇɫʆɬʪƓʝɾƒ\n఻ছۥਚʇɣʂɾ޼ࠬϷஞʊʎ෢ߪ଺ʊ౩Ͽɴʫʉɣ๗য়\nʇƒз⾁࣮൙ʇɣʂɾ޼ࠬϷஞʊ౩Ͽɴʫʪ๗য়ʱ஍܏ɸ\nʪɲʇɫњఉʊʉʪƓ\n2.Ԫໞٰ֖\n࡞๨ƒС৭ʊ੆ɸʪܤҾ࣮൙ʍஞػݴ໑ॲ२ʎƒ޼ࠬʍ\n݈ೀ࣮൙ʱॣตʇɶɾօϷஞӌฆ੠ʇɶʅଜ߲ѓɴʫʅ\nɣʪƓօϷஞӌʊ଼ঔʉॣตʱঙɰʪɲʇʆƒ߭োʉஞݴ\nʱॲ२ɶɾʩ [4]ƒˣƪˏ˜˻ʶˌɴʫɾஞݴʱॲ२ʆɬ\nʪ[5]ƓɶɪɶƒօϷஞӌʴ˩˿ƪ˓ʊʎ 3ʃʍ੝ɬʉѳ\n੠ɫɡʪƓਫ਼φʊƒறଜʍڎऩʊ੆ɸʪஞݴʍற१ʎƒॣ\nตʍঙكʣࠬஞʍˣ˻˳ƪˑ˓˷ƪ˝̅˂ʉʈʊʧʩڇࠄ\nɴʫʅɣɾɾʠƒறଜʍڎऩʍஞݴʱӁமɸʪɲʇɫܪ௟\nʆɡʪɲʇƓਫ਼௡ʊƒॲ२ɴʫʪஞݴʎٴɪʨ޼ঢʝʆʍ\nʞʆɡʩƒச೼ʣࣣ੄ʍإɬʉʈʎ˴˙˽ѓɴʫʅɣʉɣ\nɲʇƓਫ਼ޔʊƒ޼ࠬΦપʊʎ૰খԪؤɶʉɣƒ఻ছۥਚʇ\nɣʂɾёӎ଺ʉഞැ࣮൙ʱࠪʩ௬ʫʪɲʇɫʆɬʉɣɲ\nʇƓС৭ஞݴʇʎёӎ଺ʉഞැʊʡϾ׏ɴʫʪɾʠ [1,2]ƒ\nɲʍʧɥʉഞැ࣮൙ʱࠪʩ௬ʫʪɲʇɫۍʝɶɣƓёӎʍ\nഞැʱகʝɧʅ଼ঔʉз⾁࣮൙ʱॲ२ɸʪฆ੠ঙଜʇɶʅ\nʎС৭೅࣮ೝɰɫɡʪɫ [6]ƒС৭೅࣮ೝɰʆʎஞݴॲ२\nʱ੆ࣛʇɶʅɣʉɪʂɾƓ\n3.ࠬ൥\nචࠬ൥ʆʎƒऩԨɫС৭ɶɾ˦ʴˠС৭ʍз⾁࣮൙ʇࢬ\nছসɪʨؼѷɶɾ఻ॐʍˋ˚˼ƪ˲ʱ௬ອʇɶƒ௬ອʊட\nՎɶɾऩԨʍܤҾ݈ೀʍˋ˚˼ƪ˲ʱφଜʍભМʱؼʅɪ\nʨࡰອɸʪƓ࡞๨ʍ޼ࠬϷஞʍॲ२ࠬ൥ʇʎ੆ࣆ଺ʊƒݟ\nɪʉ޼ࠬϷஞ߭੄ʎ˴˙˽ѓɶʉɣਜ਼ʮʩʊƒӎטʊ܏ʂ\nɾƒ੝ʝɪʉৌतʍС৭ஞݴʱॲ२ɸʪɲʇʱ෾ೀʇɸʪƓ\nܤҾ݈ೀʇɶʅʎƒ˦ʴˠС৭ʊɩɣʅࡥ๗ʇ޻ʮʫ\nʪƒச೼Ɣ࠵ƔຜٴƔຜಲƔຜ࠵ࠬʍ 8Ԫছʍ݈ೀʱ˴\n˙˽ѓɸʪƓ݈ೀʎઅφʍʴ̅˂˽ʆއϾɴʫɾ˦ʴ\nˠС৭ஞѾʊ੆ɸʪ௡݈ٿߣೀʇɸʪƓΤ݈گೀʶ̅\n˙˕ˁˋʱ d∈{1,2=D}ʇɶƒԪছʍʶ̅˙˕ˁˋʱ\nk∈{1,···,8=K}ʇɸʪƓ௬ອʊʎ MIDIʍౙёޮ຿ɪ\nʨமʨʫʪˠƪ˚ಀ܎ʇ˫˿ˉ˘ʵʱ๑ɣʪƓஞݴʍۡओʎφଜʍࡀՎ ∆Tʆۼʮʫƒ τ˧˾ƪ˲ʍભМɫॲɷʪʡ\nʍʇɸʪƓʉɻʉʨʏС৭ʆʎ฽ಡஞݴɫ԰ʝʫʪɾʠƒ\nз⾁࣮൙ʱᳪʂʅஞݴʱॲ२ɸʪ಴๗ɫɡʪɾʠɿƓ\nචࠬ൥ʆʎƒࢊח଺ʉС৭࣮൙ɪʨС৭ʱற૙ೝɰʪʧ\nɥʉକٿߣ˙ƪˑƸ ǄС৭ற૙ສǅʇڐʕƹʇƒ఻ছۥਚʱ\n๗ตɶɾʧɥʉକٿߣ˙ƪˑƸ Ǆ఻ছۥਚற૙ສǅʇڐʕƹ\nʱાࡰɶƒɲʫʨʍற૙ສʍػߢ໑ʊՂʄɣʅܤҾ݈ೀػ\n໑ʱॲ२ɸʪƓற૙ສʍࠬஞঙكʎܪ௟ʆɡʪɲʇʇƒ଼\nঔʉܤҾػߢ໑ʍ˴˙˽ѓɫܪ௟ʆɡʪɲʇɪʨƒ ऺ2ʊ\nߪɸʧɥʉ˝˷ƪ˻˽˟˕˚́ƪˁʱ๑ɣʅƒ˙ƪˑ˛˼\n˨̅ʊற૙ાࡰʣػߢ໑˴˙˽ѓʱۼɥɲʇʱ۵ɧʪƓ\n3.1 CNN ʊʧʪС৭ற૙ສાࡰ\nС৭ற૙ສʱાࡰɸʪɾʠʊƒ˦ʴˠС৭ʍˋ˚˼ƪ\n˲ɪʨƒࡀՎ ∆Tʆ˦ʴˠ˿ƪ˽ Xt,nʱࡰޟɸʪƓ˦ʴ\nˠ˿ƪ˽ʇʎƒܗߢ t∆Tʆё܊ nɫС৭ɴʫʅɣɾߢʊ\nXt,n=1ʇʉʪʧɥʉ˙ƪˑʆɡʪƓߣʊƒҺ˧˾ƪ˲\ntʊɩɣʅƒ˧˾ƪ˲ t−2τɪʨtʝʆʍ˦ʴˠ˿ƪ˽ʱ\n2τ×Nٿߣʍ௡ٿߣѾਔʇٵʉɶƒ௡ৰʍ CNNʇৌ܏ٗ\nৰʍࢇʆ૾ɸɲʇʆƒ˧˾ƪ˲ tʊɩɰʪƒ 50ٿߣʍС৭\nற૙ສʱமʪƓ\nС৭ற૙ສʊʎƒࡀܗߢݥڇ഻ʊɩɰʪ޼ࠬΦપʱߪɶ\nʅɣʪʇ۵ɧʨʫʪƓʉɻʉʨʏƒ CNNʎஞݴॲ२ʊɩ\nɣʅࡥ๗ʉࢊח଺ʉ˧˾ƪˌʇƒɼʍౙॲΦપʱ˴˙˽ѓ\nɸʪɪʨɿƓ\n3.1.1 CNN ʊʧʪ఻ছۥਚற૙ສ\n఻ছۥਚற૙ສʱાࡰɸʪɾʠƒࡀܗߢݥڇ഻ʆʍ఻ছ\nۥਚʱକٿߣ˫ˁ˚˽ʆ೅ɸɲʇʱ۵ɧʪƓɼɲʆƒҺ˧\n˾ƪ˲ tʊ੆ɶʅƒɼʍ˧˾ƪ˲ɫࢬছࣣʍѕ఻෾ʱચɣ\nʅɣʪɪʱ֑ʠƒ 1఻෾ʍ܏࣪ 1ಀ෾ʍ๗য়ƒࢬছসʍ 1\n఻ৈʍ܏࣪ 2ಀ෾ʍ๗য়ƒɼʫΤҤʍ܏࣪ʎ 3ಀ෾ʍ๗য়\nɫ1ʇʉʩƒɼʫΤҤɫ 0ʇʉʪʧɥʉȥٿߣʍ˫ˁ˚\n˽ctʱࡰޟɸʪƓߣʊƒҺ˧˾ƪ˲ tʊɩɣʅƒ˧˾ƪ˲\nt−2τɪʨtʝʆʍ˫ˁ˚˽ʱୋʠɾʡʍʱ 2τ×3ٿߣʍ\n௡ٿߣѾਔʇٵʉɶƒ CNNʇৌ܏ٗৰʱؼʪɲʇʆƒ˧\n˾ƪ˲ tʊɩɰʪƒ 10ٿߣʍ఻ছۥਚற૙ສʱமʪƓ\n3.2 LSTM ʊʧʪܤҾஞݴॲ२\nܤҾஞݴʍॲ२ʍɾʠƒС৭ற૙ສʇ఻ছۥਚற૙ສʱ\n௬ອʇɶɾػߢ໑˴˙˽ʱ۵ɧʪƓஞݴʊɩɣʅʎܤҾΦ\nપʊɩɰʪߢԨࣣ߶ʆʍໞ਩१ɫࡥ๗ʆɡʪɾʠƒɲʫʨ\nʍற૙ສʱ௬ອʇɶɾ 2ৰʍ LSTMʱۥયɸʪƓ LSTM\nʍࡰອ˫ˁ˚˽ʱৌ܏ٗৰʊ฿ɧʪƒ˧˾ƪ˲ tʊɩɰʪ\nԪছkʍ݈ೀ dʍ२ഒ yt,k,dʱமʪƓ\nɲʍʧɥʊҺ˧˾ƪ˲ʊɩɣʅƒ૫ɴ 2τʍ˦ʴˠ˿ƪ\n˽x֊ʒ఻ছ࣮൙ cɪʨƒԪছ݈ೀ yʱࡰອɸʪ˟˕˚\ńƪˁʱƏ y(x,c|θ)ʇ೅ɸƓɲɲʆƒ θʎ˟˕˚́ƪˁʍ\nˣ˻˳ƪˑʆɡʪƓ\nc⃝2018 Information Processing Society of Japan 23 4 1࣮൙࢈๽ӌٰ֖҇൙ܘ\nIPSJ SIG Technical Report\nTime\nInput\nOutputMIDI stream\nPose sequeneTime\nPitch\n+ Metric info.Beat in\nthe bar4 1 3 4 1 3 41 2 23\nऺ1චࠬ൥ʍҩ๗Ɠ⾁ಁС৭ʍз⾁࣮൙ػߢ໑ɪʨƒܤҾΦપʍߢ\nػ໑ʱॲ२ɸʪƓ\n˴˙˽ʱӌࡌɸʪɲʇʆƒ௰ίʍС৭ࠖʊɩɰʪஞݴʍற\n१ʱƒ௰ίʍԪছʊ੆ɶʅ౩ϿɴɺʪɲʇɫʆɬʪƓʝɾƒ\n఻ছۥਚʇɣʂɾ޼ࠬϷஞʊʎ෢ߪ଺ʊ౩Ͽɴʫʉɣ๗য়\nʇƒз⾁࣮൙ʇɣʂɾ޼ࠬϷஞʊ౩Ͽɴʫʪ๗য়ʱ஍܏ɸ\nʪɲʇɫњఉʊʉʪƓ\n2.Ԫໞٰ֖\n࡞๨ƒС৭ʊ੆ɸʪܤҾ࣮൙ʍஞػݴ໑ॲ२ʎƒ޼ࠬʍ\n݈ೀ࣮൙ʱॣตʇɶɾօϷஞӌฆ੠ʇɶʅଜ߲ѓɴʫʅ\nɣʪƓօϷஞӌʊ଼ঔʉॣตʱঙɰʪɲʇʆƒ߭োʉஞݴ\nʱॲ२ɶɾʩ [4]ƒˣƪˏ˜˻ʶˌɴʫɾஞݴʱॲ२ʆɬ\nʪ[5]ƓɶɪɶƒօϷஞӌʴ˩˿ƪ˓ʊʎ 3ʃʍ੝ɬʉѳ\n੠ɫɡʪƓਫ਼φʊƒறଜʍڎऩʊ੆ɸʪஞݴʍற१ʎƒॣ\nตʍঙكʣࠬஞʍˣ˻˳ƪˑ˓˷ƪ˝̅˂ʉʈʊʧʩڇࠄ\nɴʫʅɣɾɾʠƒறଜʍڎऩʍஞݴʱӁமɸʪɲʇɫܪ௟\nʆɡʪɲʇƓਫ਼௡ʊƒॲ२ɴʫʪஞݴʎٴɪʨ޼ঢʝʆʍ\nʞʆɡʩƒச೼ʣࣣ੄ʍإɬʉʈʎ˴˙˽ѓɴʫʅɣʉɣ\nɲʇƓਫ਼ޔʊƒ޼ࠬΦપʊʎ૰খԪؤɶʉɣƒ఻ছۥਚʇ\nɣʂɾёӎ଺ʉഞැ࣮൙ʱࠪʩ௬ʫʪɲʇɫʆɬʉɣɲ\nʇƓС৭ஞݴʇʎёӎ଺ʉഞැʊʡϾ׏ɴʫʪɾʠ [1,2]ƒ\nɲʍʧɥʉഞැ࣮൙ʱࠪʩ௬ʫʪɲʇɫۍʝɶɣƓёӎʍ\nഞැʱகʝɧʅ଼ঔʉз⾁࣮൙ʱॲ२ɸʪฆ੠ঙଜʇɶʅ\nʎС৭೅࣮ೝɰɫɡʪɫ [6]ƒС৭೅࣮ೝɰʆʎஞݴॲ२\nʱ੆ࣛʇɶʅɣʉɪʂɾƓ\n3.ࠬ൥\nචࠬ൥ʆʎƒऩԨɫС৭ɶɾ˦ʴˠС৭ʍз⾁࣮൙ʇࢬ\nছসɪʨؼѷɶɾ఻ॐʍˋ˚˼ƪ˲ʱ௬ອʇɶƒ௬ອʊட\nՎɶɾऩԨʍܤҾ݈ೀʍˋ˚˼ƪ˲ʱφଜʍભМʱؼʅɪ\nʨࡰອɸʪƓ࡞๨ʍ޼ࠬϷஞʍॲ२ࠬ൥ʇʎ੆ࣆ଺ʊƒݟ\nɪʉ޼ࠬϷஞ߭੄ʎ˴˙˽ѓɶʉɣਜ਼ʮʩʊƒӎטʊ܏ʂ\nɾƒ੝ʝɪʉৌतʍС৭ஞݴʱॲ२ɸʪɲʇʱ෾ೀʇɸʪƓ\nܤҾ݈ೀʇɶʅʎƒ˦ʴˠС৭ʊɩɣʅࡥ๗ʇ޻ʮʫ\nʪƒச೼Ɣ࠵ƔຜٴƔຜಲƔຜ࠵ࠬʍ 8Ԫছʍ݈ೀʱ˴\n˙˽ѓɸʪƓ݈ೀʎઅφʍʴ̅˂˽ʆއϾɴʫɾ˦ʴ\nˠС৭ஞѾʊ੆ɸʪ௡݈ٿߣೀʇɸʪƓΤ݈گೀʶ̅\n˙˕ˁˋʱ d∈{1,2=D}ʇɶƒԪছʍʶ̅˙˕ˁˋʱ\nk∈{1,···,8=K}ʇɸʪƓ௬ອʊʎ MIDIʍౙёޮ຿ɪ\nʨமʨʫʪˠƪ˚ಀ܎ʇ˫˿ˉ˘ʵʱ๑ɣʪƓஞݴʍۡओʎφଜʍࡀՎ ∆Tʆۼʮʫƒ τ˧˾ƪ˲ʍભМɫॲɷʪʡ\nʍʇɸʪƓʉɻʉʨʏС৭ʆʎ฽ಡஞݴɫ԰ʝʫʪɾʠƒ\nз⾁࣮൙ʱᳪʂʅஞݴʱॲ२ɸʪ಴๗ɫɡʪɾʠɿƓ\nචࠬ൥ʆʎƒࢊח଺ʉС৭࣮൙ɪʨС৭ʱற૙ೝɰʪʧ\nɥʉକٿߣ˙ƪˑƸ ǄС৭ற૙ສǅʇڐʕƹʇƒ఻ছۥਚʱ\n๗ตɶɾʧɥʉକٿߣ˙ƪˑƸ Ǆ఻ছۥਚற૙ສǅʇڐʕƹ\nʱાࡰɶƒɲʫʨʍற૙ສʍػߢ໑ʊՂʄɣʅܤҾ݈ೀػ\n໑ʱॲ२ɸʪƓற૙ສʍࠬஞঙكʎܪ௟ʆɡʪɲʇʇƒ଼\nঔʉܤҾػߢ໑ʍ˴˙˽ѓɫܪ௟ʆɡʪɲʇɪʨƒ ऺ2ʊ\nߪɸʧɥʉ˝˷ƪ˻˽˟˕˚́ƪˁʱ๑ɣʅƒ˙ƪˑ˛˼\n˨̅ʊற૙ાࡰʣػߢ໑˴˙˽ѓʱۼɥɲʇʱ۵ɧʪƓ\n3.1 CNN ʊʧʪС৭ற૙ສાࡰ\nС৭ற૙ສʱાࡰɸʪɾʠʊƒ˦ʴˠС৭ʍˋ˚˼ƪ\n˲ɪʨƒࡀՎ ∆Tʆ˦ʴˠ˿ƪ˽ Xt,nʱࡰޟɸʪƓ˦ʴ\nˠ˿ƪ˽ʇʎƒܗߢ t∆Tʆё܊ nɫС৭ɴʫʅɣɾߢʊ\nXt,n=1ʇʉʪʧɥʉ˙ƪˑʆɡʪƓߣʊƒҺ˧˾ƪ˲\ntʊɩɣʅƒ˧˾ƪ˲ t−2τɪʨtʝʆʍ˦ʴˠ˿ƪ˽ʱ\n2τ×Nٿߣʍ௡ٿߣѾਔʇٵʉɶƒ௡ৰʍ CNNʇৌ܏ٗ\nৰʍࢇʆ૾ɸɲʇʆƒ˧˾ƪ˲ tʊɩɰʪƒ 50ٿߣʍС৭\nற૙ສʱமʪƓ\nС৭ற૙ສʊʎƒࡀܗߢݥڇ഻ʊɩɰʪ޼ࠬΦપʱߪɶ\nʅɣʪʇ۵ɧʨʫʪƓʉɻʉʨʏƒ CNNʎஞݴॲ२ʊɩ\nɣʅࡥ๗ʉࢊח଺ʉ˧˾ƪˌʇƒɼʍౙॲΦપʱ˴˙˽ѓ\nɸʪɪʨɿƓ\n3.1.1 CNN ʊʧʪ఻ছۥਚற૙ສ\n఻ছۥਚற૙ສʱાࡰɸʪɾʠƒࡀܗߢݥڇ഻ʆʍ఻ছ\nۥਚʱକٿߣ˫ˁ˚˽ʆ೅ɸɲʇʱ۵ɧʪƓɼɲʆƒҺ˧\n˾ƪ˲ tʊ੆ɶʅƒɼʍ˧˾ƪ˲ɫࢬছࣣʍѕ఻෾ʱચɣ\nʅɣʪɪʱ֑ʠƒ 1఻෾ʍ܏࣪ 1ಀ෾ʍ๗য়ƒࢬছসʍ 1\n఻ৈʍ܏࣪ 2ಀ෾ʍ๗য়ƒɼʫΤҤʍ܏࣪ʎ 3ಀ෾ʍ๗য়\nɫ1ʇʉʩƒɼʫΤҤɫ 0ʇʉʪʧɥʉȥٿߣʍ˫ˁ˚\n˽ctʱࡰޟɸʪƓߣʊƒҺ˧˾ƪ˲ tʊɩɣʅƒ˧˾ƪ˲\nt−2τɪʨtʝʆʍ˫ˁ˚˽ʱୋʠɾʡʍʱ 2τ×3ٿߣʍ\n௡ٿߣѾਔʇٵʉɶƒ CNNʇৌ܏ٗৰʱؼʪɲʇʆƒ˧\n˾ƪ˲ tʊɩɰʪƒ 10ٿߣʍ఻ছۥਚற૙ສʱமʪƓ\n3.2 LSTM ʊʧʪܤҾஞݴॲ२\nܤҾஞݴʍॲ२ʍɾʠƒС৭ற૙ສʇ఻ছۥਚற૙ສʱ\n௬ອʇɶɾػߢ໑˴˙˽ʱ۵ɧʪƓஞݴʊɩɣʅʎܤҾΦ\nપʊɩɰʪߢԨࣣ߶ʆʍໞ਩१ɫࡥ๗ʆɡʪɾʠƒɲʫʨ\nʍற૙ສʱ௬ອʇɶɾ 2ৰʍ LSTMʱۥયɸʪƓ LSTM\nʍࡰອ˫ˁ˚˽ʱৌ܏ٗৰʊ฿ɧʪƒ˧˾ƪ˲ tʊɩɰʪ\nԪছkʍ݈ೀ dʍ२ഒ yt,k,dʱமʪƓ\nɲʍʧɥʊҺ˧˾ƪ˲ʊɩɣʅƒ૫ɴ 2τʍ˦ʴˠ˿ƪ\n˽x֊ʒ఻ছ࣮൙ cɪʨƒԪছ݈ೀ yʱࡰອɸʪ˟˕˚\ńƪˁʱƏ y(x,c|θ)ʇ೅ɸƓɲɲʆƒ θʎ˟˕˚́ƪˁʍ\nˣ˻˳ƪˑʆɡʪƓ\nc⃝2018 Information Processing Society of Japan 23 4 1 2࣮൙࢈๽ӌٰ֖҇൙ܘ\nIPSJ SIG Technical Report\nTime\nInput\nOutputMIDI stream\nPose sequeneTime\nPitch\n+ Metric info.Beat in\nthe bar4 1 3 4 1 3 41 2 23\nऺ1චࠬ൥ʍҩ๗Ɠ⾁ಁС৭ʍз⾁࣮൙ػߢ໑ɪʨƒܤҾΦપʍߢ\nػ໑ʱॲ२ɸʪƓ\n˴˙˽ʱӌࡌɸʪɲʇʆƒ௰ίʍС৭ࠖʊɩɰʪஞݴʍற\n१ʱƒ௰ίʍԪছʊ੆ɶʅ౩ϿɴɺʪɲʇɫʆɬʪƓʝɾƒ\n఻ছۥਚʇɣʂɾ޼ࠬϷஞʊʎ෢ߪ଺ʊ౩Ͽɴʫʉɣ๗য়\nʇƒз⾁࣮൙ʇɣʂɾ޼ࠬϷஞʊ౩Ͽɴʫʪ๗য়ʱ஍܏ɸ\nʪɲʇɫњఉʊʉʪƓ\n2.Ԫໞٰ֖\n࡞๨ƒС৭ʊ੆ɸʪܤҾ࣮൙ʍஞػݴ໑ॲ२ʎƒ޼ࠬʍ\n݈ೀ࣮൙ʱॣตʇɶɾօϷஞӌฆ੠ʇɶʅଜ߲ѓɴʫʅ\nɣʪƓօϷஞӌʊ଼ঔʉॣตʱঙɰʪɲʇʆƒ߭োʉஞݴ\nʱॲ२ɶɾʩ [4]ƒˣƪˏ˜˻ʶˌɴʫɾஞݴʱॲ२ʆɬ\nʪ[5]ƓɶɪɶƒօϷஞӌʴ˩˿ƪ˓ʊʎ 3ʃʍ੝ɬʉѳ\n੠ɫɡʪƓਫ਼φʊƒறଜʍڎऩʊ੆ɸʪஞݴʍற१ʎƒॣ\nตʍঙكʣࠬஞʍˣ˻˳ƪˑ˓˷ƪ˝̅˂ʉʈʊʧʩڇࠄ\nɴʫʅɣɾɾʠƒறଜʍڎऩʍஞݴʱӁமɸʪɲʇɫܪ௟\nʆɡʪɲʇƓਫ਼௡ʊƒॲ२ɴʫʪஞݴʎٴɪʨ޼ঢʝʆʍ\nʞʆɡʩƒச೼ʣࣣ੄ʍإɬʉʈʎ˴˙˽ѓɴʫʅɣʉɣ\nɲʇƓਫ਼ޔʊƒ޼ࠬΦપʊʎ૰খԪؤɶʉɣƒ఻ছۥਚʇ\nɣʂɾёӎ଺ʉഞැ࣮൙ʱࠪʩ௬ʫʪɲʇɫʆɬʉɣɲ\nʇƓС৭ஞݴʇʎёӎ଺ʉഞැʊʡϾ׏ɴʫʪɾʠ [1,2]ƒ\nɲʍʧɥʉഞැ࣮൙ʱࠪʩ௬ʫʪɲʇɫۍʝɶɣƓёӎʍ\nഞැʱகʝɧʅ଼ঔʉз⾁࣮൙ʱॲ२ɸʪฆ੠ঙଜʇɶʅ\nʎС৭೅࣮ೝɰɫɡʪɫ [6]ƒС৭೅࣮ೝɰʆʎஞݴॲ२\nʱ੆ࣛʇɶʅɣʉɪʂɾƓ\n3.ࠬ൥\nචࠬ൥ʆʎƒऩԨɫС৭ɶɾ˦ʴˠС৭ʍз⾁࣮൙ʇࢬ\nছসɪʨؼѷɶɾ఻ॐʍˋ˚˼ƪ˲ʱ௬ອʇɶƒ௬ອʊட\nՎɶɾऩԨʍܤҾ݈ೀʍˋ˚˼ƪ˲ʱφଜʍભМʱؼʅɪ\nʨࡰອɸʪƓ࡞๨ʍ޼ࠬϷஞʍॲ२ࠬ൥ʇʎ੆ࣆ଺ʊƒݟ\nɪʉ޼ࠬϷஞ߭੄ʎ˴˙˽ѓɶʉɣਜ਼ʮʩʊƒӎטʊ܏ʂ\nɾƒ੝ʝɪʉৌतʍС৭ஞݴʱॲ२ɸʪɲʇʱ෾ೀʇɸʪƓ\nܤҾ݈ೀʇɶʅʎƒ˦ʴˠС৭ʊɩɣʅࡥ๗ʇ޻ʮʫ\nʪƒச೼Ɣ࠵ƔຜٴƔຜಲƔຜ࠵ࠬʍ 8Ԫছʍ݈ೀʱ˴\n˙˽ѓɸʪƓ݈ೀʎઅφʍʴ̅˂˽ʆއϾɴʫɾ˦ʴ\nˠС৭ஞѾʊ੆ɸʪ௡݈ٿߣೀʇɸʪƓΤ݈گೀʶ̅\n˙˕ˁˋʱ d∈{1,2=D}ʇɶƒԪছʍʶ̅˙˕ˁˋʱ\nk∈{1,···,8=K}ʇɸʪƓ௬ອʊʎ MIDIʍౙёޮ຿ɪ\nʨமʨʫʪˠƪ˚ಀ܎ʇ˫˿ˉ˘ʵʱ๑ɣʪƓஞݴʍۡओʎφଜʍࡀՎ ∆Tʆۼʮʫƒ τ˧˾ƪ˲ʍભМɫॲɷʪʡ\nʍʇɸʪƓʉɻʉʨʏС৭ʆʎ฽ಡஞݴɫ԰ʝʫʪɾʠƒ\nз⾁࣮൙ʱᳪʂʅஞݴʱॲ२ɸʪ಴๗ɫɡʪɾʠɿƓ\nචࠬ൥ʆʎƒࢊח଺ʉС৭࣮൙ɪʨС৭ʱற૙ೝɰʪʧ\nɥʉକٿߣ˙ƪˑƸ ǄС৭ற૙ສǅʇڐʕƹʇƒ఻ছۥਚʱ\n๗ตɶɾʧɥʉକٿߣ˙ƪˑƸ Ǆ఻ছۥਚற૙ສǅʇڐʕƹ\nʱાࡰɶƒɲʫʨʍற૙ສʍػߢ໑ʊՂʄɣʅܤҾ݈ೀػ\n໑ʱॲ२ɸʪƓற૙ສʍࠬஞঙكʎܪ௟ʆɡʪɲʇʇƒ଼\nঔʉܤҾػߢ໑ʍ˴˙˽ѓɫܪ௟ʆɡʪɲʇɪʨƒ ऺ2ʊ\nߪɸʧɥʉ˝˷ƪ˻˽˟˕˚́ƪˁʱ๑ɣʅƒ˙ƪˑ˛˼\n˨̅ʊற૙ાࡰʣػߢ໑˴˙˽ѓʱۼɥɲʇʱ۵ɧʪƓ\n3.1 CNN ʊʧʪС৭ற૙ສાࡰ\nС৭ற૙ສʱાࡰɸʪɾʠʊƒ˦ʴˠС৭ʍˋ˚˼ƪ\n˲ɪʨƒࡀՎ ∆Tʆ˦ʴˠ˿ƪ˽ Xt,nʱࡰޟɸʪƓ˦ʴ\nˠ˿ƪ˽ʇʎƒܗߢ t∆Tʆё܊ nɫС৭ɴʫʅɣɾߢʊ\nXt,n=1ʇʉʪʧɥʉ˙ƪˑʆɡʪƓߣʊƒҺ˧˾ƪ˲\ntʊɩɣʅƒ˧˾ƪ˲ t−2τɪʨtʝʆʍ˦ʴˠ˿ƪ˽ʱ\n2τ×Nٿߣʍ௡ٿߣѾਔʇٵʉɶƒ௡ৰʍ CNNʇৌ܏ٗ\nৰʍࢇʆ૾ɸɲʇʆƒ˧˾ƪ˲ tʊɩɰʪƒ 50ٿߣʍС৭\nற૙ສʱமʪƓ\nС৭ற૙ສʊʎƒࡀܗߢݥڇ഻ʊɩɰʪ޼ࠬΦપʱߪɶ\nʅɣʪʇ۵ɧʨʫʪƓʉɻʉʨʏƒ CNNʎஞݴॲ२ʊɩ\nɣʅࡥ๗ʉࢊח଺ʉ˧˾ƪˌʇƒɼʍౙॲΦપʱ˴˙˽ѓ\nɸʪɪʨɿƓ\n3.1.1 CNN ʊʧʪ఻ছۥਚற૙ສ\n఻ছۥਚற૙ສʱાࡰɸʪɾʠƒࡀܗߢݥڇ഻ʆʍ఻ছ\nۥਚʱକٿߣ˫ˁ˚˽ʆ೅ɸɲʇʱ۵ɧʪƓɼɲʆƒҺ˧\n˾ƪ˲ tʊ੆ɶʅƒɼʍ˧˾ƪ˲ɫࢬছࣣʍѕ఻෾ʱચɣ\nʅɣʪɪʱ֑ʠƒ 1఻෾ʍ܏࣪ 1ಀ෾ʍ๗য়ƒࢬছসʍ 1\n఻ৈʍ܏࣪ 2ಀ෾ʍ๗য়ƒɼʫΤҤʍ܏࣪ʎ 3ಀ෾ʍ๗য়\nɫ1ʇʉʩƒɼʫΤҤɫ 0ʇʉʪʧɥʉȥٿߣʍ˫ˁ˚\n˽ctʱࡰޟɸʪƓߣʊƒҺ˧˾ƪ˲ tʊɩɣʅƒ˧˾ƪ˲\nt−2τɪʨtʝʆʍ˫ˁ˚˽ʱୋʠɾʡʍʱ 2τ×3ٿߣʍ\n௡ٿߣѾਔʇٵʉɶƒ CNNʇৌ܏ٗৰʱؼʪɲʇʆƒ˧\n˾ƪ˲ tʊɩɰʪƒ 10ٿߣʍ఻ছۥਚற૙ສʱமʪƓ\n3.2 LSTM ʊʧʪܤҾஞݴॲ२\nܤҾஞݴʍॲ२ʍɾʠƒС৭ற૙ສʇ఻ছۥਚற૙ສʱ\n௬ອʇɶɾػߢ໑˴˙˽ʱ۵ɧʪƓஞݴʊɩɣʅʎܤҾΦ\nપʊɩɰʪߢԨࣣ߶ʆʍໞ਩१ɫࡥ๗ʆɡʪɾʠƒɲʫʨ\nʍற૙ສʱ௬ອʇɶɾ 2ৰʍ LSTMʱۥયɸʪƓ LSTM\nʍࡰອ˫ˁ˚˽ʱৌ܏ٗৰʊ฿ɧʪƒ˧˾ƪ˲ tʊɩɰʪ\nԪছkʍ݈ೀ dʍ२ഒ yt,k,dʱமʪƓ\nɲʍʧɥʊҺ˧˾ƪ˲ʊɩɣʅƒ૫ɴ 2τʍ˦ʴˠ˿ƪ\n˽x֊ʒ఻ছ࣮൙ cɪʨƒԪছ݈ೀ yʱࡰອɸʪ˟˕˚\ńƪˁʱƏ y(x,c|θ)ʇ೅ɸƓɲɲʆƒ θʎ˟˕˚́ƪˁʍ\nˣ˻˳ƪˑʆɡʪƓ\nc⃝2018 Information Processing Society of Japan 24 1 2 3Input\nOutputpitch\ntimebeatTime\nFigure 1 . Outline of the proposed system. It generates\nexpressive body movements as skeleton sequences like hu-\nman playing on a keyboard instrument, given the input of\nMIDI note stream and metric structure information.\naccompaniment systems, appropriate body movements of\nmachine musicians provide visual cues for human musi-\ncians to coordinate with, leading to more effective human-\ncomputer interaction in music performance settings.\nFor generating visual music performance, i.e., body po-\nsition and motion data of a musician, it is important to\ncreate an expressive and natural movement of the whole\nbody in an online fashion. To consider both expressiveness\nand naturalness, the challenge is to maintain some com-\nmon principles in music performance constrained by the\nmusical context being played. Most previous work for-\nmulates it as an inverse kinematics problem with physi-\ncal constraints, where the generated visual performance is\nlimited to hand shapes and ﬁnger positions. Unfortunately,\nthis kind of formulation fails to address the two challenges;\nspeciﬁcally, (1) it fails to generate the whole body move-\nments that are relevant to music expression, such as the\nhead and body tilt, and (2) it fails to take into account the\nmusical context constraints for generation, which do not\ncontribute to ergonomics.\nTherefore, we propose a body movement generation\nsystem as outlined in Figure 1. The input is a real-time\nMIDI note stream and a metric structure , without any addi-\ntional indication of phrase structures or expression marks.\nThe MIDI note stream provides the music characteristics\nand the artistic interpretations, such as note occurrence,\nspeed, and dynamics. The metric structure indicates bar-\nlines and beat positions as auxiliary information. Given\nthese the system can automatically generate expressive and\nnatural body movements from any performance data in the218MIDI format. We design two Convolutional Neural Net-\nworks (CNN) to parse the two inputs and then feed the ex-\ntracted feature representations to a Long Short-Term Mem-\nory (LSTM) network to generate proper body movements.\nThe generated body movements are represented as a se-\nquence of positions of the upper body joints1. The two\ncomplementary inputs serve to maintain a correct hand po-\nsition on the keyboard while conveying musical ideas in\nthe upper body movements. To learn a natural movement,\nwe employ a two-stage training strategy, where the model\nis trained to learn the joint positions ﬁrst, then later trained\nto also learn the body limb lengths.\n2. RELATED WORK\nThere has been work on cross-modal generation, mostly\nfor speech signals tracing back to the 1990s [1], where a\nperson’s lips shown in video frames are warped to match\nthe given phoneme sequence. Given the speech audio, sim-\nilar work focuses on synthesizing photo-realistic lip move-\nments [14], or landmarks of the whole face [6]. Some other\nwork focuses on the generation of dancers’ body move-\nments [9, 12] and behaviors of animated actors [11].\nSimilar problem settings for music performances have\nbeen rarely studied. When the visual modality is available,\nthe system proposed in [8] explores the correlation be-\ntween the MIDI score and visual actions, and is able to tar-\nget the speciﬁc player in an ensemble for any given track.\nPurely from the audio modality, Chen et al. [3] propose to\ngenerate images of different instrumentalists in response to\ndifferent timbres using cross-modal Generative Adversar-\nial Networks (GAN). Regarding the generation of videos,\nrelated work generates hand and ﬁnger movements of a\nkeyboard player from an MIDI input [17] through inverse\nkinematics with appropriate constraints. All of the above-\nmentioned works, however, do not model musicians’ cre-\native body behavior in expressive music performances.\nGiven the original MIDI score, Widmer et al. [16]\npropose to predict three expressive dimensions (timing,\ndynamics, and articulations) on each note event using a\nBayesian model trained on a corpus of human interpreta-\ntions of piano performances. It further gives a comprehen-\nsive analysis of computer’s creative ability in generating\nexpressive music performances, and proves that certain as-\npects of personal styles are identiﬁable and even learnable\nfrom MIDI performances. Regarding to the expressive per-\nformance generation in visual modality, Shlizerman et al.\n[13] propose to generate expressive body skeleton move-\nments and adapt them into textured characters for pianists\nand violinists. Different from our proposed work, they\ntake the input of audio waveforms rather than MIDI perfor-\nmances. We argue that MIDI data is a more scalable for-\nmat to carry context information, regardless of recording\nconditions and piano acoustic characteristics. And most of\npiano pieces have the sheet music in MIDI format, which\ncan be aligned with a waveform recording.\n1We do not generate lower body movements as they are often paid less\nattention by the audience.\nLSTMCNNCNN50d10d\nMIDI Note StreamMetric StructureOutput\nInputtimetimepitchbeatBody SkeletonFigure 2 . The proposed network structure.\n3. METHOD\nThe goal of our method is to generate a time sequence of\nbody joint coordinates, given a live data stream of note\nevents from the performer’s actions on the keyboard (MIDI\nnote stream), and synchronized metric information. We\nseek to create the motion at 30 frames-per-second (FPS),\na reasonable frame-rate to ensure a perceptually smooth\nmotion. In this section, we introduce the technical details\nof the proposed method, including the network design and\ntraining conditions. We ﬁrst use two CNN structures to\nparse the raw input of the MIDI note stream and the metric\nstructure, and feed the extracted feature representations to\nan LSTM network to generate the body movements, as a\nsequence of upper-body joint coordinates forming a skele-\nton. The network structure is shown in Figure 2.\n3.1 Feature Extraction by CNN\nIn contrast to traditional methods, our goal is to model ex-\npressive body movements that are associated with the key-\nboard performance. In this sense, the system should be\naware of the general phrases and the metric structure in\naddition to each individual note event. Instead of design-\ning hand-crafted features, we use CNNs to extract features\nfrom the raw input of the MIDI note stream and the metric\nstructure, respectively.\n3.1.1 MIDI Note Stream\nWe convert the MIDI note stream into a series of two-\ndimensional representations known as the piano-roll ma-\ntrix, and for each of them extract a feature vector \u001exas the\npiano-roll feature .\nTo prepare the piano roll, the MIDI note stream input is\nsampled at 30 frames-per-second (FPS) to match the target\nframe rate. This quantizes the time resolution into the unit\nof 33 ms, as a video frame. Then for each time frame t\nwe deﬁne a binary piano-roll matrix X2R128\u00022\u001c, where\nelement (m;n)is 1 if there is a key depression action at\npitchm(in MIDI note number) and frame t\u0000\u001c+n\u00001,\nand 0 otherwise. We set \u001c= 30 . The key depression tim-\ning is quantized to the closest unit boundary. Note that the\nsliding window covers both past \u001cframes and future \u001c\u00001\nframes, and the note onset interval in Xcaptures enoughProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 219Convolutional Layer + BatchnormMax-poolingConvolutional Layer + BatchnormMax-poolingFully-connected Layer\n6012850d\nkernel size: [5, 5]channel: 10kernel size: [5, 5]channel: 20pool size: [3, 2]pool size: [2, 2]layer size: 50\nConvolutional Layer + Batchnorm603Max-poolingFully-connected Layer10d\nkernel size: [3, 4]channel: 20pool size: [3, 1]layer size: 10Piano-roll Feature !xMetric Feature !c\nPiano-roll Matrix XMetric Matrix C(a)\nConvolutional Layer + BatchnormMax-poolingConvolutional Layer + BatchnormMax-poolingFully-connected Layer\n6012850d\nkernel size: [5, 5]channel: 10kernel size: [5, 5]channel: 20pool size: [3, 2]pool size: [2, 2]layer size: 50\nConvolutional Layer + Batchnorm603Max-poolingFully-connected Layer10d\nkernel size: [3, 4]channel: 20pool size: [3, 1]layer size: 10Piano-roll Feature !xMetric Feature !c\nPiano-roll Matrix XMetric Matrix C (b)\nFigure 3 . The CNN structures and parameters for feature\nextraction from the (a) MIDI note stream and (b) metric\nstructure information.\ninformation for motion generation to “schedule” its tim-\ning. Looking into the future is necessary for the generation\nof proper body movements, which is also true for human\nmusicians: to express natural and expressive body move-\nments, a human musician should either look ahead on the\nsheet music, or be acquainted with it beforehand. Later in\nSection 3.2 we will introduce in which cases we can avoid\nthe potential delays in real-time applications.\nWe then use a CNN to extract features from the binary\npiano-roll matrix X, as CNNs are capable of capturing lo-\ncal context information. The design of our CNN struc-\nture is illustrated in Figure 3.a. The input is the piano-\nroll matrix Xand the output is a 50-d feature vector \u001exas\nthe piano-roll feature. There are two convolutional layers\nfollowed by max-pooling layers, and we use leaky recti-\nﬁed linear units (ReLU) for activations. The kernel spans\n5 semitones and 5 time steps, assuming that the whole\nbody movement is not sensitive to detailed note occur-\nrence. Overall, it is thought that in addition to generating\nexpressive body movements, the MIDI note stream con-\nstrains the hand positions on the keyboard.\n3.1.2 Metric Structure\nSince the body movements are likely to correlate with the\nmusical beats, we also input the metric structure to the pro-\nposed system to obtain another feature vector. This metric\nstructure indexes beats within each measure, which is not\nencoded in the MIDI note stream. The metric structure can\nbe obtained by aligning the live MIDI note stream with\nthe corresponding symbolic music score with explicitly-\nannotated beat indices and downbeat positions.\nSimilar to the MIDI note stream feature, we sample\nthem with the same FPS and window length, and, at each\nframet, deﬁne the metric information as a binary metric\nmatrix C2RM\u00022\u001c, withM= 3. Here, element (m;n)\nis a one-hot encoding of the metric information at frame\nt\u0000\u001c+n\u00001, where the three rows correspond to down-\nFully-connected LayerLSTM Layerlayer size: 50layer size: 16LSTM Layerlayer size: 12816d\n60dBody Joint Coordinates y\nPiano-roll Feature !xMetric Feature !cFigure 4 . The LSTM network structure for body move-\nment generation.\nbeats, pick-up beats, and other positions, respectively. We\nthen build another CNN to parse the metric matrix Cand\nobtain a 10-d output vector \u001ecas the metric feature , as il-\nlustrated in Figure 3.b.\n3.2 Skeleton Movement Generation by LSTM\nTo generate the skeleton sequence, we apply the LSTM\nnetwork, which is capable of preserving the temporal co-\nherence of the output skeleton sequence while learning\nthe pose characteristics associated with the MIDI input.\nThe input to the LSTM is a concatenation of the piano-\nroll feature \u001exand the metric feature \u001ec, and the out-\nput is the normalized coordinates of the body joints y.\nSince musical expression of a human pianist is mainly re-\nﬂected through upper body movements, we model the x-\nandy- visual coordinates of Kjoints in the upper body as\ny=hy1;y2;\u0001\u0001\u0001;y2Ki, whereKis 8 in this work, corre-\nsponding to nose, neck, both shoulders, both elbows, and\nboth wrists. The ﬁrst Kindices denote the x-coordinates\nand the remaining denote the y-coordinates. Note that\nall the coordinate data in y, for each piece, are shifted\nsuch that the average centroid is at the origin, and scaled\nisotropically such that the average variance along x- and\ny-axis sums to 1. The network structure is illustrated in\nFigure 4. It has two LSTM layers, and the output layer\nis fully-connected to get the 16-d vector approximating y\nfor the current frame. The output skeleton coordinates are\ntemporally smoothed using a 5-frame moving window. We\ndenote the predicted body joint coordinates, given X,C\nand network parameters \u0012, as^ y(X;Cj\u0012).\nSince the LSTM is unidirectional, the system is capable\nof generating motion data in an online manner, with a la-\ntency of 30 frames (i.e., 1 second). However, feeding the\npre-existing reference music score (after aligned to the live\nMIDI note stream online) to the system enables an antic-\nipation mechanism like human musicians, which makes it\napplicable in real-time scenarios without the delay.\n3.3 Training Condition\nTo train the model, we minimize, over \u0012, the sum of a loss\nfunctionJ(y;C;X;\u0012)evaluated over the entire training\ndataset. The loss function expresses a measure of discrep-\nancy between the predicted body joint coordinates ^ yand220 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Joint ConstraintLimb Constraint(a)\nJoint ConstraintLimb Constraint (b)\nFigure 5 . The two constraints applied during training.\nthe ground-truth coordinates y.\nWe use different loss functions during the course of\ntraining. In the ﬁrst 30 epochs, we simply minimize the\nManhattan distance between the estimated and the ground-\ntruth body joint coordinates with weight decay:\nJ(y;C;X;\u0012) =X\nkj^yk(X;Cj\u0012)\u0000ykj+\fk\u0012k2;(1)\nwherekis the index for the body joints and \f= 10\u00008is a\nweight parameter. We call this kind of loss the body joint\nconstraint (see Figure 5.a). After 30 training epochs, we\nadd another loss to ensures that not only the coordinates are\ncorrect but also consistent with the expected limb lengths:\nJ(y;C;X;\u0012) =X\nkj^yk(X;Cj\u0012)\u0000ykj\n+X\n(i;j)2Ej^zij(X;Cj\u0012)\u0000zijj+\fk\u0012k2;(2)\nwherezij= (yi\u0000yj)+(yK+i\u0000yK+j)is the displacement\nbetween two joints iandjon a limb (e.g., elbow-wrist),\nE=f(i;j)gis the set of possible limb connections (i;j)\nof a human body. We call the added term the body limb\nconstraint (see Figure 5.b). This is similar to the geometric\nconstraint as described in [10]. There are 7 limb connec-\ntions in total, given the 8 upper body joints. We then train\nanother 120 epochs using the limb constraint. We use the\nAdam [7] optimizer, which is a stochastic gradient descent\nmethod, to minimize the loss function.\nHere we propose to combine the two kinds of con-\nstraints in our training epochs. The body limb constraints\nare important because the loss of joint positions are min-\nimized independently of each other in the body joint con-\nstraint. Figure 6 demonstrates several generated skeleton\nsamples on the normalized plane, where the limb con-\nstraint is not applied in the following 120 epochs. Limb\nconstraint adds dependencies between the loss among dif-\nferent joints, encouraging the model to learn a natural\nmovement that considers the consistency of limb lengths.\nWe only use this constraint at later epochs, however, be-\ncause the body joint constraint is an easier optimization\nproblem; if we optimize with body limb constraints from\nthe very beginning, the training sometimes fails and re-\nmains a state of what seems a local optima, perhaps be-\ncause the loss function wants to minimize the body joint\nerrors but the gradient must pass through regions where the\n0123 -1 -2 -30123\n-1\n-2\n-3\n0123 -1 -2 -30123\n-1\n-2\n-3\n0123 -1 -2 -30123\n-1\n-2\n-3\n0123 -1 -2 -30123\n-1\n-2\n-3Figure 6 . Several generated unnatural skeleton samples\nwithout the limb constraint.\nlimb constraint increases. In this case, the arrangements of\nthe body joints tend to be arbitrary and not ergonomically\nreasonable.\n4. EXPERIMENTS\nWe perform objective evaluations to measure the accuracy\nof the generated movements, and subjective evaluations to\nrate their expressiveness and naturalness.\n4.1 Dataset\nAs there is no existing dataset for the proposed task, we\nrecorded a new audio-visual piano performance dataset\nwith synchronized MIDI stream information on a MIDI\nkeyboard. The dataset contains a total of 74 performance\nrecordings (3 hours and 8 minutes) of 16 different tracks\n(8 piano duets) played by two pianists, one male and one\nfemale. The two players were respectively assigned the\nprimo and the secondo parts of 8 piano duets. Each player\nthen played the 8 tracks multiple times (1-7 times) to ren-\nder different expressive styles, e.g., normal, exaggerated,\netc. At each time the primo and secondo are recorded to-\ngether to ensure enough visual expressiveness on the play-\ners for interactions. The key depression information (pitch\n, timing, and velocity) is automatically encoded into the\nMIDI format by the MIDI keyboard. For each record-\ning, the quantized beat number and the downbeat positions\nwere annotated by semi-automatically aligning the MIDI\nstream and the corresponding MIDI score data. The cam-\nera was placed on the left-front side of the player and the\nperspective was ﬁxed throughout all of the performances.\nThe video frame rate was 30 FPS. The 2D skeleton coordi-\nnates were extracted from the video using a method based\non OpenPose [2]. The video stream and the MIDI stream\nof each recording were manually time-shifted to align with\nthe key depression actions. Note that we extract the 2D\nbody skeleton data purely from computer vision techniques\ninstead of capturing 3D data using motion sensors, which\nmakes it possible to use the massive online video record-\nings of great pianists (e.g., Lang Lang) to train the system.\n4.2 Objective Evaluations\nWe conduct two experiments to assess our method. Since\nthere is no similar previous work to model the players’\nwhole body pose from MIDI input, we set different experi-\nmental conditions for the proposed model as baselines and\ncompare them. First, we investigate the effect of incor-\nporating the metric structure information, which is likely\nto be relevant for expressive motion generation but does\nnot directly affect the players’ key depression actions onProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 221the keyboard. Second, we compare the performance of the\nnetwork when training on a speciﬁc player versus training\non multiple players. To numerically evaluate the quality of\nthe system output, we use the mean absolute error (MAE)\nbetween the generated and the ground-truth skeleton coor-\ndinates at each frame.\n4.2.1 Effectiveness of the Metric Structure\nThe system takes as the inputs the MIDI note stream and\nthe metric information. Here we investigate if the latter\none can help in the motion generation process, by setting a\nbaseline system that takes the MIDI note stream as the in-\nput, ignoring the metric structure by ﬁxing \u001ecto 0. We\nevaluate the MAE of the two models, using piece-wise\nleave-one-out testing over all the 16 tracks.\nResults show that adding the metric structure informa-\ntion into the network can decrease the MAE from 0.180\nto0.173 . The unit is in the scale of the normalized plane,\nwhere the length of an arm-wrist limb is around 1.2 (see\nFigure 6). The result is signiﬁcant because it not only\ndemonstrates that our proposed method can effectively\nmodel the metric structure, but also that features that are\nnot indirectly related to physical placement of the hand\ndoes have an effect on expressive body movements. Al-\nthough our dataset for evaluation is small, we argue that\noverﬁt should not exist since the pieces are quite different.\nOn the other hand, we also observe that even without the\nmetric structure information, the system output is still rea-\nsonable by learning the music context from the MIDI note\nstream. This setting broadens the use scenarios of the pro-\nposed system, such as when the MIDI note stream is from\nan improvised performance without corresponding metric\nstructure information. Nevertheless, including a reference\nmusic score is beneﬁcial for the system not only because\nit improves the MAE measure, but it also enables an antic-\nipation mechanism to favor real-time generation without\npotential delays.\n4.2.2 Training on A Speciﬁc Player\nIn this experiment, we evaluate the model’s performance\nwhen ﬁxing the same player for training and testing. Now\nthe experiments are carried out on the two players sepa-\nrately. We ﬁrst divide the dataset into two subsets, each\nobtaining the 8 different tracks performed by the two play-\ners respectively. On each subset we use the leave-one-out\ntesting for the 8 tracks and calculate the MAE between the\ngenerated and ground-truth coordinates of body skeletons.\nThe average of the MAE from the two subsets is 0.170 .\nComparing the MAE of 0:173 in Section 4.2.1 and the\nMAE of 0:170in this experiment, we see that training on\na generic model only on a target player is slightly better\nthan training over different players. This slight improve-\nment may not be statistically signiﬁcant. The marginal dif-\nference also suggests that even when trained on multiple\nplayers as in Section 4.2.1, the system is capable of re-\nmembering the motion characteristic of each player.\nFigure 7 . One sample frame of the assembled video for\nsubjective evaluation.\n4.3 Subjective Evaluation\nAlthough the objective evaluation using MAE reﬂects\nthe system’s capability of reproducing the players’ body\nmovements on a new MIDI performance stream, this mea-\nsure is still limited. There can be multiple creative ways\non body motions to expressively interpret the same music,\nand the ground-truth body motion is just one possibility.\nIn addition, from MAE we cannot infer the naturalness of\nthe generated body movements, which is even more impor-\ntant than simply learning to reproduce the motion. In this\nsection, we conduct subjective tests to evaluate the qual-\nity of the generated body movements, addressing both ex-\npressiveness and naturalness. The strategy is to mix the\nground-truth body movements with the generated ones and\nlet the testers to tell if each sample is real (ground-truth\nfrom human) or fake (generated).\n4.3.1 Arrangements\nIn the subjective evaluation, we mix the two players to-\ngether and cross-validate on the 16 tracks, as in Sec-\ntion 4.2.1. Here we do not add the metric structure input\nbecause positive feedbacks on the generation results purely\nfrom the keyboard actions will promise broader use cases\nof the system, i.e., improvised performance without a ref-\nerence music score.\nFrom the generated skeleton coordinates, we recover\nthem to the original pixel positions on real video frames us-\ning the same scaling factor when normalizing the ground-\ntruth skeleton before training. Then we generate an anima-\ntion showing body joints as circles and limb connections as\nstraight lines on the background environment image taken\nby the camera from the same perspective. In the same\ngenerated video, we also render a dynamic piano-roll that\ncovers a rolling 5-second segment around the current time\nframe together with the synthesized audio. For a fair com-\nparison, instead of using the original video recordings of\nreal human performances, we generate human body skele-\ntons by repeating the same process using the ground-truth\nskeletal data. Figure 7 shows one sample frame of the as-\nsembled video as a visualization.\nWe arrange 16 pairs of the generated and ground-truth\nskeleton motions on all the 16 tracks, and randomly crop\na 10-second excerpt from each one (excluding several\nchunks containing long silence parts or page turning mo-\ntions). This results in 32 video excerpts. We shufﬂe the 32\nexcerpts before showing them to subjects for evaluation.\nWe recruit 18 subjects from Yamaha employees, who\nare in their 20’s to 50’s, all with rich experience in musical222 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Piece IndexHumanGenerated\n1615141312*111098*76543*21*\nSubjective Rating ScaleAbsolutely GeneratedProbably GeneratedUnsureProbably HumanAbsolutely HumanFigure 8 . Subjective evaluation on expressiveness and nat-\nuralness of the generated and human skeleton performance\nvideos. The tracks with signiﬁcant different ratings are\nmarked with “*”.\nacoustics or music audio signal processing. 17 subjects\nhave instrument performance experiences (15 on keyboard\ninstruments). This guarantees that most of them have a\ngeneral knowledge of how a human pianist performance\nmay look like based on a given MIDI stream, considering\ndifferent factors such as hand positions on the keyboard\naccording to pitch height, dominant motions for leading\nonsets, etc. Based on expressiveness and naturalness they\nrated the videos on a 5-point scale: absolutely generated\n(1), probably generated (2), unsure (3), probably human\n(4), and absolutely human (5).\n4.3.2 Results\nFigure 8 shows the average subjective ratings as bar plots\nand their standard deviations as whiskers. A Wilcoxon\nsigned rank test on each piece shows that no signiﬁcant\ndifference is found in 12 out of the 16 pairs ( p= 0:05).\nThis suggests that for 3=4of the observation videos, the\ngenerated body movements achieve the same level of ex-\npressiveness and naturalness as the real human videos.\nIn Figure 8, the pieces with signiﬁcant differences in\nthe subjective ratings between generated and real human\nvideos are marked with “*”. On the 1st piece, we observe\nan especially signiﬁcant difference. Further investigation\nreveals that this piece is in a fast tempo (130 BPM), where\nthe eighth notes are alternatively played by the right and\nleft hand with an agile motion, as shown in Figure 9.a.\nThe generated performance lacks this kind of dexterity. In\nReal Human Time(a) The agile fashion in left-right hand alternative playing is not learned.\nReal Human Generated\n(b) The exaggerated head nodding on the leading bass note (in red mark)\nis not learned.\nFigure 9 . The two typical failure cases.\naddition, the physical body motions from the human play-\ners are distinct and exaggerated around the phrase bound-\naries, but the generated ones tend to create more conser-\nvative motions. Figure 9.b gives an example, where in the\nreal human’s performance the head moves forward exten-\nsively on the leading bass note (marked in red), whereas\nthe generated one does not. Another observed drawback\nis the improper wrist positioning of a resting hand; a ran-\ndom position is often predicted in these cases. This is be-\ncause the left/right hand information is not encoded in the\nMIDI ﬁle, and when only one hand is used, the system\ndoes not know which hand to use and how to position the\nother hand. Generally speaking, the generated movements\nthat are rated signiﬁcantly lower than real human move-\nments tend to be somewhat dull, which might provide the\nsubjects a cue to discriminate between human and gener-\nated movements. We present all of the generated videos\nonline2.\n5. CONCLUSION\nIn this paper, we proposed a system for generating a\nskeleton sequence that corresponds to an input MIDI note\nstream. Thanks to data-driven learning between the MIDI\nnote stream and the skeleton, the system is capable of gen-\nerating natural playing motions like a human player with\nno explicit constraints on the physique or ﬁngering, reﬂect-\ning musical expressions, and attuning the generated motion\nto a particular performer.\nFor future work, we will apply more music contextual\nfeatures to generate richer skeleton movements, and ex-\ntend our method to the generation of 3D joint coordinates.\nGenerating textured characters based on these skeletons is\nanother future direction.\n2http://www.ece.rochester.edu/projects/air/\nprojects/skeletonpianist.htmlProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 2236. ACKNOWLEDGEMENT\nThis work is partially supported by the National Science\nFoundation grant 1741472.\n7. REFERENCES\n[1] Christoph Bregler, Michele Covell, and Malcolm\nSlaney. Video rewrite: Driving visual speech with au-\ndio. In Proceedings of the ACM Conference on Com-\nputer Graphics and Interactive Techniques , pages 353–\n360, 1997.\n[2] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser\nSheikh. Realtime multi-person 2D pose estimation us-\ning part afﬁnity ﬁelds. In Proceedings of the Inter-\nnational Conference on Conputer Vision and Pattern\nRecognition (CVPR) , 2017.\n[3] Lele Chen, Sudhanshu Srivastava, Zhiyao Duan, and\nChenliang Xu. Deep cross-modal audio-visual genera-\ntion. In Proceedings of the ACM International Confer-\nence on Multimedia Thematic Workshops , pages 349–\n357, 2017.\n[4] Soﬁa Dahl and Anders Friberg. Visual perception of\nexpressiveness in musicians body movements. Music\nPerception: An Interdisciplinary Journal , 24(5):433–\n454, 2007.\n[5] Jane W Davidson. Visual perception of performance\nmanner in the movements of solo musicians. Psychol-\nogy of Music , 21(2):103–113, 1993.\n[6] Seﬁk Emre Eskimez, Ross K Maddox, Chenliang Xu,\nand Zhiyao Duan. Generating talking face landmarks\nfrom speech. In Proceedings of the International Con-\nference on Latent Variable Analysis and Signal Sepa-\nration (LVA-ICA) , 2018.\n[7] Diederik P Kingma and Jimmy Ba. Adam: A method\nfor stochastic optimization. In Proceedings of the In-\nternational Conference on Learning Representations\n(ICLR) , pages 1–5, 2015.\n[8] Bochen Li, Karthik Dinesh, Zhiyao Duan, and Gaurav\nSharma. See and listen: Score-informed association of\nsound tracks to players in chamber music performance\nvideos. In Proceedings of the IEEE International Con-\nference on Acoustics, Speech, and Signal Processing\n(ICASSP) , 2017.\n[9] Zimo Li, Yi Zhou, Shuangjiu Xiao, Chong He, and Hao\nLi. Auto-conditioned recurrent networks for extended\ncomplex human motion synthesis. In Proceedings of\nthe International Conference on Learning Representa-\ntions (ICLR) , 2018.\n[10] Guanghan Ning, Zhi Zhang, and Zhiquan He.\nKnowledge-guided deep fractal neural networks for\nhuman pose estimation. IEEE Transactions on Multi-\nmedia , 20(5):1246–1259, 2017.[11] Ken Perlin and Athomas Goldberg. Improv: A system\nfor scripting interactive actors in virtual worlds. In Pro-\nceedings of the ACM Annual Conference on Computer\nGraphics and Interactive Techniques , pages 205–216,\n1996.\n[12] Ju-Hwan Seo, Jeong-Yean Yang, Jaewoo Kim, and\nDong-Soo Kwon. Autonomous humanoid robot dance\ngeneration system based on real-time music input. In\nProceedings of the IEEE International Conference on\nRobot and Human Interactive Communication , pages\n204–209, 2013.\n[13] Eli Shlizerman, Lucio Dery, Hayden Schoen, and\nIra Kemelmacher-Shlizerman. Audio to body dynam-\nics. 2017. Available: https://arxiv.org/pdf/\n1712.09382.pdf .\n[14] Supasorn Suwajanakorn, Steven M Seitz, and Ira\nKemelmacher-Shlizerman. Synthesizing obama: learn-\ning lip sync from audio. ACM Transactions on Graph-\nics (TOG) , 36(4), 2017.\n[15] Chia-Jung Tsay. The vision heuristic: Judging mu-\nsic ensembles by sight alone. Organizational Behavior\nand Human Decision Processes , 124(1):24–33, 2014.\n[16] Gerhard Widmer, Sebastian Flossmann, and Maarten\nGrachten. YQX plays chopin. AI magazine , 30(3):35–\n48, 2009.\n[17] Kazuki Yamamoto, Etsuko Ueda, Tsuyoshi Suenaga,\nKentaro Takemura, Jun Takamatsu, and Tsukasa Oga-\nsawara. Generating natural hand motion in playing a\npiano. In Proceedings of the IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS) ,\npages 3513–3518, 2010.224 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "On the Impact of Music on Decision Making in Cooperative Tasks.",
        "author": [
            "Elad Liebman",
            "Corey N. White",
            "Peter Stone"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492511",
        "url": "https://doi.org/10.5281/zenodo.1492511",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/298_Paper.pdf",
        "abstract": "Numerous studies have demonstrated that mood affects emotional and cognitive processing. Previous work has established that music-induced mood can measurably alter people's behavior in different contexts. However, the nature of how decision-making is affected by music in social settings hasn't been sufficiently explored. The goal of this study is to examine which aspects of people's decision making in inter-social tasks are affected when exposed to music. For this purpose, we devised an experiment in which people drove a simulated car through an intersection while listening to music. The intersection was not empty, as another simulated vehicle, controlled autonomously, was also crossing the intersection in a different direction. Our results indicate that music indeed alters people's behavior with respect to this social task. To further understand the correspondence between auditory features and decision making, we have also studied how individual aspects of music affected response patterns.",
        "zenodo_id": 1492511,
        "dblp_key": "conf/ismir/LiebmanWS18",
        "keywords": [
            "mood",
            "decision-making",
            "social settings",
            "music-induced mood",
            "inter-social tasks",
            "behavioral alteration",
            "simulated car",
            "intersection",
            "autonomous vehicle",
            "response patterns"
        ],
        "content": "ON THE IMPACT OF MUSIC ON DECISION MAKING IN COOPERATIVE\nTASKS\nElad Liebman\nThe University of Texas at Austin\nComputer Science Department\neladlieb@cs.utexas.eduCorey N. White\nMissouri Western State University\nDepartment of Psychology\ncwhite34@missouriwestern.eduPeter Stone\nThe University of Texas at Austin\nComputer Science Department\npstone@cs.utexas.edu\nABSTRACT\nNumerous studies have demonstrated that mood affects\nemotional and cognitive processing. Previous work has\nestablished that music-induced mood can measurably al-\nter people’s behavior in different contexts. However, the\nnature of how decision-making is affected by music in so-\ncial settings hasn’t been sufﬁciently explored. The goal\nof this study is to examine which aspects of people’s de-\ncision making in inter-social tasks are affected when ex-\nposed to music. For this purpose, we devised an experi-\nment in which people drove a simulated car through an in-\ntersection while listening to music. The intersection was\nnot empty, as another simulated vehicle, controlled au-\ntonomously, was also crossing the intersection in a differ-\nent direction. Our results indicate that music indeed alters\npeople’s behavior with respect to this social task. To fur-\nther understand the correspondence between auditory fea-\ntures and decision making, we have also studied how indi-\nvidual aspects of music affected response patterns.\n1. INTRODUCTION\nThere is plentiful evidence that one’s mood can affect how\none processes information in a wide array of contexts and\ntasks. Previous work has established that positive mood\ninduces a relative preference for positive emotional con-\ntent and vice versa [6, 14]. Recent work has conﬁrmed\nthis effect is indeed induced by music that is culturally\ncategorized as “happy” vs. “sad”, and illustrated how the\nemotional content of music informs the apriori expectation\nfor the emotional content of verbal stimuli [11]. As for\nnon-emotional and quantitative decision-making, previous\nwork has shown robust effects of loss aversion, whereby\nparticipants put more weight on potential losses than po-\ntential gains. In a recent study, Liebman et al. presented\nevidence for the complex impact of music-induced mood\non risky decision-making in the context of gambling. They\nobserved an overall improved stimulus processing in par-\nticipants listening to “happy” music compared to “sad”\nc\rElad Liebman, Corey N. White, Peter Stone. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Elad Liebman, Corey N. White, Peter Stone. “On\nthe Impact of Music on Decision Making in Cooperative Tasks”, 19th\nInternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.music, i.e., music-induced positive mood has led to better\nand faster decision-making overall [12].\nGiven the complexity and variability of the observed ef-\nfects of music on decision-making in the context of differ-\nent tasks, an inevitable question arises - how does music\naffect more complex tasks? More speciﬁcally, how does\nmusic affect complex decision-making that involves tak-\ning into consideration the agency of other entities? In this\npaper, we study the impact of music on decision behavior\nin the context of cooperative tasks, in which a person has\nto take into account the intentions of another agent when\nattempting to achieve their own goal. To this end, we de-\nsign an experiment in which a person must cross a simu-\nlated intersection that is simultaneously being crossed by\nanother autonomous agent, controlled by artiﬁcial intelli-\ngence. Our results indicate different types of music indeed\nhave a differential effect on people’s behavior in this set-\nting.\nThe structure of the paper is as follows. In Section\n3 we discuss our experimental design and how data was\ncollected from participants. In Section 4 we present and\nanalyze the results of our behavioral study. In Section 5\nwe examine more closely how music altered the partici-\npants’ behavior in more speciﬁc contexts. In Section 6,\nwe analyze how individual auditory components correlate\nwith the behavioral patterns observed in our human study.\nIn Section 2 we provide additional context about previous\nwork leading up to this paper. Lastly, in Section 7 we recap\nour results and discuss them in a broader context.\n2. RELATED WORK\nStudies that induce mood through listening to happy/sad\nmusic have shown mood-congruent bias across a range of\ntasks. Behen et al. [9] showed participants happy and sad\nfaces while they listened to positively or negatively va-\nlenced music and underwent fMRI. Participants rated the\nhappy faces as more happy while listening to positive mu-\nsic, and the fMRI results showed that activation of the su-\nperior temporal gyrus was greater when the face and music\nwere congruent with each other. In a study of mood and re-\ncall, De l’Etoile [4] found that participants could recall sig-\nniﬁcantly more words when mood was induced (through\nmusic) at both encoding and retrieval.\nPrevious work at the intersection of musicology and\ncognitive science has also studied the connection between695music and emotion. As Krumhansel points out [10], emo-\ntion is a fundamental part of music understanding and ex-\nperience, underlying the process of building tension and\nexpectations. There is neurophysical evidence of music\nbeing strongly linked to brain regions linked with emotion\nand reward [2], and different musical patterns have been\nshown to have meaningful associations to emotional affec-\ntations [15]. Similarly, studies have indicated that mood\nalso affects the perception of music [18]. Not only is emo-\ntion a core part of music cognitive processing, it can also\nhave a resounding impact on people’s mental state, and aid\nin recovery, as shown for instance by Zumbansen et al. [19]\nin the case of people suffering from Brocas aphasia. Peo-\nple regularly use music to alter their moods, and evidence\nhas been presented that music can alter the strength of\nemotional negativity bias [3]. All this evidence indicates\na deep and profound two-way connection between music\nand emotional perception.\nConsidering the impact of music on risk-related deci-\nsion making, previous work has studied the general con-\nnection between gambling behavior and ambiance factors\nincluding music [5, 8, 17] in an unconstrained casino envi-\nronment. Additionally, Noseworthy and Finlay have stud-\nied the effects of music-induced dissociation and time per-\nception in gambling establishments [13].\nLastly, in the context of music and its impact on coop-\neration, not much research has been done to quantitatively\nexplore how music impacts the cooperative and adversar-\nial behaviors of participants in social settings. Greitemeyer\npresented evidence that Exposure to music with prosocial\nlyrics reduces aggression [7]. From a different perspective\nentirely, Baron was able to show how environmentally-\ninduced mood helped improve negotiation and decrease\nadversarial behavior [1]. To the best of our knowledge, this\nis the ﬁrst work to study how different types of music dif-\nferentially affect people’s decision-making in the context\nof tasks involving other agents.\n3. EXPERIMENTAL SETUP\nIn this section we describe the details of the experiment\nconducted in this study. First, we describe the overall pro-\ncedure. We proceed to describe the participants, the au-\ntonomous car behavior, the music selected for the experi-\nment, and the data collected for analysis.\n3.1 Procedure\nIn this study, participants were given control of a simu-\nlated vehicle crossing an intersection. They had three con-\ntrol options - speed forward, go in reverse, and brake. In\naddition to the human-controlled vehicle, another vehicle,\ncontrolled autonomously by an artiﬁcial agent, was also\ncrossing the intersection from a different direction. If the\ntwo cars collided, they would crash. Participants were in-\nstructed to safely cross the intersection without crashing.\nParticipants were also instructed that the autonomous car\nwould generally respect the laws of trafﬁc but cannot be\nblindly relied upon to drive safely. Each time both vehicles\nFigure 1 . (A) A screen capture of the experiment. The\nred car was controlled by the participant. The blue car was\ncontrolled autonomously. (B) A collision would result in\na crash, as demonstrated in this screen capture. After the\ncrash, the trial terminates and the next trial begins.\ncleared the intersection and reached the end of the screen\nsafely, the trial would end and a new trial would commence\n(a 2 second pause was introduced between trials). The ex-\nperiment was divided into 8 blocks of 12 trials (for a total\nof 96 trials per participant). In each trial the behavior of the\nblue vehicle was randomized, determining its speed and\nthe amount of time it would wait by default in the intersec-\ntion if it had arrived to the intersection ﬁrst. In each block,\na different song was played, alternating between positive\nand negative music across blocks. The order of the songs\nwas counterbalanced across subjects. A 3 second pause\nbefore the beginning of each block to make sure the new\nsong had started before a new trial commenced. Each ex-\nperiment lasted approximately 20 minutes. A snapshot of\nthe experiment is presented in Figure 1.\n3.2 Participants\nFor this paper we have originally collected data from 20\nparticipants. All participants were graduate students who\nvolunteered to participate in the study. Two participants\nwere ﬁltered out for behaving uniformly without paying696 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018attention to the experimental conditions (always going for-\nward at the beginning of each trial without slowing, stop-\nping or paying attention to the autonomous vehicle), leav-\ning a total of 18 participants. Note that the comparisons\nof interest were within participants (happy vs. sad mu-\nsic). Thus, the sample size was sufﬁcient to detect sta-\ntistically signiﬁcant differences in behavior between these\nconditions.\n3.3 Autonomous Car Behavior\nThe key variability in stimuli in this experiment was pre-\nsented through randomization of the autonomous car be-\nhavior. The three main aspects of the autonomous car be-\nhavior that were variable were its speed approaching the in-\ntersection, how long it would wait in the intersection before\ngoing forward if it arrived to the intersection ﬁrst, and how\nfast it would move into the intersection and onward after\nentering the intersection. Participants were instructed not\nto blindly rely on the autonomous car’s behavior, but in the\nscope of this experiment we opted to have the autonomous\ncar always give right of way if the human-controlled car\nmade it to the intersection ﬁrst. The consequence of this\nwas that the decision whether to give right of way or move\nforward was almost always in the hands of the human par-\nticipant. Indeed, one of the explicit goals of this study were\nto examine how different music-induced mood would af-\nfect people’s aggressiveness vs. their inclination to give\nright of way.\n3.4 Music\nThe music used for this experiment is the same as that used\nin [11]. It is a collection of 8 publicly available songs\nwhich was surveyed to isolate two clear types - music that\nis characterized by slow tempo, minor keys and somber\ntones, typical to traditionally “sad” music, and music that\nhas upbeat tempo, major scales and colorful tones, which\nare traditionally considered to be typical to “happy” music.\nThe principal concern in selecting these musical stimuli,\nrather than their semantic categorization as either happy or\nsad, was to curate two separate “pools” of music sequences\nthat were broadly characterized by a similar temperament\n(described above), and show they produced consistent re-\nsponse patterns. In [11], it has been shown experimentally\nthat the selected music was effective for inducing the ap-\npropriate mood. This was done by selecting a separate pool\nof 40 participants and having them rate each song on a 7-\npoint Likert scale, with 1 indicating negative mood and 7\nindicating positive mood. It was then shown that the songs\ndesignated as positive received meaningfully and statisti-\ncally signiﬁcantly higher scores than those denoted as sad.\n4. OVERVIEW OF RESULTS\nIn this section we survey the key ﬁndings of the study, ex-\namining the participants’ behavior globally (that is, across\nall types of circumstances and autonomous vehicle behav-\nior).\nFigure 2 . Normalized minimal distance kept from the au-\ntonomous car by the participants in the sad and happy mu-\nsic conditions (here and elsewhere, bars represent std. er-\nror). Participants tended to keep a lower minimal distance\nwhen listening to sad music.\nFigure 3 . The average normalized speed of the participants\nin the happy and sad music conditions. Participants were\nmore likely to go faster when listening to happy music.\n4.1 Minimal Distance from Autonomous Car\nThe most statistically signiﬁcant difference (p <0.05 us-\ning a paired t-test) across all trials was that participants\nlistening to sad music kept a lower minimal distance over-\nall from the autonomous car compared to when they were\nlistening to happy music. In other words, their behavior\nwhen listening to sad music was riskier and less consider-\nate (“cutting it closer” with respect to how much margin\nfor error they kept when entering the intersection). This\nresult is illustrated in Figure 2.\n4.2 Driving Speed\nParticipants also differed in their driving speed in the sad\nand happy music conditions (signiﬁcant at p <0.05 using\na paired t-test). Overall, participants were more likely to\ngo fast in the happy music condition compared to the sad\nmusic condition, as reﬂected in Figure 3.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 697Figure 4 . The likelihood of the participants to go ﬁrst\ninto the intersection in the sad and happy music conditions.\nParticipants were more likely to go ﬁrst when listening to\nhappy music.\n4.3 Right of Way\nAnother difference, which is strongly related to the pre-\nvious observation, and is borderline signiﬁcant1(at p <\n0.1 using a paired t-test) was that participants listening to\nhappy music were more likely to go into the intersection\nﬁrst compared to when they were listening to sad music, as\nillustrated in Figure 4.\n5. BREAKDOWN OF USER BEHA VIOR UNDER\nDIFFERENT TRIAL CONDITIONS\nIn this section we consider how different music induced\ndifferent participant behavior when breaking down the tri-\nals by the different types of autonomous car behavior. It\nis worth noting that the observation made in the previous\nsection held under most partitions of the trial data.\n5.1 Behavior under Different Autonomous Car\nIntersection Wait Times\nIf we compare how participants behaved when the au-\ntonomous vehicle waited <4 seconds at the intersection,\nthe difference in the participants’ driving speed because\ndramatically more accentuated in the happy vs. sad mu-\nsic conditions. Additionally, the participants’ difference\nin wait times at the intersection in the happy and sad mu-\nsic conditions also becomes more differentiated when we\nonly consider trials in which the autonomous car waited\nless than <4 seconds. These observations are presented in\nﬁgures 5(a) and 5(b), and are both statistically signiﬁcant\nwith p <0.05 using an unpaired t-test.\n1A 0.1 threshold for testing the signiﬁcance of p-values is accepted in\nthe context of relatively small samples sizes. Nonetheless, we strive to\nuse these measures responsibly in our choice of language, thus using the\nequally common term “borderline signiﬁcance” to describe results with\np-value <0.1 but >0.05\nFigure 5 . (a) Normalized average per-trial speed of partic-\nipants in the happy and sad music conditions, speciﬁcally\nin the case that the autonomous vehicle waited less than 4\nseconds. (b) Normalized per-trial time waiting at the inter-\nsection of participants in the happy and sad music condi-\ntions, speciﬁcally in the case that the autonomous vehicle\nwaited less than 4 seconds.\n5.2 Behavior under Different Autonomous Car\nAverage Speed\nA similar related trend to that observed in the previous sec-\ntion were observed when considering the average speed of\nthe autonomous car. In trials in which the average speed\nof the autonomous vehicle was above the median, people\nwere slower to drive and took longer to wait at the intersec-\ntion while listening to sad music, compared to when listen-\ning to happy music (again with p <0.05 using an unpaired\nt-test).\n6. IMPACT OF MUSICAL PARAMETERS ON\nUSER BEHA VIOR\nThe partition between “positive” and “negative” mood-\ninducing songs is easy to understand intuitively, and in it-\nself is enough to induce the different behavioral patterns\ndiscussed in the previous section. However, similarly to\nthe analysis performed in [11] and [12], we are interested\nin ﬁnding a deeper connection between the behavior ob-\nserved in the experiment and the different characteristics\nof music. More exactly, we are interested in ﬁnding the\ncorrespondence between various musical features, which\nalso happen to determine how likely a song is to be per-\nceived as happy or sad, and the driving decision-making\nmanifested by participants. To this end, we considered the\n8 songs used in this experiment, extracted key character-698 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018izing features which we assume are relevant to their mood\nclassiﬁcation, and examined how they correlate with the\nsubject behavior we observed.\n6.1 Extracting Raw Auditory Features\nWe focused on four major auditory features: a) overall\ntempo; b) overall “major” vs. “minor” harmonic charac-\nter (we will refer to this feature as “major chord ratio”\nfor simplicity); c) average amplitude, representing overall\nloudness; and d) maximum amplitude, representing peak\nloudness. Features (a), (c) and (d) were computed using\nthe Librosa library [16]. To compute feature (b), we imple-\nmented the following procedure, similar to that described\nin [11]. For each snippet of 20 beats an overall spec-\ntrum was computed and individual pitches were extracted.\nThen, for that snippet, according to the amplitude intensity\nof each extracted pitch, we identiﬁed whether the domi-\nnant harmonic was major or minor. The major/minor score\nwas deﬁned to be the proportion of major snippets out of\nthe overall song sequence. Analysis done in [11] conﬁrms\nthese features are indeed associated with our identiﬁcation\nas “positive” vs. “negative”. Having labeled “positive” and\n“negative” as 1 and 0 respectively, a Pearson correlation of\n0:7\u00000:8with p-values \u00140:05was observed between these\nfeatures and the label. Signiﬁcance was further conﬁrmed\nby applying an unpaired t-test for each feature for positive\nvs. negative songs (p-values < :05).\n6.2 Results\nOverall, the most prominently inﬂuential aspect of the mu-\nsic as observed by statistical analysis is the loudness of the\nmusic. Additional effects were observed relating to tempo\nand major chord ratio, but they did not meet the same cri-\nteria for signiﬁcance.\n6.3 Loudness and Overall Time Out of Intersection\nThe normalized overall time out of intersection is the total\ntime it took the participant to drive up to the intersection,\nwait, and cross the intersection, normalized per subject.\nThe normalized time out of the intersection was statisti-\ncally signiﬁcantly (p <0.052) inversely correlated with\nboth the average loudness (r =-0.72) and the maximum\nloudness (r =-0.77) of the music. The correspondence\nbetween the average loudness and the overall time out of\nintersection is presented in Figures 6 (the ﬁndings for the\nmaximum loudness are similar). In other words, the louder\nthe music was, the faster people were to complete the task.\n6.4 Loudness, Speed, Time Stopped, and Minimal\nDistance\nLoudness also impacted various aspects of participant be-\nhavior that are related to the participants’ driving speed and\noverall aggressiveness. These results are borderline signif-\nicant at p <0.1 for all correlations reported in this subsec-\ntion.\n2P-values for correlation are results obtained by analysis of the distri-\nbution of correlation values given the null hypothesis.\nFigure 6 . Correlation between the average loudness of the\nmusic and the normalized total time out of the intersection\nfor the participants.\nFigure 7 . Correlation between the average loudness and\nthe average speed of the participants.\n\u000fMost straightforwardly, the average loudness was\npositively correlated (r =0.65) with the normalized\naverage speed of the participants, meaning that par-\nticipants drove faster when listening to louder music.\nThis result is illustrated in Figure 7.\n\u000fSimilarly, other metrics reﬂect overall speed, includ-\ning the minimum speed, the median speed and the\ninitial speed (speed after 1 second from the begin-\nning of the trial) were positively correlated with r >\n0.6.\n\u000fThe overall normalized time the participants stopped\nat the intersection was inversely correlated at r =\n-0.67 with the average loudness, meaning people\nwere faster to continue into the intersection when lis-\ntening to louder music. This ﬁnding is presented in\nFigure 8\n\u000fLastly, the minimal distance the participants kept\nfrom the autonomous car was positively correlatedProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 699Figure 8 . Correlation between the average loudness and\nthe average time the participants stopped at the intersec-\ntion.\nFigure 9 . Correlation between the normalized key press\ncount of the participants and the tempo.\nwith the average loudness, meaning the louder the\nmusic was, the higher the minimal distance was.\nConsidering the other ﬁndings in this section and the\nfact that the minimal distance and the average speed\nare positively correlated at r =0.75 (and p <0.05),\nit is reasonable to assume this relationship is a re-\nsult of the impact of loudness on the participants’\nspeed rather than an indication of how loud music\nincreases people’s risk aversion, for instance.\n6.5 Tempo and Hesitancy\nThe total number of key presses per trial, normalized per\nparticipant, is a good proxy for hesitancy in decision mak-\ning (speeding and slowing down, going forward and brak-\ning, etc). Interestingly, the key press count was inversely\ncorrelated to the tempo (r =-0.59 and p <0:1), suggest-\ning faster music reduced people’s hesitancy. This results is\npresented in Figure 9.6.6 Additional Observations\nBeyond the results reported thus far in this section, sev-\neral relationships between musical features and participant\nbehavior were observed that did not meet the p <0.1 cri-\nterion for signiﬁcance, but came sufﬁciently close to merit\nmention:\n\u000fThe normalized key press count was also inversely\ncorrelated with the major chord ratio (at r =-0.52),\nimplying it’s possible that music that leans heavier\ntowards major harmonies also reduces hesitancy in\nthe participants.\n\u000fThe the major chord ratio was also positively corre-\nlated with the maximum speed of the participants,\nand the minimal distance the participants kept from\nthe autonomous car, at r =0.54 and r =0.52, re-\nspectively.\n\u000fThe tempo was positively correlated with both the\naverage and the max speed at r =0.53 for both.\n7. SUMMARY AND DISCUSSION\nIn this study we analyzed how people’s decision-making\nbehavior is affected by music in the context of a social task\nwhich requires a certain level of cooperation to avoid ad-\nverse consequences. Participants were required to drive\na simulated car through an intersection while another car,\ncontrolled by an autonomous agent, was also crossing from\na different direction. Examining the results reveals a com-\npound picture beﬁtting the subtleties of the performed task.\nWhile happy music induced some aspects of behavior that\ncould be described as more social, namely that participants\nkept a safer distance from the other car when crossing,\nthey also manifested less social behavior by driving faster\nand being less likely to let the autonomous vehicle go ﬁrst.\nAll in all, our initial expectation that happier music would\nmake people more cooperative was not supported by the\nﬁndings. Conversely, it can be argued that sad music made\npeople slower and more cautious, and therefore safer to\ntheir environment and to the other agent speciﬁcally. This\nstudy is the ﬁrst step towards a better understanding of how\nmusic informs people’s decision-making in multi-agent en-\nvironments that require some level of cooperation. Follow-\nup work would help reﬁne our observations, as well as pos-\nsibly leverage them in the context of human-agent interac-\ntion and negotiation.\n8. ACKNOWLEDGMENTS\nThis work has taken place in the Learning Agents Research\nGroup (LARG) at UT Austin. LARG research is supported\nin part by NSF (IIS-1637736, IIS-1651089, IIS-1724157),\nIntel, Raytheon, and Lockheed Martin. Peter Stone serves\non the Board of Directors of Cogitai, Inc. The terms of\nthis arrangement have been reviewed and approved by the\nUniversity of Texas at Austin in accordance with its policy\non objectivity in research.700 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20189. REFERENCES\n[1] Robert A Baron. Environmentally induced positive af-\nfect: Its impact on self-efﬁcacy, task performance, ne-\ngotiation, and conﬂict. Journal of Applied Social Psy-\nchology , 20(5):368–384, 1990.\n[2] Anne J Blood and Robert J Zatorre. Intensely pleasur-\nable responses to music correlate with activity in brain\nregions implicated in reward and emotion. Proceedings\nof the National Academy of Sciences , 98(20):11818–\n11823, 2001.\n[3] Jie Chen, Jiajin Yuan, He Huang, Changming Chen,\nand Hong Li. Music-induced mood modulates the\nstrength of emotional negativity bias: An erp study.\nNeuroscience Letters , 445(2):135–139, 2008.\n[4] Shannon K de lEtoile. The effectiveness of music ther-\napy in group psychotherapy for adults with mental ill-\nness. The Arts in Psychotherapy , 29(2):69–78, 2002.\n[5] Laura Dixon, Richard Trigg, and Mark Grifﬁths. An\nempirical investigation of music and gambling be-\nhaviour. International Gambling Studies , 7(3):315–\n326, 2007.\n[6] Rebecca Elliott, Judy S Rubinsztein, Barbara J Sa-\nhakian, and Raymond J Dolan. The neural basis\nof mood-congruent processing biases in depression.\nArchives of general psychiatry , 59(7):597–604, 2002.\n[7] Tobias Greitemeyer. Exposure to music with prosocial\nlyrics reduces aggression: First evidence and test of the\nunderlying mechanism. Journal of Experimental So-\ncial Psychology , 47(1):28–36, 2011.\n[8] Mark Grifﬁths and Jonathan Parke. The psychology of\nmusic in gambling environments: An observational re-\nsearch note. Journal of Gambling Issues , 2005.\n[9] Jeong-Won Jeong, Vaibhav A Diwadkar, Carla D\nChugani, Piti Sinsoongsud, Otto Muzik, Michael E Be-\nhen, Harry T Chugani, and Diane C Chugani. Con-\ngruence of happy and sad emotion in music and faces\nmodiﬁes cortical audiovisual activation. NeuroImage ,\n54(4):2973–2982, 2011.\n[10] Carol L Krumhansl. Music: A link between cognition\nand emotion. Current Directions in Psychological Sci-\nence, 11(2):45–50, 2002.\n[11] Elad Liebman, Peter Stone, and Corey N. White. How\nmusic alters decision making - impact of music stimuli\non emotional classiﬁcation. In Proceedings of the 16th\nInternational Society for Music Information Retrieval\nConference, ISMIR 2015, M ´alaga, Spain, October 26-\n30, 2015 , pages 793–799, 2015.\n[12] Elad Liebman, Peter Stone, and Corey N White. Impact\nof music on decision making in quantitative tasks. In\nISMIR , pages 661–667, 2016.[13] Theodore J Noseworthy and Karen Finlay. A compar-\nison of ambient casino sound and music: Effects on\ndissociation and on perceptions of elapsed time while\nplaying slot machines. Journal of Gambling Studies ,\n25(3):331–342, 2009.\n[14] Kristi M Olafson and F Richard Ferraro. Effects of\nemotional state on lexical decision performance. Brain\nand Cognition , 45(1):15–20, 2001.\n[15] S ´ebastien Paquette, Isabelle Peretz, and Pascal Belin.\nThe musical emotional bursts: a validated set of mu-\nsical affect bursts to investigate auditory affective pro-\ncessing. Frontiers in psychology , 4, 2013.\n[16] Brian McFee ; Matt McVicar ; Colin Raf-\nfel ; Dawen Liang ; Douglas Repetto. Librosa.\nhttps://github.com/bmcfee/librosa , 2014.\n[17] Jenny Spenwyn, Doug JK Barrett, and Mark D Grif-\nﬁths. The role of light and music in gambling be-\nhaviour: An empirical pilot study. International Jour-\nnal of Mental Health and Addiction , 8(1):107–118,\n2010.\n[18] Jonna K Vuoskoski and Tuomas Eerola. The role of\nmood and personality in the perception of emotions\nrepresented by music. Cortex , 47(9):1099–1106, 2011.\n[19] Anna Zumbansen, Isabelle Peretz, and Sylvie H ´ebert.\nThe combination of rhythm and pitch can account\nfor the beneﬁcial effect of melodic intonation therapy\non connected speech improvements in brocas aphasia.\nFrontiers in human neuroscience , 8, 2014.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 701"
    },
    {
        "title": "Controlled Vocabularies for Music Metadata.",
        "author": [
            "Pasquale Lisena",
            "Konstantin Todorov",
            "Cécile Cecconi",
            "Françoise Leresche",
            "Isabelle Canno",
            "Frédéric Puyrenier",
            "Martine Voisin",
            "Thierry Le Meur",
            "Raphaël Troncy"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492441",
        "url": "https://doi.org/10.5281/zenodo.1492441",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/68_Paper.pdf",
        "abstract": "We present a set of music-specific controlled vocabularies, formalized using Semantic Web languages, describing topics like musical genres, keys, or medium of performance. We have collected a number of existing vocabularies in various formats, converted them to SKOS and performed the interconnection of their equivalent terms. In addition, novel vocabularies, not available online before, have been designed by an editorial team. Next to multilingual labels and definitions, we provide hierarchical relations as well as links to external resources. We also show the application of those vocabularies for the production of vector embeddings, allowing for the calculation of distances between keys or between instruments.",
        "zenodo_id": 1492441,
        "dblp_key": "conf/ismir/LisenaTCLCPVMT18",
        "keywords": [
            "music-specific controlled vocabularies",
            "Semantic Web languages",
            "music genres",
            "keys",
            "medium of performance",
            "existing vocabularies",
            "conversion to SKOS",
            "interconnection of terms",
            "novel vocabularies",
            "multilingual labels and definitions"
        ],
        "content": "CONTROLLED VOCABULARIES FOR MUSIC METADATA\nPasquale Lisena1Konstantin Todorov2C´ecile Cecconi3Franc ¸oise Leresche4\nIsabelle Canno5Fr´ed´eric Puyrenier4Martine Voisin5Thierry Le Meur3Rapha ¨el Troncy1\n1EURECOM, Sophia Antipolis, France2LIRMM, University of Montpellier, CNRS, France\n3Philharmonie de Paris, France4Biblioth `eque nationale de France5Radio France\nlisena@eurecom.fr, todorov@lirmm.fr, ccecconi@cite-musique.fr\nABSTRACT\nWe present a set of music-speciﬁc controlled vocabularies,\nformalized using Semantic Web languages, describing top-\nics like musical genres, keys, or medium of performance.\nWe have collected a number of existing vocabularies in\nvarious formats, converted them to SKOS and performed\nthe interconnection of their equivalent terms. In addition,\nnovel vocabularies, not available online before, have been\ndesigned by an editorial team. Next to multilingual labels\nand deﬁnitions, we provide hierarchical relations as well as\nlinks to external resources. We also show the application\nof those vocabularies for the production of vector embed-\ndings, allowing for the calculation of distances between\nkeys or between instruments.\n1. INTRODUCTION\nDescribing music is an activity that involves an important\nnumber of terms coming from domain-speciﬁc glossaries.\nIn addition to the cross-domain concept of genre, we can\nmention musical keys, instruments or catalogues of com-\npositions. Libraries and musical institutions have different\npractices for describing this kind of information. In the\nbest case, they make use of thesauri that are often available\nin different incompatible formats, and that can be either\ninternally deﬁned or standardised by larger communities\nsuch as the International Association of Musical Libraries\n(IAML). In other cases, this information is codiﬁed in free\ntext ﬁelds, delegating to the editors the responsibility of\nfollowing the living practice about syntax and lexical form.\nThe new attitude for sharing the knowledge beyond\nthe institutional and national borders—embodied by in-\nternational consortia like IAML or in projects like Euro-\npeana [7] and OpenGlam [4]—brings its effect also on the\nmusic domain. Accordingly, Semantic Web technologies\nhave gained a central role in music representation, that has\nreached the Linked Open Data world. A second conse-\nquence is the request for a change in the previously de-\nscribed current practices towards the adoption of publicly\navailable controlled vocabularies. The use of vocabularies\nc\rLisena et al. Licensed under a Creative Commons At-\ntribution 4.0 International License (CC BY 4.0). Attribution: Lisena\net al. “Controlled V ocabularies for Music Metadata”, 19th International\nSociety for Music Information Retrieval Conference, Paris, France, 2018.opens up different possibilities, like the deﬁnition of labels\nin different languages or of alternate lemmata in the same\nlanguage (i.e. the French terms “ut majeur” and “do ma-\njeur” which both refer to the key of C major ). Different\nkinds of relationships between terms can be deﬁned and it\nis possible to deﬁne a hierarchy between them (for exam-\nple, “violin” is a narrower concept with respect to “string”)\nwhich can produce, as beneﬁt, a more powerful advanced\nsearch for the ﬁnal user. Previous research demonstrated\nhow an RDF (for Resource Description Framework) struc-\nture helps reasoning engines to discover links between dif-\nferent levels in the hierarchy of instruments [8].\nPublishing Semantic Web vocabularies is not new in\nthe ﬁeld of music. The Musical Instruments Museum On-\nline (MIMO)1published the biggest taxonomy of musi-\ncal instrument in RDF, as result of the contribution of in-\nstitutions and universities all over the world. The librar-\nian practice draws on the UNIMARC2thesauri of musi-\ncal forms (genres) and medium of performance standard-\nised by IAML. Historically adopted by librarians world-\nwide, these thesauri have recently been published in the\nWeb of Data, marking the growing interest in this techno-\nlogical environment. The French National Library (BnF)\nrelies on an authority vocabulary in RDF for subject head-\nings called RAMEAU,3containing a list of labels for en-\ntities of encyclopedic interest which includes also music\ngenres and instruments. A musical key vocabulary is pub-\nlished as side resource of the MusicOntology [14], con-\nsisting in a list of English labelled concepts, with some\nadditional information—like the mode ( major /minor ), the\ntonic, etc.—, without any links describing semantic con-\nnections between them.\nOn the one hand, a large number of thesauri cover\nfew well-deﬁned categories (genres and medium of per-\nformance), making the reconciliation of data coming from\ndifferent sources difﬁcult, also because of the different for-\nmats of these thesauri. A reconciliation that would add a\nbroader and deeper nomenclature has a beneﬁt, increasing\nboth the number of elements and alternate labels. On the\nother hand, a large set of concepts—handled so far through\nerror-prone free-text—is asking for standardisation in spe-\ncialised vocabularies.\n1http://www.mimo-db.eu/\n2They are commonly named after the UNIMARC standard for librar-\nian records, in which they are widely used.\n3R´epertoire d’autorit ´e-mati `ere encyclop ´edique et alphab ´etique uniﬁ ´e:\nhttp://rameau.bnf.fr/424This paper presents a set of controlled vocabularies for\nthe description of the music information as Linked Open\nData, extending and ﬁnalising related work in [10]. This\nresearch have the primary goal of the interconnection of\nmusic information datasets, building bridges between ex-\nisting vocabularies and providing tools for the automatic\nmatching. The ﬁnal aim consists in the contribution to\nthe achievement of a global music knowledge graph [1]\nin the Web of Data, looking at all the applications that\nsemantically structured data have in the music informa-\ntion retrieval and recommendation ﬁeld [2, 12, 13, 18]. In\nSection 2, we present the complete set of vocabularies, giv-\ning detailed information about their content. The process\nof realisation, collection and interlinking is described in\nSection 3, while we present applications, such as embed-\ndings and literal dereferencing in Section 4. Finally, we\nconclude and outline some and future work in Section 5.\n2. MUSIC VOCABULARIES\nA controlled vocabulary is a thematic thesaurus of entities.\nIn the Semantic Web, each term is identiﬁed with a URI.\nTheSimple Knowledge Organization System (SKOS) [11]\nhave been chosen as format because of its capability of\ndeﬁning preferred and alternate labels in each language,\nrelationships between terms, comments and notes for de-\nscribing the entity and help the annotation activity. In the\ncase of the vocabulary of Catalogues of works , the used\nontology is the RDF version of Metadata Object Descrip-\ntion Schema (MODS) [17], that suits the need of deﬁning\nidentiﬁers, publication date, subject, etc.\nEach vocabulary fulﬁls a set of requirements, including\nmultilingualism, open and public access, presence of def-\ninitions. It must also be suitable for different contexts of\nuse and conceptual models of musical information, which\nis guaranteed by the presence in the editorial team of ex-\nperts from different types of cultural institutions (libraries,\nradio broadcasting networks, concert halls).\nThe vocabularies are all available in a triple store\nserver, which provides a SPARQL endpoint4for re-\nquesting the data in different formats like RDF, JSON,\ncsv, etc. Alternatively, the vocabularies can be ex-\nplored by a web browser starting from http://data.\ndoremus.org/vocabularies/ . The server enables\nthe HTTP dereferencing of URIs: this means that a\nweb browser pointing to the URI of a speciﬁc concept\n(e.g. http://data.doremus.org/vocabulary/\nderivation/medley ) will land on a page containing\nits human-readable description, showing all its properties.\nThe triple store includes also music data that use these vo-\ncabularies, so that the deﬁnition and the usage of a concept\nin musical works can be appreciated as part of the same\nknowledge base. Finally, an RDF version in Turtle format\nis available on GitHub.5\nEach vocabulary is licensed for free distribution, fol-\n4http://data.doremus.org/sparql\n5https://github.com/DOREMUS-ANR/\nknowledge-base/tree/master/vocabularieslowing a Creative Commons Attribution 4.0 license,6and\nit is open to the community for any kind of contribution.\nWe collected, implemented and published 18 controlled\nvocabularies belonging to 7 different families, containing\nmore than 9500 distinct concepts and involving 26 differ-\nent languages or dialects. In the following paragraphs, we\ndescribe the content of those vocabularies, subdivided in\ntwo groups.\n2.1 Collection of interlinked vocabularies\nThis group includes vocabularies that were already avail-\nable in the Web of Data, in the community or internally\nto a speciﬁc institution. When two or more vocabularies\nshare the same high-level topic—e.g. the musical genre—\nwe call that group family . In order to interconnect the dif-\nferent knowledge sources, an alignment process is needed\nfor discovering when terms coming from vocabularies be-\nlonging to the same family refers to the same concept. This\nprocess will be detailed in Section 3.3.\nMusical genres. This family includes vocabularies about\nthe genre of a musical work. By genre, we mean the main\ncategories by which we describe the works, like rock, lir-\nica, funk, opera, gospel, polka, jazz, including genres of\nworld music. The term genre is very broad and also in-\ncludes musical “forms” that gained in the centuries their\nown genre deﬁnition like symphony, concerto, sonata.\nWe collected, republished as SKOS and interlinked the\nfollowing vocabularies:\n\u000fIAML , 607 concepts, multilingual. This list, largely\nadopted in librarian environments, was available as a\nset of labels and codes, in some cases with deﬁnitions\nor editorial notes. We converted this big vocabulary to\nSKOS from different sources (librarian tabular data, on-\nline HTML version). Afterwards, a SKOS version7has\nbeen published by IFLA (International Federation of Li-\nbrary Associations), which is however less rich than ours\nin terms of alternate labels. We provide owl:sameAs\nlinks from our vocabulary to the IFLA version.\n\u000fRAMEAU , 654 concepts, French, hierarchised. It is\npublished as Linked Data by the French National Li-\nbrary (BnF). We extracted from this large nomenclature\nthe part related to musical genres.\n\u000fDiabolo , 629 concepts, French, hierarchised. It is the\nset of labels used in the disc catalogue of Radio France\n(RF). It also includes some skos:related links, e.g.\nbetween spiritual andgospel .\n\u000fItema3 , 40 concepts, French. It is used in the technical\ndocumentation of the concert archive of RF.\n\u000fItema3-MusDoc , 172 concepts, French. It is used in the\nmusical documentation of the concert archive of RF.\n\u000fRedomi , 297 concepts, French, hierarchised. It is used\nin the musical work documentation of RF.\nMedium of performance. Any instrument able to pro-\nduce sounds can be considered as a medium of perfor-\n6https://creativecommons.org/licenses/by/4.0/\n7http://iflastandards.info/ns/unimarc/terms/\nfom/Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 425mance or MoP. In this family of vocabularies, we can ﬁnd\nmusical instruments coming from different cultures (west-\nern, oriental, African, Indian, etc.), the voices in differ-\nent ranges (soprano, alto, etc.), aside from group of instru-\nments (orchestras, ensembles) and voices (choirs).\nWe collected, republished as SKOS and interlinked the\nfollowing vocabularies:\n\u000fMIMO , 2480 concepts, multilingual, hierarchised. The\nMusical Instrument Museum Online comes from the\njoint international effort of different music institutions\nand museum. Despite being the most complete vocabu-\nlary of instruments, it does not include voices. MIMO is\npublicly available as Linked Data.8\n\u000fIAML , 419 concepts, multilingual, hierarchised. De-\nspite its smaller granularity, this vocabulary has a good\ncoverage for voices and groups. Like for the homonym\ngenre vocabulary, also in this case an ofﬁcial version\nfrom IFLA is online,9less rich both with respect to the\nlanguages covered and to the number of concepts (392).\n\u000fRAMEAU , 876 concepts, French, hierarchised. As in\nthe genre case, we selected the part related to MoP.\n\u000fDiabolo , 2117 concepts, French, hierarchised. It is the\nset of labels used in the disc catalogue of RF. For ethnic\nor traditional instrument, it includes also the reference\nto the relative geographic area.\n\u000fItema3 , 314 concepts, French. It is used in the docu-\nmentation of the concert archive of RF.\n\u000fRedomi , 179 concepts, French, hierarchised. It is used\nin the musical work documentation of RF.\n2.2 New vocabularies\nThis section presents vocabularies for which we did not\nrely on any previous material, because it was not existing\nor not suitable for our goals. We designed these vocabu-\nlaries on the basis of real data coming from institutions,\nenriched by an editorial process that involved also librar-\nians. Since the work has been conducted in French, the\ndeﬁnitions of the terms are so far available only in this lan-\nguage. However, every label has been translated at least in\nEnglish and Italian in order to facilitate their reuse.\nMusical keys. 30 concepts, English, French, Spanish,\nItalian. This vocabulary contains the set of keys used in\nwestern music, labelled with the tone followed from the\ntype of scale (e.g. C major ). The concept are linked among\nthem by speciﬁc properties for keys relationships, like rel-\native ,parallel andclosely related keys. It contains also\nsameAs links with the key vocabulary of MusicOntology.\nMusical modes. 22 concepts, English, French, Italian,\nLatin, hierarchised. The word mode generally refers to a\ntype of scale, coupled with a set of characteristic melodic\nbehaviours. They are mostly used for describing ancient or\nmedieval music.\nCatalogues of works. 152 MODS resources. A thematic\ncatalogue orcatalogue of works is a recognised editorial\n8http://www.mimo-international.com/\n9http://iflastandards.info/ns/unimarc/terms/\nmop/list of all known works of a composer. In practice, a clas-\nsical composition can be univocally identiﬁed by the cata-\nlogue code and number. For example, Eine kleine Nacht-\nmusik is identiﬁed with K 525 , where Kis the K ¨ochel\ncatalogue of Mozart’s work. Each resource contains the\ninformation about the catalogue editor and publisher, the\nlanguage of drafting, the date of publication. The sub-\nject artist of each catalogue is disambiguated through the\nDOREMUS dataset [1, 10].\nTypes of derivations. 16 concepts, English, French, Ital-\nian, Spanish, German, hierarchised. A work can be de-\nrived from another by transforming its material into an-\nother through orchestration, harmonisation, etc. All these\ntypes (with deﬁnitions) are collected in this vocabulary.\nFunctions. 106 concepts, English, French and Italian,\nhierarchised. A music event—a performance, a composi-\ntion, a recording, etc.—involves a number of different roles\norfunctions like author, performer, conductor, sound engi-\nneer, etc.Additional details can also be provided to account\nfor the different kinds of author, like composer, lyricist or\narranger. These functions are identiﬁed in this vocabulary,\ntogether with their deﬁnitions.\n3. MODELING PROCESS\nWe detail the modeling process, which is based on an inter-\naction between music metadata experts and automatic data\nconversion and fusion tools.\n3.1 Editorial work\nAn editorial committee grouping 7 members coming from\ndifferent backgrounds (library, radio, concert hall) played\nan important role in the vocabulary modeling. First, ex-\nisting vocabularies have been inventoried and assessed as\ncandidates for being interlinked on the basis of their com-\npleteness and adoption. Next, the committee made choices\nabout which new vocabularies to create and what should\nbe their scope. These choices reﬂect the aim of produc-\ning powerful tools to describe recordings, publications and\ntheir contexts of creation, instead of producing exhaus-\ntive vocabularies about every aspects of the music. The\ncommittee relies on the members experience in music data\nmanagement practices. The experts had to confront their\npoint of views—necessarily different because depending\non the missions of their institutions—until the list of terms,\ntheir contexts of use and their deﬁnitions were coherent.\nFirst of all, the group had to be consistent with respect\nto the data available in those institutions. For example, we\nchose not to publish a rhythmic patterns vocabulary since\nthe data which is available was not created in a musical\nanalysis perspective. Then, the work had to be based upon\nthe team’s area of competence. This is one of the reasons\nwhy we decided to limit the scope of the musical modes\nvocabulary to old and European ones only. Listing and\ndescribing scales coming from other continents would re-\nquire an additional work that would largely involve musi-\ncologists.426 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018We chose ﬁrst to create the new vocabularies described\nin Section 2.2. The catalogues of works vocabulary is a\nspecial one since it contains only a list of bibliographic\nreferences and does not have the structure of a thesaurus.\nIt was established from the titles used by the BnF. The mu-\nsical keys and musical modes vocabularies were the eas-\niest to model since they contain a small number of well-\ndeﬁned concepts with clear translations. The other ones\nwere more complex to model. A ﬁrst set of entities was\ngenerated on the basis of the initial datasets. Then, the ex-\nperts had to confront their point of views until the list of\nterms, their contexts of use and their deﬁnitions were co-\nherent. The functions vocabulary was especially complex\nto deﬁne since it had to reconcile very different ways of\ndescribing performing art activities: What is the relevant\ndescription for “sound engineer”? How to describe the act\nof improvising? etc.\n3.2 Conversion to Semantic Web formats\nTwo different steps take part in the generation of the vo-\ncabularies as RDF graphs.\nThe ﬁrst one is a preliminary conversion from spread-\nsheets or XML ﬁles to RDF, using the OpenReﬁne [6] tool\nor with speciﬁc scripts. The collections of concepts already\nin the Web of Data (like RAMEAU) have been instead ex-\ntracted through speciﬁc SPARQL queries on an endpoint.\nIn the second step, additional vocabulary-speciﬁc ac-\ntions are performed. In some cases, hierarchy is inferred\non the basis of speciﬁc properties and rules (e.g. in the\nIAML MoP vocabulary, the hierarchy is taken from the let-\nters included in the last part of the URI). All the language\ntags are normalised in order to follow the ISO 639 stan-\ndard. Moreover, the indication of the use of Latin script\nis made explicit for transliterated labels in languages that\nuse different alphabets. In this phase, some interlinking\nto external datasets is performed, using SPARQL queries\n(DOREMUS dataset, MusicOntology keys vocabulary) or\nREST APIs like GeoNames [19].\n3.3 Vocabulary Alignments\nThe sets of vocabularies of musical genres and those of\nmedium of performance, described in Section 2.1, group\ntogether a number of well-established or internally used\nwithin a given institution reference lists. There is an im-\nportant overlap between the sets of entities (genres or mu-\nsical instruments) described across these vocabularies in\neach of the two categories. For example, the music genre\n“folk song” is described both in the IAML vocabulary (la-\nbelled by the French “chanson populaire” and the English\n“folk song”) and in the Radio France-hosted Diabolo vo-\ncabulary (labelled by “folksong”). The task of vocabulary\nalignment consists in automatically establishing links of\nidentity between the elements of two vocabularies from the\nsame category. This would allow to discover automatically\nthe equivalence between the two folk-song terms across\nIAML and Diabolo. Since our vocabularies are described\nin SKOS, the procedure comes down to discovering and\ndeclaring skos:exactMatch relations across the termsof two given vocabularies. In our example, this would re-\nsult in bounding the IAML and Diabolo identiﬁers10of\nthe folk-song genre in a skos:exactMatch relation.\nWe have proceeded to establish pairwise alignments\nbetween the concepts of the vocabularies in each of\nthese two categories (genres and MoP). We have cho-\nsen IAML as a target vocabulary for the alignments of\nthe genres-vocabularies, meaning that all remaining genre-\nvocabularies will be aligned to IAML. This decision is mo-\ntivated by the fact that this vocabulary is large in size and\nalso largely adopted in the librarian world. In the same line\nof thought, we have selected MIMO from the MoP family\nas a target for the alignments. This results in the perfor-\nmance of ﬁve pair-wise alignments for each category (all\nto IAML in the genre category and all to MIMO in the MoP\ncategory).\nFigure 1 . The overall alignment and expert-validation\nframework. An example with the genres family.\nThe overall alignment process consists of automatic\nalignment and manual validation and enrichment (see\nFigure 1). For the automatic alignment, we have taken a\nsimplistic string-based approach that relies on a compari-\nson between the labels of SKOS concepts by looking both\nat preferred and alternative labels, returning in output a\nconﬁdence score. Note that in many cases, we have lan-\nguage tags associated to the terms. However, there is no\nconsensus among the different vocabulary providers on the\nlanguage of origin of the terms of interest – in many cases a\nmusical instrument or a genre will be labelled as “French”\nin one vocabulary and “Italian” in another, although the la-\nbel originates in, say, Italian in both cases, simply because\nthe Italian word is commonly employed in French. For that\nreason, we have ignored the language tags when compar-\ning the labels. This process aims to generate a large pool of\nmapping candidates, ensuring high recall at this step. The\nalignments are stored in the standard EDOAL format,11\nwhich allows to keep the conﬁdence score of each aligned\npair of terms.\nIn order to guarantee high quality of the produced align-\nments and to improve precision, the automatically gener-\nated mappings are subjected to validation by the librar-\nian experts. In order to facilitate this laborious task, we\nhave developed a web application that allows to assist this\nprocess. The application has been conceived as a module\nof YAM++ online [3]—a multi-task web platform for on-\n10Respectively, http://data.doremus.org/vocabulary/\niaml/genre/fso and http://data.doremus.org/\nvocabulary/diabolo/genre/folksong\n11http://alignapi.gforge.inria.fr/edoal.htmlProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 427Figure 2 . The mapping validator interface.\ntology and thesaurus matching and validation,12although\ntheValidator interface can be seen as a standalone tool.\nIt takes as input a valid EDOAL alignment ﬁle, together\nwith its two OWL ontologies or SKOS vocabularies (via\nan URL or a ﬁle path). A list of mappings (pairs of la-\nbels of aligned concepts) appears on the main page, to-\ngether with information about the portions of the vocabu-\nlaries covered by the alignment (Figure 2, above). A con-\ntext description of each of the two concepts in each line is\ndisplayed (Figure 2, right), containing all alternate labels,\nas well as the labels (or URIs) of parents and children. The\nuser can take beneﬁt of the conﬁdence score of the previ-\nously computed alignments (shown at the right end of each\nline) for ﬁltering the pairs of concepts by the help of a hori-\nzontal cursor. For each concept pair, the expert is given the\npossibility to modify and select a relation type from a list\nof SKOS relations, or simply discard the mapping. Experts\nspotted out and corrected a number of invalid alignments\n(507 over 1022 for genres and 2039 over 3981 for MoP).\nIn particular for small differences in the label (i.e. plural\nforms) the role of the human validation is crucial for en-\nsuring quality in the vocabularies.\nThe expert is given the possibility to manually enrich\nthe proposed alignment by the help of the alignment en-\nrichment environment, accessible via the “Add new map-\npings” button (Figure 2, bottom left). A new page opens\ncontaining the full label lists of the two vocabularies. A\nkey-word search on both lists, including preferred and al-\nternate labels, allows to browse and select manually a pair\nof concepts and deﬁne their relation. The newly deﬁned\nmappings are added to the initial alignment. Finally, all\nmodiﬁcations are added to the alignment ﬁle, which can\nbe either saved in the default EDOAL format, or exported\nin the form of RDF/XML triples.\n4. USAGE OF VOCABULARIES\nThe availability of controlled vocabularies opens up new\npossibilities that involve the data conversion and usage. In\nthis section, we present recent work that aims to be com-\nplementary to the vocabulary publishing, providing further\ntools and resources to support their effectiveness.\n12http://yamplusplus.lirmm.fr4.1 String2Vocabulary\nA common task in what is called knowledge graph pop-\nulation (which is the generation of semantic triples start-\ning from differently structured data sources) is the passage\nfrom plain text nodes or literals to a more representative\nobject node or entity . Often, the target of this task consists\nin a set of vocabularies.\nAstring2uri algorithm – developed in the context of the\nDatalift platform [15] – performs an automatic mapping of\nstring literals to URIs coming from controlled vocabularies\nin SKOS. The software reads a RDF graph and searches for\nexact matches between literal nodes and vocabulary terms.\nSome experiences in knowledge base population of\nclassical music data, have shown up some critical points.\nOften the title of a classical work includes or, even more,\nconsists in the name of an instrument or a key or a genre\n(e.g. Ravel’s Bolero ), that should be excluded from the\nreplacement process and be kept as textual literals. More-\nover, the complexity itself of this data – involving an im-\nportant number of properties – in addition to the commonly\nused ﬁle formats (i.e. MARC), has led in the years to a cat-\naloguing practise particularly prone to editorial mistakes.\nThis is the case of musical keys declared as genre, or ﬁelds\nfor the opus number that contain actually a catalog number\nand vice-versa [10].\nFor these reasons, we adapted the Datalift strategy in a\nnew String2Vocabulary open-source library.13The soft-\nware uses the ﬁle name of vocabularies for grouping them\nin families: mop-mimo.ttl andmop-iaml.ttl are part of the\nfamily mop, while key.ttl is the sole member of the fam-\nilykey. This library accepts a conﬁguration ﬁle that as-\nsigns a family to a RDF property. For each input graph,\nit searches for the properties one after the other, retrieving\ntheir values. Each value is then compared to all the terms\nof the vocabulary, until it ﬁnds one equal to the value. All\nvariants for a concept label – namely skos:prefLabel\nandskos:altLabel – are considered in order to deal\nwith potential differences in naming terms, and both graph\nvalues and terms receive a normalisation that has the effect\nof removing the punctuation, lower-casing the text and de-\ncoding it to ASCII. Then, a substitution of that node with\n13https://github.com/DOREMUS-ANR/\nstring2vocabulary428 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018the found concept URI is performed.\nString2Vocabulary works both with literal values and\nwith entities labelled through rdfs:label . In the latter\ncase, the label to be matched against the vocabulary and the\nwhole node – with all its properties – is replaced. For max-\nimising the possibilities of selecting, if it exists, the right\nconcept, two searches are performed in sequence. The ﬁrst\nrequires that both the given text and language match with\nthe concept ones. If this search fails, a second one requires\na match excluding the language information.\nAs additional feature, the conﬁguration ﬁle allows to\nrequest the lemmatisation for certain vocabularies. Tak-\ning the MoP vocabulary as representative example, three\nsequential matches are tried: singularising the ﬁrst word\nof the label (for matching cases such as “cornets `a pis-\ntons”@fr ), singualising the whole label ( “sassofoni con-\ntralti”@it ) and leaving the label as is for matching instru-\nments that are always plural ( “cymbals”@en ).\n4.2 Music Embeddings\nWhat are the closest keys to C major ? Is it possible to de-\ncide which instrument between the cello and the oboe is\nmore similar to the clarinet ? The answer to those ques-\ntions would provide application in different ﬁelds, from\nmusicology studies to the development of specialised rec-\nommendation systems. The graph structure of RDF allows\nto deﬁne some kind of distance between two entities, by\nconsidering the number of nodes that separate them. Hier-\narchies and other kind of links between vocabulary terms\ncan be considered for computing this distance.\nNode2vec [5] is a state-of-the-art algorithm for comput-\ning entity embeddings. The algorithm computes random\nwalks in the graph following the links (edges) between\nnodes, computing the neighbourhood for each of them.\nEach edge can have a different weight , which affects the\nprobability that it participates to the walk. Through this\nmethod, the graph is mapped to a vector space, in which\nnodes becomes points represented by numeric vectors.\nA set of music embeddings for the concepts deﬁned\nin controlled vocabularies are being produced and pub-\nlished.14Two different graphs are considered:\n\u000fthe graph of vocabularies, which deﬁnes structural and\nsemantic connections between entities, such as hierar-\nchies, sameAs links, properties in common, speciﬁc\nmusic properties (i.e. relationships between keys);\n\u000fthe graph of usage, which includes all the usages of the\nvocabularies in the DOREMUS dataset. We considered\nmusical works for the genre and the key, castings and\nperformances for MoPs, composition and performance\nevents for functions.\nOn these two graphs, we computed the embeddings us-\ningnode2vec . We arbitrarily set to the graph of vocabular-\nies a weight 6 times bigger than the graph of usage in order\nto counterbalance the richly larger number of triples15and\n14https://github.com/DOREMUS-ANR/\nmusic-embeddings\n15More than 16 millions triples against around 100.000 ones for the\nvocabulary graph.\nFigure 3 . A 2D representation of the vector space of\nmedium of performance , with some recognisable clusters.\navoid to nullify the contribution of each one. After a post-\nprocessing step that removes all the literals and the extra\nnodes involved, a L2 normalisation is then applied in order\nto have values inf-1;+1g.\nIn order to appreciate the effectiveness of this strategy,\nwe used t-SNE [16] for visualising the embeddings on a\n2D image. As an example, Figure 316shows the vector\nspace of medium of performance . By observing the groups\nof closer entities, we can clearly identiﬁes clusters of in-\nstruments. It is interesting to observe that even if the hi-\nerarchy of the instrument families is preserved, the usage\ngraph strongly inﬂuenced the result, by reﬂecting the dif-\nferences of instruments in genres and periods. This is the\ncase of the orchestra instruments group, which puts the vi-\nolin closer to his orchestra colleague clarinet than to its\n15th-century relative tromba d’amore .\nFurther research is being conducted about the combina-\ntion of this embedding in more complex ones (artists and\nworks embeddings) in order to compute the similarity be-\ntween musical entities [9].\n5. CONCLUSION\nWe have presented a set of multilingual vocabularies for\nthe description of music-speciﬁc concepts using the Se-\nmantic Web framework. Two main contributions consist\nin the interconnection of already in-use vocabularies of\ngenres and medium of performance and the realisation of\npreviously-unreleased ones. We described our working\nstrategies as an interaction between editors and an auto-\nmatic system. A dereferencing library and a set of em-\nbeddings are presented as side works, allowing to identify\napplication beneﬁts coming from these vocabularies.\nThose vocabularies are intended to become references\nin the ﬁeld and we strongly encourage their reuse and\nadoption by the community at large in all their forms. Sev-\neral additional vocabularies are currently under develop-\nment, covering concepts like vocal techniques, types of\nwork, type of recording support or partitioning of musical\nworks. Once completed, these vocabularies will be pub-\nlished by following the procedures described in this paper.\n16A higher-resolution image is available at https://github.\ncom/DOREMUS-ANR/music-embeddings/tree/master/imgProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 429ACKNOWLEDGMENTS This work has been partially\nsupported by the French National Research Agency (ANR)\nwithin the DOREMUS Project, under grant number ANR-\n14-CE24-0020.\n6. REFERENCES\n[1] Manel Achichi, Pasquale Lisena, Konstantin Todorov,\nRapha ¨el Troncy, and Jean Delahousse. DOREMUS: A\nGraph of Linked Musical Works. In 17thInternational\nSemantic Web Conference (ISWC) , Monterey, Califor-\nnia, USA, 2018.\n[2] Alo Allik, Florian Thalmann, and Mark Sandler. Mu-\nsiclynx: Exploring music through artist similarity\ngraphs. In Companion Proceedings of the The Web\nConference 2018 , pages 167–170, Lyon, France, 2018.\n[3] Zohra Bellahsene, Vincent Emonet, Duyhoa Ngo, and\nKonstantin Todorov. YAM++ Online: A Web Plat-\nform for Ontology and Thesaurus Matching and Map-\nping Validation. In Extended Semantic Web Confer-\nence, P&D , pages 137–142. Springer, 2017.\n[4] Beat Estermann. “OpenGLAM” in Practice–How Her-\nitage Institutions Appropriate the Notion of Open-\nness. In 20thInternational Research Society for Public\nManagement Conference (IRSPM) , pages 13–15, Hong\nKong, China, 2016.\n[5] Aditya Grover and Jure Leskovec. node2vec: Scal-\nable feature learning for networks. In Proceedings of\nthe 22ndACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining , 2016.\n[6] Kelli Ham. Openreﬁne (version 2.5). http://openreﬁne.\norg. free, open-source tool for cleaning and transform-\ning data. Journal of the Medical Library Association:\nJMLA , 101(3):233, 2013.\n[7] Bernhard Haslhofer and Antoine Isaac.\ndata.europeana.eu: The europeana linked open\ndata pilot. International Conference on Dublin Core\nand Metadata Applications , 0:94–104, 2011.\n[8] Sefki Kolozali, Mathieu Barthet, Gy ¨orgy Fazekas, and\nMark B Sandler. Knowledge Representation Issues in\nMusical Instrument Ontology Design. In 12thInterna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 465–470, 2011.\n[9] Pasquale Lisena and Rapha ¨el Troncy. Combining mu-\nsic speciﬁc embeddings for computing artist similar-\nity. In 18thInternational Society for Music Information\nRetrieval Conference (ISMIR), Late-Breaking Demo\nTrack , Suzhou, China, 2017.\n[10] Pasquale Lisena, Rapha ¨el Troncy, Konstantin Todorov,\nand Manel Achichi. Modeling the complexity of music\nmetadata in semantic graphs for exploration and dis-\ncovery. In Proceedings of the 4thInternational Work-\nshop on Digital Libraries for Musicology (DLfM) ,\npages 17–24, Shanghai, China, 2017.[11] Alistair Miles and Jos ´e R P ´erez-Ag ¨uera. SKOS: Sim-\nple knowledge organisation for the web. Cataloging &\nClassiﬁcation Quarterly , 43(3-4):69–83, 2007.\n[12] Sergio Oramas, Oriol Nieto, Mohamed Sordo, and\nXavier Serra. A deep multimodal approach for cold-\nstart music recommendation. In 2ndWorkshop on Deep\nLearning for Recommender Systems, at RecSys 2017 ,\nComo, Italy, 2017.\n[13] Sergio Oramas, Vito Claudio Ostuni, Tommaso Di\nNoia, Xavier Serra, and Eugenio Di Sciascio. Sound\nand music recommendation with knowledge graphs.\nACM Transactions on Intelligent Systems and Technol-\nogy (TIST) , 8:1–21, 2016.\n[14] Yves Raimond, Samer A. Abdallah, Mark B. Sandler,\nand Frederick Giasson. The music ontology. In 15th\nInternational Conference on Music Information Re-\ntrieval (ISMIR) , pages 417–422, 2007.\n[15] Franc ¸ois Scharffe, Ghislain Atemezing, Rapha ¨el\nTroncy, Fabien Gandon, Serena Villata, B ´en´edicte\nBucher, Fayc ¸al Hamdi, Laurent Bihanic, Gabriel\nK´ep´eklian, Franck Cotton, et al. Enabling linked-data\npublication with the datalift platform. In AAAI work-\nshop on semantic cities , 2012.\n[16] Laurens van der Maaten and Geoffrey Hinton. Visu-\nalizing data using t-sne. Journal of machine learning\nresearch , 9(Nov):2579–2605, 2008.\n[17] Melanie Wacker, Jan Ashton, Ann Caldwell, Ray De-\nnenberg, Angela Di Iorio, Rebecca Guenther, Myung-\nJa Han, Sally McCallum, Tracy Meehleib, Stefanie\nRhle, and Robin Wendler. Metadata Object Description\nSchema (MODS). Speciﬁcation, Library of Congress’\nNetwork Development and MARC Standards Ofﬁce,\n2013.\n[18] David M. Weigl and Kevin R. Page. A framework\nfor distributed semantic annotation of musical score:\n”take it to the bridge!”. In 18thInternational Society\nfor Music Information Retrieval Conference (ISMIR) ,\nSuzhou, China, 2017.\n[19] Mark Wick and Bernard Vatant. The geonames geo-\ngraphical database, 2012.430 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Vocal Melody Extraction with Semantic Segmentation and Audio-symbolic Domain Transfer Learning.",
        "author": [
            "Wei Tsung Lu",
            "Li Su 0004"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492467",
        "url": "https://doi.org/10.5281/zenodo.1492467",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/286_Paper.pdf",
        "abstract": "The melody extraction problem is analogue to semantic segmentation on a time-frequency image, in which every pixel on the image is classified as a part of a melody object or not. Such an approach can benefit from a signal processing method that helps to enhance the true pitch contours on an image, and, a music language model with structural information on large-scale symbolic music data to be transfer into an audio-based model. In this paper, we propose a novel melody extraction system, using a deep convolutional neural network (DCNN) with dilated convolution as the semantic segmentation tool. The candidate pitch contours on the time-frequency image are enhanced by combining the spectrogram and cepstral-based features. Moreover, an adaptive progressive neural network is employed to transfer the semantic segmentation model in the symbolic domain to the one in the audio domain. This paper makes an attempt to bridge the semantic gaps between signal-level features and perceived melodies, and between symbolic data and audio data. Experiments show competitive accuracy of the proposed method on various datasets.",
        "zenodo_id": 1492467,
        "dblp_key": "conf/ismir/LuS18",
        "keywords": [
            "melody extraction problem",
            "analogue to semantic segmentation",
            "time-frequency image",
            "signal processing method",
            "enhance true pitch contours",
            "large-scale symbolic music data",
            "transfer into audio-based model",
            "novel melody extraction system",
            "deep convolutional neural network",
            "dilated convolution"
        ],
        "content": "VOCAL MELODY EXTRACTION WITH SEMANTIC SEGMENTATION\nAND AUDIO-SYMBOLIC DOMAIN TRANSFER LEARNING\nWei-Tsung Lu and Li Su\nInstitute of Information Science, Academia Sinica\ns603122001@gmail.com, lisu@iis.sinica.edu.tw\nABSTRACT\nThe melody extraction problem is analogue to semantic\nsegmentation on a time-frequency image, in which every\npixel on the image is classiﬁed as a part of a melody object\nor not. Such an approach can beneﬁt from a signal process-\ning method that helps to enhance the true pitch contours on\nan image, and, a music language model with structural in-\nformation on large-scale symbolic music data to be trans-\nfer into an audio-based model. In this paper, we propose\na novel melody extraction system, using a deep convolu-\ntional neural network (DCNN) with dilated convolution as\nthe semantic segmentation tool. The candidate pitch con-\ntours on the time-frequency image are enhanced by com-\nbining the spectrogram and cepstral-based features. More-\nover, an adaptive progressive neural network is employed\nto transfer the semantic segmentation model in the sym-\nbolic domain to the one in the audio domain. This pa-\nper makes an attempt to bridge the semantic gaps between\nsignal-level features and perceived melodies, and between\nsymbolic data and audio data. Experiments show compet-\nitive accuracy of the proposed method on various datasets.\n1. INTRODUCTION\nMelody extraction of polyphonic music has been ac-\ncounted a key towards bridging the semantic gap in music\nprocessing, as melody is an intermediate object that corre-\nlates to both low-level signal attributes such as pitch and\nhigh-level semantics, i.e. the difference between melody\nand accompaniment, of music [3, 12, 29]. However, it is\nchallenging because the notion of melody is complicated\nby two levels of information extraction and data modali-\nties. For information extraction, both pitch detection and\nsemantic segmentation levels are required to specify the\nposition and shape of a melody out of other pitch con-\ntours in a time-frequency representation. As to data modal-\nities, the problem arises from the difference of melody-\nrelated features between the composed data (e.g., sym-\nbolic data such as MIDI) and the performed data (e.g., au-\ndio data): the former provides structural information such\nc\rWei-Tsung Lu and Li Su. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Wei-Tsung Lu and Li Su. “V ocal melody extraction with seman-\ntic segmentation and audio-symbolic domain transfer learning”, 19th In-\nternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.as voiced/unvoiced segments and chord/non-chord notes,\nwhile the latter provides interpretational information such\nas sliding and vibrato. Both kinds of information are es-\nsential for accurately identifying the melody pitch contour.\nWe perform vocal melody extraction using semantic\nsegmentation techniques. Semantic segmentation parti-\ntions an image into semantically meaningful objects with\nprecise boundaries. Rendered as a pixel-wise classiﬁca-\ntion problem and able to be implemented by an encoder-\ndecoder network with 2-D convolutional feature mappings,\nit brings great success in computer vision [6,7,14,25]. Se-\nmantic segmentation also makes a breakthrough in solving\nthe source separation problem in music processing [17],\nwhich analogously needs to resolve components coexist-\ning in a time-frequency image. In this work, a deep con-\nvolutional neural network (DCNN) is adopted with dilated\nconvolution for semantic segmentation as it achieves better\nperformance in multi-resolution images.\nTo fully utilize the advance of semantic segmentation\nin vocal melody extraction, we further attend to the afore-\nmentioned issues, pitch detection and multiple data modal-\nities, both of which are absent from typical image-based\nsemantic segmentation. For pitch detection, we notice that\nwhen performing melody extraction with semantic seg-\nmentation, the spectrogram is usually suboptimal since\nit captures the harmonic peaks and information unrelated\nto the melody, which accounts for one of the major er-\nrors among all the melody extraction methods. This issue\nis addressed by modifying the spectrogram with cepstral-\nfeatures, which results in a novel time-frequency represen-\ntation that enhances the true pitch contour while also sup-\npresses harmonic contours [26, 32].\nThe modality difference between symbolic and audio\ndata is relatively less noticed in melody extraction. We\naddress this issue with transfer learning: we ﬁrst train\na melody extraction model with symbolic data, and the\nmodel parameters are then reused in the vocal melody ex-\ntraction model trained with audio data. In this way, the\nsymbolic-based model assists in music language modeling\nthat audio-based models may fall short of. Incorporating\nsymbolic music data is of great potential to mitigate the\ndata scarcity problem, since building a symbolic dataset\nwith melody annotations is much easier than building an\naudio one, and it is also very straightforward to perform\ndata augmentation on symbolic data. In this work, we\nadopt the progressive neural network (PNN) [1], a network\nstructure providing cross-domain network parameter shar-521ing to accomplish symbolic-audio transfer learning task.\nTo sum up, this paper attempt to apply image seman-\ntic segmentation to vocal melody extraction, forming a\nsystematic method to perform singing voice activity de-\ntection, pitch detection and melody extraction all at the\nsame time. This segmentation method gives competitive\nresults on pitch accuracy, and even works unprecedentedly\nwell on singing voice activity detection compared to other\ndeep-learning-based methods. With the integration with\nthe PNN, we leverage large-scale symbolic data to train the\nmodel, and attain similar performance to the segmentation\nmethod with less training time.\n2. RELATED WORK\nMelody extraction of polyphonic music has been widely\ninvestigated with various signal processing and machine\napproaches. Recent works using convolution neural net-\nworks (CNNs) or recurrent neural networks (RNNs) are\nmostly classiﬁcation-based, where the output is the frame-\nlevel likelihood score of every pitch at a time instance\n[4,22,27,30,37]. [2] adopts a fully convolution neural net-\nwork and output a salience representation at the song level.\nAdvanced semantic segmentation networks such as the U-\nnet [28] have been utilized in source separation [17] and\nshows high potential in melody extraction.\nMost of the melody extraction studies focus on the sig-\nnal processing level, possibly because signal-level charac-\nteristics such as slides and vibrato are still the principal fac-\ntors in recognizing a melody contour. In contrast, melody\nextraction on symbolic data is rarely discussed in the liter-\nature. Although not the main topic of this work, we man-\nage to pose the problem of symbolic melody extraction and\nemphasize its importance in music language modeling for\ncross-domain transfer learning.\nPrevious works on transfer learning for music informa-\ntion retrieval mostly aim under the same type of input data\nrepresentation [8,13]. Contrararily, transfer learning across\nthe data from different domains , such as adapting a model\nlearned from symbolic data to another learned from audio\ndata, is relatively less discussed. Previous works dealing\nwith cross-domain data mainly focus exploring audio-to-\nMIDI or audio-to-sheet correspondence [10, 11].\n3. METHOD\nAn overview of the proposed model is shown in Fig.1. The\nmodel contains a feature extractor which computes the au-\ndio data representation and a PNN which consists of two\nsegmentation models, with one trained on the symbolic\ndata and the other on the audio data. The ﬁlter is for di-\nmension reduction of the audio representation to ﬁt the\nsymbolic segmentation model in the PNN. Details of the\nmodel are discussed below.\n3.1 Audio data representation\nIn music processing, designing a data representation suit-\nable for the machine learning models to better identify and\ncapture the information of interest can help signiﬁcantly\nFigure 1 : The system diagram of the proposed method.\nimprove the performance [18]. In the task of pitch de-\ntection in polyphonic music, related methods include the\nfeature scaling [18], the harmonic constant-Q transform\n(HCQT) that combines the CQTs based on different oc-\ntave numbers [2], the combined frequency and periodicity\n(CFP) representation that intergrates a temporal or spectral\nrepresentation with its Fourier dual [26, 32, 34], and oth-\ners. All of these methods are designed to emphasize the\nsaliency of pitch contours in the music signal.\nWe adopt the data representation used in [33], which\nhas been shown effective in enhancing the true pitch com-\nponents of polyphonic signals. The adopted data repre-\nsentation is essentially the product of a generalized cep-\nstrum (GC), a classical time-based pitch detection func-\ntion [16,20,21,35,36], and a generalized cepstrum of spec-\ntrum (GCoS), a modiﬁed spectrum lying in the frequency\ndomain [32]. The GC and GCoS are complementary: a\nGCoS reveals the presence of a pitch object by its funda-\nmental frequency ( f0) and harmonics ( nf0), while a GC\nreveal it by its f0and sub-harmonics ( f0=n) [26, 32, 34].\nBy simply multiplying GC by GCoS, we effectively sup-\npress the harmonic and sub-harmonic peaks, and at the\nsame time localize a pitch object.\nThe GC and GCoS are both computed by the discrete\nFourier transform (DFT) and nonlinear activation func-\ntions. Consider an input signal x:=x[n]wherenis the\nindex of time. Let the magnitude of the short-time Fourier\ntransform (STFT) of xbeX. Given anN-point DFT ma-\ntrixF, high-pass ﬁlters WfandWtfor eliminating the\nDC terms, and activation functions \u001bi, the power-scaled\nspectrogram, GC and GCoS are represented as:\nZS[k;n] :=\u001b0(WfX); (1)\nZGC[q;n] :=\u001b1\u0000\nWtF\u00001ZS\u0001\n; (2)\nZGCoS[k;n] :=\u001b2(WfFZ GC); (3)\n\u001bi(Z) =jrelu(Z)j\ri; i = 0;1;2 (4)\nwhere relu (\u0001)represents a rectiﬁed linear unit, j\u0001j\r0is an\nelement-wise root function, and we choose (\r0;\r1;\r2) =\n(0:24;0:6;1)for a feature scaling in the power scale [32].\nBesides, to ﬁt the perceptive scale of musical pitches,\nZGCandZGCoS are mapped onto the log-frequency scale,\nby88\u00034 = 352 triangular ﬁlters ranging from 27.5 Hz522 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018(A0) to 4487 Hz , with 48 bands per octave. The GC and\nGCoS after the ﬁlterbank are then both on the pitch scale,\nas denoted by ~ZGCand~ZGCoS. The ﬁnal 2-D data repre-\nsentation for semantic segmentation is\nC[p;n] =~ZGC[p;n]~ZGCoS[p;n]; (5)\nwherepis the index on the log-frequency scale. The audio\nﬁles are resampled at 16 kHz and merged into one mono\nchannel. Data representations are computed with a Hann\nwindow of 2048 samples. The hop size is 320 samples, and\ntherefore the time step is 20ms. The upper two subplots of\nFigure 4 illustrate a comparison between the spectrogram\nandC. We can observe that with the aid of cepstral feature,\nthe unwanted harmonic peaks are highly suppressed in C.\n3.2 Semantic segmentation\nThe proposed segmentation model for vocal melody ex-\ntraction is mainly based on the DeepLabV3 and its im-\nproved version, DeepLabV3+ [6,7], which are the state-of-\nthe-art models for semantic segmentation tasks. The model\nis a fully convolution neural network with an encoder-\ndecoder architecture. The encoder is implemented by a\nResNet [15], followed by an atrous spatial pyramid pooling\nprocess, and a decoder implemented by stacks of decoder\nblocks, as shown in Figure 2.\nOne major utility in DeepLabV3 is the use of dilated\nconvolution, which can be represented as a generalized\nversion of the standard convolution as follows:\ny[i] =X\nkx[i+r\u0001k]w[k] (6)\nwhere xandydenotes the input and output 2-D feature\nmaps, respectively, wis the convolution ﬁlter and iin-\ndicates the locations on the feature maps. The number r\nis the dilated rate which determines the stride with which\nthe input are sampled and standard convolution is a spe-\ncial case when r= 1. To capture the context in differ-\nent ranges, one can apply dilated convolution with differ-\nent values of ron the same input feature map parallely,\ncalled Atrous Spatial Pyramid Pooling (ASPP) in [6]. The\noutputs of these parallel convolution operations are then\nconcatenated to provide information collected from vari-\nous scales, as shown in Figure 2c.\nDifferent from normal image segmentation task that\ntarget objects usually holds certain area compared to the\nwhole image, the melody part of music occupies only a\nsmall portion and appears as thin lines when visualized in\na 2-D image. To overcome this difﬁculty, We proposed two\nmodiﬁcations to improve the performance of the model.\nFirst, the decoder module in DeepLabV3, which is\noriginally an up-sampling operation, is replaced by stacks\nof convolution and transpose convolution layers for ﬁne-\ngrained outputs. It is shown in [7] that by doing this, the\nsmall and detailed objects in an image can be better recog-\nnized. Also, better performance is achieved by introducing\nthe U-net [28] structure, which lets the output from each\nlayer of the encoder be concatenated to the corresponding\nblock of the decoder. This idea is also mentioned in [7].\n(a)\n (b)\n(c)\nFigure 2 : Model descriptions. (a) The overall structure of\nthe segmentation model. (b) The encoder block. The stride\nrate in Stride Conv is (2,2). Stride Conv can be replaced\nwith standard convolution so it allows more layers in the\nencoder. It can also be changed to transpose convolution\nwith stride (2,2), so the block can serve as a decoder block.\n(c) The Atrous Spatial Pyramid Pooling unit.\nSecond, we adopt the focal loss [23] as the loss func-\ntion for the proposed model, in order to solve the class im-\nbalance problem, where the negative labels, i.e., the time-\nfrequency pixels corresponding to accompaniment and si-\nlence parts, could dominate in the input feature and thus\naffect the performance. The focal loss is represented as:\nFL(pt) =\u0000\u000bt(1\u0000pt)\rlog(pt); (7)\nwhereptdenotes the model’s estimated probability for an\ninput to be classiﬁed to class t,\u000bt2[0;1]is a weighting\nfactor for balancing the importance of positive and neg-\native examples and the term (1\u0000pt)\racts as a modu-\nlating factor with \rcontrolling the rate at which domi-\nnant examples are down-weighted. Following [23], we set\n\u000bt= 0:25,\r= 2in this work.\n3.3 Domain adaptation\nMost of the existing deep learning models require a large\namount of training data to reach good performance. How-\never, annotating melody pitch contours on audio data pre-\ncisely is quite challenging; it is labor-intensive and also\nneeds strong expertise in music. Recent attempts to ad-Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 523dress the issue of data scarcity mostly focus on weakly su-\npervised learning [17, 24, 31].\nIn this work, we consider the potential of domain-\nadaptive transfer learning, which incorporates the informa-\ntion in MIDI data to assist in training the audio melody\nextraction model. The primary motivation for using MIDI\nﬁles is the capability of data augmentation: one can use\nMIDI ﬁles to easily create large-scale symbolic dataset\nwith detailed and precise notations. Besides, the symbolic\ndata also present some musical characteristics clearer than\nthe audio data do. This therefore gives more insights to the\nmusic language modeling, such as musical structures and\nphrases. Moreover, the space efﬁciency of symbolic data\nalso allows more training examples than audio data given\nthe same memory resource.\nTo discuss transfer learning between audio and sym-\nbolic data, we ﬁrst discuss the difference in their data for-\nmats. One difference is the pitch resolution, which is 0.25\nsemitones in the audio data (i.e., 48 bins per octave), and 1\nsemitone in the symbolic data; this results in the difference\nof dimension between the audio and the symbolic data. As\nfor the time resolution, there are some more ﬂexible ways\nto deﬁne it. Therefore, we consider two types of time res-\nolution for the symbolic data: the ﬁrst is time-based res-\nolution with its unit length in time (e.g., 20 ms), and the\nsecond is note-based resolution with its unit length in note\nname (e.g., a 32nd note). Both the symbolic and audio\ndata can be represented in time-based resolution. Symbolic\ndata can also be represented in a more musically informa-\ntivenote-based resolution since obtaining beat and tempo\ninformation in symbolic data is more straightforward.\nTo achieve domain-adaptive transfer learning for two\ndifferent domains, we adopt the progressive neural net-\nwork (PNN) [1], in which an adapter network (see\nFigure 3) is designed to make one network connected to\nanother in different domains, regardless of the difference\nin data dimension. In the general scenario of PNN, multi-\nple networks trained on various tasks are connected layer-\nto-layer in parallel through the adapters, so the trained net-\nworks can transfer the previously learned knowledge into a\nnew task and to accelerate the training speed or to improve\nthe performance of the new task.\nIn our melody extraction method, we ﬁrst trained a seg-\nmentation model using the symbolic dataset. We connect\nthe symbolic segmentation model to another segmentation\nmodel, and the latter model is then trained on the audio\ndataset, with the parameters in the symbolic segmenta-\ntion model frozen. In the testing phase, the input audio\nrepresentation is fed into both of the segmentation mod-\nels. To make the dimension of audio representation match\nthe symbolic segmentation model, a triangular ﬁlterbank is\nused to map the pitch resolution from 0.25 to 1 semitone,\nas illustrated in Figure 1.\nThe adapter between two models in the PNN is illus-\ntrated in Figure 3. It modiﬁes the dimension of the inter-\nlayer outputs and make such information be propagated\nahead. In the proposed method, transpose convolution lay-\ners are adopted for the adapter networks, since transpose\nFigure 3 : The connection between the two networks in\nthe proposed model. The parameter of the i-th layer in\nthe symbolic model is ﬁrst fed into the adapter, and then\nconnected to the (i+ 1) -th layer of the audio model with\nan addition operation.\nconvolution can up-sample the output of the symbolic rep-\nresentation (with lower pitch resolution) in order to ﬁt the\naudio representation (with higher pitch resolution).\n3.4 Inference\nSince the segmentation model only allows a limited range\nof input at one time, to perform melody extraction on a\ngiven score, we slide a window along the score and then\nsuperpose all the resulting matrices. The analysis window\nwith a ﬁxed dimension is shifted from one time-step to an-\nother. As to the beginning and ending time, we pad the\nscore with zeros for it captures the process in which in-\nformation feeds only the last column then gradually ﬁlling\nup all the columns in the beginning, and gradually leaving\nthe window column by column at the end. After the pro-\ncess above, the segmentation output is a superposed im-\nage representing the salience of vocal melody in the time-\nfrequency plane. We then ﬁnd the max value for each col-\numn of the image and set all the other elements to zero,\ni.e., unvoiced. Finally, the elements smaller than the aver-\nage of each column’s maximum are also set to zero, and\nthe remaining non-zero elements is considered as voiced.\n3.5 Implementation details\nThe models are implemented using the Keras [9] library\nwith tensorﬂow as the back end. The width of the input\nwindow equals 128 timesteps, and for computational con-\nvenience, we pad the dimension of pitch from 88 to 128,\nand 352 to 384 for the symbolic and audio data, so the in-\nput dimension will be (128;128;1)and(128;384;1)for\nthe symbolic and audio model, respectively. As shown in\nFig.1, the input feature will ﬁrst be passed into a 29-layer\nencoder based on Resnet. Then, the output from the en-\ncoder which is 16 times smaller than the original input\nwill be fed into the ASPP unit. Finally, a decoder which\ncontains 4 decoder blocks will up-sample the dense fea-\ntures to the original shape by transpose convolutional lay-\ners with strides equal (2;2). The output dimension will\nbe(128;128;2)and(128;384;2)for the symbolic and\naudio model, respectively, with the ﬁrst channel indicat-\ning the presence melody and the other is for non-melody.524 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 4 : Data representation and melody extraction re-\nsults of the ﬁrst 15s of ’train06.wav’ in MIREX2005 as\ninput. From top to bottom: power-scale spectrogram, data\nrepresentation C, the result using segmentation, and the\nresult using segmentation and note-based PNN.\nThe superposition in the inference process is performed on\nthe ﬁrst channel. To implement the PNN, two segmen-\ntation networks with same structure are connected using\nthe adapters which is composed of transpose convolution\nlayer. These connections happen in layers with dimen-\nsion changing. Batch normalizations are applied after each\nactivations, and a dropout rate of 30% is added after the\nbatch normalizations. ADAM [19] is used for optimiza-\ntion. Source codes can be found at https://github.\ncom/s603122001/Vocal-Melody-Extraction .\n4. EXPERIMENT\n4.1 Data\nThe training data for the audio comes from two datasets,\none is the MIR1K1, which contains 1000 Chinese karaoke\nclips, another is MedleyDB [5], where 48 songs with vo-\ncal tracks are included. The total dataset contains about 3\nhours of audio and without data augmentation.\nA MIDI corpus contains 600 folk songs with a melody\ntrack is used as the training data for the symbolic model.2\nIn the training process, we perform data augmentation, by\npitch-shifting each song up and down by at most 6 semi-\ntones in order to cover all possible keys. In addition, half\nof the pieces in the dataset are modiﬁed by shifting the\nmelody by one octave down. As a result, we produce 7673\npieces of symbolic training data. The pieces in the dataset\nare represented in two different formats. One is the time-\nbased with 20 ms length in each time step and the other is\n1https://sites.google.com/site/unvoicedsoundseparation/mir-1k\n2https://goo.gl/aPgzrWthenote-based that each time step equals a thirty-second\nnote. Due to limited computational resources, we only\nuse 2048 pieces when training the time-based model since\ntime-based data is space consuming.\nThe testing data are from three datasets: ADC2004,\nMIREX05,3and MedleyDB. As the proposed model is\ntrained solely for singing voice melody, we follow [22] and\nselect only samples having melody sung by human voice\nfrom ADC2004 and MIREX05. As a result, 12 clips in\nADC2004 and 9 clips in MIREX05 are selected. To ob-\ntain the annotation of singing voice in medleyDB, 12 songs\nhaving singing voice included in their ‘MELODY2’ anno-\ntations are selected. The vocal melody labels are obtained\nfrom the MELODY2 annotations occurring in the inter-\nvals labeled by ‘female singer’ or ‘male singer’. These\n12 songs are not included in the training data.\n4.2 Experiment setting\nTo assess the performance of semantic segmentation and\nthe effects of transfer learning on vocal melody extraction,\nwe experiment on the following three different settings:\n1)Segmentation : using simply the audio-level semantic\nsegmentation model. This audio-only semantic segmenta-\ntion model is trained on the MIR1K dataset.\n2)Segmentation with note-based progressive neural\nnetwork (Seg + note PNN) : using both the audio-level and\nsymbolic-level segmentation models. The symbolic seg-\nmentation model is ﬁrst trained using the note-based sym-\nbolic dataset, then this model is incorporated into the train-\ning stage of the audio segmentation model with the PNN.\n3)Segmentation with time-based progressive neural\nnetwork (Seg + time PNN) : similar to 2), while the symbo-\nlic model in trained with the time-based symbolic dataset.\nWe compare the above-mentioned models with three\nbaseline methods in deep learning approaches: the multi-\ncolumn DNN (MCDNN) [22], the patch-based CNN\n(pathc-CNN) [33], and the deep salience map (DSM), for\nwhich on-line source code with the vocal option is avail-\nable [2]. Since the detection results of DSM are sensitive\nto the thresholding parameter, the parameter is tuned from\n0 to 0.9 for all datasets to ﬁnd the optimal value for bet-\nter comparison. The resulting optimal threshold th=0.1 is\nused in the experiment.\nThe performance metrics include overall accuracy\n(OA), raw pitch accuracy (RPA), raw chroma accuracy\n(RCA), voice recall (VR) and voice false alarm (VFA);4\nall these metrics are computed from the mir eval standard\nwith the tolerance of pitch detection being 50 cents.\n4.3 Result\nTable 1 lists the performance metrics of all the proposed\nmethods together with the baselines on the three testing\ndatasets. Among the three proposed models, Segmentation\noutperforms the other two PNN-based models in terms of\nOA for all datasets except MedleyDB, where Segmenta-\ntion performs on par with Seg + note PNN . Through the\n3https://labrosa.ee.columbia.edu/projects/melody/\n4http://www.music-ir.org/mirex/wiki/2016:Audio Melody ExtractionProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 525Method OA RPA RCA VR VFA\nSegmentation 74.9 71.7 74.8 73.8 3.0\nSeg + note PNN 73.5 70.2 73.2 72.2 3.1\nSeg + time PNN 73.2 70.4 72.9 73.2 5.4\nMCDNN [22] 73.1 75.8 78.3 88.9 41.2\nPatch-CNN [33] 72.4 74.7 75.7 90.1 41.3\nDSM [2] 70.8 77.1 78.8 92.9 50.5\n(a) ADC2004 (vocal)\nMethod OA RPA RCA VR VFA\nSegmentation 85.8 82.2 82.9 87.3 7.9\nSeg + note PNN 84.5 79.6 80.3 84.7 6.9\nSeg + time PNN 84.8 82.3 83.0 87.3 9.9\nMCDNN 68.4 76.3 77.4 87.0 49.0\nPatch-CNN 74.4 83.1 83.5 95.1 41.1\nDSM 69.6 76.3 77.3 93.6 42.8\n(b) MIREX2005 (vocal)\nMethod OA RPA RCA VR VFA\nSegmentation 70.0 68.3 70.0 77.9 22.4\nSeg + note PNN 70.0 67.1 68.7 77.0 21.5\nSeg + time PNN 69.1 67.4 69.0 78.7 23.6\nPatch-CNN 55.2 59.7 63.8 78.4 55.1\nDSM 66.2 72.0 74.8 88.4 48.7\n(c) MedleyDB (vocal)\nTable 1 : V ocal melody extraction results of the proposed\nmethods and other methods on various datasets. The pro-\nposed methods are: segmentation, segmentation with note-\nbased progressive neural network (Seg + note PNN), and\nsegmentation with time-based progressive neural network\n(Seg + time PNN).\nmelody extraction accuracies of the segmentation model\nare not improved by introducing the PNN structure, there\nis still a notable improvement when comparing training ef-\nﬁciency. In fact, it takes 6 epochs for Segmentation to con-\nverge, but Seg + note PNN reach similar performance with\nonly 2 epochs of training. Therefore, introducing the PNN\nimproves the training speed.\nOne reason why PNN does not improve the accuracy\nis related to the symbolic dataset we are using: the sym-\nbolic data contains only one style of music and turns out\nto be of low diversity. Another reason is the lack of in-\ntensity labels in symbolic data. Our pilot study indicated\nthat a segmentation model trained on symbolic data may\nresult in high RCA and RPA but also relatively high VFA.\nHowever, a segmentation model trained on the audio data\ngives inverse results, with low VFA, as shown here. This\nmight have something to do with the sound intensity in\nthe audio signal, which is an important sign for to deter-\nmine the present of melody. However, our symbolic data\ndo not have such labels on intensity. Model training with\na larger symbolic music dataset with higher diversity and\nwith MIDI velocity labels are for future investigation.\nThe two PNN-based methods, Seg + note PNN andSeg\n+ time PNN , achieve similar OA, while the former modelhas lower VFA. This implies that the performance of the\nsymbolic model trained with note-based symbolic data is\nbetter than training with time-based data. One reason may\nbe that compiling symbolic data in time-based resolution\nmay result in the ambiguity of musical information; in\ntime-based data, the same type of note may have differ-\nent lengths in time due to different tempi among the music\npieces. This could affect the model capability in learning\nthe musical structure.\nComparing the proposed Segmentation model to the\nbaseline methods, we observe that Segmentation out-\nperforms all of them in terms of OA. Particularly, in\nMIREX2005, Segmentation achieves an OA at 85.8%, a\nhigh accuracy outperforming DSM by 16.2%, patch-CNN\nby 11.4% and MCDNN by 17.4%. In other two datasets,\nSegmentation also outperforms other methods by around\n1\u00184% in terms of OA. These experiment results reveal\nthe competitiveness of the proposed semantic segmenta-\ntion method in audio melody extraction. On the other hand,\nwhen focusing on the pitch accuracy (i.e., RPA and RCA),\nDSM is still competitive among all.\nThe high OA of Segmentation is mainly resulted from\nthe excellent performance of VFA with the semantic seg-\nmentation approach. Among all methods and datasets,\nthe proposed methods signiﬁcantly outperform the base-\nline methods by a 20-40% reduction in VFA. In ADC2004,\nSegmentation further achieves a low VFA of 3.0%. This\nimplies that the proposed melody extraction method itself\nis highly robust to non-vocal interference, and is without\nthe need of a voice activity detector [27]. In other words,\nthe semantic segmentation model with fully convolutional\nlayers itself behaves as a melody pitch classiﬁer and a\nvoice activity detector at the same time.\nFinally, the lower two subplots in Figure 4 illustrate two\nmelody extraction results using Segmentation without and\nwith a note-based PNN. Both methods perform well in seg-\nmenting the main melody part from the representation C\nshown in second subplot in Figure 4. This example also\ndemonstrates one part that using the note-based PNN does\nwell: in the lowest subplot, the Seg + note PNN method\nwell detects the unvoiced part between the 6th and the 9th\nsecond, in which the Segmentation method regards the ex-\ntended instrument part as melody.\n5. CONCLUSION\nWe proposed a melody extraction method utilizing the se-\nmantic segmentation model, the input combining spectral\nand cepstral representations, and domain-adaptive transfer\nlearning. Experiments using a low-diversity training data\nindicate the competitiveness of the segmentation model\nwith the data representation, especially in reducing voice\nfalse alarm. Incorporating large-scale symbolic data pro-\nvides better efﬁciency and exhibits potential in enhancing\ncontextual information. Future work will focus on the im-\nprovement of domain adaption. Note-level segmentation\ncan be considered as a future work as it is also feasible\napplying symbolic-audio transfer learning and would also\nbeneﬁt the melody extraction task.526 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20186. ACKNOWLEDGEMENT\nThe authors would like to thank the anonymous reviewers\nfor their valuable comments and suggestions to improve\nthe quality of the paper. This work is partially supported\nby MOST Taiwan, under the contract MOST 106-2218-E-\n001-003-MY3.\n7. REFERENCES\n[1] R. Andrei A., R. Neil C., D.Guillaume, S. Hubert,\nK. James, K. Koray, P. Razvan, and H. Raia. Progres-\nsive neural networks. eprint arXiv:1606.04671 , 2016.\n[2] R. M. Bittner, B. McFee, J. Salamon, P. Li, and J. P.\nBello. Deep salience representations for f0estima-\ntion in polyphonic music. In 18th Int. Soc. for Music\nInfo. Retrieval Conf. , Suzhou, China, Oct. 2017.\n[3] R. M. Bittner, J. Salamon, J. J. Bosch, and J. P. Bello.\nPitch contours as a mid-level representation for music\ninformatics. In Audio Engineering Society Conference:\n2017 AES International Conference on Semantic Au-\ndio. Audio Engineering Society, 2017.\n[4] R. M. Bittner, J. Salamon, S. Essid, and J. P. Bello.\nMelody extraction by contour classiﬁcation. In Proc.\nISMIR , pages 500–506, 2015.\n[5] R. M. Bittner, J. Salamon, M. Tierney, M. Mauch,\nC. Cannam, and J. P. Bello. Medleydb: A multitrack\ndataset for annotation-intensive mir research. In Proc.\nISMIR , volume 14, pages 155–160, 2014.\n[6] L.-C. Chen, P. George, S. Florian, and A. Hartwig.\nRethinking atrous convolution for semantic image seg-\nmentation. eprint arXiv:1706.05587 , 2017.\n[7] L.-C. Chen, Y . Zhu, P. George, S. Florian, and\nA. Hartwig. Encoder-decoder with atrous separable\nconvolution for semantic image segmentation. eprint\narXiv:1802.02611 , 2018.\n[8] K. Choi, G. Fazekas, M. Sandler, and K. Cho. Transfer\nlearning for music classiﬁcation and regression tasks.\narXiv preprint arXiv:1703.09179 , 2017.\n[9] F. Chollet et al. Keras. https://github.com/\nfchollet/keras , 2015.\n[10] R. B. Dannenberg and C. Raphael. Music score align-\nment and computer accompaniment. Communications\nof the ACM , 49(8):38–43, 2006.\n[11] M. Dorfer, A. Arzt, and G. Widmer. Learning audio-\nsheet music correspondences for score identiﬁcation\nand ofﬂine alignment. In 18th Int. Soc. for Music\nInfo. Retrieval Conf. , Oct.\n[12] M. Goto. A predominant-F0 estimation method for\npolyphonic musical audio signals. In Proc. Int. Cong.\nAcoustics , pages 1085–1088, 2004.[13] P. Hamel, M. Davies, K. Yoshii, and M. Goto. Transfer\nlearning in mir: Sharing learned latent representations\nfor music audio classiﬁcation and similarity. 2013.\n[14] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick. Mask\nr-cnn. arXiv preprint arXiv:1703.06870 , 2017.\n[15] K. He, X. Zhang, S Ren, and J. Sun. Identity mappings\nin deep residual networks. In ECCV , 2016.\n[16] H. Indefrey, W. Hess, and G. Seeser. Design and\nevaluation of double-transform pitch determination al-\ngorithms with nonlinear distortion in the frequency\ndomain-preliminary results. In Proc. IEEE Int. Conf.\nAcoust. Speech Signal Process , pages 415–418, 1985.\n[17] A. Jansson, E. Humphrey, N. Montecchio, R. M. Bit-\ntner, A. Kumar, and T. Weyde. Singing voice sep-\naration with deep u-net convolutional networks. In\n18th Int. Soc. for Music Info. Retrieval Conf. , Suzhou,\nChina, Oct. 2017.\n[18] R. Kelz, M. Dorfer, F. Korzeniowski, S. B ¨ock, A. Arzt,\nand G. Widmer. On the potential of simple frame-\nwise approaches to piano transcription. arXiv preprint\narXiv:1612.05153 , 2016.\n[19] D. P. Kingma and J. Ba. Adam: A method for stochas-\ntic optimization. 2014.\n[20] A. Klapuri. Multipitch analysis of polyphonic music\nand speech signals using an auditory model. IEEE\nTrans. Audio, Speech, Lang. Proc. , 16(2):255–266,\n2008.\n[21] T. Kobayashi and S. Imai. Spectral analysis using gen-\neralized cepstrum. IEEE Trans. Acoust., Speech, Signal\nProc. , 32(5):1087–1089, 1984.\n[22] S. Kum, C. Oh, and J. Nam. Melody extraction on\nvocal segments using multi-column deep neural net-\nworks. In Proc. ISMIR , pages 819–825, 2016.\n[23] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and\nP. Dolla r. Focal loss for dense object detection. eprint\narXiv:1708.02002 , 2017.\n[24] J.-Y . Liu and Y .-H. Yang. Event localization in music\nauto-tagging. In Proc. ACM Multimedia , pages 1048–\n1057. ACM, 2016.\n[25] Jonathan Long, Evan Shelhamer, and Trevor Darrell.\nFully convolutional networks for semantic segmenta-\ntion. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition , pages 3431–\n3440, 2015.\n[26] G. Peeters. Music pitch representation by periodicity\nmeasures based on combined temporal and spectral\nrepresentations. In Proc. IEEE ICASSP , 2006.\n[27] F. Rigaud and M. Radenen. Singing voice melody tran-\nscription using deep neural networks. In ISMIR , pages\n737–743, 2016.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 527[28] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convo-\nlutional networks for biomedical image segmentation.\nInInternational Conference on Medical image com-\nputing and computer-assisted intervention , pages 234–\n241. Springer, 2015.\n[29] J. Salamon and E. G ´omez. Melody extraction from\npolyphonic music audio. Music Information Retrieval\nEvaluation eXchange (MIREX) , 2010.\n[30] J. Salamon and E. G ´omez. Melody extraction from\npolyphonic music signals using pitch contour charac-\nteristics. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 20(6):1759–1770, 2012.\n[31] J. Schl ¨uter. Learning to pinpoint singing voice from\nweakly labeled examples. In ISMIR , pages 44–50,\n2016.\n[32] L. Su. Between homomorphic signal processing and\ndeep neural networks: Constructing deep algorithms\nfor polyphonic music transcription. In Asia Paciﬁc Sig-\nnal and Infor. Proc. Asso. Annual Summit and Conf.\n(APSIPA ASC) , 2017.\n[33] L. Su. V ocal melody extraction using patch-based cnn.\nInProc. ICASSP , 2018.\n[34] L. Su and Y .-H. Yang. Combining spectral and tem-\nporal representations for multipitch estimation of poly-\nphonic music. Audio, Speech, and Language Process-\ning, IEEE/ACM Transactions on , 23(10):1600–1612,\n2015.\n[35] K. Tokuda, T. Kobayashi, T. Masuko, and S. Imai.\nMel-generalized cepstral analysis: a uniﬁed approach\nto speech spectral estimation. In Proc. Int. Conf. Spo-\nken Language Processing , 1994.\n[36] T. Tolonen and M. Karjalainen. A computationally ef-\nﬁcient multipitch analysis model. IEEE Speech Audio\nProcessing , 8(6):708–716, 2000.\n[37] P. Verma and R. W. Schafer. Frequency estimation\nfrom waveforms using multi-layered neural networks.\nInINTERSPEECH , pages 2165–2169, 2016.528 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Transferring the Style of Homophonic Music Using Recurrent Neural Networks and Autoregressive Model.",
        "author": [
            "Wei Tsung Lu",
            "Li Su 0004"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492523",
        "url": "https://doi.org/10.5281/zenodo.1492523",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/107_Paper.pdf",
        "abstract": "Utilizing deep learning techniques to generate musical contents has caught wide attention in recent years. Within this context, this paper investigates a specific problem related to music generation, music style transfer. This practical problem aims to alter the style of a given music piece from one to another while preserving the essence of that piece, such as melody and chord progression. In particular, we discuss the style transfer of homophonic music, composed of a predominant melody part and an accompaniment part, where the latter is modified through Gibbs sampling on a generative model combining recurrent neural networks and autoregressive models. Both objective and subjective test experiment are performed to assess the performance of transferring the style of an arbitrary music piece having a homophonic texture into two different distinct styles, Bachs chorales and Jazz.",
        "zenodo_id": 1492523,
        "dblp_key": "conf/ismir/LuS18a",
        "keywords": [
            "Deep learning",
            "Music generation",
            "Music style transfer",
            "Homophonic music",
            "Gibbs sampling",
            "Recurrent neural networks",
            "Autoregressive models",
            "Chorales",
            "Jazz",
            "Objective and subjective tests"
        ],
        "content": "TRANSFERRING THE STYLE OF HOMOPHONIC MUSIC USING\nRECURRENT NEURAL NETWORKS AND AUTOREGRESSIVE MODELS\nWei-Tsung Lu and Li Su\nInstitute of Information Science, Academia Sinica, Taiwan\ns603122001@gmail.com, lisu@iis.sinica.edu.tw\nABSTRACT\nUtilizing deep learning techniques to generate musical\ncontents has caught wide attention in recent years. Within\nthis context, this paper investigates a speciﬁc problem re-\nlated to music generation, music style transfer. This prac-\ntical problem aims to alter the style of a given music piece\nfrom one to another while preserving the essence of that\npiece, such as melody and chord progression. In partic-\nular, we discuss the style transfer of homophonic music,\ncomposed of a predominant melody part and an accom-\npaniment part, where the latter is modiﬁed through Gibbs\nsampling on a generative model combining recurrent neu-\nral networks and autoregressive models. Both objective\nand subjective test experiment are performed to assess the\nperformance of transferring the style of an arbitrary mu-\nsic piece having a homophonic texture into two different\ndistinct styles, Bachs chorales and Jazz.\n1. INTRODUCTION\nAutomatic music generation is gaining traction in the mu-\nsic industry because of its potential in mass producing mu-\nsic according to a user-assigned style, such as genre or\nmood. For example, the artiﬁcial intelligence (AI) mu-\nsic composition service, Jukedeck, supports four genre op-\ntions (i.e., folk, rock, electronic, and ambient) and allows\nusers to choose how the music feels (i.e., ambient, sparse,\nmeditate, and sci-ﬁ) [1]. Most of the existing advanced\ntechniques employ deep learning techniques to perform\nend-to-end generative modeling of a music style based on a\nsymbolic music format such as the musical instrument dig-\nital interface (MIDI) [3]. Various kinds of model conﬁgu-\nrations were explored in this fashion, such as the encoder-\ndecoder framework [16], generative adversarial networks\n(GAN) [6], autoregressive models [13], variational autoen-\ncoders (V AE) [8], long-short-term memory (LSTM) net-\nworks [7, 12], recurrent Boltzmann machines (RBM) [2]\nand tied parallel networks [10]. These models are de-\nsigned for two slightly different scenarios of music gen-\neration: one is to simply generate music by taking noise as\nc\rWei-Tsung Lu and Li Su. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Wei-Tsung Lu and Li Su. “Transferring the Style of Homophonic\nMusic Using Recurrent Neural Networks and Autoregressive Models”,\n19th International Society for Music Information Retrieval Conference,\nParis, France, 2018.input [16], while the other is to generate adapted accompa-\nniment or voices for a given melody or a lead sheet, also\nknown as reharmonization [7] or reorchestration [14].\nIn this paper, we discuss the music style transfer prob-\nlem. More speciﬁcally, we aim at rearranging the ele-\nments that highly affects style of a given music piece (e.g.,\nrhythm patterns in the accompaniment) while preserving\nthe essence (e.g., melody and chord progression) of that\npiece. This problem has been of interest for a long time;\nprevious related studies include the use of genetic algo-\nrithm (GA) [15] and optimization approaches [19]. In\ncontrast to reharmonization, the style transfer problem in\nthis paper uses the entirety of a music piece, including all\nvoices and accompaniment, as the input of a model. In\nother words, we aim to obtain a system that can automati-\ncally determine which part of an input music is to be pre-\nserved and which part is to be adapted to another style.\nBesides, by leveraging the end-to-end modeling capa-\nbility of deep learning, we investigate the potential of a\nsingle neural network to model two or more distinct styles\nbased on training data of each style. To solve the style\ntransfer problem, there are two main challenges, model\ncomplexity and the diversity over various musical genres.\nFor model complexity, since the DeepBach model is de-\nsigned for generating polyphonic music that only has a\nﬁxed number of voices (i.e., number of polyphony) such as\nBach’s four-part chorales instead of music having varying\nnumbers of polyphony, such as Jazz music, the number of\nvoices in the DeepBach model needs to be increased to ac-\ncommodate the maximum number of voices. For example,\nthe DeepBach model can be extended to 10 voices or more,\nbut doing so also increases the model size and complexity\nconsiderably. For the second challenge, the diversity of\nvarious musical genres means that different music styles\ncorrespond to different preferable model setting, making it\nvirtually impossible to develop a universal framework ap-\nplicable to all kinds of music. For example, [14] proposed\na solution to generate music having different styles but had\nto adopt different models for those styles.\nThis paper proposes a solution of music style trans-\nfer with one single generalized model. It assumes that\ninput music is homophonic music decomposable into a\npredominant melody and an accompaniment part. There-\nfore, we only need to consider style transfer of the ac-\ncompaniment; the melody, while being unaltered in the\noutput, can be used as a condition of the network. We\nemploy a DeepBach-based model to model temporal in-740Figure 1 : The proposed model for music style transfer.\nThe past and future part of the input data (in green) is\nﬁrst transferred to another feature mapping (in dark gray)\nthrough a shared fully-connected network. The LSTM net-\nwork then takes this feature mapping as input and outputs\na 150-by-2 matrix (in yellow), which acts as the condition\nof the autoregressive model (in light gray). Finally, the au-\ntoregressive model predicts the activation of the interested\nnote given the current part of the input data (in red).\nformation, and combine this model with an autoregressive\nmodel (Wavenet [13]) to model the pitch information with-\nout restricting the number of note co-occurrence in a sin-\ngle frame. Experiment results of transferring the style of\nan arbitrary homophonic music piece to Bachs chorale and\nJazz styles are provided in this paper since these two styles\nare arguably the two most extensively investigated mu-\nsic styles in the literature of automatic composition. The\nsource code and listening samples are available on-line.1\n2. MODEL\nOur proposed model of an input music piece is shown in\nFig. 1. The model contains two LSTM networks, with\none taking data preceding a reference time as input and\nanother taking data following the reference time as input.\nIn addition, a speciﬁc autoregressive model, WaveNet, is\nused to process the data of the reference time. Details of\nthe proposed model are discussed below.\n2.1 Data representation\nWe represent a music score as a piano-roll matrixS2\nRI\u0002J\n+ and a metadata matrixM2R3\u0002J\n+. The concate-\nnated matrix [S;M]is then used as the input of the system.\nThe element at the i-th row and the j-th column of Sis de-\nnoted asSij, representing the pitch activation at pitch iand\nat timej, wherei2[1;I],j2[1;J],I= 88 , andJis the\nnumber of time steps of the music piece. S:j2RIis then\nthej-th column of S, representing the pitch proﬁle at time\nj. To represent homophonic music data, a note activation,\nSijfori2[1;88], is represented as a Bernoulli random\nvariable, and Sij= 1if there is a note activation at pitch i\nand timej, andSij= 0if no note activation occurs at time\nj. Since the number of polyphony of homophonic music\n1https://github.com/s603122001/Music-Style-Transfervaries with time, S:jbecomes a multi-hot vector, where its\nnumber of non-zero elements varies with j.\nThe metadata matrix Mdescribes the time grids, the\nstart and the end symbol of the music piece, thereby form-\ning a 3-by-Jmatrix. The time grids used in this paper are\nthe same as the ones in DeepBach: each beat interval is\ndivided into four subdivisions, and are indexed by 1, 2, 3,\nand 4 respectively. The starting time and ending time are\ndenoted as 1 and others as 0. As a result, the dimension of\nthe input, [S:j;M:j], is 91.\nTo facilitate our discussion, we simplify the input data\nin the following two ways. First, sustained note are con-\nsidered as repeating notes with the same pitch . Secondly,\nany two notes with the same time and pitch are considered\nas one note.\n2.2 Model Architecture\nThe proposed model with parameterization \u0012is obtained\nby the following optimization problem:\nmax\n\u0012X\nijlogp\u0000\nSij= 1jSnij;M;\u0012\u0001\n: (1)\nThe formulation (1) can be viewed as a generalized ver-\nsion of the original DeepBach network discussed in [7],\nwhere the number of voices is ﬁxed at 4 and the pitch of\neach voice is modeled individually:\nmax\n\u0012iX\njlogpi\u0000\nVijjVnij;M;\u0012i\u0001\n;fori2[1;4]:(2)\nThe data representation Vij2R4\u0002Jin (2) is different\nfromSij;Vijis the pitch number of the i-th voice (i.e., 1\nfor soprano, 2 for alto, 3 for tenor, and 4 for bass) at time\nj. The four networks in DeepBach have the same struc-\nture, each processing only one part of the four-part Bach’s\nchorales and producing an output limited to monophonic\nmusic. By using the four networks together, the condi-\ntional probability of notes occurred simultaneously in dif-\nferent parts can be covered.\nWe tackle our task on the basis of DeepBach because\naccessing both past and future parts of a score mitigates\nthe problem of transition modeling [18], a major obstacle\nfor music modeling. Besides, the temporal feature of mu-\nsic can be well captured with this model (shown in Sec-\ntion 3.4). Although DeepBach succeeds in handling Bachs\n4-part chorales, it is not readily suitable for our problem\nscenario. To adapt the original DeepBach to accommodate\nmore music types, especially for the music having a ho-\nmophonic texture with varying numbers of polyphony, we\nremove the restriction of voicing and abandon the origi-\nnal four-network structure, and adopt one single network\nto process the multi-hot piano-roll representation.\nReducing the number of networks to one causes our\ngeneralized DeepBach to lose the ability to model the joint\ndistribution of notes articulated simultaneously at a given\ntime step. Besides, [7] indicated that using the piano-roll\nrepresentation causes the generated result to be trapped in\nisolated regions during the Gibbs sampling process (seeProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 741(a) Dilated convolutional block\n(b) Autoregressive block\nFigure 2 : Illustration of the autoregressive block and di-\nlated convolutional block of a conditional Wavenet.\nSection 2.4). In order to overcome these issues, we employ\nan autoregressive model, Wavenet [13], to control the rela-\ntionship among the 88 possible pitch activations at a given\ntime stepj. The joint distribution of S:j=fS1j::::S 88jg\ncan be written as the product of conditional probabilities\nof all pitches:\np(S:j) =88Y\ni=1P(SijjS1j;:::;S (i\u00001)j;SnS:j):(3)\nThe Wavenet models the conditional probabilities by\nstacking dilated causal convolution layers [13]. We then\nemploy the output of the generalized DeepBach model as a\nconstraint. This is implemented by a dilated convolutional\nblock with constraint (see Fig. 2a) and an autoregressive\nblock to predict the output by running from i= 1toi= 88\n(see Fig. 2b). Implementation details of them can be seen\nin [13].\nIn summary, the output of the generalized DeepBach\nrepresents the temporal context of music, and it constrains\nthe Wavenet model to ensure that the output is musicallyreasonable in terms of the harmony progression and other\ncontextual structures.\n2.3 Implementation details\nThe model is implemented using the Keras [4] library with\ntensorﬂow as the back end. First, input data is divided,\nsuch that each unit of the input data contains a segment\nof four time steps, i.e., a 91\u00024matrix, and the segments\ndo not overlap (see the dark green part in Fig. 1). Every\nsegment is ﬁrst ﬂattened, and the ﬂattened segment is em-\nbedded into a 150-D vector with a shared fully connected\nlayer for dimension reduction (see the dark gray part in Fig.\n1), so as to incorporate information over a larger temporal\nrange with a smaller model capacity. Similar to the original\nDeepBach model, two LSTM models are employed, one\ndealing with the past embedded feature mappings and the\nother dealing with the future ones. Both LSTM networks\ntake a series of embedded features with 32 time steps, i.e.,\na 150-by-32 matrix, as the input. Both networks contain 4\nLSTM layers, each having 150 hidden units (see the blue\nblock in Fig. 1). The outputs of the two networks are con-\ncatenated and then transformed to an 88-D vector with an-\nother fully-connected layer. This merged LSTM output is\nthen used as the condition of the Wavenet that employs the\noriginal input feature map at the current time step, as il-\nlustrated in Fig. 2a. The Wavenet consists of ﬁve dilated\nconvolution layers as shown in Fig. 2b, where only the top\ntwo of the ﬁve layers are conditioned by the merged LSTM\noutput and the other three are not conditioned. A dropout\nrate of 30% is adopted for each layer, and batch normal-\nization is added after the activation of each convolutional\nlayer. The model is optimized using ADAM [11].\n2.4 Style transfer\nThe algorithm of style transfer is shown in Algorithm 1,\nand the number of pitch range pis 88 in this paper. Inspired\nby the idea in [7], the style transfer is conducted by using\nGibbs sampling to sample the model and then performing\nan iterative update on the elements of the input score ma-\ntrix. In contrast to those models using noise as input [7,16],\nthe input of our model is the music score to be transferred,\nand this initialization enables the resulting musical struc-\nture to follow the original one. In every iteration of the op-\ntimization process, all the elements at the same time step\nin the input matrix (including both note activation and si-\nlence) are visited and updated iteratively. It is important to\npoint out that all notes at the same time step are updated\ntogether for the chosen target time step. While doing this\npartly violates the original concept of Gibbs sampling, it\nproduces stable results in the experiments.\nSampling from an autoregressive model is inefﬁcient\ndue to the sequential property that every output is condi-\ntioned on all the previous ones. To speed up, we adopt the\nstrategy of independent Gibbs sampling [9, 17]. Indepen-\ndent Gibbs sampling uses an annealed masking probability\n\u000bthat controls the percentage of the elements in the matrix\nthat are to be updated independently, making the input ap-\nproach a stable condition in a short time [9,17]. In the n-th742 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Algorithm 1 Music Style Transfer\nInput:IbyJscore matrix S, number of pitch range P,\nmaximum number of iteration N, maximum and min-\nimum annealed masking probability [ \u000bmax\u000bmin], an-\nnealed masking ratio \u0011\n1:\u000b \u000bmax,^S S,c 0\n2:forn from 1 to N do\n3: Choose time index jin the range of J\n4: if\u000b>\u000bminthen\n5: UpdatefSijgP\ni=1byp(Sijj^S1j;\u0001\u0001\u0001;^S(i\u00001)j;\n^Sn^S:j)\n6:c c+P\n7: ifc>(\u000b\u0001P\u0001J)then\n8: c 0\n9: ^S S\n10: end if\n11:\u000b \u000b\u0000\u000bmax\u0000\u000bmin\n\u0011\u0001N\n12: else\n13: UpdatefSijgP\ni=1byp(SijjS1j;\u0001\u0001\u0001;S(i\u00001)j;\nSnS:j)\n14: end if\n15:end for\nOutput: Transferred score matrix S\niteration of such a sampling process, and for some maximal\nand minimal probabilities \u000bmaxand\u000bmin,\u000bis updated by\nthe following formula:\n\u000bn= max\u0012\n\u000bmin;\u000bmax\u0000n(\u000bmax\u0000\u000bmin)\n\u0011N\u0013\n;(4)\nwhereNand\u0011represent the total number of Gibbs steps\nand the annealed masking ratio controlling the required\ntime for\u000bapproaching \u000bmin. As\u000bis reduced to the mini-\nmum, the procedure approximates the standard Gibbs sam-\npling, which updates only one element at one time and\ncompensates the poor result produced in the independent\nphase. The advantage of using independent Gibbs sam-\npling is its efﬁciency. Besides, independent Gibbs sam-\npling gives more stable outcomes, especially when trans-\nferring to challenging genres like Jazz.\nThis research also discovers that during the transfer pro-\ncess, the melody in the original music tends to vanish dur-\ning the iteration. As a result, we currently apply a con-\nstraint on the melody part to address this issue, and leave\nthe style transfer of the melody part to future work.\n3. EXPERIMENTS AND DISCUSSION\n3.1 Datasets\nTwo datasets, Bach’s four-part chorales and Jazz music,\nare employed as the training data. The Bach dataset con-\ntains 357 Bach four-part chorales included in the music21\ntoolkit [5]. The Jazz dataset contains 487 songs either col-\nlected manually on-line or generated on our own according\nto the scores in the well-known Real Book. To simplify the\nexperiment, we did not distinguish among the sub-genres\nof Jazz, and all of the Jazz pieces are played in Jazz trio,containing one piano, one double bass and one drum. The\ndrummer part is ignored since we consider only the har-\nmonic part of music in this paper. In the training process,\nwe perform data augmentation, by pitch-shifting each song\nin the two datasets up and down by at most 6 semitones in\norder to cover all possible keys. As a result, we have 4858\npieces and 6331 pieces in the Bach dataset and the Jazz\ndataset, respectively. In addition, the two datasets are com-\npiled in different time resolutions. In the Bach dataset, a\nsixteenth note is deﬁned as one time step, while in the Jazz\ndataset, a thirty-second note is deﬁned as one time step, as\nthe latter one contains faster note groups.\nFour songs with different styles were selected as the\ntesting data: Rocky Raccoon by Beatles, Paranoid Android\nby Radiohead, Live and Let Die by Paul McCartney, and\nBeethoven’s Moonlight Sonata , Op. 27, No. 2. Each of\nthe song was cropped to 30 seconds long. These four test-\ning songs will be used in both the objective evaluation and\nthe subjective test.\n3.2 Experiment settings\nExperiments are conducted to demonstrate the effect of our\nsolution to the task of transferring the style of an input mu-\nsic piece to Bach or Jazz style, and we employ the pro-\nposed models trained from the afore-mentioned datasets of\nBachs chorales and Jazz, respectively. To verify the ca-\npability of the network in modeling signals with a vary-\ning number of voices, we employed two different versions\nof the network, one without the autoregressive model (de-\nnoted as “LSTM only”), and the other incorporating the\nautoregressive model (denoted as “LSTM-WN”). We com-\npare the following three models:\n1. LSTM-to-Bach: transfer the style of input music to\nBach’s chorale using the LSTM network only.\n2. LSTM-WN-to-Bach: transfer the style of input mu-\nsic to Bach’s chorale using the LSTM combined\nwith the Wavenet.\n3. LSTM-WN-to-Jazz: transfer the style of input music\nto Jazz using the LSTM combined with the Wavenet.\nLSTM-to-Jazz is excluded from the experiment results\nbecause our pilot study showed that without the autoregres-\nsive model, the generated outputs appear to be composed\nof random note groups due to the wide diversity of the Jazz\ndataset. Since a subjective test is hard to be performed with\nsuch output samples, we eliminate this case in the follow-\ning experiments. For further comparison, we also com-\npare our model with the original DeepBach model, under\nthe scenario of reharmonizing given melodies using a pre-\ntrained DeepBach model.\n3.3 Metrics for objective evaluation\nTo analyze the performance of a style transfer system and\nto compare the songs before and after a transferring pro-\ncess, we divide the style transfer task into 3 sub-parts and\nevaluate them with different metrics.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 7431.Content preservation of the original style . Ac-\ncording to our deﬁnition of music style transfer, a\ngood music style transfer system should preserve\nthe backbone of an input song. This means that the\noverall structure of a song, such as chord progres-\nsion, should not be changed. Therefore, the content\nsimilarity between the original song and the trans-\nformed one should be considered. To evaluate the\ncontent similarity between two music pieces, we ﬁrst\ntransfer the MIDI representation (in piano roll) of\nevery time step into a chroma vector with the 12\npitch classes, then compute the moving average of\nthe chroma vector within a frame size of half bar,\nwhich is 8 time steps in the Bach’s dataset and 16\ntime steps in the Jazz dataset. Finally, the cosine\nsimilarity between every time step in the two score\nmatrices is calculated to measure the content preser-\nvation degree of the transfer process.\n2.Harmony structure similarity to the transferal target\nstyle. Common harmony sets (i.e., combination of\nnotes) vary across different music styles. For exam-\nple, chords with an extension note like 9th and 11th\nare more often used in Jazz music than in Bach’s\nchorales. This characteristic is utilized to visualize\nhow such distribution changes after a style trans-fer.\nFirst the transfer target styles of interest in this pa-\nper are represented by collecting all existing har-\nmony combinations in the two datasets, and then\nmapping them to a 2-D plane by using the principal\ncomponent analysis (PCA) and then the t-distributed\nstochastic neighbor embedding (t-SNE) method. By\nvisualizing such 2-D features of the original and\ntransferred songs on the plane, we could observe the\ndifference of locations between them, and how the\ntransferred song moves toward the target dataset.\n3.Temporal structure imitating the transferred style .\nRhythmic patterns are an important characteristic in\ndistinguish different music styles. To model this\nproperty, we utilize the fact that rhythm has a strong\ncorrelation with the timing of harmony changes. For\nexample, it is common that a chord change coincides\nwith a strong beat. Therefore, we deﬁne the har-\nmonic transition point of a song to be a time step\nwhere at least three notes change in comparison to\nthe previous time step, and then we plot the distri-\nbution of these points within every bar with the tem-\nporal unit being an eighth note. The result is a 32-D\nvectors representing the major pattern of rhythm and\nharmonic transitions. We examine how similar such\npattern of the transferred music pieces is to the pat-\ntern of a target dataset.\n3.4 Objective evaluation\nTable 1 shows the performance index of content preserva-\ntion, the average cosine similarity, of the four testing songs\nbefore and after style transfer. We list the results of the pro-\nposed models and the original DeepBach [7], being used asTo Bach To Jazz\nDeepBach 0.39 N/A\nLSTM-to-Bach 0.86 N/A\nLSTM-WN-to-Bach 0.76 N/A\nLSTM-WN-to-Jazz N/A 0.56\nTable 1 : Evaluation results of Content Preservation in\nterms of cosine similarity.\nFigure 3 : Result of the Harmony Distribution. Data points\nare the piano roll features mapped to a 2-D space through\nPCA and tSNE. Blue: Jazz dataset. Yellow: Bach dataset.\nBlack: testing clips of Jazz music. Orange: testing clips\ntransferred to Bach’s style.\na baseline. We do not use the original DeepBach model for\nstyle transfer to Jazz because its number of voice is ﬁxed\nat 4. Notably, the original DeepBach, which uses only the\nmain melody to perform reharmonization, is less effective\nin following the structure of the original pieces than the\nproposed models designed for homophonic music.\nFig. 3 illustrates the harmony distribution of the\ncropped segments of ﬁve pieces in the Jazz dataset, before\nand after a style transfer using the LSTM-WN-to-Bach\nmodel. Here we illustrate the result of Jazz data instead\nof the testing songs because this is a genre-to-genre com-\nparison. As shown in Fig. 3, the resulting harmony dis-\ntributions indicate that all data points of the Jazz data are\noriginally within the distribution of the Jazz dataset. After\nstyle transfer, most of the data points move toward the dis-\ntribution of the Bach dataset, and some of the transferred\ndata points are even located within the Bach distribution.\nFig. 4 shows the results of temporal structure similarity\nof the four testing songs before and after style transfer. We\nused the same songs and models in the content preserva-\ntion part. Results show that for the transfer-to-Bach case,\nall models ﬁt fairly well to the pattern representing the\nBach dataset, with the original DeepBach model produc-\ning a few extra transitions unseen in the Bach dataset. For\nthe transfer-to-Jazz task, the proposed model also ﬁts the\npattern of the target dataset well.744 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 4 : Evaluation results on temporal structures.\nFigure 5 : Result of the subjective test. The scores rep-\nresent the subjects’ evaluation on how similar the style of\nmusic to the genre listed in the questionnaire.\n3.5 Subjective tests\nTo evaluate the performance of our model from a human\nperception perspective, a listening test was conducted with\n63 participants. Among the participants, 51 of them have\nthe experience of being a music performer, and 17 of those\n51 participants receive formal music education or have\nwork experience in related ﬁelds.\nEach of the four testing songs was transferred using the\nthree afore-mentioned models: LSTM-to-Bach, LSTM-\nWN-to-Bach, and LSTM-WN-to-Jazz, respectively. As\na result, three different versions were produced for each\nsong, and every participant evaluated a total of 12 songs.\nFor each song to be transferred, the participants were asked\nto determine the style of the main melody from 4 music\nstyles: Baroque music (i.e., Bachs), Jazz, Romanticism,\nand Blues. This question aims to direct their attention to\nboth melody and accompaniment parts. Note that Roman-\nticism and Blues are extra options added to avoid a possi-\nble bias in the questionnaire. After answering the question,\nthe participants then evaluated the degree of similarity be-\ntween the transferred songs and the 4 music styles above,\nbased on their music knowledge and personal perception.\nThe evaluation was in the scale from 1 (low) to 5 (high).\nThe results of the subjective test are shown in Fig. 5.\nThe results are averaged for different models. It can be\nseen that, for the cases of transferring to Bach’s style,\nthe rated degrees of similarity of the transferred songs to\nBaroque andRomanticism are both high. According to a\n(a) Original version\n(b) Transfer to Bach\n(c) Transfer to Jazz\nFigure 6 : Transfer results of Live and Let Die using the\nproposed model. The score is output by LogicPro.\nparticipant who is a major in music, this phenomenon is re-\nlated to the main melodies and the original songs we pick.\nHowever, the model with Wavenet produced transferred\nsongs rated with the highest similarity degree to Baroque ,\ndemonstrating the necessity of the Wavenet component in\nour model. For the case of transferring to Jazz style, the\ndegree of similarity to Jazz surpasses other types of music.\nOne test sample used in the subjective test is outputted\nas music scores illustrated in Fig. 6 to give us some in-\nsights into the capability of the proposed models. In Fig.\n6(b), the harmony and music contents are simpliﬁed with\nrespect to the original version since the contents of Bach’s\n4-part chorales are usually not complicated. Apart from\nthis, the difference between the temporal structure of the\ntwo scores is a clear example that our model has learned\nthe temporal feature of the music style. In Fig. 6(c), we\nﬁnd many non-chord notes and some taste of syncopated\nrhythm, both marking the characteristics of Jazz music.\n4. CONCLUSION AND FUTURE WORK\nWe have demonstrated the capability of our model in trans-\nferring arbitrary homophonic music scores into the styles\nof Bach’s 4-part chorales and Jazz, and both objective\nand subjective tests are conducted. The advantage of our\nmethod is that it does not pose any restrictions on input\nmusic scores, and thus it can be easily applied in other sce-\nnarios. Besides, different styles of music can be modeled\nusing the same framework, simplifying the process when\nwe want to expand the collection of target music genres.\nFuture work will focus on the style transfer of a melody,\nwhich is not considered in this paper, as well as further\ninvestigation into complicated music styles and extensive\napplications based on the proposed model.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 7455. ACKNOWLEDGEMENT\nWe would like to thank the anonymous reviewers for their\nvaluable comments and suggestions to improve the quality\nof the paper. This work is partially supported by MOST\nTaiwan, under the contract MOST 106-2218-E-001-003-\nMY3.\n6. REFERENCES\n[1] Jukedeck. https://www.jukedeck.com . [On-\nline; accessed 26-Nov-2017].\n[2] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Modeling temporal dependencies in\nhigh-dimensional sequences: Application to poly-\nphonic music generation and transcription. In Proc. In-\nternational Conference on Machine Learning (ICML) ,\n2012.\n[3] Jean-Pierre Briot, Ga ¨etan Hadjeres, and Franc ¸ois Pa-\nchet. Deep learning techniques for music generation-a\nsurvey. arXiv preprint arXiv:1709.01620 , 2017.\n[4] Franc ¸ois Chollet et al. Keras. https://github.\ncom/fchollet/keras , 2015.\n[5] Cuthbert, Michael Scott, and Christopher Ariza. mu-\nsic21: A toolkit for computer-aided musicology and\nsymbolic music data. 2010.\n[6] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-\nHsuan Yang. Musegan: Symbolic-domain music gen-\neration and accompaniment with multi-track sequen-\ntial generative adversarial networks. arXiv preprint\narXiv:1709.06298 , 2017.\n[7] Ga ¨etan Hadjeres and Franc ¸ois Pachet. Deepbach:\na steerable model for bach chorales generation. In\nProc. International Conference on Machine Learning\n(ICML) , 2017.\n[8] Jay A Hennig, Akash Umakantha, and Ryan C\nWilliamson. A classifying variational autoencoder with\napplication to polyphonic music generation. arXiv\npreprint arXiv:1711.07050 , 2017.\n[9] Cheng-Zhi Anna Huang, Tim Cooijmans, Adam\nRoberts, Aaron Courville, and Douglas Eck. Counter-\npoint by convolution. In ISMIR , 2017.\n[10] Daniel D Johnson. Generating polyphonic music using\ntied parallel networks. In International Conference on\nEvolutionary and Biologically Inspired Music and Art ,\npages 128–143. Springer, 2017.\n[11] Kingma, Diederik, and Jimmy Ba. Adam: A method\nfor stochastic optimization. 2014.\n[12] Feynman Liang, Mark Gotham, Matthew Johnson,\nand Jamie Shotton. Automatic stylistic composition of\nbach chorales with deep lstm. In ISMIR , 2017.[13] Aaron van den Oord, Sander Dieleman, Heiga\nZen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray\nKavukcuoglu. Wavenet: A generative model for raw\naudio. arXiv preprint arXiv:1609.03499 , 2016.\n[14] Franc ¸ois Pachet. A joyful ode to automatic orches-\ntration. ACM Transactions on Intelligent Systems and\nTechnology (TIST) , 8(2):18, 2016.\n[15] Dimitrios Tzimeas and Eleni Mangina. Jazz sebastian\nbach: A ga system for music style modiﬁcation. In Sys-\ntems and Networks Communications, 2006. ICSNC’06.\nInternational Conference on . IEEE, 2006.\n[16] Li-Chia Yang, Szu-Yu Chou, and Yi-Hsuan Yang.\nMidinet: A convolutional generative adversarial net-\nwork for symbolic-domain music generation. In IS-\nMIR, 2017.\n[17] Li Yao, Sherjil Ozair, Kyunghyun Cho, and Yoshua\nBengio. On the equivalence between deep nade and\ngenerative stochastic networks. Joint European Con-\nference on Machine Learning and Knowledge Discov-\nery in Databases , pages 322–336, 2014.\n[18] Adrien Ycart and Emmanouil Benetos. A study on lstm\nnetworks for polyphonic music sequence modelling. In\nISMIR , 2017.\n[19] Frank Zalkow, Stephan Brand, and Bejamin Graf. Mu-\nsical style modiﬁcation as an optimization problem. In\nProc. Int Conf. Computer Music (ICMC) , 2016.746 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Learning Domain-Adaptive Latent Representations of Music Signals Using Variational Autoencoders.",
        "author": [
            "Yin-Jyun Luo",
            "Li Su 0004"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492501",
        "url": "https://doi.org/10.5281/zenodo.1492501",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/169_Paper.pdf",
        "abstract": "In this paper, we tackle the problem of domain-adaptive representation learning for music processing. Domain adaptation is an approach aiming to eliminate the distributional discrepancy of the modeling data, so as to transfer learnable knowledge from one domain to another. With its great success in the fields of computer vision and natural language processing, domain adaptation also shows great potential in music processing, for music is essentially a highly-structured semantic system having domaindependent information. Our proposed model contains a Variational Autoencoder (VAE) that encodes the training data into a latent space, and the resulting latent representations along with its model parameters are then reused to regularize the representation learning of the downstream task where the data are in the other domain. The experiments on cross-domain music alignment, namely an audioto-MIDI alignment, and a monophonic-to-polyphonic music alignment of singing voice show that the learned representations lead to better higher alignment accuracy than that using conventional features. Furthermore, a preliminary experiment on singing voice source separation, by regarding the mixture and the voice as two distinct domains, also demonstrates the capability to solve music processing problems from the perspective of domain-adaptive representation learning.",
        "zenodo_id": 1492501,
        "dblp_key": "conf/ismir/LuoS18",
        "keywords": [
            "domain-adaptive representation learning",
            "music processing",
            "domain adaptation",
            "eliminate distributional discrepancy",
            "transfer learnable knowledge",
            "latent space",
            "regularize representation learning",
            "cross-domain music alignment",
            "monophonic-to-polyphonic music alignment",
            "singing voice source separation"
        ],
        "content": "LEARNING DOMAIN-ADAPTIVE LATENT REPRESENTATIONS OF\nMUSIC SIGNALS USING VARIATIONAL AUTOENCODERS\nYin-Jyun Luo and Li Su\nInstitute of Information Science, Academia Sinica, Taiwan\nffredomluo,lisu g@iis.sinica.edu.tw\nABSTRACT\nIn this paper, we tackle the problem of domain-adaptive\nrepresentation learning for music processing. Domain\nadaptation is an approach aiming to eliminate the distri-\nbutional discrepancy of the modeling data, so as to transfer\nlearnable knowledge from one domain to another. With\nits great success in the ﬁelds of computer vision and nat-\nural language processing, domain adaptation also shows\ngreat potential in music processing, for music is essen-\ntially a highly-structured semantic system having domain-\ndependent information. Our proposed model contains a\nVariational Autoencoder (V AE) that encodes the training\ndata into a latent space, and the resulting latent represen-\ntations along with its model parameters are then reused to\nregularize the representation learning of the downstream\ntask where the data are in the other domain. The experi-\nments on cross-domain music alignment, namely an audio-\nto-MIDI alignment, and a monophonic-to-polyphonic mu-\nsic alignment of singing voice show that the learned rep-\nresentations lead to better higher alignment accuracy than\nthat using conventional features. Furthermore, a prelimi-\nnary experiment on singing voice source separation, by re-\ngarding the mixture and the voice as two distinct domains,\nalso demonstrates the capability to solve music processing\nproblems from the perspective of domain-adaptive repre-\nsentation learning.\n1. INTRODUCTION\nMusic is composed, arranged, and performed in various\nforms residing in different data modalities and domains,\nyet sharing some common underlying information with\neach other. Almost all of the music processing tasks es-\nsentially extract such commonality as a protocol that en-\nables the transferring or communication among various do-\nmains. For example, a piece of music can be either writ-\nten as a musical score, or rendered as an audio recording;\nthough the later encompasses much more information such\nas intonation, articulation, emotion, and others not found\nin the former, they still share common information such\nc\rYin-Jyun Luo and Li Su. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attri-\nbution: Yin-Jyun Luo and Li Su. “Learning Domain-adaptive Latent\nRepresentations of Music Signals Using Variational Autoencoders”, 19th\nInternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.as note, pitch, and meter with each other. In light of this\nproperty, we devise a framework that aims at eliminating\ndomain-dependent information, to achieve a feature repre-\nsentation that is semantically shared across domains.\nIn this paper, we study learning representations that em-\nbed shared semantic information across different domains,\nspeciﬁcally in applications of music signal processing. In\norder to achieve domain-invariant feature representations,\nwe are essentially considering a domain adaptation prob-\nlem [28]. Take audio-to-MIDI alignment [30] as an exam-\nple, while audio and MIDI data are drawn from distinct\ndomains of representation, they share pitch information in\ncommon. We explore the transfer learning technique [25]\nto tackle the problem. Speciﬁcally, in addition to transfer-\nring model parameters, we also transfer latent representa-\ntions from one domain to the other.\nWith its success in computer vision [24,28,34] and nat-\nural language processing [16, 19, 32], transfer learning has\nalso shown great potential in music information retrieval\n(MIR). In [6], a linear transformation is learned to project\ndata into a shared latent representation that captures se-\nmantic similarity of music. Choi et al. uses feature maps\nof multiple layers derived from a pre-trained convolutional\nneural network (CNN) for music classiﬁcation and regres-\nsion tasks [2], and Park et al. exploits the deep model\ntrained for artist recognition as a general feature extractor\nused for various tasks [26].\nOur proposed framework1is different from the above-\nmentioned works. With pairwise training data2from two\ndistinct domains, our framework ﬁrst utilizes a V AE [12],\na state-of-the-art unsupervised generative model shown to\nbe effective in representation learning [9,14], to embed in-\nformation of data from one domain (the source domain )\nwhich contains mostly shared semantics into latent rep-\nresentations. Data from the other domain (i.e., the tar-\nget domain ) is then mapped to the learned embeddings\nthrough a separate neural network, in order to eliminate\ndomain-dependent information. Therefore, the novelty of\nthis paper is an uniﬁed framework that combines repre-\nsentation learning and transfer learning altogether, which\nlearns domain-adaptive representations with V AEs that are\nthen transfered from source to target domain. In particu-\nlar, we empirically validate the framework through three\n1https://github.com/yjlolo/Domain-Adaptive-VAE\n2Pairwise data in the context means parallel music events in different\ndomains, e.g., a piece of music written as a score or rendered as an audio,\nand a recording of singing voice with or without accompaniment.653Figure 1 . The general architecture of the proposed training\nframework.\nwell-known tasks in music signal processing that have not\nbeen considered from the perspective of domain adap-\ntation: audio-to-MIDI alignment [1, 13], audio-to-audio\nalignment [17, 22], and singing voice separation [5, 10].\nThe rest of the paper is organized as follows. In\nSection 2, we describe our proposed architecture. The\nexperiments and results are detailed in Section 3 and\nSection 4, respectively. Conclusions and future work are\npresented in Section 5.\n2. ARCHITECTURE\n2.1 Overview\nFigure 1 shows our proposed framework in the training\nphase, which is divided into two modules: the ﬁrst is a\nV AE which models the data in source domain, and the sec-\nond, which can be either an autoencoder (AE) or simply\nan encoder depending on tasks, models the data in target\ndomain. To facilitate the discussion, we refer Encoder\n(orDecoder ) and Encoder* (orDecoder* ) to the source-\ndomain encoder (orsource-domain decoder ) and target-\ndomain encoder (ortarget-domain decoder ), respectively.\nThe two models are trained sequentially in two steps.\nFirst, we train the V AE, using the source-domain data as\ninputs, and obtain the source-domain latent representations\nz:=z(\u0016;\u001b). More speciﬁcally, given the observation\ndataxin the source domain, and z\u0018p(z)the latent rep-\nresentation, the posterior distribution p(zjx)is modeled\nas a Gaussian distribution parameterized by the estimated\nmean and standard deviation of the posterior distribution,\nnamely\u0016and\u001b, respectively. In other words, we have\np(z) =N\u0000\nz;\u0016;\u001b2I\u0001\nin practice.\nSecond, after the source-domain model is trained, we\ntrain the target-domain model, with the following two\ntransfer learning schemes: 1) the source-domain model pa-\nrameters are used to initialize the target-domain model pa-\nrameters, and 2) the source-domain latent representation\nzis used to regularize the target-domain latent represen-\ntationz\u0003with an regularization term L(z;z\u0003). The intu-\nition behind this is to leverage knowledge learned by the\nsource-domain V AE to reduce the distributional discrep-\nancy between the source and target domain.\nThe target-domain decoder, colored in gray in Figure 1,\nis optional. For example, in the task of music alignment,Conv1 Conv2 Conv3 Fc1 Gauss\n#ﬁlters/units 64 128 256 512 L\nﬁlter size 1\u0002F 3\u00021 2\u00021 - -\nstride (1,1) (2,1) (2,1) - -\nTable 1 . Encoder network architecture. Conv refers to con-\nvolutional layers, Fcrefers to fully connected layers, and\nGauss refers to the Gaussian parametric layer modeling z.\nour purpose is to learn the domain-adaptive features by\nmapping the data in target domain into the feature distri-\nbution of data in source domain, without the need to re-\nconstruct the input data from the latent representation.\nIt should be noticed that in the inference phase, shown\nin the left-hand side of Figure 2, the parameter \u0016is re-\ngarded asz. That is, when encoding the source-domain\ndata,\u0016, the center of a Gaussian distribution, is the repre-\nsentative ofz. Therefore,\u0016is the true latent representation\nthat is transferred to the target domain. More details about\nthe models and experiments are in Section 3.\n2.2 Source-domain Model: Variational Autoencoder\nSince the source-domain V AE is task-independent, we in-\ntroduce its detailed architecture ﬁrst in this subsection, and\nthe task-dependent target-domain model will be introduced\nlater in Section 3. We adopt the V AE architecture proposed\nin [9], which learns the latent representations and models\nthe generative process of speech segments for voice con-\nversion. In our work, the source-domain input represen-\ntation is either a segment of singing voice in the tasks of\nsinging voice alignment and separation, or a piano roll for\naudio-to-MIDI alignment.\nThe inputxof the source-domain V AE is a two-\ndimensional image of size T\u0002F, whereTis the num-\nber of time steps and Fis the number of frequency bands.\nThe encoder network of this V AE is a CNN with 3 con-\nvolution ( Conv ) layers and 1 fully-connected ( Fc) layer\nthat outputs the latent representation zwith dimension L\nat the Gauss layer. The parameters of this CNN are sum-\nmarized in Table 1. The decoder network is symmetric to\nthe encoder network; it takes zas the input to reconstruct\nx. Batch normalization followed by the activation function\ntanh are used for every layer except for the Gauss and the\noutput layers. The objective function for training the V AE\nis expressed as (1):\nLvae=Lrec+LKL; (1)\nwhere the total loss function of the V AE, Lvae,\ncontains two terms: the reconstruction loss function\nLrec=\u0000Eq(zjx)[logp(xjz)], the negative expected log-\nlikelihood of x, and the KL-Divergence loss LKL=\nKL[q(zjx)jjp(z)], which regularizes the distance be-\ntween the posterior and the Gaussian distribution. In varia-\ntional inference, the true posterior p(zjx)is approximated\nbyq(zjx). For more implementation details of the V AE,\nwe refer the readers to [3, 12].654 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20183. EXPERIMENTS\nWe discuss the following three tasks: 1) audio-to-MIDI\nalignment, 2) audio-to-audio alignment, and 3) singing\nvoice separation. In this section, we elaborate the goals,\nthe datasets, the task-dependent target-domain models, the\ninput data representations, and the evaluation processes for\neach of these three tasks. Experiment results will be dis-\ncussed in Section 4.\nAll of the models discussed in the following are imple-\nmented with PyTorch [27], and are trained using stochas-\ntic gradient descent with the Adam optimizer [11]. The\noptimizer is parametrized by: learning rate = 10\u00003,\f1=\n0:9,\f2= 0:999, and\u000f= 10\u00008. The mini-batch size is set\nto 128 instances of input segments.\n3.1 Task 1: Audio-to-MIDI Alignment\nThe ﬁrst experiment we consider is to align an audio\nrecording of piano to its corresponding MIDI ﬁle. Al-\nthough this is a rather well-studied task [13,23,30], we re-\ninvestigate this task from the perspective of domain adapta-\ntion: using the learned latent representations for the feature\non which dynamic time warping (DTW) is performed.\n3.1.1 Dataset\nWe use a subset of the MAPS dataset [4], ENSTD-\nkCI, which contains 30 piano recordings performed by a\nYamaha Disklavier auto-piano together with MIDI ﬁles\nthat generate the recordings. We use 24 and 6 pieces of\nthe subset for training and validation, respectively.\n3.1.2 Model\nThe goal of our framework is to map a frame of audio\nfeature and piano roll into the same representation if they\nare of the same music event. To do this, we ﬁrst deﬁne\nthe MIDI pieces as the source domain data, and the au-\ndio pieces as the target-domain data. Then, we train the\nsource-domain V AE and obtain the learned source-domain\nlatent representation z. We then use this representation z\nas the learning target to train the target-domain model, a\nsingle encoder taking audio data as input, with its architec-\nture the same as the source-domain encoder. To be more\nspeciﬁc: given a pair of MIDI-audio input data that are of\nthe same event in the music, the source-domain V AE maps\nthe MIDI into a representation in a low-dimensional Gaus-\nsian distribution, and the target-domain encoder is then\ntrained to map the audio input data to that distribution.\nThe learning task in the target domain is essentially a\nregression task. The training objective function for source\ndomain is the same as (1), while the objective function for\nthe target domainLencoder is\nLencoder =LMSE (z;z\u0003); (2)\nwhich is the mean squared error between the encoded la-\ntent representations of the source-domain encoder zand\nthe ones of the target-domain encoder z\u0003. Notice that (2) is\nonly applicable when we have parallel source-target pairs.Instead of audio, MIDI is regarded as the source-\ndomain data because the latent representation we obtain\nshould be more related to MIDI which contains mostly the\nshared semantics with audio, i.e., pitch; we let the target-\ndomain encoder eliminate the information residing in au-\ndio while unrelated to MIDI (e.g., spectral-related informa-\ntion) in order to get succinct representations for alignment.\n3.1.3 Data Representation\nMIDI ﬁles are represented as piano-roll representation with\n128 pitch classes, while its associated audio recordings are\nrepresented using Mel-scaled spectrogram with 128 ﬁlter\nbanks, derived from power magnitude spectrum of 1024-\npoint short-time Fourier transform (STFT). To compute the\nSTFT, we use Hanning window with window size of 64 ms\nand hop size of 20 ms. An input data for the source-domain\nV AE (or the target-domain encoder) is a segment of a\npiano-roll (or Mel-scaled spectrogram) with 21 frames, or\nequivalently, 400 ms, leading to the input dimensions of\nT= 21 andF= 128 . To reduce the memory load, we only\ncollect segments every 10 frames of each clip for training.\n3.1.4 Evaluation\nTo evaluate our proposed feature representation for audio-\nto-MIDI alignment, we apply non-linear time-stretching to\nthe audio recordings so as to see if the features are robust\nagainst the distortion and can still be aligned to the original\nMIDI well. We follow the methodology in [17] for non-\nlinear time-stretch.\nThe proposed feature representation of MIDI can be\nderived as follows: we express MIDI as piano roll and\nuse it as the input to the source-domain encoder to obtain\nthe encoded latent representation as our proposed feature;\nthe process is illustrated in Figure 3 with the solid blue\nline. On the other hand, in the target domain, we ﬁrstly\napply time-stretching distortion to the audio recordings,\nrepresent the audio stream with Mel-scaled spectrogram\ndescribed in Section 3.1.3, and utilize the outputs of the\ntarget-domain encoder as our ﬁnal feature representation;\nthe green solid line in Figure 3 describes the process. Over-\nall, the derivation of the proposed feature representation\nduring inference is illustrated in the left panel of Figure 2.\nFor comparison, we consider chroma as a baseline\nto represent both domains, illustrated in Figure 3 with\nthe loose dash lines colored in blue and green, respec-\ntively. Regarding the implementation of chroma, we use\nchroma stft in the librosa library [18] for audio\nandgetchroma in the pretty midi library [29] for\nMIDI. The other baseline is to use the piano roll for\nMIDI and Mel-scaled spectrogram for audio, illustrated\nin Figure 3 with the dense dash lines colored in blue and\ngreen, respectively.\nWe utilize DTW to align the feature representations and\ncompute the alignment accuracy. The accuracy is calcu-\nlated by an error measure ewhichp compares the discrep-\nancy between the estimated warping path and the ground-\ntruth one [36] instead of the conventional note-level align-\nment accuracy, because the error measure eallows more\nsubtle comparison on frame-level evaluation.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 655Figure 2 . Left: the derivation of the proposed feature representation in task 1 andtask 2 . Right: the training scheme of the\ntarget-domain DAE in task 3 .\nFigure 3 . Extraction pipelines of feature representations\nfor the two alignment tasks, i.e., task 1 andtask 2 .\n3.2 Task 2: Audio-to-Audio Alignment\nIn this experiment, we consider singing voice alignment,\nin particular the alignment of song recordings performed\nby singers with their artiﬁcially-distorted versions. Specif-\nically, two subtasks are considered: 1) aligning the dis-\ntorted monophonic singing recordings to the original ver-\nsion, denoted as mono-to-mono , and 2) the distorted mono-\nphonic singing recordings to the original singing record-\nings mixed with the corresponding background music, de-\nnoted as mono-to-poly . The goal is to demonstrate the ro-\nbustness of our proposed feature representation against the\nartiﬁcial distortion effects, i.e, pitch-shift and time-stretch,\nas well as interference of the background music.\n3.2.1 Dataset\nWe adopt the MIR-1k dataset [8] which contains the 1,000\nChinese karaoke excerpts with separated voice and accom-\npaniment tracks, clipped from 110 songs. We then divided\nthe 110 songs into two subsets, one containing 88 songs for\ntraining, and the other containing 22 songs for validation.3.2.2 Model\nThe training procedure resembles the one mentioned in\nSection 3.1.2. The difference is that the source domain\nrefers to monophonic singing, and the target domain refers\nto its polyphonic version; the shared information is the\nsinging voice. Notice that, similar to Section 3.1, the syn-\nthetic dataset with artiﬁcial distortion is not used for train-\ning. The target-domain encoder for modeling polyphonic\nmusic learns not only to output features that are compara-\nble to monophonic singing voice, but also features that are\nmore robust to artiﬁcial distortion.\n3.2.3 Data Representation\nThe inputs are represented the same way as the audio data\ndescribed in Section 3.1.1. As suggested by the prelim-\ninary experiments, the number of ﬁlter banks is set to\nF= 256 instead of 128.\n3.2.4 Evaluation\nWe evaluate our proposed feature representation under\nboth time-stretching and pitch-shifting distortion. The set-\ntings of the distortion follow the one in [17].\nAs shown in Figure 3, for the subtask mono-to-mono ,\nwe ﬁrst apply the artiﬁcial distortion to the monophonic\nsinging, followed by a Z-score normalization. The Mel-\nscaled spectrogram is then extracted as the input to the\nsource-domain encoder, which gives the proposed feature\nrepresentation of monophonic singing after a post Z-score\nnormalization. We align the distorted monophonic singing\nto the intact version. For the subtask mono-to-poly , we\nadopt the identical process to the monophonic singing.\nWhile for polyphonic singing, the target-domain encoder\ntakes the input as the Mel-spectrogram to output the pro-\nposed feature representation. We align the distorted mono-\nphonic to the original polyphonic version. The overview of\nderivation of the proposed feature representation and align-\nment are illustrated in the left panel of Figure 2.\nWe compare our proposed feature representation with\nthe 24-ordered Mel-cepstral coefﬁcients (MCEPs) [33], a\nwidely used features regarding speech alignment for voice\nconversion [20], in terms of the error measure e, as in\nSection 3.1.4. We use the spectral envelope which is ex-\ntracted by WORLD [21] to derive MCEPs. DTW in this656 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018task searches for the optimal alignment path according to\nsquared Euclidean distance, as suggested by preliminary\nexperimental results.\n3.3 Task 3: Singing Voice Separation\nSinging voice separation is an essential yet notoriously\nchallenging problem in music signal processing; the goal\nis to separate singing voice from music mixture. We inves-\ntigate the potential of domain adaptation on this problem.\n3.3.1 Dataset\nWe again adopt the MIR-1k dataset for experiment, and\nsplit the dataset in a way identical to that in Section 3.2.1.\n3.3.2 Model\nThe basic idea is a follow-up of the mono-to-poly\nscheme in Section 3.2: given the fact that we have\nobtained domain-adaptive latent representations shared\nacross monophonic singing and its polyphonic version\nwith accompaniment, one step further is to consider de-\ncoding the outputs of the target-domain encoder in order\nto reconstruct the monophonic singing voice in the target\ndomain. Therefore, we adopt a Denoising Autoencoder\n(DAE) [35] in the target domain.\nThe training scheme of the target domain is illustrated\nin the right panel of Figure 2. It is important to note that,\ndifferent from the vanilla DAE for source separation [5],\nwe regularize the bottleneck layer with the learned latent\nrepresentation encoded by the V AE along with the model\nparameters for weight initialization. The training objective\nfunction for the target domain therefore becomes:\nLDAE =Ll1(x;~x) +\u000bLMSE (z;z\u0003); (3)\nwhere the reconstruction loss Ll1denotes the l1-norm;x\nand~xare the clean source of singing voice and estimated\none, respectively. \u000bis the weight of the regularization term\nwhich is set to 1 without further investigation in this pre-\nliminary work.\n3.3.3 Data Representation\nFor audio representation, the magnitude spectrogram in-\nstead of the Mel-scaled spectrogram is used as the input;\nthe parameters for computation of STFT remain the same\nas in Section 3.1.3.\n3.3.4 Evaluation\nThe music mixture which contains the ground-truth source\nof singing voice xand background music is ﬁrstly normal-\nized with a Z-score normalization, and is represented as\nthe magnitude spectrogram. The trained DAE then takes\nas the input the magnitude spectrogram, and outputs the\nestimated source of signing voice ~x.\nFor evaluation, we use mireval [31] to calculate\nand report source-to-distortion ratio (SDR), source-to-\ninference ratio (SIR), and source-to-artifact (SAR) ratio\ntogether with normalized SDR (NSDR). All scores are\nweighted by number of frames of each song. We com-\npare the performance among vanilla DAE with or withouterror measure\nProposed (L= 128 ) 2.48\nProposed (L= 12 ) 4.08\nChroma 6.71\nSpec 39.24\nTable 2 . The error measure eof audio-to-MIDI alignment\nusing different feature representations.\nour proposed regularization term and weight initialization\nduring training phase.\n4. RESULTS\nIn this section, we report the performance evaluated on the\nvalidation sets for each experiment.\n4.1 Task 1: Audio-to-MIDI Alignment\nTable 2 lists the median value of e, the alignment error\nmeasure, over the 6 audio-MIDI pairs in the validation set\nusing four different feature representations: two of them\nare the proposed latent representations with dimensions\nL= 128 and12(Proposed ), one is the 12-dimensional\nchroma ( Chroma ), and the other uses Mel-scaled spec-\ntrogram for audio and piano-roll representation for MIDI,\nboth are 128-dimension ( Spec ). One can see our proposed\ndomain-adaptive features outperform with both L= 128\nand12. This implies that the plane pitch information of\nMIDI domain is properly modeled in the latent represen-\ntations by the source-domain encoder, and is efﬁciently\ntransferred to audio domain by treating the latent represen-\ntations as learning targets for the target-domain encoder.\n4.2 Task 2: Audio-to-Audio Alignment\nWe evaluate on the validation set of 22 songs and report\nthe alignment error measure eof different feature repre-\nsentations under the mono-to-mono andmono-to-poly sub-\ntasks, along with the artiﬁcial distortion in pitch-shift and\nlinear/non-linear time-stretch. Figure 4 shows the median\nof the error measure eusing different feature representa-\ntions; the baseline feature and proposed one are denoted\nasMCEP andProposed , respectively. Each individual plot\nshows the error measure along pitch-shift steps of -2, -1, 0,\n1, and 2. The top panel and bottom panel refer to mono-to-\nmono andmono-to-poly , respectively. The leftmost to the\nﬁfth column correspond to linear time-stretching rates of\n0.8, 0.9, 1.0, 1.1 and 1.2, respectively, while the rightmost\ncolumn corresponds to the non-linear time-stretch.\nThe results of momo-to-mono in the top panel suggest\nthat our proposed feature representation encoded by the\nsource-domain encoder is more robust to the artiﬁcial dis-\ntortion than the baseline feature. The bottom panel, which\ncorresponds to mono-to-poly , shows that by transferring\nthe latent representations from source to target domain, the\ntarget-domain encoder indeed learns to output features that\nare robust against both the artiﬁcial distortion and the in-\nterference of background music.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 65702040\n-2 -1 0 1 202040\n-2 -1 0 1 2-2 -1 0 1 2-2 -1 0 1 2-2 -1 0 1 2-2 -1 0 1 2\npitch-shift step (semitones)error measureProposed\nMCEPFigure 4 . The error measure eof singing voice alignment using our proposed features or MCEPs. Top panel: mono-to-\nmono ; bottom panel: mono-to-poly . The leftmost column to the ﬁfth column refer to time-stretch rate r= 0:8;0:9;1:0;1:1;\nand1:2, respectively; the rightmost column refers to non-linear time-stretch.\nSDR SIR SAR NSDR\nDAE 4.73 16.13 5.35 3.16\nDAE +wt 6.50 20.40 6.85 4.93\nrDAE 4.97 14.96 5.74 3.40\nrDAE +wt 7.20 18.98 7.74 5.63\nTable 3 . The source-to-distortion ratio (SDR), source-to-\ninference ratio (SIR), and source-to-artifact ratio (SAR)\nand normalized SDR (NSDR) of different models.\n4.3 Task 3: Singing Voice Separation\nTable 3 demonstrates the SDR, SIR, and SAR together\nwith NSDR of different models in the task of singing voice\nseparation. Four models are compared: 1) DAE referring\nto the vanilla DAE, the baseline model, 2) DAE +wtde-\nnoting the DAE trained with weight initialization using the\nsource-domain model parameters, 3) rDAE referring to\nthe DAE trained with the objective function whose weight\nof the regularization term \u000b= 1in (3), and 4) rDAE +wt,\nthe DAE trained with both the weight initialization and reg-\nularization term.\nFrom the SDR in Table 3, one can observe that DAE +\nwtoutperforms DAE by 1.77 dB, while rDAE outper-\nformsDAE by only 0.24 dB. However, by combining\nweight initialization and regularization together, rDAE +\nwtachieves an improvement of 2.47 dB over DAE . This\nimplies that the effect of transferring the latent represen-\ntation from the source to target domain as a regularization\nterm can be optimized by the transfer of the source-domain\nmodel parameters.\nNotice that though the reported performance is not on\npar with the state-of-the-art method [10], our model still\nshow potentials in solving the singing voice separation\nproblem from the perspective of domain adaptation. Mean-\nwhile, as a preliminary work, we evaluate the framework\non a relatively small dataset without data augmentation and\nﬁne-tuning parameters.5. CONCLUSION AND FUTURE WORK\nIn this paper, we re-investigate three well-known tasks of\nmusic signal processing from the perspective of domain\nadaptation, namely task 1 : audio-to-MIDI alignment, task\n2: audio-to-audio alignment and task 3 : singing voice sep-\naration. To this end, we devise an uniﬁed framework that\nachieve both representation learning and transfer learning\nat once. Speciﬁcally, we use a V AE to learn latent repre-\nsentation of source-domain data, which is then transfered\nto train a separate model that maps target-domain data to\nthe representation.\nWe empirically validate our idea by demonstrating the\nsuperiority of our proposed feature representations over\nbaseline ones across all the tasks. In both task 1 and2, the\nproposed features are shown to properly model the source-\ndomain data and are efﬁciently transfered to the target do-\nmain; they are more robust against various settings of ar-\ntiﬁcial distortion compared to baseline features. In task 3 ,\nit is shown that transferring of both model parameters and\nlatent representations, used for weight initialization and as\na regularization term, respectively, can beneﬁt the perfor-\nmance of singing voice separation, which indicates the po-\ntential of the framework for such a challenging problem.\nAs a preliminary work, though we share most of the pa-\nrameters and model architectures across all the tasks with-\nout tailoring for each individual task, the proposed frame-\nwork consistently outperforms the baselines. For future\nwork, we would like to include larger datasets and opti-\nmize the system architectures and their parameters. More-\nover, expanding the framework for classiﬁcation is of par-\nticular interest. For example, it is possible to transfer the\nlatent representation from source to target domain by di-\nrectly leveraging it as the classifying feature [15] or inter-\nmediate condition to models in target domain [7].\n6. ACKNOWLEDGEMENT\nThe authors would like to thank the anonymous reviewers\nfor their valuable comments and suggestions to improve\nthe quality of the paper. This work is partially supported\nby MOST Taiwan, under the contract MOST 106-2218-E-\n001-003-MY3.658 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20187. REFERENCES\n[1] J. J. Carabias-Orti, F. J. Rodr ´ıguez-Serrano, P. Vera-\nCandeas, N. Ruiz-Reyes, and F. J. Ca ˜nadas-Quesada.\nAn Audio to Score Alignment Framework Using Spec-\ntral Factorization and Dynamic Time Warping. In IS-\nMIR, pages 742–748, 2015.\n[2] K. Choi, G. Fazekas, M. Sandler, and K. Cho. Trans-\nfer Learning for Music Classiﬁcation and Regression\nTasks. In ISMIR , 2017.\n[3] C. Doersch. Tutorial on Variational Autoencoders.\nArXiv e-prints , June 2016.\n[4] V . Emiya, R. Badeau, and B. David. Multipitch Es-\ntimation of Piano Sounds using a New Probabilistic\nSpectral Smoothness Principle. IEEE Trans. on Audio,\nSpeech, and Language Processing , 18(6):1643–1654,\n2010.\n[5] E. M. Grais and M. D. Plumbley. Single Channel Au-\ndio Source Separation using Convolutional Denoising\nAutoencoders. ArXiv e-prints , March 2017.\n[6] P. Hamel, M. E. Davies, K. Yoshii, and M. Goto. Trans-\nfer Learning in MIR: Sharing Learned Latent Repre-\nsentations for Music Audio Classiﬁcation and Similar-\nity. In ISMIR , 2013.\n[7] J. A. Hennig, A. Umakantha, and R. C. Williamson.\nA Classifying Variational Autoencoder with Applica-\ntion to Polyphonic Music Generation. ArXiv e-prints ,\nNovember 2017.\n[8] C.-L. Hsu and J.-S. R. Jang. On the Improvement of\nSinging V oice Separation for Monaural Recordings us-\ning MIR-1k Dataset. TASLP , 18(2):310–319, 2010.\n[9] W.-N. Hsu, Y . Zhang, and J. Glass. Learning Latent\nRepresentations for Speech Generation and Transfor-\nmation. In Interspeech , pages 1273–1277, 2017.\n[10] A. Jansson, E. Humphrey, N. Montecchio, R. Bittner,\nA. Kumar, and T. Weyde. Singing V oice Separation\nWith Deep U-Net Convolutional Networks. In ISMIR ,\n2017.\n[11] D. P. Kingma and J. Ba. Adam: A Method for Stochas-\ntic Optimization. ArXiv e-prints , December 2014.\n[12] D. P Kingma and M. Welling. Auto-Encoding Varia-\ntional Bayes. ArXiv e-prints , December 2013.\n[13] T. Kwon, D. Jeong, and J. Nam. Audio-to-score Align-\nment of Piano Music Using RNN-based Automatic\nMusic Transcription. ArXiv e-prints , November 2017.\n[14] S. Latif, R. Rana, J. Qadir, and J. Epps. Variational\nAutoencoders for Learning Latent Representations of\nSpeech Emotion: A Preliminary Study. ArXiv e-prints ,\nDecember 2017.[15] S. Latif, R. Rana, J. Qadir, and J. Epps. Variational\nAutoencoders for Learning Latent Representations of\nSpeech Emotion: A Preliminary Study. ArXiv e-prints ,\nDecember 2017.\n[16] Q. Li. Literature Survey: Domain Adaptation Algo-\nrithms for Natural Language Processing. Technical re-\nport, Department of Computer Science The Graduate\nCenter, The City University of New York, 2012.\n[17] Y .-J. Luo, M.-T. Chen, T.-S. Chi, and L. Su. Singing\nV oice Correction using Canonical Time Warping.\nArXiv e-prints , November 2017.\n[18] B. McFee, M. McVicar, O. Nieto, S. Balke, C. Thome,\nD. Liang, E. Battenberg, J. Moore, R. Bittner, R. Ya-\nmamoto, and et al. librosa 0.5.0, February 2017.\n[19] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Ef-\nﬁcient Estimation of Word Representations in Vector\nSpace. ArXiv e-prints , January 2013.\n[20] S. H. Mohammadi and A. Kain. An Overview of V oice\nConversion Systems. Speech Communication , 2017.\n[21] M. Morise, F. Yokomori, and K. Ozawa. WORLD: A\nV ocoder-based High-quality Speech Synthesis System\nfor Real-time Applications. IEICE Trans. Information\nand Systems , 99(7):1877–1884, 2016.\n[22] M. M ¨uller, F. Kurth, and M. Clausen. Audio Matching\nvia Chroma-Based Statistical Features. In ISMIR , page\n6th, 2005.\n[23] M. M ¨uller, F. Kurth, and T. R ¨oder. Towards an Ef-\nﬁcient Algorithm for Automatic Score-to-Audio Syn-\nchronization. In ISMIR , 2004.\n[24] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning\nand Transferring Mid-level Image Representations us-\ning Convolutional Neural Networks. In CVPR , pages\n1717–1724. IEEE, 2014.\n[25] S. J. Pan and Q. Yang. A Survey on Transfer Learn-\ning.IEEE Trans. on knowledge and data engineering ,\n22(10):1345–1359, 2010.\n[26] J. Park, J. Lee, J. Park, J.-W. Ha, and J. Nam. Repre-\nsentation Learning of Music Using Artist Labels. ArXiv\ne-prints , October 2017.\n[27] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang,\nZ. DeVito, Z. Lin, A. Desmaison, L. Antiga, and\nA. Lerer. Automatic Differentiation in PyTorch. In\nNIPS-W , 2017.\n[28] V . M. Patel, R. Gopalan, R. Li, and R. Chellappa.\nVisual Domain Adaptation: A survey of Recent Ad-\nvances. IEEE signal processing magazine , 32(3):53–\n69, 2015.\n[29] C. Raffel and D. P. W. Ellis. Intuitive Analysis, Cre-\nation and Manipulation of MIDI Data with pretty midi.\nInISMIR Late Breaking and Demo Papers , 2014.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 659[30] C. Raffel and D. P. W. Ellis. Optimizing DTW-based\nAudio-to-MIDI Alignment and Matching. In ICASSP ,\npages 81–85. IEEE, 2016.\n[31] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nieto, D. Liang, and D. P. W. Ellis. mir eval: A\nTransparent Implementation of Common MIR Metrics.\nInISMIR , 2014.\n[32] R. Raina, A. Battle, H. Lee, B. Packer, and A. Y . Ng.\nSelf-taught learning: Transfer learning from unlabeled\ndata. In ICML , pages 759–766. ACM, 2007.\n[33] K. Tokuda, T. Kobayashi, T. Masuko, and S. Imai. Mel-\ngeneralized Cepstral Analysis-A Uniﬁed Approach to\nSpeech Spectral Estimation. In International Confer-\nence on Spoken Language Processing , 1994.\n[34] T. Tommasi, N. Quadrianto, B. Caputo, and C. H.\nLampert. Beyond Dataset Bias: Multi-task Unaligned\nShared Knowledge Transfer. In Asian Conference on\nComputer Vision , pages 1–15. Springer, 2012.\n[35] P. Vincent, H. Larochelle, I. Lajoie, Y . Bengio, and\nP.-A. Manzagol. Stacked Denoising Autoencoders:\nLearning Useful Representations in a Deep Network\nwith a Local Denoising Criterion. Journal of Machine\nLearning Research , 11(Dec):3371–3408, 2010.\n[36] F. Zhou and F. De la Torre. Generalized Time Warp-\ning for Multi-modal Alignment of Human Motion. In\nCVPR , pages 1282–1289, 2012.660 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "The Northwestern University Source Separation Library.",
        "author": [
            "Ethan Manilow",
            "Prem Seetharaman",
            "Bryan Pardo"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492407",
        "url": "https://doi.org/10.5281/zenodo.1492407",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/37_Paper.pdf",
        "abstract": "Audio source separation is the process of isolating individual sonic elements from a mixture or auditory scene. We present the Northwestern University Source Separation Library, or nussl for short. nussl (pronounced 'nuzzle') is an open-source, object-oriented audio source separation library implemented in Python. nussl provides implementations for many existing source separation algorithms and a platform for creating the next generation of source separation algorithms. By nature of its design, nussl easily allows new algorithms to be benchmarked against existing algorithms on established data sets and facilitates development of new variations on algorithms. Here, we present the design methodologies in nussl, two experiments using it, and use nussl to showcase benchmarks for some algorithms contained within.",
        "zenodo_id": 1492407,
        "dblp_key": "conf/ismir/ManilowSP18",
        "keywords": [
            "audio",
            "source",
            "separation",
            "library",
            "Northwestern",
            "University",
            "Source",
            "Separation",
            "Library",
            "nussl"
        ],
        "content": "THE NORTHWESTERN UNIVERSITY SOURCE SEPARATION LIBRARY\nEthan Manilow, Prem Seetharaman, Bryan Pardo\nNorthwestern University\nfethanmanilow@u., prem@u., pardo@ gnorthwestern.edu\nABSTRACT\nAudio source separation is the process of isolating individ-\nual sonic elements from a mixture or auditory scene. We\npresent the Northwestern University Source Separation Li-\nbrary, or nussl for short. nussl (pronounced ‘ nuzzle ’)\nis an open-source, object-oriented audio source separation\nlibrary implemented in Python. nussl provides imple-\nmentations for many existing source separation algorithms\nand a platform for creating the next generation of source\nseparation algorithms. By nature of its design, nussl\neasily allows new algorithms to be benchmarked against\nexisting algorithms on established data sets and facilitates\ndevelopment of new variations on algorithms. Here, we\npresent the design methodologies in nussl , two experi-\nments using it, and use nussl to showcase benchmarks\nfor some algorithms contained within.\n1. INTRODUCTION\nAudio source separation is the process of isolating indi-\nvidual sonic elements from a mixture or auditory scene.\nThe underdetermined case is where there are fewer mix-\nture channels (e.g. a stereo recording) than sources (a\nstring quartet). Examples of underdetermined source sep-\naration include extracting a single speaker from a single-\nmic recording of a crowded cocktail party, extracting a\nsinger from a rock band recording, or removing an extrane-\nous car horn from a ﬁeld recorded interview. Applications\nof source separation include end-user tools for extracting\nvocals (e.g., Audionamix ADX Trax), upmixing vintage\nrecordings to stereo or 5.1 surround sound, and as a pre-\nprocessing step for speech recognition [15] and other audio\ntasks.\nThere have been many approaches taken to source sep-\naration in the underdetermined case. These include Non-\nnegative Matrix Factorization (NMF) [37, 38], harmon-\nic/percussive separation [7], deep learning [11, 13, 14, 16,\n21, 25], pitch tracking [5, 34], spatialization [8, 32], re-\npeating vs non-repeating elements [29,30,36], low-rank vs\nsparse decomposition [12], and common fate [24, 39, 44].\nc\rEthan Manilow, Prem Seetharaman, Bryan Pardo. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Ethan Manilow, Prem Seetharaman, Bryan\nPardo. “The Northwestern University Source Separation Library”, 19th\nInternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.The research community has centered around a col-\nlection of common data sets to benchmark results from\nthese different approaches. Perhaps the best known are the\ndata sets used for the recurring Signal Separation Evalua-\ntion Campaign (SiSEC) [19, 42]. SiSEC previously used\nDSD100 [20],and now uses both DSD100 and MedleyDB\n[1], calling the combined data set MUSDB18 [28]. Other\ncommon data sets include iKala [2], MIR-1K [3], TIMIT\n[10], and WSJ0 [9]. The community also typically uses\nthe signal quality measures provided by BSS-Eval [6, 42]\n(SDR, SIR, and SAR) when reporting results.\nThough there is some debate about this [4], it can be\nargued that using common data sets and evaluation mea-\nsures strengthens research through standardizing metrics\nby making new and existing research directly compara-\nble. While the source separation community has common\ndata sets and common evaluation measures, there exists no\nsuch common code repository for actual implementations\nof proposed algorithms.\nVandewalle et al. [41] argue that in the computational\nsciences, implementation details are crucial to reproduc-\ning the results of academic papers, despite being routinely\nomitted from publications. They establish six degrees of\nreproducibility, scored from 0 (lowest) to 5 (highest). A\nscore of 5 is deﬁned as “The results can be easily repro-\nduced by an independent researcher with at most 15 min of\nuser effort, requiring only standard, freely available tools.”\nA 0 indicates research completely unreproducible by an in-\ndependent researcher.\nThe ubiquity of code repositories like Github has al-\nlowed many researchers to share their code, but using\nGithub is not a guarantee of easy reproducibility. A re-\ncent seminar1convened to reproduce results from six MIR\npapers (including two source separation papers) and con-\ncluded that not a single paper, despite including code ,\nscored better than “Can be reproduced, requiring consid-\nerable effort” using the reproducibility scorecard by Van-\ndewalle et al. [41]. Of the two source separation papers,\nboth scored “Could be reproduced, requiring extreme ef-\nfort.”\nThis work aims to provide a common platform for re-\nsearchers to contribute their source separation algorithms\nto ﬁll the implementation gap and promote reproducibility\nwithin the source separation research community. Further-\nmore, this work strives to make every algorithm in the pro-\nposed framework achieve the highest reproducibility rating\nusing the Vandewalle et al. scorecard: reproducible results\n1https://github.com/audiolabs/APSRR-2016297in under 15 minutes. The Northwestern University Source\nSeparation Library ( nussl ) is the culmination of that ef-\nfort.\nIn this paper we will explore nussl and introduce\nsome core aspects of its design methodology, provide an\noutline about how to add a new algorithm to nussl and\nbenchmark many of the source separation algorithms in\nnussl . We also leverage the ﬂexibility of the nussl\nframework to implement and test novel combinations of\nexisting source separation algorithms. We examine how\nsource separation algorithms interact with methods such as\noverlap and add, which apply the same source separation\nalgorithm to overlapping windows in the mixture and re-\ncombine the sources afterwards, rather than applying them\nto the entire mixture. More information about nussl can\nbe obtained at the project’s online documentation.2A\ncompanion website is also provided for this paper.3\n2. RELATED WORK\nThe biennial Signal Separation Evaluation Campaign\n(SiSEC) [19,42] is an open call for members of the MIR re-\nsearch community to submit source separation algorithms\nto be run and evaluated on a common dataset. While the\ndataset is widely distributed and used, not all of the code\nsubmissions from previous campaigns have been made\navailable to scrutinize. Additionally, SiSEC offers no stan-\ndard API to adhere to, and only a minimal framework to\nwork with. We have submitted many algorithms within\nnussl to the most recent SiSEC campaign.\nOther source separation libraries have been presented\nin the past, as well. The Flexible Audio Source Separation\nToolbox (FASST) [23, 35]4was written in MATLAB and\nC++, but did not have a process for outside submissions.\nuntwist [33] is an open source Python source separa-\ntion library, but it is based on a different design framework\nthannussl , implements a different set of algorithms than\nnussl , and has no built-in interfaces for common evalua-\ntion metrics, data sets, or loading pre-trained models.\n3. DESIGN FRAMEWORK OF NUSSL\nnussl is built with extensibility in mind. It would be im-\npossible to provide implementations for every source sep-\naration algorithm upon the announcement of this library.\nAs such nussl is built to an API so that the community\ncan easily add their own algorithms, models, datasets and\nhave them automatically work with every other aspect of\nnussl .\nUnder the hood, nussl uses many common Python\ntools for signal processing and machine learning,\nsuch as librosa ,numpy ,scipy ,scikit-learn ,\nmireval ,musdb , and museval , so developing with\nnussl should be familiar to any MIR researcher working\nin Python.\n2https://interactiveaudiolab.github.io/nussl\n3https://interactiveaudiolab.github.io/demos/nussl.html\n4Related Python library: https://github.com/wslihgt/pyfasst/1import nussl\n2\n3# Load audio\n4signal = nussl.AudioSignal(’path/to/mix.wav’)\n5\n6# Run REPET for foreground/background separation\n7algorithm = nussl.Repet(sig)\n8algorithm.run()\n9fg, bg = algorithm.make_audio_signals()\n10\n11# Save results to wav files\n12fg.write_audio_to_file(’fg.wav’)\n13bg.write_audio_to_file(’bg.wav’)\nFigure 1 : Using nussl to run a single algorithm (REPET\n[30] for foreground/background separation) on a single\nmixture. In a recent seminar on reproducibility, REPET\nscored “could be reproduced, requiring extreme effort.”\nnussl aims to improve the reproducibility score of mul-\ntiple source separation algorithms, including REPET.\nIn the next sections, we provide a high-level overview\nof some of the more important aspects of the nussl API.\nFor more information, please see our full online documen-\ntation.\n3.1AudioSignal\nThe main entry point to nussl for end-users and algo-\nrithm developers is through the AudioSignal object.\nTheAudioSignal object has methods for reading and\nwriting audio, padding or truncating the audio, adding and\nsubtracting audio signals from one another, checking and\naltering properties of the audio, computing invertible sig-\nnal transforms (e.g. short time Fourier transform), and\nmuch more. AudioSignal can read all of the most com-\nmon audio codecs. Once in memory, audio is represented\nas a 2-dimensional (channels and time series within a chan-\nnel)numpy array of pulse-code modulated (PCM) sam-\nples.\nAll source separation algorithms in nussl accept as\ntheir ﬁrst argument an AudioSignal object. Each\nalgorithm copies the content of the audio object, per-\nforms separation on that copy and returns a set of of new\nAudioSignal objects, one per source, leaving the origi-\nnalAudioSignal object unchanged.\n3.2 Source Separation Algorithms\nAll source separation algorithms in nussl are encapsu-\nlated in classes that are derived from SeparationBase .\nFor each class, the constructor does minimal set up, the\nrun() method does the computation required for the\nsource separation, and the make audio signals()\nmethod returns AudioSignal objects containing the es-\ntimated signals. An example of this whole process is\nshown in Figure 1.\n3.2.1 MaskSeparationBase vsSeparationBase\nSource separation algorithms in nussl are segregated into\ntwo categories: those that produce a mask and apply it298 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Algorithms in nussl\nRepetition Other Fore/Background Spatialization Composite\nRepet [31] Harmonic/Percussive (HPSS) [7] DUET [32] Overlap/Add\nRepetSim [29] Melody Masking (Melodia [34]) PROJET [8] Algorithm Picker [22]\n2DFT [36] Component Analysis Benchmarking Neural Networks\nMatrix Decomposition ICA [18] High/Low Pass Filter Deep Clustering [11, 21]\nNMF w/ MFCC Clustering [38] RPCA [12] Ideal Mask\nTable 1 : Source Separation algorithms by category currently implemented in nussl .\nto a representation (e.g. a spectrogram) built from the\nwaveform, and those that do separation via other means\n(e.g. time domain methods such as independent com-\nponent analysis). The former group of algorithms are\nderived from the MaskSeparationBase base class,\nwhich is a subclass of the SeparationBase base\nclass. The run() method in MaskSeparationBase -\nderived algorithms are expected to return mask ob-\njects (see Section 3.2.2). Some algorithms inherit di-\nrectly from SeparationBase and have no require-\nment about what their run() method returns. With\nMaskSeparationBase separation classes, it is easy to\nswitch between running an algorithm with a binary or soft\nmask.\n3.2.2 Masks\nMasks are encapsulated by the MaskBase base class.\nSoftMask andBinaryMask are the two classes that de-\nrive from MaskBase .MaskBase -derived objects have a\nnumpy array that contains the data, and utilities for apply-\ning masks to AudioSignal objects. SoftMask objects\nare applied using a classical approach:\n^S(i)\n!;t=v(i)\n!;tPN\ni=0v(i)\n!;t\nHere, v(i)\n!;tis the estimate of source iat frequency !\nand time t,^S(i)\n!;tis the value of the mask for that source\nat that time and frequency, and Nis the total number of\nsources. The BinaryMask objects simply put a 1when\na source estimate dominates all other source estimates in a\ntime-frequency bin and a 0elsewhere. More masking types\n(e.g. consistent Wiener ﬁltering [17]) can be implemented\nby subclassing MaskBase .\n3.3 Evaluation\nnussl also has a common interface to evaluate the esti-\nmates from source separation algorithms using established\nmetrics, such as BSS-Eval [6] using implementations from\nmireval [26] or museval [40].nussl also has meth-\nods for comparing binary masks to an ideal binary mask\nusing accuracy, precision, recall, and F-Score [43]. Simi-\nlar to the rest of nussl , all of the evaluation metrics are\nencapsulated by the EvaluationBase base class so that\nall of its child classes are built to a common API.1import nussl\n2\n3m1k = ’path/to/MIR-1K’\n4\n5# List of algorithms to test\n6sep_classes = [nussl.RepetSim, nussl.Melodia]\n7\n8# Loop through all of MIR-1K\n9for mix, vox, acc in nussl.datasets.mir1k(m1k):\n10 mix.to_mono(overwrite=True)\n11\n12 for alg in sep_classes:\n13\n14 # Run the algorithm\n15 a = alg(mix)\n16 a.run()\n17 est = a.make_audio_signals()\n18\n19 # Evaluate results\n20 gt = [acc, vox] # Ground truth\n21 bss = nussl.evaluate.BssEval(mix, gt, est)\n22 scores = bss.evaluate()\nFigure 2 : Running two algorithms on allof MIR-1K and\nevaluating using BSS-Eval.\n3.4 Data Sets\nAlthough nussl does not ship with any data sets, it does\nprovide “hooks” for interfacing with common data sets.\nThe hooks are basic utilities for reading the audio ﬁles into\nAudioSignal objects. The user ﬁrst points nussl to\nthe top-level directory of the downloaded data set. The\nutilities can then iterate through every audio ﬁle, a sub-\nset of ﬁles, or shufﬂe the order in which they are read.\nThere structure of the directories is assumed to be Data\nsets that nussl can currently interface with include iKala\n[2], MIR-1K [3], MUSDB18 [28] (using musdb ), and\nDSD100 [20]. An example of running multiple algorithms\non the entirety of MIR-1K and evaluating the results using\nBSS-Eval is shown in Figure 2.\n3.5 Modelers and Deep Learning Models\nnussl also contains a section for generic modeling and\nmatrix manipulation classes. Classes in this section are\nnot source separation algorithms, but are used by the al-\ngorithms in nussl . An example is the Non-negative Ma-\ntrix Factorization (NMF) class, NMF. This receives a non-\nnegative numpy matrix as input, factorizes it into a tem-\nplate matrix and an activation matrix, and outputs the two\nresults to be used by a separation algorithm. It does not in-\nput or output audio, spectrograms, or masks. For those util-Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 299Figure 3 : SDR evaluations for vocal estimates using\nOverlapApp on the MIR-1K data set. Three different\nwindow sizes are shown: 10 sec (red), 20 sec (blue), 30\nsec (yellow). Hops are half of the window size.\nities, a wrapper separation class is needed, like NMFMFCC ,\nwhich clusters the templates using mel-frequency cepstral\ncoefﬁcients. Other classes train deep learning models for\nseparation use. Classes in this section have a more lax API\nbecause of their heterogeneous nature.\nnussl currently supports deep learning models written\ninPyTorch , but does not ship with any pre-trained mod-\nels, only the code to train with. Similar to frameworks\nlikePyTorch ,nussl offers a way to download pre-\ntrained models from nussl servers for algorithms which\nrequire them. Developers can see what models exist on\nour servers and download a models via utilities built into\nnussl . There also exists a process for contributers to up-\nload their own pre-trained models (see Section 4.2 for more\ndetails).\n4. ALGORITHMS IN NUSSL\n4.1 Currently in nussl\nAt the time of this writing, the source separation algo-\nrithms are implemented in nussl to the API speciﬁca-\ntion are presented in Table 1, by category. The algorithms\ncurrently in nussl provide a good starting point for fu-\nture benchmark work, and we hope to expand the set of\noffered algorithms to include many more state-of-the-art\napproaches.\n4.2 Adding new algorithms to nussl\nThe process of adding new source separation algorithms\ninto nus nussl sl is similar to other open source projects,\nin many ways. A researcher who wishes to add an algo-\nrithm must clone the Github repository, make a new branch\nfor their algorithm, add their code, push to Github, and\nthen create a pull request. At this point, the nussl con-\ntributing process deviates from that standard open-source\nprocess.\nAfter the new code passes the style and error checks, the\nresearcher must provide benchmark ﬁles for tests. Thesecan be created by using standard metrics on a set of exam-\nple ﬁles. For example, when adding a new algorithm, a re-\nsearcher could provide BSS-Eval metrics on a few songs\nfrom MIR-1K dataset. If an implementation existed else-\nwhere prior to being incorporated into nussl , then a copy\nof the original implementation will be requested to bench-\nmark against. Authors of new algorithms, must also pro-\nvide a reference to a paper or other documentation which\noutlines the algorithm in more detail. Additionally, any\nlarge supplemental materials that are needed for the algo-\nrithm (such as pre-trained neural network models) must be\nprovided so that they can be distributed through nussl ’s\nAPI as outlined in Section 3.5.\nAll of this is outlined in more detail on the contributions\nsection of the nussl Github page and documentation.\n5. EXAMPLE USES OF NUSSL\nBecause all of the algorithms and supporting infrastructure\ninnussl are built to an API, this allows a very simple\nway to ﬁnd novel combinations of multiple source separa-\ntion algorithms and evaluate them on a variety of data sets\nunder different evaluation metrics. In this section, we will\nshowcase two novel experiments using nussl and present\nresults from these experiments.\n5.1 Cascading algorithms\nThenussl API facilitates combining several different al-\ngorithms. To illustrate this point, we reproduce and expand\nupon work demonstrated by Raﬁi et al. [27] in combining\nrhythm-based and pitch-based approaches to source sepa-\nration.\nRaﬁi et al. present two methods for cascading algo-\nrithms: Parallel, where the background and foreground\nmasks created by each algorithm are combined after the\nalgorithms run on the mixture; and, Series, where the fore-\nground estimation of algorithm A is fed in as the “mixture”\nto algorithm B. The mask estimates, in each case, are com-\nbined using weighted Weiner Filtering.\nFor this experiment, we use four background/fore-\nground algorithms, RepetSim, Separation via 2DFT,\nRCPA, and Melodic masking with Melodia. We chose\neach pair from the set of algorithms and resulting in a total\nof 16 combinations. Based on values reported by Raﬁi et\nal., for running in Parallel we set wB= 1:0andwM= 0:3\nas the weights of the background and foreground masks,\nrespectively. We set the weight parameter w= 0:5for\nrunning in Series. All algorithms created soft masks. We\nevaluated results using BSS-Eval on the undivided MIR-\n1K data set.5Mean SDR values (with 1 standard devia-\ntion) are shown in Figure 5 for vocals. We ﬁnd that series\nconﬁgurations outperform parallel conﬁgurations overall,\nand RepetSim is best as a second algorithm run especially\nwhen it is also the ﬁrst.\n5MIR-1K has 110 tracks of mean duration 72:7\u000617:3seconds, that\nare divided into 1000 smaller tracks of 8:0\u00061:8seconds. The divided\ntracks are too small to capture multiple repetitions.300 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018(a) Benchmarks for the iKala data set. Algorithms apply binary masks to the mixtures and the results were evaluated using\nprecision, recall, F-Score, and accuracy (precision is red, recall is blue, F-Score is yellow, and accuracy is pink).\n(b) Benchmarks for the MIR-1K data set. Algorithms apply binary masks to the mixtures and the results were evaluated\nusing BSS-Eval (SDR is red, ISR is blue, SIR is yellow, and SAR is pink).\n(c) Benchmarks for the MUSDB18 data set. Algorithms apply soft masks to the mixtures and the results were evaluated\nusing BSS-Eval (SDR is red, ISR is blue, SIR is yellow, and SAR is pink).\nFigure 4 : Illustrative benchmarks for a set of algorithms and conﬁgurations in nussl .Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 301(a) Running in algorithms in Parallel.\n (b) Series\nFigure 5 : Mean SDR for vocal estimations from cascading pairs of algorithms.\n5.2 Combining algorithms with Overlap/Add\nIn addition to classes that run one type of algorithm\n(likeHPSS , etc), nussl also contains composite algorithm\nclasses, i.e.those that run other algorithms in nussl . One\nsuch simple example is the OverlapAdd class, which\ndoes the overlap add method when running an algorithm.\nIn this experiment, we ran two repetition-based algo-\nrithms, RepetSim and Separation via 2DFT, wrapped in the\nOverlap/Add class to do vocal extraction. We tested three\ndifferent window lengths, 10, 20, and 30 seconds, with hop\nlength at half of the window length and using Hamming\nwindows. We ran this experiment on the undivided MIR-\n1K data set6and evaluated the estimates using BSS-Eval.\nResults from this experiment show that smaller windows\nlead to better vocal separation performance, according to\nSDR. These results are shown in Figure 3.\n6. BENCHMARKS\nIn this section, we provide a selection of benchmarks for\na set of algorithms in nussl . We benchmarked all al-\ngorithms that explicitly perform vocal separation with de-\nterministic output source ordering ( i.e.for an output ar-\nray of sources, accompaniment is always index 0 and vo-\ncals is always index 1). We ran the algorithms on the\niKala, MIR-1K, and MUSDB18 data sets. We ran REPET,\nREPET-SIM, Separation by 2DFT, HPSS, Masking from\nPitch Tracking (using Melodia as the pitch tracker), RPCA,\nHigh/Low Pass ﬁltering (cutoff at 100Hz).\nFor brevity, we only report one evaluation type for each\ndata set here. We aim not to be complete, but rather show-\ncase what nussl is capable of. For iKala, we show\nprecision/recall/F-Score/accuracy computed from output\nbinary masks, Figure 4a. For MIR-1K, we show BSS-Eval\nmetrics computed from estimates using binary masks, Fig-\nure 4b. And for MUSDB18, we show BSS-Eval metrics\ncomputed from estimates using soft masks, Figure 4c.\n6We excluded two signals that were shorter than the largest window\nsizes.All algorithms were run using the default parame-\nter values for the algorithm in nussl ’s implementation.\nSpeciﬁcs of all of the parameters are contained in the\nproject’s documentation website.\n7. FUTURE WORK AND CONCLUSION\nIn the future, we hope to expand upon nussl in a num-\nber of ways. First, while nussl is currently focused on\nmusical source separation (the expertise of its authors), we\nwould like to expand it to include source separation meth-\nods for speech. This would also necessitate adding hooks\nfor speech data sets (like TIMIT and WSJ0) and adding\npre-trained models for speech. Second, we would like to\nadd an extensible API for spectral transformations. Cur-\nrently, the STFT is at the core of AudioSignal , but in\nthe future, it should be abstracted so that it is easy to run\nany algorithm on a CQT, Mel-Spaced STFT, etc.\nFinally, and importantly, we would like buy-in from the\nMIR and audio community. The aim of nussl is to be-\ncome the community’s central repository for audio source\nseparation. This goal is impossible without the support and\ncontributions of the research community. We encourage\ninterested participants to read the guidelines for contribut-\ning on this project’s documentation page and get involved.\nWe have presented the Northwestern University Source\nSeparation Library ( nussl ), an open-source, object-\noriented audio source separation library implemented in\nPython. nussl implements many popular source sep-\naration algorithms, and a low barrier API for end-users\nand developers alike. We have demonstrated its design\nframework, including its ability to interface with common\ndata sets and evaluation metrics. We also showcased two\nnovel experiments using the API and a set of benchmarks.\nThis project is actively seeking submissions from eager re-\nsearchers and avid open source developers. Readers can\nﬁnd more information at interactiveaudiolab.\ngithub.io/nussl . This work was supported by USA\nNational Science Foundation Award 1420971.302 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20188. REFERENCES\n[1] Rachel M. Bittner, Justin Salamon, Mike Tierney,\nMatthias Mauch, Chris Cannam, and Juan Pablo Bello.\nMedleyDB: A Multitrack Dataset for Annotation-\nIntensive MIR Research. 15th International Society for\nMusic Information Retrieval Conference, ISMIR 2014,\nTaipei, Taiwan, October 27-31, 2014 , pages 155–160,\n2014.\n[2] Tak-Shing Chan, Tzu-Chun Yeh, Zhe-Cheng Fan,\nHung-Wei Chen, Li Su, Yi-Hsuan Yang, and Roger\nJang. V ocal Activity Informed Singing V oice Separa-\ntion with the iKala Dataset. In 2015 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , pages 718–722, April 2015.\n[3] Chao-Ling Hsu and J.-S.R. Jang. On the Improvement\nof Singing V oice Separation for Monaural Recordings\nUsing the MIR-1K Dataset. IEEE Transactions on Au-\ndio, Speech, and Language Processing , 18(2):310–\n319, February 2010.\n[4] Chris Drummond and Nathalie Japkowicz. Warning:\nStatistical Benchmarking is Addictive. Kicking the\nHabit in Machine Learning. Journal of Experimental &\nTheoretical Artiﬁcial Intelligence , 22(1):67–80, March\n2010.\n[5] Jean-Louis Durrieu, Bertrand David, and Ga ¨el\nRichard. A Musically Motivated Mid-Level Represen-\ntation for Pitch Estimation and Musical Audio Source\nSeparation. IEEE Journal of Selected Topics in Signal\nProcessing , 5(6):1180–1191, October 2011.\n[6] C ´edric F ´evotte, R ´emi Gribonval, and Emmanuel\nVincent. Bss eval toolbox user guide–revision 2.0.\npage 19, 2005.\n[7] Derry FitzGerald. Harmonic/Percussive Separation\nUsing Median Filtering. Proceedings of the 13th Inter-\nnational Conference on Digital Audio Effects , 2010.\n[8] Derry FitzGerald, Antoine Liutkus, and Roland\nBadeau. PROJET — Spatial Audio Separation Using\nProjections. In 2016 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\npages 36–40, March 2016.\n[9] John Garofalo, David Graff, Doug Paul, and David Pal-\nlett. Continous speech recognition (csr-i) wall street\njournal (wsj0) news, complete. Linguistic Data Con-\nsortium, Philadelphia , 1993.\n[10] John S Garofolo, Lori F Lamel, William M Fisher,\nJonathan G Fiscus, and David S Pallett. Darpa timit\nacoustic-phonetic continous speech corpus cd-rom.\nnist speech disc 1-1.1. NASA STI/Recon technical re-\nport n , 93, 1993.\n[11] John R. Hershey, Zhuo Chen, Jonathan Le Roux, and\nShinji Watanabe. Deep Clustering: Discriminative Em-\nbeddings for Segmentation and Separation. In 2016IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , pages 31–35. IEEE,\nMarch 2016.\n[12] Po-Sen Huang, Scott Deeann Chen, Paris Smaragdis,\nand Mark Hasegawa-Johnson. Singing-V oice Separa-\ntion from Monaural Recordings Using Robust Princi-\npal Component Analysis. In 2012 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , pages 57–60. IEEE, March 2012.\n[13] Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson,\nand Paris Smaragdis. Deep Learning for Monaural\nSpeech Separation. In 2014 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 1562–1566. IEEE, May 2014.\n[14] Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson,\nand Paris Smaragdis. Singing-V oice Separation from\nMonaural Recordings using Deep Recurrent Neural\nNetworks. 15th International Society for Music Infor-\nmation Retrieval Conference, ISMIR 2014, Taipei, Tai-\nwan, October 27-31, 2014 , pages 477–482, 2014.\n[15] Yusuf Isik, Jonathan Le Roux, Zhuo Chen, Shinji\nWatanabe, and John R Hershey. Single-Channel Multi-\nSpeaker Separation Using Deep Clustering. arXiv\npreprint arXiv:1607.02173 , 2016.\n[16] Minje Kim and Paris Smaragdis. Bitwise Neural Net-\nworks. In International Conference on Machine Learn-\ning (ICML) Workshop on Resource-Efﬁcient Machine\nLearning , Lille, France, Jul 2015.\n[17] Jonathan Le Roux and Emmanuel Vincent. Con-\nsistent Wiener Filtering for Audio Source Separa-\ntion. IEEE Signal Processing Letters , 20(3):217–220,\nMarch 2013.\n[18] Te-Won Lee. Independent Component Analysis: The-\nory and Applications . Springer, New York; London,\n2011. OCLC: 752483521.\n[19] Antoine Liutkus, Fabian-Robert St ¨oter, Zafar Raﬁi,\nDaichi Kitamura, Bertrand Rivet, Nobutaka Ito, Nobu-\ntaka Ono, and Julie Fontecave. The 2016 SIgnal Sepa-\nration Evaluation Campaign. In Latent Variable Anal-\nysis and Signal Separation - 13th International Con-\nference, LVA/ICA 2017, Grenoble, France, February\n21-23, 2017, Proceedings , pages 323–332, 2017.\n[20] Antoine Liutkus, Fabian-Robert St ¨oter, Zafar Raﬁi,\nDaichi Kitamura, Bertrand Rivet, Nobutaka Ito, Nobu-\ntaka Ono, and Julie Fontecave. The 2016 Signal Sepa-\nration Evaluation Campaign. In International Confer-\nence on Latent Variable Analysis and Signal Separa-\ntion, pages 323–332. Springer, 2017.\n[21] Yi Luo, Zhuo Chen, John R. Hershey, Jonathan\nLe Roux, and Nima Mesgarani. Deep Clustering\nand Conventional Networks for Music Separation:Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 303Stronger Together. In 2017 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 61–65. IEEE, March 2017.\n[22] Ethan Manilow, Prem Seetharaman, Fatemeh Pishda-\ndian, and Bryan Pardo. Predicting Algorithm Efﬁcacy\nfor Adaptive Multi-Cue Source Separation. In 2017\nIEEE Workshop on Applications of Signal Processing\nto Audio and Acoustics (WASPAA) , pages 274–278,\nOct 2017.\n[23] Alexey Ozerov and Emmanuel Vincent. Using the\nFASST source separation toolbox for noise robust\nspeech recognition. In International Workshop on Ma-\nchine Listening in Multisource Environments (CHiME\n2011) , Florence, Italy, Sept 2011.\n[24] Fatemeh Pishdadian, Bryan Pardo, and Antoine Li-\nutkus. A Multi-resolution Approach to Common Fate-\nbased Audio Separation. In 2017 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , pages 566–570. IEEE, March 2017.\n[25] Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson,\nand Paris Smaragdis. Joint Optimization of Masks\nand Deep Recurrent Neural Networks for Monaural\nSource Separation. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing , 23(12):2136–2147,\nDecember 2015.\n[26] Colin Raffel, Brian McFee, Eric J. Humphrey, Justin\nSalamon, Oriol Nieto, Dawen Liang, and Daniel P. W.\nEllis. MIR EV AL: A Transparent Implementation of\nCommon MIR Metrics. In 15th International Soci-\nety for Music Information Retrieval Conference, IS-\nMIR 2014, Taipei, Taiwan, October 27-31, 2014 , pages\n367–372, 2014.\n[27] Zafar Raﬁi, Zhiyao Duan, and Bryan Pardo. Combin-\ning Rhythm-Based and Pitch-Based Methods for Back-\nground and Melody Separation. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing ,\n22(12):1884–1893, December 2014.\n[28] Zafar Raﬁi, Antoine Liutkus, Fabian-Robert Stter,\nStylianos Ioannis Mimilakis, and Rachel Bittner. The\nMUSDB18 corpus for music separation, December\n2017.\n[29] Zafar Raﬁi and Bryan Pardo. Music/V oice Separation\nUsing the Similarity Matrix. 13th International Soci-\nety for Music Information Retrieval Conference, ISMIR\n2012, Mosteiro S.Bento Da Vit ´oria, Porto, Portugal,\nOctober 8-12, 2012 , pages 583–588, 2012.\n[30] Zafar Raﬁi and Bryan Pardo. REpeating Pattern Ex-\ntraction Technique (REPET): A Simple Method for\nMusic/V oice Separation. IEEE Transactions on Audio,\nSpeech, and Language Processing , 21(1):73–84, Jan\n2013.[31] Zafar Raﬁi and Bryan Pardo. REpeating Pattern Ex-\ntraction Technique (REPET): A Simple Method for\nMusic/V oice Separation. IEEE Transactions on Audio,\nSpeech, and Language Processing , 21(1):73–84, Jan-\nuary 2013.\n[32] Scott Rickard. The DUET blind source separation al-\ngorithm. In Blind Speech Separation , pages 217–241.\nSpringer, 2007.\n[33] Gerard Roma, Emad M Grais, Andrew JR Simpson,\nIwona Sobieraj, and Mark D Plumbley. Untwist: A\nNew Toolbox for Audio Source Separation. Extended\nabstracts for the Late-Breaking Demo Session of the\n17th International Society for Music Information Re-\ntrieval Conference, ISMIR 2016, New York City, United\nStates, August 7-11, 2016 , 2016.\n[34] Justin Salamon and Emilia Gomez. Melody Extraction\nFrom Polyphonic Music Signals Using Pitch Contour\nCharacteristics. IEEE Transactions on Audio, Speech,\nand Language Processing , 20(6):1759–1770, August\n2012.\n[35] Yann Sala ¨un, Emmanuel Vincent, Nancy Bertin,\nNathan Souviraa-Labastie, Xabier Jaureguiberry,\nDung T Tran, and Fr ´ed´eric Bimbot. The Flexible\nAudio Source Separation Toolbox Version 2.0. In\nICASSP , 2014.\n[36] Prem Seetharaman, Fatemeh Pishdadian, and Bryan\nPardo. Music/V oice Separation Using the 2D Fourier\nTransform. In 2017 IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics (WASPAA) ,\npages 36–40, Oct 2017.\n[37] Paris Smaragdis and J.C. Brown. Non-Negative Ma-\ntrix Factorization for Polyphonic Music Transcrip-\ntion. In 2003 IEEE Workshop on Applications of Sig-\nnal Processing to Audio and Acoustics (IEEE Cat.\nNo.03TH8684) , pages 177–180. IEEE, 2003.\n[38] Martin Spiertz and V olker Gnann. Source-Filter Based\nClustering for Monaural Blind Source Separation. Pro-\nceedings of the 12th International Conference on Dig-\nital Audio Effects , 2009.\n[39] Fabian-Robert Stoter, Antoine Liutkus, Roland\nBadeau, Bernd Edler, and Paul Magron. Common Fate\nModel for Unison Source Separation. In 2016 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , pages 126–130. IEEE,\nMarch 2016.\n[40] Fabian-Robert St ¨oter, Antoine Liutkus, and Nobutaka\nIto. The 2018 signal separation evaluation campaign.\nInInternational Conference on Latent Variable Anal-\nysis and Signal Separation , pages 293–305. Springer,\n2018.\n[41] Patrick Vandewalle, Jelena Kovacevic, and Martin\nVetterli. Reproducible Research in Signal Processing.304 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018IEEE Signal Processing Magazine , 26(3):37–47, May\n2009.\n[42] Emmanuel Vincent, Shoko Araki, Fabian Theis, Guido\nNolte, Pau Boﬁll, Hiroshi Sawada, Alexey Oze-\nrov, Vikrham Gowreesunker, Dominik Lutter, and\nNgoc Q.K. Duong. The SIgnal Separation Evaluation\nCampaign (2007-2010): Achievements and Remaining\nChallenges. Signal Processing , 92(8):1928–1936, Au-\ngust 2012.\n[43] DeLiang Wang. On ideal binary mask as the com-\nputational goal of auditory scene analysis. In Speech\nseparation by humans and machines , pages 181–197.\nSpringer, 2005.\n[44] Guy Wolf, Stephane Mallat, and Shihab Shamma.\nRigid Motion Model for Audio Source Separation.\nIEEE Transactions on Signal Processing , 64(7):1822–\n1831, April 2016.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 305"
    },
    {
        "title": "Conditioning Deep Generative Raw Audio Models for Structured Automatic Music.",
        "author": [
            "Rachel Manzelli",
            "Vijay Thakkar",
            "Ali Siahkamari",
            "Brian Kulis"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492375",
        "url": "https://doi.org/10.5281/zenodo.1492375",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/192_Paper.pdf",
        "abstract": "Existing automatic music generation approaches that feature deep learning can be broadly classified into two types: raw audio models and symbolic models. Symbolic models, which train and generate at the note level, are currently the more prevalent approach; these models can capture long-range dependencies of melodic structure, but fail to grasp the nuances and richness of raw audio generations. Raw audio models, such as DeepMind's WaveNet, train directly on sampled audio waveforms, allowing them to produce realistic-sounding, albeit unstructured music. In this paper, we propose an automatic music generation methodology combining both of these approaches to create structured, realistic-sounding compositions. We consider a Long Short Term Memory network to learn the melodic structure of different styles of music, and then use the unique symbolic generations from this model as a conditioning input to a WaveNet-based raw audio generator, creating a model for automatic, novel music. We then evaluate this approach by showcasing results of this work.",
        "zenodo_id": 1492375,
        "dblp_key": "conf/ismir/ManzelliTSK18",
        "keywords": [
            "automatic music generation",
            "deep learning",
            "symbolic models",
            "raw audio models",
            "melodic structure",
            "WaveNet",
            "structured compositions",
            "novel music",
            "Long Short Term Memory network",
            "stylistic conditioning"
        ],
        "content": "CONDITIONING DEEP GENERATIVE RAW AUDIO MODELS FOR\nSTRUCTURED AUTOMATIC MUSIC\nRachel Manzelli\u0003Vijay Thakkar\u0003Ali Siahkamari Brian Kulis\n\u0003Equal contributions ECE Department, Boston University\nfmanzelli, thakkarv, siaa, bkulis g@bu.edu\nABSTRACT\nExisting automatic music generation approaches that fea-\nture deep learning can be broadly classiﬁed into two types:\nraw audio models and symbolic models. Symbolic mod-\nels, which train and generate at the note level, are cur-\nrently the more prevalent approach; these models can cap-\nture long-range dependencies of melodic structure, but fail\nto grasp the nuances and richness of raw audio genera-\ntions. Raw audio models, such as DeepMind’s WaveNet,\ntrain directly on sampled audio waveforms, allowing them\nto produce realistic-sounding, albeit unstructured music.\nIn this paper, we propose an automatic music generation\nmethodology combining both of these approaches to cre-\nate structured, realistic-sounding compositions. We con-\nsider a Long Short Term Memory network to learn the\nmelodic structure of different styles of music, and then use\nthe unique symbolic generations from this model as a con-\nditioning input to a WaveNet-based raw audio generator,\ncreating a model for automatic, novel music. We then eval-\nuate this approach by showcasing results of this work.\n1. INTRODUCTION\nThe ability of deep neural networks to generate novel mu-\nsical content has recently become a popular area of re-\nsearch. Many variations of deep neural architectures have\ngenerated pop ballads,1helped artists write melodies,2\nand even have been integrated into commercial music gen-\neration tools.3\nCurrent music generation methods are largely focused\non generating music at the note level, resulting in outputs\nconsisting of symbolic representations of music such as se-\nquences of note numbers or MIDI-like streams of events.\nThese methods, such as those based on Long Short Term\nMemory networks (LSTMs) and recurrent neural networks\n(RNNs), are effective at capturing medium-scale effects\nin music, can produce melodies with constraints such as\nmood and tempo, and feature fast generation times [14,22].\n1http://www.ﬂow-machines.com/\n2https://www.ampermusic.com/\n3https://www.jukedeck.com/\nc\rRachel Manzelli, Vijay Thakkar, Ali Siahkamari, Brian\nKulis. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Rachel Manzelli, Vijay Thakkar,\nAli Siahkamari, Brian Kulis. “Conditioning Deep Generative Raw Audio\nModels for Structured Automatic Music”, 19th International Society for\nMusic Information Retrieval Conference, Paris, France, 2018.In order to create sound, these methods often require an in-\ntermediate step of interpretation of the output by humans,\nwhere the symbolic representation transitions to an audio\noutput in some way.\nAn alternative is to train on and produce raw audio\nwaveforms directly by adapting speech synthesis models,\nresulting in a richer palette of potential musical outputs,\nalbeit at a higher computational cost. WaveNet, a model\ndeveloped at DeepMind primarily targeted towards speech\napplications, has been applied directly to music; the model\nis trained to predict the next sample of 8-bit audio (typi-\ncally sampled at 16 kHz) given the previous samples [25].\nInitially, this was shown to produce rich, unique piano\nmusic when trained on raw piano samples. Follow-up\nwork has developed faster generation times [16], generated\nsynthetic vocals for music using WaveNet-based architec-\ntures [3], and has been used to generate novel sounds and\ninstruments [8]. This approach to music generation, while\nvery new, shows tremendous potential for music genera-\ntion tools. However, while WaveNet produces more real-\nistic sounds, the model does not handle medium or long-\nrange dependencies such as melody or global structure in\nmusic. The music is expressive and novel, yet sounds un-\npracticed in its lack of musical structure.\nNonetheless, raw audio models show great potential for\nthe future of automatic music. Despite the expressive na-\nture of some advanced symbolic models, those methods re-\nquire constraints such as mood and tempo to generate cor-\nresponding symbolic output [22]. While these constraints\ncan be desirable in some cases, we express interest in gen-\nerating structured raw audio directly due to the ﬂexibility\nand versatility that raw audio provides; with no speciﬁca-\ntion, these models are able to learn to generate expression\nand mood directly from the waveforms they are trained on.\nWe believe that raw audio models are a step towards less\nguided, unsupervised music generation, since they are un-\nconstrained in this way. With such tools for generating raw\naudio, one can imagine a number of new applications, such\nas the ability to edit existing raw audio in various ways.\nThus, we explore the combination of raw audio and\nsymbolic approaches, opening the door to a host of new\npossibilities for music generation tools. In particular,\nwe train a biaxial Long Short Term Memory network\nto create novel symbolic melodies, and then treat these\nmelodies as an extra conditioning input to a WaveNet-\nbased model. Consequently, the LSTM model allows us\nto represent long-range melodic structure in the music,182while the WaveNet-based component interprets and ex-\npands upon the generated melodic structure in raw audio\nform. This serves to both eliminate the intermediate inter-\npretation step of the symbolic representations and provide\nstructure to the output of the raw audio model, while main-\ntaining the aforementioned desirable properties of both\nmodels.\nWe ﬁrst discuss the tuning of the original unconditioned\nWaveNet model to produce music of different instruments,\nstyles, and genres. Once we have tuned this model appro-\npriately, we then discuss our extension to the conditioned\ncase, where we add a local conditioning technique to the\nraw audio model. This method is comparable to using a\ntext-to-speech method within a speech synthesis model.\nWe ﬁrst generate audio from the conditioned raw audio\nmodel using well-known melodies (e.g., a C major scale\nand the Happy Birthday melody) after training on the Mu-\nsicNet dataset [24]. We also discuss an application of our\ntechnique to editing existing raw audio music by changing\nsome of the underlying notes and re-generating selections\nof audio. Then, we incorporate the LSTM generations as a\nunique symbolic component. We demonstrate results of\ntraining both the LSTM and our conditioned WaveNet-\nbased model on corresponding training data, as well as\nshowcase and evaluate generations of realistic raw audio\nmelodies by using the output of the LSTM as a unique lo-\ncal conditioning time series to the WaveNet model.\nThis paper is an extension of an earlier work originally\npublished as a workshop paper [19]. We augment that\nwork-in-progress model in many aspects, including more\nconcrete results, stronger evaluation, and new applications.\n2. BACKGROUND\nWe elaborate on two prevalent deep learning models for\nmusic generation, namely raw audio models and symbolic\nmodels.\n2.1 Raw Audio Models\nInitial efforts to generate raw audio involved models used\nprimarily for text generation, such as char-rnn [15] and\nLSTMs. Raw audio generations from these networks are\noften noisy and unstructured; they are limited in their ca-\npacity to abstract higher level representations of raw audio,\nmainly due to problems with overﬁtting [21].\nIn 2016, DeepMind introduced WaveNet [25], a gen-\nerative model for general raw audio, designed mainly for\nspeech applications. At a high level, WaveNet is a deep\nlearning architecture that operates directly on a raw audio\nwaveform. In particular, for a waveform modeled by a vec-\ntorx=fx1;:::;x Tg(representing speech, music, etc.), the\njoint probability of the entire waveform is factorized as a\nproduct of conditional probabilities, namely\np(x) =p(x1)TY\nt=2p(xtjx1;:::;x t\u00001): (1)\nThe waveforms in WaveNet are typically represented as\n8-bit audio, meaning that each xican take on one of\nFigure 1 : A stack of dilated causal convolutions as used\nby WaveNet, reproduced from [25].\n256 possible values. The WaveNet model uses a deep\nneural network to model the conditional probabilities\np(xtjx1;:::;x t\u00001). The model is trained by predicting val-\nues of the waveform at step tand comparing them to the\ntrue valuext, using cross-entropy as a loss function; thus,\nthe problem simply becomes a multi-class classiﬁcation\nproblem (with 256 classes) for each timestep in the wave-\nform.\nThe modeling of conditional probabilities in WaveNet\nutilizes causal convolutions, similar to masked convolu-\ntions used in PixelRNN and similar image generation net-\nworks [7]. Causal convolutions ensure that the prediction\nfor time step tonly depends on the predictions for previ-\nous timesteps. Furthermore, the causal convolutions are\ndilated; these are convolutions where the ﬁlter is applied\nover an area larger than its length by skipping particular\ninput values, as shown in Figure 1. In addition to dilated\ncausal convolutions, each layer features gated activation\nunits and residual connections, as well as skip connections\nto the ﬁnal output layers.\n2.2 Symbolic Audio Models\nMost deep learning approaches for automatic music gen-\neration are based on symbolic representations of the mu-\nsic. MIDI (Musical Instrument Digital Interface),4for ex-\nample, is a ubiquitous standard for ﬁle format and proto-\ncol speciﬁcation for symbolic representation and transmis-\nsion. Other representations that have been utilized include\nthe piano roll representation [13]—inspired by player pi-\nano music rolls—text representations (e.g., ABC nota-\ntion5), chord representations (e.g., Chord2Vec [18]), and\nlead sheet representations. A typical scenario for produc-\ning music in such models is to train and generate on the\nsame type of representation; for instance, one may train on\na set of MIDI ﬁles that encode melodies, and then generate\nnew MIDI melodies from the learned model. These mod-\nels attempt to capture the aspect of long-range dependency\nin music.\nA traditional approach to learning temporal dependen-\ncies in data is to use recurrent neural networks (RNNs). A\nrecurrent neural network receives a timestep of a series xt\nalong with a hidden state htas input. It outputs yt, the\nmodel output at that timestep, and computes ht+1, the hid-\nden state at the next timestep. RNNs take advantage of\n4https://www.midi.org/speciﬁcations\n5http://abcnotation.comProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 183Figure 2 : A representation of a biaxial LSTM network.\nNote that the ﬁrst two layers have connections across\ntimesteps, while the last two layers have recurrent connec-\ntions across notes [14].\nthis hidden state to store some information from the pre-\nvious timesteps. In practice, vanilla RNNs do not perform\nwell when training sequences have long temporal depen-\ndencies due to issues of vanishing/exploding gradients [2].\nThis is especially true for music, as properties such as key\nsignature and time signature may be constant throughout a\ncomposition.\nLong Short Term Memory networks are a variant of\nRNNs that have proven useful in symbolic music gener-\nation systems. LSTM networks modify the way memory\ninformation is stored in RNNs by introducing another unit\nto the original RNN network: the cell state, ct, where the\nﬂow of information is controlled by various gates. LSTMs\nare designed such that the interaction between the cell\nstate and the hidden state prevents the issue of vanish-\ning/exploding gradients [10, 12].\nThere are numerous existing deep learning symbolic\nmusic generation approaches [5], including models that\nare based on RNNs, many of which use an LSTM as a\nkey component of the model. Some notable examples\ninclude DeepBach [11], the CONCERT system [20], the\nCeltic Melody Generation system [23] and the Biaxial\nLSTM model [14]. Additionally, some approaches com-\nbine RNNs with restricted Boltzmann machines [4,6,9,17].\n3. ARCHITECTURE\nWe ﬁrst discuss our symbolic method for generating\nunique melodies, then detail the modiﬁcations to the raw\naudio model for compatibility with these generations.\nModifying the architecture involves working with both\nsymbolic and raw audio data in harmony.\n3.1 Unique Symbolic Melody Generation with LSTM\nNetworks\nRecently, applications of LSTMs speciﬁc to music genera-\ntion, such as the biaxial LSTM, have been implemented\nand explored. This model utilizes a pair of tied, paral-\nlel networks to impose LSTMs both in the temporal di-\nmension and the pitch dimension at each timestep. Each\nnote has its own network instance at each timestep, and\nFigure 3 : An overview of the model architecture, showing\nthe local conditioning time series as an extra input.\nreceives input of the MIDI note number, pitchclass, beat,\nand information on surrounding notes and notes at previous\ntimesteps. This information ﬁrst passes through two lay-\ners with connections across timesteps, and then two layers\nwith connections across notes, detailed in Figure 2. This\ncombination of note dependency and temporal dependency\nallow the model to not only learn the overall instrumen-\ntal and temporal structure of the music, but also capture\nthe interdependence of the notes being played at any given\ntimestep [14].\nWe explore the sequential combination of the symbolic\nand raw audio models to produce structured raw audio out-\nput. We train a biaxial LSTM model on the MIDI ﬁles of\na particular genre of music as training data, and then feed\nthe MIDI generations from this trained model into the raw\naudio generator model.\n3.2 Local Conditioning with Raw Audio Models\nOnce a learned symbolic melody is obtained, we treat it\nas a second time series within our raw audio model (anal-\nogous to using a second time series with a desired text\nto be spoken in the speech domain). In particular, in the\nWaveNet model, each layer features a gated activation unit.\nIfxis the raw audio input vector, then at each layer k, it\npasses through the following gated activation unit:\nz=tanh (Wf;k\u0003x)\f\u001b(Wg;k\u0003x); (2)\nwhere\u0003is a convolution operator, \fis an elementwise\nmultiplication operator, \u001b(\u0001)is the sigmoid function, and\ntheWf;kandWg;kare learnable convolution ﬁlters. Fol-\nlowing WaveNet’s use of local conditioning, we can intro-\nduce a second time series y(in this case from the LSTM\nmodel, to capture the long-term melody), and instead uti-\nlize the following activation, effectively incorporating yas\nan extra input:\nz=tanh (Wf;k\u0003x+Vf;k\u0003y)\f\u001b(Wg;k\u0003x+Vg;k\u0003y);(3)\nwhereVare learnable linear projections. By condition-\ning on an extra time series input, we effectively guide the\nraw audio generations to require certain characteristics; y\ninﬂuences the output at all timestamps.184 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Instrument Minutes Labels\nPiano 1,346 794,532\nViolin 874 230,484\nCello 621 99,407\nSolo Piano 917 576,471\nSolo Violin 30 8,837\nSolo Cello 49 10,876\nTable 1 : Statistics of the MusicNet dataset. [24]\nIn our modiﬁed WaveNet model, the second time series\nyis the upsampled MIDI embedding of the local condition-\ning time series. In particular, local conditioning (LC) em-\nbeddings are 128-dimensional binary vectors, where ones\ncorrespond to note indices that are being played at the cur-\nrent timestep. As with the audio time series, the LC em-\nbeddings ﬁrst go through a layer of causal convolutions\nto reduce the number of dimensions from 128 to 16, which\nare then used in the dilation layers as the conditioning sam-\nples. This reduces the computational requirement for the\ndilation layers without reducing the note state information,\nas most of the embeddings are zero for most timestamps.\nThis process along with the surrounding architecture is\nshown in Figure 3.\n3.3 Hyperparameter Tuning\nTable 2 enumerates the hyperparameters used in the\nWaveNet-based conditioned model to obtain our results.\nWe note that the conditioned model needs only 30 dila-\ntion layers as compared to the 50 we had used in the un-\nconditioned network. Training with these parameters gave\nus comparable results as compared to the unconditioned\nmodel in terms of the timbre of instruments and other nu-\nances in generations. This indicates that the decrease in\nparameters is offset by the extra information provided by\nthe conditioning time series.\n4. EMPIRICAL EVALUATION\nExample results of generations from our models are posted\non our web page.6\nOne of the most challenging tasks in automated music\ngeneration is evaluating the resulting music. Any gener-\nated piece of music can generally only be subjectively eval-\nuated by human listeners. Here, we qualitatively evaluate\nour results to the best of our ability, but leave the results\non our web page for the reader to subjectively evaluate.\nWe additionally quantify our results by comparing the re-\nsulting loss functions of the unconditioned and conditioned\nraw audio models. Then, we evaluate the structural compo-\nnent by computing the cross-correlation between the spec-\ntrogram of the generated raw audio and conditioning input.\n4.1 Training Datasets and Loss Analysis\nAt training time, in addition to raw training audio, we must\nalso incorporate its underlying symbolic melody, perfectly\n6http://people.bu.edu/bkulis/projects/music/index.htmlHyperparameter Value\nInitial Filter Width 32\nDilation Filter Width 2\nDilation Layers 30\nResidual Channels 32\nDilation Channels 32\nSkip Channels 512\nInitial LC Channels 128\nDilation LC Channels 16\nQuantization Channels 128\nTable 2 : WaveNet hyperparameters used for training of the\nconditioned network.\naligned with the raw audio at each timestep. The problem\nof melody extraction in raw audio is still an active area of\nresearch; due to a general lack of such annotated music,\nwe have experimented with multiple datasets.\nPrimarily, we have been exploring use of the recently-\nreleased MusicNet database for training [24], as this data\nfeatures both raw audio as well as melodic annotations.\nOther metadata is also included, such as the composer of\nthe piece, the instrument with which the composition is\nplayed, and each note’s position in the metrical structure\nof the composition. The music is separated by genre; there\nare over 900 minutes of solo piano alone, which has proven\nto be very useful in training on only one instrument. The\ndifferent genres provide many different options for train-\ning. Table 1 shows some other statistics of the MusicNet\ndataset.\nAfter training with these datasets, we have found that\nthe loss for the unconditioned and conditioned WaveNet\nmodels follows our expectation of the conditioned model\nexhibiting a lower cross-entropy training loss than the un-\nconditioned model. This is due to the additional embed-\nding information provided along with the audio in the con-\nditioned case. Figure 5 shows the loss for two WaveNet\nmodels trained on the MusicNet cello dataset over 100,000\niterations, illustrating this decreased loss for the condi-\ntioned model.\n4.2 Unconditioned Music Generation with WaveNet\nWe preface the evaluation of our musical results by ac-\nknowledging the fact that we ﬁrst tuned WaveNet for\nunstructured music generation, as most applications of\nWaveNet have explored speech applications. Here we\nworked in the unconditioned case, i.e., no second time se-\nries was input to the network. We tuned the model to gener-\nate music trained on solo piano inputs (about 50 minutes of\nthe Chopin nocturnes, from the YouTube-8M dataset [1]),\nas well as 350 songs of various genres of electronic dance\nmusic, obtained from No Copyright Sounds7.\nWe found that WaveNet models are capable of produc-\ning lengthy, complex musical generations without losing\ninstrumental quality for solo instrumental training data.\nThe network is able to learn short-range dependencies, in-\n7https://www.youtube.com/user/NoCopyrightSoundsProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 185Figure 4 : Example MIDI generation from the biaxial\nLSTM trained on cello music, visualized as sheet music.\nFigure 5 : Cross entropy loss for the conditioned (solid\ngreen) and unconditioned (dotted orange) WaveNet mod-\nels over the ﬁrst 100,000 training iterations, illustrating the\nlower training loss of the conditioned model.\ncluding hammer action and simple chords. Although gen-\nerations may have a consistent energy, they are unstruc-\ntured and do not contain any long-range temporal depen-\ndencies. Results that showcase these techniques and at-\ntributes are available on our webpage.\n4.3 Structure in Raw Audio Generations\nWe evaluate the structuring ability of our conditioned raw\naudio model for a generation based on how closely it fol-\nlows the conditioning signal it was given, ﬁrst using pop-\nular existing melodies, then the unique LSTM genera-\ntions. We use cross-correlation as a quantitative evalua-\ntion method. We also acknowledge the applications of our\nmodel to edit existing raw audio.\n4.3.1 Raw Audio from Existing Melodies\nWe evaluate our approach ﬁrst by generating raw audio\nfrom popular existing melodies, by giving our conditioned\nmodel a second time series input of the Happy Birthday\nmelody and a C major scale. Since we are familiar with\nthese melodies, they are easier to evaluate by ear.\nInitial versions of the model evaluated in this way were\ntrained on the MusicNet cello dataset. The generated\nraw audio follows the conditioning input, the recognizable\nHappy Birthday melody and C major scale, in a cello tim-\nbre. The results of these generations are uploaded on our\nwebpage.\n4.3.2 Raw Audio From Unique LSTM Generations\nAfter generating novel melodies from the LSTM, we pro-\nduced corresponding output from our conditioned model.\nSince it is difﬁcult to qualitatively evaluate such melodies\n(a) Unedited training sample from the MusicNet dataset.\n(b) Slightly modiﬁed training sample.\nFigure 6 : MIDI representations of a sample from the Mu-\nsicNet solo cello dataset, visualized as sheet music; (b) is a\nslightly modiﬁed version of (a), the original training sam-\nple. We use these samples to showcase the ability of our\nmodel to “edit” raw audio.\nby ear due to unfamiliarity with the melody, we are inter-\nested in evaluating how accurately the conditioned model\nfollows a novel melody quantitatively. We evaluate our re-\nsults by computing the cross-correlation between the MIDI\nsequence and the spectrogram of the generated raw au-\ndio as shown in Figure 7. Due to the sparsity of both the\nspectrogram and the MIDI ﬁle in the frequency dimension,\nwe decided to calculate the cross-correlation between one-\ndimensional representations of the two time series. We\nchose the frequency of the highest note in the MIDI at each\ntimestep as its one-dimensional representation. In the case\nof the raw audio, we chose the most active frequency in\nits spectrogram at each timestep. We acknowledge some\nweakness in this approach, since some information is lost\nby reducing the dimensionality of both time series.\nCross-correlation is the “sliding dot product” of two\ntime series — a measure of linear similarity as a function\nof the displacement of one series relative to the other. In\nthis instance, the cross-correlation between the MIDI se-\nquence and the corresponding raw audio peaks at delay 0\nand is equal to 0.3. In order to assure that this correlation\nis not due to chance, we have additionally calculated the\ncross-correlation between the generated raw audio and 50\ndifferent MIDI sequences in the same dataset. In Figure 7,\nwe can see that the cross-correlation curve stays above the\nother random correlation curves in the the area around de-\nlay 0. This shows that the correlation found is not by\nchance, and the raw audio output follows the conditioning\nvector appropriately.\nThis analysis generalizes to any piece generated with\nour model; we have successfully been able to transform\nan unstructured model with little long-range dependency\nto one with generations that exhibit certain characteristics.\n4.3.3 Editing Existing Raw Audio\nIn addition, we explored the possibility of using our ap-\nproach as a tool similar to a MIDI synthesizer, where we\nﬁrst generate from an existing piece of a symbolic melody,186 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 7 : Comparison of the novel LSTM-generated melody (top) and the corresponding raw audio output of the condi-\ntioned model represented as a spectrogram (middle). The bottom plot shows the cross-correlation between the frequency of\nthe highest note of the MIDI and the most active frequency of raw audio from the WaveNet-based model, showing strong\nconditioning from the MIDI on the generated audio.\nin this case, from the training data. Then, we generate new\naudio by making small changes to the MIDI, and evaluate\nhow the edits reﬂect in the generated audio. We experi-\nment with this with the goal of achieving a higher level of\nﬁdelity to the audio itself rather using a synthesizer to re-\nplay the MIDI as audio, as that often forgoes the nuances\nassociated with raw audio.\nFigure 6(a) and 6(b) respectively show a snippet of the\ntraining data taken from the MusicNet cello dataset and the\nsmall perturbations made to it, which were used to evalu-\nate this approach. The results posted on our webpage show\nthat the generated raw audio retains similar characteristics\nbetween the original and the edited melody, while also in-\ncorporating the changes to the MIDI in an expressive way.\n5. CONCLUSIONS AND FUTURE WORK\nIn conclusion, we focus on combining raw and symbolic\naudio models for the improvement of automatic music gen-\neration. Combining two prevalent models allows us to take\nadvantage of both of their features; in the case of raw audio\nmodels, this is the realistic sound and feel of the music, and\nin the case of symbolic models, it is the complexity, struc-\nture, and long-range dependency of the generations.\nBefore continuing to improve our work, we ﬁrst plan\nto more thoroughly evaluate our current model using rat-\nings of human listeners. We will use crowdsourced evalua-\ntion techniques (speciﬁcally, Amazon Mechanical Turk8)\nto compare our outputs with other systems.\nA future modiﬁcation of our approach is to merge the\nLSTM and WaveNet models to a coupled architecture.\n8https://www.mturk.com/mturk/This joint model would eliminate the need to synthesize\nMIDI ﬁles, as well as the need for MIDI labels aligned with\nraw audio data. In essence, this adjustment would create a\ntrue end-to-end automatic music generation model.\nAdditionally, DeepMind recently updated the WaveNet\nmodel to improve generation speed by 1000 times over the\nprevious model, at 16 bits per sample and a sampling rate\nof 24kHz [26]. We hope to investigate this new model to\ndevelop real-time generation of novel, structured music,\nwhich has many signiﬁcant implications.\nThe potential results of our work could augment and\ninspire many future applications. The combination of our\nmodel with multiple audio domains could be implemented;\nthis could involve the integration of speech audio with mu-\nsic to produce lyrics sung in tune with our realistic melody.\nEven without the additional improvements considered\nabove, the architecture proposed in this paper allows for\na modular approach to automated music generation. Mul-\ntiple different instances of our conditioned model can be\ntrained on different genres of music, and generate based\non a single local conditioning series in parallel. As a re-\nsult, the same melody can be reproduced in different genres\nor instruments, strung together to create effects such as a\nquartet or a band. The key application here is that this type\nof synchronized effect can be achieved without awareness\nof the other networks, avoiding model interdependence.\n6. ACKNOWLEDGEMENT\nWe would like to acknowledge that this research was sup-\nported in part by NSF CAREER Award 1559558.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 1877. REFERENCES\n[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev,\nG. Toderici, B. Varadarajan, and S. Vijayanarasimhan.\nYouTube-8M: A large-scale video classiﬁcation bench-\nmark. CoRR , abs/1609.08675, 2016.\n[2] Y . Bengio, P. Simard, and P. Frasconi. Learning long-\nterm dependencies with gradient descent is difﬁcult.\nIEEE transactions on neural networks , 5(2):157–166,\n1994.\n[3] M. Blaauw and J. Bonada. A neural parametric singing\nsynthesizer. ArXiv preprint 1704.03809 , 2017.\n[4] N. Boulanger-Lewandowski, Y . Bengio, and P. Vin-\ncent. Modeling temporal dependencies in high-\ndimensional sequences: Application to polyphonic\nmusic generation and transcription. In Proceedings of\nthe 29th International Conference on Machine Learn-\ning, Edinburgh, Scotland, UK, 26 Jun–1 Jul 2012.\n[5] J. Briot, G. Hadjeres, and F. Pachet. Deep learn-\ning techniques for music generation—a survey. ArXiv\npreprint 1709.01620 , 2017.\n[6] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio. Empir-\nical evaluation of gated recurrent neural networks on\nsequence modeling. ArXiv preprint 1412:3555 , 2014.\n[7] A. Van den Oord, N. Kalchbrenner, and\nK. Kavukcuoglu. Pixel recurrent neural networks.\nInProceedings of The 33rd International Conference\non Machine Learning , volume 48 of Proceedings of\nMachine Learning Research , pages 1747–1756, New\nYork, New York, USA, 20–22 Jun 2016.\n[8] J. Engel, C. Resnick, A. Roberts, S. Dieleman,\nM. Norouzi, D. Eck, and K. Simonyan. Neural au-\ndio synthesis of musical notes with WaveNet autoen-\ncoders. In Proceedings of the 34th International Con-\nference on Machine Learning , volume 70 of Proceed-\nings of Machine Learning Research , pages 1068–1077,\nInternational Convention Centre, Sydney, Australia,\n06–11 Aug 2017.\n[9] K. Goel, R. V ohra, and JK Sahoo. Polyphonic music\ngeneration by modeling temporal dependencies using a\nrnn-dbn. In International Conference on Artiﬁcial Neu-\nral Networks , pages 217–224. Springer, 2014.\n[10] I. Goodfellow, Y . Bengio, and A. Courville.\nDeep Learning . MIT Press, 2016.\nhttp://www.deeplearningbook.org.\n[11] G. Hadjeres, F. Pachet, and F. Nielsen. DeepBach: a\nsteerable model for Bach chorales generation. In Pro-\nceedings of the 34th International Conference on Ma-\nchine Learning , volume 70 of Proceedings of Ma-\nchine Learning Research , pages 1362–1371, Interna-\ntional Convention Centre, Sydney, Australia, 06–11\nAug 2017.[12] S. Hochreiter and J. Schmidhuber. Long short-term\nmemory. Neural computation , 9(8):1735–1780, 1997.\n[13] A. Huang and R. Wu. Deep learning for music. ArXiv\npreprint 1606:04930 , 2016.\n[14] D. D. Johnson. Generating polyphonic music using\ntied parallel networks. In International Conference on\nEvolutionary and Biologically Inspired Music and Art ,\npages 128–143. Springer, 2017.\n[15] A. Karpathy, J. Johnson, and L. Fei-Fei. Visual-\nizing and understanding recurrent networks. CoRR ,\nabs/1506.02078, 2015.\n[16] T. Le Paine, P. Khorrami, S. Chang, Y . Zhang, P. Ra-\nmachandran, M. A. Hasegawa-Johnson, and T. S.\nHuang. Fast waveNet generation algorithm. ArXiv\npreprint 1611.09482 , 2016.\n[17] Q. Lyu, J. Zhu Z. Wu, and H. Meng. Modelling high-\ndimensional sequences with LSTM-RTRBM: Applica-\ntion to polyphonic music generation. In Proc. Interna-\ntional Artiﬁcial Intelligence Conference (AAAI) , 2015.\n[18] S. Madjiheurem, L. Qu, and C. Walder. Chord2Vec:\nLearning musical chord embeddings. In Proceedings\nof the Constructive Machine Learning Workshop at\n30th Conference on Neural Information Processing\nSystems , Barcelona, Spain, 2016.\n[19] R. Manzelli, V . Thakkar, A. Siahkamari, and B. Kulis.\nAn end to end model for automatic music generation:\nCombining deep raw and symbolic audio networks. In\nProceedings of the Musical Metacreation Workshop at\n9th International Conference on Computational Cre-\nativity , Salamanca, Spain, 2018.\n[20] M. C. Mozer. Neural network composition by predic-\ntion: Exploring the beneﬁts of psychophysical con-\nstraints and multiscale processing. Connection Sci-\nence, 6(2–3):247–280, 1994.\n[21] A. Nayebi and M. Vitelli. Gruv: Algorithmic mu-\nsic generation using recurrent neural networks. Course\nCS224D: Deep Learning for Natural Language Pro-\ncessing (Stanford) , 2015.\n[22] I. Simon and S. Oore. Performance RNN: Generat-\ning music with expressive timing and dynamics, 2017.\nhttps://magenta.tensorﬂow.org/performance-rnn.\n[23] B. L. Sturm, J. F. Santos, O. Ben-Tal, and I. Kor-\nshunova. Music transcription modelling and composi-\ntion using deep learning. ArXiv preprint 1604:08723 ,\n2016.\n[24] J. Thickstun, Z. Harchaoui, and S. M Kakade. Learning\nfeatures of music from scratch. In International Con-\nference on Learning Representations (ICLR) , 2017.\n[25] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan,\nO. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and\nK. Kavukcuoglu. WaveNet: A generative model for\nraw audio. ArXiv preprint 1609.03499 , 2016.188 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[26] A. van den Oord, Y . Li, I. Babuschkin, K. Simonyan,\nO. Vinyals, K. Kavukcuoglu, G. van den Driessche,\nE. Lockhart, L. C. Cobo, F. Stimberg, N. Casagrande,\nD. Grewe, S. Noury, S. Dieleman, E. Elsen, N. Kalch-\nbrenner, H. Zen, A. Graves, H. King, T. Walters,\nD. Belov, and D. Hassabis. Parallel wavenet: Fast\nhigh-ﬁdelity speech synthesis. CoRR , abs/1711.10433,\n2017.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 189"
    },
    {
        "title": "SE and SNL diagrams: Flexible data structures for MIR.",
        "author": [
            "Melissa R. McGuirl",
            "Katherine M. Kinnaird",
            "Claire Savard",
            "Erin H. Bugbee"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492419",
        "url": "https://doi.org/10.5281/zenodo.1492419",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/105_Paper.pdf",
        "abstract": "to interpret. The matrix-based representations commonly used in MIR tasks are often difficult This work introduces start-end (SE) diagrams and start(normalized)length (SNL) diagrams, two novel structure-based representations for sequential music data. Inspired by methods from topological data analysis, both SE and SNL diagrams come equipped with efficiently computable and stable metrics. Utilizing SE or SNL diagrams as input, we address the cover song task for score-based data with high accuracy. While both representations are concisely defined and flexible, SNL diagrams in particular address issues introduced by commonly used resampling methods.",
        "zenodo_id": 1492419,
        "dblp_key": "conf/ismir/McGuirlKSB18",
        "keywords": [
            "interpret",
            "matrix-based representations",
            "MIR tasks",
            "difficult",
            "novel structure-based representations",
            "sequential music data",
            "topological data analysis",
            "efficiently computable and stable metrics",
            "cover song task",
            "score-based data"
        ],
        "content": "SE AND S NL DIAGRAMS: FLEXIBLE DATA STRUCTURES FOR MIR\nMelissa R. McGuirl1Katherine M. Kinnaird1Claire Savard2\nErin H. Bugbee3\n1Division of Applied Mathematics, Brown University, USA\n2Department of Mathematics, University of Michigan, USA\n3Department of Biostatistics, Brown University, USA\nmelissa mcguirl@brown.edu\nABSTRACT\nThe matrix-based representations commonly used in MIR\ntasks are often difﬁcult to interpret. This work in-\ntroduces start-end (SE) diagrams and start(normalized)-\nlength (S NL) diagrams, two novel structure-based repre-\nsentations for sequential music data. Inspired by methods\nfrom topological data analysis, both SE and S NL diagrams\ncome equipped with efﬁciently computable and stable met-\nrics. Utilizing SE or S NL diagrams as input, we address the\ncover song task for score-based data with high accuracy.\nWhile both representations are concisely deﬁned and ﬂex-\nible, S NL diagrams in particular address issues introduced\nby commonly used resampling methods.\n1. INTRODUCTION\nSince Foote’s introduction of the self-similarity matrix\n(SSM) in [8], matrix-based representations for music-\nbased data streams have been commonly used in MIR liter-\nature. Both SSMs and self-dissimilarity matrices (SDMs)\nhave been used as the starting point for a variety of tasks\nincluding the cover song task [2,10,13,21], the chorus de-\ntection task [9], and segmentation task [14, 18, 19].\nWhile straightforward to compute, these matrix-based\nrepresentations are challenging to interpret, requiring ex-\ntensive post-processing, such as smoothing and resampling\ntechniques used in [10] or path enhancement applied in\n[15–17]. These post-processing steps can also introduce\nuncertainty or reduce some of the intuitive explanations\nfor the resulting visualizations. The aligned hierarchies\nfrom [13] is an intuitive structure-based representation that\nis also the result of post-processing SDMs. However, this\nrepresentation is rigid as it requires two songs to be the\nexactly the same length for comparisons. The aligned sub-\nhierarchies attempt to address this rigidity, but many songs\ndo not have enough structure to have this collection of\nstructure-based representations for sections of a song [12].\nc\rMelissa R. McGuirl, Katherine M. Kinnaird, Claire\nSavard, Erin H. Bugbee. Licensed under a Creative Commons Attribu-\ntion 4.0 International License (CC BY 4.0). Attribution: Melissa R.\nMcGuirl, Katherine M. Kinnaird, Claire Savard, Erin H. Bugbee. “SE\nand S NL diagrams: Flexible data structures for MIR”, 19th International\nSociety for Music Information Retrieval Conference, Paris, France, 2018.In this paper, we contribute two new structure-based vi-\nsualizations for music-based data streams: Start-end di-\nagrams (in Section 3) and Start(normalized)-length dia-\ngrams (in Section 4). With roots in topological data anal-\nysis, the presented methods are ﬂexible, computationally\nefﬁcient, and easily adaptable. Moreover, we present ex-\nperiments applying these methods to a version of the cover\nsong task. We discuss contributions of our novel methods\n(in Section 6) and share future directions (in Section 7).\n2. MOTIVATION AND BACKGROUND\nThis work builds upon aligned hierarchies developed in\n[13]. The aligned hierarchies for a song encodes all pos-\nsible hierarchical structure decompositions of that song on\none common time axis. The aligned hierarchies represen-\ntation is deﬁned as a collection of three components: a bi-\nnary onset matrix BH, a length vector, and an annotation\nvector that acts as a key for BH[13]. Each row of BH\ncorresponds to one kind of repetition, with entries equal to\none denoting where instances of a repeat begins.\nAligned hierarchies have been used to compare songs\nunder the ﬁngerprint task by leveraging that this represen-\ntation can be embedded into a classiﬁcation space with\na natural notion of distance. This distance computes the\nnumber of dissimilarities between start-times for repeats\nof each size and then totals those dissimilarities across all\nsizes. Using the aligned hierarchies as the basis of compar-\nison yields precise results, yet the metric is both rigid with\nrespect to the length of the songs and computationally ex-\npensive as it is based on a binary classiﬁcation [13].\nIn this work, we produce novel methods of representing\nand comparing songs. Inspired by work in topological data\nanalysis, our methods extend the aligned hierarchies while\naddressing their limitations. Moreover, we offer several\nvariations of our method, which make our representations\nﬂexible and easily adaptable to many applications such as\ncover song and remix detection.\n3. START-END DIAGRAMS\nAligned hierarchies represents repeated structures of mu-\nsic data. Similarly, topological data analysis (TDA), an\nemerging ﬁeld of mathematics, aims to extract structural,\nor topological, information from complex data. The start-341end diagram is a transformation of the aligned hierarchies\nthat is reminiscent of persistence diagrams from TDA.\nIn TDA, data are thresholded via a sequence of param-\neter values. Topological summaries, such as the number\nof loops in the data, are then computed for each param-\neter value in the sequence [4, 5, 7]. A common way to\nrepresent this topological information is with persistence\ndiagrams. Brieﬂy, a persistence diagram is a collection of\npointsf(bi;di)gN\ni=1\u001aR2\n+, such that (bi;di)corresponds\nto a topological structure that appears at some parameter\nvaluebiand disappears at parameter value di\u0015bi[4,5,7].\nInspired by TDA, we transform aligned hierarchies into\nastart-end (SE) diagram . The SE diagram corresponding\nto aligned hierarchies with N repeated structures is deﬁned\nas a collection of points f(si;ei)gN\ni=1\u001aR2\n+, wheresiand\neiare the start and end times, respectively, of the ithre-\npeated structure. Under this transformation, we adjust the\ntime scale such that time zero refers to the start of the ﬁrst\nblock of the aligned hierarchies and truncate the song to\nend where the last block of the aligned hierarchies ends.\nSE diagrams are not inherently topological (in a mathe-\nmatical sense), rather we are adapting data structures from\nTDA. While SE diagrams cannot delineate two different\ntypes of repeats of the same length, there are several ad-\nvantages of using SE diagrams over aligned hierarchies.\nFirst, they are a more concisely deﬁned structure, as each\ndiagram is simply a ﬁnite collection of points. Second,\nleveraging theoretical results from TDA, there are easily\nadaptable metrics on the space of SE diagrams (Subsec-\ntion 3.1). Third, these metrics are more ﬂexible than those\nfor the aligned hierarchies while maintaining accuracy and\nprecision in the cover song task (Subsection 3.2).\n3.1 Metrics for SE diagrams\nIn TDA there are two common metrics for persistence dia-\ngrams that can be extended to SE diagrams: the bottleneck\nmetric andthe Wasserstein metric . Both metrics measure\nthe error of an optimal alignment of points in two persis-\ntence (or SE) diagrams. The metrics are stable, meaning\nsmall differences between aligned hierarchies will yield a\nsmall SE diagram distance [6, 7, 11]. Moreover, as shown\nin [11], these distances can be computed efﬁciently using\nk-dimensional trees. Thus, under either the Wasserstein or\nbottleneck notions, SE diagrams are equipped with stable\nand computable metrics which facilitate their ability to ad-\ndress the cover song task efﬁciently and accurately.\n3.1.1 Intuitive deﬁnitions\nIntuitively, the Wasserstein and bottleneck metrics attempt\nto ﬁnd the best alignment of points between two SE dia-\ngrams and then measure cost of the alignment using an lp\nmetric. When aligning two diagrams for comparison, each\ndiagram point must have a corresponding aligned point in\nthe other diagram and no points can be aligned with more\nthan one point. The aligned points thus form a pair.\nRecall, thelpnorm for any point ~ x=(x1;:::;xn)2Rn\nis given byjj~ xjjp= (Pn\ni=1jxijp)1\npfor1\u0014p <1, and\nthel1norm isjjxjj1= max\ni(jxij). Norms naturally give1\n12\n23\n34\n45\n5\u000f\u000f\u000f\n\u0003\u0003\u0003\n\u0001\n\u000fS1\n\u0003S21\n12\n23\n34\n45\n5FF\nFF\n\u0003\u0003\u0003\n\u0001\nFS3\n\u0003S2\nFigure 1 . Optimal alignment of S1andS2without align-\ning with \u0001(left), and optimal alignment of S2andS3\nwhile allowing for alignments with \u0001(right). Note that\n\u0001\u0018(1;2)2S3and thel2distances between the pairs arenq\n1\n2;0;1;0o\n;sod2;1\nW(S2;S3)=1+p\n2p\n2,d2\nB(S2;S3) = 1:\nrise to metrics. Speciﬁcally, the lpmetricdp:Rn\u0002Rn!\nR+between any two points ~ x,~ y2Rnis deﬁned for 1\u0014\np\u00141 asdp(~ x,~ y) =jj~ x\u0000~ yjjp:Note that in R2,d2is the\nthe straight line distance between two points in the plane,\nwhiled1is the maximum of the horizontal and vertical\ndistance between two points in the plane.\nFor example, consider two SE diagrams S1=\nf(1;2);(1;5);(2;4)gandS2=f(1;3);(1;5);(2;3)g.\nTo ﬁnd the optimal alignment we pair points in di-\nagramS1with points in diagram S2in a way\nthat minimizes the total distance between all pairs.\nThe best alignment of S1andS2is given by\nf(1;2)\u0018(1;3);(1;5)\u0018(1;5);(2;4)\u0018(2;3)g(see Figure\n1). The corresponding l1distances of these pairs are\nf1;0;1g. The (1,q)-Wasserstein and 1-bottleneck met-\nrics are then deﬁned as the lqandl1, respectively, norms\nof thel1distances of the pairs in the optimal alignment.\nIn this example, the ( 1,2)-Wasserstein distance be-\ntweenS1andS2isd1;2\nW(S1;S2) =p\n2, whereas the1-\nbottleneck distance between S1andS2isd1\nB(S1;S2) = 1:\nNote, the ﬁrst superscript in d1;2\nWcorresponds to taking the\nl1distances between the points in each pair (inner norm),\nand the second superscript denotes taking l2norm of those\nl1distances (outer norm).\nThus far we have deﬁned distances between two SE di-\nagrams with the same number of points. By deﬁnition,\ncomputing the Wasserstein or bottleneck distance between\ntwo SE diagrams requires both diagrams to have the same\nnumber of points. In practice, however, we want to com-\npare SE diagrams with any number of points, as songs have\nvarying amounts of repeated structures.\nTo compare SE diagrams of differing numbers of\npoints we ﬁnd the optimal alignment of points in two\nSE diagrams, while also allowing diagram points to\nmatch to repeated structures existing for no time, mean-\ning their start and end times are the same. Formally,\nwe allow points to align with the diagonal , deﬁned as\n\u0001 =f(s;e) :s=e;s\u00150g(see Figure 1) [6, 7, 11].\nThe motivation for allowing points to align with re-\npeated structures that exist for no time is two-fold. First,\nunlike arbitrary insertions or deletions of points in either\nSE diagrams, aligning points with \u0001will give rise to a342 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018metric that respects the triangle inequality. With the trian-\ngle inequality, it is impossible to have the case where songs\nBandCare both cover songs of song Aso thatd(A;B)\nandd(A;C)are small, but d(B;C)is large.\nSecond, pairing unmatched points with \u0001enforces that\ntwo songs will be considered dissimilar when their long-\nlasting repeated structures do not have a corresponding pair\nunder the optimal alignment of points. To see this, observe\nthat when a point (s1\n\u0003;e1\n\u0003)2S1does not have a corre-\nsponding match in S2then(s1\n\u0003;e1\n\u0003)\u0018\u0001and this pairing\ncontributesdp((s1\n\u0003;e1\n\u0003);\u0001) = 21\np\u00001je1\n\u0003\u0000s1\n\u0003jto the overall\ncost of the alignment. Thus, the cost for unmatched points\naligning with \u0001increases as the length ( je1\n\u0003\u0000s1\n\u0003j) of the\nunmatched repeated structure increases.\nIn short, the (p,q)-Wasserstein and p-Bottleneck metrics\nmeasure the distances between pairs of points in the opti-\nmal alignment of two SE diagrams and \u0001. Whenq= 2,\nthe Wasserstein distance is the Euclidean norm of the dis-\ntances between pairs in the optimal alignment. In contrast,\nthe Bottleneck distance is to the maximum distance be-\ntween pairs in the optimal alignment. In the following sub-\nsection we provide rigorous deﬁnitions of these metrics.\n3.1.2 Rigorous Deﬁnitions\nLetS1=f(s1\ni;e1\ni)gi2IandS2=f(s2\nj;e2\nj)gj2Jbe SE\ndiagrams, and let \u001ebe a bijection between subsets ~I\u001aI\nand\u001e(~I)\u001aJ. The p-q penalty of\u001eis deﬁned as:\nPp\nq(\u001e) =X\ni2~Idp((s1\ni;e1\ni);(s2\n\u001e(i);e2\n\u001e(i)))q\n+X\ni2In~Idp((s1\ni;e1\ni);\u0001)q+X\nj2Jn\u001e(~I)dp((s2\nj;e2\nj);\u0001)q;\nfor1\u0014q<1, and the1-q penalty of\u001eis deﬁned as:\nPp\n1(\u001e) = max\u001a\nmax\ni2~Idp((s1\ni;e1\ni);(s2\n\u001e(i);e2\n\u001e(i)));\nmax\ni2In~Idp((s1\ni;e1\ni);\u0001);\nmax\nj2Jn\u001e(~I)dp((s2\nj;e2\nj);\u0001)\u001b\n:\nThese penalties deﬁne a cost function for aligning points\ninS1with points in S2(encoded in the ﬁrst terms), and for\naligning all unmatched points with \u0001(encoded in the re-\nmaining terms). The p-bottleneck distance is then deﬁned\nasdp\nB(S1;S2) = min\n\u001ePp\n1(\u001e)and the (p,q)-Wasserstein\ndistance isdp;q\nW(S1;S2) = min\n\u001ePp\nq(\u001e))1\nq[6, 7, 11].\n3.2 Applications of SE diagrams\nUtilizing the metrics described in the previous section,\nthere are several ways of comparing songs for the cover\nsong task. This work explores the efﬁcacy of using the\npairwise p-bottleneck and (p,q)-Wasserstein distances as\ninput for a mutual nearest neighbor search.\nNoting that the presented methods take aligned hierar-\nchies as input, we pre-process music-based data in three\n50 150 250 35050\n150\n250\n350\n50 150 250 35050150250350\n50 150 250 350\nStart\n0 0.5 1\nStart-Normalized020406080LengthFigure 2 . The thresholded SDM (top left), aligned hierar-\nchies (bottom left), SE diagram (top right), and S NL dia-\ngram with\u000b= 1(bottom right) corresponding to Mazurka\n52 expanded with threshold=0.02, shingle=12. Each dark\nblock diagonal in the thresholded SDM represents two sec-\ntions that are repeats of each other. Repetitions of all sizes\nare encoded in the aligned hierarchies as blocks, separated\ninto rows. Each block in the aligned hierarchies is rep-\nresented as a point in both the SE and S NL diagrams. The\nsmallest repeats are close to the diagonal in the SE diagram\nand are near the horizontal axis in the S NL diagram. The\ntops of the peaks in the SE and S NL diagrams represent the\nlongest repetitions, which are the blocks at the bottom of\nthe aligned hierarchies.\nsteps: 1) build audio shingles from the concatenated beat-\nsynchronous chroma features, 2) compute the SDM, and\nﬁnally 3) construct the aligned hierarchies for each song’s\nSDM (see [13] for more details). After the aligned hierar-\nchies are created, the procedure is as follows:1:\n1. Transform each aligned hierarchies into the corre-\nsponding SE diagram as described in Section 3\n2. Compute bottleneck or Wasserstein distances be-\ntween pairs of SE diagrams using Hera [11]2\n3. Mark a pair of songs as cover songs of each other if\nthe songs are mutual nearest neighbors\nSee Figure 2 for a visual example of our method. To\ntest our method we apply it to 52 Mazurka scores by\n1Code and processed data are publicly available here: https://\ngithub.com/MelissaMcguirl/SE_SNL_analysis.git .\n2Hera is publicly available here: https://bitbucket.org/\ngrey_narn/hera .Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 343Threshold 0.01 0.02 0.03 0.04 0.05\nShingle 6 12 6 12 6 12 6 12 6 12\nPrecision 0.871 0.622 0.848 0.718 0.871 0.800 0.818 0.725 0.824 0.757d1\nBMetricRecall 0.519 0.538 0.538 0.538 0.519 0.538 0.519 0.558 0.538 0.538\nPrecision 0.966 0.675 0.879 0.903 0.933 0.824 0.909 0.875 0.875 0.848d2;2\nWMetricRecall 0.538 0.519 0.558 0.538 0.538 0.538 0.576 0.538 0.538 0.538\nPrecision 1.0 0.683 0.909 0.909 0.933 0.882 0.906 0.906 0.879 0.853d1;2\nWMetricRecall 0.558 0.538 0.577 0.577 0.538 0.577 0.558 0.558 0.558 0.558\nTable 1 . Precision and recall values for the mutual nearest neighbor matching of SE diagrams for 104 Mazurka scores.\nChopin downloaded in **kern format from KernScore on-\nline database3(see [20]). Each score produces two data\nelements, an expanded version which includes all repeated\nsections as marked in the score, and a non-expanded ver-\nsion which has each repeated section played once. In the\ncover song task, the goal is to match the expanded and non-\nexpanded versions of each song.\nWe construct SE diagrams for all 104 songs in our\ndataset and compute their pairwise distances for three met-\nrics:d1\nB,d2;2\nW, andd1;2\nW. We perform 10 experiment trials\nper metric, varying the number of chroma vectors per au-\ndio shingle and varying the threshold applied to the SDM.\nThe precision and recall values are presented in Table 1.\nThese results show that SE diagrams can accomplish\nthis challenging version of the cover song task with high\nprecision and moderate recall values regardless of the met-\nric. It is crucial and exciting to note that the SE diagrams\nachieved these results without any resampling to the dia-\ngrams. Moreover, as we will see in the next section, this\nmethod is easily adaptable to several useful variations.\n4. START(NORMALIZED)-LENGTH DIAGRAMS\nSince the length of repeats (e-s) is represented diagonally\non SE diagrams, these representations can be difﬁcult to\ninterpret. In this section we describe a transformation\nof SE diagrams called start-length (SL) diagrams , along\nwith normalizations. SL diagrams are more intuitive to\nread than their predecessor and yield stronger experimen-\ntal results. While reminiscent of the constellation maps\nfrom [22], SL diagrams encode structural repeats instead\nof audio spectrogram peaks.\n4.1 Start-Length Diagrams\nConsider a SE diagram S=f(si;ei)gN\ni=1. The associ-\nated SL diagram isS0=f(si;ei\u0000si)gN\ni=1, where the x-\ncoordinate corresponds to the start time of a repeated struc-\nture and the y-coordinate denotes the length of that repeat.\nWhile SE and SL diagrams encode the same information,\nSL diagrams emphasize the lengths of repeats. This trans-\nformation has also been applied to persistence diagrams for\nTDA applications [1].\n3http://kern.humdrum.org/search?s=t\\&keyword=\nChopin4.2 Start(Normalized)-Length Diagrams\nIn most cases, normalizing SL diagrams before compar-\nison proves to be more effective. We note that simi-\nlar normalizations can also be applied to SE diagrams.\nThe start(normalized)-length (S NL) diagrams are deﬁned\nasS0\nN=f(\u000b(si=M);ei\u0000si)gN\ni=1, where\u000bis a positive\nscaling factor and Mis a normalization factor. Through-\nout this paper we will use M= max\nisi, but other normal-\nizations may be applied. The vertical coordinate of S NL\ndiagrams are not normalized in order to maintain the em-\nphasis on the lengths of the repeated structures.\nSimilar to SL diagrams, S NL diagrams encode the\nlengths of repeated structures, except the start times in\nthe normalized diagrams are proportional to the length of\nthe song. This normalization acts as a kind of resampling\nby condensing start times of each song to be on the same\nscale, while also preserving the lengths of the found repe-\ntition patterns. The \u000bparameter is a hyper-parameter that\nimbues a maximum tolerance on our comparisons. The\nimpact of this parameter is left to Section 4.4.\n4.3 Metrics for S NL diagrams\nAs with SE diagrams, the bottleneck and Wasserstein met-\nrics can be used to compare S NL diagrams with one modiﬁ-\ncation. Deﬁne ~\u0001to be the setf(s;l) :s= 0;(s;l)2R2\n+g.\nThe set ~\u0001is they-axis of SL and S NL diagrams and it\nencodes repeats that start at zero. The S NL p-bottleneck\nand S NL (p,q)-Wasserstein metrics, ~dp\nBand~dp;q\nW, are then\nthe same as the p-bottleneck metric and (p,q)-Wasserstein\nmetric deﬁned in Section 3.1, except \u0001is replaced by ~\u0001in\nthe deﬁnitions of Pp\n1andPp\nq, respectively. We use a mod-\niﬁed version of Hera to implement these metrics [11].\nPairing unmatched points with repeated structures that\nstart at zero rather than repeated structures that exist for\nno time allows the user to control the penalty for having\nunmatched points while maintaining the emphasis on the\nlength of the repeated structures. To better understand this,\nconsider a point (s1\n\u0003;e1\n\u0003\u0000s1\n\u0003)2S1that does not have a\ncorresponding pair in S2under the optimal alignment of\npoints inS1andS2. In this case we pair (s1\n\u0003;e1\n\u0003\u0000s1\n\u0003)with\n~\u0001so thatdp((s1\n\u0003;e1\n\u0003\u0000s1\n\u0003);~\u0001) =js1\n\u0003j. The penalty for\nunmatched points aligning with ~\u0001consequently increases\nas the start time of the corresponding repeated structure\nincreases. It is critical to note, however, that s2[0;\u000b]344 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Threshold 0.01 0.02 0.03 0.04 0.05\nShingle 6 12 6 12 6 12 6 12 6 12\nPrecision 1.0 0.827 1.0 0.818 1.0 0.978 1.0 0.935 0.975 0.936\u0016d1\nBMetricRecall 0.788 0.827 0.769 0.865 0.788 0.865 0.750 0.827 0.750 0.846\nPrecision 1.0 0.833 0.974 0.975 1.0 0.976 0.976 1.0 0.976 1.0\u0016d2;2\nW,\u0016d1;2\nWMetricRecall 0.731 0.769 0.731 0.750 0.731 0.788 0.788 0.769 0.788 0.788\nTable 2 . Precision and recall values for mutual nearest neighbor matching using the distance between S NL diagrams with\n\u000b= 1corresponding to 104 Mazurkas. Note, we observe \u0016d1;2\nWand\u0016d2;2\nWto be equivalent when the optimal alignment only\nrequires shifts in the start component so that (js2\u0000s1jp+je2\u0000e1jp)1\np=js2\u0000s1j= max(js2\u0000s1j;je2\u0000e1j):\nfor all start times sunder the S NL normalization. Thus, the\npenalty for having an unmatched diagram point is bounded\nabove by\u000bfor S NL diagrams with ~dp\nBor~dp;q\nW. We further\nexplain the choice of \u000bin the following section.\n4.4 Choosing \u000b\nThe hyper-parameter \u000bis a positive scaling factor in the\nnormalization. For small \u000b, the cost of aligning points\nwith ~\u0001is low. For example, if \u000b= 0:5, then the cost\nof aligning a S NL diagram point with ~\u0001is at most 0:5.\nConsequently, when comparing two S NL diagrams, points\nwithin each diagram will be paired only when the differ-\nence between their lengths is negligible. Otherwise, it will\nbe more effective to pair both points with ~\u0001. Thus, a small\nvalue for\u000binduces a strict matching criterion, where re-\npeated structures are mostly shifted in the start coordinate\nto pair with a repeated structure of similar or equal length,\nand structures with unmatched lengths get paired to ~\u0001.\nThe penalty for matching points with ~\u0001increases as \u000b\nincreases. For a large value of \u000b, two S NL diagram points\nof slightly different lengths are more likely to be matched\nwith each other than with ~\u0001. Consequently, a larger \u000b\nvalue yields a more ﬂexible length-based matching system.\nThe choice of \u000bdepends on the importance of the length\nof the repeated structures. One might consider 0<\u000b\u00141\nif repeated structures of different lengths are considered\nsigniﬁcantly dissimilar, or \u000b\u001d1to allow for ﬂexibility in\nlength-based matchings. The inclusion of this parameter\nfurther adds to the ﬂexibility of S NL diagrams.\n4.5 Applications of S NL diagrams\nWe apply the same algorithm to the same dataset deﬁned in\nSection 3.2 with SL and S NL diagrams using the adapted\nbottleneck and Wasserstein metrics for the cover song task.\nAgain, 10 experiment runs are performed per metric, vary-\ning the number of beats per audio shingle and the thresh-\nold applied to the SDM. For SL diagrams, the mean pre-\ncision values across the 10 experiments are 0.791, 0.803,\nand 0.791 with \u0016d1\nB,\u0016d2;2\nW, and \u0016d1;2\nW, respectively. The cor-\nresponding mean recall values are 0.596, 0.581, and 0.577.\nExperiments on S NL diagrams yield a signiﬁcant in-\ncrease in accuracy over both SL and SE diagrams on the\ncover song task, suggesting that a strict matching criterion\nin the length coordinate and ﬂexibility in the start coordi-\nnate is the most accurate way to approach the cover songtask with these diagram representations. Across the 10 ex-\nperiments, the1-Bottleneck metric yields mean precision\nand recall values of 0.947 and 0.808, respectively. The\n(2,2) and (1,2)-Wasserstein metrics yield a mean preci-\nsion value of 0.971 and a mean recall value 0.763.\nThe experimental results for S NL diagrams with \u000b= 1\nare presented in Table 2. Separate analyses show that pre-\ncision and recall remain constant for 0< \u000b\u00141, and de-\ncrease monotonically as \u000bincreases above 1 for this data.\nSNL diagrams are not restricted to the standard start-\nnormalization presented here. Applying the same method\nunder the (2,2)-Wasserstein metric with a Chebyshev-\nnormalized start component yields comparable results with\nslightly lower precision values and higher recall values.\nThis further demonstrates the robustness of S NL diagrams.\nTo push the limits of S NL diagrams, we applied this rep-\nresentation to audio-based data without making any mod-\niﬁcations in the preprocessing steps. We extracted beat-\nsynchronous chroma feature vectors using librosa tool-\nbox4for a collection of performances of Mazurka5and\nconstructed the corresponding S NL diagrams. Following\nthe evaluation method in [2], we ranked the songs based\non their pairwise-distances for the cover song task. While\nthe mean average precision values were less than 0.1 across\na range of metrics and \u000bvalues, these results demonstrate\nthat audio-speciﬁc preprocessing must be done in order to\nmitigate noise and other artifacts on tracks. Since there ex-\nist theoretical guarantees of stability of the S NL diagrams,\nwe are conﬁdent that with the appropriate preprocessing\nmethods S NL diagrams are suitable for a range of both\nscore-based and audio-based music.\n5. COMPARISON TO PREVIOUS WORK\nPrevious experiments on this Mazurka score dataset were\ndone with the aligned hierarchies [13] and with the aligned\nsub-hierarchies (AsH) [12]. The metric for aligned hier-\narchies only allowed for pairwise comparisons between\nsongs that were of the same length, meaning that it could\nonly be used for the ﬁngerprint task [13].\nInitial experiments using AsH to address the cover song\ntask were completed in [12]. Following the same exper-\n4https://librosa.github.io/librosa/\n5The CHARM Project Discography website maintains a list of com-\nmercially available Mazurka recordings at http://www.mazurka.\norg.uk/info/discography/?Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 345Mean Median Min. Max.\nPrecision 0.847 0.873 0.622 1.0SERecall 0.545 0.538 0.519 0.577\nPrecision 0.963 0.976 0.818 1.0SNLRecall 0.778 0.779 0.731 0.865\nPrecision 0.9511 0.9808 0.840 1.0AsHRecall 0.771 0.754 0.692 0.882\nTable 3 . Summary statistics of the precision and re-\ncall values for mutual nearest neighbor matching of the\nMazurka score data using SE diagrams, S NL diagrams, and\naligned sub-hierarchies (AsH). The AsH statistics are com-\nputed over the 10 combinations of thresholds and shingles,\nwhereas the SE and S NL statistics are computed over the\n30 combinations of thresholds, shingles, and metrics. The\nSNL experiments apply to a more complete dataset than the\nAsH and still attain similar high precision-recall values.\nimental design varying the thresholds between 0.01 and\n0.05 and testing shingle widths of 6 and 12, these exper-\niments produced high precision rates (between 0.840 and\n1.0) and modest recall rates (between 0.692 and 0.882).\nTable 3 presents the summary statistics of the precision-\nrecall values for mutual nearest neighbor matching using\nSE diagrams, S NL diagrams, and AsH on the Mazurka\nscore data. The AsH results are comparable to the\nprecision-recall values of the S NL experiments, while the\nSE experiments yield slightly lower precision-recall rates\nthan both AsH and S NL diagrams. However, the AsH post-\nprocessing technique requires repetitions to have enough\nrepeated structure within them to build a smaller aligned\nhierarchies for these song sections. Consequently, as many\nas 68 songs (depending on the shingle size and threshold)\ndo not have an AsH representation.\nAn advantage of the presented methods is that if a song\nhas an aligned hierarchies representation then it has a SE\ndiagram and S NL diagram. Thus, the S NL experiments\nwork with a more complete dataset than the AsH experi-\nments and still attain similar high precision-recall values.\n6. DISCUSSION\nBoth the SE diagrams and the S NL diagrams offer exciting\ncontributions to the representation of music-based sequen-\ntial data streams. These diagrams offer a clear represen-\ntation of the relationships between repeated structural ele-\nments and have advantages over previous structure-based\nmethods. By allowing for two recordings of different\nlengths to be directly compared without altering the beat-\nsynchronized lengths of structural repeats, the S NL di-\nagram addresses an issue created by current resampling\nmethods for music-based data streams.\nThe S NL diagram provides a new method for resam-\npling music-based data streams. The goal of resampling\nis to ensure that all matrix representations are the same\nsize, which eases comparisons between music-based data\nstreams. Current resampling methods compress all mu-sical structures to represent a proportion of the length of\nthe song, which results in comparing sections of a song\nthat are proportionally the same length but not actually the\nsame number of beats. In [10], the proportional compar-\nisons of structural elements had issues comparing versions\nof Mazurka Op. 68, No. 4, which could be mitigated using\nthe kind of resampling offered by S NL diagrams.\nStructure-based comparisons on resampled representa-\ntions of a piece of music are then between proportions of\nthe song instead of the true lengths of the repeats. This is\nespecially an issue in cases where one artist plays a song\nonce through, while another plays the piece through twice\nin its entirety. In such an example, a section of 100 beats\nlong in the piece will look twice as long in the ﬁrst repre-\nsentation when compared to the second representation.\nThe S NL diagrams balance resampling all representa-\ntions to be of the same length with maintaining the lengths\nof repeated structures. To accomplish this, the S NL dia-\ngrams resample only the starting position of each repeated\nstructure, while leaving the lengths alone. What is excit-\ning about this innovative approach is that it not only allows\nfor uniform comparisons – as desired by traditional resam-\npling – but it also allows for comparisons between sections\nof the same length of time (or beats) instead of sections of\nthe same proportional length of the song.\n7. CONCLUSION\nIn this paper we present SE and S NL diagrams, two novel,\nconcisely deﬁned, and ﬂexible representations for music-\nbased data. Leveraging theory from TDA, these diagrams\ncome equipped with stable metrics which allow us to apply\na mutual nearest neighbor search for the cover song task.\nExperimental results demonstrate that SE and S NL dia-\ngrams address the cover song task with high accuracy for\nscore-based data, and these results are robust with respect\nto the choice of metric. Moreover, SE diagrams avoid re-\nsampling all together, while S NL diagrams resample only\nthe starting positions of repeated structures.\nOverall, S NL diagrams yield the highest accuracy in ad-\ndressing the cover song task and they are more ﬂexible.\nIn addition to the choice of normalization, S NL diagrams\ninclude the hyper-parameter \u000bwhich allows the user to di-\nrectly control the rigidity of the length-matching criterion.\nIn future work we plan to apply SE and S NL diagrams\nto preprocessed audio data, and to extend these diagram\nrepresentations so that they are suitable for machine learn-\ning tasks. Theoretical guarantees provide strong evidence\nthat SE and S NL diagrams will be applicable to both score\nand audio data after appropriate preprocessing. Beyond\nthe method presented here, SE and S NL diagrams can be\nmapped into spaces that are more suitable for machine\nlearning tasks, just as, for example, persistence diagrams\nhave been transformed to sequences of piecewise-linear\nfunctions and vectors in Euclidean space [1, 3]. Thus, SE\nand S NL diagrams open up a range of new opportunities\nfor applying machine learning methods through the lens of\nTDA to music-based tasks.346 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20188. ACKNOWLEDGEMENTS\nThe ﬁrst author is supported by the National Science\nFoundation Graduate Research Fellowship Program under\nGrant No. 1644760. The third and fourth authors are sup-\nported by the National Science Foundation under Grant\nNo. DMS-1439786 and The Karen T. Romer Undergradu-\nate Teaching and Research Awards while the authors were\nin residence at the Institute for Computational and Exper-\nimental Research in Mathematics in Providence, RI, dur-\ning the Summer@ICERM 2017 program. Any opinions,\nﬁndings, and conclusions or recommendations expressed\nin this material are those of the authors and do not neces-\nsarily reﬂect the views of the National Science Foundation.\nThe authors would like to thank Arnur Nigmetov for his\ngenerous help with adapting the Hera software for the S NL\ndiagrams.\n9. REFERENCES\n[1] H. Adams, T. Emerson, M. Kirby, R. Neville, C. Pe-\nterson, P. Shipman, S. Chepushtanova, E. Hanson,\nF. Motta, and L. Ziegelmeier. Persistence images: A\nstable vector representation of persistent homology.\nJournal of Machine Learning Research , 18(8):1–35,\n2017.\n[2] J. Bello. Measuring structural similarity in music.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 19(7):2013–2025, 2011.\n[3] P. Bubenik. Statistical topological data analysis using\npersistence landscapes. Journal of Machine Learning\nResearch , 16:77–102, 2015.\n[4] G. Carlsson. Topology and data. Bulletin of the Ameri-\ncan Mathematical Society , 46(2):255–308, 2009.\n[5] G. Carlsson, A. Zomorodian, A. Collins, and L. J.\nGuibas. Persistence barcodes for shapes. International\nJournal of Shape Modeling , 11(02):149–187, 2005.\n[6] F. Chazal, D. Cohen-Steiner, L.J. Guibas, F. Memoli,\nand S.Y . Oudot. Gromov-Hausdorff Stable Signatures\nfor Shapes using Persistence. Computer Graphics Fo-\nrum, 2009.\n[7] H. Edelsbrunner and J. L. Harer. Computational Topol-\nogy: An Introduction . American Mathematical Society,\n2010.\n[8] J. Foote. Visualizing music and audio using self-\nsimilarity. Proc. ACM Multimedia 99 , pages 77–80,\n1999.\n[9] M. Goto. A chorus-section detection method for musi-\ncal audio signals and its application to a music listen-\ning station. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 14(5):1783–1794, 2006.\n[10] P. Grosche, J. Serr `a, M. M ¨uller, and J.Ll. Arcos.\nStructure-based audio ﬁngerprinting for music re-\ntrieval. Proc. of 13thISMIR Conference , pages 55–60,\n2012.[11] M. Kerber, D. Morozov, and A. Nigmetov. Geometry\nHelps to Compare Persistence Diagrams. 2016 Pro-\nceedings of the Eighteenth Workshop on Algorithm\nEngineering and Experiments (ALENEX).\n[12] K. M. Kinnaird. Aligned Hierarchies for Sequential\nData . PhD thesis, Dartmouth College, 2014.\n[13] K. M. Kinnaird. Aligned hierarchies: A multi-scale\nstructure-based representation for music-based data\nstreams. Proc. of 17thISMIR Conference , 2016.\n[14] B. McFee and D. P. W. Ellis. Learning to segment\nsongs with ordinal linear discriminant analysis. In In-\nternational conference on acoustics, speech and signal\nprocessing , ICASSP, 2014.\n[15] M. M ¨uller. Information Retrieval for Music and Mo-\ntion. Springer Verlag, 2007.\n[16] M. M ¨uller, P. Grosche, and N. Jiang. A segment-based\nﬁtness measure for capturing repetitive structures of\nmusic recordings. Proc. of 12thISMIR Conference ,\npages 615–620, 2011.\n[17] M. M ¨uller and F. Kurth. Enhancing similarity matrices\nfor music audio analysis. Proc. of ICASSP , 2006.\n[18] J. Paulus, M. M ¨uller, and A. Klapuri. Audio-based mu-\nsic structure analysis. Proc. of 11thISMIR Conference ,\npages 625–636, 2010.\n[19] C. Rhodes and M. Casey. Algorithms for determining\nand labelling approximate hierarchical self-similarity.\nProc. of 8thISMIR Conference , 2007.\n[20] C.S. Sapp. Online database of scores in the humdrum\nﬁle format. Proc. of 6thISMIR Conference , pages 664–\n665, 2005.\n[21] C. J. Tralie. Early MFCC and HPCP fusion for robust\ncover song identiﬁcation. Proc. of 18thISMIR Confer-\nence, 2017.\n[22] A. L. Wang. An industrial-strength audio search algo-\nrithm. In Proc. of 4thISMIR Conference , 2003.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 347"
    },
    {
        "title": "JSYMBOLIC 2.2: Extracting Features from Symbolic Music for use in Musicological and MIR Research.",
        "author": [
            "Cory McKay",
            "Julie Cumming",
            "Ichiro Fujinaga"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492421",
        "url": "https://doi.org/10.5281/zenodo.1492421",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/26_Paper.pdf",
        "abstract": "jSymbolic is an open-source platform for extracting features from symbolic music. These features can serve as inputs to machine learning algorithms, or they can be analyzed statistically to derive musicological insights.  jSymbolic implements 246 unique features, comprising 1497 different values, making it by far the most extensive symbolic feature extractor to date. These features are designed to be applicable to a diverse range of musics, and may be extracted from both symbolic music files as a whole and from windowed subsets of them. Researchers can also use jSymbolic as a platform for developing and distributing their own bespoke features, as it has an easily extensible plug-in architecture.  In addition to implementing 135 new unique features, version 2.2 of jSymbolic places a special focus on functionality for avoiding biases associated with how symbolic music is encoded. In addition, new interface elements and documentation improve convenience, ease-of-use and accessibility to researchers with diverse ranges of technical expertise. jSymbolic now includes a GUI, command-line interface, API , flexible configuration file format, extensive manual and detailed tutorial.  The enhanced effectiveness of jSymbolic 2.2's features is demonstrated in two sets of experiments: 1) genre classification and 2) Renaissance composer attribution.",
        "zenodo_id": 1492421,
        "dblp_key": "conf/ismir/McKayCF18",
        "keywords": [
            "open-source platform",
            "extracting features from symbolic music",
            "machine learning algorithms",
            "musicological insights",
            "246 unique features",
            "1497 different values",
            "diverse range of musics",
            "symbolic music files",
            "windowed subsets",
            "easily extensible plug-in architecture"
        ],
        "content": "JSYMBOLIC 2.2: EXTRACTING FEATURES FROM \nSYMBOLIC MUSIC FOR USE IN MUSICOLOGICAL AND \nMIR RESEARCH  \nCory McKay  Julie  E. Cumming  Ichiro Fujinaga  \nMarianopolis College  \ncory.mckay@mail.mcgill.ca  McGill University   \njulie.cumming@mcgill.ca  McGill University  \nichiro.fujinaga@mcgill.ca  \nABSTRACT  \njSymbolic is an open -source platform for extracting fe a-\ntures from symbolic music. These features can serve as \ninputs to machine learning algorithms, or they can be \nanalyzed statistically to derive musicological insights.  \njSymbolic implements 246 unique features, compri s-\ning 1497 different values, making it by far the most e x-\ntensive symbolic feature extractor to date . These features \nare designed to be applicable to a diverse range of m u-\nsics, and may be extracted from both symbo lic music \nfiles as a whole and from windowed subsets of them. R e-\nsearchers can also use jSymbolic as a platform for deve l-\noping and distributing their own bespoke features, as it \nhas an easily extensible plug -in architecture.  \nIn addition to implementing 135 new unique features, \nversion 2.2 of jSymbolic places a special focus on fun c-\ntionality for avoiding biases associated with how symbo l-\nic music is encoded. In addition, n ew interface elements \nand documentation improve convenience, ease -of-use and \naccessibilit y to researchers with diverse ranges of tec h-\nnical expertise. jSymbolic now includes a GUI, co m-\nmand -line interface, API , flexible configuration file fo r-\nmat, extensive manual and detailed tutorial.  \nThe enhanced effectiveness of jSymbolic  2.2’s fe a-\ntures is demonstrated  in two sets of experiments: 1) genre \nclassification and 2) Renaissance composer attribution.  \n1. INTRODUCTION  \nThe majority  of research performed by musicologists, \nmusic theorists, music librarians  and others focuses on \nsymbolic music representation s. Unfortunately, relatively \nfew MIR -oriented software tools are available to assist \nsuch research, particularly with respect to research i n-\nvolving the increasingly  large corpora  being studied . \njSymbolic  is an open -source Java framework designed \nto at lea st partially address this shortcoming. Its primary \nfunction is to extract a large number of features ( statist i-\ncal descriptors ) from potentially huge collections of dig i-tally-represented symbolic music. These features can then \nbe used to directly assist mus ic researchers in analysis \nand se arch-based tasks, as well as  in research incorpora t-\ning machine learning.  \nPossible research applications include: empirical tes t-\ning of existing musi cological theor ies [11]; exploratory \nresearch that can  reveal unexpected ins ights  [11]; reco n-\nciling  historical evidence with content -based data [12]; \nannotation of large corpora to allow content -based \nsearches  [10]; performing multimodal re search by co m-\nbining s ymbolic features with audio, textual  and ot her \nfeatures  [9]; and generating  novel music in specific styles \nby using feature values as stylistic guideposts  [23].  \njSymbolic  2.2 has been  dramatically improved and e x-\npanded since its  last properly published  version ( 1.2) was \nreleased in 2010  [9]. It is  also a component  of the larger \njMIR research software framework [ 9]. jSymbolic  and \nthe other jMIR components ( including  source code) can \nall be downloaded from [13].  \n2. RELATED RESEARCH  \nSurprisingly  few frameworks designed specifically for \nextracting features from symbolic music have been pu b-\nlished , although there are several MIR  toolkits for analyz-\ning symbolic music  more generally . The MIDI Toolbox \n[6] is one pa rticularly well -known system i mplemented in \nMatlab . The po werful m usic21 analysis toolkit [ 4] in-\ncludes ports of 57 of the orig inal jSymbolic  1.2 features, \nand also o ffers su bstantial a dditional u seful functionality.  \nThe Humdrum toolkit [ 8] is a well -known tool for an a-\nlyzing music, although it does not extract features as \nsuch. The Melisma Music Analyzer [ 22] is another excel-\nlent analysis -oriented system , and pretty_midi  [17] pro-\nvides helpful crea tion, manipulation and extraction tools.  \nAdditional work has been published where symbolic \nfeature extraction is performed as a part of larger research \nprojects, but where the feature extract ion code has not \nitself been published. Standouts include [1] and [15]. \nCorrêa and Rodrigues have written a nice survey of relat-\ned symbolic genre classification research [2].  \nTo the best of our knowledge, there is no existing \nsoftware that extracts anywhere near the number or dive r-\nsity of features as jSymbolic , nor is there any with the \nsame focus on broad accessibility and extensibility.   © Cory McKay, Julie E. Cumming, Ichiro Fujinaga.  \nLicensed under a Creative Commons Attribution 4.0 International \nLicense (CC BY 4.0). Attribution:  Cory McKay, Julie E. Cumming, \nIchiro Fujinaga. “jSymbol ic 2.2: Extracting Features from Symbolic \nMusic for use in Musicological and MIR Research”, 19th International \nSociety for Music Information Retrieval Conference, Paris, France, \n2018.  \n348  \n \n3. CHARACTERISTICS OF J SYMBOLIC  \n3.1 Features E xtracted  \njSymbolic  2.2 extract s 246 unique features , some of \nwhich  are multidimensional, for a total of 1497  values  \n(version  1.2 extracted 111 features and  1022 values ). De-\ntails on the  original  musicological and music theoretical \nsources and motivations for the features are available in \n[9]. The features can be divided into eight  general groups : \n Pitch Statistics:  How  common are various pitches \nand pitch classes relative to one another ? How are \nthey distributed  and how much do they vary ?  \n Melodic Intervals:  What melodic intervals are pr e-\nsent? How much melodic variation is there? What can \nbe obs erved from melodic contour meas urements?  \n Chords and Vertical Intervals:  Wha t vertical in ter-\nvals are present? Wha t types of chords do they repr e-\nsent? What kinds of  harmonic movement are present ? \n Rhythm:  Information associated with note  attacks , \ndurations  and rests , measured in ways that are both \ndependent and independent  of tempo . Information on \nrhythmic variability, i ncluding r ubato, and meter .  \n Instrumentation:  Which instruments are present, \nand which are emphasized relative to others? Both \npitched and non -pitched instruments are considered.  \n Texture:  How many independent voices are there \nand how do they interact (e.g. parallel vs. contrary \nmotion )? What is the relative importance of voices?  \n Dynamics:  How loud are notes and what kinds of \nvariations in dynamics occur?  \n MEI -Specific:  Information that cannot be represen t-\ned explicitl y in MIDI (e.g. slurs) but can be in the \nMusic Encoding Initiative ( MEI) file format  [16]. \nSee Figure 1 for a complete list of the jSymbolic  2.2 fea-\ntures, including indications of which ones are new , as \nwell as  which ones are multidimensional .  \nThese features ar e designed to be wide -ranging, in o r-\nder to be applicable to a diverse range of musics from a \nvariety of cultures , styles  and time periods. A few fe a-\ntures are intentionally partially  redundant ; for example, \nthe Vertical Interval Histogram indicates the numb er of \nminor thirds and major thirds (among other things) sep a-\nrately, but the Vertical Thirds feature combines them. \nSuch partial redundancies help highlight patterns in alter-\nnative  ways to musicologists examining fea tures . Also, \nsome  features are based on information explicitly (but not \nnecessarily correctly) specified as metadata in the input \nfiles, such as meter or key , and others attempt to infer  \nsuch information directly  from the music itself.  \n3.2 Designing New Features  \nExtensibility and modularity are k ey priorities, as jSym-\nbolic  is intended to be a platform for developing and tes t-\ning new features just as much as it is an out -of-the-box \ntool. New features can be added as plug -ins simply by \nextending an existing Java class, and it is easy to incorp o-rate t he values of existing features  into new features in \norder to iteratively build new features of increasing s o-\nphistication. jSymbolic  also automatically handles all i n-\nfrastructure relating to feature dependencies and extra c-\ntion scheduling.  The overall design  of the software is ex-\ntensible, as is its configuration file format.  \nA tool has been added for exploring MIDI messages \ndirectly at a low -level , in order to help  debug new fe a-\ntures.  jSymbolic  also now automatically  validat es and \nerror -check s new featur es as they are added, and there is \nsubstantial new general unit testing infrastructure.  \n3.3 Configuration Files  \njSymbolic  now includes a flexible configuration file fo r-\nmat that can be used for batch processing, as a way of a p-\nplying consistent settings across s essions and for keeping \na record of settings used in individual  experiments. These \nconfiguration files  can be saved with the GUI , or they can \nbe edited directly . \n3.4 Avoiding Systematic Encoding  Bias \nOne must always be careful that extracted features are not \ncorrelated with the source of data rather than its underl y-\ning music al content . This could happen, f or example, in a \ncorpus constructed  by joining data from different sources , \nwhere  each source uses different encoding conventions  \n(e.g. different instrumentat ion designations for voices, or \ndifferent interpretations of tempo markings).  Such issues \nhave been discussed regarding  audio [21], but less so for \nsymbolic data . Ideally, all data in a corpus would be sy s-\ntematically encoded in the same way, but thi s is rarely  the \ncase  in practice . \njSymbolic therefore now includes functionality for \ngenerating “consistency reports .” These  automatically \ncheck sets of symbolic music files  for such biases.  \nAn optional  “safe” configuration fil e is also provide d, \nwhich  disables features associated with instrumentation, \ndynamics, microtonal pitches and tempo , as these tend to \nbe particular ly vulnerable to encoding bias. This is esp e-\ncially useful for musics  where these qualities are ty pically  \nunspecified , such as Renai ssance music.  \nMany of jSymbolic 2.2’s new features are also specif i-\ncally designed to avoid such biases. For example, many \nof the new rhythmic features are  tempo -independent, so \nthat they can be used even if tempo is source -correlated, \nwhile the old tempo -linked features can still be used if \ntempos are meaningfully and consistently encoded.  \n[3] presents a more detailed analysis of related  issues, \nincluding empirical results produced with jSymbolic  2.2. \n3.5 Windowed Extraction  \nUsers can n ow perform windowed feature extraction with \njSymbolic , with overlapping or non -overlapping wi n-\ndows, as well as extraction over entire pieces. Although \ncommon with audio, this ability to extract features sep a-\nrately from subsets  of a piece is rare in the sym bolic d o-\nmain, and enables powerful new kinds of analysis.  Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 349  \n \n \nOverall Pitch Statistics  \n(128) Basic Pitch Histogram  \n(12) Pitch Class Histogram  \n(12) Folded Fifths Pitch Class Histogram  \nNumber of Pitches  \nNumber of Pitch Classes  \nNumber of Common Pitches  \nNumber o f Common Pitch Classes  \nRange  \nImportance of Bass Register  \nImportance of Middle Register  \nImportance of High Register  \nDominant Spread  \nStrong Tonal Centres  \nMean Pitch  \nMean Pitch Class  \nMost Common Pitch  \nMost Common Pitch Class  \nPrevalence of Most Common Pitch  \nPrevalence of Most Common Pitch Class  \nRelative Prevalence of Top Pitches  \nRelative Prevalence of Top Pitch Classes  \nInterval Between Most Prevalent Pitches  \nInterval Between Most Prevalent Pitch Classes  \nPitch Variability  \nPitch Class Variability  \nPitch Class Va riability After Folding  \nPitch Skewness  \nPitch Class Skewness  \nPitch Class Skewness After Folding  \nPitch Kurtosis  \nPitch Class Kurtosis  \nPitch Class Kurtosis After Folding  \nMajor or Minor  \nFirst Pitch  \nFirst Pitch Class  \nLast Pitch  \nLast Pitch Class  \nGlissando Prevale nce \nAverage Range of Glissandos  \nVibrato Prevalence  \nMicrotone Prevalence  \n \nMelodic Intervals  \n(128) Melodic Interval Histogram  \nMost Common Melodic Interval  \nMean Melodic Interval  \nNumber of Common Melodic Intervals  \nDistance Between Most Prevalent Melodic Interv als \nPrevalence of Most Common Melodic Interval  \nRelative Prevalence of Most Common Melodic Intervals  \nAmount of Arpeggiation  \nRepeated Notes  \nChromatic Motion  \nStepwise Motion  \nMelodic Thirds  \nMelodic Perfect Fourths  \nMelodic Tritones  \nMelodic Perfect Fifths  \nMelodi c Sixths  \nMelodic Sevenths  \nMelodic Octaves  \nMelodic Large Intervals  \nMinor Major Melodic Third Ratio  \nMelodic Embellishments  \nDirection of Melodic Motion  \nAverage Length of Melodic Arcs  \nAverage Interval Spanned by Melodic Arcs  \nMelodic Pitch Variety  \n \nChords and V ertical Intervals  \n(128) Vertical Interval Histogram  \n(12) Wrapped Vertical Interval Histogram  \n(11) Chord Type Histogram  \nAverage Number of Simultaneous Pitch Classes  \nVariability of Number of Simultaneous Pitch Classes  \nAverage Number of Simultaneous Pitches  \nVariability of Number of Simultaneous Pitches  \nMost Common Vertical Interval  \nSecond Most Common Vertical Interval  \nDistance Between Two Most Common Vertical Intervals  \nPrevalence of Most Common Vertical Interval  \nPrevalence of Second Most Common Vertical Interv al \nPrevalence Ratio of Two Most Common Vertical Intervals  \nVertical Unisons  \nVertical Minor Seconds  \nVertical Thirds  Vertical Tritones  \nVertical Perfect Fourths  \nVertical Perfect Fifths  \nVertical Sixths  \nVertical Sevenths  \nVertical Octaves  \nPerfect Vertical Interva ls \nVertical Dissonance Ratio  \nVertical Minor Third Prevalence  \nVertical Major Third Prevalence  \nChord Duration  \nPartial Chords  \nStandard Triads  \nDiminished and Augmented Triads  \nDominant Seventh Chords  \nSeventh Chords  \nNon-Standard Chords  \nComplex Chords  \nMinor Major  Triad Ratio  \n \nRhythm  \n(2) Initial Time Signature  \nSimple Initial Meter  \nCompound Initial Meter  \nComplex Initial Meter  \nDuple Initial Meter  \nTriple Initial Meter  \nQuadruple Initial Meter  \nMetrical Diversity  \nTotal Number of Notes  \nNote Density per Quarter Note  \nNote D ensity per Quarter Note per Voice  \nNote Density per Quarter Note Variability  \n(12) Rhythmic Value Histogram  \nRange of Rhythmic Values  \nNumber of Different Rhythmic Values Present  \nNumber of Common Rhythmic Values Present  \nPrevalence of Very Short Rhythmic Values  \nPrevalence of Short Rhythmic Values  \nPrevalence of Medium Rhythmic Values  \nPrevalence of Long Rhythmic Values  \nPrevalence of Very Long Rhythmic Values  \nPrevalence of Dotted Notes  \nShortest Rhythmic Value  \nLongest Rhythmic Value  \nMean Rhythmic Value  \nMost Common R hythmic Value  \nPrevalence of Most Common Rhythmic Value  \nRelative Prevalence of Most Common Rhythmic Values  \nDifference  Between Most Common Rhythmic Values  \nRhythmic Value Variability  \nRhythmic Value Skewness  \nRhythmic Value Kurtosis  \n(12) Rhythmic Value Median R un Lengths Histogram  \nMean Rhythmic Value Run Length  \nMedian Rhythmic Value Run Length  \nVariability in Rhythmic Value Run Lengths  \n(12) Rhythmic Value Variability in Run Lengths Histogram  \nMean Rhythmic Value Offset  \nMedian Rhythmic Value Offset  \nVariability of R hythmic Value Offsets  \nComplete Rests Fraction  \nPartial Rests Fraction  \nAverage Rest Fraction Across Voices  \nLongest Complete Rest  \nLongest Partial Rest  \nMean Complete Rest Duration  \nMean Partial Rest Duration  \nMedian Complete Rest Duration  \nMedian Partial Rest Dur ation  \nVariability of Complete Rest Durations  \nVariability of Partial Rest Durations  \nVariability Across Voices of Combined Rests  \n(161) Beat Histogram Tempo Standardized  \nNumber of Strong Rhythmic Pulses - Tempo Standardized  \nNumber of Moderate Rhythmic Pulses - Tempo Standardized  \nNum . Relatively Strong Rhythmic Pulses - Tempo Standardized  \nStrongest Rhythmic Pulse - Tempo Standardized  \nSecond Strongest Rhythmic Pulse - Tempo Standardized  \nHarmonicity of Two Strongest Rhythmic Pulses - Tempo Stand . \nStrength of Stro ngest Rhythmic Pulse - Tempo Standardized  \nStrength of Second Strongest Rhythmic Pulse - Tempo Stand . \nStrength Ratio of Two Strongest Rhythmic Pulses - Tempo Stand . \nCombined Strength of 2 Strongest Rhyt h. Pulses - Tempo Stand.  \nRhythmic Variability - Tempo S tandardized  \nRhythmic Looseness - Tempo Standardized  \nPolyrhythms - Tempo Standardized  Initial Tempo  \nMean Tempo  \nTempo Variability  \nDuration in Seconds  \nNote Density  \nNote Density Variability  \nAverage Time Between Attacks  \nAverage Time Between Attacks for Each Voi ce \nVariability of Time Between Attacks  \nAverage Variability of Time Between Attacks for Each Voice  \nMinimum Note Duration  \nMaximum Note Duration  \nAverage Note Duration  \nVariability of Note Durations  \nAmount of Staccato  \n(161) Beat Histogram  \nNumber of Strong Rhyth mic Pulses  \nNumber of Moderate Rhythmic Pulses  \nNumber of Relatively Strong Rhythmic Pulses  \nStrongest Rhythmic Pulse  \nSecond Strongest Rhythmic Pulse  \nHarmonicity of Two Strongest Rhythmic Pulses  \nStrength of Strongest Rhythmic Pulse  \nStrength of Second Stronges t Rhythmic Pulse  \nStrength Ratio of Two Strongest Rhythmic Pulses  \nCombined Strength of Two Strongest Rhythmic Pulses  \nRhythmic Variability  \nRhythmic Looseness  \nPolyrhythms  \n \nInstrumentation  \n(128) Pitched Instruments Present  \n(47) Unpitched Instruments Present  \n(128) Note Prevalence of Pitched Instruments  \n(47) Note Prevalence of Unpitched Instruments  \n(128) Time Prevalence of Pitched Instruments  \nVariability of Note Prevalence of Pitched Instruments  \nVariability of Note Prevalence of Unpitched Instruments  \nNumber of Pi tched Instruments  \nNumber of Unpitched Instruments  \nUnpitched Percussion Instrument Prevalence  \nString Keyboard Prevalence  \nAcoustic Guitar Prevalence  \nElectric Guitar Prevalence  \nViolin Prevalence  \nSaxophone Prevalence  \nBrass Prevalence  \nWoodwinds Prevalence  \nOrche stral Strings Prevalence  \nString Ensemble Prevalence  \nElectric Instrument Prevalence  \n \nTexture  \nMaximum Number of Independent Voices  \nAverage Number of Independent Voices  \nVariability of Number of Independent Voices  \nVoice Equality - Number of Notes  \nVoice Equalit y - Note Duration  \nVoice Equality - Dynamics  \nVoice Equality - Melodic Leaps  \nVoice Equality - Range  \nImportance of Loudest Voice  \nRelative Range of Loudest Voice  \nRelative Range Isolation of Loudest Voice  \nRelative Range of Highest Line  \nRelative Note Density of Highest Line  \nRelative Note Durations of Lowest Line  \nRelative Size of Melodic Intervals in Lowest Line  \nVoice Overlap  \nVoice Separation  \nVariability of Voice Separation  \nParallel Motion  \nSimilar Motion  \nContrary Motion  \nOblique Motion  \nParallel Fifths  \nParallel Octa ves \n \nDynamics  \nDynamic Range  \nVariation of Dynamics  \nVariation of Dynamics i n Each Voice  \nAverage Note to Note Change in Dynamics  \n \nMEI -Specific  \nNumber of Grace Notes  \nNumber of Slurs  \nFigure 1. All features implemented by jSymbolic 2.2 . Headings in bold refer to feature groups, not features. Features in \nitalics are new (added since jSymbolic  1.2). Numbers in parentheses indicate the size of multi -dimensional features. De-\ntailed descriptions of individual  feature s are available in jSymbolic’s manual [ 14] and in its GUI.  350 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018  \n \n \nFigure 2. The new jSymbolic 2.2 manual . \n \nFigure 3. The redesigned jSymbolic  2.2 GUI. \n3.6 I/O Formats and jMei2Midi  \njSymbolic can extract features from music stored in either \nMIDI or MEI [ 16]. MIDI, despite its well -documented \nlimitations, has the e ssential ad vantage that it can be read \nor generated by almost any symbolic music software, and \nalso permits live performance encoding. This latter ben e-\nfit makes  MIDI compatible  with non-Western (and Wes t-\nern) musics that do not conform to the quantized tuni ngs \nand rhythms typical of common practice music notation  \nand the symbolic music formats based on it. MID I is also \nmore suitable for transcribing  audio, which also rarely  \nconform s to strict quantization . \nMEI, in turn, is a rich and extensible form at that a l-\nlows many kinds of important information to be repr e-\nsented that cannot be encapsulated with MIDI. jSymbo l-\nic’s new support for MEI is achieved via our custom -built \nJava MEI parser and MEI -to-MIDI converter called \njMei2Midi, which can also be used as a standalone sof t-\nware library. jMei2Midi performs a more extensive level \nof MEI conversion than any other converter, and also \nmaintains a channel for preserving and transmitting i n-\nformation that cannot be represented in MIDI.  \nAlthough jSymbolic cannot y et directly parse formats \nsuch as Music XML, OSC, Humdrum **kern or LilyPond, there are fortunately many converters that can \ntranslate such formats to MIDI or MEI for jSymbolic fe a-\nture extraction. jSymbolic’s Rodan [ 7] wrapper can a l-\nready do this with Musi c XML.  \nExtracted features can now be saved as both Weka \nARFF [ 24] files (a machine learning format ) and as  gen-\neral-purpose CSV files.  Previously , ACE XML [ 9] was \nthe only output file format option.  \n3.7 Usability and Interfaces  \nIt is crucial that jSymbolic be easy to learn and use for \nusers with diverse technical backgrounds, and that it be \neasily adaptable to a broad range of use cases. Th is has \nbeen a primary focus of the upgrades since version 1.2 . \njSymbolic now includes a detailed HTML  manual \n(Figure 2)  [14] and an extensive step -by-step tutorial  that \ninclud es worked exercises  with both jSymbolic and the \nWeka  data mining framework [ 24]. jSymbolic’s Java i m-\nplementation and lack of external dependencies make the \nsoftwar e platform -independent and easy -to-install . \nThe original jSymbolic was only usable via a GUI, \nwhich has been substantially improved in version 2.2 \n(Figure 3). jSymbolic also now also includes command -\nline interface for batch processing, a well -documented \nJava API for programmatic access and a Roda n [7] wor k-\nflow for those wishing to take advantage of distributed \nprocessing. New  feedback on progress is provided as pr o-\ncessing proceeds , and cleaner error handling and more \ndetailed reporting in general have been instituted.  \n4. GENRE CLASSIFICATION  EXPERIME NTS  \n4.1 Experimental Goals and Methodology  \nOur first set of experiments involved using the jSymbolic \nfeatures to classify music by genre. This was done using \nthe MIDI portion of our (balanced) “SAC ” dataset [ 9], \nwhich consists of 250 pieces of music. SAC is di vided \ninto ten genres: Hardcore Rap, Pop Rap, Bop, Swing, B a-\nroque, Romantic, Alternative Rock, Metal, Modern Blues \nand Traditional Blues. These genres can be combined  \npairwise into five parent genres: Rap, Jazz, Classical, \nRock and Blues. This ontological structure permits one to \nevaluate how well a given approach can distinguish b e-\ntween both dissimilar genres (the five parent genres) and \nsimilar genres (the two classes comprising each pair).  \nFeatures were extracted from SAC using  both the old \njSymbolic 1.2  [9] and the new jSymbolic 2.2 , in order to \nexplore the effects  of the new features . All  implemented  \nfeatures were used, as no systematic encoding biases  \nwere found  in the data (see Section 3.4) . \n The Weka machine learning framework [ 24] was used \nto perfor m 10 -fold cross -validation experiments using its \nSMO support vector machine implementation (with d e-\nfault hyper -parameter settings). No dimensionality redu c-\ntion pre -processing was applied, beyond what SMO does \nitself. This simple and generic  classification methodology \nwas chosen intentionally, as an important goal of this p a-\nProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 351  \n \nper is to emphasize the accessibility of jSymbolic ’s fea-\ntures  to music researchers who may have little or no \nbackground in machine learning. A more sophisticated \napproach would have been used if this were a paper sp e-\ncifically on classification.  \n4.2 Classification  Results and Discussion  \n \nCorpus  jSymbolic 1.2  jSymbolic 2.2  \n Accuracy  F-score  Accuracy  F-score  \nSAC 5  90.4%  0.809  93.2%  0.872  \nSAC 10  75.6%  0.703  77.6%  0.631  \nTable 1. SAC (5 -class  and 10-class) genre c lassification \naccuracies and F -scores (averaged across 10 folds ).  \njSymbolic’s performance (Table 1) is quite impressive  \noverall , especially since  such basic machine learning \ntechniques were used. Although there has not been a \nsymbolic g enre classification MIREX event in over a \ndecade, the 2017 audio genre classification results [ 5] \nprovide a  rough general context:  the highest classification \naccuracies were 75.9% in the 10 -class Latin genre task, \n76.8% in the 10 -class popular genre task a nd 67.9% in \nthe 7 -class K -Pop genre task.  \nThe new 2.2 features provided  better  classification a c-\ncuracies  than the  old 1.2  features on both  versions of \nSAC , by 2.8% and 2.0% . The F -score, ho wever, declined \nfor SAC 10, but i mproved for SAC 5 . \nThe value of jSymbolic 2.2’s grea tly expanded fe ature \ncatalogue has a scope well b eyond  its classif ication pe r-\nformance gains. Many music researchers are i nterested in \nspecifically what it is that differentiates various kinds  of \nmusic, and a greater nu mber of  features make it poss ible \nto explore and understand music more thoroughly  and \nprecisely . This is revisited below.  \n5. COMPOSER ATTRIBUTION  EXPERIMENTS  \n5.1 Experimental Goals and Methodology  \nThe second set of experiments involved the same Weka -\nbased classification  methodologies described in Section \n4.1. This time, however, the experiments involved R e-\nnaissance composer attribution ; this  is much more than a \ntoy problem in early music studies, as there are many \npieces  whose composer is unknown or disputed, and fea-\nture-based machine learning holds significant potential \nfor resolving such debates.  \nWe constructed our (unbalanced) “RenComp7” dataset \nby combining the Josquin  (top two Rodin security levels  \n[19] only, based on historical sources ), La Rue, Ock e-\nghem, Busnoys an d Martini data from [ 19] with John \nMiller’s Palestrina  data and the Victoria da ta used in [20]. \nAll files not already encoded as  MIDI were converted. \nThe resultant RenComp7 corpus consists  of 1584 pieces.  \nAn analysis of the data found that certain  features  were \ninfluenced by systematic encoding bias (see Section 3.4) , \nnamely those based on instrumentation, dynamics and \ntempo. Since including these features would have artif i-cially inflated performance , it was necessary to exclude \ncertain features from consid eration. As a result, only 335 \nof 1022 jSymbolic 1.2 feature  value s and 801 of 1497 \njSymbolic 2.2 feature  values  were used.  \nWe conducted one experiment where classification \nwas performed among all seven RenComp7 composers. \nThis was followed by two pairwis e classifications  that are \nof particular musicological interest: Josquin vs. La Rue \n(exact contemporaries  who are musically similar ) and \nJosquin vs. Ockeghem ( from different generations ). \n5.2 Classification  Results and Discussion  \n \nCorpus  jSymbolic 1.2  jSymboli c 2.2  \n Accuracy  F-score  Accuracy  F-score  \nAll 7  Composers   87.9%  0.634  92.4%  0.715  \nJosq / Ock eghem  84.7%  0.818  92.6%  0.911  \nJosq uin / La Rue  82.0%  0.771  86.3%  0.824  \nTable 2. RenComp7  composer attribution c lassification \naccuracies and F -scores (averaged across 10 folds ).  \nThe overall ability of the jSymbolic 2.2 features to disti n-\nguish between the composers  (Table 2)  is unprecedented \nin the automatic classification literature , and is all the \nmore impressive given the  simple machine learning  \nmethodology  used. The excellent work of Brinkman et al. \n[1] provides the best basis for comparison : the authors \nused 53 features to classify 6 composers (J. S. Bach and \nfive Renaissance composers), and obtained success rates \nof roughly 63% on average. Their ap proach did  very well \nat discriminating  Bach from  the Renaissance composers \n(97%). This highlights both the quality of their approach \nand the particular difficulty of identifying  Renaissance \ncomposers, and makes the success of the jSymbolic fe a-\ntures on exclusively Re naissance music all the more e n-\ncouraging.  The new 2.2 features  outperformed the  old 1.2 \nfeatures  in all cases.   \n5.3 Diving into Features  \nAs noted above, the  relative performance of individual  \nfeatures can be  at least as important in revealing music o-\nlogical insights as overall  classification  performance . As \nan example of research along these lines, we asked two \nexperts on Renais sance music , Julie  E. Cumming and P e-\nter Schubert,  to predict  what characteristics  they though t \nwould  best differentiate the music  of Josquin and Ock e-\nghem , based on their extensive  general  experience stud y-\ning the music of the two composers, and  without any a \npriori  exposure to the feature data . The jSymbolic feature  \ndata was then used to test these expectations . The results, \nas outlined in Figure 4, demonstrate how some of their \npredictions were indeed confirmed, but others were \nshown to be incorrect . This emphasizes  the general need \nin musicology and music theory for  empir ical validation \nof a wide range of wide spread beliefs and assumptions  \nthat have never been confirmed via systematic studies of \nlarge datasets . It is hoped that jSymbolic and similar \nsoftware can help address this issue.  352 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018  \n \nEmpirical Testing  of Expert Predictions of Characteri s-\ntics More Evident in Ockeghem than Josquin  \nCONFIRMED : Less music for more than 4 voices  \nCONFIRMED : More 3 -voice music  \nCONFIRMED : More triple meter  \nSAME:  Less stepwise motion  \nSAME:  More notes at the bottom of the range  \nSAME:  More chords (or simultaneities) without a third  \nSAME:  More varied rhythmic note values  \nOPPOSITE:  More large leaps (larger than a 5th)  \nOPPOSITE:  More dissonance  \nFigure  4. Results of e mpirical testing of expert predi c-\ntions. “CONFIRMED ” means the expectations were \nempirically correct, “SAME” indicates no statis tically \nsignificant difference between the two composers  and \n“OPPOSITE” means the expected characteristic was in-\nfact more associated with Josquin than Ockeghem.  \nNext, in order to demonstrate the types of novel mus i-\ncological insights jSymbolic’s features c an reveal,  Weka \nwas used to apply seven statistical feature analysis tec h-\nniques (based on feature -class correlation s, information \ngain, etc.) to highlight the features that most effectively \ndistinguish  the composers in each of the two composer \npairs. The results were compiled  into ranked feature list s. \n It turns out that a combination of rhythmic characte r-\nistics are particularly important in distinguishing Josquin \nfrom Ockeghem and, furthermore, Ockeghem tends to \nhave more vertical sixths and diminished t riads, as well \nas longer melodic arcs. With respect to Josquin and La \nRue, Josquin tends to have: more vertical unisons and \nthirds; fewer vertical fourths and octaves; and more m e-\nlodic octaves.   \n6. CONCLUSIONS  \njSymbolic is a powerful and accessible tool that music \nresearch ers can apply to diverse research areas  and types \nof music. It can also serve as a platform that researchers \ncan use to develop their own  bespoke  features. It is hoped \nthat this will help address the paucity of symbolic music \nsoftware produce d by the MIR community to date , rela-\ntive to the extensive range of software it has produced  \nassociated with  audio and other data, and will encourage \ngreater  MIR e ngagement with  musicologists and music \ntheorists.  jSymbolic’s easy -to-use interfaces and exte n-\nsive doc ument ation are inten ded to facili tate this.  \nAlthough jSymbolic features can certainly be used in \nclassification tasks, as in the experiments described \nabove, the direct study of feature values also has i m-\nportant potential. Such work  can combine  expert manual \nstudy with the use of  statistical  analysis  techniques. R e-\nsearch can consist of empirical validation of existing h y-\npotheses or of purely exploratory research, all involving \nthe study of potentially huge quantities of music. Both \napproaches can help scholars arrive at in itially unintuitive \nbut potentially crucial musicological insights.  \nIn terms of experimental conclusions, the results from  \nSections 4 and 5 permit the following observations:   The new jSymbolic  2.2 features were quite effective \nin both genre and composer c lassification , even using \ngeneric machine learning approaches. They were able \nto distinguish betwee n seven Renaissance composers \n92.4% of the time, and achieved 9 3.2% genre class i-\nfication accura cy when applied to a 5-genre ontology, \nand 7 7.6% when classify ing amongst 10 genres.  \n The new jSymbolic 2.2 features produced  better clas-\nsification accuracies than the old jSymbolic 1.2 fe a-\ntures in all tests, and better F -scores in all but one . \n The new jSymbolic 2.2 features were effective in \ntesting expert expectations  about differences in the \nmusical style s of pairs of Renaissance composers, \nand in revealing additional unanticipated  differences.  \n7. FUTURE WORK  \nWe will continue to work with musicologists and music \ntheorists by helping them carry out research on large m u-\nsical datasets  with jSymbolic . We will also assist them in \nimplementing  specialized features of their own. A parti c-\nular focus of this collaborative work will be placed on the \ndetermination of which features are most effective in di s-\ntinguishing different musical classes (composers, genr es, \nregions, etc.), and on investigating why. We will also e x-\npand our work on using machine learning to help resolve \ncontroversial composer attribution. We also intend to \nwork on expanding the extent to which jSymbolic can \nextract features from non -Western  musics by adding still \nmore  relevant features.  \nWe are currently working on integrating jSymbolic2 \ninto the SIMSSA/MIRAI architecture [ 10], so that r e-\nsearchers can search the project’s rich music databases \nusing content -based queries formulated using feature va l-\nues and ranges. A researcher could thus filter results \nbased on the amount of chromaticism in a piece, for e x-\nample, or the amount of parallel motion between voices. \nOf even greater interest, queries could potentially be fo r-\nmulated based on hard -to-quantify  high-level characteri s-\ntics, such as degree of tonality, made possible  by machine \nlearning models trained on jSymbolic features.  \nRelated to this project, we also plan to  apply  jSymbo l-\nic to symbolic files that have been generated using optical \nmusic recogn ition or automatic audio transcription sof t-\nware, and to investigate  the robustness of the features to \nsuch error -prone data. In addition to making a n enormous  \namount of new music available for symbolic feature e x-\ntraction, doing this successfully would also  greatly facil i-\ntate multimodal research . The huge Lakh dataset [ 18] can \nalso be studied with similar goals . \nAn additional priority will be to add new parsers so \nthat features can be extract ed from additional file formats. \nWe are especially looking at addin g features specially d e-\nsigned for music encoded using mensural or other early \nmusic notations. Finally, we intend to work towards por t-\ning jSymbol ic2’s new features to other platforms, esp e-\ncially music21  [4]. Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 353  \n \n8. ACKNOWLEDGEMENTS  \nWe would like to thank the Soci al Sciences and Human i-\nties Research Council of Canada (SSHRC) and the Fonds \nde recherche du Québec - Société et culture (FRQSC) for \ntheir generous funding. We would also like to \nacknowledge the formidable contributions of our many \ncollaborators on the MIRA I and SIMSSA projects, esp e-\ncially Tristano Tenaglia.  \n9. REFERENCES  \n[1] A. Brinkman, D. Shanahan and C. Sapp, “Musical \nstylometry, machine learning and attribution studies: \nA semi -supervised approach to the works of Josquin,” \nProc. of the Biennial Int. Conf. on Mu sic Perception \nand Cognition,  pp. 91 –97, 2016.  \n[2] D. C. Corrê a and F. A. Rodrigues, “ A survey on \nsymbolic data -based music genre classification ,” \nExpert Systems with Applications,  Vol. 60, pp. 190 –\n210, 2016.  \n[3] J. E. Cumming et al., “ Methodologies for creating \nsymbolic corpora of Western music before 1600 ,” \nProc. of the Int. Soc. for Music Information \nRetrieval Conf ., accepted for publication, 2018.  \n[4] M. S. Cuthbert, C. Ariza  and L. Friedland,  “Feature \nextraction and machine learning on symbolic music \nusing the mus ic21 toolkit ,” Proc. of the Int. Soc. for \nMusic Information Retrieval Conf.,  pp. 387 –92, \n2011.  \n[5] J. S. Downie et al., “ MIREX201 7 Results - MIREX \nWiki ,” Music -ir.org, 201 6. [Online]. Available: \nhttp://www.music -\nir.org/mirex/wiki/2017:MIREX2017_Results . \n[Acces sed: 28-March -2018]. \n[6] T. Eerola and P. Toiviainen , “MIR in Matlab: The \nMIDI Toolbox ,” Proc. of the Int. Conf. on Music \nInformation Retrieval,  pp. 22–7, 2004 . \n[7] A. Hankinson, “Optical music recognition \ninfrastructure for larg e-scale music document \nanalysis,” Ph.D. diss., Schulich School of Music, \nMcGill Univ., Montreal, Canada, 2015 . \n[8] D. Huron, “Music information processing using the \nHumdrum toolkit: Concepts, examples, and \nlessons ,” Computer Music J. , Vol. 26, No. 2, pp. 11–\n26, 2002.  \n[9] C. McKay, “Automatic music classification with \njMIR,” Ph.D. diss., Schulich School of Music, \nMcGill Univ., Montreal, Canada, 2010.  \n[10] C. McKay and I. Fujinaga, “Building an \ninfrastructure for a 21st -century global music \nlibrary,” Int. Soc. for Music Information Retrieval \nConf. Late Bre aking and Demo Papers,  2015 . [11] C. McKay et al., “Using statistical feature extraction \nto distinguish the styles of different composers,” 45th \nMedieval and Renaissance Music Conf.,  2017.  \n[12] C. McKay et al., “Characterizing composers using \njSymbolic2 features”, Extended Abstracts for the \nLate-Breaking Demo Session of the Int. Soc. for \nMusic Information Retrieval Conf. , 2017.  \n[13] C. McKay, “jMIR,” SourceForge.net, \nSourceForge.net,  2018. [Online]. Available: \nhttp://jmir.sourceforge.net. [Accessed: 6-June-2018].  \n[14] C. McKay , “jSymbolic Manual,” SourceForge.net, \n2018. [Online]. Available: \nhttp://jmir.sourceforge.net/manuals/jSymbolic_manu\nal/home.html. [Accessed: 6-June-2018].   \n[15] P. J. Ponce de León and J. M. Iñesta , “Statistical \ndescription models for melody a nalysis and \ncharac terization,”  Proc. of the Int. Computer Music \nConf.,  pp. 149 –56, 2004.  \n[16] P. Roland, “The music encoding initiative (MEI),” \nProc. of the First Int. Conf. on Musical Applications \nUsing XML , pp. 55–9, 200 2. \n[17] C. Raffel, and D. P. W. Ellis , “Intuitive analysis, \ncreation and manipulation of MIDI data with \npretty_midi ,” Int. Soc. for Music Information \nRetrieval Conf. Late Breaking and Demo Papers,  \n2014 .  \n[18] C. Raffel. “Learning -based m ethods for comparing \nsequences, with a pplications to audio-to-MIDI \nalignment and match ing,”  Ph.D. diss., Columbia \nUniv ., New York, USA, 2016.  \n[19] J. Rodin, C. S. Sapp and C. Bokulich, “The Josquin \nResearch Project ,” Stanford Univ ., 2017. [Online]. \nAvailable: http://josquin.stanford.edu. [ Accessed: \n28-March -2018]. \n[20] A. Sigler, J. Wild and E. Hande lman, “Schematizing \nthe treatment of dissonance in 16th-century \ncounterpoint,” Proc. of the Int. Soc. for Music \nInformation Retrieval Conference , pp. 645 –51, 2015.  \n[21] B. L. Sturm, “ A simple method to determine if a \nmusic informatio n retrieval system is a ‘hor se’,” \nIEEE Trans. on Multimedia , Vol. 16, No. 6, pp. \n1636 –44, 2014.  \n[22] D. Temperley, The cognition of basic musical \nstructures.  Cambridge, MA: MIT Press , 2001.  \n[23] E. Verdaguer Morales, “ Anàlisi i generació \nalgorísmica de línies de baix en estil Funk ,” Thesis, \nPompeu Fabra  Univ., Barcelona, Spain, 2014.  \n[24] I. H. Witten, E. Frank  and M. A. Hall, Data mining: \nPractical machine learning tools and techniques.  \nNew York: Morgan Kaufman , 2011 . 354 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Evaluating Automatic Polyphonic Music Transcription.",
        "author": [
            "Andrew McLeod",
            "Mark Steedman"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492339",
        "url": "https://doi.org/10.5281/zenodo.1492339",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/148_Paper.pdf",
        "abstract": "Automatic Music Transcription (AMT) is an important task in music information retrieval. Prior work has focused on multiple fundamental frequency estimation (multi-pitch detection), the conversion of an audio signal into a timefrequency representation such as a MIDI file. It is less common to annotate this output with musical features such as voicing information, metrical structure, and harmonic information, though these are important aspects of a complete transcription. Evaluation of these features is most often performed separately and independent of multi-pitch detection; however, these features are non-independent. We therefore introduce M V 2H, a quantitative, automatic, joint evaluation metric based on musicological principles, and show its effectiveness through the use of specific examples. The metric is modularised in such a way that it can still be used with partially performed annotation— for example, when the transcription process has been applied to some transduced format such as MIDI (which may itself be the result of multi-pitch detection). The code for the evaluation metric described here is available at https://www.github.com/apmcleod/MV2H.",
        "zenodo_id": 1492339,
        "dblp_key": "conf/ismir/McleodS18",
        "keywords": [
            "Automatic Music Transcription",
            "Important task in music information retrieval",
            "Multiple fundamental frequency estimation",
            "Conversion of audio signal",
            "Time-frequency representation",
            "MIDI file",
            "Annotating output with musical features",
            "Voicing information",
            "Metrical structure",
            "Harmonic information"
        ],
        "content": "EVALUATING AUTOMATIC POLYPHONIC MUSIC TRANSCRIPTION\nAndrew McLeod\nUniversity of Edinburgh\nA.McLeod-5@sms.ed.ac.ukMark Steedman\nUniversity of Edinburgh\nsteedman@inf.ed.ac.uk\nABSTRACT\nAutomatic Music Transcription (AMT) is an important\ntask in music information retrieval. Prior work has focused\non multiple fundamental frequency estimation (multi-pitch\ndetection), the conversion of an audio signal into a time-\nfrequency representation such as a MIDI ﬁle. It is less\ncommon to annotate this output with musical features such\nas voicing information, metrical structure, and harmonic\ninformation, though these are important aspects of a com-\nplete transcription. Evaluation of these features is most of-\nten performed separately and independent of multi-pitch\ndetection; however, these features are non-independent.\nWe therefore introduce MV 2H, a quantitative, automatic,\njoint evaluation metric based on musicological principles,\nand show its effectiveness through the use of speciﬁc ex-\namples. The metric is modularised in such a way that\nit can still be used with partially performed annotation—\nfor example, when the transcription process has been ap-\nplied to some transduced format such as MIDI (which may\nitself be the result of multi-pitch detection). The code\nfor the evaluation metric described here is available at\nhttps://www.github.com/apmcleod/MV2H.\n1. INTRODUCTION\nAutomatic Music Transcription (AMT) involves convert-\ning an acoustic musical signal into some form of music\nnotation. The process has generally been divided into two\nsteps: ﬁrst, multi-pitch detection, which is the conversion\nof the signal into a piano-roll notation (such as a MIDI\nﬁle) by detecting which pitches are present at each time;\nand second, the conversion of that piano-roll notation into\na musical score by annotating it with further musical infor-\nmation. Readers can refer to [2] for an overview of AMT.\nThe ﬁrst step, multi-pitch detection, has been the focus\nof a great amount of research in AMT. The second step in-\nvolves many subtasks of musical analysis, including voice\nseparation, metrical alignment, note value detection, and\nharmonic analysis. Each of these has been the subject of\nresearch both directly from the acoustic signal and from\nother input formats such as MIDI. They are usually per-\nformed separately, though some recent work has attempted\nc\rAndrew McLeod, Mark Steedman. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Andrew McLeod, Mark Steedman. “Evaluating Automatic\nPolyphonic Music Transcription”, 19th International Society for Music\nInformation Retrieval Conference, Paris, France, 2018.to analyse subsets of them jointly. For example, [27] es-\ntimates both chords and downbeats directly from acoustic\ninput. [33] performs voice streaming, metrical alignment,\nand harmonic analysis jointly from symbolic input. How-\never, even in the case of these joint models, evaluation is\nperformed separately for each subtask. Rather than simply\ntaking an average of a model’s score on each subtask, there\nis a need for a standardised way to compute the joint score\nin a way that reﬂects overall AMT performance.\nIn this paper, we introduce MV 2H(from Multi-pitch\ndetection, Voice separation, Metrical alignment, note\nValue detection, and Harmonic analysis), a metric to quan-\ntitatively evaluate AMT systems that perform both multi-\npitch detection and musical analysis. The metric can be\nused for AMT systems that do not perform all aspects of\na full musical analysis—for example, those that perform\nmulti-pitch detection and meter detection, but nothing else.\nOne of the main principles of the new metric is that of\ndisjoint penalties: that mistakes should only be penalised\nonce. That is, if an error in one part of the transcrip-\ntion causes a mistake in another part, that error should not\nbe counted twice. For example, if a pitch is missed dur-\ning multi-pitch detection, the metric should not further pe-\nnalise missing that note from the voice separation results.\nBased on this principle, we do not include errors re-\nlated to the proper typesetting of a transcription in our met-\nric, and we do not even require a typeset musical score\nto perform our evaluation. Most typesetting decisions\ncome down to the proper analysis of the underlying piece.\nFor example, if metrical alignment is performed properly,\nbeaming comes naturally. Likewise, stem directions can\nfollow from voice separation and pitch spelling is a conse-\nquence of a proper harmonic analysis. For details related\nto the proper typesetting of music and its relation to the\nunderlying music analysis, see [14].\n2. EXISTING METRICS\nEach of the separate tasks involved in the full AMT process\nhas been the subject of much prior research, and there are\nexisting metrics for each of them. This section gives a brief\noverview of the most widely used metrics for each subtask.\n2.1 Multi-pitch Detection\nMulti-pitch detection is evaluated both at the frame level\nand at the note level depending whether a given model in-\ncludes some form of note tracking or not. As the goal of\nthis paper is to deﬁne a metric which is useful for a com-42plete AMT system, the note-level evaluation metrics are\nmost applicable here, and readers interested in the frame-\nbased evaluation, an accuracy metric, should refer to [28].\nFor the note-level metric, a note is deﬁned by its pitch,\nonset time, and offset time. [1] deﬁnes two different preci-\nsion, recall, and F-measures for note-level multi-pitch de-\ntection. For the ﬁrst, they deﬁne true positives as those\nnotes detected whose pitch lies within a quartertone of that\nof a ground truth note, and whose onset time is within\n50msof the same ground truth note’s onset time, regard-\nless of offset time. Spuriously detected notes are each as-\nsigned a false positive, and ground truth notes which are\nnot matched by a detected note are each assigned a false\nnegative. The second metric they propose is identical, with\nthe additional constraint that a detected note’s offset time\nmust be accurate to within a certain threshold for it to be\nconsidered a true positive. Both of these metrics are used\nby both the Music Information Retrieval Evaluation Ex-\nchange (MIREX) [25] and the mir eval package [29].\nFor our purposes, we care mostly about onset time and\npitch (to the nearest semitone) as these aspects are most\ndirectly relevant to the underlying musical score. Offset\ntime, on the other hand, is applicable as far as it relates\nto note value, and is discussed in Section 2.4. Our multi-\npitch detection metric will therefore be based most closely\non the ﬁrst multi-pitch F-measure, which doesn’t account\nfor offset time.\n2.2 Voice Separation\nV oice separation refers to the separation of the notes of a\npiece of music into perceptual streams called voices. There\nis no standardised deﬁnition of what constitutes a voice,\nand a full discussion can be found in [3]. In this work, we\nrestrict each voice to be monophonic. This aligns with the\nmajority of work on voice separation, and is beneﬁcial in\nAMT in that it allows simpler processing of monophonic\ndata to occur in the later musical analysis steps.\nThere is no standardised metric for evaluating voice\nseparation performance. [5] deﬁnes Average Voice Consis-\ntency (A VC), which returns an average of the percentage of\nnotes in each voice which have been assigned to the correct\nvoice. (A note is said to be assigned to the correct voice if\nits ground truth voice is the most common one for notes\nassigned to its voice.) This metric has a problem in that if\na model assigns each note to a distinct voice, it achieves\na perfect A VC of 100% . For acoustic input, [19, 30] use\na similar metric, with the addition that spuriously detected\nnotes automatically count as incorrect.\n[17] deﬁnes two metrics: soundness , which measures\nthe percentage of consecutive notes in an assigned voice\nwhich belong to the same ground truth voice; and com-\npleteness , which measures the percentage of consecutive\nnotes in a ground truth voice which were assigned to the\nsame voice. Finally, [12] deﬁnes a precision, recall, and\nF-measure evaluation, in which the problem of voice as-\nsignment is treated as a graph problem where each note is\nrepresented by a node, and two nodes are connected by an\nedge if and only if they are consecutive notes in an assignedvoice. The values are calculated by counting the number\nof correct edges (true positives), spurious edges (false pos-\nitives), and omitted edges (false negatives).\nEach of these metrics would penalise an AMT system\nfor any spurious notes, so for our proposed metric, we will\nneed to use a modiﬁed version of one of them (or design\na new metric) in order to enforce the principle of disjoint\npenalties.\n2.3 Metrical Alignment\nMetrical alignment is most often approached as one of\nthree different tasks: downbeat tracking, beat tracking,\nor metrical structure detection. Downbeat tracking and\nbeat tracking each involve identifying points in time, and\nthus can theoretically be evaluated using the same metrics,\nwhich are summarised by [8, 9]. F-measure [11] (which\ndownbeat tracking work uses almost exclusively), is cal-\nculated by counting the number of (down)beats within\nsome window length (usually 70ms) of an annotated\n(down)beat. [4] proposes a similar metric, where accuracy\nis calculated instead using a Gaussian window around each\nannotated beat. [13] proposes a binary metric which is 1if\nthe beats are correctly tracked for at least 25% of the piece,\nand0otherwise. P-score [18], is the proportion of tracked\nbeats which correctly match an annotated beat, normalised\nby either the number of tracked beats or the number of\nannotated beats (whichever is greater). Finally, [15] pro-\nposes metrics based on the longest continuously tracked\nsection of music. All of the above are used to some extent\nin beat-tracking, and all are used by both mir eval [29] and\nMIREX. [21, 23] In addition, evaluation is also often pre-\nsented at twice and half the annotated beat length, to handle\nmodels which may track a beat at the wrong metrical level.\nBy comparison, the evaluation of metrical structure de-\ntection is far less sophisticated. Meter detection is the or-\nganisation of the beats of a given musical performance into\na sequence of trees at the bar level, in which each node\nrepresents a single note value. The structure of each of\nthese trees is directly related to the music’s time signature,\nwhere the head of each tree splits into a number of nodes\nequal to the number of beats per bar, and each of these beat\nnodes splits into a number of nodes equal to the number of\nsub-beats per beat. Thus, each time signature uniquely de-\nscribes a single metrical tree structure as deﬁned by the\nnumber of beats per bar and sub-beats per beat in that time\nsignature. The most basic evaluation is to simply report\nthe proportion of musical excerpts for which the model\nguesses the correct metrical structure and phase (such that\neach tree aligns correctly with a single bar). Another ap-\nproach is to simply report the proportion of musical ex-\ncerpts for which the model correctly classiﬁes the meter as\nduple or triple [10]. Both of these metrics are simplistic,\nand fail to take into account some idea of partially correct\nmetrical structure trees.\nTwo metrics have been used for metrical structure de-\ntection evaluation which contain within them an evaluation\nof beat tracking and downbeat tracking, making them ideal\nfor an evaluation of a joint model. [31] proposes a metricProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 43which takes into account the level on the metrical tree at\nwhich each note lies in order to capture some idea of partial\ncorrectness. However, since it is based on detected notes,\nit is not robust to errors in multi-pitch detection. [20] intro-\nduces an F-measure metric where each level of the detected\nmetrical structure is assigned a true positive if it matches\nany level of the ground truth metrical structure (even if it is\nnot the same level). A false positive is given for any level\nof the detected metrical structure which clashes with a met-\nrical grouping in the ground truth, and a false negative for\nany metrical level in the ground truth which remains un-\nmatched by a level of the detected metrical structure. As\nit is based solely on metrical groupings, rather than notes,\nit is robust to multi-pitch detection errors, and would not\nviolate our principle of disjoint penalties. However, it was\ndesigned for use with metronomic input, and would there-\nfore need to be adapted for our purposes of evaluating a\ncomplete AMT system on live performance data.\n2.4 Note Value Detection\nNote value detection (identifying a note as a quarter note,\neighth note, dotted note, tied note, etc.) is not a widely\nresearched problem, related to a combination of note offset\ntime and metrical alignment. [26] describes two metrics\nfor the task. One, error rate, is simply the percentage of\nnotes whose transcribed value is incorrect. The other, scale\nerror, takes into account the magnitude of the error as well\n(relative to the metrical grid), in log space such that errors\nfrom long notes do not dominate the calculation.\nHowever, since the measured note values are reported\nrelative to the underlying meter, they violate our property\nof disjoint penalties and we must design a new measure of\nnote value detection accuracy for our metric.\n2.5 Harmonic Analysis\nHarmonic analysis involves both key detection, a classi-\nﬁcation problem of identifying one of twelve tonic notes,\neach with two possible modes (major or minor—alternate\nmode detection has not been widely researched); and chord\ntracking, identifying a sequence of chords and times given\nan audio recording. The possible chords to identify range\nfrom simply identifying the correct root note, to determin-\ning major or minor, identifying seventh chords, and even\nidentifying different chord inversions.\nThe standard key detection evaluation, used by both\nmireval [29] and MIREX [24], is to assign a score of 1:0\nto the correct key, 0:5to a key which is a perfect ﬁfth too\nhigh, 0:3to the relative major or minor of the correct key,\n0:2to the parallel major or minor of the correct key, and\n0:0otherwise.\nThe standard chord tracking evaluation is chord sym-\nbol recall (CSR)—described by [16], and used by both\nMIREX [22], and mir eval [29]—deﬁned as the propor-\ntion of the input for which the annotated chord matches the\nground truth chord. There can be varying levels of speci-\nﬁcity for what exactly constitutes a match, since different\nsets of possible chords can be used as described above.2.6 Joint Metric\nFor the joint evaluation of AMT performance, [7] presents\na system to transcribe MIDI input into a musical score\n(thus including errors from typesetting), and evaluate it us-\ning ﬁve human evaluators. The evaluators were asked to:\n“1) Rate the pitch notation with regard to the key signature\nand the spelling of notes. 2) Rate the rhythmic notation\nwith regard to the time signature, bar lines, and rhythmic\nvalues. 3) Rate the notation with regard to stems, voicing,\nand placement of notes on staves,” each on a scale of 1\nto10. The three questions roughly correspond with four\nof our sections above: 1) harmonic analysis; 2) metrical\nalignment, note value detection; and 3) voice separation.\n[6] describes an automatic metric for the same task,\nsimilar to string edit distance, taking into account the or-\ndering of 12 different aspects of a musical score: barlines,\nclefs, key signatures, time signatures, notes, note spelling,\nnote durations, stem directions, groupings, rests, rest dura-\ntion, and staff assignment.\nWhile this metric is a great step towards an automatic\nevaluation of AMT performance, it violates our principle\nof disjoint penalties. A single mistake in metrical align-\nment can manifest itself in the time signature, rest dura-\ntions, note durations, and even additional notes (tied notes\nare counted as separate objects in the metric).\nIt appears that both of the above metrics measure some-\nthing slightly different from what we want. They measure\nthe readability of a score produced by an AMT system,\nwhile we really want a metric which measures the accuracy\nof the analysis performed by the AMT system, a slightly\ndifferent task. To our knowledge, no metric exists which\nmeasures the accuracy of the analysis performed by a com-\nplete AMT system in the way we desire.\n3. NEW METRIC\nOur proposed metric, MV 2H, draws from existing met-\nrics where possible, though we take care to ensure that our\nprinciple of disjoint penalties is not violated. Essentially,\nwe calculate a single score for each aspect of the transcrip-\ntion, and then combine them all into the ﬁnal joint metric.\n3.1 Multi-pitch Detection\nFor multi-pitch detection, we use an F-measure very simi-\nlar to the one by [1] described above, counting a detected\nnote as a true positive if its detected pitch (in semitones) is\ncorrect and its onset lies within 50ms of the ground truth\nonset time. All other detected notes are false positives, and\nany unmatched ground truth notes are false negatives. Note\noffset time does not factor into our evaluation; rather, see\nSection 3.4 for a discussion on the related problem of note\nvalue detection.\n3.2 Voice Separation\nFor voice separation, we use an F-measure very similar\nto [12], taking care not to violate our principle of dis-\njoint penalties. Speciﬁcally, we don’t want to penalise any\nmodel in voice separation for multi-pitch detection errors.44 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 1 : An example transcription of the ground truth bar\n(left) is shown (right). The voice connection between the\nlast two notes in the lower voice counts as a true positive,\neven though they are not consecutive in the ground truth.\nRecall that the F-measure is calculated as a binary clas-\nsiﬁcation problem where for each ordered pair of notes, we\nmust decide if they occur consecutively in the same voice\nor not. To address the disjoint penalties violation, we alter\nthis slightly. We ﬁrst remove from both the ground truth\nvoices and the detected voices any notes which have not\nbeen matched as a true positive. Then, we perform the\nsame F-measure calculation with the new voices.\nAs an illustration of this, see Figure 1. In the tran-\nscribed music, the last two notes in the lower voice are\nboth matched with a ground truth note (in pitch and on-\nset time), but are not immediately sequential in the ground\ntruth voice. However, because the intervening note was\nnot correctly transcribed, the link between these two notes\ncounts as a true positive. (The second note in the tran-\nscribed lower voice does indeed count as an error.) This\nnew F-measure calculation is equivalent to the standard\nvoice separation F-measure when multi-pitch detection is\nperformed perfectly.\n3.3 Metrical Alignment\nFor metrical alignment, we would like to use a metric sim-\nilar to that from [20] which has some idea of the partial\ncorrectness of a metrical alignment. However, as it is de-\nsigned for use mainly on metronomic data where a metri-\ncal hypothesis cannot move in and out of phase throughout\na piece, a few adjustments must be made to adapt it for\nuse on live performance data. We call our newly designed\nevaluation metric the metrical F-measure. It takes into ac-\ncount every grouping at three levels of the metrical hierar-\nchy throughout an entire piece: the sub beat level, the beat\nlevel, and the bar level.\nFor each hypothesised grouping at these metrical lev-\nels, we check if it matches a ground truth grouping at any\nlevel. A hypothesised grouping is said to match a ground\ntruth grouping if its beginning and ending times are each\nwithin 50msof the beginning and ending times of that\nparticular ground truth grouping, regardless of the metrical\nlevel of either grouping.1Each matched pair of group-\nings within a piece count as a true positive, while any un-\nmatched hypothesis groupings count as false positives, and\nany unmatched ground truth groupings count as false nega-\ntives. The metrical F-measure of a piece is then calculated\n1We use a 50msthreshold, rather than the more common 70ms,\nbecause it was shown by [8] that 50mscorresponds more exactly with\nhuman judgement for beat tracking. However, this threshold may need to\nbe tuned for different genres as regular syncopation can tend to misalign\nnotes with the metrical grid in certain genres more than others [32].\nFigure 2 : An example transcription of the ground truth\nbar (left) is shown (right). Those notes which are assigned\na note value score are coloured. Of those, the C (assuming\ntreble clef) is assigned a score of 0.5, while the others are\nassigned a score of 1.\nas the harmonic mean of precision and recall as usual.\n3.4 Note Value Detection\nIt is difﬁcult to disentangle note value detection from\nmulti-pitch detection, voice separation, and metrical align-\nment in order to include it in our evaluation without violat-\ning our principle of disjoint penalties. Clearly, note value\nshould only be regarded if the note has been counted as a\ntrue positive in the multi-pitch detection evaluation. Less\nobviously, we also disregard any detected note which is not\nfollowed in its transcribed voice by the correct note. Ad-\nditionally, note value depends directly on meter such that\nany note value accuracy metric must measure note value\nrelative to time rather than the underlying metrical grid.\nTherefore, we deﬁne a note value score which measures\nonly a subset of the detected notes: those which both (1)\ncorrespond with a true positive multi-pitch detection; and\n(2) correspond with a true positive ground truth voice seg-\nment as described in the previous paragraph. Each note\nwhich matches those two criteria is assigned a score ac-\ncording to the accuracy of its normalised duration (that is,\nthe duration corresponding to its note value rather than its\nperformed duration). Speciﬁcally, each note is counted as\ncorrect and assigned a score of 1if its normalised dura-\ntion is within 100msof the normalised duration of the\ncorresponding ground truth note.2Otherwise, its score\nis calculated as in Equation 1, where dur gtis the ground\ntruth note’s normalised duration and dur detis the detected\nnote’s normalised duration. This score is 1when the dura-\ntions match exactly and scales linearly on both sides to a\nscore of 0for a note with 0duration or a note with twice\nthe ground truth note’s duration. The overall note value\nscore is calculated as the arithmetic mean of the scores of\nthose notes which are assigned a score.\nscore =max\u0010\n0;1\u0000jdur gt\u0000dur detj\ndur gt\u0011\n(1)\nFigure 2 illustrates this note value score. Only those\nnotes which are coloured are considered for the note value\nscore. Notice that the two C’s (assuming treble clef) on the\ndownbeat are not considered due to errors in voice sepa-\nration. Likewise, the last two notes in the lower voice are\nalso not counted against note value score due to note de-\ntection errors, even though they count as true positives for\n2We use 100mshere to allow for a 50mserror in both onset and\noffset time, although this value again may need to be tuned for different\ngenres.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 45the voice separation F-measure. Of the coloured notes, the\nC would be assigned a score of around 0:5(depending on\nexact timing), since its value duration is off by exactly half\nof the ground truth note’s value duration. The others would\nreceive scores of 1. Thus, the ﬁnal note value score would\nbe the average of 1,1,1, and 0:5, or about 0:875.\n3.5 Harmonic Analysis\nFor harmonic analysis, we use the standard key detection\nand CSR metrics described above, as neither one violates\nour principle of disjoint penalties since they are based on\ntime rather than notes or the metrical alignment. For now,\nwe take the set of possible chords to include a major and\nminor version for each root note, but not sevenths or inver-\nsions, although the full collection of chords should be used\nfor the ﬁnal version of our metric.\nTo combine the two into a single harmonic analysis\nscore, we take the arithmetic mean of the two values, since\nthey are both on the range [0–1]. Models which only per-\nform one of the above tasks may simply use that task’s\nscore as their harmonic analysis score.\n3.6 Joint Metric\nWe now have ﬁve values to combine into a single number:\nthe multi-pitch detection F-measure, the voice separation\nF-measure, the metrical F-measure, the note value detec-\ntion accuracy score, and the harmonic analysis mean. All\nof these values are on the range [0–1] such that a value of\n1results from a perfect transcription in that aspect. We\nconsider three different approaches for their combination:\nharmonic mean, geometric mean, and arithmetic mean.\nHarmonic mean is most useful when there is potential\nfor one of the values involved to be signiﬁcantly larger than\nthe others, and thus dominate the overall result. F-measure,\nfor example, is the harmonic mean between precision and\nrecall, and is used so that models cannot receive a high F-\nmeasure by simply tuning their model to have a very high\nrecall or precision; rather, both recall and precision must\nbe relatively high in order for their harmonic mean to also\nbe high. This is not relevant in our case as there is no way\nfor a model to tune itself towards one very high score at\nthe expense of the others as is the case with some binary\nclassiﬁcation problems.\nGeometric mean is most useful when the values in-\nvolved are on different scales. Then, a given percent\nchange in one of the values will result in the same change\nin mean as the same percent change to another of the val-\nues. This property is not necessary for us because all of\nour values lie on the same range.\nArithmetic mean is a simple calculation that weights\neach of the values involved equally. This property is de-\nsirable for us because, for a complete transcription, all ﬁve\naspects of an analysis must be correct. Furthermore, due\nto our property of disjoint penalties, we have kept the ﬁve\nvalues involved disjoint, and a model must fairly perform\nwell on all aspects in order for its overall score to be high.\nTherefore, for the ﬁnal joint metric, MV 2H(forMulti-\npitch detection, Voice separation, Metrical alignment, noteValue detection, and Harmonic analysis), we take the\narithmetic mean of the ﬁve previously calculated values.\nWe also want the metric to be usable no matter what subset\nof analyses is performed, for example, for models which\nrun on MIDI input and therefore do not perform multi-\npitch detection. In these cases, we advise using our met-\nric and simply taking the arithmetic mean of only those\nscores which correspond with analyses performed. In fu-\nture work, we will investigate whether a linear combina-\ntion of the ﬁve values involved, perhaps weighting some\nmore strongly than others, aligns more exactly with human\njudgements than the current arithmetic mean.\n4. EXAMPLES\nTo illustrate the effectiveness and appropriateness of our\nmetric, we present in Figure 3 two example transcriptions\nof the ﬁrst four bars of Bach’s Minuet in G, each exhibit-\ning different errors. Figure 3a shows the ground truth tran-\nscription (where the chord progression is shown beneath\nthe staff), and the example transcriptions are shown be-\nlow. We make two assumptions: (1) ground truth voices\nare separated by clef (plus the bottom two notes in the ini-\ntial chord, which each belong to their own voice); and (2)\nThe sub beats of each transcription align in time with the\nsub beats of the ground truth.\nFigure 3b shows an example transcription which is good\nin general, with just a few mistakes, mostly related to the\nmetrical alignment. First, for the multi-pitch detection F-\nmeasure, we can see that the transcription has 20 true pos-\nitives, 3 false negatives (a G on the second beat in the ﬁrst\nbar, a C on the second beat of the third bar, and the ﬁnal\nG in the fourth bar), and 0 false positives, resulting in an\nF-measure of 0:93. For voice separation, this transcription\nis generally good, making a single bad assignment in the\nsecond bar, resulting in 3 false positives (the connections\nto and from the incorrect assignment, as well as the incor-\nrect connection in the treble clef), 3 false negatives (the\ncorrect connections to and from the misclassiﬁed note, as\nwell as the correct connection in the bass clef), and a voice\nseparation F-measure of 0:83. Notice that the missed G in\nthe upper voice in the treble clef of the ﬁrst bar does not re-\nsult in a penalty for voice assignment due to our principle\nof disjoint penalties. For metrical alignment, we can see\nthat this transcription is notated in68time, correctly group-\ning all sub beats (eighth notes) and bars, yielding 28 true\npositives, but incorrectly grouping three sub beats together\ninto dotted quarter note beats, yielding 8 false positives and\n12 false negatives. This results in a metrical F-measure of\n0:74. For note value detection, 14 notes are counted: all\nof the bass clef notes and all of the eighth notes in the ﬁrst\nbar, only the high D in the second bar, the low C and all\nof the eighth notes in the third bar, and the high G and the\nlow B in the fourth bar. Notice that the initial high D isn’t\ncounted because the next note in its voice has not been de-\ntected. Similarly, neither the G on the second beat of the\nsecond bar nor any of the bass clef notes in the second bar\nare counted due to voice separation errors. Of the 14 notes,\n13 of them are assigned the correct note value (even the46 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018(a) Ground truth\n(b) Transcription 1\n(c) Transcription 2\nFigure 3 : Two different example transcriptions of the ﬁrst\nfour bars of Bach’s Minuet in G.\nﬁrst bass chord, since its incorrect typesetting and the ties\nare related to the incorrect metrical alignment—the note\nvalue still ends at the correct point in time). One note (the\nC in the bass clef on the downbeat of the third bar) is as-\nsigned a value score of 0:5(since its value duration is half\nof the correct value duration). This results in a note value\ndetection score of 0:96. The harmonic analysis in this tran-\nscription is entirely correct, resulting in a harmonic score\nof1:0. Thus, the MV 2Hof the ﬁrst transcription is 0:89.\nThis makes sense because the transcription is quite good in\ngeneral, but a few mistakes are made, the most glaring of\nwhich is the metrical alignment (its lowest score).\nFigure 3c shows another example transcription which\nis again good in general, this time with a few more errors\nin multi-pitch detection, as well as a poor harmonic anal-\nysis. For multi-pitch detection, it contains 17 true posi-\ntives, 4 false positives, and 6 false negatives, resulting in\nan F-measure of 0:77. This number is 0:16lower than\nthat the previous transcription’s corresponding F-measure,\nand this makes sense intuitively: the ﬁrst transcription\ndoes seem to have resulted from a more accurate multi-\npitch detection than the second. For voice separation, this\nsecond transcription contains no errors. Some erroneous\nnotes are placed into one voice or the other, but all of the\ncorrectly detected notes are also correctly separated into\nvoices, resulting in a perfect voice separation F-measure\nof1:0. Likewise the metrical alignment is performed per-\nfectly, resulting in a metrical F-measure of 1:0. For note\nvalue detection, we look at all of the true positive note de-\ntections except (1) the initial D on the downbeat of the ﬁrst\nbar, (2) the B in the bass clef of the ﬁrst bar, (3) the C in\nthe bass clef of the third bar, and (4) the high F at the end\nof the third bar. (All of these exceptions are due to missed\nnote detections of the following note in each voice.) All of\nthe remaining notes have been assigned the correct value,\nresulting in a note value detection score of 1:0. For the\nharmonic analysis, the model has incorrectly transcribed\nthe excerpt in D major, resulting in a key score of 0:5.Transcription 1 2\nMulti-pitch 0.93 0.77\nV oice 0.83 1.0\nMeter 0.74 1.0\nNote Value 0.96 1.0\nHarmonic 1.0 0.5\nMV 2H 0.89 0.85\nTable 1 : The resulting evaluation scores from each of the\nexample transcriptions from Figure 3.\nLikewise, the model has incorrectly labelled the chord pro-\ngression as D-G-G-G, rather than G-G-C-G. Thus, it has\ntranscribed the correct chord for half of the transcription,\nresulting in a CSR of 0:5, and a harmonic score of 0:5.\nTheMV 2Hof the second transcription is therefore 0:85:\nslightly worse than the ﬁrst transcription, but still good.\nThe scores of both transcriptions are summarised in Ta-\nble 1, and intuitively, they make sense. Both seem good\noverall, though they both contain errors. The ﬁrst tran-\nscription has an incorrectly notated meter (although its bars\nand sub beats still align correctly) and a few other smaller\nmistakes related to multi-pitch detection, voice separation,\nand note value detection. The second transcription, on the\nother hand, correctly aligns the meter, and makes its only\nerrors in its harmonic analysis (which is quite poor), and\nin multi-pitch detection (it is worse than the ﬁrst model\nin this regard). Given these examples, for applications\nwhich need a good all-around transcription, we would rec-\nommend the system which produced the ﬁrst transcription.\nHowever, applications which emphasise metrical structure\ndetection or voice separation should consider using the sys-\ntem which produced the second transcription instead.\n5. CONCLUSION\nAs research moves towards the goal of a complete AMT\nsystem, an automatic, standardised, quantitative metric for\nthe task will become a necessity. To that end, we have pro-\nposed a joint metric, MV 2H, which measures multi-pitch\ndetection, voice separation, metrical alignment, note value\ndetection, and harmonic analysis and summarises them in\na single number. Our metric is based on the property of\ndisjoint penalties: that a model should not be penalised\ntwice for errors which come from a single mistake or mis-\ninterpretation. While our metric may not be the ﬁnal stan-\ndardised metric used for the task, we believe that it should\nbecome part of the discussion, and that the principles that\nguided us through its creation should continue to be ad-\ndressed by any future proposed metrics.\nFuture work will evaluate our metric on a wider corpus\nof realistic transcriptions. In particular, we will investigate\nhow well our metric aligns with human judgements, testing\na weighted average of the ﬁve values involved, rather than\nusing the arithmetic mean. A more advanced multi-pitch\ndetection metric, for example one which weights errors ac-\ncording to their perceptual salience, could be another av-\nenue for improvements.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 476. ACKNOWLEDGEMENTS\nThis work was partially funded by EU ERC H2020 Ad-\nvanced Fellowship GA 742137 SEMANTAX and a Google\nfaculty award.\n7. REFERENCES\n[1] Mert Bay, Andreas F. Ehmann, and J. Stephen Downie.\nEvaluation of Multiple-f0 Estimation and Tracking\nSystems. In ISMIR , pages 315–320, 2009.\n[2] Emmanouil Benetos, Simon Dixon, Dimitrios Gian-\nnoulis, Holger Kirchhoff, and Anssi Klapuri. Auto-\nmatic music transcription: challenges and future di-\nrections. Journal of Intelligent Information Systems ,\n41(3):407–434, July 2013.\n[3] Emilios Cambouropoulos. V oice And Stream: Percep-\ntual And Computational Modeling Of V oice Separa-\ntion. Music Perception , 26(1):75–94, September 2008.\n[4] A. Taylan Cemgil, Bert Kappen, Peter Desain, and\nHenkjan Honing. On tempo tracking: Tempogram\nRepresentation and Kalman ﬁltering. Journal of New\nMusic Research , 29(4):259–273, December 2000.\n[5] Elaine Chew and Xiaodan Wu. Separating V oices in\nPolyphonic Music: A Contig Mapping Approach. In\nComputer Music Modeling and Retrieval , pages 1–20,\n2004.\n[6] Andrea Cogliati and Zhiyao Duan. A metric for music\nnotation transcription accuracy. In ISMIR , pages 407–\n413, 2017.\n[7] Andrea Cogliati, David Temperley, and Zhiyao Duan.\nTranscribing Human Piano Performances into Music\nNotation. In ISMIR , pages 758–764, 2016.\n[8] Matthew E. P. Davies and Sebastian B ¨ock. Evaluating\nthe Evaluation Measures for Beat Tracking. In ISMIR ,\npages 637–642, 2014.\n[9] Matthew E. P. Davies, Norberto Degara, and Mark D.\nPlumbley. Evaluation Methods for Musical Audio\nBeat Tracking Algorithms. Queen Mary University of\nLondon, Centre for Digital Music, Technical Report\nC4DM-TR-09-06 , 2009.\n[10] W. Bas De Haas and Anja V olk. Meter Detection in\nSymbolic Music Using Inner Metric Analysis. In IS-\nMIR, pages 441–447, 2016.\n[11] Simon Dixon. Evaluation of the Audio Beat Track-\ning System BeatRoot. Journal of New Music Research ,\n36(1):39–50, March 2007.\n[12] Ben Duane and Bryan Pardo. Streaming from MIDI us-\ning constraint satisfaction optimization and sequence\nalignment. In Proceedings of the International Com-\nputer Music Conference , pages 1–8, 2009.[13] Masataka Goto and Yoichi Muraoka. Issues in Evalu-\nating Beat Tracking Systems. In Workshop on Issues in\nAI and Music , pages 9–16, 1997.\n[14] Elaine Gould. Behind bars : the deﬁnitive guide to mu-\nsic notation . Faber Music, 2011.\n[15] Stephen W. Hainsworth. Techniques for the Automated\nAnalysis of Musical Audio . PhD thesis, University of\nCambridge, 2003.\n[16] Christopher Harte. Towards automatic extraction of\nharmony information from music signals . PhD thesis,\nQueen Mary University of London, 2010.\n[17] Phillip Kirlin and Paul Utgoff. VOISE: Learning to\nSegregate V oices in Explicit and Implicit Polyphony.\nInISMIR , pages 552–557, 2005.\n[18] M. F. McKinney, D. Moelants, Matthew E. P. Davies,\nand Anssi Klapuri. Evaluation of Audio Beat Tracking\nand Music Tempo Extraction Algorithms. Journal of\nNew Music Research , 36(1):1–16, March 2007.\n[19] Andrew McLeod, Rodrigo Schramm, Mark Steed-\nman, and Emmanouil Benetos. Automatic transcription\nof polyphonic vocal music. Applied Sciences , 7(12),\n2017.\n[20] Andrew McLeod and Mark Steedman. Meter detection\nin symbolic music using a lexicalized pcfg. In Proceed-\nings of the 14th Sound and Music Computing Confer-\nence, pages 373–379, 2017.\n[21] MIREX. Audio beat tracking. http://www.\nmusic-ir.org/mirex/wiki/2017:\nAudio\\_Beat\\_Tracking , 2017. Accessed:\n2017-07-18.\n[22] MIREX. Audio chord estimation. http:\n//www.music-ir.org/mirex/wiki/2017:\nAudio\\_Chord\\_Estimation , 2017. Accessed:\n2017-07-18.\n[23] MIREX. Audio downbeat estimation. http:\n//www.music-ir.org/mirex/wiki/2017:\nAudio\\_Downbeat\\_Estimation , 2017.\nAccessed: 2017-07-18.\n[24] MIREX. Audio key detection. http://www.\nmusic-ir.org/mirex/wiki/2017:\nAudio\\_Key\\_Detection , 2017. Accessed:\n2017-07-18.\n[25] MIREX. Multiple fundamental frequency esti-\nmation & tracking. http://www.music-ir.\norg/mirex/wiki/2017:Multiple\\\n_Fundamental\\_Frequency\\_Estimation\\\n_\\%26\\_Tracking , 2017. Accessed: 2017-07-18.\n[26] Eita Nakamura, Kazuyoshi Yoshii, and Simon Dixon.\nNote value recognition for piano transcription using\nmarkov random ﬁelds. IEEE/ACM Transactions on Au-\ndio, Speech, and Language Processing , 25(9):1846–\n1858, sep 2017.48 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[27] H ´el`ene Papadopoulos and Geoffroy Peeters. Joint Esti-\nmation of Chords and Downbeats From an Audio Sig-\nnal. IEEE Transactions on Audio, Speech, and Lan-\nguage Processing , 19(1):138–152, January 2011.\n[28] Graham E Poliner and Daniel P. W. Ellis. A Dis-\ncriminative Model for Polyphonic Piano Transcription.\nEURASIP Journal on Advances in Signal Processing ,\n2007(1):154–162, 2007.\n[29] Colin Raffel, Brian Mcfee, Eric J. Humphrey, Justin\nSalamon, Oriol Nieto, Dawen Liang, Daniel P. W. El-\nlis, C Colin Raffel, Brian Mcfee, and Eric J. Humphrey.\nmireval: A Transparent Implementation of Common\nMIR Metrics. In ISMIR , 2014.\n[30] Rodrigo Schramm, Andrew McLeod, Mark Steedman,\nand Emmanouil Benetos. Multi-pitch detection and\nvoice assignment for a cappella recordings of multi-\nple singers. In ISMIR , pages 552–559, Suzhou, Octo-\nber 2017.\n[31] David Temperley. An Evaluation System for Metri-\ncal Models. Computer Music Journal , 28(3):28–44,\nSeptember 2004.\n[32] David Temperley. Communicative pressure and the\nevolution of musical styles. Music Perception , 21:313–\n337, 2004.\n[33] David Temperley. A Uniﬁed Probabilistic Model for\nPolyphonic Music Analysis. Journal of New Music Re-\nsearch , 38(1):3–18, March 2009.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 49"
    },
    {
        "title": "Meter Detection and Alignment of MIDI Performance.",
        "author": [
            "Andrew McLeod",
            "Mark Steedman"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492357",
        "url": "https://doi.org/10.5281/zenodo.1492357",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/136_Paper.pdf",
        "abstract": "Metrical alignment is an integral part of any complete automatic music transcription (AMT) system. In this paper, we present an HMM for both detecting the metrical structure of given live performance MIDI data, and aligning that structure with the underlying notes. The model takes as input only a list of the notes present in a performance, and labels bars, beats, and sub beats in time. We also present an incremental algorithm which can perform inference on the model efficiently using a modified Viterbi search. We propose a new metric designed for the task, and using it, we show that our model achieves state-of-the-art performance on a corpus of metronomically aligned MIDI data, as well as a second corpus of live performance MIDI data. The code for the model described in this paper is available at https://www.github.com/apmcleod/met-align.",
        "zenodo_id": 1492357,
        "dblp_key": "conf/ismir/McleodS18a",
        "keywords": [
            "HMM",
            "metrical structure",
            "notes",
            "Viterbi search",
            "incremental algorithm",
            "performance",
            "corpus",
            "live performance",
            "code",
            "GitHub"
        ],
        "content": "METER DETECTION AND ALIGNMENT OF MIDI PERFORMANCE\nAndrew McLeod\nUniversity of Edinburgh\nA.McLeod-5@sms.ed.ac.ukMark Steedman\nUniversity of Edinburgh\nsteedman@inf.ed.ac.uk\nABSTRACT\nMetrical alignment is an integral part of any complete au-\ntomatic music transcription (AMT) system. In this paper,\nwe present an HMM for both detecting the metrical struc-\nture of given live performance MIDI data, and aligning that\nstructure with the underlying notes. The model takes as in-\nput only a list of the notes present in a performance, and\nlabels bars, beats, and sub beats in time. We also present\nan incremental algorithm which can perform inference on\nthe model efﬁciently using a modiﬁed Viterbi search. We\npropose a new metric designed for the task, and using it,\nwe show that our model achieves state-of-the-art perfor-\nmance on a corpus of metronomically aligned MIDI data,\nas well as a second corpus of live performance MIDI data.\nThe code for the model described in this paper is available\nat https://www.github.com/apmcleod/met-align.\n1. INTRODUCTION\nMeter detection is the organisation of the beats of a given\nmusical performance into a sequence of trees at the bar\nlevel, in which each node represents a single note value\n(although the actual durations of a node at a given level\nwill vary with the tempo). In common-practice Western\nmusic (the subject of our work), the children of each node\nin the tree divide its duration into some number of equal-\nvalue notes such that every node at a given depth has equal\nvalue. The metrical structure of a single44bar, down to the\nquaver level, is shown in Figure 1. Each level of a metrical\ntree corresponds with a pulse level in the underlying mu-\nsic: bar, beat, and sub beat, from top to bottom. The nodes\nshould align in time with corresponding pulses in the per-\nformed music. There are theoretically more divisions fur-\nther down the tree all the way to the tatum level (the fastest\npulse present in a piece of music, often the 32nd note), but\nas these three levels are enough to unambiguously identify\nthe time signature of a piece, we do not consider any lower.\nThe task is an integral component of automatic music\ntranscription (AMT), particularly when trying to identify\nthe time signature of a given performance. The time sig-\nnature may change between bars (though this is not par-\nticularly common). However, such changes in structure\nc\rAndrew McLeod, Mark Steedman. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Andrew McLeod, Mark Steedman. “Meter Detection and\nAlignment of MIDI Performance”, 19th International Society for Music\nInformation Retrieval Conference, Paris, France, 2018.\u000e\n\f\n\u000b \u000b\f\n\u000b \u000b\f\n\u000b \u000b\f\n\u000b \u000b\nFigure 1 . The metrical structure of a44bar.\nare not currently handled by our model, and are left for\nfuture work. The proposed model can be applied to any\npiece where the metrical tree structure under each node\nat a given level of the tree is identical. In this work, we\nevaluate our model only on the simple and compound me-\nter types2\nX,3\nX,4\nX,6\nX,9\nX, and12\nX(whereXmay take any\nvalue), and leave more uncommon and irregular meters for\nfuture work. Those interested in asymmetric meter detec-\ntion should refer to [9].\nExisting work on full metrical alignment of live perfor-\nmance MIDI data is sparse. There is a good amount of\nexisting work on meter detection (but not alignment) from\nmetronomic data (e.g., [2, 14]), including some which la-\nbels the meter type (i.e., duple or compound) of a given\npiece of music, but does not align a full metrical structure\nwith the notes of the piece (except for synthetic rhythms, as\nin [8]). There is existing work which performs full metrical\nalignment of MIDI data, but not from live performance [4].\nIn the acoustic domain, beat tracking and downbeat detec-\ntion are relatively common areas of research, but stop short\nof a full meter detection and alignment (e.g. [1, 7]).\nThe related problems of rhythm quantisation and note\nvalue detection have also seen some attention, but neither\nare directly relevant to our task. For example, [17] quan-\ntises performed rhythms to a grid, but the set of possible\nonset locations for notes is known a priori (and changes\nbased on the time signature of the underlying piece). [3]\ntracks beats and tempo, but does not go so far as to align\na full metrical grid with bars and sub beats. [15] assigns\na note value to each note, but does not explicitly align the\nnotes with any underlying beat or meter.\n[22] performs full metrical structure detection and\nalignment probabilistically from live performance data by\njointly modelling tempo, meter, and rhythm; however, the\nevaluation was very brief, only testing the model on 3 bars\nof a single Beatles piano performance, and the idea was\nnot used further on MIDI data to our knowledge. [19] pro-\nposes a Bayesian model for the meter detection and align-\nment of monophonic MIDI performance data which mod-113els the probability of a note onset occurring given the cur-\nrent level of the metrical tree at any time with Bayes’ rule.\nThis is combined with a simple Bayesian model of tempo\nchanges, giving a model which can detect the full metri-\ncal structure of a performance. [20] extends this model to\nwork on polyphonic data, combining it into a joint model\nwith a Bayesian voice separator and a Bayesian model of\nharmony. This joint model performs well on full metri-\ncal structure detection and alignment on a corpus of piano\nexcerpts, and we compare against it in this work.\n2. PROPOSED MODEL\nOur proposed model tracks pulses at the tatum level of a\nmusical performance based on two musicological princi-\nples: (1) the rate of these tatums should be relatively con-\nstant without large discontinuities; and (2) notes should lie\nrelatively close to these tatums. The model is an HMM\nwhere the observed data is the notes of a given piece,\ngrouped into sets.\n2.1 State Space\nEach stateSin our model represents a single bar, contain-\ning (1) a list of the tatums from that bar and (2) a metrical\nhierarchy, describing which of those tatums are beats and\nsub beats. The list of tatums is represented by S:t, where\nS:tiis theith tatum in the bar, and S:tjS:tjis the downbeat\nof the following bar. The tatums are in increasing time\norder, whereT(S:ti)represents the time of tatum S:ti.\nA state’s metrical hierarchy has some number of tatums\nper sub beat, sub beats per beat, and beats per bar, as well\nas an anacrusis length, measured by the number of tatums\nwhich fall before the ﬁrst downbeat of a given piece. In our\nmodel, we restrict the number of tatums per sub beat to be\n4, although in theory, any number could be used. We also\nrestrict the anacrusis length to be some integer multiple of\nthe number of tatums per sub beat, a simplifying assump-\ntion that ensures the ﬁrst note of each piece will fall on a\nsub beat. The set of possible sub beat per beat and beat per\nbar pairs (i.e., time signatures) are taken all of those found\nin our training set (2\nX,3\nX,4\nX,6\nX,9\nX, and12\nX). A state’s tempo,\nT(S), is deﬁned as the average length of its beats.\nEach possible initial state S0contains no tatums, and\nevery possible metrical hierarchy is considered equally\nprobable. To reduce our model’s search space, we place\na restriction on the range of allowed values for T(S1):\ntmin\u0014T(S1)\u0014tmax. Nonetheless, because the pos-\nsible tatum times for each state are unbounded, our model\ncontains an inﬁnite number of possible states. Thus, in-\nstead of predeﬁned emission and transition probabilities,\nwe deﬁne emission and transition functions, presented in\nthe following sections.\n2.2 Emission Function\nAfter the initial state (which emits nothing), each state Si\nemits a set of notes Ni, containing only notes nwhose\nonset times lie between that state’s ﬁrst (inclusive) and\nlast (exclusive) tatum. This set is allowed to be empty.Each emitted note has an onset time On (n), an offset time\nOff(n), and a pitch Pitch (n)(though it is unused).\nThe probability of a state Sito emit the note set Niis\npresented as P(NijSi)in Eqn (1). The ﬁrst term, calcu-\nlated entirely by the lexicalised probabilistic context-free\ngrammar (LPCFG) presented in [13], is used to prefer gen-\nerating rhythms which have a high probability according\nto the grammar. The LPCFG is a replacement grammar\nwhich ﬁrst parses a given rhythm into a metrical tree struc-\nture. It then assigns strengths to nodes in the tree based\non note duration in a process called lexicalisation. The\nprobability of a tree is calculated by taking the product\nof the learned probabilities of each grammar transition,\nbased on counting occurrences of a given transition from\na training corpus of parsed rhythms. Each note is aligned\nto the nearest tatum by the LPCFG in order to calculate\nP(rhythm ), but this alignment is neither saved nor emit-\nted. The LPCFG is designed to work directly on mono-\nphonic melodies only. Therefore, for polyphonic input,\nthisP(rhythm )term is in fact a product of one probability\nper voice, each of which is calculated by the LPCFG. For\nvoice assignments, we use [12] as a preprocessing step.\nP(NijSi) =P(rhythm )Y\nn2NP(njSi:t) (1)\nThe second term in Eqn (1) is used to prefer states\nwhose tatums align closely with the emitted notes, and is\ncalculated as in Eqn (2), where N1(\u0016;\u001b;x )conceptually\nrepresents a normal distribution with mean \u0016and standard\ndeviation\u001bevaluated at x.1Thus,P(njSi:t)is used to\nassign higher probabilities to those states which emit notes\nwhich are closely-aligned with their tatums. In this equa-\ntion,closest (Si:t)represents the tatum from Siwhose\ntime is closest to the onset time of the note n.\nP(njSi:t) =N1\u0000\n0;\u001bn;On(n)\u0000T(closest (Si:t))\u0001\n(2)\n2.3 Transition Function\nA stateSi\u00001may transition to a state Siif and only if:\n(1) the two states’ metrical hierarchies are identical (our\nmodel cannot handle pieces with time signature changes)\nand (2) the time of the last tatum in Si\u00001is equal to the\ntime of the ﬁrst tatum in Si. Note that the second condition\nis invalid in the case of a transition from S0toS1sinceS0\ncontains no tatums; in this case, we instead restrict S1:t1\nto lie exactly on the ﬁrst observed note’s onset time.\nThe transition probability P(SijSi\u00001)is shown in Eqn\n(3), where the ﬁrst term, deﬁned in Eqn (4), models the\nprobability of a tempo change and the second term, deﬁned\nin Eqn (5), models the spacing of the tatums themselves.\nP(SijSi\u00001) =P(T(Si)jT(Si\u00001))P(S:t) (3)\n1Normal distributions are used in multiple places throughout this\nmodel with potentially widely varying standard deviations, resulting in\npotentially wildly different results when evaluated at an identical number\nof standard deviations from the mean for different normal distributions.\nSince the distributions are used in contexts in which they cannot be prop-\nerly normalised (due to their continuous domain), the precise probability\nvalue forN1(\u0016;\u001b;x )is calculated using a standard normal distribution\nwith mean 0and standard deviation 1evaluated atx\u0000\u0016\n\u001b.114 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018P(T(Si)jT(Si\u00001)) =(\nN1(\u0016t0;\u001bt0;T(Si))i= 1\nN1\u0000\n0;\u001bt;T(Si)\u0000T(Si\u00001)\nT(Si\u00001)\u0001\ni\u00152\n(4)\nP(S:t) =E(b2S:t)Y\nb2S:t\u0000\nE(sb2b)Y\nsb2bE(t2sb)\u0001\n(5)\nIn Eqn (4), the tempo of the ﬁrst bar (where i= 1) is as-\nsumed to be normally distributed around \u0016t0with standard\ndeviation\u001bt0, while subsequent tempo changes are evalu-\nated as the proportional change from the tempo of the pre-\nvious bar, again normally distributed, this time with mean\n0and standard deviation \u001bt. Percent change is used rather\nthan absolute change because human perception of tempo\nchanges have been shown to follow Weber’s Law [21].\nFor the tatum timings in Eqn (5), the function E(t), de-\nﬁned in Eqn (6), evaluates the probability of the evenness\nof any given list of times. E(b2S:t)calculates this for all\nof the beats bin the state, while the terms E(sb2b)and\nE(t2sb)perform the same calculation for the sub beats\nin each beat and the tatums in each sub beat respectively.\nE(t) =(\nN1(\u0016e;\u001be;\u001b(t)\n\u0016(t))=Enorm\u001b(t)\n\u0016(t)\u0015\u0016e\nN1(\u0016e;\u001be;\u0016e)=Enorm\u001b(t)\n\u0016(t)<\u0016e(6)\nEnorm =1\n2+\u0016e\n\u001beN1(\u0016e;\u001be;\u0016e) (7)\nE(t)is a piecewise function which takes as input a list\nof the lengths of a group of tatums, sub beats, or beats\n(rather than their times). Here, \u0016(t)represents the mean of\nthose lengths and \u001b(t)represents the standard deviation of\nthose lengths. The function is calculated as a modiﬁed nor-\nmal distribution with mean \u0016eand standard deviation \u001be,\nbased on the input list’s standard deviation as a proportion\nof its mean. If this proportion is greater than or equal to\n\u0016e, the result is calculated from a straightforward normal\ndistribution. Otherwise, the result is exactly the value of a\nstandard normal distribution evaluated at its mean.\nThis value is then normalised so as to ensure the new\ndistribution’s integral to again sum to 1by dividing by\nthe factorEnorm , deﬁned in Eqn (7) as the sum of two\nterms.1\n2is the area of the standard normal distribution\ngreater than the mean, and\u0016e\n\u001beN1(\u0016e;\u001be;\u0016e)is the area of\nthe rectangle formed by extending the peak of the standard\nnormal distribution to the left until the value correspond-\ning to 0from the non-standardised normal distribution, as\nvalues less than this correspond to a negative \u001b(t), which\nis not possible.\n2.4 Search Space Reduction\nWe use a modiﬁed Viterbi search to perform inference on\nour model, using a beam search where at each step we save\nonly theBmost probable hypothesis states (not including\nthose still at S0with no tatums yet).\nFor the transition from S0toS1, we introduce two\nheuristics: (1) the ﬁrst tatum in S1must lie exactly on\nthe onset of the ﬁrst observed note and (2) the last tatum\ninS1must also lie exactly on a note onset, though which\nnote speciﬁcally is not restricted by any means other thanlimiting the tempo of the ﬁrst bar using tmin andtmax.\nAccording to these heuristics, for each S0, the supervisor\ncreates the observed note set for every possible S1. Al-\nlowed times for the tatums in S1:tare also restricted based\non each observed note set N1. Essentially, all tatums are\nplaced evenly unless there is a speciﬁc reason not to (i.e.,\nunless a note onset lies close to a tatum).\nSpeciﬁcally, a given value for S1:tis legal if it can ever\nbe generated by the following procedure. First, the appro-\npriate number of beats (according to a given state’s metri-\ncal hierarchy) are placed between the ﬁrst and last tatum\ntimes, as if each tatum was evenly spaced. Next, each\nplaced beat—excluding the last beat as well as the ﬁrst—\nmay be shifted to the location of any note whose onset time\nis within half of one sub beat length of the original beat lo-\ncation. Each beat (again excluding the ﬁrst and last as ap-\npropriate) may then be nudged up to half of a tatum length\naround its location with a magnetism of Mb, as shown in\nEqn (8). Here, tis the original time of the beat, Mis the\nmagnetism ( Mbin this case) which is used to control how\nfar the beat is nudged, and Nis the set of notes which lie\nwithin the given window. This equation can always return\nthe original time, though it is also allowed to nudge the\ngiven time towards either the onset time of the closest note\n(closest (N)) or the average onset time of all notes within\nthe window ( avg(N)), ifNis large enough. Sub beats are\nplaced similarly: initially evenly between any of the exist-\ning beats, then nudged up to one tatum length around its\nlocation with magnetism Msb. Notice that the sub beats\nare not shifted. Finally, tatums are placed evenly between\nthe sub beats (and neither shifted nor nudged).\nnudge (t;M;N ) =8\n><\n>:t always\nt+M(closest (N)\u0000t)jNj>0\nt+M(avg(N)\u0000t)jNj>1\n(8)\nAllowed times for the tatums in Si:tfori >1are re-\nstricted to those which can be generated by the same proce-\ndure, with the exception that the ﬁnal beat in Si:tmay now\nbe shifted and nudged. Initial beat locations are calculated\nsuch thatT(Si\u00001) =T(Si).\nIntuitively, this process of shifting and nudging allows a\nhypothesis’ tempo to smoothly increase or decrease based\non the observed notes. Beats are allowed to change the\ntempo more drastically than sub beats because they are\nmore salient, and more likely to align with note onsets.\nEven with the above restrictions, the search space is\nstill large. As mentioned we use a beam search, where\nat each step we save only the top Bmost probable hypoth-\nesis states (not including those still at S0with no tatums\nyet). Before we remove those hypotheses which fall out-\nside the beam, we remove hypotheses which are deemed\nto be too close to another more probable hypothesis based\non a threshold \u0001min. Speciﬁcally, a hypothesis which has\nidentical metrical hierarchy to a more probable hypothe-\nsis, and whose tempo and most recent tatum time both lie\nwithin \u0001minof that other hypothesis’ tempo and most re-\ncent tatum time is removed.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 1152.5 Supervisor\nIt is important to note that due to the way in which the\nobserved note sets are grouped by bar, the individual note\nsets for different paths through the HMM state space for\na given piece will not be identical, although the union of\nall note sets on any given path equals exactly the set of\nnotes present in the piece. To handle this complication, we\nintroduce a supervisor during the HMM decoding process\nwhich takes each note individually in onset order, grouping\nthem into note sets and passing the sets to the appropriate\nhypothesis state at each step. Speciﬁcally, for a given hy-\npothesis state, the supervisor determines the longest and\nshortest possible lengths for the following bar (based on\nallowed shifts and nudges as described in the previous sec-\ntion). Then, it creates every possible note set given those\nbounds, and allows the hypothesis state to transition and\nbranch on each of those note sets.\n2.6 Optimisations\nHere we describe two changes used to make our model\nmore robust in regards to the idiosyncrasies of live perfor-\nmance such as staccato and ornamentation.\nFor handling staccato notes which are much shorter than\ntheir note values would suggest in the score, we extend\neach note’s offset until either the onset of the following\nnote in the bar within its voice (if one exists), or to the\nend of its bar. This allows the LPCFG, which is trained on\nmetronomic MIDI where staccato is not present, to better\nrecognise the rhythms present in live performance.\nFor handling ornamentation such as trills, we use a\nthresholdtrillmax. Any note whose onset time is within\ntrillmax of the onset time of the previous note within its\nvoice is removed (though the removed notes are still used\nwhen deciding whether to remove the subsequent note).\nThe overall effect of this process is that trills or any very\nfast ornamentation (which again would not be present in\nthe LPCFG’s training data) are reduced to a single short\nnote with its onset at the start of the trill or ornamentation.\nIf this optimisation is used in conjunction with the extend\nnotes optimisation, the remaining notes are extended only\nafter trills and ornamentation are removed, and the result is\nthat a fast ornamentation is replaced by a single long note.\n3. EVALUATION\n3.1 Corpora\nFor evaluation, we use two corpora: one containing\nmetronomic MIDI ﬁles of the 48 fugues from Bach’s\nWell-Tempered Clavier (WTC)2and Bach’s 15 Inven-\ntions,3and another of 13 live performance MIDI ﬁles of\nBach’s fugues and preludes from the WTC, from Crest-\nMusePEDB4[10]. For training, we also use the miscel-\nlaneous corpus, released and used by [20] for training, di-\nvided into a live performance portion (containing 22 pieces\n2The fugues were acquired from www.musedata.org .\n3The inventions were acquired from www.imslp.org .\n4We do not include the 13th prelude from WTC Book I due to an error\nin the ﬁle.by various composers recorded from a MIDI keyboard)\nand a metronomic portion (containing 45 non-performed\npieces by various composers). For voice assignments in all\ncorpora, we run [12] as a preprocessing step.\n3.2 Training\nTo train most of the parameters for the beat tracking model,\nwe measure statistics from the live performance portion of\nthe miscellaneous corpus. This results in values of \u0016t0=\n1:0885s,\u001bt0= 709:918ms,\u001bt= 0:0743 ,\u0016e= 0:0181 ,\n\u001be= 0:0336 ,\u001bn= 6:655ms,tmin= 0:4s, andtmax=\n3s.\nThe remaining parameters are set in an ad hoc fashion\nthrough testing on the miscellaneous corpus, and we have\nfound our model’s performance not to be very sensitive to\nthe precise values used. Speciﬁcally, we use Mb= 1:0,\nMsb= 0:5, andtrillmax= 0:1s.\u0001minandBare sim-\nply optimisations used to improve the speed of our model,\nand we use values of 1msand200respectively, though in\npractice, lower values of \u0001minor higher values of Bonly\nimprove our model’s performance.\nFor our standard evaluation, we train the LPCFG’s\nprobabilities from the metronomic portion of the miscel-\nlaneous corpus, since this allows for a direct comparison\nwith the model of [20]. However, it is noted in [13] that\nthe grammar is sensitive to a lack of training data, partic-\nularly a lack of training data in the style of the evaluation\ncorpus, which happens when training on the miscellaneous\ncorpus for evaluation on Bach compositions. To investi-\ngate this further, we also run experiments when training the\nLPCFG’s probabilities on a superset containing the metro-\nnomic portion of the miscellaneous corpus as well as the\nentire metronomic corpus of Bach compositions. Note that\nwhen evaluating this version of our model, we leave out the\npiece currently being evaluated from the grammar’s train-\ning set so as to avoid overﬁtting. In all experiments, we\ntrain the LPCFG with data that has undergone the same\noptimisations as the data to be evaluated (in terms of ex-\ntending notes and removing trills and ornamentation).\n3.3 Metric\nQuantitative evaluation of previous work on meter align-\nment, particularly with MIDI data, is uncommon, and a\nfew possible metrics are discussed in [18]. [20] reports\nﬁve values which take into account tempo, phase, and the\nbranching factor at each level of the metrical tree. Work\non acoustic meter detection (e.g. [11]) often reports F-\nmeasures of beats and downbeats, treated as points in time.\nTo evaluate our model’s performance, we would rather\nuse a metric similar to that from [13] which is a single\nvalue, takes into account the tree structure’s groupings\n(rather than just its beat locations), and has some idea of\nthe partial correctness of a metrical alignment. However,\nas it is designed for use mainly on beat-aligned data where\na metrical hypothesis cannot move in and out of phase\nthroughout a piece, a few adjustments must be made to\nadapt it for use on live performance data. We call our\nnewly designed evaluation metric the metrical F-measure.116 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Method Metronomic Live Performance\nTemperley [20] 67.65 47.62\nThis Work 78.71 39.63\n+T 75.36 39.40\n+X 79.89 45.27\n+X +T 77.67 47.81\n+Bach 80.48 38.21\n+Bach +T 80.08 42.35\n+Bach +X 80.50 55.43\n+Bach +X +T 80.48 56.51\nTable 1 . The average metrical F-measure of our method\ncompared against those of [20] on our two corpora. +T\nindicates use of the remove trills and ornamentation opti-\nmisation, +X indicates use of the extend notes optimisa-\ntion, and +Bach indicates using the additional Bach train-\ning data for the LPCFG.\nIt takes into account every grouping at three levels of the\nmetrical hierarchy throughout an entire piece: the sub beat\nlevel, the beat level, and the bar level.\nFor each hypothesised grouping at these metrical levels,\nwe check if it matches a ground truth grouping. A hypoth-\nesised grouping is said to match a ground truth grouping if\nits beginning and ending times are each within 70msof\nthe beginning and ending times of that particular ground\ntruth grouping, regardless of the metrical level of either\ngrouping.5Each matched pair of groupings within a piece\ncount as a true positive, while any unmatched hypothe-\nsis groupings count as false positives, and any unmatched\nground truth groupings count as false negatives. The metri-\ncal F-measure of a piece is then calculated as the harmonic\nmean of precision and recall as usual, and our reported re-\nsults average these metrical F-measures across all songs in\neach corpus.\n3.4 Results\nWe compare our model against that of Temperley [20],\nwhich is trained entirely on the miscellaneous corpus. For\ndirect comparison, the standard version of our model is\ntrained on the same corpus, but we present an evaluation\nof a few different versions of it based on different optimi-\nsations or training data. Results can be found in Table 1,\nwhere +T indicates use of the remove trills and ornamen-\ntation optimisation, +X indicates use of the extend notes\noptimisation, and +Bach indicates that the LPCFG training\nwas augmented with the additional Bach compositions. We\ndo not also augment Temperley’s model with additional\ntraining data because there is no straightforward way to\ndo so, and the model does not seem to be one which would\nbe as sensitive to a lack of training data as our model.\nThe results show that on metronomic data, our model\nwithout optimisations clearly outperforms Temperley’s\nwhen using identical training data. The optimisations offer\nno signiﬁcant improvement (which is unsurprising as they\nwere designed speciﬁcally to help with live performance),\n5This 70mswindow is taken directly from a popular beat tracking\nmetric [6].\n+Bach +X +T\nBar:\nBeat:\nSub beat:\nTemperley\nBar:\nBeat:\nSub beat:\nFigure 2 . The ﬁrst bar of the 1st prelude from WTC Book\nI (BWV 846). Above the music, the results from Temper-\nley’s model (bottom) are shown as well as the results from\nour +Bach +X +T model (top).\nbut augmented training data leads to a small but consis-\ntent increase in performance across all optimisation con-\nﬁgurations. On live performance, our model without opti-\nmisations underperforms Temperley’s, both with and with-\nout augmented training data. However, the optimisations\nlead to increased performance: our model using both opti-\nmisations matches Temperley’s performance with identical\ntraining data, and exceeds it by almost 9points with aug-\nmented training data. The effect of each optimisation is\ndiscussed in detail in Section 3.4.1.\nThe distribution of metrical F-measures for Temper-\nley’s model, run on live performance data, appears to be\nbinomial: of the 13 pieces, three score below 20, while\nsix score above 55, indicating that while the model per-\nforms well in general, it sometimes guesses a meter which\nis nearly entirely incorrect. With both optimisations, on\nthe other hand, our model’s scores are normally distributed\naround 65, with 8 pieces scoring between 55and75. Ad-\nditionally, no pieces score below 20, indicating that it is\nmore likely to make some partially correct guess, even if\nit is not entirely correct. The 1st prelude from WTC Book\nI illustrates this difference in performance, and its ﬁrst bar\nis shown in Figure 2 along with the results of Temperley’s\nmodel and our +Bach +X +T model. The piece is in44time,\nand Temperley’s model achieves a score of only 15:74,\nguessing a38time whose beats are even out of phase with\nthe ground truth sub beats throughout much of the piece.\nOn the other hand, our model scores 93:27, guessing a44\ntime which begins perfectly aligned, although it does shift\nslightly out of phase later in the piece.\nOne example of a piece for which Temperley’s model\noutperforms ours is the 2nd prelude of WTC Book II,\nthe ﬁrst bar of which is shown in Figure 3 along with\nthe results of Temperley’s model and our +Bach +X +T\nmodel. For this piece, Temperley’s model achieves a score\nof78:99while ours only manages a score of 61:83. This\npiece is in44time and contains relatively non-syncopatedProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 117+Bach +X +T\nBar:\nBeat:\nSub beat:\nTemperley\nBar:\nBeat:\nSub beat:\nFigure 3 . The ﬁrst bar of the 2nd prelude from WTC Book\nII (BWV 871), showing an example the nearly isochronous\nbars which give our model problems. Above the music,\nthe results from Temperley’s model (bottom) are shown as\nwell as the results from our +Bach +X +T model (top).\nrhythms, with many bars containing either only sixteenth\nnotes or only eighth notes in a given voice, as can be seen\nin the ﬁgure. While Temperley’s model captures this me-\nter correctly (with some phase errors), our model guesses\na44time which is early by a single beat. Our model has\nsome difﬁculty ﬁnding the correct phase of isochronous\nmelodies since it uses no pitch or harmonic information\n(which are the most salient indicators of metrical phase in\nsuch isochronous pieces). Temperley’s model, on the other\nhand, also includes chord detection, allowing it to better\nhandle such melodies.\n3.4.1 Optimisations\nAnother aspect of our model to investigate is the effect of\nthe different optimisations on its performance. As can be\nseen from Table 1, they (+X and +T) have little effect on\nmetronomic data (which is not surprising given that they\nare designed speciﬁcally for live performance). However,\non live performance data, they improve performance sig-\nniﬁcantly. Both with and without augmented training data,\nthe remove trills optimisation has a small effect by itself\n(essentially none without the data and very small with it),\nbut extending notes leads to a signiﬁcant improvement.\nThe combination of both optimisations improves perfor-\nmance even further, leading to peak performance both with\nand without augmented training data.\nOne speciﬁc example where the remove trills optimisa-\ntion leads to improvement with augmented training data is\non the 7th fugue from WTC Book I, where our +Bach and\n+Bach +T models achieve scores of 31:58and60:20re-\nspectively. There is a repeated trill throughout this piece,\nleading the +Bach model to lengthen its beat length such\nthat the trill is interpreted as 16th notes. With the remove\ntrills optimisation, however, our model is able to ﬁnd the\ncorrect metrical structure. Essentially, the remove trills op-\ntimisation frees our model from the constraint of trying to\nalign its tatums with each note in a trill or ornamentation.\nAn example of a piece for which the extend notes opti-misation makes an improvement is the 17th prelude from\nWTC Book I. In this piece, in34time, the lowest voice has a\nvery common repeated rhythm of an eighth note followed\nby two sixteenth notes followed by four more eighth notes,\nwhere the eighth notes are all played staccato. With the\noptimisation, our model correctly recognises the beat and\nsub beat levels, although it incorrectly guesses24time rather\nthan the correct34time, scoring 53:59. Without the op-\ntimisation, on the other hand, these eighth notes are not\nas salient, and the model instead guesses a22meter which\nmoves in and out of phase throughout the piece, achieving\na score of only 16:47. Throughout the corpus, the extend\nnotes optimisation helps ﬁnd strong notes whenever they\nare played staccato.\nThe combination of both optimisations improves over-\nall performance even further, enabling the model to han-\ndle both staccato passages and ornamentation. The im-\nprovements from both optimisations are seen in the fully\noptimised model, alongside other slight improvements\nthroughout the corpus such as ﬁxing the placement of a\nsingle misaligned beat here or there. For example, in the\npreviously discussed 17th prelude from WTC Book I, the\nfully optimised model achieves a metrical F-measure of\n60:35while no other model eclipses a score of 54, even\nthough the basic metrical alignment (a24meter) does not\nchange between the it and the +Bach +X model.\n4. CONCLUSION\nIn this paper, we have described a model for metrical struc-\nture detection and alignment from live performance MIDI.\nOur model is in the form of an HMM which performs\nmetrical structure detection and alignment given only a\nlist of note pitches and onset and offset times, and we\nhave shown that the model achieves state-of-the-art per-\nformance when evaluated on a corpus of metronomic data,\nas well as a second corpus of live performance data. The\nHMM incorporates a rhythmic grammar as one compo-\nnent, working with the grammar to align an input piece\nwith a metrical structure. This joint model is probabilis-\ntic and incremental, and requires no information a priori\nexcept for note onset and offset times. We have also pro-\nposed a new metric for the task, which takes into account\nvertical misalignments (for example, those which align the\nbeat level of a piece with bars) and partial correctness.\nIn future work, we would like to extend the evaluation\nof our model with more data. In particular, our corpus of\n13 pieces of live performance MIDI would beneﬁt from an\nexpansion, and allow us to perform a more in-depth analy-\nsis of the results.\nMetrical structure detection and alignment is clearly an\nimportant task for any complete transcription system, and\nwe have shown that our joint model is able to perform the\ntask well, even using only rhythmic data. Incorporating ad-\nditional information such as pitch or harmony should only\nlead to better performance. Speciﬁcally, it has been shown\nthat harmonic changes are most likely to occur at the begin-\nnings of bars [16], and low notes may be a salient feature\nof strong beats in addition to note duration [5].118 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20185. ACKNOWLEDGEMENTS\nThis work was partially funded by EU ERC H2020 Ad-\nvanced Fellowship GA 742137 SEMANTAX and a Google\nfaculty award.\n6. REFERENCES\n[1] Sebastian B ¨ock, Florian Krebs, and Gerhard Widmer.\nJoint beat and downbeat tracking with recurrent neural\nnetworks. In ISMIR , pages 255–261, 2016.\n[2] Judith C. Brown. Determination of the meter of mu-\nsical scores by autocorrelation. The Journal of the\nAcoustical Society of America , 94(4):1953, 1993.\n[3] A. T. Cemgil and B. Kappen. Monte carlo methods for\ntempo tracking and rhythm quantization. Journal of Ar-\ntiﬁcial Intelligence Research , 18:45–81, January 2003.\n[4] W. Bas De Haas and Anja V olk. Meter detection in\nsymbolic music using inner metric analysis. In ISMIR ,\npages 441–447, 2016.\n[5] Simon Dixon. Automatic extraction of tempo and beat\nfrom expressive performances. Journal of New Music\nResearch , 30(1):39–58, March 2001.\n[6] Simon Dixon. Evaluation of the audio beat track-\ning system beatroot. Journal of New Music Research ,\n36(1):39–50, March 2007.\n[7] Simon Durand, Juan P. Bello, Bertrand David, and\nGael Richard. Robust downbeat tracking using an en-\nsemble of convolutional networks. IEEE/ACM Trans-\nactions on Audio Speech and Language Processing ,\n25(1), 2017.\n[8] Douglas Eck and Norman Casagrande. Finding me-\nter in music using an autocorrelation phase matrix and\nshannon entropy. In ISMIR , pages 504–509, 2005.\n[9] Thanos Fouloulis, Aggelos Pikrakis, and Emilios Cam-\nbouropoulos. Traditional asymmetric rhythms: A re-\nﬁned model of meter induction based on asymmetric\nmeter templates. In Proceedings of the Third Interna-\ntional Workshop on Folk Music Analysis , pages 28–32,\n2013.\n[10] Mitsuyo Hashida, Toshie Matsui, and Haruhiro\nKatayose. A new music database describing deviation\ninformation of performance expressions. ISMIR , pages\n489–494, 2008.\n[11] Florian Krebs, Andre Holzapfel, A. Taylan Cemgil,\nand Gerhard Widmer. Inferring metrical structure in\nmusic using particle ﬁlters. 23(5):817–827, 2015.\n[12] Andrew McLeod and Mark Steedman. HMM-based\nvoice separation of MIDI performance. Journal of New\nMusic Research , 45(1):17–26, January 2016.[13] Andrew McLeod and Mark Steedman. Meter detection\nin symbolic music using a lexicalized PCFG. In Pro-\nceedings of the 14th Sound and Music Computing Con-\nference , pages 373–379, 2017.\n[14] Benoit Meudic. Automatic meter extraction from MIDI\nﬁles. In Journ ´ees d’informatique musicale , 2002.\n[15] Eita Nakamura, Kazuyoshi Yoshii, and Shigeki\nSagayama. Rhythm Transcription of Polyphonic Pi-\nano Music Based on Merged-Output HMM for Multi-\nple V oices. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing , 25(4):794–806, April 2017.\n[16] H ´el`ene Papadopoulos and Geoffroy Peeters. Joint esti-\nmation of chords and downbeats from an audio signal.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 19(1):138–152, January 2011.\n[17] Christopher Raphael. Automated rhythm transcription.\nInISMIR , 2001.\n[18] David Temperley. An evaluation system for metri-\ncal models. Computer Music Journal , 28(3):28–44,\nSeptember 2004.\n[19] David Temperley. Music and Probability . The MIT\nPress, 2007.\n[20] David Temperley. A uniﬁed probabilistic model for\npolyphonic music analysis. Journal of New Music Re-\nsearch , 38(1):3–18, March 2009.\n[21] Kim Thomas. Just noticeable difference and tempo\nchange. Journal of Scientiﬁc Psychology , pages 14–20,\n2007.\n[22] Nick Whiteley, A. Taylan Cemgil, and Simon Godsill.\nBayesian modelling of temporal structure in musical\naudio. In ISMIR , 2006.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 119"
    },
    {
        "title": "StructureNet: Inducing Structure in Generated Melodies.",
        "author": [
            "Gabriele Medeot",
            "Srikanth Cherla",
            "Katerina Kosta",
            "Matt McVicar",
            "Samer Abdallah",
            "Marco Selvi",
            "Ed Newton-Rex",
            "Kevin Webster"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492519",
        "url": "https://doi.org/10.5281/zenodo.1492519",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/126_Paper.pdf",
        "abstract": "We present the StructureNet - a recurrent neural network for inducing structure in machine-generated compositions. This model resides in a musical structure space and works in tandem with a probabilistic music generation model as a modifying agent. It favourably biases the probabilities of those notes that result in the occurrence of structural elements it has learnt from a dataset. It is extremely flexible in that it is able to work with any such probabilistic model, it works well when training data is limited, and the types of structure it can be made to induce are highly customisable. We demonstrate through our experiments on a subset of the Nottingham dataset that melodies generated by a recurrent neural network based melody model are indeed more structured in the presence of the StructureNet.",
        "zenodo_id": 1492519,
        "dblp_key": "conf/ismir/MedeotCKMASNW18",
        "keywords": [
            "recurrent neural network",
            "inducing structure",
            "machine-generated compositions",
            "musical structure space",
            "probabilistic music generation model",
            "biasing probabilities",
            "limited training data",
            "customisable structure",
            "melodies generated",
            "presence of StructureNet"
        ],
        "content": "StructureNet: INDUCING STRUCTURE IN GENERATED MELODIES\nGabriele Medeot1Srikanth Cherla1Katerina Kosta1Matt McVicar1\nSamer Abdallah1Marco Selvi1Ed Newton-Rex1Kevin Webster2\n1Jukedeck Ltd., London, United Kingdom\n2Imperial College London, London, United Kingdom\nfgabriele, srikanth, katerina, matt, samer, marco, ed g@jukedeck.com\nkevin.webster@imperial.ac.uk\nABSTRACT\nWe present the StructureNet - a recurrent neural network\nfor inducing structure in machine-generated compositions.\nThis model resides in a musical structure space and works\nin tandem with a probabilistic music generation model as\na modifying agent. It favourably biases the probabilities of\nthose notes that result in the occurrence of structural ele-\nments it has learnt from a dataset. It is extremely ﬂexible\nin that it is able to work with any such probabilistic model,\nit works well when training data is limited, and the types\nof structure it can be made to induce are highly customis-\nable. We demonstrate through our experiments on a sub-\nset of the Nottingham dataset that melodies generated by a\nrecurrent neural network based melody model are indeed\nmore structured in the presence of the StructureNet.\n1. INTRODUCTION\nAutomated generation of symbolic music using comput-\ners involves the application of computer algorithms to the\ncreation of novel musical scores. The natural predisposi-\ntion of computers to quickly enumerate and choose from\na large set of compositional alternatives makes them suit-\nable candidates for discovering novelty in the vast space\nof musical possibilities that could be daunting to a human\ncomposer. Leveraging computing power for this purpose\nhas the potential to aid and accelerate the creative process,\nthus lowering the bar for composition and democratising\nit. So-called machine-generated music has been a subject\nof steady interest since the pioneering work of a few mu-\nsically inclined information theorists [5, 8]. This interest\nhas surged during the past decade or so within academia\nand especially outside it with the rise of certain industry\nplayers (such as Jukedeck1and the Magenta project2).\n1https://www.jukedeck.com/\n2https://magenta.tensorflow.org/\nc\rGabriele Medeot, Srikanth Cherla, Katerina Kosta, Matt\nMcVicar, Samer Abdallah, Marco Selvi, Ed Newton-Rex, Kevin Webster.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Gabriele Medeot, Srikanth Cherla, Ka-\nterina Kosta, Matt McVicar, Samer Abdallah, Marco Selvi, Ed Newton-\nRex, Kevin Webster. “StructureNet: Inducing Structure in Generated\nMelodies”, 19th International Society for Music Information Retrieval\nConference, Paris, France, 2018.The roughly seven decade-long history of machine-\ngenerated symbolic music has seen the application of a\nplethora of algorithms to varying degrees of success [8].\nWith the increasing digitisation of musical scores, those\nrelying on machine learning have gained importance in re-\ncent times. The relatively successful approaches among\nthese have been Probabilistic Grammars [9], (Hidden)\nMarkov models [19, 21], and Connectionist architectures\n[2,18]. The latter in particular have proven to be highly ef-\nfective at representing musical information and modelling\nlong-term dependencies which are crucial to generating\ngood-quality music [3].\nThis paper addresses the issue of long-term structure in\nmachine-generated symbolic monophonic music. Struc-\nture is a key aspect of music composed by humans that\nplays a crucial role in giving a piece of music a sense of\noverall coherence and intentionality. It appears in a piece\nas a collection of musical patterns, variations of these pat-\nterns, literal or motivic repeats and transformations of sec-\ntions of music that have occurred earlier in the same piece.\nHampshire underlines that a piece can be conceived as a\nwork of art if and only if the listener’s mind is actively\ntracing the structure of the work using her own natural im-\nagery and musical memory [7, p. 16].\nHere we introduce StructureNet - a recurrent neu-\nral network that induces structure in machine-generated\nmelodies. It learns about structure from a dataset consist-\ning of structural elements and their occurrence statistics,\nwhich is created using a structure-tagging algorithm from\nan existing dataset of melodies. Once trained, StructureNet\nworks in tandem with a melody model which generates a\nprobability distribution over a set of musical notes. Given\nthe melody model’s prediction at any given time during\ngeneration, StructureNet uses the structural elements im-\nplied by the melody so far to alter the prediction, leading\nto a more structured melody in the future. Our experiments\nreveal that music generated with StructureNet contains sig-\nniﬁcantly better structure, even when it is trained on a rel-\natively small dataset. We provide musical examples that\nhighlight this fact.\nThe next section introduces relevant state-of-the-art.\nSome preliminaries and a description of StructureNet fol-\nlow in Sections 3 and 4 respectively. Based on the results\npresented in Section 5, we summarise our ﬁndings and sug-\ngest potential future work in Section 6.7252. RELATED WORK\nIn order to repeat verbatim or with variations sections that\nhave occurred previously in a piece of machine-generated\nmusic, i.e. to induce structure in it, the model must be able\nto encode and recall in some way what has happened in the\npast. This can be achieved in a variety of ways. In a ﬁrst\ninstance, improving structure simply involves making the\ngeneration model more powerful. An example of this is the\nRNN-RBM [2] that was enhanced purely by replacing its\ncomponents - the Recurrent Neural Network (RNN) by a\nLong-Short Term Memory (LSTM) Network to improve its\ntemporal memory, making it the LSTM-RTRBM [16], and\nthe Restricted Boltzmann Machine (RBM) by a Deep Be-\nlief Network (DBN) to improve its output layer, making it\nthe RNN-DBN [10]. Similarly, it was demonstrated in [4]\nthat connectionist models outperform Markov models in\nmodelling melodic sequences. Closely related to these is a\nmusically informed improvement that enriches the feature\nencoding to include those features that have the potential to\nadd more information about structure [6, 19]. Along simi-\nlar lines, the Magenta Project proposed two neural network\narchitectures to model higher-level structure in music - the\nLookback RNN and Attention RNN [25]. While the for-\nmer augments the model’s feature vector with information\nabout notes from previous measures, repeat information\nand metrical location, the latter adopts an attention-based\nmechanism [1] wherein a weighted sum of the model’s out-\nputs in the previous nlocations is used in addition to its\ncurrent state to make better predictions. Such approaches\naddress the overall quality of music, of which high-level\nstructure is just one aspect. Moreover, the improvements\nafforded by the former kind are highly dependent on the\ntraining loss, which does not explicitly take into account\nstructure of the kind observed in music. So while an im-\nprovement in the model or feature representation does tend\nto improve the overall quality of music in a piece, improve-\nment is often observed over short time-spans and not nec-\nessarily in the higher-level structure.\nAlternatively, one can explicitly addresses the issue of\nhigh-level structure in machine-generated compositions.\nOne simple solution involves dividing the generation task\nbetween multiple models. The MELONET system [13],\nwhose goal is to produce variations of a given melodic\ntheme, achieves structural coherence by dividing the ef-\nfort between two mutually interacting neural networks op-\nerating at different time-scales. The ﬁrst network learns\nto recognise musical structure while the second network\npredicts the musical notes. Similarly, Todd [24] proposed\ntwo cascaded networks that allow the explicit representa-\ntion of structure in a hierarchy. The ﬁrst network gener-\nates a sequence of plans which correspond to descriptions\nof melodic chunks, and the second a sequence of notes\ngiven a plan. More recently, Roig et al. [22] devised a\nsystem in which melodic and rhythmic patterns existing\nin the dataset are concatenated according to statistically\ngoverned rules to form new patterns that are not too dis-\ntant from those occurring in the dataset. In the system\nknown as MorpheuS [11] music generation is formulatedas a combinatorial optimisation problem in which a tem-\nplate of musical structure acts as a hard-constraint, and\nsolved using a meta-heuristic search algorithm known as\nVariable Neighbourhood Search. Patterns contained in the\ndataset of pieces are discovered using an existing pattern-\ndetection algorithm [17]. In a similar vein, [20] control the\ngeneration of chord sequences and melodies using steer-\nable constraints Markov chains. Lattner et al. [14] adopt a\nsimilar approach where a Convolutional Restricted Boltz-\nmann Machine is combined with a constraint optimisation\ntechnique to constrain the music sampled from the C-RBM\naccording to the musical structure of a given template.\n3. BACKGROUND\nStructureNet is a Recurrent Neural Network (RNN) that\noperates in the space of musical structure and learns se-\nquences of features that denote the presence or absence\nof repeats at a point in time and their type, if present.\nHere we give an overview of the Long Short-Term Mem-\nory (LSTM) RNN that underlies StructureNet and the def-\ninition of structural repeats that we rely on.\n3.1 Long Short-Term Memory\nThe RNN is a type of neural network for modelling se-\nquences and its basic architecture consists of an input layer,\na hidden layer and an output layer. The state of its hidden\nlayer acts as a memory of the past information it encoun-\nters while traversing a sequence. At each location in the se-\nquence, the RNN makes use of both the input and the state\nof its hidden layer from the previous location to predict an\noutput. Here we use a special case of the RNN known as\nthe Long-Short Term Memory (LSTM) network [12] that,\nowing to the presence of purpose-built memory cells to\naugment its hidden layer, boasts a greater temporal mem-\nory than the standard RNN. Given an input vector xtat\nsequence location t, the output of the LSTM ht\u00001and its\nmemory cell ct\u00001(collectively, its state) from the previous\nlocation, the output of the LSTM layer htis computed and\nfurther propagated into another layer of a larger model.\n3.2 Modelling Melodies and Structure Elements\nThe output layer of the note-based (as opposed to frame-\nbased) melody model in the present work contains two\ngroups of softmax units. Each group of softmax units mod-\nels a single probability distribution over a set of mutually\nexclusive possibilities. The ﬁrst of these denotes the musi-\ncal pitch of the note, and the second its duration. Given the\noutput of the LSTM layer htat any given location tin the\nsequence, this is transformed into two independent proba-\nbility distributions ptand dtthat together make up the out-\nput layer of the network. From these two distributions, the\nprobability of a certain note (with pitch and duration) can\nbe obtained simply by multiplying the probabilities of its\ncorresponding pitch and duration respectively. Note that\nthe output layer of StructureNet contains three groups of\nsoftmax units. Although these represent different quanti-\nties that deﬁne aspects of structure (explained in detail in726 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 1 . A 16-measure melody generated by our LSTM\nmelody model together with the StructureNet. A selection\nof repeats in this melody are as follows: measures 9-12 are\na duration-interval repeat of measures 5-8, as are measures\n13-14 of measures 9-10; and measures 15 and 16 are both\nduration repeats of measure 12.\nSections 4.1 and 4.2), the manner in which these are com-\nbined to generate the probabilities of structural elements is\nidentical to the melody model. Also note that the choice of\nthe LSTM as the melody model is arbitrary and it can be\nreplaced by any other probabilistic prediction model.\n3.3 A Deﬁnition of Structure\nThere are various types of structure present in music.\nComposers use techniques such as instrumental variation,\nchanges and repeats in timbre, and dynamics to induce a\nfeeling of familiarity in the listener. In the present work,\nhowever, we focus on the score-level repeat information.\nIn a score, perhaps the two most obvious types of repeat\nare of (1) duration (rhythmic), and (2) pitches (melodic).\nA duration repeat is a section of the melody, the du-\nrations of whose notes are the same as those of a previ-\nous section. Examples of duration repeats can be found in\nthe melody of Figure 1. These are determined purely by\nthe sequences of crotchets and quavers contained in these\nmeasures. When it comes to pitch, it is helpful to think\nof these repeats in terms of intervals rather than absolute\npitch . The interval between two notes can be deﬁned in a\nnumber of ways, but in this work we use the scale degree\ndistance between notes. For instance, in the key of C ma-\njor, the scale degree between a C note and subsequent E\nnote would be the same as the scale degree between a D\nnote and subsequent F note. Given this deﬁnition of an in-\nterval, a duration-interval repeat is a section of the melody\nthat holds the same relationship to a previous section as\na duration repeat, and additionally the intervals between\nwhose consecutive notes are the same as those between the\nconsecutive notes of that previous section. Figure 1 also\nillustrates duration-interval repeats. In the present work,\nwe consider duration repeats as well as repeats of both du-\nrations and intervals. Purely interval repeats were found to\nbe very few in our chosen dataset and were thus ignored.\n4. StructureNet\nStructureNet is only able to produce structural repeat in-\nformation that biases the predictions of an accompanying\nmusic (in the present case melody) model. In Section 4.2we will outline a methodology whereby it modiﬁes the\nprobability of notes that the melody model produces, thus\nencouraging structure but not enforcing it. Crucially, this\nmeans that the structure network is able to suggest repeats\nof certain types, but if the melody network assigns very\nlow probability to notes that would form these repeats, it\nis free to “override” the structure network’s suggestions in\na probabilistic and ﬂexible manner. The speciﬁcs of how\nStructureNet achieves these goals is outlined in the remain-\nder of this section, beginning with the type of structure we\ncapture and how we identify it.\n4.1 A Dataset of Structure\nStructureNet operates in a space of musical structure. In\norder to train the model, we ﬁrst create this structure\ndataset by processing a dataset of melodies with a musical\nrepeat-detection algorithm. The algorithm encodes each\nmelody into a sequence of binary feature vectors in the\nsemi-quaver temporal resolution (although this resolution\nis not a strict requirement: if the dataset contains no notes\nshorter than a quaver, one may use a quaver as the minimal\nresolution). The feature vector itself is a concatenation of\nthree one-hot sub-vectors. The ﬁrst is given by\n[f;d;ditr;dint]\nwherein each bit of the ﬁrst sub-vector indicates which of\nfour categories a given frame of music belongs to. These\nare (1)f- free music, (2) d- duration repeat, (3) ditr-\nduration-interval repeat with transposition and (4) dint-\nduration-interval repeat without transposition. The only\ndistinction between the two types of duration-interval re-\npeats is that in the case of ditrthe section to which the\nframe belongs is a transposed version of the original sec-\ntion whereas in the case of dintthe section to which the\nframe belongs is at the same musical pitch as the original\nsection. The free music bit findicates that the frame is a\npart of a section that is not a repeat of any previous section\nof the melody. The second one-hot sub-vector is given by\n[f;l0:5;l0:75;l1:0;l1:5;l2:0;l3:0;l4:0;l8:0;l16:0]\nand contains bits that indicate the lookback , i.e. the dis-\ntance (in crotchets) between the original section and the\nsection containing the current frame, if the section con-\ntaining the current frame is a repeat of the original sec-\ntion. If it is not a repeat, the free music bit fis on.\nNote that the value of the free music bits in both sub-\nvectors is identical and hence uses the same notation. Also\nnote that the choice of the set of lookbacks is completely\nopen to change, and highlights another ﬂexible aspect of\nthe model; it may even be possible to learn the optimal\nset of lookbacks for a given dataset. Finally, the third 8-\ndimensional one-hot vector \u001eencodes the location of a\nframe via its beat strength \fand its measure strength \u001a.\nThe beat strength [15] encodes the strength of each met-\nrical location in a measure. In a measure divided into 16\nsemi-quaver beats (as in the present work), its values are\n\f= [0;4;3;4;2;4;3;4;1;4;3;4;2;4;3;4]. The measureProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 727strength extends the notion of beat strength to a sequence\nof measures. The strengths associated with beats in a mea-\nsure are associated with measures in a piece, beginning\nwith the ﬁrst measure. In the present work, we choose a\ncycle of 8measures that correspond to the following se-\nquence of measure strengths \u001a= [0;3;2;3;1;3;2;3]. In\nboth cases, a lower value indicates a higher strength. Our\nencoding is deﬁned as\n\u001et=(\n\u001a(mod(t;8)) if mod( t;16) = 0\nmax(\u001a) +\f(mod(t;16))otherwise\n(1)\nNote that just as the beat strength, the measure cycle dura-\ntion for the measure length can also be varied as desired.\nStructureNet models the vector that is a concatenation\nof these three sub-vectors as three groups of softmax units\nin its output layer. As noted earlier in Section 3.2, the\nmanner in which one combines the probability distribu-\ntions represented by these two groups of softmax units (for\ninstance, a duration-interval repeat of lookback 8:0, or an\ninterval repeat of lookback 1:5) is by multiplying the cor-\nresponding probabilities one from each group.\nThe repeat-detection algorithm works by ﬁrst convert-\ning a sequence of notes into two strings - one correspond-\ning to durations and the other to intervals. In each of these\nstrings, it then uses a string matching algorithm to ﬁnd\nsubstrings that repeat. Single-note repeats are trivial and\nthus discarded, and only those repeats that correspond to\nthe above listed lookbacks are retained. Any note that is\nlonger than 2 measures is split into multiple notes of the\nsame pitch to limit the number of characters required to\nrepresent the piece as a string. Then the list of duration re-\npeats are ﬁltered such that only the longest repeats remain\nand all overlapping and shorter repeats are discarded. At\nthis stage, the duration-interval repeats are nothing but du-\nration repeats with coinciding interval repeats. So from the\nlist of interval repeats only those are retained that coincide\nexactly with the current list of duration repeats with the\nsame lookbacks. These are tagged as duration-interval re-\npeats, replacing the corresponding duration repeats to give\nthe ﬁnal list of duration repeats and duration-interval re-\npeats. While it is indeed possible to look for other types\nof repeats, we limit ourselves in this paper to the above as\nit is sufﬁcient to demonstrate the efﬁcacy of StructureNet.\nThis also highlights the ﬂexibility of the model wherein\none may change the type of repeats detected and also cus-\ntomise the number of lookbacks as needed.\n4.2 Inﬂuencing Event Probabilities\nOnce trained on the above described structure dataset,\nStructureNet is then put to use with the probabilistic\nmelody prediction model Mm. At timet(that is, given\nthe history of notes generated up to time t), the modelMm\npredicts a probability distribution Ptover a set of notes N.\nAt the same time, given the history of repeats generated so\nfar, the structure model Mspredicts a probability distribu-\ntionQtover a set of possible repeats \u0005, which includes an\nelement\u0019f, representing ‘free music’. Each note \u00172Ncan be consistent with a subset \u0005\u0017\ntof these repeats, which\nwill always include \u0019f, meaning that every note is consis-\ntent with ‘free music’.\nStructureNet inﬂuences the prediction Ptby modifying\nthe probability of each note according to the probabilities\nof the repeats with which it is consistent. Let \u001et:N\u0002\u0005!\nf0;1gbe a function such that \u001et(\u0017;\u0019) = 1 when note\u0017is\nconsistent with repeat \u0019at timetand0otherwise. In terms\nof this we can express \u0005\u0017\ntasf\u00192\u0005j\u001et(\u0017;\u0019) = 1g, and\nfurther deﬁne N\u0019\nt=f\u00172Nj\u001et(\u0017;\u0019) = 1g, which is the\nset of notes consistent with \u0019. Each note\u0017is then assigned\na weight\nWt(\u0017) =Pt(\u0017)X\n\u00192\u0005\u0017\ntQt(\u0019)\n\u0016\u0019\nt; (2)\nwhere\u0016\u0019\nt=P\n\u00172N\u0019\ntPt(\u0017). In this way, the relative prob-\nability of a note \u0017is increased when it is consistent with\nrepeat(s) to whichMshas assigned high probability.\nIt is important to note that MmandMsoperate at\ndifferent temporal resolutions—note-level and semiquaver\nframe-level respectively—and that this difference becomes\nsigniﬁcant here. Suppose note \u0017is of duration \u0001\u0017=\u001c\u0017\u000e,\nwhere\u000eis the frame duration and \u001c\u0017is the number of\nframes occupied by \u0017. Ideally, in order to get an accurate\nestimate of the joint probability of the note \u0017and the repeat\n\u0019, one should consider the probability that Msassigns to\n\u001c\u0017consecutive frames of\u0019. This would be expressed as\nWt(\u0017) =Pt(\u0017)X\n\u00192\u0005\u0017\nt\u001c\u00001Y\nk=0Qt+k(\u0019)\n\u0016\u0019\nt+k: (3)\nHowever, we found in our experiments that the single-step\napproximation (2) works well in practice and is less com-\nputationally intensive than (3).\nNext, the weight distribution Wtis normalised to obtain\na probability distribution Rt:\nRt(\u0017) =Wt(\u0017)P\n\u00172NWt(\u0017): (4)\nWe may now sample a note \u0017tfrom this distribution and\nupdate the internal state of the melodic model Mmwith\nthis observation.\nIt remains to update the state of the structure model Ms\nwith some observed repeat. The note \u0017tsampled at time t\ncould be associated with any of the repeats that were con-\nsistent with it. We choose one by sampling \u0019tfrom a dis-\ntributionStover\u0005\u0017t\ntdeﬁned as\nSt(\u0019) =Qt(\u0019)P\n\u001902\u0005\u0017t\ntQt(\u00190): (5)\nAt this point the two models are misaligned due to the dif-\nferent time-scales they operate in, with Mmbeing\u001csemi-\nquaver frames ahead of Ms. Since each update of the state\nofMstakes it ahead by just one semi-quaver frame, it is\nnecessary to update Ms\u001ctimes repeatedly with the same\nstructure vector so that it is once again aligned with Mm.\nAt the end of the process described above, we have a\nmelody note sampled from our melody model that has been728 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018inﬂuenced by StructureNet. StructureNet has also updated\nits own state according to the sampled note and is ready to\ninﬂuence the choice of next note.\n5. EXPERIMENTS\nWe demonstrate the efﬁcacy of StructureNet on a well-\nknown dataset of melodies by comparing statistics over\nseveral musical quantities computed both on the dataset\nand compositions generated by the melody model alone\nand the melody and structure models combined. The re-\nsults show that the presence of StructureNet leads to music\nthat is more structured and closer in the statistics to the\ndataset. We also share the generated music to allow the\nreader to her or himself be the judge of our claims.\n5.1 Dataset\nWe evaluate StructureNet on the cleaned Nottingham folk\nmelody dataset that was released by the Jukedeck Research\nTeam [23]. This publicly available dataset facilitates repro-\nducibility. We carry out our experiments on the subset of\n450 4=4time-signature pieces out of the 1;548contained\nin it. Each piece of the dataset was truncated to its ﬁrst\n16measures and transposed into the Key of C, and all up-\nbeats at the beginning of each piece were removed prior to\ntraining. We used 20% (90 segments) of the data as the val-\nidation and the rest (360 segments) for training the models.\nStructureNet is also trained on the same dataset following\nthe application of the repeat-tagging algorithm. However,\none must note that this is not a requirement and a different\ndataset may be used for learning structure and could poten-\ntially lead to interesting results. StructureNet successfully\ninduces structure in the generated melodies despite the few\nexamples contained in the training data.\n5.2 Training methodology\nAs mentioned earlier, both the structure network and the\nmelody network are LSTMs and contain a single hidden\nlayer. A Bayesian Optimisation based method was em-\nployed to carry out model selection. In the case of the\nmelody model, the single best outcome of the grid search\nwas used. As for StructureNet, ten models with the same\nbest set of hyperparameters as determined by the model\nselection step, and different initial conditions, were trained\nand used in tandem with the melody model. This was done\nin order to be able to compute conﬁdence intervals in the\nﬁgures. The hidden layer size of each network was varied\nbetween 50and1000 in steps of 50during model selection,\nwhich led to ns\nhid= 950 in the former and nm\nhid= 250 in\nthe latter. Early stopping was used as a regulariser, such\nthat the training was stopped and the best models thus far\nretrieved after no improvement in the validation cost for\n25epochs. The models were trained using the ADAM op-\ntimiser with an initial learning rate \u0011init= 0:001, and pa-\nrameters\f1= 0:9,\f2= 0:999and\u000f= 10\u00008.5.3 Evaluation\nOur hypothesis is two-fold: (1) that repeat-related statis-\ntics computed over the melodies generated with Struc-\ntureNet are closer to those over the dataset melodies than\nthose over melodies generated without StructureNet, and\n(2) that non repeat-related statistics do not differ between\nthe melodies generated by the melody model with and\nwithout StructureNet. This would demonstrate that the\nuse of StructureNet leads to more structured melodies than\nare generated by the melody model on its own, and that\nare musically at least as similar to the original data as the\nmelody model alone achieves. The statistics are:\n1.Repeat Count: Number of repeats corresponding to\nvarious lookback values (in crotchets).\n2.Repeat Duration: Number of repeats of various du-\nrations (in crotchets).\n3.Repeat Onsets: Number of repeats beginning at\nvarious locations (in crotchets) in a piece.\n4.Pitch, start time and duration distributions: Oc-\ncurrence statistics of pitches, start times in measure,\nand durations.\nThe ﬁrst three are repeat-related statistics and the rest\nare not. A histogram of each is ﬁrst computed per col-\nlection of melodies (dataset, generations with and with-\nout StructureNet), and then normalised by the count of\nmelodies in the collection to generate a probability distri-\nbution (as the counts vary between the different collections\nof melodies). The KL-Divergences (KLD) \u0014data;SN and\n\u0014data;NoSN between the distribution pairs (dataset, Struc-\ntureNet) and (dataset, No StructureNet) respectively high-\nlight the effect of introducing StructureNet (Table 1). Ide-\nally, among the structure-related distributions, we would\nwant\u0014data;SN<\u0014data;NoSN . And among the non-repeat-\nrelated distributions, we wish for \u0014data;SN\u0014\u0014data;NoSN .\n\u0014data;NoSN \u0014data;SN\nRepeat Count (D) 0.0356 \u00060.0022 0.0069\u00060.0043\nRepeat Duration (D) 0.1071 \u00060.0047 0.0389\u00060.0168\nRepeat Onset (D) 0.0844 \u00060.0038 0.0357\u00060.0094\nRepeat Count (DI) 0.0511 \u00060.0049 0.0173\u00060.0095\nRepeat Duration (DI) 0.2402 \u00060.0069 0.0634\u00060.0352\nRepeat Onset (DI) 0.1209 \u00060.0073 0.0639\u00060.0194\nRepeat Count (all) 0.0483 \u00060.0035 0.0083\u00060.0045\nRepeat Duration (all) 0.0996 \u00060.0033 0.025\u00060.0081\nRepeat Onset (all) 0.0875 \u00060.0036 0.031\u00060.0103\nPitch 0.0079 \u00060.0011 0.0061\u00060.0012\nDuration 0.0049 \u00060.0016 0.0042\u00060.0014\nOnset 0.058 \u00060.0081 0.0275\u00060.0082\nTable 1 . KL-divergences between the training data and\nmelodies generated with and without StructureNet (com-\nputed over 10sets of 450 melodies generated with each\ntrained StructureNet) for the Duration (D), Duration-\nInterval (DI) and all repeat types.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 7295.4 Observations\nIn Table 1, the KLD values show a greater match between\nthe dataset and the set of generated melodies in the pres-\nence of StructureNet than in its absence. This holds true\nfor both duration and duration-interval repeats. Figure 2\nillustrates such similarities (over all repeat types) visually.\nOne will see here that overall StructureNet (a) is conducive\nto the creation of longer repeats while generally having a\npositive effect on shorter ones as well, (b) is conducive to\nthe creation of repeats that have lookback values similar to\nthose in the dataset, particularly larger lookbacks (encour-\naging distant repeats), and (c) encourages repeats to begin\non those metrical locations in a generated piece where they\ntend to occur in the dataset.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\nDuration (in crotchets)0.000.050.100.150.200.250.300.35 Count (normalised)(a) Duration of all patterns\nNo StructureNet\nWith StructureNet\nDataset\n0.5\n0.75\n1.0\n1.5\n2.0\n3.0\n4.0\n8.0\n16.0Pattern lookback0.000.050.100.150.200.250.300.350.40 Count (normalised)(b) Lookbacks for all patterns\nNo StructureNet\nWith StructureNet\nDataset\n1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61\nStart onset of pattern in piece (in crochets)No StructureNet\nDataset\nWith StructureNet\n0.00.20.40.60.81.0\n(c) Repeat starts for all patterns\nFigure 2 . Repeat-related statistics of the dataset to the two\ngeneration modes (with/without StructureNet).\nIt is also evident from the set of three non-repeat-related\nstatistics of Figure 3 that the presence of StructureNet has,\nmore often than not, led to a better match of the gener-\nated melody statistics to the dataset. This is also sup-\nported by the very similar KLD values (often lower in the\n\u0014data;NoSN column) for these three musical quantities at\nthe bottom of Table 1. And ﬁnally, each plot in Figure 4\nshows the percentage of generated melodies with various\ndegrees of free music in them. The three plots together re-\nveal that using StructureNet reduces the proportion of free\nmusic (and thus increases the proportion of repeats) in the\ngenerated melodies in a way that more closely matches the\nproportions of free music and repeats in the dataset. Note\nthat the statistics in Figures 2, 3 and 4 have been computed\nover the same number of melodies (of the same duration\nin measures). We have made a representative subset of\nmelodies generated with and without StructureNet in the\nMIDI format available for scrutiny3.\n3https://goo.gl/hL9RhZ\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\nrestMIDI note0.000.030.050.080.100.120.150.18 Count (normalised)Note labels\nNo StructureNet\nWith StructureNet\nDataset\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n20\nDuration (in semiquavers)0.00.10.20.30.40.50.6 Count (normalised)Note durations\nNo StructureNet\nWith StructureNet\nDataset\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nStart time in the measure (in semiquavers)0.000.030.050.080.100.120.150.180.20 Count (normalised)Start time of a note relevant to that of the measure\nNo StructureNet\nWith StructureNet\nDatasetFigure 3 . Non repeat-related statistics of the dataset to the\ntwo generation modes (with/without StructureNet).\n20.0% 40.0% 60.0% 80.0%0.0%10.0%20.0%PiecesWith StructureNet - Percentage of time free music\n20.0% 40.0% 60.0% 80.0%0.0%10.0%20.0%PiecesNo StructureNet - Percentage of time free music\n20.0% 40.0% 60.0% 80.0%\nFree Music0.0%10.0%20.0%PiecesDataset - Percentage of time free music\nFigure 4 . The amount of generated melodies with various\ndegrees of free music in them.\n6. CONCLUSIONS & FUTURE WORK\nWe introduced StructureNet - an RNN that inﬂuences the\npredictions of a melody model so as to give the gener-\nated melodies greater structure. We demonstrated using\nstatistics, as well as several musical examples, that this\nmodel does indeed increase the probability of encounter-\ning longer and more distant (greater lookback) patterns in\nmusic generated by a melody model. Given these initially\nsuccessful results, we foresee some interesting directions\nfor future work. Firstly, we are interested in experimenting\nwith a more evolved pattern detection algorithm such as\nSIATEC and COSIATEC [17]. This will lead to new fea-\nture representations over and beyond just repeats that can\nperhaps provide a better insight into musical structure to\nStructureNet. We would like to expand the three musical\nquantities introduced in Section 5.3 into a more compre-\nhensive set of quantities that can lead to a more thorough\nevaluation of musical structure.730 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20187. REFERENCES\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. Neural machine translation by jointly learning to\nalign and translate. arXiv preprint arXiv:1409.0473 ,\n2014.\n[2] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Modeling temporal dependencies in\nhigh-dimensional sequences: application to poly-\nphonic music generation and transcription. In Intl.\nConf. on Machine Learning , pages 1881–1888. Om-\nnipress, 2012.\n[3] Jean-Pierre Briot, Ga ¨etan Hadjeres, and Franc ¸ois Pa-\nchet. Deep learning techniques for music generation-a\nsurvey. arXiv preprint arXiv:1709.01620 , 2017.\n[4] Srikanth Cherla, Son N Tran, Artur d’Avila Garcez,\nand Tillman Weyde. Discriminative learning and infer-\nence in the recurrent temporal rbm for melody mod-\nelling. In Intl. Joint Conf. on Neural Networks , pages\n1–8. IEEE, 2015.\n[5] Joel E Cohen. Information theory and music. Systems\nResearch and Behavioral Science , 7(2):137–163, 1962.\n[6] Darrell Conklin and Ian H Witten. Multiple viewpoint\nsystems for music prediction. Journal of New Music\nResearch , 24(1):51–73, 1995.\n[7] Nicholas Cook. Music, imagination, and culture . Ox-\nford University Press, 1992.\n[8] Jose David Fernndez and Francisco Vico. AI methods\nin algorithmic composition: A comprehensive survey.\nJournal of Artiﬁcial Intelligence Research , 48:513–\n582, 2013.\n[9] Jon Gillick, Kevin Tang, and Robert M Keller. Machine\nlearning of jazz grammars. Computer Music Journal ,\n34(3):56–66, 2010.\n[10] Kratarth Goel, Raunaq V ohra, and JK Sahoo. Poly-\nphonic music generation by modeling temporal depen-\ndencies using a rnn-dbn. In Intl. Conf. on Artiﬁcial\nNeural Networks , pages 217–224. Springer, 2014.\n[11] Dorien Herremans and Elaine Chew. Morpheus: gen-\nerating structured music with constrained patterns and\ntension. IEEE Trans. on Affective Computing , 2017.\n[12] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-\nterm memory. Neural computation , 9(8):1735–1780,\n1997.\n[13] Dominik H ¨ornel and Wolfram Menzel. Learning musi-\ncal structure and style with neural networks. Computer\nMusic Journal , 22(4):44–62, 1998.\n[14] Stefan Lattner, Maarten Grachten, and Gerhard\nWidmer. Imposing higher-level structure in poly-\nphonic music generation using convolutional restricted\nboltzmann machines and constraints. arXiv preprint\narXiv:1612.04742 , 2016.[15] Fred Lerdahl and Ray S Jackendoff. A generative the-\nory of tonal music . MIT press, 1985.\n[16] Qi Lyu, Zhiyong Wu, and Jun Zhu. Polyphonic music\nmodelling with lstm-rtrbm. In Proceedings of the 23rd\nACM international conference on Multimedia , pages\n991–994. ACM, 2015.\n[17] David Meredith. Cosiatec and siateccompress: Pattern\ndiscovery by geometric compression. In Intl. Society\nfor Music Information Retrieval Conf. Intl. Society for\nMusic Information Retrieval, 2013.\n[18] Michael C Mozer. Connectionist music composition\nbased on melodic, stylistic and psychophysical con-\nstraints. Music and connectionism , pages 195–211,\n1991.\n[19] Francois Pachet. The continuator: Musical interaction\nwith style. Journal of New Music Research , 32(3):333–\n341, 2003.\n[20] Franc ¸ois Pachet and Pierre Roy. Markov constraints:\nsteerable generation of markov sequences. Constraints ,\n16(2):148–172, 2011.\n[21] Jean-Francois Paiement, Yves Grandvalet, and Samy\nBengio. Predictive models for music. Connection Sci-\nence, 21(2-3):253–272, 2009.\n[22] Carles Roig, Lorenzo J Tard ´on, Isabel Barbancho, and\nAna M Barbancho. Automatic melody composition\nbased on a probabilistic model of music style and har-\nmonic rules. Knowledge-Based Systems , 71:419–434,\n2014.\n[23] Jukedeck R&D Team. “Releasing a cleaned version of\nthe Nottingham Dataset.” Web blog post. Jukedeck Re-\nsearch , 7 Mar. 2017. Web. 30 Mar. 2018.\n[24] Peter M Todd. A connectionist approach to algorith-\nmic composition. Computer Music Journal , 13(4):27–\n43, 1989.\n[25] Elliot Waite. “Generating Long-Term Structure in\nSongs and Stories.” Web blog post. Magenta , 15 Jul.\n2016. Web. 30 Mar. 2018.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 731"
    },
    {
        "title": "DALI: A Large Dataset of Synchronized Audio, Lyrics and notes, Automatically Created using Teacher-student Machine Learning Paradigm..",
        "author": [
            "Gabriel Meseguer-Brocal",
            "Alice Cohen-Hadria",
            "Geoffroy Peeters"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.3576083",
        "url": "https://doi.org/10.5281/zenodo.3576083",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/35_Paper.pdf",
        "abstract": "The DALI dataset isa large Dataset of synchronised audio, lyrics and notes for the audiofull-duration, with  its time-aligned lyrics and  its time-aligned notes (of the vocal melody). Lyrics are described according to four levels of granularity: notes (and textual information un- derlying a given note), words, lines and paragraphs. For each song, we also provide additional multimodal information such as genre, language, musician, album covers or links to video clips.\n\n\n\nGo tohttps://github.com/gabolsgabs/DALIwhere you can find all the tools to work with the DALI dataset and adetailed description of how to use it.\n\n\n\nFor this version cite the article:\n\n\n@article{meseguer2020creating,\n  title={Creating DALI, a Large Dataset of Synchronized Audio, Lyrics, and Notes},\n  author={Meseguer-Brocal, Gabriel and Cohen-Hadria, Alice and Peeters, Geoffroy},\n  journal={Transactions of the International Society for Music Information Retrieval}, volume={3}, number={1}, year={2020},\n  publisher={Ubiquity Press}\n}\n\n\n\n\n\nand the original paper:\n\n\n@inproceedings{meseguer2019dali,\n  title={Dali: A large dataset of synchronized audio, lyrics and notes, automatically created using teacher-student machine learning paradigm},\n  author={Meseguer-Brocal, Gabriel and Cohen-Hadria, Alice and Peeters, Geoffroy},\n  journal={arXiv preprint arXiv:1906.10606},\n  year={2019}\n}\n\n\n\n\n\nThis research has received funding from the French National Research Agency under the contract ANR-16-CE23-0017-01 (WASABI project)",
        "zenodo_id": 3576083,
        "dblp_key": "conf/ismir/Meseguer-Brocal18",
        "keywords": [
            "DALI dataset",
            "large Dataset",
            "synchronized audio",
            "time-aligned lyrics",
            "time-aligned notes",
            "audiofull-duration",
            "granularity levels",
            "genre",
            "language",
            "multimodal information"
        ],
        "content": "DALI: A LARGE DATASET OF SYNCHRONIZED AUDIO, LYRICS AND\nNOTES, AUTOMATICALLY CREATED USING TEACHER-STUDENT\nMACHINE LEARNING PARADIGM\nGabriel Meseguer-Brocal Alice Cohen-Hadria Geoffroy Peeters\nIrcam Lab, CNRS, Sorbonne Universit ´e, Minist `ere de la Culture, F-75004 Paris, France\ngabriel.meseguerbrocal@ircam.fr, alice.cohenhadria@ircam.fr, geoffroy.peeters@ircam.fr\nABSTRACT\nThe goal of this paper is twofold. First, we introduce\nDALI, a large and rich multimodal dataset containing 5358\naudio tracks with their time-aligned vocal melody notes\nand lyrics at four levels of granularity.\nThe second goal is to explain our methodology where\ndataset creation and learning models interact using a\nteacher-student machine learning paradigm that beneﬁts\neach other. We start with a set of manual annotations of\ndraft time-aligned lyrics and notes made by non-expert\nusers of Karaoke games. This set comes without audio.\nTherefore, we need to ﬁnd the corresponding audio and\nadapt the annotations to it. To that end, we retrieve audio\ncandidates from the Web. Each candidate is then turned\ninto a singing-voice probability over time using a teacher,\na deep convolutional neural network singing-voice detec-\ntion system (SVD), trained on cleaned data. Comparing\nthe time-aligned lyrics and the singing-voice probability,\nwe detect matches and update the time-alignment lyrics ac-\ncordingly. From this, we obtain new audio sets. They are\nthen used to train new SVD students used to perform again\nthe above comparison. The process could be repeated it-\neratively. We show that this allows to progressively im-\nprove the performances of our SVD and get better audio-\nmatching and alignment.\n1. INTRODUCTION\nSinging voice is one of the most important elements in pop-\nular music. It combines its two main dimensions: melody\nand lyrics. Together, they tell stories and convey emo-\ntions improving our listening experience. Singing voice is\nusually the central element around which songs are com-\nposed. It adds a linguistic dimension that complements the\nabstraction of the musical instruments. The relationship\nbetween lyrics and music is both global (lyrics topics are\nusually highly related to music genre) and local (it con-\n© Gabriel Meseguer-Brocal, Alice Cohen-Hadria, Geoffroy\nPeeters. Licensed under a Creative Commons Attribution 4.0 Interna-\ntional License (CC BY 4.0). Attribution: Gabriel Meseguer-Brocal,\nAlice Cohen-Hadria, Geoffroy Peeters. “DALI: a large Dataset of syn-\nchronized Audio, LyrIcs and notes, automatically created using teacher-\nstudent machine learning paradigm”, 19th International Society for Music\nInformation Retrieval Conference, Paris, France, 2018.nects speciﬁc musical parts with a concrete lexical mean-\ning, and also deﬁnes the structure of a song).\nDespite its importance, singing voice has not received\nmuch attention from the MIR community. It has only been\nintroduced a few years ago as a standalone topic [12, 17].\nOne of the most important factors that prevents its de-\nvelopment is the absence of large and good quality ref-\nerence datasets. This problem also exists in other MIR\nﬁelds, nevertheless several solutions have been proposed\n[3, 9]. Currently, researchers working in singing voice\nuse small designed dataset following different methodol-\nogy [10]. Large datasets as the one used in [13] are private\nand not accessible to the community.\nThe goal of this paper is to propose such a dataset and\nto describe the methodology followed to construct it.\n1.1 Proposal\nWe present the DALI dataset: a large Dataset of synchro-\nnised Audio, LyrIcs and notes that aims to stand as a ref-\nerence for the singing voice community. It contains 5358\nsongs (real music) each with – its audio in full-duration,\n– its time-aligned lyrics and – its time-aligned notes (of\nthe vocal melody). Lyrics are described according to four\nlevels of granularity: notes (and textual information un-\nderlying a given note), words, lines and paragraphs. For\neach song, we also provide additional multimodal infor-\nmation such as genre, language, musician, album covers or\nlinks to video clips. The rest of this paper focuses on our\nmethodology for creating DALI. In Figure 1, we illustrate\nthe input and output of our dataset creation system. See\nSection 4 for more details about the dataset itself.\nThe DALI dataset has been created automatically. Our\napproach consists in a constant interaction between dataset\ncreation and learning models where they beneﬁt from each\nother. We developed a system that acquires lyrics and notes\naligned in time and ﬁnds the corresponding audio tracks.\nThe time-aligned lyrics and notes come from Karaoke re-\nsources (see Section 3.1 for more deatils). Here, non-\nexpert users manually describe the lyrics of a song as a se-\nquence of annotations: time aligned notes with their asso-\nciated textual information. While this information is pow-\nerful it has two major problems: 1)there is no information\nabout the exact audio used for the annotation process (only\nthe song title and artist name which may lead to many dif-\nferent audio versions), 2)even if the right audio is found,431Figure 1 : [Left part] The inputs of our dataset creation system are karaoke-user annotations presented as a triple of ftime\n(start + duration), musical-notes, text g. [Right part] Our dataset creation system automatically ﬁnds the corresponding\nfull-audio track and aligned the vocal melody and the lyrics to it. In this example, we illustrate the alignment for a small\nexcerpts. We only represent two levels of lyrics granularity: notes and lines.\nannotations may need to be adjusted to ﬁt the audio per-\nfectly. In Section 3.2, we deﬁne how we retrieve from the\nWeb the possible audio candidates for each song. In Sec-\ntion 3.3, we describe how we select the right audio among\nall the possible candidates and how we automatically adapt\nthe annotated time-alignment lyrics to this audio. In order\nto do this, we propose a distance that measures the corre-\nspondence between an audio track and a sequence of man-\nual annotations. This distance is also used to perform the\nnecessary adaptations on the annotations to be perfectly\naligned with the audio. Our distance requires the audio\nto be described as a singing voice probability sequence.\nThis is computed using a singing voice detection (SVD)\nsystem based on deep convolutional neural network (Con-\nvNet). The performance of our system highly depends on\nthe precision of the SVD. Our ﬁrst version is trained on\nfew but accurately-labeled ground truths. While this sys-\ntem is sufﬁcient to select the right audio it is not to get\nthe best alignment. To improve the SVD, in Section 3.4\nwe propose to use a teacher-student paradigm. Thanks to\nthe ﬁrst SVD system (the teacher) we selected a ﬁrst set\nof audio tracks and their corresponding annotations. Using\nthem, we train new SVD systems (the students). We show\nin Section 3.4.1 that new SVD systems (the students) are\nbetter than the initial one (the teacher). With this new ver-\nsion, we increase the quality and size of the DALI dataset.\nFinally, we discuss our research in Section 5.\n2. RELATED WORKS\nWe review previous works related to our work: singing\nvoice detection methods and the teacher-student paradigm.\nSinging Voice detection. Most approaches share a\ncommon architecture. Short-time observations are used to\ntrain a classiﬁer that discriminates observations (per frame)\nin vocal or non-vocal classes. The ﬁnal stream of predic-\ntions is then post-processed to reduce artifacts.\nEarly works explore classiﬁcation techniques such as\nSupport Vector Machines (SVMs) [16, 20], Gaussian mix-\nture model (GMM) [11] or multi-layer perceptron (MLP)\n[4]. Other approaches also tried to use speciﬁc vocal traits\nsuch as vibrato and tremolo [21] or to adapt speech recog-\nnition systems for the particularities of singing voice [5].Over the past few years, most works focus on the use of\ndeep learning techniques. For example, [23, 24] propose\nthe use of ConvNet combined with data augmentation tech-\nniques (to increase the size of the training set) or trained\non weakly labeled data (the data are only labeled at the\nﬁle level, not at the segment level). [13] also proposes the\nuse of CNN but with a Constant-Q input and a training\non a very large private datasets mined from Spotify re-\nsources. Some researchers suggest the use of Recurrent\nNeural Networks (RNN) [15] or Long Short-Term Mem-\nory (LSTM) [14]. One advantage of these models is that\nthey directly model the decisions sequence over time and\nno post-processing is needed. Other singing voice detec-\ntion systems are developed to be used as a pre-processing-\nstep: for lyrics transcription [17] or for source separation\n[25] trained then to obtain ideal binary masks.\nTeacher-student paradigm. Teacher-student learning\nparadigm [2,28] has appeared as a solution to overcome the\nproblem of insufﬁcient labeled training data in MIR. Since\nmanual labeling is a time-consuming tasks, the teacher-\nstudent paradigm explores the use of unlabeled data for su-\npervised problems. The two main agents of this paradigm\nare: the teacher and the student. The teacher is trained with\nlabels of well known ground truths datasets (often manu-\nally annotated). It is then used to automatically label unla-\nbeled data on a (usually) larger dataset. These new labels\n(the one given by the teacher) are the ones used for training\nthestudent(s) . Student(s) indirectly acquire(s) the desired\nknowledge by mimicking the “teacher behaviour”. This\nmodel has achived great results for tasks in speech recog-\nnition [27] and multilingual models [8]. It has also been\nproved that student(s) can achieve superior performances\nthan the teacher [8, 28].\n3. SINGING VOICE DATASET: CREATION\n3.1 Karaoke resources\nOutside the MIR community there are rich sources of in-\nformation that can be explored. One of these sources is\nKaraoke video games that ﬁt exactly our requirements. In\nthese games, users have to sing along with the music to\nwin points according to their singing accuracy. To mea-\nsure their accuracy, the user melody is compared with a432 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Table 1 : Terms overview: deﬁnition of each term used in this paper.\nTerm Deﬁnition\nAnnotation basic alignment unit as a triple of time (start + duration wrt Fr), musical-notes (with 0 = C3) and text.\nA file with annotations group of annotations that deﬁne the alignment of a particular song.\nOffset time asO it indicates the start of the annotations, its modiﬁcations moves all bock to the right or left.\nFrame rate asFr it controls the annotation grid size stretching or compressing its basic unit.\nAnnotation voice sequence asavs(t)2f0;1g singing voice (SV) sequence extracted from karaoke-users annotations.\nPredictions as^p(t)2[0;1] SV probability sequence provided by our singing voice detection.\nLabels labels sequence of well known ground truths datasets checked by the MIR community.\nTeacher ﬁrst SV detection (SVD) system trained on Labels .\nStudent new SVD system trained on the avs(t)for the subset of track for which NCC (^o;^fr)\u0015Tcorr.\nreference timing note (that has ﬁne time and frequency).\nHence, large datasets of time-aligned note and lyrics exist.\nSuch datasets can be found as open-source. Nowadays,\nthere are several active and big karaoke open-source com-\nmunities. In those, non-expert users exchange text ﬁles\ncontaining lyrics and melody annotations. However there\nis no further professional revision. Each ﬁle contains all\nthe necessary information to describe a song:\n• the sequence of triplets ftime, musical-notes, text g,\n• the offset time (start of the sequence) and frame\nrate (annotation time-grid),\n• the song title and the artist name .\nWe refer to Table 1 for the deﬁnition of all the terms we\nuse. These annotations can be transformed to get the time\nand note frequencies as seen Figure 1.\nWe were able to retrieve 13339 karaoke annotation ﬁles.\nAlthough this information is outstanding for our commu-\nnity, it presents several problems that have to be solved:\nGlobal. When performing the annotation, users can choose\ntheaudio ﬁle they want. The problem is that only the\nsong title andartist name are provided. This combi-\nnation might refer to different audio versions (studio, ra-\ndio edit, live, remix, etc.). Consequently, we do not know\nwhich audio version has been used. Annotations made\nfor a version do not work for another. Besides, even if\nthe correct audio is known, annotations may not perfectly\nﬁt it. As a result annotations must be adapted. This is\ndone by modifying the provided offset time andframe\nrate . These issues are not problematic for karaoke-users\nbut critical To the automatic creation of a large dataset for\nMIR research.\nLocal. It refers to errors due to fact the that users are non-\nprofessionals. It covers local alignment problems of par-\nticular lyric blocks, text misspellings or note mistakes.\nIn this paper we only focus on global problems leaving\nthe local ones for future works.\nWASABI is a semantic database of song knowledge\ngathering metadata collected from various music databases\non the Web [18]. In order to beneﬁt from the richness\nof this database, we ﬁrst linked each annotation ﬁle to\nWasabi. To that end, we connected a speciﬁc song title\nand artist name with all possible corresponding audio\nversions (studio, radio, edit, live, remix, etc.). TheWASABI also provides lyrics in a text only annotations\n(grouped by lines and paragraphs). Using the two lyrics\nrepresentations (note-based annotations and text only an-\nnotations), we created four levels of granularity: notes,\nwords, lines and paragraphs. Finally, WASABI also pro-\nvides extra multimodal information such as cover images,\nlinks to video clips, metadata, biography, expert notes, etc.\n3.2 Retrieving audio candidates\nOur input is an annotation ﬁle connected to the WASABI\ndatabase. This database provides us with the different ex-\nisting versions (studio, radio, edit, live, remix, etc.) for a\nsong title andartist name combination. Knowing the\npossible versions, we then automatically query YouTube1\nto get a set of audio candidates. We now need to select\namong the set of audio candidates the one corresponding\nto the annotation ﬁle.\n3.3 Selecting the right audio from the candidate and\nadapting annotation to it\nEach audio candidate is compared to the reference annota-\ntion ﬁle. We do this by measuring a distance between both\nand keeping the one with the largest value.\nAudio and annotations live in two different representa-\ntion spaces that cannot be directly compared. In order to\nﬁnd a proper distance, we need to transform them to a com-\nmon representation space. Two directions were studied:\nAnnotations as audio. We have explored lyrics synchro-\nnization techniques [10] but their complexity and pho-\nnetic model limitations prevent us to use them. As anno-\ntations can be transformed into musical notes, score align-\nment approaches [7, 26] seem a natural choice. However,\ndue to missing information in the corresponding score\n(we only have the score of the vocal melody) these sys-\ntems failed. We then tried to reduce the audio to the vocal\nmelody (using Melodia [22]) and then align it to the vocal\nmelody score but this also failed. Consequently, we did\nnot persist in this direction.\nAudio as annotations. The idea we develop in the remain-\nder is the following. We convert the audio track to a\nsinging-voice probability ^p(t)over timet. This sequence\nhas value ^p(t)!1when voice is present at time tand\n^p(t)!0otherwise. This probability is computed from\nthe audio signal using a Singing Voice Detection (SVD)\n1We use https://github.com/rg3/youtube-dlProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 433Figure 2 : Architecture of our Singing V oice Detection system using a ConvNet.\nsystem described below. We name this function predic-\ntions . Similarly, the sequence of annotated triplets ftime,\nmusical-notes, text gcan be mapped to the same space:\navs(t) = 1 when a vocal note exists at tandavs(t) = 0\notherwise. We name this function annotation voice se-\nquence .\nSinging Voice Detection system. Our system is based\non the deep Convolutionnal Neural Network proposed\nby [24]. The audio signal is ﬁrst converted to a sequence\nof patches of 80 Log-Mel bands over 115 time frames.\nFigure 2 shows the architecture of the network. The output\nof the system represents the singing voice probability\nfor the center time-frame of the patch. The network is\ntrained on binary target using cross-entropy loss-function,\nADAMAX optimizer, mini-batch of 128, and 10 epochs.\nCross-correlation. To compare audio and annotation,\nwe simply compare the functions ^p(t)andavs(t). As ex-\nplained before, the annotation ﬁles also come with a pro-\nposed offset time andframe rate . We denote them by\nOandFrin the following. The alignment between ^p(t)\nandavs(t)depends on the correctness of OandFrvalues.\nWe will search around OandFrto ﬁnd the best possible\nalignment. We denote by othe correction to be applied to\nOand byfrthe bestFr. Our goals are to:\n1. ﬁnd the value of oandfrthat provides the best\nalignment between ^p(t)andavs(t),\n2. based on this best alignment, deciding if ^p(t)and\navs(t)actually match each other and establishing if\nthe match is good enough to be kept.\nSince we are interested in a global matching between\n^p(t)2[0;1]andavs(t)2f0;1gwe use the normalized\ncross-correlation (NCC) as distance2:\nNCC (o;fr) =P\ntavsfr(t\u0000o)^p(t)pP\ntavsfr(t)2pP\nt^p(t)2\nThe NCC provides us directly with the best ^ovalue.\nThis value directly provides the necessary correction to be\napplied toOto best align both sequences.\nTo ﬁnd the best value of frwe compress or stretch\nannotation by changing the grid size. This warp is con-\nstant and respect the annotation structure. We denote it as\navsfr(t). The optimal frvalue is computed using a brute\n2Matches between ^p(t)andavs(t)can also be found using Dynamic\nTime Warping (DTW). However, we found its application not successfull\nfor our purpose. Indeed, DTW computes local warps that does not respect\nthe global structure of the user annotations. In addition, its score is not\nnormalized preventing its use for matches selection.force approach, testing the values of fraround the original\nFrin an interval controlled by \u000b(we use\u000b=Fr\u00030:05):\n^fr;^o= arg max\nfr2[Fr\u0000\u000b;Fr +\u000b];oNCC (o;fr)\nOur ﬁnal score is given by NCC (^o;^fr).\nThe audio is considered as good match the annotation\nifNCC (^o;^fr)\u0015Tcorr. The value of Tcorr has been\nfound empirically to be Tcorr = 0:8. For a speciﬁc\nannotation, if several audio candidate tracks have a value\nNCC\u0015Tcorr, we only keep the one with the largest\nvalue.Tcorr= 0:8is quite restrictive but even if we may\nloose good pairs we ensure that those we keep are well\naligned. When an audio match is found, the annotations\nare adapted to it using ^frand^o.\nNecessity to improve the Singing Voice Detection\nsystem. The scoreNCC proposed above strongly de-\npends on the quality of ^p(t)(the prediction provided by\ntheSinging Voice Detection (SVD) system). Small dif-\nferences in predictions lead to similar NCC (^o;^fr)values\nbut very different alignments. While the predictions of the\nbaseline SVD system are good enough to select the cor-\nrect audio candidates (although there are still quite a few\nfalse negatives), it is not good enough to correctly estimate\n^frand^o. As improving the SVD system the number of\nfalse negatives will be reduced and we will also ﬁnd better\nalignments. We hence need to improve our SVD system.\nThe idea we propose below is to re-train the SVD sys-\ntem using the set of candidates audio that match the an-\nnotations. This is a much larger training set (around 2000)\nthan the one used to train the baseline system (around 100).\nWe do this using a teacher-student paradigm.\n3.4 Teacher-Student\nOur goal is to improve our Singing V oice Detection (SVD)\nsystem. If it becomes better, it will ﬁnd better matches\nand align more precisely audio and annotations. Conse-\nquently, we will obtain a better DALI dataset. This larger\ndataset can then be used to train a new SVD system which\nagain, can be used to ﬁnd more and better matches improv-\ning and increasing the DALI dataset. This can be repeated\niteratively. After our ﬁrst iteration and using our best SVD\nsystem, we reach 5358 songs in the DALI dataset.\nWe formulate this procedure as a Teacher-Student\nparadigm. The processing steps of the whole Singing\nV oice Dataset creation is summarized in Figure 3.\nUpper left box. We start from Karaoke resources that pro-\nvide our set of annotation ﬁles. Each annotation ﬁle de-\nﬁnes a sequence of triplets ftime, note, nextgthat we con-434 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 3 : Singing V oice Dataset creation using a teacher-student paradigm.\nvert to an annotation voice sequence avs(t). For each an-\nnotation ﬁle, we retrieve a set of audio candidates.\nUpper right box. We independently trained a ﬁrst version\nof the SVD system (based on ConvNet) using the training\nset of a ground truth labeled dataset as provided by the\nJamendo [20] or MedleyDB [6] datasets. We call this ﬁrst\nversion the teacher .\nUpper middle part. This teacher is then applied on each\naudio candidate to predict ^p(t).\nLower left box. We measure the distance between avs(t)\nand^p(t)using our cross-correlation method. It allows us\nto ﬁnd the best audio candidate for an annotation ﬁle and\nthe best alignment parameters ^fr;^o.\nLower middle box. We select the audio annotation pairs\nfor whichNCC (^o;^fr)\u0015Tcorr= 0:8. The set of se-\nlected audio tracks forms a new training set.\nLower right box. This new set is then used to train a new\nSVD systems based on the same CNN architecture. This\nnew version is called the student . To this end, we need to\ndeﬁne the target pto be minimized in the loss L(p;^p).\nThere are three choices:\na) we use as target pthe predicted value ^pgiven right\nby the teacher (usual teacher-student paradigm).\nb) we use as target pthe valueavscorresponding to the\nannotations after aligning them using ^frand^o.\nc) a combination of both, keeping only these frames for\nwhich ^p(t) =avs(t).\nUp to now and since the avshave been found more pre-\ncise than the ^pwe only investigated option b).\nWe compare in the following part the results obtained\nusing different teachers and students.3.4.1 Validating the teacher-student pardigm\nIn this part, we demonstrate that the students trained on the\nnew training-set actually perform better than the teacher\ntrained on the ground-truth label dataset.\nGround-truth datasets: We use two ground-truth labels\ndatasets: Jamendo [20] and MedleyDB [6]. We created a\nthird dataset by merging Jamendo andMedleyDB named\nasJ+M . Each dataset is split into a train and a test part us-\ning an artist ﬁlter (the same artist cannot appear in both).\nTeachers: With each ground-truth datasets we trained a\nteacher using only the training part. Once trained, each\nteacher is used to select the audio matches as described\nin Section 3.3. As a result, we produce three new train-\ning sets. They contains 2440, 2673 and 1596 items for\nthe teacher J+M ,Jamendo andMedleyDB respectively.\nThe intersection of the three sets (not presented here) in-\ndicates that 89.8 % of the tracks selected using the Med-\nleyDB teacher are also present within the tracks selected\nusing the J+M teacher or the Jamendo teacher. Also, 91.4\n% of the tracks selected using the Jamendo teacher are\nwithin the tracks selected using the J+M teacher. It means\nthat the three teachers agree most of the time on selecting\nthe audio candidates.\nStudents: We train three students using the audio and the\navsvalue of the new training sets. Even if there is a large\naudio ﬁles overlap within the training sets, their alignment\n(and therefore the avsvalue) is different. The reason to\nthis is that each teacher gets a different ^pwhich results in\ndifferent ^fr;^ovalues.\n3.4.2 Results\nWe evaluate the performances of the various teachers\nand students SVD systems using the test parts of Ja-\nmendo (Jtest) and MedleyDB (Mtest). We measure the\nquality of each SVD system using the frame accuracy i.e.\naverage value over all the tracks of the test set.\nResults are indicated in Table 2. In this table, e.g. “Stu-\ndent (Teacher Jtrain) (2673)” refers to the student trainedProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 435on the 2673 audio candidates and the avsvalues computed\nwith the Teacher trained on Jamendo train set.\nTable 2 : Performances of the teachers and students using\nthe various datasets. Number of tracks in brackets.\nSVD systemTest setJtest (16) Mtest (36)\nTeacher Jtrain (61) 87% 82%\nStudent (Teacher Jtrain) (2673) 82% 82%\nTeacher Mtrain (98) 76% 85%\nStudent (Teacher Mtrain) (1596) 80% 84%\nTeacher J+M train (159) 82% 82%\nStudent (teacher J+M train) (2440) 86% 87%\nPerformance of the teachers. We ﬁrst test the teachers.\nTeacher Jtrain obtains the best results on J test (87%).\nTeacher Mtrain obtains the best results on M test (85%).\nIn both cases, since training and testing are performed on\ntwo parts of the same dataset, they share similar audio\ncharacteristics. These results are artiﬁcially high. To best\ndemonstrate the generalization of the trained SVD sys-\ntems, we need to test them in a cross-dataset scenario,\nnamely train and test in different datasets.\nIndeed, in this scenario the results are quite differ-\nent. Applying Teacher Jtrain on M test the results de-\ncreases down to 82% (a 5% drop). Similarly when apply-\ningTeacher Mtrain on J test the results decreases down\nto 76% (a 9% drop). Consequently, we can say that the\nteachers do not generalize very well.\nLastly, the Teacher J+M train trained on J+M train\nactually performs worse on both J test (82%) and M test\n(82%) than their non-joined teacher (87% and 85%).\nThese results are surprising and remain unexplained.\nPerformance of the students. We now test the students. It\nis important to note that students are always evaluated in\na cross-dataset scenario since the DALI dataset (on which\nthey have been trained) does not contain any track from\nJamendo orMedleyDB . Hence, there is no possible over-\nﬁtting for those. Our hypothesis is that students achieve\nbetter the results than the teachers because they have seen\nmore data. Especially, we assume that their generalization\nto unseen data will be better.\nThis is true for the performances obtained with the stu-\ndent based on Teacher Mtrain . When applied to J test,\nit reaches 80% which is higher than the performances of\ntheTeacher Mtrain directly (76%).\nThis is also true for the performances computed with\nthe student based on Teacher J+M train . When applied\neither to J test or M test, it reaches 86.5% (86% on\nJamendo and 86% on MedleyDB ) which is above the\nTeacher J+M train (82%). Also, 86.5% is similar or\nabove the results obtained with Teacher Jtrain on J test\n(87%) and Teacher Mtrain on M test (85%). This is a\nvery interesting result that demonstrates the generaliza-\ntion of the student system whichever data-set it is applied\nto. The student based on Teacher J+M train is the one\nused for deﬁning the ﬁnal 5358 songs of the DALI dataset.\nHowever, the performances obtained with the studentbased on Teacher Jtrain applied to M test (82%) do not\nimprove over the direct use of the Teacher Jtrain (82%).\nOn alignment. Not explained in this paper is the fact that\nthe^frand^ovalues computed with the students are much\nbetter (almost perfect) than the ones obtained with the\nteacher. However, we cannot measure it precisely since\nDALI dataset does not have ground-truth label annota-\ntions to that end. Indeed, the goal of this paper is exactly\nto obtain such annotations automatically.\n4. SINGING VOICE DATASET: ACCESS\nThe DALI dataset can be downloaded at https://github.\ncom/gabolsgabs/DALI . There, we provide the detailed de-\nsciption of the dataset as well as all the necessary informa-\ntion for using it. DALI is presented under the recommen-\ndation made by [19] for the description of MIR corpora.\nThe current version of DALI is 1.0. Future updates will be\ndetailed in the website.\n5. CONCLUSION AND FUTURE WORKS\nIn this paper we introduced DALI, a large and rich mul-\ntimodal dataset containing 5358 audio tracks with their\ntime-aligned vocal melody notes and lyrics at four levels\nof granularity.\nWe explained our methodology where dataset cre-\nation and learning models interact using a teacher-student\nparadigm beneﬁting one-another. From manual karaoke\nuser annotations of time-aligned lyrics and notes, we found\na set of matching audio candidates from the Web. To se-\nlect and align the best candidate, we compare the annotated\nvocal sequence (corresponding to the lyrics) to the singing\nvoice probability (obtained with a ConvNet). To improve\nthe latter (and therefore obtain a better selection and align-\nment) we applied a teacher-student paradigm.\nThrough an experiment, we proved that the students\noutperform the teachers notably in a cross-dataset scenario,\nwhen train-set and test-set are from different datasets.\nIt is important to note that the results of the students are\nhigher than the teacher ones, even if they have been train-\ning on imperfect data. In our case, we showed that, in the\ncontext of deep learning, it is better to have imperfect but\nlarge dataset rather than small and perfect ones. However,\nother works went in the opposite direction [1].\nFuture work. We have only performed the teacher-\nstudent iteration once. In next works will use the results\nof the ﬁrst student generations to train a second student\ngenerations. This will deﬁne a new DALI dataset. We plan\nto quantitative measure the quality of ^o;^frand to continue\nexploring the alignments between note annotations and the\naudio. Currently, we trained our student using as target\np=avs, which do not transfer directly the knowledge of\nthe teacher. We will explore other possibilities of knowl-\nedge transfer using other targets (points a) and c) in Section\n3.4) as well as the local problems describe at Section 3.1.\nAcknowledgement. This research has received fund-\ning from the French National Research Agency under the\ncontract ANR-16-CE23-0017-01 (WASABI project).436 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20186. REFERENCES\n[1] Xavier Amatriain. ”in machine learning, is more data\nalways better than better algorithms?”. https://\nbit.ly/2seQzj9 .\n[2] A. Ashok, N. Rhinehart, F. Beainy, and K. M. Kitani.\nN2N learning: Network to network compression via\npolicy gradient reinforcement learning. CoRR , 2017.\n[3] K. Benzi, M. Defferrard, P. Vandergheynst, and\nX. Bresson. FMA: A dataset for music analysis. CoRR ,\nabs/1612.01840, 2016.\n[4] A. Berenzweig, D. P. W. Ellis, and S. Lawrence. Us-\ning voice segments to improve artist classiﬁcation of\nmusic. In AES 22 , 2002.\n[5] A. L. Berenzweig and D. P. W. Ellis. Locating singing\nvoice segments within music signals. In WASPAA ,\npages 119–122, 2001.\n[6] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Can-\nnam, and J. Bello. Medleydb: A multitrack dataset for\nannotation-intensive mir research. In ISMIR , 2014.\n[7] A. Cont, D. Schwarz, N. Schnell, and C. Raphael. Eval-\nuation of Real-Time Audio-to-Score Alignment. In IS-\nMIR, Vienna, Austria, 2007.\n[8] J. Cui, B. Kingsbury, B. Ramabhadran, G. Saon,\nT. Sercu, K. Audhkhasi, A. Sethy, M. Nussbaum-\nThom, and A. Rosenberg. Knowledge distillation\nacross ensembles of multilingual models for low-\nresource languages. In ICASSP , 2017.\n[9] E. Fonseca, J. Pons, X. Favory, F. Font, D. Bog-\ndanov, A. Ferraro, S. Oramas, A. Porter, and X. Serra.\nFreesound datasets: A platform for the creation of open\naudio datasets. In ISMIR , Suzhou, China, 2017.\n[10] H. Fujihara and M. Goto. Lyrics-to-Audio Alignment\nand its Application. In Multimodal Music Process-\ning, volume 3 of Dagstuhl Follow-Ups , pages 23–36.\nDagstuhl, Germany, 2012.\n[11] H. Fujihara, M. Goto, J. Ogata, and H. G. Okuno.\nLyricsynchronizer: Automatic synchronization system\nbetween musical audio signals and lyrics. 5(6):1252–\n1261, 2011.\n[12] M. Goto. Singing information processing. In ICSP ,\npages 2431–2438, 2014.\n[13] E. J. Humphrey, N. Montecchio, R. Bittner, A. Jansson,\nand T. Jehan. Mining labeled data from web-scale col-\nlections for vocal activity detection in music. In ISMIR ,\n2017.\n[14] S. Leglaive, R. Hennequin, and R. Badeau. Singing\nvoice detection with deep recurrent neural networks. In\nIEEE, editor, ICASSP , pages 121–125, Brisbane, Aus-\ntralia, 2015.[15] B. Lehner, G. Widmer, and S. Bock. A low-latency,\nreal-time-capable singing voice detection method with\nlstm recurrent neural networks. In 2015 23rd European\nSignal Processing Conference (EUSIPCO) , 2015.\n[16] M. Mauch, H. Fujihara, K. Yoshii, and M. Goto. Tim-\nbre and melody features for the recognition of vocal\nactivity and instrumental solos in polyphonic music. In\nISMIR 2011 , pages 233–238, 2011.\n[17] A. Mesaros. Singing voice identiﬁcation and lyrics\ntranscription for music information retrieval invited pa-\nper. In 7th Conference on Speech Technology and Hu-\nman - Computer Dialogue (SpeD) , pages 1–10, 2013.\n[18] G. Meseguer-Brocal, G. Peeters, G. Pellerin, M. Buffa,\nE. Cabrio, C. Faron Zucker, A. Giboin, I. Mirbel,\nR. Hennequin, M. Moussallam, F. Piccoli, and T. Fil-\nlon. WASABI: a Two Million Song Database Project\nwith Audio and Cultural Metadata plus WebAudio en-\nhanced Client Applications. In Web Audio Conf. , Lon-\ndon, U.K., 2017. Queen Mary University of London.\n[19] G. Peeters and K. Fort. Towards a (better) Deﬁnition\nof Annotated MIR Corpora. In ISMIR , Porto, Portugal,\n2012.\n[20] M. Ramona, G. Richard, and B. David. V ocal detec-\ntion in music with support vector machines. In Proc.\nICASSP ’08 , 2008.\n[21] L. Regnier and G. Peeters. Singing V oice Detection in\nMusic Tracks using Direct V oice Vibrato Detection. In\nICASSP , page 1, taipei, Taiwan, 2009.\n[22] J. Salamon and E. G ´omez. Melody extraction from\npolyphonic music signals using pitch contour char-\nacteristics. IEEE Transactions on Audio, Speech and\nLanguage Processing , 20:1759–1770, 2012.\n[23] J. Schl ¨uter. Learning to pinpoint singing voice from\nweakly labeled examples. In ISMIR , New York City,\nUSA, 2016. ISMIR.\n[24] J. Schl ¨uter and T. Grill. Exploring Data Augmenta-\ntion for Improved Singing V oice Detection with Neural\nNetworks. In ISMIR 2015 , Malaga, Spain, 2015.\n[25] A. J. R. Simpson, G. Roma, and M. D. Plumb-\nley. Deep karaoke: Extracting vocals from musical\nmixtures using a convolutional deep neural network.\nabs/1504.04658, 2015.\n[26] F. Soulez, X. Rodet, and D. Schwarz. Improving poly-\nphonic and poly-instrumental music to score align-\nment. In ISMIR , page 6, Baltimore, United States,\n2003.\n[27] S. Watanabe, T. Hori, J. Le Roux, and J. Hershey.\nStudent-teacher network learning with enhanced fea-\ntures. In ICASSP , pages 5275–5279, 2017.\n[28] C. Wu and A. Lerch. Automatic drum transcription us-\ning the student-teacher learning paradigm with unla-\nbeled music data. In ISMIR , 2017.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 437"
    },
    {
        "title": "Understanding a Deep Machine Listening Model Through Feature Inversion.",
        "author": [
            "Saumitra Mishra",
            "Bob L. Sturm",
            "Simon Dixon"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492527",
        "url": "https://doi.org/10.5281/zenodo.1492527",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/272_Paper.pdf",
        "abstract": "Methods for interpreting machine learning models can help one understand their global and/or local behaviours, and thereby improve them. In this work, we apply a global analysis method to a machine listening model, which essentially inverts the features generated in a model back into an interpretable form like a sonogram. We demonstrate this method for a state-of-the-art singing voice detection model. We train up-convolutional neural networks to invert the feature generated at each layer of the model. The results suggest that the deepest fully connected layer of the model does not preserve temporal and harmonic structures, but that the inverted features from the deepest convolutional layer do. Moreover, a qualitative analysis of a large number of inputs suggests that the deepest layer in the model learns a decision function as the information it preserves depends on the class label associated with an input.",
        "zenodo_id": 1492527,
        "dblp_key": "conf/ismir/MishraSD18",
        "keywords": [
            "interpretation",
            "machine learning models",
            "global analysis",
            "interpretable form",
            "singing voice detection",
            "up-convolutional neural networks",
            "inverted features",
            "temporal and harmonic structures",
            "decision function",
            "class label"
        ],
        "content": "UNDERSTANDING A DEEP MACHINE LISTENING MODEL THROUGH\nFEATURE INVERSION\nSaumitra Mishra, Bob L. Sturm, Simon Dixon\nCentre for Digital Music, School of Electronic Engineering and Computer Science\nQueen Mary University of London, United Kingdom\nfsaumitra.mishra, b.sturm, s.e.dixon g@qmul.ac.uk\nABSTRACT\nMethods for interpreting machine learning models can\nhelp one understand their global and/or local behaviours,\nand thereby improve them. In this work, we apply a global\nanalysis method to a machine listening model, which es-\nsentially inverts the features generated in a model back into\nan interpretable form like a sonogram. We demonstrate\nthis method for a state-of-the-art singing voice detection\nmodel. We train up-convolutional neural networks to in-\nvert the feature generated at each layer of the model. The\nresults suggest that the deepest fully connected layer of\nthe model does not preserve temporal and harmonic struc-\ntures, but that the inverted features from the deepest con-\nvolutional layer do. Moreover, a qualitative analysis of a\nlarge number of inputs suggests that the deepest layer in\nthe model learns a decision function as the information it\npreserves depends on the class label associated with an in-\nput.\n1. INTRODUCTION\nDeep neural networks (DNNs) are state-of-the-art in nu-\nmerous machine learning applications. This success is due\nto their high expressive power and strong generalisation\ncapability [10]. DNNs acquire these capabilities automati-\ncally through training over large amounts of data and tun-\ning of millions of parameters. Despite their success, DNNs\nremain “black-boxes” as we know very little about the pro-\ncess by which they form their predictions.\nRecent research has highlighted problems associated\nwith DNNs. For example, researchers have demonstrated\nthat attacking these models with carefully generated in-\nputs, called “adversarial examples”, changes their predic-\ntions [11, 15, 44]. Such behaviour may be dangerous to a\nsystem (e.g., autonomous vehicle) if its decision making\ndepends on DNN predictions [32]. Moreover, like shallow\nmachine learning models, a DNN may exploit confounders\nin a dataset and behave correctly for the wrong reasons.\nSuch behaviour limits the performance of a model in the\nc\rSaumitra Mishra, Bob L. Sturm, Simon Dixon. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Saumitra Mishra, Bob L. Sturm, Simon Dixon. “Un-\nderstanding a Deep Machine Listening Model through Feature Inversion”,\n19th International Society for Music Information Retrieval Conference,\nParis, France, 2018.real world where such confounders are absent. Thus, there\nis an urgent need to bring interpretability to these black-\nbox models, i.e. to understand the behaviour of a DNN [4].\nResearchers have attempted to analytically [33, 48] and\nempirically explain the behaviour of DNNs. In this work,\nwe focus on understanding these models empirically using\npost-hoc visualisation methods [27, 31]. We can broadly\nclassify such methods into two categories: (1) methods that\nexplain model predictions (local analysis); and (2) meth-\nods that explain a model (global analysis). Local anal-\nysis uses variants of sensitivity analysis (e.g., gradient-\nbased sensitivity analysis) to generate attribution maps that\nhighlight the input dimensions [39,40,43] or input regions\n(groups of contiguous dimensions) [35,47] in favour of (or\nagainst) a prediction. Such analysis is useful but may result\nin inconsistent [16] and uninterpretable (noisy) explana-\ntions [41]. Although some local explanation methods can\ngenerate cleaner visualisations, they depend on the type of\nnon-linearity [42] or network architecture [38,47] and thus\nare not generalisable.\nIn another direction, the global analysis of DNNs aims\nfor an insight that generalises across input instances. For\nexample, irrespective of the class label associated with an\ninput image, the shallow layers of image content recogni-\ntion models show sensitivity to low-level structures, e.g.,\nedges and gradients [47]. There exist several methods\nfor global analysis. For example, activation maximisation\naims to synthesise examples in the input space (e.g., pix-\nels) that maximally activate a speciﬁc neuron [9,29,40,46]\nor layer [28] in a model. Similarly, feature inversion aims\nto highlight the input content (features) preserved by any\nlayer in a DNN model by inverting the corresponding fea-\nture [6, 23].\nIn this work, we apply feature inversion to a machine\nlistening model that classiﬁes an input audio frame (or ex-\ncerpt) into predeﬁned classes. Previous work in the anal-\nysis of deep machine listening models has focused both\non local and global analysis. The methods to generate\nlocal explanations for model predictions use bin-level [1]\nor region-level [26] attribution maps. On the other hand,\nDieleman et al. [3] globally analyse a music autotagging\nmodel by visualising ﬁlters in the ﬁrst convolutional layer.\nSimilarly, in [36] the authors globally analyse a scaled-\ndown version of their deep onset detector by visualising\nthe most activated feature maps and their corresponding\nﬁlter kernels. Our method differs from these global analy-755Φ\"(𝑋%)ReconstructedInput𝑋'%\"FeatureDiscriminatorD𝑋%InputΨ%)*+,\nΘ(𝑋'%\")Ψ./0,+1/Feature\tInverter𝐺\"ComparatorCΘ(𝑋%)Figure 1 : Functional block diagram of our feature inver-\nsion method. The method inverts a feature \bL(xi)from a\nlayer L by training a feature inverter GLthat jointly min-\nimises the input space loss \tinput and feature space loss\n\tfeature .\bLand\u0002are the representation functions of a\ndiscriminator Dand comparator C, respectively.\nsis methods as it neither limits the analysis only to shallow\nlayers nor puts any restriction on the depth of a model.\nWe demonstrate our method for a state-of-the-art\nsinging voice detection (SVD) model that classiﬁes an in-\nput mel spectrogram excerpt into two classes: ‘vocal’ and\n‘non-vocal’ [37]. We ﬁrst train up-convolutional neural\nnetworks [7], one per layer of the SVD model, to invert the\nfeatures generated by it. We then quantify the performance\nof the inversion models (we call them “feature inverters”)\nby calculating the normalised reconstruction error (NRE)\nthat [23] deﬁnes as the normalised Euclidean distance be-\ntween an input and its inverted representation. The results\ndemonstrate that NRE is largest for a feature inverter that\ninverts the deepest layer (the last fully connected layer)\nin the SVD model and decreases for feature inverters that\ninvert features from shallow layers. Finally, we qualita-\ntively analyse the inverted features for both classes (vocal\nand non-vocal) to understand the preserved input content\nat each layer of the SVD model. Similarly, we analyse the\ninverted features for inputs selected from different datasets\nto test whether the conclusions from one dataset generalise\nto the other. The experimental code and results are avail-\nable online.1\n2. FEATURE INVERSION\nFeature inversion aims to map the feature generated at any\nlayer of a DNN back to a plausible input. Each layer in\na DNN maps an input feature to an output feature and in\nthe process ignores the input content that does not seem\nrelevant to the classiﬁcation task. Thus, the inversion of a\nfeature from any layer of a DNN will highlight the input\ncontent preserved by that layer.\n2.1 Prior Work\nMahendran et al. [23] and Dosovitskiy et al. [6] apply fea-\nture inversion to analyse the global behaviour of convolu-\ntional neural networks (CNNs). Mahendran et al. [23, 24]\n1https://github.com/saum25/ISMIR-2018invert the features from AlexNet [18] (a CNN for image\nrecognition). Their work demonstrates that the inverted\nfeatures from the deepest convolutional layer in AlexNet\nare visually similar (preserve the spatial layout and colour)\nto the input image. They also demonstrate that although\nthe reconstructions from fully connected layers are visually\npoor, they still depict the presence of high-level features\n(e.g., the facial features of an animal). Their work also\nhighlights the invariances captured by the AlexNet layers.\nFor example, the inverted representations from the deep-\nest layer in the model (a fully connected layer) depict an\nobject at different locations, orientations and scales.\nThe method introduced by Mahendran et al. [23] gener-\nates an inverted representation x\u0003\niL2Rnfrom anLthlayer\nfeature by iteratively minimising the feature space loss be-\ntween an input image xi2Rnand an intermediate repre-\nsentation x0\niL2Rn. The method starts with a randomly\nsampled x0\niLand in each iteration updates it by calculating\nthe gradient of the loss function at x0\niL. Formally, given\na CNN with representation function \bL:Rn!Rdthat\nmaps ann-dimensional input to a d-dimensional feature\n\bL(xi)at a layerL, the method inverts \bL(xi)by solving\nx\u0003\niL= arg min\nx0\niLk\bL(x0\niL)\u0000\bL(xi)k2+\u000bf(x0\niL)(1)\nwheref:Rn!Ris a regularisation function (a natu-\nral image prior) that limits the search to realistic images\nand\u000bis a scaling constant. Regularisation is needed since\nan unrestricted search may output fooling examples [30]\nthat cause high activations to a neuron (or a layer), but do\nnot possess features found in natural images. The method\nby Mahendran et al. [23, 24] has two major limitations:\n(1) hand-crafting a prior is challenging as for some inputs\n(e.g., images, audio) deﬁning the constituents of a real in-\nput is difﬁcult; and (2) the method needs to solve Eq. 1 for\nevery new feature it needs to invert.\nThe feature inversion method proposed by Dosovitskiy\net al. [6] tackles both the above issues and demonstrates\nvisually improved reconstructions even for the fully con-\nnected layers of AlexNet. The method trains another neu-\nral network, an up-convolutional neural network (feature\ninverter) [7], to invert the features of a DNN. The method\ntrains a feature inverter by minimising the input space loss\n\tinput , deﬁned as the squared Euclidean distance between\nan input image and its inverted representation. Although\nthis method learns a natural image prior implicitly during\ntraining and is expensive only at the training time, the in-\nverted representations are blurry for all the layers. The\nreason behind this is the way a feature inverter inverts a\nfeature. A forward pass through AlexNet (or any DNN)\nmaps several inputs to the same feature. Thus, to invert a\nfeature, a feature inverter generates an input that is an aver-\nage of all the inputs that AlexNet maps to the given feature.\nThis averaging effect results in blurry reconstructions.\n2.2 Our Method\nFig. 1 provides an overview of our method. We use the\napproach of Dosovitskiy et al. [6], but modify its loss756 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Conv1MP3OutFC7Conv2Conv4MP6Conv5FC8𝑁\"#= 64𝑁\"#= 32𝑁\"#= 128𝑁\"#= 64𝑁$= 256𝑁$= 64Figure 2 : The architecture of the singing voice detection\nmodel proposed by Schl ¨uter et al. [37]. Nfmdenotes the\nnumber of feature maps in the output of a convolutional\nlayer.Nndenotes the number of neurons in a fully con-\nnected layer. Conv: convolutional layer, MP: max-pooling\nlayer, FC: fully connected layer, Out: output layer.\nfunction to reduce the effect of input averaging. Recent\nworks [5, 14] demonstrate that minimising loss in the per-\nceptual space helps to reduce the over-smoothness problem\nfor image generation models. We extend this idea to ma-\nchine listening. Thus, in addition to the input space loss\n\tinput , our method also calculates the feature space loss\n\tfeature . We deﬁne total loss \tas:\n\t =\u0015input\tinput +\u0015feature \tfeature (2)\nwhere\u0015input and\u0015feature weight the losses of the input\nspace and feature space. Thus, our method trains a feature\ninverterGLto invert a feature \bL(xi)by jointly minimis-\ning the input space and feature space losses. To evaluate\n\tfeature , we use the approach from [5] where the authors\nuse a comparator Cto map an input xiand its inverted\nrepresentation ^ xiLto the feature space. A comparator is\na pre-trained discriminative model that may or may not be\nof the same depth as the discriminator D(the model whose\nfeatures we are inverting). We can even use Das a com-\nparator by extracting feature vectors at a layer of D(e.g.,\nDosovitskiy et al. [5] use the deepest convolutional layer\nof AlexNet as a comparator for inverting AlexNet).\nFormally, given an input excerpt xi2Rnand a rep-\nresentation function \bL:Rn!Rdthat maps xito a\nd-dimensional feature \bL(xi)at a layerLof a discrimina-\ntorD, our method trains a feature inverter GLthat maps\n\bL(xi)to an inverted representation ^ xiL2Rn. In or-\nder to do that, the method calculates \tinput and\tfeature .\nGiven a comparator Cwith a representation function \u0002 :\nRn!Rd0, we deﬁne \tfeature as the squared Euclidean\ndistance between \u0002(xi)and\u0002(^ xiL), where ^ xiLis an in-\nverted representation for an input xiat layerLandd0is\nthe dimensionality of the feature space for C. Similarly,\nwe deﬁne \tinput as the squared Euclidean distance be-\ntween xiand^ xiL. The method trains an up-convolutional\nneural network GL(\bL(xi);w)with parameters wby the\noptimisation\nw\u0003= arg min\nwX\ni(kxi\u0000GLf\bL(xi);wgk2\n+k\u0002(xi)\u0000\u0002(GLf\bL(xi);wg)k2) +\fkwk2\n(3)\nwhere\f >0is the regularisation constant. Once we train\nGL, we can invert any feature \bL(xi)by a forward pass\n128236435701281140Conv212835Conv4(SVD)(128 x 35 x 23)Uconv5Uconv4Conv3Conv19246232335Input feature mapsOutput feature mapsFigure 3 : Feature inverter architecture for the Conv4 layer\nof the SVD model. The highlighted components represent\nthe ‘Conv2’ convolutional layer and its input and output\nfeature maps. Uconv: up-convolutional layer, Conv: con-\nvolutional layer.\nthroughGL:\n^ xiL=GL(\bL(xi);w\u0003) (4)\n3. INVERTING THE FEATURES OF A DEEP\nSINGING VOICE DETECTOR\nWe now demonstrate our feature inversion method from\nSection 2 for a state-of-the-art SVD model [37]. We ﬁrst\nintroduce the SVD model and then explain the architec-\ntures and training details of our feature inverters. Finally,\nwe evaluate the performance of the feature inverters on two\nSVD datasets.\n3.1 The Deep Singing Voice Detection Model\nSinging voice detection is an audio segmentation problem\nwhere the task is to classify an input audio frame (ex-\ncerpt) into one of the two categories: singing voice with or\nwithout instrumental music (‘vocal’) or instrumental mu-\nsic without singing voice (‘non-vocal’). There exist sev-\neral methods for singing voice detection. Some use hand-\ncrafted features to train shallow classiﬁers [20, 22, 34],\nwhile others jointly optimise the feature extraction and\nclassiﬁcation steps using deep learning [19, 21, 37].\nSchl¨uter et al. [37] train an SVD model using a CNN\nand seven data augmentation techniques. Their model\nachieves state-of-the-art performance on public benchmark\ndatasets.2Fig. 2 depicts the architecture of their 8-layered\nSVD model. Each convolutional layer performs convo-\nlution using 3\u00023ﬁlters with 1\u00021stride and no zero\npadding. The two max-pooling layers perform pooling\nwith 3\u00023stride and no zero padding. The input to the\nmodel is a mel spectrogram of about 1:6sec (115 frames).\nThe model was trained on the Jamendo dataset [34]. Ja-\nmendo is a dataset of pop music songs and it consists of\nnon-overlapping training, validation and test subsets with\n61, 16 and 16 audio ﬁles, respectively. The model uses the\nauxiliary data (57 frames on each sides of the centre frame)\nas context to classify the centre frame in an input.\n3.2 Feature Inverter Architectures\nWe train up-convolutional neural networks to invert the\nfeatures generated by the above SVD model. We design\n2https://github.com/f0k/ismir2015Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 757Layer Input Shape Units Output Shape\nFC1 256\u00021 256 256\u00021\nReshape 256\u00021 - 16\u00024\u00024\nUconv2 16\u00024\u00024 64 64\u00028\u00028\nConv3 64\u00028\u00028 64 64\u00028\u00028\nUconv4 64\u00028\u00028 32 32\u000216\u000216\nConv5 32\u000216\u000216 32 32\u000216\u000216\nUconv6 32\u000216\u000216 16 16\u000232\u000232\nConv7 16\u000232\u000232 16 16\u000232\u000232\nUconv8 16\u000232\u000232 8 8\u000264\u000264\nConv9 8\u000264\u000264 8 8\u000264\u000264\nUconv10 8\u000264\u000264 1 1\u0002128\u0002128\nTable 1 : Feature inverter architecture to invert the FC7\nlayer of the SVD model. Input and output shape dimen-\nsions are ordered as the number of channels \u0002time\u0002fre-\nquency. Uconv: up-convolutional layer, Conv: convolu-\ntional layer, FC: fully connected layer. Units refer to the\nnumber of neurons in a fully connected layer or the number\nof ﬁlters in a convolutional layer.\nInv-idx Inv-inp Inv-depth Inv-nconv\nFC8 64\u00021 11 4\nFC7 256\u00021 10 4\nMP6 64\u000211\u00027 5 1\nConv5 64\u000233\u000221 5 3\nConv4 128\u000235\u000223 5 3\nMP3 32\u000237\u000225 6 4\nConv2 32\u0002111\u000276 4 4\nConv1 64\u0002113\u000278 2 2\nTable 2 : Architectural overview of the feature inverters\nfor all the layers in the SVD model. Inv-idx: SVD layer\na feature inverter inverts, Inv-inp: input to a feature in-\nverter (number of channels \u0002time\u0002frequency), Inv-\ndepth: number of layers in a feature inverter, Inv-nconv:\nnumber of convolutional layers in a feature inverter. Conv:\nconvolutional layer, FC: fully connected layer and MP:\nmax-pooling layer.\ntwo categories of architectures, one to invert the fully con-\nnected (FC) layers and the other to invert the convolutional\n(Conv) and max-pooling (MP) layers of the SVD model.3\nThe architecture of inversion models in [6] inspires the de-\nsign of our feature inverters, but we adapt the architectures\nto suit the SVD model. A majority of feature inverters need\nto perform the upsampling (unpooling) operation that is an\napproximate inverse of the max-pooling operation done in\nthe SVD model. In order to perform unpooling and con-\nvolution in a single step, we use up-convolutional layers\n(Uconv) with 4\u00024ﬁlters and 2\u00022stride. This conﬁgu-\nration of Uconv layers upsamples an input feature map by\n2 [7] . The number of such layers depends on the dimen-\nsionality of the layer we are inverting. For example, the\n3“inverting a layer” is another way to refer to the inversion of the\nfeatures generated by a layer.feature inverter to invert the 256-dimensional FC7 layer\nuses5Uconv layers (Table 1), while the feature inverter\nto invert the Conv4 layer uses two Uconv layers (Fig. 3).\nThe feature inverters for the Conv1 and Conv2 layers in the\nSVD model do not use Uconv layers as for them the model\ngenerates features without using the max-pooling layer.\nWe increase the capacity of the feature inverters\nby adding convolutional layers; either after every up-\nconvolutional layer (for inverting an FC layer) or before\nthe ﬁrst up-convolutional layer (for inverting a Conv or MP\nlayer). We empirically decide the number of convolutional\nlayers for each feature inverter. The convolutional layers\nperform convolution using 3\u00023ﬁlters with 1\u00021stride and\nimprove the visual appearance of the reconstructions [8].\nTable 2 provides details about the depth and the number\nof Conv layers in each feature inverter. All the layers\nuse exponential linear unit (ELU) non-linearity given by\ny(x) =xifx>0, otherwiseex\u00001[2]. The network uses\nbatch normalisation layers [13] to make sure the input to\neach layer follows a standard normal distribution. Except\nfor the Conv1 and Conv2 layers, each feature inverter gen-\nerates an inverted representation with a larger spatial size\nand later trims it to match the input excerpt size ( 115\u000280).\nThe feature inverters for the Conv1 and Conv2 layers gen-\nerate an inverted representation of the same shape as input\nby symmetrically padding the missing dimensions.\n3.3 Training of the Feature Inverters\nWe train one feature inverter per layer of the SVD model.\nWe train a feature inverter using mel spectrogram excerpts\nof about 1:6sec that we extract from the Jamendo training\ndataset. We show one such sample in Fig. 4. We generate\nexcerpts with a hop size of 10frames ( 140ms). Thus, we\ntrain each feature inverter using a data set of about 100k\nfeatures. We do not use any data augmentation techniques.\nIn order to prevent overﬁtting, we run the optimisation to\na ﬁxed number of weight updates (30 epochs) and select\na feature inverter giving the lowest loss on the validation\nsubset. We use the Conv5 layer of the SVD model as\nthe comparator, i.e., we encode the mel spectrogram and\nthe inverted representation using Conv 5. We initialise the\nfeature inverter weights using the He normal initialisation\nmethod [12]. In each iteration, for a mini-batch of 32ran-\ndomly selected excerpts, the training objective jointly min-\nimises the feature and input space losses and updates the\nchange in parameters using ADAM [17]. We set the scal-\ning factors\u0015input and\u0015feature = 1(Eq. 2). We start train-\ning with an initial learning rate of 0:001and decay it by\n0:5when the training loss does not change for 2consec-\nutive epochs. The training procedure performs regularisa-\ntion usingL2weight decay and sets \f= 1e\u00004(Eq. 3).\n3.4 Quantitative Evaluation of the Feature Inverters\nWe train eight feature inverters using the Jamendo training\ndataset and the architectures and training methodology dis-\ncussed above. We evaluate the performance of each feature\ninverter on an evaluation set of 128mel spectrogram ex-\ncerpts. We build the evaluation set by randomly selecting758 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20180 0.5 1 1.5\nTime512102420484096HzFigure 4 : Sample mel spectrogram excerpt from the Ja-\nmendo dataset. This excerpt belongs to the vocal class.\nConv1\nConv2\nMP3\nConv4\nConv5\nMP6\nFC7\nFC8\nLayer inverted\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Average NRE\nJamendo\nRWC\nFigure 5 : Performance evaluation of the feature inverters.\nThe plot depicts the change in average normalised recon-\nstruction error (NRE) as a feature inverter inverts different\nlayers in the SVD model.\n8excerpts from each of the 16audio ﬁles in the Jamendo\ntest dataset. We quantify the performance of feature in-\nverters by calculating the average normalised reconstruc-\ntion error (NRE) for each feature inverter on the evaluation\ndataset. [23] deﬁnes NRE as:\nNRE =kxi\u0000^ xiLk=Nc (5)\nwhereNcis a normalising constant computed from the av-\nerage pairwise Euclidean distance between excerpts in the\nevaluation set.\nWe also evaluate the feature inverters on the RWC\ndataset [25] to understand whether the results of the quan-\ntitative evaluation on Jamendo extend to the RWC dataset.\nThe RWC dataset for singing voice detection is a public\nbenchmark dataset that contains a collection of 100 pop\nmusic songs, but unlike Jamendo, there is no partitioning\ninto separate subsets. Thus, to evaluate our models we ﬁrst\nbuild an RWC test dataset by randomly selecting 20audio\nﬁles from a set of 100and use them to build an evaluation\ndataset of 160randomly selected excerpts ( 8excerpts per\naudio ﬁles). Moreover, in order to evaluate the feature in-\nverters on a larger evaluation dataset, we randomly sample\n10different evaluation sets, calculate the average NRE for\neach and later take an average. Thus, effectively we eval-\nuate our feature inverters on an evaluation dataset of size\n1280 (Jamendo) and 1600 (RWC) excerpts.\nFig. 5 shows the results of the evaluation. For both\ndatasets, the reconstruction error is largest for the deep-\nest layer in the SVD model (FC8) and decreases for rep-\nresentations inverted from shallower layers. This is pre-\ndictable as the dimensionality of the features in shallowlayers is larger than in deep layers, making it easier to in-\nvert them. For instance, the dimensionality reduction of\nfeatures from MP6 to FC7 is about 19 times, compressing\na 4928-dimensional feature to 256 dimensions. Similarly,\nwe see a large increase in the average NRE between the\nfeature inverters for the Conv2 and MP3 layers. This likely\noccurs due to max-pooling operation that compresses fea-\nture dimensionality by 9 times between the two layers.\nThe results also depict that the feature inverters have\nlarger reconstruction error on the RWC dataset at all but\ntwo layers. This is expected since both the discrimina-\ntor (the SVD model) and the feature inverters are trained\non the Jamendo dataset. One possible explanation for the\ncomparable average NRE of the Conv1 and Conv2 layers\nis that these shallow layers of the model are learning gen-\neralisable features [47]. This becomes less so at deeper\nlayers, where features are likely tuned to speciﬁc traits of\nthe training data.\nWe also compare the performance of the feature inver-\nsion method we use in this work (we call it ‘ Mjoint ’) with\na baseline method (we call it ‘ Minput ’) that trains feature\ninverters using image loss only. We train and test the fea-\nture inverters of Minput on the Jamendo dataset. We ﬁnd\nthat across all the layers of the SVD model, the average\nNRE of the feature inverters using Minput is either similar\nor slightly lower than for those using Mjoint . Such a be-\nhaviour is predictable as Minput aims to only minimise the\ninput reconstruction loss, while Mjoint aims to jointly op-\ntimise both the loss functions, which may or may not result\nin lower NRE [5]. The beneﬁt of using Mjoint comes from\nthe generation of inverted representations that are percep-\ntually closer to an input, a property that is challenging for\nMinput to achieve.\n4. QUALITATIVE ANALYSIS OF THE INVERTED\nFEATURES\nFig. 6 shows visualisations for each layer of the SVD\nmodel. We generate these visualisations by selecting four\ninputs, two from each dataset (Jamendo and RWC), in\nwhich one belongs to each of the two classes (vocal and\nnon-vocal). We then use the feature inverters to invert the\nfeatures extracted by the SVD model from each input. The\nresults provide some insights into the model behaviour. For\nexample, reconstructions from the FC8 layer suggest that\nFC8 does not retain the harmonic structures present in the\ninputs. Moreover, it appears that this layer preserves ei-\nther the high frequency or the low-frequency content of an\ninput. Similarly, FC8 does not preserve any temporal infor-\nmation (musical onset locations) present in the inputs. In-\nterestingly, for a large number of cases (in addition to these\nfour inputs), we found a clear demarcation between the vo-\ncal and the non-vocal class visualisations from this layer.\nWe ﬁnd that for the vocal class, energy appears in higher\nfrequencies while for the non-vocal class energy appears\nin lower frequencies. These visualisations suggest that the\nSVD model learns a class-decision function in this layer.\nSimilarly, reconstructions from the FC7 layer suggest\nthat the layer preserves some harmonic content and ap-Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 759(A)\nInput\nConv1\nConv2\nMP3\nConv4\nConv5\nMP6\nFC7\nFC8\n(B)\n(C)\n(D)\nFigure 6 : Feature inversion from successive layers of the SVD model. Each row corresponds to one input excerpt: (A), (B)\nare respectively non-vocal and vocal excerpts from “03 - Say me Good Bye.mp3” in the Jamendo test dataset. Similarly,\n(C) and (D) are respectively non-vocal and vocal excerpts from “RWC- MDB-P-2001-M04/5 Audio Track.aiff” in the RWC\ntest dataset. Columns contain mel spectrograms of (from left to right): the input signal then inverted representations from\nsuccessive SVD model layers (as labelled). The visualisations highlight how the model ignores the input content as it\nforms higher-level representations. Inversions of shallow layers resemble the input, but the reconstruction quality reduces\nfor deeper layers. Conv: convolutional layer, MP: max-pooling layer, FC: fully connected layer.\nproximate onset locations of the inputs. But, there are\nsome deviations from this behaviour. For instance, in Fig.\n6 row B, the harmonic structures are less evident. Simi-\nlarly, for the input in row C, the feature inverter at FC7 is\nunable to reconstruct all the harmonic and temporal con-\ntent present in the input. This may be due to the fact that\nwe do not train the feature inverters on RWC, thus the re-\nconstruction error is higher for this input, resulting in poor\nreconstruction.\nWe ﬁnd that reconstructions from the deepest convolu-\ntional layer of the model contain more information than\nthose from the two fully connected layers. For both in-\nputs from Jamendo (Fig 6A-B), the model preserves much\nof the input content (e.g., the reconstructions capture the\nharmonic structure and approximately align the temporal\nboundaries with the input). This conﬁrms the quantita-\ntive results of model inversion for Conv5 and FC7 layers,\nwhere we show that the average NRE is about 18% less\nfor Conv5. The visualisations for the RWC excerpts (Fig.\n6C-D) report similar results. Finally, reconstructions from\nall the other layers follow a similar pattern. Moving toward\nshallower layers, they become visually similar to the input,\nincreasingly showing the presence of ﬁner harmonics and\ntemporal structures. Moreover, the inversions from Conv1\nand Conv2 are very close to the respective inputs. This sug-\ngests that the ﬁlters of the ﬁrst 2 convolutional layers act\nas a bijective map, e.g., performing an invertible frequency\ntransform. Moreover, the visualisations from deeper lay-\ners in the model are more blurry than from shallow layers.\nThis suggests that deeper layers capture more invariances\nfrom data than shallow layers.5. CONCLUSION AND FUTURE WORK\nIn this work, we applied a model analysis method called\nfeature inversion to a state-of-the-art singing voice detec-\ntion model. Feature inversion helped to understand the\nglobal behaviour of the SVD model by visualising the in-\nformation preserved by any layer in the model. We trained\nup-convolutional neural networks to invert the features of\nthe model. We quantitatively analysed the feature inverters\nfor each layer in the model to understand the change in in-\nput reconstruction error across different layers. We found\nthat the average NRE changes by about 15% for Jamendo\nbetween the MP6 and FC7 layers due to high dimensional-\nity reduction. Moreover, we qualitatively visualised the in-\nverted representations to understand the input content pre-\nserved by any layer in the model. We found that the deepest\nfully connected layer does not retain any of the temporal or\nharmonic structures present in an input. We also found that\nfor a large number of inputs this layer seems to learn a de-\ncision function that depends on the class associated with\nan input. Qualitative analysis of other layers revealed that\nthe FC7 layer preserves some harmonic and temporal in-\nformation of an input while the reconstructions from the\nConv5 layer are visually similar to the input.\nIn our future work, we plan to improve the loss func-\ntion by adding adversarial loss [5] that helps to generate\nrealistic inverted representations that are close to one of\nthe classes. This facilitates soniﬁcation of inverted repre-\nsentations, giving more insights into the model behaviour.\nMoreover, we plan to extend the analysis by applying fea-\nture inversion to different architectures of the SVD model\nand also to different machine listening tasks.760 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20186. ACKNOWLEDGEMENTS\nWe would like to thank Jan Schl ¨uter for discussions and\nsharing his implementation of the SVD model. We also\nthank the anonymous reviewers for their valuable com-\nments and suggestions.\n7. REFERENCES\n[1] K. Choi, G. Fazekas, and M. B. Sandler. Explaining\nDeep Convolutional Neural Networks on Music Clas-\nsiﬁcation. arXiv e-prints , arXiv:1607.02444, 2016.\n[2] DA. Clevert, T. Unterthiner, and S. Hochreiter. Fast and\nAccurate Deep Network Learning by Exponential Lin-\near Units (ELUs). ArXiv e-prints , arXiv:1511.07289,\n2015.\n[3] S. Dieleman and B. Schrauwen. End-to-End Learning\nfor Music Audio. In Proc. ICASSP , 2014.\n[4] F. Doshi-Velez and B. Kim. Towards a Rigorous\nScience of Interpretable Machine Learning. arXiv e-\nprints , arXiv:1702.08608, 2017.\n[5] A. Dosovitskiy and T. Brox. Generating Images with\nPerceptual Similarity Metrics Based on Deep Net-\nworks. In Proc. NIPS , 2016.\n[6] A. Dosovitskiy and T. Brox. Inverting Visual Represen-\ntations with Convolutional Networks. In Proc. CVPR ,\n2016.\n[7] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learn-\ning to Generate Chairs with Convolutional Neural Net-\nworks. In Proc. CVPR , 2015.\n[8] A. Dosovitskiy, J. T. Springenberg, M. Tatarchenko,\nand T. Brox. Learning to Generate Chairs, Tables and\nCars with Convolutional Networks. IEEE Transactions\non Pattern Analysis and Machine Intelligence , 2017.\n[9] D. Erhan, Y . Bengio, A. Courville, and P. Vincent. Vi-\nsualising Higher-Layer Features of a Deep Network.\nTechnical Report 1341, University of Montreal, June\n2009.\n[10] I. Goodfellow, Y . Bengio, and A. Courville. Deep\nLearning . MIT Press, 2016.\n[11] I. Goodfellow, J. Shlens, and C. Szegedy. Explaining\nand Harnessing Adverserial Examples. In Proc. ICLR ,\n2015.\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun. Delving Deep into Rectiﬁers: Surpassing\nHuman-Level Performance on ImageNet Classiﬁca-\ntion. In Proc. ICCV , 2015.\n[13] S. Ioffe and C. Szegedy. Batch Normalization: Accel-\nerating Deep Network Training by Reducing Internal\nCovariate Shift. In Proc. ICML , 2015.[14] J. Snell and K. Ridgeway and R. Liao and B.D. Roads\nand M. C. Mozer and R. S. Zemel. Learning to Gener-\nate Images with Perceptual Similarity Metrics. In Proc.\nICIP , 2017.\n[15] C. Kereliuk, Bob L. Sturm, and J. Larsen. Deep Learn-\ning, Audio Adversaries, and Music Content Analysis.\nInProc. WASPAA , 2015.\n[16] PJ. Kindermans, S. Hooker, J. Adebayo, M. Alber,\nK. T. Sch ¨utt, S. D ¨ahne, D. Erhan, and B. Kim. The\n(Un)reliability of Saliency Methods. In Proc. NIPS\nWorkshop , 2017.\n[17] D. Kingma and B. Jimmy. Adam: A Method for\nStochastic Optimization. In Proc. ICLR , 2015.\n[18] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet\nClassiﬁcation with Deep Convolutional Neural Net-\nworks. In Proc. NIPS , 2012.\n[19] S. Leglaive, R. Hennequin, and R. Badeau. Singing\nV oice Detection with Deep Recurrent Neural Net-\nworks. In Proc. ICASSP , 2015.\n[20] B. Lehner, R. Sonnleitner, and G. Widmer. Towards\nLight-Weight, Real-Time-Capable Singing V oice De-\ntection. In Proc. ISMIR , 2013.\n[21] B. Lehner, G. Widmer, and S. Bock. A Low-Latency,\nReal-Time-Capable, Singing V oice Detection Method\nwith LSTM Recurrent Neural Networks. In Proc. EU-\nSIPCO , 2015.\n[22] B. Lehner, G. Widmer, and R. Sonnleitner. On the Re-\nduction of False Positives in Singing V oice Detection.\nInProc. ICASSP , 2014.\n[23] A. Mahendran and A. Vedaldi. Understanding Deep\nImage Representations by Inverting Them. In Proc.\nCVPR , 2015.\n[24] A. Mahendran and A. Vedaldi. Visualizing Deep Con-\nvolutional Neural Networks Using Natural Pre-images.\nInternational Journal of Computer Vision , pages 1–23,\n2016.\n[25] M. Mauch, H. Fujihara, K. Yoshii, and M. Goto. Tim-\nbre and Melody Features for the Recognition of V ocal\nActivity and Instrumental Solos in Polyphonic Music.\nInProc. ISMIR , 2011.\n[26] S. Mishra, B. L. Sturm, and S. Dixon. Local Inter-\npretable Model-Agnostic Explanations for Music Con-\ntent Analysis. In Proc. ISMIR , 2017.\n[27] G. Montavon, W. Samek, and KR. M ¨uller. Meth-\nods for Interpreting and Understanding Deep Neural\nNetworks. Digital Signal Processing , 73(Supplement\nC):1–15, 2018.\n[28] A. Mordvintsev, C. Olah, and M. Tyka. Inceptionism:\nGoing Deeper into Neural Networks . “ https:\n//research.googleblog.com/2015/06/Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 761inceptionism-going-deeper-into-neural.\nhtml ”, 2015.\n[29] A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, and\nJ. Clune. Synthesizing the Preferred Inputs for Neurons\nin Neural Networks Via Deep Generator Networks. In\nProc. NIPS , 2016.\n[30] A. Nguyen, J.Yosinski, and J. Clune. Deep Neural Net-\nworks are Easily Fooled: High Conﬁdence Predictions\nfor Unrecognizable Images. In Proc. CVPR , 2015.\n[31] C. Olah, A. Mordvintsev, and L. Schubert. Feature Vi-\nsualization. Distill , 2017.\n[32] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B.\nCelik, and A. Swami. Practical Black-Box Attacks\nagainst Machine Learning. In Proc. ACM ASIACCS ,\n2017.\n[33] R. Vidal and J. Bruna and R. Giryes and S.\nSoatto. Mathematics of Deep Learning. ArXiv e-prints ,\narXiv:1712.04741, 2017.\n[34] M. Ramona, G. Richard, and B. David. V ocal Detec-\ntion in Music Using Support Vector Machines. In Proc.\nICASSP , pages 1885–1888, 2008.\n[35] M. T. Ribeiro, S. Singh, and C. Guestrin. Why Should\nI Trust You?”: Explaining the Predictions of Any Clas-\nsiﬁer. In Proc. KDD , 2016.\n[36] J. Schl ¨uter and S. B ¨ock. Improved Musical Onset De-\ntection with Convolutional Neural Networks. In Proc.\nICASSP , 2014.\n[37] J. Schl ¨uter and T. Grill. Exploring Data Augmenta-\ntion for Improved Singing V oice Detection with Neural\nNetworks. In Proc. ISMIR , 2015.\n[38] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam,\nD. Parikh, and D. Batra. Grad-cam: Visual Explana-\ntions from Deep Networks Via Gradient-Based Local-\nization. In Proc. ICCV , 2017.\n[39] A. Shrikumar, P. Greenside, and A. Kundaje. Learn-\ning Important Features Through Propagating Activa-\ntion Differences. In Proc. ICML , pages 3145–3153,\n2017.\n[40] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep In-\nside Convolutional Networks: Visualising Image Clas-\nsiﬁcation Models and Saliency Maps. In Proc. ICLR ,\n2014.\n[41] D. Smilkov, N. Thorat, B. Kim, F. Vi ´egas, and M. Wat-\ntenberg. Smoothgrad: Removing Noise by Adding\nNoise. In Proc. ICML Workshop on Visualisation for\nDeep Learning , 2017.\n[42] J. T. Springenberg, A. Dosovitskiy, T. Brox, and\nM. Riedmiller. Striving for Simplicity: The All Con-\nvolutional Net. In Proc. ICLR Workshop , 2015.[43] M. Sundararajan, A. Taly, and Q. Yan. Axiomatic At-\ntribution for Deep Networks. In Proc. ICML , 2017.\n[44] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Er-\nhan, I. Goodfellow, and R. Fergus. Intriguing Proper-\nties of Neural Networks. In Proc. ICLR , 2014.\n[45] A. Weller. Challenges for Transparency. In Proc.\nICML Workshop on Human Interpretability in Machine\nLearning , 2017.\n[46] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lip-\nson. Understanding Neural Networks Through Deep\nVisualization. In Proc. ICML Deep Learning Work-\nshop , June 2015.\n[47] M. D. Zeiler and R. Fergus. Visualizing and Un-\nderstanding Convolutional Networks. In Proc. ECCV ,\n2014.\n[48] C. Zhang, S. Bengio, M. Hardt, B. Recht, and\nO. Vinyals. Understanding Deep Learning Requires\nRethinking Generalization. In Proc. ICLR , 2017.762 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Shared Generative Representation of Auditory Concepts and EEG to Reconstruct Perceived and Imagined Music.",
        "author": [
            "André Ofner",
            "Sebastian Stober"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492433",
        "url": "https://doi.org/10.5281/zenodo.1492433",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/101_Paper.pdf",
        "abstract": "Retrieving music information from brain activity is a challenging and still largely unexplored research problem. In this paper we investigate the possibility to reconstruct perceived and imagined musical stimuli from electroencephalography (EEG) recordings based on two datasets. One dataset contains multichannel EEG of subjects listening to and imagining rhythmical patterns presented both as sine wave tones and short looped spoken utterances. These utterances leverage the well-known speech-to-song illusory transformation which results in very catchy and easy to reproduce motifs. A second dataset provides EEG recordings for the perception of 10 full length songs. Using a multi-view deep generative model we demonstrate the feasibility of learning a shared latent representation of brain activity and auditory concepts, such as rhythmical motifs appearing across different instrumentations. Introspection of the model trained on the rhythm dataset reveals disentangled rhythmical and timbral features within and across subjects. The model allows continuous interpolation between representations of different observed variants of the presented stimuli. By decoding the learned embeddings we were able to reconstruct both perceived and imagined music. Stimulus complexity and the choice of training data shows strong effect on the reconstruction quality.",
        "zenodo_id": 1492433,
        "dblp_key": "conf/ismir/OfnerS18",
        "keywords": [
            "brain activity",
            "electroencephalography (EEG)",
            "musical stimuli",
            "perceived and imagined",
            "speech-to-song illusory transformation",
            "rhythmical motifs",
            "audio recordings",
            "deep generative model",
            "latent representation",
            "stimulus complexity"
        ],
        "content": "SHARED GENERATIVE REPRESENTATION OF AUDITORY CONCEPTS\nAND EEG TO RECONSTRUCT PERCEIVED AND IMAGINED MUSIC\nAndr ´e Ofner Sebastian Stober\nResearch Focus Cognitive Sciences, University of Potsdam, Germany\n{ofner, sstober }@uni-potsdam.de\nABSTRACT\nRetrieving music information from brain activity is a chal-\nlenging and still largely unexplored research problem. In t his\npaper we investigate the possibility to reconstruct percei ved and\nimagined musical stimuli from electroencephalography (EE G)\nrecordings based on two datasets. One dataset contains mult i-\nchannel EEG of subjects listening to and imagining rhythmic al\npatterns presented both as sine wave tones and short looped\nspoken utterances. These utterances leverage the well-kno wn\nspeech-to-song illusory transformation which results in v ery\ncatchy and easy to reproduce motifs. A second dataset provid es\nEEG recordings for the perception of 10 full length songs. Us -\ning a multi-view deep generative model we demonstrate the fe a-\nsibility of learning a shared latent representation of brai n activ-\nity and auditory concepts, such as rhythmical motifs appear ing\nacross different instrumentations. Introspection of the m odel\ntrained on the rhythm dataset reveals disentangled rhythmi cal\nand timbral features within and across subjects. The model a l-\nlows continuous interpolation between representations of differ-\nent observed variants of the presented stimuli. By decoding the\nlearned embeddings we were able to reconstruct both perceiv ed\nand imagined music. Stimulus complexity and the choice of\ntraining data shows strong effect on the reconstruction qua lity.\n1. INTRODUCTION\nStudying the human brain‘s response to music gained a lot\nof attention in recent years. Many studies in the field rely o n\nelectroencephalography (EEG) recordings, as they provide\nbetter temporal resolution than other techniques, such as f unc-\ntional magnetic resonance imaging (fMRI). Previous resear ch\nsuggests that a listener’s brain response is modulated in co rrela-\ntion to the perceived auditory stimuli on many different lev els\nand that these modulations can be detected within EEG. One\nof these effects is the correlation between the frequency an d\nmagnitude of neural oscillation patterns, which are modula ted\nby accents and rhythmical patterns in music [3,20,21]. Othe r\nstudies indicate that tracking auditory attention towards a\nspecific sound source in EEG recordings is possible [1,30].\nEEG data has been used to research event-related potentials\n(ERPs) as a repeatable and distinguishable response to aspe cts\nc/circlecopyrtAndr ´e Ofner, Sebastian Stober. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribution:\nAndr ´e Ofner, Sebastian Stober. “Shared generative representat ion of auditory\nconcepts and EEG to reconstruct perceived and imagined musi c”, 19th Interna-\ntional Society for Music Information Retrieval Conference , Paris, France, 2018.of perceived music. The characteristic brain activity patt erns\nunderlying ERPs can be specific, for example, to the structu re\nof musical events, such as note onsets or rhythm and pitch\npatterns [19, 24]. Other ERPs are related to the timbre of\nsound and can be modulated even by differences within timbre ,\nsuch as changes in harmonics [17, 25]. While many ERP\ncomponents show similar activation across subjects, studi es\nsuggest that some are caused by more fine-grained aspects\nof music, especially within trained musicians [25]. These\nbrain activity patterns extend over the temporal, spatial a nd\nfrequency domain of the EEG signal.\nMotivated by the existence of such features, EEG recordings\nhave been used in several music information retrieval studi es\nbased on EEG, such as perceived rhythm or tempo classifi-\ncation [28]. First attempts have been made to reconstruct th e\nloudness envelope of perceived and imagined musical stimul i,\nbut with unsatisfying accuracy [22,26,27]. Some of these st ud-\nies use deep neural networks for classification and regress ion\nand the achieved results hint at their usefulness in explori ng\nthe complex brain signal. However, the power of employed\nnetworks is restricted by size and their general applicatio n ex-\nclusively to EEG signal denoising or classification. Outsi de\nfrom research on music cognition, recent studies have shown\nthe possibility to use generative models to reconstruct per ceived\nvisual stimuli both from fMRI and EEG recordings [4, 10].\nGenerative models learn to encode a meaningful internal lat ent\nrepresentation of a given signal. In addition, they contain a de-\ncoding part to either reconstruct the input or another signa l that\nis extractable from the internal latent variable. A recent s tudy\nhas demonstrated the possibility to learn such shared laten t em-\nbeddings for EEG recordings of music perception and use them\nas a continuous semantic space representation of the audio [ 23].\nThis suggests that a more elaborate generative model could\nlearn a shared encoding of music and brain signals, leading\nto a conjoint representation of those auditory concepts tha t are\nperceived and processed by the brain. As previous research s ug-\ngests, these concepts span a spectrum of complexity, starti ng\non the level of the subject-specific manifestation and mean ing\nof specific ERP responses to high-level semantic or emotion al\nmeaning of music. Therefore, they provide the necessary\ninformation to reconstruct the musical stimuli as they are\nperceived or imagined. Based on this motivation, we propose\na generative multi-view model that makes use of deep neural\nnetworks to encode and decode spatio-temporal brain signal\nusing a latent embedding. This embedding is simultaneously\nused to reconstruct and classify music presented and imagin ed\nduring EEG recording. In this paper we introduce our view392on auditory concepts and the suggested method. We describe\ntwo datasets of EEG recordings during music perception\nand imagination that are used for training and evaluation.\nFurthermore, we perform model introspection to demonstrat e\nthe possibility of interpolating between musically meanin gful\npoints within the learned latent space. Finally, we suggest\npossible ways to extend the framework to include multi-moda l\nprocessing and learning high level musical concepts.\n2. AUDITORY CONCEPTS\nOur approach relies on three assumptions for auditory conce pts:\n1. Coupled auditory and conceptual processing\n2.Shared neural representation of music perception and\nimagination\n3. Hierarchical structure of music\nFirstly, we assume that there is a tight coupling between au-\nditory and conceptual processing [12]. Several studies sug gest\nthat auditory stimuli are processed in a conceptual system t hat\nis shared with other modalities, such as visual perception [ 31].\nFurthermore, music processing is based on concepts inheren t\nto the auditory stimuli as well as on external factors, such a s\nvisual and social environment or musical training [7]. Seco ndly,\nfollowing the ideas of embodied cognition, we assume that th e\nhuman conceptual system is essentially grounded in percept ion\nand that through its interplay with action and cognitive sta tes,\nmusic perception at least partially shares conceptual and\nneural representation with musical imagination [11]. Prev ious\nresearch suggests that auditory concept formation can be\ntraced back to specific ERPs and that the magnitude of some\nERP component can be controlled by the presence of an\nauditory concept in the listeners mind [29]. Thirdly, we fol low\nthe idea that music is essentially hierarchical in structur e and\nthat auditory concepts equivalently exist on a spectrum of a b-\nstraction levels, reflecting and augmenting this structur e. They\ncan range from concepts related to single sounds or rhythm\nto concepts within the emotional or aesthetic processing of\nmusic. Together with the previous two assumptions this mean s\nthat basic elements of perceptual musical processing, such as\nERPs related to note onset expectancy, are influenced by the ir\nintegration into conceptual processing. Music cognition a nd\nconcept formation can be highly subjective, stimulus-driv en\nas well as context-dependent, e.g. on visual and social aspe cts\nof a performance [18]. For these reasons, we hypothesize tha t\na simultaneous retrieval of auditory concepts from multipl e\nsources aids the reconstruction of the processed stimuli wh ile\nfurther deepening our understanding of music cognition.\n3. RELATED WORK\nV arious approaches exist to learning a shared embedding fro m\ntwo or more datasets. One method is Canonical Correlation\nAnalysis (CCA) [8]. CCA is non-probabilistic and enables th e\nextraction of linear components to optimize the correlatio ns be-\ntween two multivariate datasets. CCA in combination with co n-\nvolutional neural networks has recently been used by Raposoet al. to learn a shared semantic space between audio and EEG\nsignal [23]. Based on CCA, Fujiwara et al. have introduced\nBayesian Canonical Correlation Analysis (BCCA), a probabi lis-\ntic interpretation of CCA [5]. However, BCCA still contains\nlinear observation models, while EEG data is very complex\nand noisy and requires non-linear computation. To surpass t his\nlimitation, Deep Canonically Correlated Autoencoders (DC -\nCAEs) were proposed by Wang et al. [32]. DCCAEs maximize\nthe correlation between the latent embeddings of two separa te\nautoencoders, but do not enable cross-reconstruction betw een\ntheir inputs. While this problem is solved by correlational\nneural networks (CorrNets), the unregularized latent embe d-\ndings of both DCCAE and CorrNet are prone to overfitting,\nespecially in combination with the representational power of\nnon-linear observation models [2]. For these reasons, we fo llow\nthe suggestion of Wang et al. to use a deep, generative and pro b-\nabilistic latent variable interpretation of CCA called Dee p V aria-\ntional Canonical Correlation Analysis (VCCA) [32]. A simil ar\napproach tailored specifically to a missing view reconstru ction\nfor visual stimuli in fMRI data has successfully been demon-\nstrated recently [4]. Here, we show that we can derive a gener al\nmulti-view generative model capable of joint EEG and stimul us\nprocessing that allows multi-modal learning from physiolo gical\ndata as well as directly from the stimuli. To our knowledge, n o\ncomparable framework for EEG-based audio stimulus recon-\nstruction or for shared auditory concept learning exists.\n4. DATASETS AND PREPROCESSING\nWe use two datasets, the OpenMIIR speech and the Naturalisti c\nMusic EEG Dataset - Tempo (NMED-T) dataset. They are\nsimilar in experimental setup but differ in focus and size.\n4.1 OpenMIIR speech dataset\nOne dataset contains EEG of subjects listening to and\nimagining four rhythmical patterns presented both as sine\nwave tones and short looped spoken utterances. It stems from\nthe Open Music Imagery Information Retrieval (OpenMIIR)\ninitiative [28] and features four different catchy and easy to\nreproduce motifs superimposed on a constant metronome\nclick. We refer to it as ”OpenMIIR speech dataset”. The trial s\nare annotated for containing either speech or sine wave tone s\nand can be used to train and evaluate model performance for\nthe perception and imagination of the same rhythmical trial s\nwithin two timbres. The metronome clicks serve as cues that\nare present during perception as well as imagination. The ma in\nintention behind this dataset is to reduce stimulus complex ity\nas far as possible while still retaining enough musical stru cture\nfor building and evaluating models. This dataset contains d ata\nfrom seven subjects with normal hearing and no history of\nbrain injury. It was recorded with 64 EEG channels, horizont al\nand vertical Electrooculography (EOG) channels sampled at\n512 Hz. All perception stimuli have equal tempo and duration\nof 12 s. Presentation was done in randomized order after\n2 s of metronome clicks. They were immediately followed\nby another 12 s of metronome cues. Participants were asked\nto imagine the perceived stimulus directly after presentat ion\nusing these subsequent cue clicks. The concatenatedProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 393perception-imagination trials sum up to 26 s of recorded EEG\ndata for each trial. As each trial was presented 6 times, this\nsums up to a total of 96 presented trials. In total, the datase t\ncontains about 2500 s (42 min) of EEG recordings per subject.\nWe performed common-practice preprocessing steps using th e\nMNE-python toolbox by Gramfort et al. including manual bad\nchannel removal and interpolation after visual inspection [6].\nAll EEG data was bandpass filtered between 0.5 and 50 Hz.\nExtended Infomax Independent Component Analysis (ICA)\nwas used to remove EEG artifacts using the EOG signal.\n4.2 NMED-T dataset\nThe NMED-T dataset provides EEG recordings for the\nperception of 10 naturalistic full length songs. The songs a re\nin Western musical tradition, have durations between 4:30 a nd\n5:00 min in length and contain vocals. They are real-world\nmusical works with pronounced rhythmical properties. 125\nchannel EEG at 1 kHz sampling rate was recorded for all of\nthe 20 subjects with normal hearing and no history of brain\ninjury. We used the preprocessed version of the dataset, whi ch\nfeatures EEG down-sampled to 125 Hz and bandpass filtered\nbetween 0.3 and 50 Hz. Ocular and cardiac artifacts were\nremoved using the additional EOG channels with ICA after\nmanual bad channel removal. A more detailed description of\nthe preprocessed dataset can be found in [15].\nSubjects in both experiments were not required to have\nmusical training, nor did they execute a particular task dur ing\nlistening or imagination. All EEG channels were normalized\nto zero mean and range [-1, 1]. For training, EEG data was\nsplit into excerpts of 1 s length, resulting in 512 samples\n(OpenMIIR) and 125 samples (NMED-T) length.\nWe computed Mel spectrograms of audio targets at full\nsample-rate of 44100 Hz using the librosa library [16] with\n64 frequency bands between 0 and 2000 Hz, FFT window size\nof 2048 and hop length of 1024. Furthermore, we generated\nloudness envelopes for each stimulus using Hilbert transfo rm\nof the scipy library at the full sample rate [9]. We then\ndown-sampled the Mel spectrograms and loudness envelopes\nto the sample rates of the EEG (512 Hz for OpenMIIR and 125\nHz for NMED-T) before splitting into excerpts of 1 s length.\n5. LEARNING SHARED REPRESENTATIONS\nOF AUDIO AND BRAIN SIGNAL\nWe propose an adaptation of VCCA as proposed by Wang\net al. [32] to perform multi-view learning on audio and EEG\nsignal by defining EEG and audio to be two views that can\nbe generated independently from a shared latent embedding z :\np(audio,eeg,z )=p(z)p(audio|z)p(eeg|z). (1)\nAs we are essentially interested in the auditory informa-\ntion within EEG signal, we formulate a default model with a\nsingle encoder, which processes EEG. Here, z is a learnable\nspace of auditory concepts which are contained implicitly b oth\nin the audio and the EEG signal and which generate signifi-\ncant parts of both views. Following the VCCA principle, we\nproject both audio and EEG signal into the shared space z.\nBy declaring the prior p(z),p(audio|eeg), andp(eeg|z)to be Gaussian, we ensure that the projections E[z|audio]\nandE[z|eeg]of the maximum likelihood solution are in the\nsame space as the projections through CCA. As we deal with\nthe reconstruction of complex EEG data, we parametrize the\nmean ofpΘ(eeg|z)with deep neural networks (DNNs) and\napply the same procedure for the mean of pΘ(audio|z). The\napproximate posterior qφ(z|eeg)is optimized by a third DNN.\nTraining the VCCA model is done in analogy to V ariational Au-\ntoencoders (V AEs) with variational inference by sampling f rom\nqφ(z|eeg). Optimizing the lower bound of the log likelihood\nL(eeg,audio ;θ,φ)with stochastic backpropagation is done by\noptimizing the reconstruction loss of audio and EEG decoder\nand the Kullback-Leibler (KL) divergence between the learn ed\nqφ(z|eeg)andp(z)using the reparameterization trick [14].\n5.1 Multimodal data and additional views\nThis model can be extended to arbitrary amount of decoders\nto reconstruct multiple views, as long as they are dependent\nmainly of a shared latent variable. Here, we use several\ndecoders to reconstruct different aspects of the audio sign al:\nMel spectrograms of the audio stimuli, their loudness envel ope\nas well as an additional decoder to classify the trial types.\nBased on our retrieval intention, here we focus on the learne d\nembedding and the reconstructed Mel spectrograms. We\nuse the remaining decoders to enhance the training quality.\nSimilarly, we can add additional encoders, if they represen t\ndata based on the latent variable, by making use of additiona l\nprivate latent variables introduced with the VCCA model.\nThey store only the view-specific aspects of additional inp ut,\ne.g. from other biological modalities, such as fMRI, audio o r\nEEG signal during imagination. Figure 1 shows an example\nof the modified VCCA architecture with one EEG decoder\nand two audio decoders. Here, we test the model with a single\nEEG encoder and multiple decoders.\nFigure 1 . VCCA architecture for shared auditory concept and\nEEG representation learning. Latent variables parametriz ed by\noptional private encoders are indicated with dashed lines.394 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20185.2 EEG encoder architectures\nBoth NMED-T and OpenMIIR speech EEG encoders featured\n4 convolutional layers with filter numbers linearly ascend ing\nfrom 64 to 512 per layer. Convolution was performed on two\ndimensional inputs. Each column of the input represented th e\nsame linear concatenation of EEG channels for a single sampl e\nwithin the inputs of 1 s length. This resulted in inputs of siz e\n512*64 for the OpenMIIR speech and 125*125 channels for\nthe NMED-T inputs. The kernel size was set to [2x2] for all\nlayers. Here and for all further kernel dimensions, we defin e\nthe first index to be within the channel domain (or frequency\nfor spectrograms) and the second within the temporal domain .\nEach convolutional layer was followed by 30 % dropout.\n5.3 EEG and audio decoder architectures\nWe used similar EEG decoder architectures for both datasets .\nThe OpenMIIR speech EEG decoder featured 6 hidden decon-\nvolution layers with three layers of 16 and another three lay ers\nof 32 filters. The kernel size was set uniformly to [2x16] wit h\nstride 2 except for a [2x1] kernel in the third layer with stri de\n1. A final dense output layer consisted of 512*64 units. The\ndecoder for the NMED-T dataset followed the same deconvo-\nlution architecture, except for kernels with dimension of [ 4x16]\nand [4x1] instead of [2x16] and [2x1]. A final dense layer con -\nsisted of 125*125 units. Both OpenMIIR speech and NMED-T\ndecoders for Mel spectrograms consisted of four layers: Two\ndeconvolution layers of 32 filters and two layers with 64 fil ters.\nAs the length of Mel spectrograms mirrors those of the EEG\nexcerpts, but in combination with a frequency resolution of\n64 bins, the final dense layer featured 512*64 and 125*64\nunits respectively. The kernel dimensions were set to [4x8]\nuniformly, except for the fourth deconvolution layer of the\nOpenMIIR speech decoder, with a [2x8] kernel. The decoder\nfor loudness envelope reconstruction consisted of a bidire c-\ntional LSTM layer with 128 hidden units, followed by a dense\nlayer of size equal to the length of the audio excerpt. Finall y,\nthe decoder used for classification of the OpenMIIR speech\ndataset consisted of two hidden dense layers with 32 filters and\na dense output layer of 1 unit. All internal units used Rectif ied\nLinear Unit (ReLU) activations, all output units had sigmoi d\nactivation. The size of the latent embedding was 128 units.\n5.4 VCCA training and prediction\nThe extended VCCA model was trained both intra-subject and\ncross-subject in an end-to-end fashion purely on the percep tion\ntrials using Adam optimization with a constant learning rat e of\n0.0001 [13]. For both datasets we used 60 % of available per-\nception trials for training and another 20 % for validation. The\nremaining 20 % and the imagination trials were used for testi ng.\nAll trials were shuffled randomly before training. For test s on\nimagination data, we evaluated both imagination trials who se\ncorresponding perception trials were included in the train ing as\nwell as entirely unknown trials. All models were trained up t o\nsaturation of the Mel spectrogram reconstruction loss, bet ween\n1000-2000 epochs. Reconstruction loss was computed as the\nmean squared error between reconstructions and targets.5.5 Introspection\nAfter training we inspected the learned latent space by line arly\ninterpolating between multiple existing EEG inputs extrac ted\neither from the training or testing dataset. This way, we\nreceived embeddings for the given inputs as well as a fixed\nnumber of embeddings that connect them in the learned\nprojection space. We then used the model to reconstruct the\nMel spectrogram and EEG signal for the embeddings.\n6. QUALITATIVE ANALYSIS\nOF MUSICAL STIMULUS RECONSTRUCTION\n6.1 Perceived stimulus reconstruction\nWe were able to use the modified VCCA model to reconstruct\nthe Mel spectrograms of perceived audio within both dataset s\nat various levels of accuracy. Figure 2 shows exemplary re-\nconstructions of speech and sine wave tone patterns for intr a-\nsubject training and testing on both trial types of the OpenM IIR\nspeech dataset. The reconstructions are characterized by r hyth-\nmical and timbral alignment with the target. In some cases we\nnoticed erroneous temporal shifts of the whole predicted rh yth-\nmical pattern within a reconstructed excerpt. Additional t ests\nwith smaller window sizes lead to a decrease in amount and siz e\nof such errors, while increasing the amount of false positiv e pre-\ndictions of both sine wave and speech patterns. In some cases\nspeech and sine wave patterns were mixed up, but still with co r-\nrect temporal alignment of note onset positions between tar get\nand predictions. Figure 3 shows reconstructions after training\non all subjects of the OpenMIIR speech dataset. Multi-subje ct\ntraining lead to results with improved temporal alignment o f tar-\ngets and predictions. Here, in more cases the two timbres (si ne\nwave and speech pattern) were confused. This indicates that the\ncorrect prediction of the timbre is more subject-specific t han\nthe temporal and rhythmical aspects. Increasing the amount of\ntraining data for both trials enhanced the overall reconstr uction\nquality, training only on the speech trials still lead to tem po-\nrally meaningful reconstructions of the sine wave tone patt erns.\nWe found the stimulus reconstruction quality to be best when\nincluding 4 subjects for cross-subject training and testin g.\nIncreasing the amount of dropout within the EEG decoder\n(up to 40 %) turned out to be crucial for reconstructions of\ncomparable quality for trials in subjects that were exclude d\nentirely from the training procedure. Training with random ized\nwindow start positions and using overlapping overlapping\nwindows proved to enhance the reconstruction quality. This\nsuggests that Mel spectrogram reconstruction quality for t his\ndataset is limited by the amount of available training data.\nCompared to the OpenMIIR dataset, the NMED-T dataset\nprovided more training data with increased target complexi ty.\nThe reconstructions showed different characteristic in vi sual\ninspection. Often times, the timbre reconstruction domina ted\nthe reconstruction of temporal aspects, especially in part s that\nfeatured multiple instruments or singing voice. In fewer ca ses,\nbut within all songs, the onsets of percussion, speech or oth er\nsounds were reconstructed. For all trained models, timbre\nreconstruction was visible after around 500 epochs, while\ntemporal aspects were learned at later stages. Figure 4 provides\nexamples for reconstructed excerpts of the perceived full- lengthProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 395Figure 2 . Mel spectrogram reconstructions of perceived rhyth-\nmical trials for the VCCA model trained on subject ’P13’ of\nthe OpenMIIR speech dataset. Target stimuli are presented\nabove their reconstructions.\nFigure 3 . Mel spectrogram reconstructions of perceived rhyth-\nmical trials for the VCCA model trained on all subjects of the\nOpenMIIR speech dataset. Target stimuli are presented abov e\ntheir reconstructions.\nsongs contained in the NMED-T dataset. We found no substan-\ntial difference in the quality of reconstructions within su bjects\nincluded into training and those from subjects excluded dur ing\ntraining. This might be due to the small amount and long dura-\ntion of 10 stimuli in combination with a single presentation per\nstimulus. Increasing the dropout rate after each convoluti onal\nlayer in the EEG encoder over 30 % increased the models\ntendency to reconstruct temporal aspects, such as percussi on\nonsets. Training sets with a larger amount of subjects gener ally\nFigure 4 . Excerpts of reconstructed Mel spectrograms from\nthe NMED-T dataset. The target stimuli are shown above their\nreconstructions. The two top rows are based on training on\nall subjects. The three bottom rows are based on training on\n10 subjects and testing on subjects that were excluded durin g\ntraining.\nimproved reconstruction quality. Furthermore, the introd uction\nof overlapping EEG input windows increased the amount\nof reconstructed temporal features. Models trained for mor e\nthan 2000 epochs showed more sparse reconstruction within\nthe frequency domain. This indicates that adding more data\nand increasing training length can further increase the rec on-\nstruction quality for naturalistic music. Often times, the size\nof temporal misalignments was equal at all positions within\nreconstructed excerpts. This indicates that the reconstru ction\nquality is dependent of the window size. Future work could\ntest this assumption by simultaneously training on EEG or\naudio excerpts of various sizes within different encoders o f\nthe VCCA model. This would furthermore allow the repre-\nsentation of the latent concepts to include contexts of vari ous\nsize. For example, in the audio domain, such contexts could\nrange from single note onsets to changes in song structure.\n6.2 Imagined stimulus reconstruction\nVCCA models trained on perceptual OpenMIIR speech data\ncould be applied to imagination trial reconstruction. The\nreconstructed stimuli showed the same typical rhythmical\npatterns and could be divided into speech and sine wave\npredictions. However, the correct rhythmical predictions\nwere less often visible and more blurry. It is important to\nnote that the imagination was performed superimposed on a\nconstant metronome click. This means, that only the differe nce396 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018between the rhythmical structure and timbre was based on\npure imaginative processes, while there were still percept ual\ncues for temporal alignment. Models trained on multi-subje ct\nperceptual data showed less blurry reconstructions. Addin g\nprivate encoders with imagination based EEG signal did not\ncause a visible increase in reconstruction quality.\n7. QUALITATIVE\nANALYSIS OF LEARNED AUDITORY CONCEPTS\nWe found musically meaningful representations of the\nOpenMIIR speech stimuli in the latent space of models\ntrained intra-subject as well as cross-subject. Both EEG si gnal\nfrom training and testing subsets could be used to produce\ncontinuous interpolation. Processing EEG inputs from both\ntesting and training data sets and using the target audio sti muli\nas validation, we found continuous representation across t he\ntemporal, rhythmical and timbral domain. For any given\ninput, we could change the temporal position of the rhythmic al\npattern as well as the timbre (within speech and sine wave\ntones). Furthermore, the latent space enabled interpolati on be-\ntween metronome clicks and the sine wave tones of increased\nloudness. However, this difference was found to a lesser deg ree\nwith data of the test set. Figure 5 (a) shows an example for\nthe interpolation between 3 embeddings based on EEG inputs\nof the OpenMIIR speech training data set. Here, interpolati on\nbetween a syncopated and non-syncopated part of the rhythm\nwas done while simultaneously shifting the temporal positi on\nof the rhythmical pattern within the reconstructed excerpt . The\nnon-syncopated excerpt was further interpolated into its r ep-\nresentation with speech signal. Figure 5 (b) shows topographic\nprojections of the brain activity reconstructed for each em bed-\nding that was computed in Subfigure (a). For the sake of clari ty\nwe show six topographic plots out of the total amount of 512\nper embedding. Qualitative comparison of the EEG signal wit h\nthe original inputs indicated that overfitting the EEG data is not\npossible when we stop training when the audio reconstructio n\nloss is saturated. For other use cases, higher quality EEG re con-\nstructions could be achieved with different training proce dures,\nsuch as unsupervised EEG reconstruction pretraining. Mode ls\nwith smaller latent embeddings sizes (e.g. 8 units) did stil l\nproduce meaningful and continuous interpolations, but wit h\nmore blurring across the temporal and frequency domains. Th e\nmodel forces EEG and audio to be shared even in these smaller\nlatent spaces. The neuroscientific meaningfulness of the E EG\nreconstructions might further be validated in future work, for\nexample with shared fMRI representation in private encoder s.\n8. CONCLUSIONS\nIn this paper, we presented the application of a multi-view\ngenerative model for shared auditory concept learning and\nmusical stimulus reconstruction from EEG signals. We showe d\nthat the model can learn representations of simple rhythm\nand timbre related concepts that are shared in audio and EEG\ndata. Furthermore, we could see first successes in approach ing\nnaturalistic music and imagined stimulus reconstruction. The\npresented framework is designed to be expandable to additio nal\nmodalities, such as fMRI data, or additional reconstructio n\nFigure 5 . (a) Reconstructed Mel spectrograms after interpola-\ntion in the learned latent space learned for Subject ’P13’ of the\nOpenMIIR speech dataset. Embeddings that correspond to rea l\nEEG inputs are framed. (b) Topographic visualization of the\nreconstructed temporal brain activity. Each row represent s the\nbrain activity reconstructed for the embedding in the same r ow\nof Subfigure (a).\ntargets, such as emotional aspects of music cognition. In\ncombination with the ability to perform introspection on\nthe shared representation of stimuli and electrophysiolog ical\nresponses, the model can be an aid for future EEG based\nmusic information retrieval and research in music cognitio n.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 3979. ACKNOWLEDGMENTS\nThe OpenMIIR speech dataset was kindly shared by the\nMusic and Neuroscience Lab at Western University in London,\nOntario. The authors especially would like to thank Avital\nSternin who recorded the data. This research has been\nfunded by the Federal Ministry of Education and Research\nof Germany (BMBF).\n10. REFERENCES\n[1]A. Aroudi, B. Mirkovic, M. De V os, and S. Doclo.\nAuditory attention decoding with eeg recordings using\nnoisy acoustic reference signals. In Acoustics, Speech and\nSignal Processing (ICASSP), 2016 IEEE International\nConference on , pages 694–698. IEEE, 2016.\n[2]S. Chandar, M. M. Khapra, H. Larochelle, and B. Ravin-\ndran. Correlational neural networks. Neural computation ,\n28(2):257–285, 2016.\n[3]L. K. Cirelli, D. Bosnyak, F. C. Manning, C. Spinelli,\nC. Marie, T. Fujioka, A. Ghahremani, and L. J. Trainor.\nBeat-induced fluctuations in auditory cortical beta-band\nactivity: using eeg to measure age-related changes.\nFrontiers in psychology , 5:742, 2014.\n[4]C. Du, C. Du, and H. He. Sharing deep generative repre-\nsentation for perceived image reconstruction from human\nbrain activity. In Neural Networks (IJCNN), 2017 Interna-\ntional Joint Conference on , pages 1049–1056. IEEE, 2017.\n[5]Y . Fujiwara, Y . Miyawaki, and Y . Kamitani. Modular\nencoding and decoding models derived from bayesian\ncanonical correlation analysis. Neural computation ,\n25(4):979–1005, 2013.\n[6]A. Gramfort, M. Luessi, E. Larson, D. A. Engemann,\nD. Strohmeier, C. Brodbeck, R. Goj, M. Jas, T. Brooks,\nL. Parkkonen, et al. Meg and eeg data analysis with\nmne-python. Frontiers in neuroscience , 7:267, 2013.\n[7]K. Hoenig, C. M ¨uller, B. Herrnberger, E.-J. Sim, M. Spitzer,\nG. Ehret, and M. Kiefer. Neuroplasticity of semantic\nrepresentations for musical instruments in professional\nmusicians. NeuroImage , 56(3):1714–1725, 2011.\n[8]H. Hotelling. Relations between two sets of variates.\nBiometrika , 28(3/4):321–377, 1936.\n[9]E. Jones, T. Oliphant, and P . Peterson. Scipy: open source\nscientific tools for python. 2014.\n[10] I. Kavasidis, S. Palazzo, C. Spampinato, D. Giordano,\nand M. Shah. Brain2image: Converting brain signals into\nimages. In Proceedings of the 2017 ACM on Multimedia\nConference , pages 1809–1817. ACM, 2017.\n[11] M. Kiefer and L. W. Barsalou. 15 grounding the human\nconceptual system in perception, action, and internal stat es.\nAction science: F oundations of an emerging discipline ,\npage 381, 2013.[12] M. Kiefer, E.-J. Sim, B. Herrnberger, J. Grothe, and\nK. Hoenig. The sound of concepts: four markers for a link\nbetween auditory and conceptual brain systems. Journal\nof Neuroscience , 28(47):12224–12230, 2008.\n[13] D. P . Kingma and J. Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980 , 2014.\n[14] D. P . Kingma and M. Welling. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114 , 2013.\n[15] S. Losorelli, D. T. Nguyen, J. P . Dmochowski, and\nB. Kaneshiro. Nmed-t: A tempo-focused dataset of cortical\nand behavioral responses to naturalistic music. ISMIR,\n2017.\n[16] B. McFee, C. Raffel, D. Liang, D. P . Ellis, M. McVicar,\nE. Battenberg, and O. Nieto. librosa: Audio and music\nsignal analysis in python. In Proceedings of the 14th\npython in science conference , pages 18–25, 2015.\n[17] M. Meyer, S. Baumann, and L. Jancke. Electrical brain\nimaging reveals spatio-temporal dynamics of timbre per-\nception in humans. Neuroimage , 32(4):1510–1523, 2006.\n[18] N. Moran. Social implications arise in embodied music\ncognition research which can counter musicological\nindividualism. Frontiers in psychology , 5:676, 2014.\n[19] R. N ¨a¨at¨anen and T. Picton. The n1 wave of the human\nelectric and magnetic response to sound: a review and an\nanalysis of the component structure. Psychophysiology ,\n24(4):375–425, 1987.\n[20] S. Nozaradan, I. Peretz, M. Missal, and A. Mouraux.\nTagging the neuronal entrainment to beat and meter.\nJournal of Neuroscience , 31(28):10234–10240, 2011.\n[21] S. Nozaradan, I. Peretz, and A. Mouraux. Selective\nneuronal entrainment to the beat and meter embed-\nded in a musical rhythm. Journal of Neuroscience ,\n32(49):17572–17581, 2012.\n[22] A. Ofner. Reconstruction of perceived and imagined\nmusic from eeg recordings with deep neural networks. In\nProceedings of the MEi:CogSci Conference 2017 , page 53.\n[23] F. Raposo, D. M. de Matos, R. Ribeiro, S. Tang, and Y . Y u.\nTowards deep modeling of music semantics using eeg\nregularizers. arXiv preprint arXiv:1712.05197 , 2017.\n[24] R. S. Schaefer, P . Desain, and P . Suppes. Structural\ndecomposition of eeg signatures of melodic processing.\nBiological psychology , 82(3):253–259, 2009.\n[25] A. Shahin, L. E. Roberts, C. Pantev, L. J. Trainor, and\nB. Ross. Modulation of p2 auditory-evoked responses by\nthe spectral complexity of musical sounds. Neuroreport ,\n16(16):1781–1785, 2005.\n[26] A. Sternin, S. Stober, J. Grahn, and A. Owen. Tempo\nestimation from the eeg signal during perception and\nimagination of music. In 1st International Workshop on\nBrain-Computer Music Interfacing/11th International398 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Symposium on Computer Music Multidisciplinary\nResearch (BCMI/CMMR15) , 2015.\n[27] S. Stober, D. J. Cameron, and J. A. Grahn. Using convo-\nlutional neural networks to recognize rhythm stimuli from\nelectroencephalography recordings. In Advances in neural\ninformation processing systems , pages 1449–1457, 2014.\n[28] S. Stober, A. Sternin, A. M. Owen, and J. A. Grahn. To-\nwards music imagery information retrieval: Introducing th e\nopenmiir dataset of eeg recordings from music perception\nand imagination. In ISMIR , pages 763–769, 2015.\n[29] D. Stuss, A. Toga, J. Hutchison, and T. Picton. Feedback\nevoked potentials during an auditory concept formation\ntask. In Progress in brain research , volume 54, pages\n403–409. Elsevier, 1980.\n[30] M. S. Treder, H. Purwins, D. Miklody, I. Sturm, and\nB. Blankertz. Decoding auditory attention to instruments\nin polyphonic music using single-trial eeg classification .\nJournal of neural engineering , 11(2):026009, 2014.\n[31] R. Vigo, M. Barcus, Y . Zhang, and C. Doan. On the\nlearnability of auditory concepts. The Journal of the\nAcoustical Society of America , 134(5):4064–4064, 2013.\n[32] W. Wang, X. Yan, H. Lee, and K. Livescu. Deep\nvariational canonical correlation analysis. arXiv preprint\narXiv:1610.03454 , 2016.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 399"
    },
    {
        "title": "Optical Music Recognition in Mensural Notation with Region-based Convolutional Neural Networks.",
        "author": [
            "Alexander Pacha",
            "Jorge Calvo-Zaragoza"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492393",
        "url": "https://doi.org/10.5281/zenodo.1492393",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/32_Paper.pdf",
        "abstract": "In this work, we present an approach for the task of optical music recognition (OMR) using deep neural networks. Our intention is to simultaneously detect and categorize musical symbols in handwritten scores, written in mensural notation. We propose the use of region-based convolutional neural networks, which are trained in an end-toend fashion for that purpose. Additionally, we make use of a convolutional neural network that predicts the relative position of a detected symbol within the staff, so that we cover the entire image-processing part of the OMR pipeline. This strategy is evaluated over a set of 60 ancient scores in mensural notation, with more than 15000 annotated symbols belonging to 32 different classes. The results reflect the feasibility and capability of this approach, with a weighted mean average precision of around 76% for symbol detection, and over 98% accuracy for predicting the position.",
        "zenodo_id": 1492393,
        "dblp_key": "conf/ismir/PachaC18",
        "keywords": [
            "optical music recognition",
            "deep neural networks",
            "musical symbols",
            "mensural notation",
            "region-based convolutional neural networks",
            "end-to-end training",
            "staff position prediction",
            "ancient scores",
            "symbol detection",
            "accuracy"
        ],
        "content": "OPTICAL MUSIC RECOGNITION IN MENSURAL NOTATION WITH\nREGION-BASED CONVOLUTIONAL NEURAL NETWORKS\nAlexander Pacha\nInstitute of Visual Computing and Human-\nCentered Technology, TU Wien, Austria\nalexander.pacha@tuwien.ac.atJorge Calvo-Zaragoza\nPRHLT Research Center\nUniversitat Polit `ecnica de Val `encia, Spain\njcalvo@upv.es\nABSTRACT\nIn this work, we present an approach for the task of opti-\ncal music recognition (OMR) using deep neural networks.\nOur intention is to simultaneously detect and categorize\nmusical symbols in handwritten scores, written in mensu-\nral notation. We propose the use of region-based convo-\nlutional neural networks, which are trained in an end-to-\nend fashion for that purpose. Additionally, we make use\nof a convolutional neural network that predicts the rela-\ntive position of a detected symbol within the staff, so that\nwe cover the entire image-processing part of the OMR\npipeline. This strategy is evaluated over a set of 60 ancient\nscores in mensural notation, with more than 15000 anno-\ntated symbols belonging to 32 different classes. The results\nreﬂect the feasibility and capability of this approach, with a\nweighted mean average precision of around 76% for sym-\nbol detection, and over 98% accuracy for predicting the\nposition.\n1. INTRODUCTION\nThe preservation of the musical heritage over the cen-\nturies makes it possible to study a certain artistic or cul-\ntural paradigm. Most of this heritage exists in written form\nand is stored in cathedrals or music libraries [10]. In addi-\ntion to the possible issues related to the ownership of the\nsources, this storage protects the physical preservation of\nthe sources over time, but also limits their accessibility.\nThat is why efforts are being made to improve this situa-\ntion through initiatives to digitize musical archives [17,21].\nThese digital copies can easily be distributed and studied\nwithout compromising their integrity.\nNevertheless, this digitalization, which indeed repre-\nsents a progress with respect to the aforementioned situ-\nation, is not enough to exploit the actual potential of this\nheritage. To make the most out of it, the musical content\nitself must be transcribed into a structured format that can\nbe processed by a computer [6]. In addition to indexing\nc\rAlexander Pacha, Jorge Calvo-Zaragoza. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Alexander Pacha, Jorge Calvo-Zaragoza. “Optical\nMusic Recognition in Mensural Notation with Region-based Convolu-\ntional Neural Networks”, 19th International Society for Music Informa-\ntion Retrieval Conference, Paris, France, 2018.the content and thereby enabling tasks such as content-\nbased search, this could also facilitate large-scale data-\ndriven musicological analysis in general [39].\nGiven that the transcription of sources is extremely\ntime-consuming, it is desirable to resort to automatic sys-\ntems. Optical music recognition (OMR) is a ﬁeld of re-\nsearch that investigates how to build systems that decode\nmusic notation from images. Regardless of the approach\nused to achieve such objective, OMR systems vary signif-\nicantly due to the differences amongst musical notations,\ndocument layouts, or printing mechanisms.\nThe work presented here deals with manuscripts writ-\nten in mensural notation, speciﬁcally with sources from\nthe 17th century, attributed to the Pan-Hispanic framework.\nAlthough this type of mensural notation is generally con-\nsidered as an extension of the European mensural notation,\nthe Pan-Hispanic situation of that time underwent a par-\nticular development that fostered the massive use of hand-\nwritten copies. Due to this circumstance, the need for de-\nveloping successful OMR systems for handwritten nota-\ntion becomes evident.\nFigure 1 . A sample page of ancient music, written in men-\nsural notation.\nWe address the optical music recognition of scores writ-\nten in mensural notation (see Figure 1) as an object detec-\ntion and classiﬁcation task. In this notation, the symbols\nare atomic units,1which can be detected and categorized\nindependently. Although there are polyphonic composi-\n1Except for beamed notes, in which the beam can be considered an\natomic symbol itself.240tions from that era, each voice was placed on its own page,\nso we can consider the notation as monophonic on the\ngraphical level. Assuming the aforementioned simpliﬁca-\ntions allows us to formulate OMR as an object detection\ntask in music score images, followed by a classiﬁcation\nstage that determines the vertical position of each detected\nobject within a staff. If the clef and other alterations are\nknown, the vertical position of a note encodes its pitch.\nWe propose using region-based convolutional neural\nnetworks, which represent the state of the art in computer\nvision for object detection, and demonstrate their capabili-\nties of detecting and categorizing the musical symbols that\nappear in the image of a music score with a high precision.\nWe believe that this work provides a solid foundation for\nthe automatic encoding of scores into a machine-readable\nmusic format like Music Encoding Initiative (MEI) [38]\nor MusicXML [15]. At present, there are thousands of\nmanuscripts of this type that remain to be digitized and\ntranscribed. Although each manuscript may have its own\nparticularities (such as the handwriting style or the lay-\nout organization), the approach developed in this work\npresents a common and extensible formulation to all of\nthem.\n2. RELATED WORK\nMost of the proposed solutions to OMR have focused on\na multi-stage approach [34]. This traditional workﬂow in-\nvolves steps that have been addressed isolatedly, such as\nimage binarization [4,47], staff and text segmentation [44],\nstaff-line detection and removal [5, 11, 46], and symbol\nclassiﬁcation [3, 30, 33]. In other works, a full pipeline is\nproposed for a particular type of music score [31, 32, 43].\nRecent works have shown that the image-processing\npipeline can largely be replaced with machine-learning ap-\nproaches, making use of deep learning techniques such\nas convolutional neural networks (CNNs) [1, 16, 29, 45].\nCNNs denote a breakthrough in machine learning, espe-\ncially when dealing with images. They have been applied\nwith great success to many computer vision tasks, often\nreaching or even surpassing human performance [18, 22].\nThese neural networks are composed of a series of ﬁlters\nthat operate locally (i.e. convolutions, pooling) and com-\npute various representations of the input image. These ﬁl-\nters form a hierarchy of layers, each of which represents\na different level of abstraction [20]. The key is that these\nﬁlters are not ﬁxed but learnt from the raw data through a\ngradient descent optimization process [23], meaning that\nthe network can learn to extract data-speciﬁc, high-level\nfeatures.\nHere, we formulate OMR for mensural notation as an\nobject detection task in music score images. Object detec-\ntion in images is one of the fundamental problems in com-\nputer vision, for which deep learning can provide excel-\nlent solutions. Traditionally, the task has been addressed\nby means of heuristic strategies based on the extraction of\nlow-level, general-purpose features such as SIFT [28] or\nHOG [7]. Szegedy and colleagues [8, 42] redeﬁned the\nuse of CNNs for object detection for the ﬁrst time. Insteadof classifying the image, the neural network predicted the\nbounding box of the object within the image. Around\nthe same time, the ground-breaking work of Girshick et\nal. [14] deﬁnitely changed the traditional paradigm. In\ntheir work, a CNN was in charge of predicting whether\neach object of the vocabulary appeared in selected bottom-\nup regions of the image. This scheme has been referred\nto as region-based convolutional neural network (R-CNN).\nAfterwards, several extensions and variations have been\nproposed with the aim of improving both the quality of the\ndetection and the efﬁciency of the process. Well-known\nexamples include Fast R-CNN [13], Faster R-CNN [37],\nR-FCN [24], SSD [27] or YOLO [35, 36].\nIn this work, we use these region-based convolutional\nneural networks for OMR, which are trained for the direct\ndetection and categorization of music symbols in a given\nmusic document. Thereby allowing for an elegant formula-\ntion of the task, since the training process only needs score\nimages along with their corresponding set of symbols and\nthe regions (bounding boxes) in which they appear.\n3. AN OMR-PIPELINE FOR MENSURAL SCORES\nMusic scores written in mensural notation share many\nproperties with scores written in modern notation: the se-\nquence of tones and pauses is captured as notes and rests\nwithin a reference frame of ﬁve parallel lines, temporally\nordered along the x-axis with the y-axis representing the\npitch of notes. But unlike modern notation, mensural\nscores are notated monophonically with a smaller vocabu-\nlary of only around 30 different glyphs, reducing the over-\nall complexity signiﬁcantly and thus allowing for a simpli-\nﬁed pipeline that consists of only three stages. A represen-\ntative subset of the symbols that appear in the considered\nnotation is depicted in Table 1.\nGroup Symbol\nNoteSemibrevis Minima Col. Minima Semiminima\nRestLonga Brevis Semibrevis Semiminima\nClefC Clef G Clef F Clef (I) F Clef (II)\nTimeMajor Minor Common Cut\nOthersFlat Sharp Dot Custos\nTable 1 . Subset of classes from mensural notation. The\nsymbols are depicted without considering their pitch or\nvertical position on the staff.\n3.1 Music Object Detection\nThe ﬁrst stage takes as input an entire high-quality image\nthat contains music symbols. The entire image is fed intoProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 241a deep convolutional neural network for object detection\nand yields the bounding boxes of all detected objects along\nwith their most likely class (e.g., g-clef ,minima ,ﬂat).\n3.2 Position classiﬁcation\nAfter detecting the symbols and classifying them, the sec-\nond stage performs position classiﬁcation of each detected\nobject to obtain the relative position with respect to the\nreference frame (staff) which is required to recover a notes\npitch. For this process, we extract a local patch from the\nfull image with the object of interest in the center and feed\nthe image into another CNN, which outputs the vertical\nposition, encoded as shown in Figure 2.\nL1L2\nL0L4L5\nL3L6\nS1S2\nS0S4S5\nS3S6\nFigure 2 . Encoding of the vertical staff line position into\ndiscrete categories. The ﬁve continuous lines in the middle\nform the regular staff and the dashed lines represent ledger\nlines, that are inserted locally as needed. A note between\nthe second and third line from the bottom would be classi-\nﬁed as S2 (orange).\n3.3 Semantics Reconstruction and Encoding\nGiven the detected objects and their relative position to the\nstaff line, the ﬁnal step is to reconstruct the musical se-\nmantics and encode the output into the desired format (e.g.,\ninto modern notation [48]). This step has to translate the\ndetected objects into an ordered sequence for further pro-\ncessing. Depending on the application and desired output,\nsemantic rules need to be taken care of, such as grouping\nbeams with their associated notes to infer the right duration\nor altering the pitch of notes when accidentals are encoun-\ntered.\n4. EXPERIMENTS\nTo evaluate the proposed approach, we conducted exper-\niments2for the ﬁrst two steps of the pipeline. While a\nfull system would also require the third step, we refrain\nfrom implementing it, to not restrict this approach to a par-\nticular applications. It is also noteworthy, that translating\nmensural notation into modern notation can be seen as its\nown ﬁeld of research that requires a deep understanding of\n2Source code is available at https://github.com/apacha/\nMensural-Detectorboth notational languages, which exceeds the scope of this\nwork.\n4.1 Dataset\nOur corpus consists of 60 fully-annotated pages in mensu-\nral notation from the 16th-18th century. The manuscript\nrepresents sacred music, composed for vocal interpreta-\ntion.3The compositions were written in music books by\ncopyists of that time. To ensure the integrity of the phys-\nical sources, the images were taken with a camera instead\nof scanning the books in a ﬂatbed scanner, leading to sub-\noptimal conditions in some cases. An overview of the con-\nsidered corpus is given in Table 2.\nPages 60\nTotal number of symbols 15258\nDifferent classes 32\nDifferent positions\nwithin a staff14\nAverage size of a\nsymbol ( w\u0002h)44\u000284pixels\nNumber of symbols per\nimage42–447 ( ?250)\nImage resolution\n(w\u0002h)\u00183000\u00022000 pixels\nDots per inch (DPI) 300\nTable 2 . Statistics of the considered corpus.\nThe ground-truth data is collected using a framework, in\nwhich an electronic pen is used to trace the music symbols,\nsimilar to that of [2]. The bounding boxes of the symbols\nare then obtained by computing the rectangular extent of\nthe users’ strokes.\n4.2 Setup\nOur experiments are based on previous research by [29],\nwhere a sliding-window-approach is used to detect hand-\nwritten music symbols in sub-regions of a music score. In\ncontrast to their work, we are able to detect hundreds of\ntiny objects in the full page within a single pass. To train\na network in a reasonable amount of time within the con-\nstraints of modern hardware, it is currently necessary to\nshrink the input image to be no longer than 1000px on the\nlongest edge, which corresponds to a downscaling opera-\ntion by a factor of three on our dataset.\nFor detecting music objects, the Faster R-CNN ap-\nproach [37] with the Inception-ResNet-v2 [41] feature ex-\ntractor has been shown to yield very good results for de-\ntecting handwritten symbols [29]. It works by having a\nregion-proposal stage for generating suggestions, where an\n3The dataset is subject to ongoing musicological research and can not\nbe made public at this point in time, so it is only available upon request.242 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018object might be, followed by a classiﬁcation stage, which\nconﬁrms or discards these proposals. Both stages are im-\nplemented as CNNs and trained jointly on the provided\ndataset. The ﬁrst stage scans the image linearly along a\nregular grid with user-deﬁned box proposals in each cell of\nthat grid.\nTo be able to generate meaningful proposals, the shape\nof these boxes has to be similar to the actual shape of the\nobjects that should be found. Since the image contains a\nlarge number of very tiny objects (sometimes only a few\npixels), a very ﬁne grid is required. After a statistical anal-\nysis of the objects appearing in the given dataset, including\ndimension clustering [35], several experiments were con-\nducted to study the effects of size, scale, and aspect ratios\nof the above-mentioned boxes, concluding that sensibly\nchosen priors for these boxes work similarly good as the\nboxes obtained from the statistical analysis. For the down-\nscaled image, boxes of 16x16 pixels, iterating with a stride\nof 8 pixels and using the scales 0.25, 0.5, 1.0, and 2.0, with\naspect ratios of 0.5, 1.0, and 2.0 represent a meaningful\ndefault conﬁguration. Accounting for the high density of\nobjects, the maximum number of box proposals is set to\n1200 with a maximum of 600 ﬁnal detections per image.\nFor the second step of our proposed pipeline, another\nCNN is trained to infer the relative position of an object\nto its staff line upon which it is notated (see Figure 2).\nDifferent off-the-shelf network architectures are evaluated\n(VGG [40], ResNet [19], Inception-ResNet-v2 [41]) with\nthe more complex models slightly outperforming the sim-\npler ones. Using pre-trained weights instead of random\ninitialization accelerates the training, improves the over-\nall result, and is therefore used throughout all experiments.\nThe input to the classiﬁcation network is a 224\u0002448pixels\npatch of the original image with the target object in the cen-\nter (see Figure 3). The exact dimensions of the patch are\nnot important, as long as the image contains enough verti-\ncal and horizontal context to classify even symbols notated\nabove or below the staff. When objects appear too close to\nthe border, the image is padded with the reﬂection along\nthe extended edge to simulate the continuation of the page\nas shown in Figures 3(d) and 3(e).\n(a)\n (b)\n (c)\n (d)\n (e)\nFigure 3 . Sample inputs for the position classiﬁcation net-\nwork depicting a g-clef (a),semiminima (b),brevis rest (c),\ncustos (d) and semibrevis (e), with vertical (d) and horizon-\ntal (e) reﬂections of the image to enforce the target object\nto be in the center, while preserving meaningful context.\nIt is important to notice that the vertical position de-\nﬁnes the semantical meaning only for some symbols (e.g.,the pitch of a note or the upcoming pitch with a custos ).\nClasses for which the position is either undeﬁned or not\nof importance include barlines ,fermatas , different time-\nsignatures ,beams and in particular for mensural notation:\ntheaugmentation dot . Symbols from these classes can be\nexcluded from the second step.\n4.3 Evaluation metrics\nConcerning the music object detection stage, the model\nprovides a set of bounding box proposals, as well as the\nrecognized class of the objects therein. The model also\nyields a score of its conﬁdence for each proposal. A bound-\ning box proposal Bpis considered positive if it overlaps\nwith the ground-truth bounding box Bgexceeding 60%,\naccording to the Intersection over Union (IoU) criterion:4\narea(Bp\\Bg)\narea(Bp[Bg)\nIf the recognized class matches the actual category of the\nobject, it is considered a true positive, being otherwise a\nfalse positive. Additional detections of the same object\nare computed as false positives as well. Those objects for\nwhich the model makes no proposal are considered false\nnegatives. Given that the prediction is associated with a\nscore, different values of precision andrecall can be ob-\ntained for each possible threshold. To obtain a single met-\nric, Average Precision (AP) can be computed, which is de-\nﬁned as the area under this precision-recall curve. An AP\nvalue can be computed independently for each class, and\nthen we provide the mean AP (mAP) as the mean across all\nclasses. Since our problem is highly unbalanced with re-\nspect to the number of objects of each class, we also com-\npute the weighted mAP (w-mAP), in which the mean value\nis weighted according to the frequency of each class. For\nthe second part of the pipeline (position classiﬁcation), we\nevaluate the performance with the accuracy rate (ratio of\ncorrectly classiﬁed samples).\n5. RESULTS\nBoth experiments yielded very promising results while\nleaving some room for improvement. The detection of\nobjects in the full image (see Figure 4) was evaluated by\ntraining on 48 randomly selected images and testing on the\nremaining 12 images with a 5-fold cross-validation. This\ntask can be performed very well and yielded 66% mAP\nand 76% w-mAP. When considering practical applications,\nthe weighted mean average precision indicates the effort\nneeded to correct the detection results, because it reﬂects\nthe fact that symbols from classes that appear frequently\nare generally detected better than rare symbols.\nWhen reviewing the error cases, a few things can be\nobserved: Very tiny objects such as the dot,semibrevis\nrestandminima rest pose a signiﬁcant challenge to the\nnetwork, due to their small size and extremely similar ap-\npearance (see Figure 5). This problem might be mitigated,\n4as deﬁned for the PASCAL VOC challenge [9]Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 243Figure 4 . Detected objects in the full image with the detected class being encoded as the color of the box. This example\nachieves a mAP of approximately 68% and a w-mAP of 85%.\n(a)\n (b)\n (c)\nFigure 5 . The smallest objects from the dataset that are\nhard to detect and often confused (from left to right): dot,\nsemibrevis rest , and minima rest .\nby allowing the network to access the full resolution im-\nage, which potentially has more discriminative information\nthan the downsized image. Unsurprisingly, classes that\nare underrepresented such as dots,barlines , or all types\nofrests are also frequently missed or incorrectly classiﬁed,\nleading to average precision rates of only 10–40% for these\nclasses.\nAnother interesting observation can be made, that in\nmany cases, objects were detected but the IoU with the\nunderlying ground-truth was too low for considering them\na true positive detection (see Figure 6 with a red box being\nvery close to a white box).\nFor the second experiment, a total of 13246 sym-bols were split randomly into a training (80%), valida-\ntion (10%) and test set (10%). The pre-trained Inception-\nResNet-v2 model is then ﬁne-tuned on this dataset and\nachieves over 98% accuracy on the test set of 1318 sam-\nples. Analyzing the few remaining errors reveals that the\nmodel makes virtually no errors and that the misclassiﬁed\nsamples are mostly human annotation errors or data incon-\nsistencies.\nFor inference, both networks can be connected in series.\nRunning both detection and classiﬁcation takes about 30\nseconds per image when running on a GPU (GeForce 1080\nTi) and 210 seconds on a CPU.\n6. CONCLUSION\nIn this work, we have shown that the optical music recogni-\ntion of handwritten music scores in mensural notation, can\nbe performed accurately and extendible by formulating it\nas an object detection problem, followed by a classiﬁcation\nstage to recover the position of the notes within the staff.\nBy using a machine learning approach with region-based\nconvolutional neural networks, this problem can be solved\nby simply providing annotated data and training a suitable\nmodel on that dataset. However, we are aware that our pro-\nposal still has room for improvement. In future work we\nwould like to:244 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018(a)\n (b)\n(c)\n (d)\nFigure 6 . Visualization of the performance of the object detection stage with selected patches of the music documents:\ngreen boxes indicate true positive detections; white boxes are false negatives, that the network missed during detection; red\nboxes are false positive detections, where the model reported an object, although there is no ground-truth; yellow boxes are\nalso false positives, where the bounding-box is valid, but the assigned class was incorrect.\n\u000fevaluate the use of different network architectures,\nsuch as feature pyramid networks [25,26], that might\nimprove the detection of small objects, which we\nhave identiﬁed as the biggest source of error at the\nmoment. These networks allow the use of high-\nresolution images directly, without the inherent in-\nformation loss, that is caused by the downscaling\noperation.\n\u000fmerge the staff position classiﬁcation with the object\ndetection network, by adding another output to the\nneural network, so the model simultaneously pre-\ndicts the staff position, the bounding box and the\nclass label.\n\u000fapply and evaluate the same techniques for other no-\ntations, including modern notation\n\u000fstudy models or strategies that reduce (or remove)\nthe need for speciﬁc ground-truth data of each type\nof manuscript. For example, unsupervised trainingschemes such as the one proposed in [12], which al-\nlows the network to adapt to a new domain by simply\nproviding new, unannotated images.\nWe believe that this research avenue represents a\nground-breaking work in the ﬁeld of OMR, as the pre-\nsented approach would potentially deal with any type of\nmusic scores by just providing undemanding ground-truth\ndata to train the neural models.\n7. ACKNOWLEDGEMENT\nJorge Calvo-Zaragoza thanks the support from the Eu-\nropean Union’s H2020 grant READ (Ref. 674943), the\nSpanish Ministerio de Econom ´ıa, Industria y Competitivi-\ndad through Juan de la Cierva - Formaci ´on grant (Ref.\nFJCI-2016-27873), and the Social Sciences and Humani-\nties Research Council of Canada.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 2458. REFERENCES\n[1] J. Calvo-Zaragoza and D. Rizo. End-to-End Neural\nOptical Music Recognition of Monophonic Scores. Ap-\nplied Sciences , 8(4):606–629, 2018.\n[2] J. Calvo-Zaragoza, D. Rizo, and J. M. I ˜nesta. Two\n(note) heads are better than one: pen-based multimodal\ninteraction with music scores. In 17th International\nSociety for Music Information Retrieval Conference ,\npages 509–514, 2016.\n[3] J. Calvo-Zaragoza, A. J. G. S ´anchez, and A. Pertusa.\nRecognition of Handwritten Music Symbols with Con-\nvolutional Neural Codes. In 14th IAPR International\nConference on Document Analysis and Recognition ,\npages 691–696, 2017.\n[4] J. Calvo-Zaragoza, G. Vigliensoni, and I. Fujinaga.\nPixel-wise binarization of musical documents with\nconvolutional neural networks. In 15th IAPR Inter-\nnational Conference on Machine Vision Applications ,\npages 362–365, 2017.\n[5] J. S. Cardoso, A. Capela, A. Rebelo, C. Guedes, and\nJ. P. da Costa. Staff detection with stable paths. IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence , 31(6):1134–1139, 2009.\n[6] G. S. Choudhury, M. Droetboom, T. DiLauro, I. Fu-\njinaga, and B. Harrington. Optical music recognition\nsystem within a large-scale digitization project. In 1st\nInternational Symposium on Music Information Re-\ntrieval , pages 1–6, 2000.\n[7] N. Dalal and B. Triggs. Histograms of oriented gradi-\nents for human detection. In IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 886–893,\n2005.\n[8] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov.\nScalable object detection using deep neural networks.\nInIEEE Conference on Computer Vision and Pattern\nRecognition , pages 2147–2154, 2014.\n[9] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.\nWilliams, J. Winn, and A. Zisserman. The PASCAL\nVisual Object Classes Challenge: A Retrospective. In-\nternational Journal of Computer Vision , 111(1):98–\n136, 2015.\n[10] I. Fujinaga, A. Hankinson, and J. E. Cumming. Intro-\nduction to SIMSSA (Single Interface for Music Score\nSearching and Analysis). In 1st International Work-\nshop on Digital Libraries for Musicology , pages 1–3,\n2014.\n[11] A.-J. Gallego and J. Calvo-Zaragoza. Staff-line re-\nmoval with selectional auto-encoders. Expert Systems\nwith Applications , 89:138–148, 2017.\n[12] Y . Ganin and V . Lempitsky. Unsupervised domain\nadaptation by backpropagation. In International Con-\nference on Machine Learning , pages 1180–1189, 2015.[13] R. Girshick. Fast R-CNN. In IEEE International Con-\nference on Computer Vision , pages 1440–1448, 2015.\n[14] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich\nfeature hierarchies for accurate object detection and se-\nmantic segmentation. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition ,\npages 580–587, 2014.\n[15] M. Good and G. Actor. Using MusicXML for ﬁle in-\nterchange. In Third International Conference on WEB\nDelivering of Music , page 153, 2003.\n[16] J. Haji ˇc Jr. and P. Pecina. Detecting Noteheads\nin Handwritten Scores with ConvNets and Bound-\ning Box Regression. Computing Research Repository ,\nabs/1708.01806, 2017.\n[17] A. Hankinson, J. A. Burgoyne, G. Vigliensoni, A.\nPorter, J. Thompson, W. Liu, R. Chiu, and I. Fujinaga.\nDigital Document Image Retrieval Using Optical Mu-\nsic Recognition. In Proceedings of the 13th Interna-\ntional Society for Music Information Retrieval Confer-\nence, pages 577–582, 2012.\n[18] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep\ninto rectiﬁers: Surpassing human-level performance on\nImageNet classiﬁcation. In IEEE International Confer-\nence on Computer Vision , pages 1026–1034, 2015.\n[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual\nlearning for image recognition. In IEEE International\nConference on Computer Vision and Pattern Recogni-\ntion, pages 770–778, 2016.\n[20] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet\nClassiﬁcation with Deep Convolutional Neural Net-\nworks. In Advances in Neural Information Processing\nSystems , pages 1106–1114, 2012.\n[21] A. Laplante and I. Fujinaga. Digitizing musical scores:\nChallenges and opportunities for libraries. In 3rd In-\nternational workshop on Digital Libraries for Musicol-\nogy, pages 45–48. ACM, 2016.\n[22] Y . LeCun, Y . Bengio, and G. Hinton. Deep learning.\nNature , 521(7553):436–444, 2015.\n[23] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner.\nGradient-based learning applied to document recog-\nnition. Proceedings of the IEEE , 86(11):2278–2324,\n1998.\n[24] Y . Li, K. He, J. Sun, et al. R-FCN: Object detec-\ntion via region-based fully convolutional networks. In\nAdvances in Neural Information Processing Systems ,\npages 379–387, 2016.\n[25] T.-Y . Lin, P. Dollar, R. Girshick, K. He, B. Hariharan,\nand S. Belongie. Feature Pyramid Networks for Ob-\nject Detection. In IEEE Conference on Computer Vi-\nsion and Pattern Recognition , 2017.246 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[26] T. Lin, P. Goyal, R. B. Girshick, K. He, and P. Doll ´ar.\nFocal Loss for Dense Object Detection. Computing Re-\nsearch Repository , abs/1708.02002, 2017.\n[27] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed,\nC.-Y . Fu, and A. C. Berg. SSD: Single shot multibox\ndetector. In European Conference on Computer Vision ,\npages 21–37, 2016.\n[28] D. G. Lowe. Distinctive image features from scale-\ninvariant keypoints. International Journal of Computer\nVision , 60(2):91–110, 2004.\n[29] A. Pacha, K.-Y . Choi, B. Co ¨uasnon, Y . Ricquebourg,\nR. Zanibbi, and H. Eidenberger. Handwritten music ob-\nject detection: Open issues and baseline results. In 13th\nIAPR Workshop on Document Analysis Systems , pages\n163–168, 2018.\n[30] A. Pacha and H. Eidenberger. Towards a Univer-\nsal Music Symbol Classiﬁer. In 12th IAPR Interna-\ntional Workshop on Graphics Recognition , pages 35–\n36, 2017.\n[31] L. Pugin. Optical music recognitoin of early typo-\ngraphic prints using hidden markov models. In 7th\nInternational Conference on Music Information Re-\ntrieval , pages 53–56, 2006.\n[32] C. Ramirez and J. Ohya. Automatic recognition\nof square notation symbols in western plainchant\nmanuscripts. Journal of New Music Research ,\n43(4):390–399, 2014.\n[33] A. Rebelo, A. Capela, and J. S. Cardoso. Optical\nrecognition of music symbols. International Journal\non Document Analysis and Recognition , 13(1):19–31,\n2010.\n[34] A. Rebelo, I. Fujinaga, F. Paszkiewicz, A. R. Mar-\ncal, C. Guedes, and J. S. Cardoso. Optical music\nrecognition: state-of-the-art and open issues. Interna-\ntional Journal of Multimedia Information Retrieval ,\n1(3):173–190, 2012.\n[35] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi.\nYou Only Look Once: Uniﬁed, Real-Time Object De-\ntection. In IEEE Conference on Computer Vision and\nPattern Recognition , pages 779–788, 2016.\n[36] J. Redmon and A. Farhadi. YOLO9000: Better, Faster,\nStronger. In IEEE Conference on Computer Vision and\nPattern Recognition , pages 6517–6525, 2017.\n[37] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-\nCNN: Towards real-time object detection with region\nproposal networks. In Advances in neural information\nprocessing systems , pages 91–99, 2015.\n[38] P. Roland. The music encoding initiative (MEI). In\nProceedings of the First International Conference on\nMusical Applications Using XML , pages 55–59, 2002.[39] X. Serra. The computational study of a musical culture\nthrough its digital traces. Acta Musicologica. 2017; 89\n(1): 24-44. , 2017.\n[40] K. Simonyan and A. Zisserman. Very Deep Convo-\nlutional Networks for Large-Scale Image Recogni-\ntion. Computing Research Repository , abs/1409.1556,\n2014.\n[41] C. Szegedy, S. Ioffe, V . Vanhoucke, and A. A. Alemi.\nInception-v4, Inception-ResNet and the Impact of\nResidual Connections on Learning. In 31st AAAI Con-\nference on Artiﬁcial Intelligence , pages 4278–4284,\n2017.\n[42] C. Szegedy, A. Toshev, and D. Erhan. Deep Neural\nNetworks for Object Detection. In Advances in Neural\nInformation Processing Systems 26 , pages 2553–2561.\n2013.\n[43] L. J. Tard ´on, S. Sammartino, I. Barbancho, V . G ´omez,\nand A. Oliver. Optical Music Recognition for Scores\nWritten in White Mensural Notation. EURASIP Jour-\nnal on Image and Video Processing , 2009(1):1–23,\n2009.\n[44] R. Timofte and L. Van Gool. Automatic stave discov-\nery for musical facsimiles. In Asian Conference on\nComputer Vision , pages 510–523, 2012.\n[45] E. van der Wel and K. Ullrich. Optical music recog-\nnition with convolutional sequence-to-sequence mod-\nels. In 18th International Society for Music Informa-\ntion Retrieval Conference , pages 731–737, 2017.\n[46] M. Visaniy, V . C. Kieu, A. Forn ´es, and N. Journet. The\nICDAR 2013 music scores competition: Staff removal.\nInInternational Conference on Document Analysis and\nRecognition , pages 1407–1411, 2013.\n[47] Q. N. V o, S. H. Kim, H. J. Yang, and G. Lee. An\nMRF model for binarization of music scores with com-\nplex background. Pattern Recognition Letters , 69:88–\n95, 2016.\n[48] Yu-Hui Huang, Xuanli Chen, Seraﬁna Beck, David\nBurn, and Luc J. Van Gool. Automatic Handwritten\nMensural Notation Interpreter: From Manuscript to\nMIDI Performance. In 16th International Society for\nMusic Information Retrieval Conference , pages 79–85,\n2015.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 247"
    },
    {
        "title": "Exploring Musical Relations Using Association Rule Networks.",
        "author": [
            "Renan de Padua",
            "Veronica Oliveira de Carvalho",
            "Solange O. Rezende",
            "Diego Furtado Silva"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492435",
        "url": "https://doi.org/10.5281/zenodo.1492435",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/268_Paper.pdf",
        "abstract": "Music information retrieval (MIR) has been gaining increasing attention in both industry and academia. While many algorithms for MIR rely on assessing feature subsequences, the user normally has no resources to interpret the significance of these patterns. Interpreting the relations between these temporal patterns and some aspects of the assessed songs can help understanding not only some algorithms' outcomes but the kind of patterns which better defines a set of similarly labeled recordings. In this work, we present a novel method to assess these relations, constructing an association rule network from temporal patterns obtained by a simple quantization process. With an empirical evaluation, we illustrate how we can use our method to explore these relations in a varied set of data and labels.",
        "zenodo_id": 1492435,
        "dblp_key": "conf/ismir/PaduaCRS18",
        "keywords": [
            "Music information retrieval",
            "interpreting temporal patterns",
            "association rule network",
            "quantization process",
            "empirical evaluation",
            "varied set of data",
            "similarly labeled recordings",
            "exploring relations",
            "novel method",
            "assessing relations"
        ],
        "content": "EXPLORING MUSICAL RELATIONS USING ASSOCIATION RULE\nNETWORKS\nRenan de Padua1;2Verˆonica Oliveira de Carvalho3\nSolange de Oliveira Rezende1Diego Furtado Silva4\n1Instituto de Ci ˆencias Matem ´aticas e de Computac ¸ ˜ao – Universidade de S ˜ao Paulo, S ˜ao Carlos, Brazil\n2Data Science Team, Ita ´u-Unibanco, S ˜ao Paulo, Brazil*\n3Instituto de Geoci ˆencias e Ci ˆencias Exatas – Universidade Estadual Paulista, Rio Claro, Brazil\n4Departamento de Computac ¸ ˜ao – Universidade Federal de S ˜ao Carlos, S ˜ao Carlos, Brazil\npadua@icmc.usp.br, veronica@rc.unesp.br, solange@icmc.usp.br, diegofs@ufscar.br\nABSTRACT\nMusic information retrieval (MIR) has been gaining in-\ncreasing attention in both industry and academia. While\nmany algorithms for MIR rely on assessing feature sub-\nsequences, the user normally has no resources to interpret\nthe signiﬁcance of these patterns. Interpreting the relations\nbetween these temporal patterns and some aspects of the\nassessed songs can help understanding not only some algo-\nrithms’ outcomes but the kind of patterns which better de-\nﬁnes a set of similarly labeled recordings. In this work, we\npresent a novel method to assess these relations, construct-\ning an association rule network from temporal patterns ob-\ntained by a simple quantization process. With an empirical\nevaluation, we illustrate how we can use our method to ex-\nplore these relations in a varied set of data and labels.\n1. INTRODUCTION\nDigital music repositories and streaming music services\nhave become increasingly popular in the last decades.\nAlong with this growth, algorithms to automatically orga-\nnize, navigate, and search on music collections are more\nand more necessary. For this reason, Music information\nretrieval (MIR) has been gaining considerable attention in\nboth industry and academia.\nThere is a multitude of MIR methods that rely on assess-\ning subsequences of features. In other words, these meth-\nods extract features from the audio in a sliding window\nfashion and use successive subsets of these features to take\ndecisions over the data. One example is the music genre\nclassiﬁcation, in which a common approach is to aggregate\n* The opinions expressed in this article are those of the authors\nand do not necessarily reﬂect the ofﬁcial policy or position of the Ita ´u-\nUnibanco.\nc\rRenan de Padua, Vernica Oliveira de Carvalho, Solange\nRezende, Diego Furtado Silva. Licensed under a Creative Commons At-\ntribution 4.0 International License (CC BY 4.0). Attribution: Renan de\nPadua, Vernica Oliveira de Carvalho, Solange Rezende, Diego Furtado\nSilva. “Exploring Musical Relations Using Association Rule Networks”,\n19th International Society for Music Information Retrieval Conference,\nParis, France, 2018.features from subsequences to obtain a more robust set of\nfeatures [2, 8]. Moreover, Silva et al. [12] showed how as-\nsessing distances between subsequences can be used as a\nsubroutine for different MIR tasks, from cover recognition\nto visualization.\nIn this work, we propose the use of a novel category\nof association rules networks to support understanding the\nrelations between sequential patterns and labels which de-\nscribe our data. The exploration of these association rules\nmay provide insights on what kind of pattern deﬁnes one\nlabel, which may have implications on musicology or other\nMIR tasks.\nFor instance, consider the genre as the target label. If\none pattern (or a group of patterns) happens with high con-\nﬁdence for only one label, this may help to explain the\ncharacteristics which deﬁne that genre. Also, it may guide\nus to understand how to improve music classiﬁcation al-\ngorithms. Besides, our method helps us to ﬁnd patterns\nshared between different labels. This kind of relation can\nbe used, for example, to improve music recommendation\nsystems, as well as provides insights on the musical inﬂu-\nences between different labels.\nFigure 1 illustrates one example of relations found by\nour method. It represents that, for a given dataset labeled\nwith genre information, when the pattern indexed by 10\nappears in a recording, we can say that recording belongs\nto the label “classical” with 100% of conﬁdence. Also, if\nthe patterns 23and4happens in the same recording, it be-\nlongs to the label “classical” with 94% of conﬁdence. The\npatterns correspond to quantized subsequences of features\n– Mel-Frequency Cepstrum Coefﬁcients (MFCC), in this\ncase – and can be assessed visually or by listening to the\nmusic excerpt that generated it.\nIn this paper, we introduce algorithms to represent as-\nsociation rules in a graph, aiming to provide a visual tool\nto understand the relations between the features that com-\npose it. Then, we apply our method on different datasets,\ndescribed by varied classes of labels, to demonstrate how\nto use this representation to understand the relations be-\ntween features and labels, as well as which patterns link\ntwo different labels.400Figure 1 : Example of association rule network\nThe remainder of this paper is organized as following.\nSection 2 introduces the main concepts of association rules\nand association rules networks, accompanied by related\nwork. The method presented in this work is presented in\nSection 3. Section 4 presents our experimental evaluation.\nFinally, Section 5 concludes this work.\n2. BACKGROUND AND RELATED WORK\nThe association rules were ﬁrst proposed by Agrawal et. al.\n[1]. The goal of the proposed approach was identifying, in\nsupermarket buying transactions, what were the items that\ncustomers used to buy together. This analysis was made\naiming to help the supermarket owners to organize their\nstock in order to raise the sales of some speciﬁc items. To\nunderstand how association rules discovery works, we ﬁrst\ndeﬁne some concepts related to it.\nDeﬁnition 1 Given a set of items I, a set of transaction T\nconsisting of subsets of I, an association rule is the relation\nA!B, where AandBare subsets of IandA\\B=;.\nAis called antecedent (or Left-Hand side - LHS) and B\nis called consequent (or Right-Hand side - RHS). The asso-\nciation rule can be read as: “given that Ahappened, Balso\nhappens inc%of the cases”, where c%is the association\nrule conﬁdence. Support ( s%) is another important mea-\nsure in the association rule, that describes the percentage\nof transactions in which all the items of the rule appear.\nDeﬁnition 2 The support \u001b(A) of a subset A\u001aIis deﬁned\nby the percentage of transactions that contain all the items\npresented in A.\nDeﬁnition 3 The conﬁdence of a rule A!Bis given by\nthe percentage of transactions that contain all the items in\nAthat also contain all the items in B. The conﬁdence is\ncalculated by\u001b(A[B)\n\u001b(A).\nAlso, the Lift is a widely used measure to assess the\nassociation rule quality. It evaluates if the items on the\nLHS are positively or negatively dependent with the items\non RHS, or if these sets are independent.\nDeﬁnition 4 The lift value of a rule A!Bis given by\nthe probability of AandBhappen together divided by the\nprobability of Atimes the probability of B, calculated by\n\u001b(A[B)\n\u001b(A)\u001b(B)The Association Rule Network (ARN) was proposed by\nChawla et. al. [5] and extended by Pandey et. al. [10] and\nby Chawla [4]. The ARN models all the association rules\nthat are directly or indirectly correlated to an speciﬁc item\n(called objective item) in a directed acyclic graph (DAG),\npruning all the other rules that are not interesting in the\nobjective item context. According to Pandey et. al. [10],\nthe ARN modeling is capable of pruning the rules into a\nspeciﬁc context, deﬁned by the selected objective item.\nAccording to Thulasiraman et. al. [14], a graph G =\n(V , E) consists of two sets: a ﬁnite set of vertices V and\na ﬁnite set of edges E. Each vertex represents an object\nin the graph and an edge represents a link between two\nvertices. Also, it is possible to deﬁne the graph G = (V , E,\nW), consisting of three sets: the V and E sets remain the\nsame, while the W set represents the weight of the edges\nin the graph G. In a graph that does not have weights, the\nW may have 1 where the connection exists and 0 where\nit does not exists. If the edges are ordered, i.e., the edges\nare identiﬁed as “from” vertex and “to” vertex, then it is\nsaid that the graph is directed because its edges contain a\ndirection. A Directed Acyclic Graph (DAG), is a particular\ntype of graph that contains no cycles.\nDeﬁnition 5 We say that a directed graph contains cycles\nif given a graph G containing N vertices V , the graph has a\npath that goes from vxtovyand there is also a path from\nvytovx.\nAn example of an ARN with objective item “G” is pre-\nsented on Figure 2. In this example, the following rules\nwere modeled: A!D,B!D,B!C,C & D!Eand\nE & F!G. All the other extracted rules were pruned be-\ncause they were not interesting in the context of the Gitem\nexploration.\nB\nAC\nDE\nFG\nFigure 2 : Example of ARN with objective item = G,\nadapted from Pandey et. al. [10].\nThe ﬁnal ARN is a directed acyclic graph that ﬂows to\nthe objective item, i. e., all elements on the graph have\ndirected connections that leads to an objective item. This\ngraph models the association rules that better explains the\noccurrence of the selected objective item. The modeling\ncan be used to build a hypothesis, based on the correlation\namong the items in the database and the objective item that\nthe user wants to understand.\nThe ARN algorithm can be described in 3 steps, de-\nscribed as follows.\nStep A Given a database D, a minimum support, and a\nminimum conﬁdence value, we extract the associa-\ntion rules using an algorithm, like apriori [1]. The\nRHS must have size 1.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 401Step B Considering all the items in the association rules’\nRHS, the user selects one item to represent the ob-\njective item.\nStep C Models all the rules that have the objective item\nin the RHS (considered level 0) or already modeled\non other levels. The modeling must fulﬁll the 2 re-\nstrictions: 1 - The LHS of the rule is not present in\nthe network or 2 - the LHS level is equal the RHS\nlevel + 1.\n3. EXTRACTING AND ASSOCIATING PATTERNS\nThe proposed method relies on two main steps, which we\ndescribe in details in this section. The ﬁrst one extracts\nframe-level features from the audio and quantize them to\ncreate a limited dictionary of patterns. With this pro-\ncedure, we transform the recordings in our database in\na transaction-like representation, where each recording is\nrepresented by the patterns presented in it. This represen-\ntation is used in the second step of our method.\nThe second one extracts and selects the best rules to de-\nscribe all the labels on the dataset. We extract the associa-\ntion rules, prune the rules which do not present interesting\nknowledge on the labels context and build a DAG, explain-\ning how the patterns correlates to the labels.\n3.1 Extracting and Quantizing Subsequence Patterns\nAs aforementioned, the ﬁrst step of our method relies on\nassociating each recording to one or more temporal pat-\nterns in a bag-of-patterns representation. For this, we split\nthis phase into different intermediate steps to create a dic-\ntionary and, then, associate each subsequence of features\nfrom each recording to a codeword in this dictionary.\nInitially, we extract frame-level Mel-Frequency Cep-\ntrum Coefﬁcients (MFCC) and Constant-Q chromagram\nfrom the raw audio. For this, we used the LibROSA pack-\nage for music and audio analysis [9]. Since our main pur-\npose is not comparing different parameter settings of each\nfeature extraction procedure, we applied the default param-\neters deﬁned by the tool. We chose these features since\nthey represent distinct characteristics of music. Speciﬁ-\ncally, the MFCC and chromagram are intrinsic to timbre\nand pitch information, respectively.\nTo associate each feature vector to a pattern, we ﬁrst\nneed to create a dictionary. For this, we applied the simple\nk-Means clustering algorithm on subsequences of the fea-\nture vectors. The centroid of each cluster deﬁnes one code-\nword, i.e., the prototype of a temporal pattern. For the sake\nof memory and time efﬁciency, we only used one-third of\nthe subsequences to estimate the centroids.\nOnce the codewords are deﬁned, we associate all fea-\nture subsequences of each song with codewords, accord-\ning to their proximity. In other words, for each recording,\nwe ﬁnd the nearest centroid of each subsequence of fea-\ntures and annotate it. At the end of this step, each record-\ning is described by the set of codewords that appear in its\nsubsequences. In the case of repetition, we remove these\nrecurrences.Although this step relies on deﬁning the number of\ncodewords, we leave details regarding this to Section 4.\n3.2Extended Association Rule Network\nThe Extended Association Rule Network (ExARN) aims\nto aid the user to understand the data and build a hypoth-\nesis from that data. The objective is to model the associa-\ntion rules in a graph, explaining the correlation among the\nattributes in the database according to a set of attributes\nselected by the user. For instance, consider the contact\nlens database1, which is aimed to automatically prescribes\ncontact lenses to patients. In this case, the user may be\ninterested not in the classiﬁcation, but in understanding\nwhich are the patient’s characteristics that deﬁne which\nkind of lens will be prescribed.\nThe ExARN is conceptually different from associative\nclassiﬁcation algorithms and decision trees, which build\nthe model only based on the classes, ignoring all other cor-\nrelations present on the database. The ExARN explores\na set of previously extracted association rules and, then,\nsearches for the best rules to describe the set of attributes\ndeﬁned by the user. Also, it has some interesting proper-\nties: i) it is built on a DAG, which means that there are no\ncycles on the network, ii) it is built on levels, every rule\nhas the LHS on level xand the RHS on level x+ 1, for\nexample, an objective attribute will mandatorily be on the\nRHS and on level 0, all the attributes on LHS that contains\nthat attribute on RHS will be on level 1 and so on.\nThe ExARN is built in three steps. The ﬁrst step con-\nsists of the association rule mining phase. The only re-\nstriction added to this step, if compared to a conventional\nassociation rule mining, is that the rules must an RHS with\nsize 1. This restriction was added, so each rule explains\nonly one attribute, reducing the complexity for the user to\nexplore the result.\nThe second step is the objective attribute deﬁnition.\nThis step will guide the entire exploration, as it will de-\nﬁne the objective attributes which the network will be built\nfrom. The user must select the attributes that will be ex-\nplored. This selection must be done considering also the\npossibility that these attributes have a common cause to be\nexplored or refuted.\nThe last step consists of the ExARN construction. This\nstep is responsible for getting all the rules that are directly\nor indirectly related to the objective attributes and model\nthem following the ExARN restrictions. The ExARN\nbuilding is done recursively. First, all the attributes se-\nlected as objective attributes are modeled in the graph on\nthe level 0. Then, all the rules that the LHS’ attributes are\nnot in the graph and have the RHS’ attributes on level 0 are\nmodeled on the network. The same process is done to all\nthe attributes on level 1, then to attributes on level 2 and\nso on. Until there are no more rules to be modeled. The\nExARN can be deﬁned as follows:\nDeﬁnition 6 Given a set of association rule R, containing\nrules with RHS of size 1, and a set of objective attributes\n1https://archive.ics.uci.edu/ml/datasets/\nlenses402 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Z with size\u00152, the ExARN is a DAG that models all the\nrules related to the items on Z, such as:\n1. Each vertex models a rule r \u001aR.\n2. From any point of the network, it is always possible\nto reach at least 1 vertex representing an attribute\nfrom Z.\n3. Given a vertex v \u001aExARN, such as v6\u001aZ. There is\nno path from any item on Z to v.\nThe ExARN presents a wider exploration if compared\nto the presented ARN because it allows the exploration of\n2 or more objective items at once. That way, the user might\ndiscover which patterns are interesting in the context of a\nsingle objective item, also discovering which patterns are\ninteresting for a set of objective items.\n4. EXPERIMENTAL EV ALUATION\nIn this section, we describe the experimental evaluation\nperformed to assess the ExARN in different scenarios of\nmusic data. We note that we made a supplementary web-\nsite2where we make available source codes and detailed\nresults, as well interactive visualizations of the networks\npresented in this section and some audio excerpts to exem-\nplify some of the mentioned patterns.\n4.1 Rule discovery and association setup\nAfter extracting the subsequence patterns, the database\npattern extraction begins. The sequence described here\nwas applied to all the databases.\nFirst, the association rules were extracted. We mined\nthe association rules using the arules package in R3. This\nstep needs the deﬁnition of some parameters. The support\nvalue, which is a threshold of minimum occurrence was\nset to 1%. This value was chosen because the databases\nwere divided into more than 10 different labels, so each\nsubpattern will have a maximum occurrence of1\nnumLabels\non each label. Deﬁning the minimum support to 1% will\nremove only the subpatterns that rarely happens. The\nother parameter is called conﬁdence, which can be deﬁned\nin terms of posterior probability as: Conf (A!B)=\nP(BjA). We deﬁned the minimum conﬁdence on 25%,\nwhich means that the B must happen in, at least, one-\nquarter of the occurrence of A.\nTo make sure that the association rules are positively\ndependent, we applied a ﬁlter using the lift measure. The\nthreshold was deﬁned at 2, as rules with lift value \u00151 are\nconsidered to have a positive dependency. We selected the\nvalue 2 instead of 1 as this value discards the rules that are\non the edge of the measure. Then, we applied the ExARN\nalgorithm over the association rules, considering all the la-\nbels as the objective items\n2https://sites.google.com/view/music-exarn\n3available at https://cran.r-project.org/web/\npackages/arules/index.html4.2 Datasets\nThe datasets used in our experimental evaluation aim to\nprovide us a diversity of characteristics and labels. For this\nreason, we used diversiﬁed datasets and for one of them,\nwe used different labels for the same bag-of-patterns.\nOne of the most common labels in MIR datasets is the\ngenre. We evaluated our method in this context using the\nGTZAN dataset [15]. This database is composed of 1000\nthirty-second tracks, perfectly balanced in ten genres.\nAnother way to categorize music data is according to\nthe artist who recorded it. We also evaluated our method\nin this scenario, using the Artist20 dataset [7]. This dataset\ncontains 1413 songs performed, as its name suggests, by\n20 artists mostly of pop or rock music. The number of\nrecordings is not balanced among the artists.\nFinally, we assessed the FMA dataset [6]. Moreover, we\ntook the fact that many of the recordings in these databases\nare associated with “social” features provided by Echon-\nest4to evaluate our method on varied labels for the same\ndata. Speciﬁcally, we applied our method targeting seven\ndistinct labels: acousticness, danceability, energy, instru-\nmentalness, liveness, speechness, and valence. In order\nto transform these continuous features in class-like values,\nwe discretized the features in ﬁve equally spaced intervals,\nrepresenting low, mid-low, mid, mid-high, and high levels\nof each characteristic. As we used the default small portion\nof this data and only kept information from the recordings\nassociated with Echonest features, we ended with 1023\ntracks.\n4.3 On the Impacts of the Codebook’s Size\nThe quantizing phase of our method has one parameter that\naffects the results of our method. The number of clusters to\ncreate the dictionary, i.e., the number of codewords, have\na direct impact on the conﬁdence. Particularly, the higher\nthe number of codewords, the lower the conﬁdence of the\nrules. Conversely, the lower the number of clusters, the\nhigher is the conﬁdence.\nAs we experimented with 25,50, and 100codewords\nin each dataset, we stick our analysis on the lower value.\nHowever, we notice that a high number of codewords may\nbe more appropriate for datasets with a high number of la-\nbels. Otherwise, the intersection between the labels would\nbe too high to ﬁnd meaningful rules. In this paper, the\nhigher number of assessed labels is 20.\nThere is another characteristic of using fewer code-\nwords regarding the interpretability of the results. As the\ncodewords are centroids, if we use too few clusters the\ncodewords will look “blurry” or few informative. How-\never, we noticed that it does not hamper the rule discovery\nand the music excerpts that were associated with each pat-\ntern can listen to a better understanding of what that pattern\nrepresents. Also, once the ExARN is computed, we can\nbreak a pattern Ain more parts, BandCfor instance, in\na procedure similar to the Bisect k-Means approach [13].\nWith this operation, we turn the patterns more speciﬁc.\n4http://the.echonest.com/Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 403Figure 3 : Examples of patterns in different resolutions,\ngiven by the number of codewords in the dictionary: 25\n(left) and 100 ( right )\nThen, instead of reading the association regarding A, one\nmay read it regarding “ BorC.” Figure 3 illustrates two\nchroma patterns obtained from different number of code-\nwords.\n4.4 Results and Discussion\nIn this section, we present some of the results obtained by\nour method. For simplicity, we split it into sections regard-\ning each analyzed dataset and the target labels of each of\nthem. Speciﬁcally, the results on GTZAN, Artist20, and\nFMA are presented in the sections regarding genre, artist,\nand the “social features” from Echonest.\nWe acknowledge that interpreting the patterns and, as\nconsequence, the meaning of some rules, only using tex-\ntual and static graphical elements is a difﬁcult matter. For\nthis reason, we make available on our website interactive\nvisualizations of ARNs obtained in our experiments, as\nwell as some music excerpts that are representative of rel-\nevant patterns.\nWe note that association rules discovery is an unsuper-\nvised task and, therefore, there is no quantitative evaluation\nmeasure to assess the quality of these rules. The only way\nto objectively evaluate the value of the learned rules would\nbe use it as an intermediate step of an algorithm to per-\nform other task, such as classiﬁcation or recommendation\nsystems. We leave this as an intention for future work.\n4.4.1 Genre\nUsing MFCC, we found a few interesting rules that asso-\nciate patterns with some of the target labels. One example\nis the one illustrated by Figure 1. We also found similar as-\nsociations to other genres. Speciﬁcally, for metal, reggae,\nand jazz. The latter two, however, with lower conﬁdences\n(around 33%).\nThe most interesting relations in this dataset come from\nthe patterns shared between distinct labels. Figure 4\npresents the entire network for this dataset when associ-\nating patterns representing 10 seconds of audio.\nSome of the patterns are associated with several genres\nin the presented network. These patterns are not suitable\nfor differing the characteristics of each genre. However,\nwe commonly see music elements that are used in songs\nbelonging to different genres. So, this kind of multiple\nrelations was expected. For instance, we observed a pattern\nassociated with the genres disco, pop, and hip-hop.\nOn the other hand, we found patterns that link pairs of\ngenres. These patterns directly associate two genres that\nFigure 4 : ExARN obtained by associating MFCC patterns\nfrom 10 seconds of audio in the GTZAN dataset\nhave somehow similar timbral information. It may help\nexplain the inﬂuence between genres or the mutual inﬂu-\nences of each pair. One example of two genres linked by\nthis kind of association is the pair metal and blues. An-\nother interesting relation regards the genres classical and\njazz. They have two patterns that are common for both.\nHowever, they usually happen together (i.e. in the same\nrecording) in classical pieces but separately in jazz songs.\nWe noticed that we did not achieved interesting associa-\ntions when using chroma features in this case.\n4.4.2 Artist\nIs there any link between Metallica and Roxette regarding\ntonal patterns in their songs? The answer is “yes, there is\nTori Amos.” Using subsequences of chroma vectors repre-\nsenting 10 seconds in the Artists20 dataset, we found that\nthese three artist have sets of four tonal patterns each that\nare conﬁdentially linked to each of them. Moreover, Tori\nAmos shares one of its patterns with Metallica and another\none with Roxette. Figure 5 illustrates these relations.\nFigure 5 : Subset of the rules obtained from chroma pat-\nterns in the Artist20 dataset\nThis kind of relation is commonly seen when using\nMFCC as the input features in this dataset. For instance,\nwhen applying the ExARN algorithm on ﬁve seconds ex-\ncerpts, we found rules with (at least) the minimum sup-404 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018port for seven artists. When using ten seconds excerpts,\nwe found rules for ten artists. In all of these cases, there is\nat least one timbral pattern for each artist which links it to\nanother one.\nWe notice that these links are relevant since they are not\ntrivial. In other words, if a timbral or chromatic pattern\nis present in many songs of several artists, the rules con-\ntaining it would have a very low support. Therefore, these\nlinks show how two (or more artists) are musically related\neach other by patterns that are not so commonly used.\n4.5 Echonest Labels\nWe evaluated the ExARN on seven different labels from\nEchonest. We found relevant rules in all of them but the\nspeechiness . For the other labels, the association rules net-\nwork demonstrated regularities in their behavior. For in-\nstance, when we assess the rules associated to a single la-\nbel, usually we cannot ﬁnd association with minimum sup-\nport for the intermediate values. This may happen because\nthe middle labels are fuzzy. In other words, the assessed\npatterns can describe solely the high and low characteris-\ntics at a minimum support. Figure 6 illustrates this fact\nregarding the acousticness.\nFigure 6 : Association rules from MFCC that are not\nshared by different intervals of acousticness\nWhen we analyze the rules that associate different la-\nbels, three main behaviors appear. The ﬁrst one is not ﬁnd-\ning patterns which relate different levels of these character-\nistics. Figure 7 illustrates the second, and most common,\nbehavior. In this case, the extremes are separate into dis-\ntinct components, i.e., the high and mid-high values are\nlinked by some patterns, similarly to what happens be-\ntween low and mid-low values.\nFinally, in some cases, the labels representing extreme\nvalues are directly linked by one or more patterns while\nsome patterns play the rule of “bridges” between these ex-\ntremes. Figure 8 illustrates this scenario.\n5. CONCLUDING REMARKS\nIn this paper we presented the use of extended association\nrules networks for exploring the correlation between tem-\nporal patterns and labels of music in different scenarios.\nTo evaluate the meaning of the discovered rules, we pre-\nsented some reasoning to verify the quality of these rules as\na qualitative approach. One example is evaluating the ex-\nistence of links between labels we consider similar to each\nother. In other cases, our rules may explicit some relations\nFigure 7 : Association rules from MFCC shared by differ-\nent intervals of energy\nFigure 8 : Association rules from chroma features shared\nby different intervals of energy\nthat are not obvious. In both cases, studying the patterns\nthat composes such relations can be useful to understand\nmusic data in several aspects. For instance, in some cases,\nwe could ﬁnd interesting relations using chroma vectors in\nscenarios where these features are not usually considered\n(e.g. to describe valence and energy).\nAs future work, we intend to improve the quantization\nstep so we reduce the impact of the codebook generation\nand we can ignore several patterns, considering them irrel-\nevant. For this, we may evaluate the use of some density-\nbased clustering strategy [3,11]. Also, we will evaluate the\nuse of ExARN as an intermediate step to improve recom-\nmendation systems. Finally, we intent to evaluate if this\nkind of association rules network can improve the inter-\npretability of music-related learned features.\n6. ACKNOWLEDGEMENT\nThis work was funded by Coordenac ¸ ˜ao de\nAperfeic ¸oamento de Pessoal de N ´vel Superior (CAPES)\nand S ˜ao Paulo Research Foundation (FAPESP) by grants\n#2013/26151-5, #2016/17078-0, and #2018/11755-6.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 4057. REFERENCES\n[1] Rakesh Agrawal, Tomasz Imielinski, and Arun Swami.\nMining association rules between sets of items in large\ndatabases. In ACM SIGMOD International Conference\non Management of Data , pages 207–216, 1993.\n[2] James Bergstra, Norman Casagrande, Dumitru Erhan,\nDouglas Eck, and Bal ´azs K ´egl. Aggregate features and\nAdaBoost for music classiﬁcation. Machine learning ,\n65(2-3):473–484, 2006.\n[3] Ricardo J. G. B. Campello, Davoud Moulavi, and J ¨org\nSander. Density-based clustering based on hierarchi-\ncal density estimates. In Paciﬁc-Asia Conference on\nKnowledge Discovery and Data Mining , pages 160–\n172, 2013.\n[4] Sanjay Chawla. Feature selection, association rules\nnetwork and theory building. In International Work-\nshop on Feature Selection in Data Mining , pages 14–\n21, 2010.\n[5] Sanjay Chawla, Bavani Arunasalam, and Joseph Davis.\nMining open source software (OSS) data using associ-\nation rules network. In Advances in Knowledge Dis-\ncovery and Data Mining , pages 461–466, 2003.\n[6] Micha ¨el Defferrard, Kirell Benzi, Pierre Van-\ndergheynst, and Xavier Bresson. Fma: A dataset for\nmusic analysis. In International Society for Music\nInformation Retrieval Conference , 2017.\n[7] Daniel P. W. Ellis. Classifying music audio with tim-\nbral and chroma features. In International Society for\nMusic Information Retrieval Conference , pages 339–\n340, 2007.\n[8] Philippe Hamel and Douglas Eck. Learning features\nfrom music audio with deep belief networks. In In-\nternational Society for Music Information Retrieval\nConference , pages 339–344. Utrecht, The Netherlands,\n2010.\n[9] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW\nEllis, Matt McVicar, Eric Battenberg, and Oriol Nieto.\nlibrosa: Audio and music signal analysis in python. In\nPython in Science Conference , pages 18–25, 2015.\n[10] Gaurav Pandey, Sanjay Chawla, Simon Poon, Bavani\nArunasalam, and Joseph G. Davis. Association rules\nnetwork: Deﬁnition and applications. Statistical Anal-\nysis and Data Mining , 1(4):260–279, 2009.\n[11] Alex Rodriguez and Alessandro Laio. Clustering\nby fast search and ﬁnd of density peaks. Science ,\n344(6191):1492–1496, 2014.\n[12] Diego F Silva, Chin-Chia M Yeh, Gustavo Enrique\nde Almeida Prado Alves Batista, and Eamonn Keogh.\nSimple: Assessing music similarity using subse-\nquences joins. In International Society for Music In-\nformation Retrieval Conference , pages 23–29, 2016.[13] Michael Steinbach, George Karypis, Vipin Kumar,\net al. A comparison of document clustering techniques.\nInACM SIGKDD Workshop on Text Mining , pages\n525–526, 2000.\n[14] K. Thulasiraman and M. N. S. Swamy. Graphs: Theory\nand Algorithms . 1992.\n[15] George Tzanetakis and Perry Cook. Musical genre\nclassiﬁcation of audio signals. IEEE Transactions on\nSpeech and Audio Processing , 10(5):293–302, 2002.406 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Musical Texture and Expressivity Features for Music Emotion Recognition.",
        "author": [
            "Renato Panda",
            "Ricardo Malheiro",
            "Rui Pedro Paiva"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492431",
        "url": "https://doi.org/10.5281/zenodo.1492431",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/250_Paper.pdf",
        "abstract": "We present a set of novel emotionally-relevant audio features to help improving the classification of emotions in audio music. First, a review of the state-of-the-art regarding emotion and music was conducted, to understand how the various music concepts may influence human emotions. Next, well known audio frameworks were analyzed, assessing how their extractors relate with the studied musical concepts. The intersection of this data showed an unbalanced representation of the eight musical concepts. Namely, most extractors are low-level and related with tone color, while musical form, musical texture and expressive techniques are lacking. Based on this, we developed a set of new algorithms to capture information related with musical texture and expressive techniques, the two most lacking concepts. To validate our work, a public dataset containing 900 30-second clips, annotated in terms of Russell's emotion quadrants was created. The inclusion of our features improved the F1-score obtained using the best 100 features by 8.6% (to 76.0%), using support vector machines and 20 repetitions of 10-fold cross-validation.",
        "zenodo_id": 1492431,
        "dblp_key": "conf/ismir/PandaMP18",
        "keywords": [
            "audio features",
            "emotional classification",
            "music concepts",
            "Russells emotion quadrants",
            "support vector machines",
            "cross-validation",
            "public dataset",
            "emotional relevance",
            "extractors",
            "musical form"
        ],
        "content": "MUSICAL TEXTURE AND EXPRESSIVITY FEATURES  FOR \nMUSIC EMOTION RECOGNITION  \nRenato Panda  Ricardo Malheiro  Rui Pedro Paiva  \nCISUC – Centre for Informatics and Systems , University of Coimbra, Portugal  \n{panda, rsmal, ruipedro} @dei.uc.pt  \nABSTRACT  \nWe present a se t of novel emotionally -relevant audio fea-\ntures to help improving  the classification of emotions in \naudio music. First, a review of the state-of-the-art regard-\ning emotion and music was conducted , to understand how \nthe various  music concepts may influence hu man emo-\ntions. Next, well know n audio frameworks were analyzed, \nassessing how their  extractors relate with the studied mu-\nsical concepts. The intersection of this data showed an un-\nbalanced representation of the eight musical concepts. \nNamely, most extr actors are low -level and related with \ntone color, while musical form, musical texture and ex-\npressive techniques are lacking. Based on this, we devel-\noped a set of new algorithms to capture information related \nwith musical texture and expressive techniques, the two \nmost lacking concepts. To validate our work, a public da-\ntaset containing  900 30 -second clips, annotated in terms of \nRussell’s emotion quadrants was created. The inclusion of \nour features improved the F1 -score obtained using the best \n100 fe atures by 8.6% (to 76.0%), using support vector ma-\nchines and 20 repetitions of 10 -fold cross -validation.  \n1. INTRODUCTION  \nMusic Emotion Recognition (MER) research has increased \nin the last decades, following the growth of  music data-\nbases and services . This interest is associated to  music’s \nability to “arouse deep and significant emotions”, being \n“its primary purpose and the ultimate reason why humans \nengage with it”  [1]. Different  problems have been tackled , \ne.g., music  classification [2]–[4], emotion tracking  [5], [6], \nplaylist s generation [7], [8], exploitation o f lyrical infor-\nmation  and bimodal approaches  [9]–[12]. Still, some limi-\ntations affect the entire MER field, among which : 1) the \nlack of public high -quality datasets, as used in other ma-\nchine learning fields to compare different works; and 2) \nthe insufficient number of emotionally -relevant acoustic \nfeatures , which we believe  are needed to narrow the exist-\ning semantic gap [13] and push the  MER  research  forward . \nFurthermore, both the state -of-the-art research papers \n                                                             \n1 http://ww w.music -ir.org/mirex/  (e.g., [14], [15]) and MIREX Audio Mood Classification \n(AMC) comparison1 result s from 2007 to 2017 are still not \naccurate enough in easier classification problems with four \nto five emotion classes, let alone higher granularity solu-\ntions and regression approaches, showi ng a glass ceiling in \nMER system performances [13].  \nMany of the audio features applied currently in MER  \nwere initially proposed  to solve other info rmation retrieval \nproblems  (e.g. MFCCs and LPCs in speech recognition \n[16]) and may lack emotional relevance. Therefore , we hy-\npothesize that, in order to advanc e the MER field, part of \nthe effort needs to focus on one key problem : the design of \nnovel audio features that better capture emotional content \nin music, currently left out by existing features.  \nThis raises the core question we aim to tackle in this \npaper:  can higher -level features, namely expressivity and \nmusical texture features, improve emotional content detec-\ntion in a song?  \nIn addition , we have constructed a dataset to validate \nour work, which we consider  better suited to the  current  \nMER state-of-the-art: avoids  overly complex or unvali-\ndated taxonomies, by using the four classes  or quadrants , \nderived from the Russell’s emotion model  [17]; does not \nrequire  a full manual annotation process, by using AllMu-\nsic annotations and data2, with a simpler human validation, \nthus reducing resources needed.  \nWe achieved an improvement of  up to 7.9% in F1 -Score  \nby adding our novel features  to the baseline set  of state -of-\nthe-art features . Moreover, even when the top 800 baseline \nfeatures is employ ed, the result is 4. 3% below the one ob-\ntained with the top100 baseline and novel features set.   \nThis paper is organized as follows. Section 2 reviews \nthe related work. Section 3 describes  the musical concepts \nand related state -of-the-art audio features . Dataset acquisi-\ntion, the novel audio features design and classification \nstrategies  are also presented . In Section 4, experimental re-\nsults are discussed. Conclusions and future work are drawn  \nin Section 5.  \n2. RELATED WORK  \nEmotions have been a research topic  for centuries, leading \nto the proposal of  different emotion paradigms (e.g., cate-\ngorical or dimensional) and associated  taxonomies (e.g., \n2 https://www.allmusic.com/moods   © Renato Panda, Ricardo Malheiro, Rui Pedro Paiva.  \nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution:  Renato Pand a, Ricardo Malheiro, Rui \nPedro Paiva . “Musical Texture and Expressivity Features for Music Emo-\ntion Recognition”, 19th International Society for Music Information Re-\ntrieval Conference, Paris, France, 2018.  \n383  \n \nHevner, Russell) [17], [18]. More recently, these have \nbeen employed in many MER computational systems, e.g., \n[2]–[7], [9], [12], [19], [20], and MER datasets, e.g., [4], \n[6], [20]. \nRegarding e motion in music , it can be view  as: i) the \nperceived  emotion , identifie d when listening; ii) emotion \nfelt, representing  the emotion felt w hen listening, which \nmay be different from the perceived ; iii) or the emotion \ntransmitted, which is the emotion a  performer intended  to \ndeliver . This work is focused on  perceived  emotion s, since \nit is more intersubjective, as opposed to emotion felt, more \npersonal and dependent of context, memories and culture.  \nAs for associations between emotions and musical at-\ntributes,  many features such as : articulation, dynamics, \nharmony, loud ness, melody, mode, musical form, pitch, \nrhythm, timbre, timing, tonality or vibrato  have  been pre-\nviously linked to emotion  [8], [21], [22]. However , many \nare yet to be  fully understood , still requiring further re-\nsearch , while others are hard to extract from audio signals. \nThese m usical attributes can be  organized into eight differ-\nent categories, e ach representing a core concept, namel y: \ndynamics, expressive techniques, harmony, melody, musi-\ncal form, musical texture, rhythm  and tone color (or tim-\nbre). Several  audio  features  have been created (hereinafter \nreferred to as standard audio or baseline features)  and are \nnowadays implemented  in audio frameworks  (e.g. \nMarsyas [23], MIR toolbox  [24] or PsySound  [25]). Even \nthough hundreds of features exist,  most belong to the same \ncategory – tone color, while others were developed to \nsolve previous research problems and thus might not be \nsuited for MER (e.g., Mel-frequency cepstral coefficients \n(MFCCs)  for speech recognition). On the other hand, the \nremaining categories are underrepresented,  with expres-\nsivity, musical texture or form nearly absent.  \nFinally, as opposed to other information retrieval fields, \nMER researchers lack standard public datasets and bench-\nmarks to compare existent works’ adequately. As a conse-\nquence, researchers use private datasets (e.g., [26]), or \nhave access o nly to features  and not the actual audio  (e.g., \n[27]). While efforts such as the MIREX AMC task  im-\nprove the situation, issues have been identified. To begin \nwith, the datase t is private, use in the annual contest only. \nAlso, it uses an unvalidated taxonomy derived from data \ncontaining semantic and acoustic overlap [3]. \n3. METHODS  \nIn this section, due to the abovementioned reasons, we start \nby introducing the dataset built to validate our work.  Fol-\nlowing, we detail  the proposed novel audio features and \nemotion  classification strategies tested.  \n3.1 Dataset Creation  \nTo bypass the limitations described in Section 2 we  have \ncreated a novel dataset based  using an accepted and vali-\ndated psychological model . We decided on Russell’s cir-\ncumplex model [17], which allo ws us to employ a simple \n                                                             \n1 http://developer.rovicorp.com/docs  taxonomy  of four emotion categories, based on the quad-\nrants resulting from the division b y the arousal and valence \n(AV) axes).   \nFirst, we obtained music data (30 -second audio clips) \nand metadata (e.g., artist, title, mood and genre ) from the \nAllMusic API1. The mood metadata consisted of several \ntags per song, from a list of 289 moods. These 289 tags are \nintersected with  the Warriner’s list [28] – an improvement \non ANEW adjectives list [29], containing 13915 English \nwords with AV ratings according to Ru ssell’s model. This  \nintersection results in 200 AllMusic tags mapped to AV, \nwhich can be translated to quadrants . Since we considered \nonly songs with three or more mood tags, each song is as-\nsigned t o the quadrant that has the hi ghest associated num-\nber of t ags (and at least 50% of the moods are from it).  \nThe AllMusic emotion tagging process  is not fully doc-\numented , apart from apparently  being made by experts \n[30]. Questions re main on whether these experts are con-\nsidering only audio, only lyrics or a combination of both. \nBesides , the 30 -second clips selection that represent each \nsong  in AllMusic  is also undocumented . We observed sev-\neral inadequate  clips (e.g., containing noise s uch as ap-\nplauses, only speech, long silences  from introductions ). \nTherefore , a manual blind validation  of the candidate set \nwas conducted. Subjects were given sets of randomly dis-\ntributed clips and asked to annotate them according to the \nperceived emotion  in terms of Russell’s quadrants.  \n The final dataset was built by removing the clips where \nthe subjects’ and AllMusic derived quadrants’ annotations \ndid not match. The dataset was rebalanced to contain ex-\nactly 225 clips and metadata per cluster, in a total of 900 \nsong entries, which is publicly available in our site2. \n3.2 Standard or Baseline Audio Features  \nMarsyas, MIR Toolbox and PsySound 3, three state-of-the-\nart audio frameworks  typically used in MER studies,  were \nused to extract a total of 1702 features . Thi s high number  \nis in part due to the computation of  several statistical for \nthe resulting time series data.  To reduce this and avoid pos-\nsible feature duplication across differ ent frameworks, first \nwe obtained the weight of each feature to the problem us-\ning ReliefF [31] feature selection algorithm. Next, we cal-\nculated the correlation between each pair of features, re-\nmoving the lowest weight one for each pair with a correla-\ntion hig her than 0.9. This process reduced the standard \naudio features  set to 898 features, which was used to \ntrain baseline models. These models were then used to \nbenchmark models trained with the baseline and novel \nfeature sets. An analogous  feature reduction procedure \nwas also performed in th e novel features set presented \nin Section 3.3 . \n3.3 Novel Audio Features  \nAlthough being used constantly in MER problems, many \nof the standard audio features are  very low -level,  extract-\ning abstract metrics from the spectrum  or dir ectly from the \naudio waveform . Still, humans naturally perceive higher -\nlevel musical concepts such as rhythm, harmony, melody \n2 http://mir.dei.uc.pt/downloads.html  384 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018  \n \nlines  or expressive techniques based on clues related with \nnotes, intervals or scores. To propose novel features that \nrelated to t hese higher -level concepts  we built on previous \nworks to estimate musical notes and extract frequency and \nintensity contours. We briefly describe this initial step in \nthe next section.  \n3.3.1 Estimating MIDI notes  \nAutomatic transcription of music audio signals to  scores is \nstill and open research problem [32]. Still, we consider that \nusing such existing algorithms, although imperfect, pro-\nvide important information currently unused in MER.  \nTo this end, we built on works  by Salomon et al. [33] \nand Dressler [34] to estimate predominant fundamental \nfrequ encies (f0) and saliences. This process starts by iden-\ntifying the frequencies present in the signal at each point \nin time (sinusoid extraction) , using  46.44 msec (1024 sam-\nples) frames with 5.8 msec (128 samples) hopsize (hereaf-\nter denoted ℎ𝑜𝑝). Next, the  pitches in each of these mo-\nments are estimated using harmonic summation (obtaining \na pitch salience function). Then, pitch contours are created \nfrom the series of consecutive pitches, representing notes \nor phrases.  Finally, a set of rules  is used to selec t the f0s \nthat are part of the predominant melody [33]. The resulting \npitch trajectories are then segmented into individual MIDI \nnotes following the work by Paiva et al. [35].  \nEach of the N obtained notes, hereafter denoted \nas 𝑛𝑜𝑡𝑒 𝑖, is characterized by: 1) the respective sequence of \nf0s (a total of 𝐿𝑖 frames),  𝑓0𝑗,𝑖,𝑗=1,2,…𝐿𝑖; the corre-\nsponding MIDI note numbers (for each f0), 𝑚𝑖𝑑𝑖 𝑗,𝑖; 2) the \noverall MIDI note value (for the entire note), 𝑀𝐼𝐷𝐼 𝑖; 3) the \nsequence of pitch saliences, 𝑠𝑎𝑙 𝑗,𝑖; 4) the note duration, \n𝑛𝑑 𝑖 (sec); starting time, 𝑠𝑡𝑖 (sec); and 5) ending time, 𝑒𝑡𝑖 \n(sec). This data is used to model higher level concepts re-\nlated with expressive techniques,  such as vibrato.  \nIn addition to the predominant melody, music typically \ncontains other melodic lines produced by distinct sources.  \nSome researchers have also  proposed algorithms to  multi-\nple (also known as polyphonic) F0 contours estimation \nfrom these cons tituent sources. We use Dressler’s multi -\nF0 approach [34] to obt ain a framewise sequence of fun-\ndamental frequencies estimates  to assess musical texture . \n3.3.2 Musical texture features  \nPrevious studies have verified that musical texture can in-\nfluence emotion in music, either directly or in combination \nwith tempo and mode [36]. However, as stated in Section \n2, very few of the available audio features are directly re-\nlated with this musical concept. Thus, we propose features \nto capture information related with the  musical layers of a \nsong , based on the  simultaneous layers in each frame using \nthe multiple frequency estimates described above.  \nMusical Layers (ML) statistics.  As mentioned , vari-\nous multiple F0s are estimated from each audio frame. \nThen , we define the number of layers in a frame as the \nnumber of obtained multiple F0s in that frame. The  ob-\ntained data series, representing the number of musical lay-\ners in each instant during the clip , is then summarized us-\ning six statistics: mean  (MLmean) , standard deviation  \n(MLstd) , skewness  (MLskw) , kurtosis  (MLkurt) , maxi-\nmum  (MLmax)  and minimum  (MLmin)  values.  The same six statistics are applied similarly to the other proposed \nfeatures . \nMusical Layers Distribution (MLD).  Here, the num-\nber of 𝑓0 estimates in each frame is categorized in one of  \nfour c lasses: i) no layers; ii) a single layer; iii) two simul-\ntaneous layers; iv) and three or more layers. The percent-\nage of frames in each of these four classes is computed, \nmeasuring, as an example, the percentage of the song iden-\ntified as having a single lay er (MLD1). Similarly, we com-\npute MLD0, MLD2 and MLD3.  \nRatio of Musical Layers Transitions (RMLT).  These \nfeatures capture the amount of transitions (changes) from \na specific musical layer sequence to another (e.g., ML1 to \nML2). To this end, we count  consec utive frames having  \ndistinct numbers of fundamental frequencies (f0s)  esti-\nmated in each as a transition. The total number of  these \ntransitions  is normaliz ed by the length of the audio seg-\nment (in secs). Additionally , we also compute the length \nin seconds o f the longest audio segment for each of the four \nmusical layer s classes . \n3.3.3 Expressivity features  \nExpressive techniques such as vibrato, tremolo and articu-\nlation  are used frequently by composers and performers , \nacross different genres.  Some studies have linke d them to \nemotions [37]–[39], still th e number of standard audio fea-\ntures studied that are primarily related with expressive \ntechniques is low . \n \nArticulation Features  \nArticulation relates to how specific notes are played and \nexpressed together . To capture this , we first detect  legato \n(i.e., co nnected notes played “smoothly”) and staccato \n(i.e., short and detached notes), as defined  in Algorithm 1. \nUsing this, we classify all the transitions between notes in \nthe song clip and, from them, extract several metrics such \nas: ratio of staccato, legato  and other  transitions, longest \nsequence of each articulation type, etc.  \n \nALGORITHM  1 \nARTICULATION DETECTION . \n1. For each pair of consecutive notes, 𝑛𝑜𝑡𝑒𝑖 and 𝑛𝑜𝑡𝑒𝑖+1: \n1.1. Compute the inter -onset interval ( IOI, in sec), i.e., the interval \nbetween the onsets of the two notes, as: 𝐼𝑂𝐼 =𝑠𝑡𝑖+1−𝑠𝑡𝑖. \n1.2.  Compute the inter -note silence ( INS, in sec), i.e., the duration of \nthe silence seg ment between the two notes, as follows: 𝐼𝑁𝑆 =\n 𝑠𝑡𝑖+1−𝑒𝑡𝑖. \n1.3. Calculate the ratio of INS to IOI ( INStoIOI ), which indicates how \nlong the interval between notes is , compared to the duration of \n𝑛𝑜𝑡𝑒𝑖. \n1.4. Define the articulation between 𝑛𝑜𝑡𝑒𝑖 and 𝑛𝑜𝑡𝑒𝑖+1, 𝑎𝑟𝑡𝑖, as: \n1.4.1.  Legato , if the distance between notes is less than 10 msec, \ni.e., 𝐼𝑁𝑆 ≤0.01⇒𝑎𝑟𝑡𝑖=1. \n1.4.2.  Staccato , if the duration of 𝑛𝑜𝑡𝑒𝑖 is short (i.e., less than \n500 msec) and the silence between the two notes is  rela-\ntively similar to this duration, i.e., 𝑛𝑑𝑖<0.5 ∧0.25≤\n𝐼𝑁𝑆𝑡𝑜𝐼𝑂𝐼 ≤0.75 ⇒𝑎𝑟𝑡𝑖=2. \n1.4.3.  Other Transitions , if none of the abovementioned two \nconditions was met ( 𝑎𝑟𝑡𝑖=0). \n \nIn Algorithm 1, the employed threshold values were set Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 385  \n \nexperimen tally. Then, we define the following features:  \nStaccato Ratio (SR), Legato Ratio (LR) and Other \nTransitions Ratio (OTR).  These features indicate the ra-\ntio of each articulation type (e.g., staccato) to the total \nnumber of transitions between notes.  \nStaccato  Notes Duration Ratio (SNDR), Legato \nNotes Duration Ratio (LNDR) and Other Transition \nNotes Duration Ratio (OTNDR) statistics.  These repre-\nsent statistics based on the duration of notes for each artic-\nulation type . As an example, with staccato (SNDR) , the ra-\ntio of the duration of notes with staccato articulation  to the \nsum of the duration of all notes, as in  Eq. 1. For each , the \n6 statistics described in Section 3.3.2 are calculated.  \n \n𝑆𝑁𝐷𝑅 = ∑ [𝑎𝑟𝑡 𝑖=1]∙𝑛𝑑 𝑖𝑁−1\n𝑖=1\n∑ 𝑛𝑑 𝑖𝑁−1\n𝑖=1 (1) \n \nGlissando Features  \nGlissando is another expressive articulation,  which is  the \nslide  from one note to another. Normally used as a n orna-\nmentation, to add interest to a piece , may be related to spe-\ncific emotions in music.   \nWe assess glissando by analyzing the transition be-\ntween two notes , as described in Algorithm 2 . This transi-\ntion part is saved at the beginning of the second note  by \nthe segmentation method applied (mentioned in Section \n3.3.1) [35]. The second note must start with a  climb or de-\nscent, of at least 100 cents, which may  contain spikes and \nslight oscillations in frequency estimates, followed by a \nstable sequence .  \n \nALGORITHM  2 \nGLISSANDO DETECTION . \n1. For each note i: \n1.1. Get the list of unique MIDI note numbers, 𝑢𝑧,𝑖,𝑧=1,2,⋯,𝑈𝑖, \nfrom the corresponding sequence of MIDI note numbers (for each \nf0), 𝑚𝑖𝑑𝑖𝑗,𝑖, where 𝑧 denotes a distinct MIDI note number (from \na total of 𝑈𝑖 unique MIDI note numbers).  \n1.2. If there are at least two unique MIDI note numbers:  \n1.2.1.  Find the start of the steady -state region, i.e., the index, 𝑘, \nof the first note in the MIDI note numbers sequence, \n𝑚𝑖𝑑𝑖𝑗,𝑖 , with the same value as the overall MIDI \nnote,  𝑀𝐼𝐷𝐼𝑖, i.e.,  𝑘= min\n1≤𝑗≤𝐿𝑖, 𝑚𝑖𝑑𝑖 𝑗,𝑖=𝑀𝐼𝐷𝐼 𝑖𝑗, \n1.2.2.  Identify the end of the glissando segment as the first index, \n𝑒, before the steady -state region, i.e., 𝑒=𝑘−1. \n1.3. Define  \n1.3.1.  𝑔𝑑𝑖 = glissando duration (sec) in note i, i.e., 𝑔𝑑𝑖 = 𝑒 ∙\nℎ𝑜𝑝. \n1.3.2.  𝑔𝑝𝑖 = glissando presence in note i, i.e., 𝑔𝑝𝑖 =1 if  𝑔𝑑𝑖>\n0;0,otherwise .  \n1.3.3.  𝑔𝑒𝑖  = glissando extent in note i, i.e., 𝑔𝑒𝑖=|𝑓01,𝑖−\n𝑓0𝑒,𝑖| in cents.  \n1.3.4.  𝑔𝑐𝑖 = glissando coverage of note i, i.e., 𝑔𝑐𝑖= 𝑔𝑑𝑖/𝑑𝑢𝑟𝑖. \n1.3.5.  𝑔𝑑𝑖𝑟𝑖  = glissa ndo direction of note i, i.e., 𝑔𝑑𝑖𝑟𝑖=\n 𝑠𝑖𝑔𝑛 (𝑓0𝑒,𝑖−𝑓01,𝑖). \n1.3.6.  𝑔𝑠𝑖  = glissando slope of note i, i.e., 𝑔𝑠𝑖= 𝑔𝑑𝑖𝑟𝑖∙𝑔𝑒𝑖/\n𝑔𝑑𝑖. \n \nBased on the output of  Algo rithm 2 we define:  \nGlissando Presence (GP).  A song clip c ontains glis-\nsando if any of its notes has glissando, as in ( 2). \n𝐺𝑃={1,if ∃ 𝑖 ∈ {1,2,…,𝑁}∶ 𝑔𝑝𝑖=1\n0,otherwise    (2) \n \nIf GP = 1, w e then compute the remaining glissando features.  \nGlissando Extent (GE) statistics.  Using the  glissando \nextent  of each note, 𝑔𝑒𝑖 (see Algorithm 2), we compute the \n6 statistics (Section 3.3.2) for notes containing glissando.  \nGlissando Duration (GD) and Glissando Slope (GS) \nstatistics.  Similarly to  GE, we also compute the same sta-\ntistics for glissando duration , based on 𝑔𝑑 𝑖 and slope, \nbased on 𝑔𝑠𝑖 (see Algorithm 2).  \nGlissando Coverage (GC).  For glissando coverage, we \ncompute the global coverage, based on 𝑔𝑐𝑖, using ( 3). \n \n𝐺𝐶=∑ 𝑔𝑐𝑖∙𝑛𝑑 𝑖𝑁\n𝑖=1\n∑ 𝑛𝑑 𝑖𝑁\n𝑖=1    (3) \n \nGlissa ndo Direction (GDIR).  This feature indicates \nthe global direction of the glissandos in a song, ( 4): \n \n𝐺𝐷𝐼𝑅 =∑ 𝑔𝑝𝑖𝑁\n𝑖=1\n𝑁,𝑤ℎ𝑒𝑛 𝑔𝑑𝑖𝑟 𝑖=1    (4) \n \nGlissando to Non -Glissando Ratio (GNGR).  This \nfeature represents the ratio of the notes  containing glis-\nsando to the total number of notes, as in ( 5): \n𝐺𝑁𝐺𝑅 =∑ 𝑔𝑝𝑖𝑁\n𝑖=1\n𝑁    (5) \n \nVibrato and Tremolo Features  \nVibrato and tremolo are  expressive technique used in vocal \nand instrumental music . Vibrato  consists in a steady  oscil-\nlation of pitch  in a note or sequence of notes . Its properties \nare the: 1) the velocity (rate) of pitch variation ; 2) amount \nof pitch variation (extent) ; and 3) duration . It varies across  \nmusic styles and emotional expression [38].  \nGiven its possible relevance to MER, w e apply the vi-\nbrato detection algorithm described in Algorithm 3, which \nwas adapted from [40]. We then compute features  such as \nvibra to presence, rate, coverage and extent . \n \nALGORITHM  3 \nVIBRATO DETECTION . \n1. For each note i: \n1.1. Compute the STFT, |F0𝑤,𝑖|,𝑤=1,2,⋯,𝑊𝑖,   of the sequence \n𝑓0𝑖 , where 𝑤  denotes an analysis window (from a total of 𝑊𝑖 \nwindows). Here, a 371.2  msec ( 128 samples) Blackman -Harris \nwindow was employed, with 185.6  msec ( 64 samples) hopsize.  \n1.2. Look for a prominent p eak, 𝑝𝑝𝑤,𝑖, in each analysis window, in the \nexpected range for vibrato. In this work, we employ the typical \nrange for vibrato in the human voice, i.e., [5, 8] Hz [40]. If a peak \nis detected, the corresponding window contains vibrato.  \n1.3. Define:  \n1.3.1.  𝑣𝑝𝑖 = vibrato presence in note i, i.e.,  \n𝑣𝑝𝑖=1 if ∃ 𝑝𝑝𝑤,𝑖;  𝑣𝑝𝑖=0,otherwise . \n1.3.2.  𝑊𝑉𝑖 = number of windows containing vibrato in note i. \n1.3.3.  𝑣𝑐𝑖 = vibrato coverage of note i, i.e., 𝑣𝑐𝑖= 𝑊𝑉𝑖𝑊𝑖⁄ (ra-\ntio of windows with vibra to to the total number of win-\ndows).  \n1.3.4.  𝑣𝑑𝑖 = vibrato duration of note i (sec), i.e., 𝑣𝑑𝑖= 𝑣𝑐𝑖∙𝑑𝑖. \n1.3.5.  freq (𝑝𝑝𝑤,𝑖) = frequency of the prominent peak 𝑝𝑝𝑤,𝑖 (i.e., \nvibrato frequency, in Hz).  \n1.3.6.  𝑣𝑟𝑖  = vibrato rate of note i (in Hz), i.e., 𝑣𝑟𝑖  = \n∑ freq (𝑝𝑝𝑤,𝑖)𝑊𝑉 𝑖\n𝑤=1 𝑊𝑉𝑖 ⁄  (average vibrato frequency).  \n1.3.7.  |𝑝𝑝𝑤,𝑖|  = magnitude of the prominent peak 𝑝𝑝𝑤,𝑖  (in \ncents).  \n1.3.8.  𝑣𝑒𝑖 = vibrato extent of note i, i.e., 𝑣𝑒𝑖 = ∑ |𝑝𝑝𝑤,𝑖|𝑊𝑉 𝑖\n𝑤=1 𝑊𝑉𝑖 ⁄  \n(average amplitude of vibrato).  386 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018  \n \n \nThen, we define the following features.  \nVibrato Presence (VP).  A song clip contains vibrato if \nany of its notes have vibrato, similarly to ( 2). \nVibrato Rate (VR) statistics.  Based on the vibrato rate \nvalue of each note, 𝑣𝑟𝑖 (see Algorithm 3), we compute 6 \nstatistics  described in Section 3.3.2 (e.g., the vibrato rate \nweighted mean of all  notes with vibrato as in Eq. 6).  \n𝑉𝑅𝑚𝑒𝑎𝑛 =∑ 𝑣𝑟𝑖∙𝑣𝑐𝑖∙𝑛𝑑 𝑖𝑁\n𝑖=1\n∑ 𝑣𝑐𝑖∙𝑛𝑑 𝑖𝑁\n𝑖=1    (6) \n \nVibrato Extent (VE) and Vibrato Duration (VD) sta-\ntistics.  Similarly to  VR, these features represent  the same \nstatistics for vibrato extent, based on 𝑣𝑒𝑖 and vibrato dura-\ntion, based on 𝑣𝑑𝑖 (see Algorithm 3).  \nVibrato Notes Base Frequ ency (VNBF) statistics.  As \nwith VR features, we compute the same statistics for the \nbase frequency (in cents) of all notes containing vibrato.  \nVibrato Coverage (VC).  This represents  the global  vi-\nbrato  coverage  in a song , based on 𝑣𝑐𝑖, similar ly to (3). \nHigh -Frequency Vibrato Coverage (HFVC). Here, \nthe VC is computed only  for notes over C4 (261.6 Hz) , \nwhich is  the lower limit of the soprano’s vocal range [41].  \n Vibrato to Non -Vibrato Ratio (VNVR).  This feature \nis defined as the ratio of the notes containing v ibrato to the \ntotal number of notes, similarly to ( 5). \n \nAn approach similar to vibrato was applied  to compute \ntremolo features.  Tremolo can be described as a trembling \neffect, to a certain degree  similar to vibrato but regarding \nvariation  of amplitude. Here, instead of  using the f0 se-\nquence s, the sequence of pitch salien ces of each note is \nused to assess  variation s in intensity or amplitude. Due to \nthe lack of  research regarding tremolo  range , we decided \nto use vibrato range (i.e., 5 -8Hz).  \n3.4 Emotion Classific ation  \nGiven the high number of features, ReliefF feature selec-\ntion algorithms [31] were used to rank the better suited \nones emotion classification . This algorithm outputs featu re \nweights in the range of -1 to 1, with higher values  indicat-\ning attributes  more suited to the problem . This, in conjunc-\ntion with the strategy described in Section 3.2, were used \nto reduce and merge baseline and novel features  sets. \nFor classification we selected Support Vector Machines \n(SVM) [42] as the machine learning technique, since it has \nperformed well in previous MER studies. SVM parameters \nwere tuned with grid search and a Gaussian kernel (RBF) \nwas selected based on preliminary tests. The experiments \nwere validated with 20 repeti tions of 10 -fold cross valida-\ntion [43], where we report the average (macro weighted) \nresults . \n4. RESULTS AND DISCU SSION  \nIn this section we discuss the results of our classification \ntests. Our main objective was to assess the relevance of \nexisting audio features to MER and understand if and how \nour novel proposed ones improve the current scenario. \nWith th is in mind, we  start by testing the existing baseline \n(standard ) features  only, followed by tests using the com-bination of baseline and novel, to assess if the obtained re-\nsults improve and if the differences are statistically signif-\nicant.  \nA summary  of the  classification  results is shown in Ta-\nble 1. The baseline feature set obtained its best result, of \n71.7% F1 -score, with an extremely high number of fea-\ntures (800). Considering a more reasonable number of fea-\ntures, up to  the best 100 according to ReliefF, the best \nmodel used the top70, and attained 67.5%. Next, including \nnovel features (with the baseline) increased the best result \nto 76.0% F1 -score using the best 100 features , a consider-\nably lower number (100 instead of 800). This difference is \nstatistically significa nt (a t p < 0.01, paired T -test). Inter-\nestingly,  we observed decreasing results with models using \nhigher number of features, indicating that those extra fea-\ntures might not be relevant but introducing noise.  \n \nClassifier  Feature set # feats.  F1-Score  \nSVM  baseline  70 67.5%  ± 0.05 \nSVM  baseline  100 67.4%  ± 0.05 \nSVM  baseline  800 71.7%  ± 0.05 \nSVM  baseline+novel  70 74.0% ± 0.05 \nSVM  baseline+novel  100 76.0% ± 0.0 5 \nSVM  baseline+novel  800 73.5% ± 0.04 \nTable 1. Results of the classification by quadrants.  \nOf the 100 features used in the best result, 29 are  nove l, \nwhich demonstrates the relevance of  adding novel features \nto MER. Of these, 8 are related with texture, such as the \nnumber of musical layers ( MLmean ), while the remaining \n21 are expressi ve techniques such as tremolo, glissando \nand especially vibrato (12) . The remaining 71 baseline fea-\ntures are mainly tone color related (50), with the few others \ncapturing dynamics, harmony, rhythm and melody.  \nFurther analysis to the results per individual quadrant, \npresent ed in Table 2, gives us a deeper understanding \nabout which emotions are harder to classify and where the \nnew features were more significant. According to it, Q1 \nand Q2 obtained a higher result compared to the remain-\ning. This seems to indic ate that emotions in songs with \nhigher arousal are easier to differentiate. Also, Q2 result is \nsignificantly higher, indicating that it might be markedly \ndistinct  from the remaining,  explained by the fact that sev-\neral excerpts from Q2 belong to genres such  as punk, hard-\ncore or  heavy -metal, which ha ve very distinctive, noise -\nlike, acoustic features.  This goes in the same direction as \nthe results obtained in  previous studies  [44]. \n \n baseline  novel  \nQuads  Prec.  Recall  F1-Score  Prec.  Recall  F1-Score  \nQ1 62.6%  73.4%  67.6%  72.9% 81.9% 77.2% \nQ2 82.3%  79.6%  80.9%  88.9% 82.7% 85.7% \nQ3 61.3%  57.5%  59.3%  73.0% 69.2% 71.1% \nQ4 62.8%  57.9%  60.2%  68.5% 68.6% 68.5% \nTable 2. Results per quadrant using 100 features.  \nSeveral factors  can be thought  to explain the lower re-\nsults in Q3 and Q4  (average of -11.7%). First, a  higher \nnumber of  ambiguous songs exist in these quadrants, con-\ntaining unclear or contrasting emotions. This is supported Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 387  \n \nby the low agreement (45.3%) between the subject’s and \nthe original AllMusic annotations  during the annotation \nprocess. In addition, the two quadrants contain songs \nwhich share similar musical characteristics, sometimes \nwith each characteristic  related to contrasting emotional \ncues (e.g., a happy melody and a sad voice or lyric). This \nagrees  with the conclusions prese nted in [45]. As a final \npoint , these similarit ies may explain why the subjects re-\nported having more difficulty distinguishing valence for \nsongs with low arousal.  \nThe addition of novel features improved the results by \n8.6% when considering the top 100 features’ results . Novel \nfeatures seemed more relev ant to Q3, with the most signif-\nicant improvement (by 11.8%), which was before the \nworst performing quadrant, followed by Q1 (9.6%). On the \nopposite end, Q2 was already the best performing with \nbaseline features and thus is lower improvement (4.8%).  \nIn addi tion to assessing the importance of baseline and \nnovel features for quadrants classification, where we iden-\ntified 29 novel features in the best 100, w e also studied  the \nbest features to discriminate each sp ecific quadrant from \nthe others. This was done by analyzing specific feature \nrankings , e.g., the ranking of features that are best to sepa-\nrate Q1 songs from non -Q1 songs  (a set containing Q2, Q3 \nand Q4 annotated as non -Q1). As expected based on for-\nmer tests, tone color is the most represented concept in t he \nlist of the 10 best features  for each of the four quadrants. \nThe reason is in part due to being overrepresented in orig-\ninal feature set , while relevant features from other concepts \nmay be missing .  \nOf the four quadrants, Q2 and Q4 seem to have the most \nsuited features to distinguish them ( e.g., features to iden-\ntify a clip as Q2 vs non -Q2), according to the obtained Re-\nliefF weights. This was confirmed experimentally, where \nwe observed that 10 features or less was enough to obtain \n95% of the max score in b inary problems for Q2 and Q4, \nwhile the top 30 and 20 features, for Q1 and Q3 respec-\ntively, were needed to attain the same goal.  \nRegarding the first quadrant, some of the novel features \nrelated with m usical texture information  were shown to be \nvery relevan t. As an example, in the top features, 3 are \nnovel, capturing information related with the number of \nmusical layers and the transitions between different texture \ntypes , together with 3 rhythmic features  related with events \ndensity and fluctuation . Q1 repre sents happy emotions, \nwhich are typically energetic. Associated songs tend to be \nhigh in energy and have appealing (“catchy”)  rhythm. \nThus, features related with rhythm , together with texture \nand tone color (mostly energy metrics) support this. Nev-\nertheles s, as stated before the weight  of these features to \nQ1 is low  when compared with the top features of other \nquadrants.   \nFor Q2 the features identified as most suited  are related \nwith tone color, such as: roughness - capturing the disso-\nnance in the song; rol loff – measuring the amount of high \nfrequency ; MFCC s – total energy in the signal; and spec-\ntral flatness measure – indicating how noise -like the sound \nis. Other important features are  related with dynamics, \nsuch as  tonal dissonance . As for novel features , expressive \ntechniques  ones, mainly vibrato, which makes 43% of the \ntop 30 features . Some research supports this association of \nvibrato and negative energetic emotions such as anger [46]. Generally, the associations found seem reasonable. \nAfter all, Q2  is made of tense, aggressive music , and mu-\nsical characteristics like sensory dissonance, high energy, \nand complexity  are usually present.  \nApart from tone color features (extracting energy infor-\nmation), quadrant 3 is also identified higher level features \nfrom concepts such as musical texture, dynamics and har-\nmony and expressive techniques. Namely, the number of \nmusical layers, spectral dissonance, inharmonicity, a nd \ntremolo s. As for quadrant 4 , in addition to tone color fea-\ntures related to spectrum (such as  skewness or entropy) or \nmeasures of how noise -like is the spectrum ( spectral flat-\nness), the remaining are again related with dynamics (dis-\nsonance) and harmony, as well as some vibrato metrics.  \nMore and better features are needed to better understand \nand d iscriminate Q3 from Q4. From our tests, songs from \nboth quadrants share some common musical characteris-\ntics such as lower tempo, less musical layers and energy, \nuse of glissandos and other expressive techniques.  \n5. CONCLUSIONS AND FUTU RE WORK  \nWe studied the r elevance of musical audio features, pro-\nposing novel features that complement the existing ones. \nTo this end, the features available in known frameworks \nwere studied and classified in one of eight musical con-\ncepts  - dynamics, expressive techniques, harmony,  mel-\nody, mu sical form, musical texture, rhythm and tone color . \nConcepts such as musical form, musical texture and ex-\npressive techniques were identified as the ones most lack-\ning available audio extractors . Based on this, we proposed \nnovel audio features to mitigate the identified gaps  and \nbreak the current glass ceiling.  Namely, related with ex-\npressive techniques, capturing information related with vi-\nbrato, tremolo, glissando  and articulation. Also, related \nwith musical texture, capturing statistics regardin g the mu-\nsical layers of a musical piece.  \nSince no public available dataset fulfilled our needs , a \nnew dataset  with 900 clips  and metadata (e.g., title, artist, \ngenre s and mood s), annotated according to the Ru ssell’s \nemotion model quadrants was built semi -automatically, \nused in our tests and is available to other researchers.  \nOur experimental tests demonstrated that the novel pro-\nposed features are relevant and improve MER classifica-\ntion. As an example, using a similar number of features \n(100), adding our no vel proposed features increased the re-\nsults by 8.6% (to 76.0%), when compared to the baseline.  \nThis result was obtained using 29 novel features and 71 \nbaseline , which demonstrates the relevance of this work . \nAdditional experiments were conducted to uncover ed \nand better understand relations between audio features, \nmusical concepts and specific emotions (quadrants) . \nIn the future, we would like to study multi -modal ap-\nproaches and the relation between the voice signal and lyr-\nics, as well as testing the feature s influence in finer grained \ncategorical and dimensional emotion models.  Also, other \nfeatures (e.g. related with musical form ), are still to be de-\nveloped.  Moreover,  we would like to derive a more under-\nstandable  set of knowledge  (e.g. rules) of how musical fea-\ntures influence emotion, something that lacks when black -\nbox classification methods such as SVMs are employed.  388 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018  \n \n6. ACKNOWLEDGMENT  \nThis work was supported by the MOODetector project \n(PTDC/EIA -EIA/102185/2008), financed by the Funda-\nção para Ciênc ia e a Tecnologia (FCT) and Pro grama \nOperacional Temático Factores de Competitivid -ade \n(COMPETE) – Portugal, as well as the PhD Scholarship \nSFRH/BD/91523/2012, funded by the Fundação para \nCiência e a Tecnologia (FCT), Programa Operacional Po-\ntencial Humano (POPH)  and Fundo Social Europeu (FSE) .  \n7. REFERENCES  \n[1] A. Pannese, M. -A. Rappaz, and D. Grandjean, \n“Metaphor and music emotion: Ancient views and \nfuture directions,” Conscious. Cogn. , vol. 44, pp. 61 –\n71, Aug.  2016.  \n[2] Y. Feng, Y. Zhuang, and Y. Pan, “Popular Music \nRetrieval by Detecting Mood,” Proc. 26th Annu. Int. \nACM SIGIR Conf. Res. Dev. Inf. Retr. , vol. 2, no. 2, \npp. 375 –376, 2003.  \n[3] C. Laurier and P. Herrera, “Audio Music Mood \nClassification Using Supp ort Vector Machine,” in \nProc. of the 8th Int. Society for Music Information \nRetrieval Conf. (ISMIR 2007) , 2007, pp. 2 –4. \n[4] Y.-H. Yang, Y. -C. Lin, Y. -F. Su, and H. H. Chen, “A \nRegression Approach to Music Emotion \nRecognition,” IEEE Trans. Audio. Speech. L ang. \nProcessing , vol. 16, no. 2, pp. 448 –457, Feb. 2008.  \n[5] L. Lu, D. Liu, and H. -J. Zhang, “Automatic Mood \nDetection and Tracking of Music Audio Signals,” \nIEEE Trans. Audio, Speech Lang. Process. , vol. 14, \nno. 1, pp. 5 –18, Jan. 2006.  \n[6] R. Panda and R. P. Paiva, “Using Support Vector \nMachines for Automatic Mood Tracking in Audio \nMusic,” in 130th Audio Engineering Society \nConvention , 2011, vol. 1.  \n[7] A. Flexer, D. Schnitzer, M. Gasser, and G. Widmer, \n“Playlist Generation Using Start and End Songs,” in \nProc. of the 9th Int. Society of Music Information \nRetrieval Conf. (ISMIR 2008) , 2008, pp. 173 –178. \n[8] O. C. Meyers, “A Mood -Based Music Classification \nand Exploration System,” MIT Press, 2007.  \n[9] R. Malheiro, R. Panda, P. Gomes, and R. P. Paiva, \n“Emotiona lly-Relevant Features for Classification \nand Regression of Music Lyrics,” IEEE Trans. Affect. \nComput. , pp. 1 –1, 2016.  [10] X. Hu and J. S. Downie, “When lyrics outperform \naudio for music mood classification: a feature \nanalysis,” in Proc. of the 11th Int. S ociety for Music \nInformation Retrieval Conf. (ISMIR 2010) , 2010, pp. \n619–624. \n[11] Y. Yang, Y. Lin, H. Cheng, I. Liao, Y. Ho, and H. H. \nChen, “Toward multi -modal music emotion \nclassification,” in Pacific -Rim Conference on \nMultimedia , 2008, vol. 5353, pp. 7 0–79. \n[12] R. Panda, R. Malheiro, B. Rocha, A. Oliveira, and R. \nP. Paiva, “Multi -Modal Music Emotion Recognition: \nA New Dataset, Methodology and Comparative \nAnalysis,” in 10th International Symposium on \nComputer Music Multidisciplinary Research – \nCMMR’2013 , 2013, pp. 570 –582. \n[13] Ò. Celma, P. Herrera, and X. Serra, “Bridging the \nMusic Semantic Gap,” in Workshop on Mastering the \nGap: From Information Extraction to Semantic \nRepresentation , 2006, vol. 187, no. 2, pp. 177 –190. \n[14] Y. E. Kim, E. M. Schmidt, R.  Migneco, B. G. Morton, \nP. Richardson, J. Scott, J. A. Speck, and D. Turnbull, \n“Music Emotion Recognition: A State of the Art \nReview,” in Proc. of the 11th Int. Society for Music \nInformation Retrieval Conf. (ISMIR 2010) , 2010, pp. \n255–266. \n[15] X. Yang, Y.  Dong, and J. Li, “Review of data \nfeatures -based music emotion recognition methods,” \nMultimed. Syst. , pp. 1 –25, Aug. 2017.  \n[16] S. B. Davis and P. Mermelstein, “Comparison of \nParametric Representations for Monosyllabic Word \nRecognition in Continuously Spok en Sentences,” \nIEEE Transactions on Acoustics, Speech, and Signal \nProcessing . 1980.  \n[17] J. A. Russell, “A circumplex model of affect,” J. Pers. \nSoc. Psychol. , vol. 39, no. 6, pp. 1161 –1178, 1980.  \n[18] K. Hevner, “Experimental Studies of the Elements of \nExpression in Music,” Am. J. Psychol. , vol. 48, no. 2, \npp. 246 –268, 1936.  \n[19] M. Malik, S. Adavanne, K. Drossos, T. Virtanen, D. \nTicha, and R. Jarina, “Stacked Convolutional and \nRecurrent Neural Networks for Music Emotion \nRecognition,” in Proc. of the 14th Sound & Music \nComputing Conference , 2017, pp. 208 –213. \n[20] A. Aljanaki, Y. -H. Yang, and M. Soleymani, \n“Developing a benchmark for emotional analysis of \nmusic,” PLoS One , vol. 12, no. 3, Mar. 2017.  Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 389  \n \n[21] C. Laurier, O. Lartillot, T. Eerola, and P. Toiviaine n, \n“Exploring relationships between audio features and \nemotion in music,” in Proc. of the 7th Triennial Conf. \nof European Society for the Cognitive Sciences of \nMusic , 2009, vol. 3, pp. 260 –264. \n[22] A. Friberg, “Digital Audio Emotions - An Overview \nof Comp uter Analysis and Synthesis of Emotional \nExpression in Music,” in Proc. of the 11th Int. Conf. \non Digital Audio Effects (DAFx) , 2008, pp. 1 –6. \n[23] G. Tzanetakis and P. Cook, “MARSYAS: a \nframework for audio analysis,” Organised Sound , \nvol. 4, no. 3, pp. 16 9–175, 2000.  \n[24] O. Lartillot and P. Toiviainen, “A Matlab Toolbox for \nMusical Feature Extraction from Audio,” in Proc. of \nthe 10th Int. Conf. on Digital Audio Effects (DAFx) , \n2007, pp. 237 –244. \n[25] D. Cabrera, S. Ferguson, and E. Schubert, \n“‘Psysound3’:  Software for Acoustical and \nPsychoacoustical Analysis of Sound Recordings,” in \nProc. of the 13th Int. Conf. on Auditory Display \n(ICAD2007) , 2007, pp. 356 –363. \n[26] C. Laurier, “Automatic Classification of Musical \nMood by Content -Based Analysis,” Universit at \nPompeu Fabra, 2011.  \n[27] T. Bertin -Mahieux, D. P. W. Ellis, B. Whitman, and \nP. Lamere, “The Million Song Dataset,” in Proc. of \nthe 12th Int. Society for Music Information Retrieval \nConf. (ISMIR 2011) , 2011, pp. 591 –596. \n[28] A. B. Warriner, V. Kuperman,  and M. Brysbaert, \n“Norms of valence, arousal, and dominance for \n13,915 English lemmas,” Behav. Res. Methods , vol. \n45, no. 4, pp. 1191 –1207, Dec. 2013.  \n[29] M. M. Bradley and P. J. Lang, “Affective Norms for \nEnglish Words (ANEW): Instruction Manual and \nAffective Ratings,” Psychology , vol. Technical, no. \nC-1, p. 0, 1999.  \n[30] X. Hu and J. S. Downie, “Exploring Mood Metadata: \nRelationships with Genre, Artist and Usage \nMetadata,” in Proc. of the 8th Int. Society for Music \nInformation Retrieval Conf. (ISMIR 200 7), 2007, pp. \n67–72. \n[31] M. Robnik -Šikonja and I. Kononenko, “Theoretical \nand Empirical Analysis of ReliefF and RReliefF,” \nMach. Learn. , vol. 53, no. 1 –2, pp. 23 –69, 2003.  \n[32] E. Benetos, S. Dixon, D. Giannoulis, H. Kirchhoff, and A. Klapuri, “Automatic music transcription: \nchallenges and future directions,” J. Intell. Inf. Syst. , \nvol. 41, no. 3, pp. 407 –434, 2013.  \n[33] J. Salamon and E. Gómez, “Melody Extraction From \nPolyphonic Music Signals Using Pitch Contour \nCharacteristics,” IEEE Trans. Audio. Speech . Lang. \nProcessing , vol. 20, no. 6, pp. 1759 –1770, 2012.  \n[34] K. Dressler, “Automatic Transcription of the Melody \nfrom Polyphonic Music,” Ilmenau University of \nTechnology, 2016.  \n[35] R. P. Paiva, T. Mendes, and A. Cardoso, “Melody \nDetection in Polyphonic M usical Signals: Exploiting \nPerceptual Rules, Note Salience, and Melodic \nSmoothness,” Comput. Music J. , vol. 30, no. 4, pp. \n80–98, Dec. 2006.  \n[36] G. D. Webster and C. G. Weir, “Emotional Responses \nto Music: Interactive Effects of Mode, Texture, and \nTempo,”  Motiv. Emot. , vol. 29, no. 1, pp. 19 –39, Mar. \n2005.  \n[37] P. Gomez and B. Danuser, “Relationships between \nmusical structure and psychophysiological measures \nof emotion.,” Emotion , vol. 7, no. 2, pp. 377 –387, \nMay 2007.  \n[38] C. Dromey, S. O. Holmes, J. A. Ho pkin, and K. \nTanner, “The Effects of Emotional Expression on \nVibrato,” J. Voice , vol. 29, no. 2, pp. 170 –181, Mar. \n2015.  \n[39] T. Eerola, A. Friberg, and R. Bresin, “Emotional \nexpression in music: contribution, linearity, and \nadditivity of primary musical c ues.,” Front. Psychol. , \nvol. 4, p. 487, 2013.  \n[40] J. Salamon, B. Rocha, and E. Gómez, “Musical genre \nclassification using melody features extracted from \npolyphonic music signals,” in IEEE International \nConference on Acoustics, Speech and Signal \nProcessing  (ICASSP) , 2012, pp. 81 –84. \n[41] A. Peckham, J. Crossen, T. Gebhardt, and D. \nShrewsbury, The Contemporary Singer: Elements of \nVocal Technique . Berklee Press, 2010.  \n[42] C.-C. Chang and C. -J. Lin, “LIBSVM: A library for \nsupport vector machines,” ACM Trans. Intell. Syst. \nTechnol. , vol. 2, no. 3, pp. 1 –27, Apr. 2011.  \n[43] R. O. Duda, P. E. (Peter E. Hart, and D. G. Stork, \nPattern classification . Wiley, 2000.  390 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018  \n \n[44] G. R. Shafron and M. P. Karno, “Heavy metal music \nand emotional dysphoria among listeners.,” Psych ol. \nPop. Media Cult. , vol. 2, no. 2, pp. 74 –85, 2013.  \n[45] Y. Hong, C. -J. Chau, and A. Horner, “An Analysis of \nLow-Arousal Piano Music Ratings to Uncover What \nMakes Calm and Sad Music So Difficult to \nDistinguish in Music Emotion Recognition,” J. Audio \nEng. Soc., vol. 65, no. 4, 2017.  \n[46] K. R. Scherer, J. Sundberg, L. Tamarit, and G. L. \nSalomão, “Comparing the acoustic expression of \nemotion in the speaking and the singing voice,” \nComput. Speech Lang. , vol. 29, no. 1, pp. 218 –235, \nJan. 2015.  \n \n \n Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 391"
    },
    {
        "title": "Identifying Emotions in Opera Singing: Implications of Adverse Acoustic Conditions.",
        "author": [
            "Emilia Parada-Cabaleiro",
            "Maximilian Schmitt",
            "Anton Batliner",
            "Simone Hantke",
            "Giovanni Costantini",
            "Klaus R. Scherer",
            "Björn W. Schuller"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492429",
        "url": "https://doi.org/10.5281/zenodo.1492429",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/22_Paper.pdf",
        "abstract": "The expression of emotion is an inherent aspect in singing, especially in operatic voice. Yet, adverse acoustic conditions, as, e. g., a performance in open-air, or a noisy analog recording, may affect its perception. State-of-the art methods for emotional speech evaluation have been applied to operatic voice, such as perception experiments, acoustic analyses, and machine learning techniques. Still, the extent to which adverse acoustic conditions may impair listeners' and machines' identification of emotion in vocal cues has only been investigated in the realm of speech. For our study, 132 listeners evaluated 390 nonsense operatic sung instances of five basic emotions, affected by three noises (brown, pink, and white), each at four Signal-to-Noise Ratios (-1 dB, -0.5 dB, +1 dB, and +3 dB); the performance of state-of-the-art automatic recognition methods was evaluated as well. Our findings show that the three noises affect similarly female and male singers and that listeners' gender did not play a role. Human perception and automatic classification display similar confusion and recognition patterns: sadness is identified best, fear worst; low aroused emotions display higher confusion.",
        "zenodo_id": 1492429,
        "dblp_key": "conf/ismir/Parada-Cabaleiro18",
        "keywords": [
            "expression of emotion",
            "operatic voice",
            "adverse acoustic conditions",
            "perception experiments",
            "acoustic analyses",
            "machine learning techniques",
            "adverse acoustic conditions",
            "listeners gender",
            "human perception",
            "automatic classification"
        ],
        "content": "IDENTIFYING EMOTIONS IN OPERA SINGING: IMPLICATIONS OF\nADVERSE ACOUSTIC CONDITIONS\nEmilia Parada-Cabaleiro1Maximilian Schmitt1Anton Batliner1\nSimone Hantke1;2Giovanni Costantini3Klaus Scherer4Björn W. Schuller1;5\n1ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany\n2Machine Intelligence & Signal Processing Group, Technische Universität München, Germany\n3Department of Electronic Engineering, University of Rome Tor Vergata, Italy\n4Department of Psychology, University of Geneva, Switzerland\n5GLAM – Group on Language, Audio & Music, Imperial College London, UK\nemilia.parada-cabaleiro@informatik.uni-augsburg.de\nABSTRACT\nThe expression of emotion is an inherent aspect in singing,\nespecially in operatic voice. Yet, adverse acoustic condi-\ntions, as, e. g., a performance in open-air, or a noisy analog\nrecording, may affect its perception. State-of-the art meth-\nods for emotional speech evaluation have been applied to\noperatic voice, such as perception experiments, acoustic\nanalyses, and machine learning techniques. Still, the extent\nto which adverse acoustic conditions may impair listen-\ners’ and machines’ identiﬁcation of emotion in vocal cues\nhas only been investigated in the realm of speech. For our\nstudy, 132 listeners evaluated 390 nonsense operatic sung\ninstances of ﬁve basic emotions, affected by three noises\n(brown, pink, and white), each at four Signal-to-Noise Ra-\ntios (-1 dB, -0.5 dB, +1 dB, and +3 dB); the performance of\nstate-of-the-art automatic recognition methods was evalu-\nated as well. Our ﬁndings show that the three noises af-\nfect similarly female and male singers and that listeners’\ngender did not play a role. Human perception and auto-\nmatic classiﬁcation display similar confusion and recog-\nnition patterns: sadness is identiﬁed best, fear worst; low\naroused emotions display higher confusion.\n1. INTRODUCTION\nSinging is a channel to communicate emotion that goes be-\nyond culture or time, as shown by a variety of common mu-\nsical representations across the world over centuries: as,\ne. g., lullabies [39] (typical expression of parental love) or\nspiritual chant [20] (typical expression of mystic feelings).\nIn western music, the emotional expression in singing\nvoice is inexorably linked to the Italian Opera which has\nc\rEmilia Parada-Cabaleiro, Maximilian Schmitt, Anton\nBatliner, Simone Hantke, Giovanni Costantini, Klaus Scherer, Björn W.\nSchuller. Licensed under a Creative Commons Attribution 4.0 Interna-\ntional License (CC BY 4.0). Attribution: Emilia Parada-Cabaleiro,\nMaximilian Schmitt, Anton Batliner, Simone Hantke, Giovanni Costan-\ntini, Klaus Scherer, Björn W. Schuller. “Identifying Emotions in Opera\nSinging: Implications of Adverse Acoustic Conditions”, 19th Interna-\ntional Society for Music Information Retrieval Conference, Paris, France,\n2018.had, from the XVIII century (through the development of\nthebelcanto [38]) till the XIX century (with the advent of\ntheMelodramma Verdiano [38]) a focus on the dramatic–\nemotional interpretation of the opera’s characters [10].\nThe Opera was born in Italy at the beginning of the\nXVII century as an ‘entertainment’ [12]. Even though\nOpera is no longer the most common leisure activity,\nits cultural importance is still shown by thousands of\n‘opera performances’ made every year—6,795 only in\nGermany for the 2015 / 2016 season1; and by thousands\nof ‘opera recordings’ available in multi-media libraries—\n21,054 items only in the Istituto Centrale per i Beni\nSonori ed Audovisivi (The National Italian Audiovisual In-\nstitute2). Yet, opera may face ‘real-world’ acoustic degra-\ndation, e. g., from open-air performances [3] or from ana-\nlog recordings [22]. Indeed, improving the acoustics of an\nopera house is a central topic of sound engineering [2], as\nwell as the application of digital signal processing solu-\ntions to the restoration of old recordings [11].\nEven though emotion in opera singing has been stud-\nied from the perceptual [34], acoustic [32], and automatic\nrecognition [7] point of view, it has not been evaluated so\nfar up to which extent restricted acoustic quality affects\nthe perception and classiﬁcation of emotion in singing.\nIn this regard, we present a perceptual study (based on a\nforced-choice categorical [6] and dimensional [28] test),\nperformed by 132 Italian listeners, who evaluated 390 non-\nsense instances, sung by 6 professional opera singers (3\nfemale), in 5 emotional states (hot anger, elated happi-\nness, depressive sadness, panicked fear, and worried fear),\nsubsequently masked by three noises (white, pink, and\nbrown) at 4 signal-to-noise ratios (-1 dB, -0.5 dB, +1 dB,\nand +3 dB). The performance of state-of-the-art emotion\nrecognition methods based on a Support Vector Machine\nclassiﬁer and ComParE features [36] is evaluated as well.\nIn Section 2, related work is described; Sections 3 and 4\nevaluate the database and the listening test; Section 5 dis-\ncusses the results for the machine learning approach; ﬁ-\nnally, Section 6 outlines conclusions and future work.\n1http://operabase.com/top.cgi?lang\n2http://opac2.icbsa.it/vufind/376% 33\nœ œ œ œ œ œ œ œ\nNe kal i bam so ud mo len - - -Figure 1 : The nonsense utterance ‘Ne kal ibam soud molen’ sung in an ascending scale for each emotional state.\nFigure 2 : Correspondence between emotion categories and\nthe bi-dimensional model of the ﬁve ‘real’ labels, i. e., hot\nanger (HOT an), elated happiness (ELA ha), depressive sadness\n(DEP sa), panicked fear (PAN fe), and worried fear (WOR fe),\nin bold; and the three dimensional ‘distractors’, i. e., cold anger\n(COL an), pleasured happiness (PLE ha), and desperate sadness\n(DES sa), considered in the perception test.\n2. RELATED WORK\nEven though emotions are typically expressed through the\nvoice, emotional singing has received little attention com-\npared to emotional speech [29]. Yet, the similarity between\nboth channels (i. e., speech and singing [19, 33]) has re-\ncently encouraged researchers to analyse the expression\nand perception of sung emotional content [4]. Methods\ntypically used in emotional speech research [1] have also\nbeen applied to singing—with special attention to the op-\neratic voice—such as acoustic evaluation [23, 27, 32, 37]\nor perception assessment [15, 16, 34]. Furthermore, in\nthe realm of affective computing, state-of-the-art machine\nlearning techniques, typically used in audio signal process-\ning for speech emotion recognition, have also been applied\nto the study of the a cappella singing voice [7, 40].\nIn the assessment of emotional speech, it has been\nshown that listeners’ perception, acoustic feature analy-\nsis, and machine learning techniques, are affected by noisy\nbackgrounds [25,35], which are typical of ‘real-world’ en-\nvironments and recordings. Yet, although singing mostly\ntakes place in adverse acoustic conditions, the extent to\nwhich these may impair a listener’s ability to perceive its\ninherent emotion, and how the robustness of automatic sys-\ntems for emotion recognition in singing might be impaired,\nhas not been, to the best of our knowledge, assessed so far.\n3. METHODOLOGY\n3.1 An Emotional Corpus of a Cappella Opera Singing\nWe took into account a selection of sentences from a\ndataset of the emotional singing voice [7,33] in which pro-\nfessional opera singers performed a variety of sentences\n100200 50010002000 50001000020000−80−60−40−20\nFrequency [Hz]Amplitude [dB]\n  \nBrown\nPink\nWhite\nRealFigure 3 : A comparison of the spectral distribution between 0–\n2 kHz and -80–0 dB, for the brown, pink, white, and real noise.\nin different emotional states correlated to several levels of\narousal (intensity) and valence (hedonistic value). Since\nlinguistic meaning may inﬂuence listener perception of the\nemotional content, in order to avoid such a bias [31], the\nnonsense sentence ne kal ibam soud molen! has been con-\nsidered. For a gender-balanced distribution of voice types,\nsix singers have been selected: three females (two sopra-\nnos and one mezzosoprano) and three male (two tenors and\none countertenor), who produced ﬁve times the nonsense\nsentence with an ascending scale (cf. Figure 1), each time\nexpressing a different emotional state.\nFollowing previous research on the perception of emo-\ntion in operatic voice [16], four basic emotions have been\nconsidered: anger, with high arousal (intensity), i. e., hot\nanger; happiness, high aroused, i. e., elated happiness; sad-\nness, low aroused, i. e., depressive sadness; and fear, with\nboth high arousal, i. e., panicked fear, and low arousal, i. e.,\nworried fear (cf. Figure 2). Thus, considering one non-\nsense sentence, expressed in ﬁve emotional states by six\nsingers, 30 ‘clean’ stimuli in total have been employed.\n3.2 Manipulation Techniques\nThe perception of emotion in speech is especially compro-\nmised by pink, and to a lesser extent by white and brown\nnoise [25]. In Figure 3, the spectrum of a ‘real’ back-\nground noise, digitised from a ‘no-musical fragment’ of an\nLP recording3, is compared with brown, pink, and white\nnoise. The ‘real’ noise displays higher energy in the lowest\nfrequencies, presenting a negative slope of approximately\n6 dB per octave up to 1 kHz, a constant area from 1 kHz\nto 3 kHz, and a fall of energy of approximately 10 dB per\noctave above 3 kHz. Its acoustic characteristics makes it\nmost similar to brown noise, which presents a negative\n3Recording of the aria Vissi d’amore (Puccini’s Tosca ), interpreted by\nGiannina Arangi and produced in 1932 by Columbia records .Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 377dB%\ncl\nBrown+3 +1-0.5 -1\nPink+3 +1-0.5 -1\nWhite+3 +1-0.5 -101020Female singers\nMF\ndB cl\nBrown+3 +1-0.5 -1\nPink+3 +1-0.5 -1\nWhite+3 +1-0.5 -1Male singers\nMF\nFigure 4 : Mean accuracy in % of the ‘real’ emotions (cf. caption Figure 2) perceived by female (F) and male (M) listeners; in clean (cl)\nconditions, background noises (brown, pink, and white), and 4 SNR (-1 dB, -0.5 dB, +1 dB, and +3 dB); sung by females and males.\nslope of around 6 dB per octave (1/f2noise); slightly sim-\nilar to pink, with a negative slope of approximately 3 dB\nper octave (1/f noise); and dissimilar to white, whose ﬂat\nspectrum presents all the frequencies at the same level.\nNote that the given comparison aims at exemplifying po-\ntential similarities between ‘real’ and ‘artiﬁcially gener-\nated’ noise; noise from different recordings may display\nhigher similarity with pink, white, or other noise types.\nWe evaluated listeners’ perception of emotion in ad-\nverse acoustic conditions by applying four Signal-to-Noise\nRatio (SNR) levels (-1 dB, -0.5 dB, +1 dB, +3 dB) and three\nnoises (brown, pink, white) to the ‘clean’ samples. The\nnoises, normalized to -1 dB, have been artiﬁcially gener-\nated and mixed (at the speciﬁed SNR value) in Matlab\nR2014a [21]. Given 6 singers, 1 sentence sung in 5 emo-\ntional states, 3 noises, and 4 applied SNR levels yields\n6 x 1 x 5 x 3 x 4 = 360 ‘noisy’ samples plus 30 ‘clean’ sam-\nples = 390 stimuli in total.\n4. PERCEPTION STUDY\n4.1 Emotion Measurement\nThe two prominent models considered to evaluate listen-\ners’ perception of emotional speech, i. e., the categorical\n[6], which identiﬁes each emotional state with a speciﬁc\ncategory, and the dimensional [28], which identiﬁes each\nemotional state within a continuous hyper-space charac-\nterised by dimensions—commonly arousal (from low to\nhigh) and valence (from negative to positive)—have al-\nready been applied to the perceptual evaluation of emotion\nin singing [16,26]. Yet, which of them would be more suit-\nable to evaluate listeners’ perception of emotion, is still an\nopen question in both the musical domain [5] and speech\nresearch [18]. Both models have been taken into account\nfor the perception test, i. e., each of the 4 considered basic\nemotions—anger, happiness, sadness, and fear (cf. Sec-\ntion 3.1)—has been deﬁned in the bi-dimensional space,\nby having a level of arousal and valence (cf. Figure 2).\nFive of these eight emotional categories (hot anger,\nelated happiness, depressive sadness, panicked fear, and\nworried fear), are ‘real’ emotions effectively expressed by\nthe singers in the dataset. The other three (cold anger, plea-\nsured happiness, and desperate sadness), so-called ‘dis-\ntractor labels’ [24]—emotion categories not displayed in\nthe evaluated data, have the purpose to ‘distract’ the listen-\ners by minimising the chances of performing ‘discrimina-\ntion’ rather than ‘recognition’ [30]. Furthermore, disgust\nand surprise (the remaining two basic emotions—in addi-\ntion to anger, fear, sadness, and happiness—amongst thoseknown as ‘big six’ [6]), have also been considered as ‘dis-\ntractors’, without indicating a speciﬁc dimensional level;\nwe thus present a balanced set of perceptual choices: ﬁve\n‘real’ emotions and ﬁve ‘distractors’.\n4.2 Listening test setup\nIn total, 132 Italian listeners (55 f, 77 m, mean age 20.7\nyears, standard deviation 2.5 years) took part in the per-\nception study. The participants were all students of the\nengineering faculty of the ‘Tor Vergata’ university (Rome)\nand received credits for their participation. To avoid fa-\ntigue, the 390 stimulus were similarly distributed into four\nsessions, each designed to last not longer than 30 minutes.\nOut of the 132 listeners, 101 had no musical instruction,\n27 were self-taught in piano or guitar, 4 had studied in the\nconservatory—piano (2), ﬂute, and accordion. Their musi-\ncal interest was mostly in pop (65 listener), rock (45 listen-\ners), and hip-hop (22 listeners); other genres as, e. g., Ital-\nian music, heavy-metal, or classic were underrepresented\n(less than 10 listeners). Since none of them had stud-\nied singing or demonstrated interest in opera, we consider\nthem as a unique group of non-experts.\nThe test was designed as a forced-choice task; the ten\nemotion categories were presented and the participants\ncould choose one out of them after listening to each stimu-\nlus (an initial training was provided). The test was hosted\non a browser based interface (accessible from any com-\nputer) provided through the gamiﬁed crowd-sourcing plat-\nform iHEARu-PLAY [14]. To ensure a consistent listening\nenvironment, the participants were instructed to use ear-\nphones. Although the listeners had the possibility of lis-\ntening to each stimulus indeﬁnitely, they were encouraged\nto answer spontaneously to the randomized samples.\n4.3 Results and discussion\nEmotions were identiﬁed best in clean conditions; female\nlisteners were slightly more accurate than male; emotions\nin male voices were somewhat better identiﬁed than in fe-\nmale voices (cf. Figure 4). Listeners’ and singers’ gender-\nrelated differences turned out to be not signiﬁcant. In the\nformer case, the biggest distance, i. e., female and male\nlisteners evaluating male voices in pink noise at -1 SNR\n(21.6% vs 17.6%), corresponds to a pvalue in Pearson Chi-\nsquare of =:47(way above the conventional threshold for\nsigniﬁcance of p< :05). In the latter case, the biggest dis-\ntance, i. e., female and male voices perceived by male lis-\nteners in clean conditions (17.2 % vs 22.2 %) did not yield\na signiﬁcant difference either ( p=:37). Thus, the further\nevaluations will not consider gender.378 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018%Real emotions Distractor labels#HOT an ELA ha DEP sa PAN fe WOR fe COL an PLE ha DES sa DIS SUR\nHOT an 26:6 14:7 05:5 03:2 05:2 26:4 9:2 02:8 01:704:5 6\nELA ha 13:1 18:4 07:6 04:4 05:9 19:7 18:3 04:1 03:305:2 6\nDEP sa 01:0 03:6 42:0 03:6 05:7 07:0 12:3 21:4 01:202:2 6\nPAN fe 09:8 06:1 22:8 06:8 06:6 19:7 12:0 09:6 02:304:3 6\nWOR fe 12:6 08:8 14:4 08:2 08:5 21:2 14:2 05:1 02:204:9 6\ntotal 63:1 51 :7 92 :3 26 :1 31 :9 93:9 66 :4 43 :0 10 :7 21 :1 30\nTable 1 : Listeners’ perception (in %) of the clean instances (#), considering ‘real’ emotions and ‘distractors’ (cf. Figure 2, disgust—DIS,\nand surprise—SUR). Each row gives the ‘reference’, darker cells indicate higher % ; listeners’ and singers’ gender is not considered.\n% HOT an ELA ha DEP sa PAN fe WOR fe mean\ncl 26:6 18 :4 42 :0 06 :8 08 :5 20:5\nbr 13:7 10 :9 42 :9 04 :8 06 :5 15:8\npi 12:9 09 :5 45 :1 05 :2 11 :5 17:0\nwh 10:0 09 :1 49 :7 04 :8 08 :5 16:4\nTable 2 : Perception accuracy (in %) of HOT an, ELA ha, DEP sa,\nPAN fe, and WOR fe(cf. Figure 2), in clean (cl) and noisy back-\nground: brown (br), pink (pi), white (wh) at -1 dB SNR. Mean\naccuracy is given; each row gives results for 30 instances.\nThe results for clean conditions show that the emo-\ntional state most accurately perceived is DEP sa(42.0%),\nfollowed by HOT an(26.6%), and ELA ha(18.4%); worse\nrecognised were WOR fe(08.5%) and PAN fe(06.8%).\nHOT anwas mainly confused with COL an, ELA hawith\nPLE ha, and DES sawith DEP sa(cf. Table 1), suggest-\ning that listeners discriminate better between two different\nemotions than between two arousal levels of the same emo-\ntion. The ‘distractors’ DIS and SUR have been rarely cho-\nsen (less than 5.5 %). Confusion between different emo-\ntions within the same arousal level took mostly place be-\ntween HOT anvs ELA ha(high arousal) and WOR fevs\nCOL an(low arousal); this can be explained by the acous-\ntic similarities between them. In Figure 5, the Chroma4\nrepresentation of emotional singing performed by a female\nsinger (soprano) displays that HOT anand DEP saare ex-\npressed differently. HOT an, as shown in acted speech [13],\nis expressed through articulated prosody, acoustically char-\nacterised by a strong decay in amplitude and lower slope\ndeclinations, which is displayed by a richer spectrum on\npartials with less differences between the energy of low\nand high frequencies. DEP sa, on the contrary, is expressed\nthrough sustained amplitude for each note, which concen-\ntrates more energy in F0 and less in higher harmonics.\nELA hapresents a spectrum and articulation at mid point\nbetween the previous ones.\nAs expected (apart from a rare exceptions in the percep-\ntion of female voices at \u00000:5dB SNR in brown noise), lis-\nteners’ accuracy decreases with the increment of noise (cf.\nFigure 4), i. e., higher SNR ( \u00001dB and \u00000:5dB) yielded\nlower accuracy. By evaluating the perception of emo-\ntion in clean and \u00001dB SNR conditions, (cf. Table 2),\nHOT anand ELA hawere affected most by noise, DEP sa\nless, WOR feand PAN fewere perceived similarly to clean\nbackground. The three noises affected perception in a\nsimilar way: brown slightly more (15.8 % mean accu-\n4Chroma features have been extracted by OPEN SMILE [8].% HOT an ELA ha DEP sa PAN fe WOR fe COL an\ncl 63:1 51 :7 92 :3 26 :1 31 :9 93:9\nbr 36:3 41 :4 112 :9 26 :3 32 :4 109:1\npi 44:1 37 :9 138 :1 26 :7 27 :7 107:2\nwh 36:3 34 :8 125 :1 25 :6 28 :8 117:7\nTable 3 : Sum of columns (in %) ‘perceived as’ for the ‘real’ emo-\ntions: HOT an, ELA ha, DEP sa, PAN fe, WOR fe; the ‘distractor’\nCOL an(cf. Figure 2), in clean (cl) and -1 dB SNR background:\nbrown (br), pink (pi), white (wh); each row encodes 30 instances.\nracy), pink less (17.0 % mean accuracy). Yet, the higher\nlevel of accuracy in pink and white noises is due to an\nimprovement—caused by an increment in the confusion\ntowards low aroused emotions—in the accuracy of DEP sa,\nrather than to a lower detriment in the overall accuracy.\nThis phenomenon relates to an acoustic ‘ﬂattening’ by the\nnoise of the characteristics typical of each emotion, caus-\ning perception as sustained, with lower energy, and atten-\nuated articulation, i. e., similarly to low aroused emotions.\nIndeed, the chromogram for HOT an, ELA ha, and DEP sa,\nmasked by pink noise at \u00001dB (cf. Figure 6), displays\ncomparable acoustic representation for the three emotions.\nTo evaluate such phenomena, for each confusion\nmatrix—obtained by the perception in clean and \u00001dB\nSNR conditions—the sum of the columns has been com-\nputed, by that counting for each emotion all the responses\n‘identiﬁed as’ (cf. ‘total’ in Table 1). Conﬁrming previous\nﬁndings [25], the confusion in background noise mostly in-\ncreases for the low aroused emotions DEP saand COL an,\nand decreases for the high aroused HOT anand ELA ha(cf.\nTable 3). No meaningful differences are displayed for the\nother emotions across conditions.\n5. AUTOMATIC RECOGNITION\n5.1 Methods\nWe employed state-of-the-art methods for emotion recog-\nnition of vocal cues by applying a Support Vector Machine\n(SVM) classiﬁer with linear kernel, from the open–source\ntoolkit L IBLINEAR [9], and the ComParE 2013 challenge\nfeatures set [36], extracted with OPEN SMILE [8]. Since\nour goal is to evaluate how background noises may affect\nthe classiﬁcation performance in general, only state-of-the-\nart methods for automatic recognition of emotion in the\noperatic voice [7] have been taken into account.\nFor speaker independence, we split the 390 instances\ninto three sets (A, B, and C), considering for each 130 in-Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 379Hot anger\nTime (ms)50 100 150 200 250B\nA\nG\nF\nE\nD\nC\nElated happiness\nTime (ms)50 100 150 200 250B\nA\nG\nF\nE\nD\nC\nDepressive sadness\nTime (ms)200 400 600B\nA\nG\nF\nE\nD\nCFigure 5 : Chroma representation of the instances expressing: Hot an, ELA ha, and DEP sa(from left to right); sung by one of the soprano.\nThe y axis gives the C natural scale; the x axis the time in miliseconds. Dark blue indicates the lower level of energy, red the higher.\nHot anger\nTime (ms)50 100 150 200 250B\nA\nG\nF\nE\nD\nC\nElated happiness\nTime (ms)50 100 150 200 250B\nA\nG\nF\nE\nD\nC\nDepressive sadness\nTime (ms)200 400 600B\nA\nG\nF\nE\nD\nC\nFigure 6 : Chroma representation of the instances given in Figure 5 masked by pink noise at \u00001dB SNR.\nstances sung by two different singers (one female and one\nmale), and performing the experiments in two phases—\ndevelopment and test. For the development phase we con-\nsidered one set as training (e. g., A), and another as test\n(e. g., B); 30 levels of complexity (from 230to20) have\nbeen tested to optimise the SVM performance. In the test\nphase, we merged the sets A and B for training and consid-\nered the set C as test; the complexity which achieved best\nresults in the development phase was taken into account as\noptimisation parameter for the SVM. This procedure was\ncarried out with the six possible permutations between the\nthree sets, and the results were averaged.\nWe performed binary classiﬁcation on ﬁve classes, i. e.,\neach class was recognised against the other four. In the\ntraining phase, the minority class was upsampled to match\nthe sample size of the remaining classes together; for each\nnoise, all the SNR were considered together. We em-\nployed the whole ComParE 2013 features set [36], en-\ncompassing 6374 acoustic features in total: 64 low-level\ndescriptors—LLD, and several functionals [7], in four sub-\nsets: mel-frequency cepstral coefﬁcients—mfcc (1,400\nfeatures), spectrum (4,300), prosody (183), and voice qual-\nity (390).\n5.2 Results and discussion\nThe classiﬁcation of ﬁve classes (cf. Table 4) mirrors\nthe perception ﬁndings (cf. Table 2) for all the feature\nsets: DEP being classiﬁed best, HOT and ELA in be-\ntween, and PAN and WOR worse. The mfcc sub-set per-\nforms best, showing the highest Unweighed Average Re-\ncall (UAR), i. e., the mean average of the recall per class\nover the six permutations. In order to visualise these re-% HOT ELA DEP PAN WOR UAR\nComParE 26:3 28 :2 77 :6 07 :0 18 :6 31:5\nmfcc 34:6 30 :1 71 :8 07 :7 26 :3 34:1\nspec 26:9 25 :0 82 :0 03 :8 12 :2 30:0\nprosody 17:3 22 :4 48 :1 08 :3 21 :1 23:5\nvq 34:6 27 :6 53 :2 11 :5 14 :7 28:3\nTable 4 : Test classiﬁcation accuracy and Unweighed Average\nRecall (UAR) in % for the ‘real’ emotions (HOT, ELA, DEP,\nPAN, WOR, cf. Figure 2), considering the four conditions—clean\nand the three noises—together, for each feature set: ComParE ,\nmfcc, spectrum (spec), prosody, and voice quality (vq).\n% ComParE mfcc spec prosody vq mean\ncl 35:0 40 :0 36 :6 23 :3 25 :0 32:0\nbr 31:6 35 :4 30 :8 23 :7 28 :7 30:0\npi 32:0 36 :2 28 :3 22 :9 25 :4 29:0\nwh 30:0 29 :1 29 :1 23 :7 31 :6 28:7\nTable 5 : UAR for test in % for each feature set (cf. caption of\nTable 4), in each condition: clean (cl), brown (br), pink (pi), white\n(wh). In noisy background the 4 SNR are considered together.\nsults, in Figure 7, a 2-dimensional Non-Metrical Multi-\nDimensional Scaling (NMDS, [17]) solution is given. It\nshows a non-metrical visual representation of the optimal\ndistances between the evaluated categories. DEP, since\nbest recognised—thus classiﬁed as different—is more dis-\ntant to the other classes in all the emotional constellations.\nThe feature set with the best performance (mfcc, cf. Ta-\nble 4) displays an arousal related pattern, the high aroused\nemotions (HOT, ELA, PAN), clustered together, the low\naroused (WOR, DEP) more distant. This may relate to the\nlevel of energy: higher in the former, lower in the latter\n(cf. Figure 5). The decline in UAR goes together with\nthe condensation of the emotions in the 2-dim space, as380 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 7 : 2-dim NMDS solution for the ﬁve feature sets in the classiﬁcation of the ﬁve ‘real’ emotions, considering the 390 instances\n(cf. caption of Table 4). Kruskal’s stress is given as a measure of ﬁt for 2-dim and 1-dim solution respectively: ComParE (6.3e-07,\n4.0e-05); mfcc (3.4e-07, 0.1); spectrum (1.3e-17, 7.5e-16), prosody (6.9e-07, 9.2e-07); voice quality (1.3e-16, 4.8e-05).\nFigure 8 : 2-dim NMDS solution for listeners’ perception (in clean condition) and classiﬁcation (in clean and noisy conditions) with mfcc\nfeatures, of the ﬁve ‘real’ emotions. Kruskal’s stress is given for 2-dim and 1-dim solution respectively: Perception (4.3e-17, 6.6e-07);\nClean (1.2e-16, 7.3e-07); Brown (1.3e-16, 0.02), Pink (3.9e-07, 1.3e-05); White (6.9e-07, 3.0e-04).\nHOT ELA DEP PAN WOR405060708090%\nClean\nBrown\nPink\nWhite\nFigure 9 : UAR and std in % for binary classiﬁcation, i. e., each\nemotion against the other four, in the four conditions (cf. caption\nof Figure 8), for the mfcc sub-set.\nprominently shown for the voice quality sub-set (cf. Fig-\nure 7). As for listeners’ perception (cf. Table 2), for mfcc,\nComParE , and to some extent for spectral features, high-\nest accuracy was achieved in clean condition, medium in\npink and brown, lower in white (cf. Table 5). Prosodic and\nvoice quality features performed worst, which relates to\nboth the considered musical instances and to the operatic\ntechnique. On the one hand, the sung melodic contour is\nthe same for all utterances and emotions; thus, there are\nno degrees of freedom for pitch leftover for the marking\nof emotions. On the other hand, opera singing is charac-\nterised by the ‘projection’ of the voice—a high control of\narticulation (and by that, mfcc conﬁgurations), and a weak\nuse of different voice qualities when expressing emotions,\nin contrast, for instance, to modern actors or pop singers.\nIn Figure 8, an NMDS visualisation for perception (in\nclean condition) and mfcc classiﬁcation (in the different\nbackgrounds) is given. The confusion in the perceptual\nconstellation relates mainly to the low accuracies achieved\nin the listening test, which is given mostly by the use of\n‘distractors’. As shown in Table 5, classiﬁcation in clean\nbackground yields the highest UAR, which is visually mir-\nrored by the arousal-related pattern previously described,\ni. e., high aroused emotions clustered together, low arouseddistant (DEP more, WOR less); this is more or less pre-\nserved for brown and pink noise but not for white noise\nwith lowest UAR, cf. Table 5.\nThe binary classiﬁcation (cf. Figure 9) conﬁrms again\nthe perceptual ﬁndings (cf. Table 2): DEP best recognised,\nHOT and ELA at a medium level, PAN worse. WOR is\nbetter classiﬁed than perceived, which relates to the spread\nof the listeners’ responses motivated by the ‘distractor’\nCOL an. Indeed, WOR—having the same arousal—was\nmainly misclassiﬁed by the listeners as COL an, thus de-\ncreasing the perception accuracy of the former. White\nnoise seems to affect binary classiﬁcation more which\nmight suggest that higher frequencies (more masked in\nwhite noise) could be more relevant for the identiﬁcation\nof emotion in singing; lower frequencies (more masked in\npink and brown noises), since related to pitch—thus to the\nmelodic contour, which is the same for all the samples—\nmight be less relevant for the emotional understanding in\nthis speciﬁc study but not in general.\n6. CONCLUSIONS\nThe present study shows that brown, pink, and white\nnoises affect similarly the perception of emotion in op-\neratic singing: the lower the SNR, the lower the percep-\ntion. Gender seems not be an inﬂuential factor, neither\nfor singers nor for listeners. In general, perception and\nclassiﬁcation shows analogous emotional constellations re-\ngardless the background, sadness being identiﬁed best, fear\nworst. The use of ‘distractors’ inﬂuences listeners’ percep-\ntion, affecting even more the accuracy of fear, an emotion\nwhich seems not to have a typical expression in singing;\nthus it is worse identiﬁed and easily confused. V oice qual-\nity features perform worst, mfcc best. In the former, this\nrelates to the voice ‘projection’ inherent to opera (which\nminimise the differences between emotions), in the latter,\nto the relevance of energy per band to discriminate between\nsung emotions. Listeners’ low accuracy suggests that iden-\ntifying emotion in opera singing may be challenging for\nnon trained subjects; thus, musically trained listeners will\nbe considered in future investigations.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 3817. ACKNOWLEDGEMENT\nThis work was supported by the European Unions’s\nSeventh Framework and Horizon 2020 Program un-\nder grant agreement No. 338164 (ERC StG iHEARu).\n8. REFERENCES\n[1] R. Banse and K. R. Scherer, “Acoustic proﬁles in vocal\nemotion expression.” Journal of personality and social\npsychology , vol. 70, p. 614, 1996.\n[2] L. Beranek, Concert halls and opera houses: Music and\nacoustics . New York, NY: Springer, 2012.\n[3] M. Cognini, A. Farina, and R. Pompoli, “L’acustica\ndell’anﬁteatro romano Arena di Verona,” in Proc. of\nAcoustics and Recovery of Spaces for Music . Ferrara,\nItaly: ACM, 1993, pp. 105–118.\n[4] E. Coutinho, K. R. Scherer, and N. Dibben, “Singing and\nemotion,” in The Oxford handbook of singing , G. Welch,\nD. M. Howard, and J. Nix, Eds. Oxford, UK: OUP, 2014.\n[5] T. Eerola and J. K. Vuoskoski, “A comparison of the dis-\ncrete and dimensional models of emotion in music,” Psy-\nchology of Music , vol. 39, no. 1, pp. 18–49, 2011.\n[6] P. Ekman, “Expression and the nature of emotion,” Ap-\nproaches to Emotion , vol. 3, pp. 19–344, 1984.\n[7] F. Eyben, G. L. Salomão, J. Sundberg, K. R. Scherer, and\nB. W. Schuller, “Emotion in the singing voice – a deeper\nlook at acoustic features in the light of automatic classiﬁ-\ncation,” EURASIP Journal on Audio, Speech, and Music\nProcessing , vol. 1, pp. 1–9, 2015.\n[8] F. Eyben, M. Wöllmer, and B. Schuller, “Opensmile: The\nMunich versatile and fast open-source audio feature ex-\ntractor,” in Proc. of ACM MM , 2010, pp. 1459–1462.\n[9] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-\nJ. Lin, “Liblinear: A library for large linear classiﬁcation,”\nJMLR , vol. 9, pp. 1871–1874, 2008.\n[10] M. García and D. V . Paschke, A complete treatise on the\nart of singing . New York, NY: Da Capo, 1975.\n[11] S. Godsill, P. Rayner, and O. Cappé, “Digital audio\nrestoration,” in Applications of digital signal processing\nto audio and acoustics , M. Kahrs and K. Brandenburg,\nEds. Boston, MA: Springer, 2002, pp. 133–194.\n[12] D. J. Grout and C. V . Palisca, A history of western music .\nNew York, NY: Norton, 2001.\n[13] M. Guzman, S. Correa, D. Muñoz, and R. Mayerhoff, “In-\nﬂuence on spectral energy distribution of emotional ex-\npression,” Journal of Voice , vol. 27, pp. 129–139, 2013.\n[14] S. Hantke, F. Eyben, T. Appel, and B. Schuller, “iHEARu-\nPLAY: Introducing a game for crowdsourced data collec-\ntion for affective computing,” in Proc. of WASA . Xi’an,\nChina: IEEE, 2015, pp. 891–897.\n[15] P. Howes, J. Callaghan, P. Davis, D. Kenny, and\nW. Thorpe, “The relationship between measured vi-\nbrato characteristics and perception in western operatic\nsinging,” Journal of Voice , vol. 18, pp. 216–230, 2004.\n[16] S. Jansens, G. Bloothooft, and G. de Krom, “Perception\nand acoustics of emotions in singing,” in Proc. of Eu-\nrospeech . Rhodes, Greece: ISCA, 1997, pp. 2155–2158.\n[17] J. Kruskal and M. Wish, Multidimensional Scaling . Lon-\ndon, U.K.: Sage University, 1978.\n[18] P. Laukka, “V ocal expression of emotion: discrete-\nemotions and dimensional accounts,” Ph.D. dissertation,\nActa Universitatis Upsaliensis, 2004.\n[19] S. R. Livingstone, K. Peck, and F. A. Russo, “Acoustic\ndifferences in the speaking and singing voice,” in Proc. of\nMeetings on Acoustics , Montreal, QC, 2013, pp. 1–5.\n[20] I. W. Mabbett, “Buddhism and music,” Asian Music ,\nvol. 25, no. 1/2, pp. 9–28, 1993.[21] I. Mathworks, “Matlab: R2014a,” Natick , 2014.\n[22] M. Mauch and S. Ewert, “The audio degradation toolbox\nand its application to robustness evaluation,” in Proc. of\nISMIR , Curitiba, Brazil, 2013, pp. 83–88.\n[23] V . P. Morozov, “Emotional expressiveness of the singing\nvoice: The role of macrostructural and microstructural\nmodiﬁcations of spectra,” Logopedics Phoniatrics Vocol-\nogy, vol. 21, pp. 49–58, 1996.\n[24] I. R. Murray and J. L. Arnott, “Implementation and test-\ning of a system for producing emotion-by-rule in synthetic\nspeech,” Speech Comm. , vol. 16, pp. 369–390, 1995.\n[25] E. Parada-Cabaleiro, A. Baird, A. Batliner, N. Cummins,\nS. Hantke, and B. Schuller, “The Perception of Emotions\nin Noisiﬁed Non-Sense Speech,” in Proc. of Interspeech .\nStockholm, Sweden: ISCA, 2017, pp. 3246–3250.\n[26] E. Parada-Cabaleiro, A. Baird, A. Batliner, N. Cummins,\nS. Hantke, and B. W. Schuller, in Proc. of DLFM . ACM,\n2017, pp. 29–36.\n[27] E. Rapoport, “Emotional expression code in opera and\nlied singing,” JNMR , vol. 25, pp. 109–149, 1996.\n[28] J. A. Russell, “A circumplex model of affect,” Journal\nof Personality and Social Psychology , vol. 39, pp. 1161–\n1178, 1980.\n[29] K. R. Scherer, “V ocal communication of emotion: A re-\nview of research paradigms,” Speech Comm. , vol. 40, pp.\n227–256, 2003.\n[30] ——, “V ocal communication of emotion: A review of re-\nsearch paradigms,” Speech Comm. , vol. 40, pp. 227–256,\n2003.\n[31] K. R. Scherer, R. Banse, and H. G. Wallbott, “Emo-\ntion inferences from vocal expression correlate across lan-\nguages and cultures,” Journal of Cross-Cultural Psychol-\nogy, vol. 32, pp. 76–92, 2001.\n[32] K. R. Scherer, J. Sundberg, B. Fantini, S. Trznadel, and\nF. Eyben, “The expression of emotion in the singing voice:\nAcoustic patterns in vocal performance,” JASA , vol. 142,\npp. 1805–1815, 2017.\n[33] K. R. Scherer, J. Sundberg, L. Tamarit, and G. L. Sa-\nlomão, “Comparing the acoustic expression of emotion in\nthe speaking and the singing voice,” Computer Speech &\nLanguage , vol. 29, pp. 218–235, 2015.\n[34] K. R. Scherer, S. Trznadel, B. Fantini, and J. Sundberg,\n“Recognizing emotions in the singing voice,” Psychomu-\nsicology: Music, Mind & Brain , vol. 27, pp. 244–255,\n2017.\n[35] B. Schuller, D. Arsi ´c, F. Wallhoff, and G. Rigoll, “Emo-\ntion recognition in the noise applying large acoustic fea-\nture sets,” in Proc. of Speech Prosody . Dresden, Ger-\nmany: ISCA, 2006, pp. 276–289.\n[36] B. Schuller, S. Steidl, A. Batliner, A. Vinciarelli,\nK. Scherer, F. Ringeval, M. Chetouani, F. Weninger, F. Ey-\nben, E. Marchi et al. , “The Interspeech 2013 computa-\ntional paralinguistics challenge: Social signals, conﬂict,\nemotion, autism,” in Proc. of Interspeech . Lyon, France:\nISCA, 2013, pp. 148–152.\n[37] H. Siegwart and K. R. Scherer, “Acoustic concomitants\nof emotional expression in operatic singing: The case of\nLucia in Ardi gli incensi,” Journal of Voice , vol. 9, pp.\n249–260, 1995.\n[38] J. Stark, Bel canto: A history of vocal pedagogy . London,\nU.K.: University of Toronto Press, 2003.\n[39] S. Trehub, A. Unyk, and L. Trainor, “Adults identify\ninfant-directed music across cultures,” Infant Behavior\nand Development , vol. 16, pp. 193–211, 1993.\n[40] B. Zhang, G. Essl, and E. M. Provost, “Recognizing emo-\ntion from singing and speaking using shared models,” in\nProc. of ACII . IEEE, 2015, pp. 139–145.382 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Musical-Linguistic Annotations of Il Lauro Secco.",
        "author": [
            "Emilia Parada-Cabaleiro",
            "Maximilian Schmitt",
            "Anton Batliner",
            "Björn W. Schuller"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492451",
        "url": "https://doi.org/10.5281/zenodo.1492451",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/11_Paper.pdf",
        "abstract": "In madrigals, The Italian madrigal, a polyphonic secular a cappella composition of the 16th century, is characterised by a strong musical-linguistic relationship, which has made it an icon of the 'Renaissance humanism'. lyrical meaning is mimicked by the music, through the utilisation of a composition technique known as madrigalism. The synergy between Renaissance music and poetry makes madrigals of great value to musicologists, linguists, and historians—thus, it is a promising repertoire for computational musicology. However, the application of computational techniques for automatic detection of madrigalisms within scores of such repertoire is limited by the lack of annotations to refer to. In this regard, we present 30 madrigals of the anthology Il Lauro Secco encoded in two symbolic formats, MEI and **kern, with hand-encoded annotations of madrigalisms. This work aims to encourage the development of algorithms for madrigalism detection, a composition procedure typical of early music, but still underrepresented in music information retrieval research.",
        "zenodo_id": 1492451,
        "dblp_key": "conf/ismir/Parada-Cabaleiro18a",
        "keywords": [
            "madrigals",
            "polyphonic",
            "secular",
            "a cappella",
            "16th century",
            "Italian madrigal",
            "Renaissance humanism",
            "lyrical meaning",
            "madrigalism",
            "synergy between Renaissance music and poetry"
        ],
        "content": "MUSICAL-LINGUISTIC ANNOTATIONS OF IL LAURO SECCO\nEmilia Parada-Cabaleiro1Maximilian Schmitt1Anton Batliner1Björn W. Schuller1;2\n1ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany\n2GLAM – Group on Language, Audio & Music, Imperial College London, UK\nemilia.parada-cabaleiro@informatik.uni-augsburg.de\nABSTRACT\nTheItalian madrigal , a polyphonic secular a cappella com-\nposition of the 16thcentury, is characterised by a strong\nmusical-linguistic relationship, which has made it an icon\nof the ‘Renaissance humanism’. In madrigals, lyrical\nmeaning is mimicked by the music, through the utilisa-\ntion of a composition technique known as madrigalism .\nThe synergy between Renaissance music and poetry makes\nmadrigals of great value to musicologists, linguists, and\nhistorians—thus, it is a promising repertoire for computa-\ntional musicology. However, the application of computa-\ntional techniques for automatic detection of madrigalisms\nwithin scores of such repertoire is limited by the lack of\nannotations to refer to. In this regard, we present 30 madri-\ngals of the anthology Il Lauro Secco encoded in two sym-\nbolic formats, MEI and **kern, with hand-encoded an-\nnotations of madrigalisms. This work aims to encourage\nthe development of algorithms for madrigalism detection,\na composition procedure typical of early music, but still\nunderrepresented in music information retrieval research.\n1. INTRODUCTION\nThe Italian madrigal of the 16thcentury is a secular\npolyphonic vocal composition characterised by the use of\nmadrigalisms , a composition technique that mimics the\nlinguistic content of the lyrics (e. g., emotional concepts\nsuch as happiness or sorrow) through the music [14]. This\nsynergy between poetry and music shows the important\nrole that the arts played in the development of the ‘Re-\nnaissance humanism’ [29]. Given the intellectual and cul-\ntural repercussion of this philosophical movement in West-\nern Europe [13], madrigals evoke high interest for musi-\ncological, linguistic, and historical research. Yet, for the\ncomprehension of madrigals, advanced knowledge of the\nItalian language and poetry, as well as music analysis ex-\npertise and knowledge of mensural notation [1] are essen-\ntial. Since music historians, literary scholars, and librari-\nans not always have all these abilities, the development of\nautomatic systems for musical-linguistic synergy detection\nc\rEmilia Parada-Cabaleiro, Maximilian Schmitt, Anton\nBatliner, Björn W. Schuller. Licensed under a Creative Commons Attribu-\ntion 4.0 International License (CC BY 4.0). Attribution: Emilia Parada-\nCabaleiro, Maximilian Schmitt, Anton Batliner, Björn W. Schuller.\n“Musical-linguistic annotations of Il Lauro Secco ”, 19th International So-\nciety for Music Information Retrieval Conference, Paris, France, 2018.within madrigals would assist them in analytical, pedagog-\nical, and cataloguing tasks.\nThe application of machine learning techniques to early\nmusic is restricted by early music being mainly con-\nserved in scanned copies of the original, i. e., no sym-\nbolic (machine-readable) information is available. To ad-\ndress this limitation, Optical Music Recognition (OMR)\nhas shown promising results in the automatic generation\nof symbolic representations of such repertoire [6]. Nev-\nertheless, in the framework of automatic analysis within\nsymbolically encoded scores, for the development of suc-\ncessful systems able to automatically interpret composition\nprocedures, appropriate annotations of such techniques are\nessential. Despite the large amount of scores from early\nmusic repertoire freely available on-line, symbolically en-\ncoded or not, labeled early music is still missing. Our work\nrepresents an initial contribution to address this lacuna,\nby presenting the symbolically encoded transcription and\nannotated representation of 30 madrigals of the Il Lauro\nSecco anthology [21]. A total of 120 scores are presented,\n60 in MEI and 60 in **kern—30 of each annotated1.\nWith the presented work, we aim at encouraging the de-\nvelopment of algorithms for pattern recognition that would\npursue identiﬁcation of musical-linguistic synergies, as\ne. g., madrigalisms. This will advance automatic analy-\nsis techniques, whose practical applications could help re-\nsearchers from diverse ﬁelds (e. g., musicology, linguistics,\nand history) by assisting them in the evaluation of artis-\ntic Renaissance manifestations. The manuscript is laid out\nas follows: an overview of related work (Section 2); an\nevaluation of musical-linguistic connections in the Italian\nmadrigal and in the presented repertoire (Sections 3 and\n4); a description of the annotation methodology (Section\n5); an outline of the annotated repertoire (Section 6); ﬁ-\nnally, conclusions and future work (Section 7).\n2. RELATED WORK\nGiven the musical, literary, and historical value of the Ital-\nian repertoire of the late 16thand early 17thcenturies,\nsome initiatives, such as Tasso in Music Project [25]2or\nThe Marenzio Online Digital Edition – MODE3, spend\ngreat effort in making available online symbolic represen-\ntations of such repertoire. Even though analytical tools are\n1https://github.com/SEILSdataset/SEILSdataset\n2http://www.tassomusic.org/\n3http://www.marenzio.org/about-mode.html461&\n&VV\n?c\ncccc\nŒœœœœœ\n1HO IR\n\n\nÇ .œJœ\n1HO IR FR\nœœœœÇ#\nÇ .œjœ\n1HO IR FR\n\nœœÇ\nG\nXQ EHO ODX\nÇœœ\nFR G\nXQ EHO\nœœÇ\nG\nXQ EHO ODX\nŒœœœœœ\n1HO IR\nÇ»\nUR\nÇ .œJœ\n1HO IR FRÇÇ\nODX UR\nœœ .œjœ\nUR 1HO IR FR\nœœœœÇ#\n» Œœ\nG\nXQ\nœœÇ\nG\nXQ EHO ODX\u0010\u0010 \u0010\n\u0010\u0010\u0010\u0010\u0010\n\u0010\u0010\n\u0010\u0010\u0010\u0010\u0010\u0010\u0010 \u0010C\nAQTBFigure 1 : Example of Contrapunctal madrigalism (CON) in Gio-\nvanelli’s madrigal. The word foco (ﬁre) is mimicked by a contra-\npunctal texture where the ﬁve voices are involved: C (Canto) and\nQ (Quinto) perform the motive 1 (highlighted in red), in which\nthe word foco is displayed by a melisma; A (Alto), T (Tenor),\nand B (Basso) perform the motive 2 (in blue for T and B), being\nconsidered the contrary motion for A (in green).\nprovided by these initiatives, such as text extraction, word\ncounting, or graphic representation of pitch and rhythm,\nthe symbolically encoded scores, presented in a variety of\nformats, such as MEI [27]4or **kern [15], do not con-\ntain annotations of the musical content. This may limit,\ne. g., the evaluation of the performance of analytical toolk-\nits, such as Humdrum Toolkit [15]5and music21 [7]6,\nsince no ground truth is provided.\nGround truth is essential in the development of algo-\nrithms for music information retrieval. Due to this, datasets\nwith annotated information have been developed in order\nto support a variety of machine learning tasks, as e. g.,\nOMR [22], or harmonic analysis [8]. With the rise of the\nworld-wide web, crowd-sourcing has become a very effec-\ntive strategy to collect annotations [9]. Indeed, within the\nframework of digital score libraries, this has been consid-\nered for web-based annotation tools [26] as well as to col-\nlaboratively perform hand-written transcription [4]. Nev-\nertheless, the annotation of musical content could require a\nmusicological expertise, as e. g., harmonic analysis [8], or\nthe identiﬁcation of melodic similarities [28] which would\nmake a collaborative annotation system impracticable, thus\nleading to consider only a limited number of annotators.\n3. RHETORIC & MUSIC IN THE ITALIAN\nMADRIGAL\nRhetoric is the discipline that, through an efﬁcient codiﬁ-\ncation of the discourse (either spoken or written), achieves\nto convince the audience. Having a consolidated tradition\nfrom the times of the ancient Greece [2], in the 16thcen-\ntury, this discipline has been directly applied to music, lay-\ning the foundation of Musica Poetica [5]. This stylistic\nmovement is founded in a close collaboration between po-\netry and music, by highlighting the emotional content of\nthe text through the use of musical-rhetoric ﬁgures, which\nwill evolve in the 17thcentury into the Affektenlehre , i. e.,\n4http://music-encoding.org/\n5http://www.humdrum.org/\n6http://web.mit.edu/music21/the ‘Doctrine of the affections’ [17]. As these musical-\nrhetoric principles are characteristic of the Italian madrigal\nfrom the 16thcentury, such ‘word painting’ strategies are\nalso known as madrigalisms [24]. In madrigalisms, the\nuse of ‘chromatism’ is progressively introduced, a prac-\ntice typical of Monteverdi, who at the beginning of the\n17thcentury coined that known as Seconda pratica [3]: a\nnew conception of composition in which the music should\nbe governed by the words, thus justifying dissonances and\nmelodic movements that were considered unacceptable till\nthat time, according to Zarlino’s harmonic rules [30].\nYet, the madrigal of the 16thcentury is characterised\nby madrigalisms which relate to the alternation of musical\ntextures, and not to chromatism, as typical for the madri-\ngal of the 17thcentury. The madrigal of the 16thcentury,\nsince based on strong musical-linguistic synergies, differs\nclearly from other contemporary musical genres such as\nfrottola , in which such ‘word painting’ strategies are not\npresent [14]. Indeed, the artistic value of this madrigal re-\nlates also to the high qualiﬁcation of poets, composers, and\ninterpreters involved in such artistic representation, though\nto be interpreted in high status social reunions, i. e., in the\ncourt [20]. In this regard, the music of the madrigal, in\ncontrast to the frottola , shows a more free representation\nof the text, highlighting its content (usually related to pas-\ntoral, sentimental, and erotic themes) through virtuous mu-\nsical writing [12]. Thus, the essential point of the Italian\nmadrigal of the 16thcentury is that the composer puts the\nmusic into the same artistic level as the poetry [14].\nThe Il Lauro Secco anthology, published for the ﬁrst\ntime by Angelo Gardano in 1582 at Ferrara (Italy) [18], is\na good example of such a repertoire, since both music and\nlyrics were created by some of the most reputable com-\nposers and poets of the time [20]. Furthermore, it was in-\ntended to be interpreted in the court of Ferrara, by the Con-\ncerto delle donne [10], a vocal ensemble of professional\nsingers, which rapidly became an example for other con-\ntemporary courts, transforming Italy, for the ﬁrst time, into\nthe center of music in Europe [14]. Moreover, Il Lauro\nSecco was conceived as a unitary anthology with a com-\nmon theme where music and poetry of all the madrigals\nwere expressively created for the anthology itself, whose\npurpose was to be a wedding present for Laura Pever-\nara [11,19], one of the singers of the Concerto delle donne .\n4. MUSICAL-LINGUISTIC SYNERGIES IN\nIL LAURO SECCO\nIn the madrigals of Il Lauro Secco (‘The Dry Laurel’), the\nmeaning of the lyrics is expressed mainly through textu-\nral ‘musical metaphors’ and diatonic writing. Thus, the\n‘word painting’ procedures are musically driven by the al-\nternation of diverse musical textures, which we will iden-\ntify as contrapunctal, homorhythmic, and antiphonal; the\nmelodic development ﬂows through step-wise motion, i. e.,\nthe melody is performed in conjunction, so each note is\nfollowed by the immediate upper or lower note. For this,\nrhythmic-melodic ‘motifs’ are chosen to represent each\nverse of the lyrics, and are placed into speciﬁc musical tex-462 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018œ\nFR\nœ\nFR\nœ\nFRI.Ç œ\nPHX QL\n.Ç œ\nPHX QL\n.Ç œ\nPHX QL\n.Ç œ#\nPHX QLœ œÇ\nFD )H QL\nœ œÇ\nFD )H QL\nœ œÇ\nFD )H QL\nœ œÇ#\nFD )H QLÇ\nFH\n.Ç\nFH\nÇ\nFH\n.Ç\nFH\u0010\u0010 \u0010 \u0010 \u0010\n\u0010\u0010 \u0010 \u0010 \u0010\n\u0010\u0010 \u0010 \u0010 \u0010\u0010\u0010 \u0010 \u0010 \u0010œœœ\nFR\nœœœœ\nFR\nœœœ\nFRI\u0010.ÇÇÇÇ\nPHX\n.ÇÇÇÇ\nPHX\n.ÇÇÇÇ\nPHX\n.ÇÇÇ\nPHXœœœ\nQL\nœœœœ\nQL\nœœœ\nQL\nœœœ###\nQL###œœœœ\nFD\nœœœ\nFD\nœœœœ\nFD\nœœœ\nFDœœœ\n)H\nœœœ\n)H\nœœœ\n)H\nœœœœ\n)HÇÇÇ\nQL\nÇÇÇÇ\nQL\nÇÇÇ\nQL\nÇÇÇ###\nQL\u0010\n\u0010\n\u0010\u0010ÇÇÇ\nFH\n.ÇÇÇ\nFH\nÇÇÇ\nFH\n.ÇÇÇ\nFHC\nAQTFigure 2 : Example of homorhythmic madrigalism (HOM) in\nGiovannelli’s madrigal. The voices C (Canto), A (Alto), Q\n(Quinto), and T (Tenor) perform the same musical-linguistic pat-\ntern simultaneously to musically mimic the lyrics’ content. No-\ntice that all the voices are written in treble clef (for T subctave).\ntures. These motifs are characterised by speciﬁc rhythms\nand melodic contours that musically mimic the meaning\nof the lyrics, both linguistically (e. g., love as positive and\nhate as negative), and metaphorically (e. g., the word green\nas a synonym for life); thus, we refer to these motifs and\ntheir related lyrics as ‘musical-linguistic patterns’. The\nmusical texture determines how the musical-linguistic pat-\nterns interact between them across the different voices.\nSince other ‘word painting’ strategies, as e. g., those based\non melodic contour and chromatism [14], are not as rep-\nresentative of the presented anthology as those based on\nmusical texture, only madrigalisms which relate to the al-\nternation of musical textures will be taken into account for\nthe annotations. For an evaluation of more ‘typical’ madri-\ngalism, as those based on chromatism, repertoire from the\n17thcentury should be considered.\n4.1Madrigalisms based on Contrapunctal Texture\nIn contrapunctal madrigalisms —CONs, the same musical-\nlinguistic pattern is staggered along the timeline over the\ndifferent voices: Canto (C), Alto (A), Quinto (Q), Tenor\n(T), and Basso (B), from the highest to the lowest. In Fig-\nure 1, an example of CON is given. The extracted pas-\nsage is composed considering two different motifs: motif\n1 highlighted in red (voices C and Q), motif 2 highlighted\nin green (voice A) and blue (voices T and B). Motif 2 in\nvoice A is displayed in contrary motion, i. e., a melody in\nopposite direction w. r. t. the voices T and B.\nIn this madrigalism, the word foco (ﬁre) is mimicked by\nmusic as a dynamic and confused state, as it relates to ﬁre\nas a physical phenomenon (and its typical instability) as\nwell as a metaphor of love. The dynamism and confusion\ninherent of this concept is enhanced through a contrapunc-\ntal texture (most typical composition technique to create\nmovement) as well as through the use of two contrasting\nmotifs. The ﬁrst of these is characterised by fast rhythm\n(made up of eighth-notes) and rising ‘melismatic prosody’\n(a single syllable of text is sung through several different\nnotes), whereas the second is characterised by a slower\nrhythm and descending ‘syllabic prosody’ (each syllable\nÇ œ\n(F FR\nÇ œ\n(F FRIŒÇ œ\n(F FR\n\nÇÇ\n(F FR\n\nŒÇ œ\n(F FRÇÇ\n(F FR\nŒÇ œ\n(F FR\n» Œ œ\nFK\nj\nŒÇ œ\n(F FR\nÇÇ\n(F FR?Œ\nÇ\n(F\nÇ\nPHÇ\n(F\n»\u0010\u0010\n\u0010\n\u0010\u0010\u0010\n\u0010\u0010\u00104$\n7\n%CFigure 3 : Example of antiphonal madrigalism (ANTIF) in Mas-\nsaino’s madrigal. The musical-linguistic pattern is displayed al-\nternatively by couples of voices: Q (Quinto) and T (Tenor)—in\ngreen, C (Canto) and B (Basso)—in blue, A and T—in red, high-\nlighting the word eco(similar to ‘echo’) by a musical metaphor.\nC, A, Q, and T are written in treble clef (for T suboctave), B in\ntenor clef, i. e., C–clef in the fourth line from the bottom.\nof the text corresponds to a different note).\n4.2Madrigalisms based on Homorhythmic Texture\nIn homorhythmic madrigalisms—HOMs, a given musical-\nlinguistic pattern occurs simultaneously in the different\nvoices. In the identiﬁcation of HOM, rhythmically char-\nacterised musical-linguistic patterns must be considered,\nregardless of the melodic contours, since in homorhythmic\ntextures, melodic changes in voices are essential for creat-\ning harmonic relationships between voices, so no charac-\nteristic melodies would be found. In Figure 2, homorhyth-\nmic texture is used to represent the sentence come unica\nFenice (as the only one Phoenix) in music. This sentence\nis a metaphor of reciprocal love, so the composer utilises\nHOM to mimic the stillness related to the stability typical\nof this emotional state. This quiet atmosphere is encour-\naged by the use of step-wise motion in all the voices.\n4.3Madrigalisms based on Antiphonal Texture\nIn antiphonal madrigalisms —ANTIFs, a given musical-\nlinguistic pattern (usually performed by two voices si-\nmultaneously) is displayed by alternating ‘entries’ through\nthe different voices, creating an acoustic effect similar to\n‘echo’. ANTIFs could be identiﬁed as a texture at the\nmid-point between counterpoint and homorhythm, since\nthe consecutive repetition of a musical-linguistic pattern\nis displayed sometimes before the previous has concluded\n(as in contrapunctal texture), and this is displayed in dif-\nferent voices simultaneously (as in homorhythmic texture).\nYet, ANTIFs are characterised by a clear alternation of the\nmusical-linguistic pattern entries, which are mainly per-\nformed by a couple of voices, thus showing a texture not\nso confused as in CON, and less dense as in HOM.\nIn Figure 3, antiphonal texture is used to highlight\nthe similarity between the word ecco (interjection used toProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 463Nelfo\nNelfoco*CON1\n*CON2--- melisma ---\nNel foco\nd'unbellau*CON2invco d'unbel\nd'unbellau\nNelfo\nro\nNelfoco--- melisma ---\n**CON1\n*CON2lau ro\nroNelfoco\nd'un\nd'unbellau*\n**CON2inv\n*imitNel\nd'unbellau\ncod'unbel\nbellau\nro Nel**CON2\n*CON1tran\n6\nfocod'unbel\nrod'un bel\nlau\nro\nfo**imit\n*\n--- melisma ---lau ro\nlau roNel\nro d'un\nd'un bel\nco*\n*\n**imit\n*imitd'un\nfo co\nbellau\nlau*imit\nbellau\nd'un bel\nrod'unbel\nrod'unbellau\nd'unbel*imitlau\nlau\nlauNel foco d'un bel lauroFigure 4 : Engraved version of the annotation in MEI for the\nﬁrst madrigalism (CON) of Giovanelli’s madrigal. Two motifs\n(CON1 and CON2), one displayed in contrary motion (inv), and\na melisma are indicated (cf. Figure 1).\nclaim attention), and eco(acoustic phenomenon for which\na sound, through the reﬂections, is repeatedly perceived,\ni. e., ‘echo’). Here, the ‘word painting’ procedure is based\non the acoustic metaphor generated by the phonetic simi-\nlarity between the two words. This is a typical example of\nANTIF, where each repetition of the musical-linguistic pat-\ntern (which consist in two repetitions of the word ecco mu-\nsicalised by a syllabic motif based on a descending third)\nstarts just before the previous has ﬁnished and is performed\nalternatively by different couples of voices.\n5. ANNOTATION METHODOLOGY\n5.1 Encoding formats\nWe present 30 madrigals of the Il Lauro Secco anthol-\nogy transcribed in modern notation and encoded in MEI\nand **kern format. For both formats, the annotated and\nnot annotated symbolic scores (cf. subsections 5.2 and\n5.3) are included—120 symbolic representations in to-\ntal, 60 for each format (30 annotated). Both represen-\ntations have been generated from the Music XML repre-\nsentation of the repertoire given in [21]. The MEI rep-\nresentation has been generated through the on-line Mu-\nsic XML converter Verovio [23]7, whereas **kern ﬁles\nhave been produced by using the xml2hum compiled pro-\ngram of Humdrum-extras toolkit [16]8. Conversion\nerrors were manually corrected; given the difﬁculty to\nﬁnd several annotators with the adequate expertise, the 30\nmadrigals were annotated by only one expert (one of the\nauthors). Aware of the limitations due to taking into ac-\ncount one single annotator, we will focus on the devel-\nopment of an annotation methodology which adequately\ndescribes the considered composition strategies; yet, the\npresented annotations might be subject to some bias. No-\ntice that both, the original Music XML ﬁle and the newly\npresented symbolic transcriptions in MEI and **kern, take\n7http://www.verovio.org/musicxml.html\n8extras.humdrum.org/man/xml2hum/into account the accidentals of the original source, some-\nthing relevant to consider since in early music, even though\nsome accidentals are not written, they might be considered\nwhen performing the repertoire. In this regard, when play-\ning the MEI and **kern ﬁles, some dissonances should\nnot be considered as ‘real’ indications of the composer,\nbut just as the result of performing a ‘diplomatic’, faithful\ntranscription of the source. A transcription which contains\ncautionary accidentals is included in ﬁnale andpdfformats\nin [21].\n5.2 Annotation in MEI\nFor the annotation of the madrigalisms in MEI, the func-\ntion<harm>has been considered, which visually en-\ngraves the annotations above each staff. For each voice,\neach single musical-linguistic pattern within a madrigal-\nism has been marked by a starting and ending point, indi-\ncated by ‘ * ’, followed by the name of the madrigalism,\ni. e., CON, HOM, and ANTIF (cf. Figure 4). Additional\ncomposition strategies have also been indicated:\nMelisma (mel) : When several notes are performed for a\nsyllable of the text (cf. Figure 4 upper staff). Notice that\ntypical embellishments, i. e., ornaments added to a note to\n‘brieﬂy’ decorate it are not considered a melisma .\nInversion (inv) : When the melodic line of a musical-\nlinguistic pattern is displayed in contrary motion w. r. t.\nthe ‘reference’, i. e., the ﬁrst presentation of such musical-\nlinguistic pattern (cf. Figure 4, second staff from the top).\nAcephalous (acef) : When a musical-linguistic pattern\nstarts without the initial part present in the reference. See,\ne. g., CON in Marenzio’s madrigal at measure 27.\nMultiple voices : Double and triple voices, i. e., voices\nthat perform simultaneously the same musical-linguistic\npattern, are intrinsic of HOMs and ANTIFs. However,\nthis procedure may also be considered in CONs—when a\nmusical-linguistic pattern is performed simultaneously by\nmore than one voice; yet, it is possible to perceive the\ncontrapunctal texture. Such voices have been indicated\nas ‘CONdouble’ or ‘CONtriple’ (see, e. g., the CON of\nGabrieli’s madrigal at measure 12). Notice that ‘anticipa-\ntions’ and ‘retardations’ (i. e., when one of the voices, per-\nformed simultaneously, starts before or ﬁnishes after the\nothers), since typical of madrigalisms, have not been taken\ninto account for the annotation.\nRepetition (rep) : When a musical-linguistic pattern is re-\npeated in the same voice within a madrigalism, this has\nbeen indicated as *rep*. When a whole madrigalism is re-\npeated, this has been indicated as *CONrep*, *HOMrep*,\nand *ANTIFrep*. Notice that the end of madrigalisms is\nusually denoted by rests, and their repetition uses to be per-\nformed by a different combination of voices. See, e. g., the\nHOM of Fronti’s madrigal at measure 19 (four voices) and\nits repetition at measure 23 (ﬁve voices).\nVariation (var) : When a musical-linguistic pattern is per-\nceived as similar to the reference, due to rhythmic-melodic\naspects still present but with modiﬁcations that goes be-\nyond minimal melodic alterations, which would be typical464 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018\u0003\u0003k er n \u0003\u0003t e x t \u0003\u0003k er n \u0003\u0003t e x t \u0003\u0003k er n \u0003\u0003t e x t \u0003\u0003k er n \u0003\u0003t e x t \u0003\u0003k er n \u0003\u0003t e x t \u0003\u0003c d a t a \u0000harm\n\u0003s t a f f 5 \u0003s t a f f 5 \u0003s t a f f 4 \u0003s t a f f 4 \u0003s t a f f 3 \u0003s t a f f 3 \u0003s t a f f 2 \u0003s t a f f 2 \u0003s t a f f 1 \u0003s t a f f 1 \u0003\n=1\u0000 =1\u0000 =1\u0000 =1\u0000 =1\u0000 =1\u0000 =1\u0000 =1\u0000 =1\u0000 =1\u0000 =1\u0000\n\u0003c l e f F 4 \u0003 \u0003 clefGv2 \u0003 \u0003 clefGv2 \u0003 \u0003 c l e f G 2 \u0003 \u0003 c l e f G 2 \u0003 \u0003\n\u0003k [ ] \u0003 \u0003 k [ ] \u0003 \u0003 k [ ] \u0003 \u0003 k [ ] \u0003 \u0003 k [ ] \u0003 \u0003\n\u0003M4/ 4 \u0003 \u0003 M4/ 4 \u0003 \u0003 M4/ 4 \u0003 \u0003 M4/ 4 \u0003 \u0003 M4/ 4 \u0003 \u0003\n1 r . 2g \\ Nel 1 r . 1 r . 4 r . <CON_5v+2mot\n. . . . . . . . 4g / Nel .\n. . 4 . f \\ fo \u0000 . . . . 8 a \\ L fo \u0000 .\n. . . . . . . . 8b \\ J . .\n. . . . . . . . 8 cc \\ L . .\n. . 8 f \\ co . . . . 8dd \\ J . .\n=2 =2 =2 =2 =2 =2 =2 =2 =2 =2 =2\n1 r . 4 e \\ d ’ un 1 r . 2g / Nel 8 ee \\ L . .\n. . . . . . . . 8 f f \\ J . .\n. . 4 e \\ b e l . . . . 8gg \\ L . .\n. . . . . . . . 8 ee \\ J . .\n. . 2d \\ lau \u0000 . . 4 . a / fo \u0000 2 f f # \\ . .\n. . . . . . 8 a / co . . .\nFigure 5 : **kern annotation for the CON at the beginning of of Giovanelli’s madrigal, in which ﬁve voices (5v) and two motifs (2mot)\nare involved. Notice that the annotation would be visually displayed above the ﬁrst staff, in the same position as *CON1 (cf. Figure 4).\nin order to prevent dissonant collisions. See, e. g., Mas-\nsaino’s madrigal at measure 50.\nImitation (imit) : When a voice within a madrigalism\n‘freely’ imitates a musical-linguistic pattern, usually by re-\npeating single elements taken from it, such as a rhythm\nand/or melodic extracts, and by repeating words or by an-\nticipating the next verse. See, e. g., the ﬁrst madrigalism\n(ANTIF) of Perue’s madrigal at measure 2–5.\nLibero (lib) : In CON and ANTIF, when all the voices per-\nform the same verse of the lyrics in ‘free musical imitation’\namong them, i. e., since no speciﬁc rhythmic-melodic pat-\ntern is associated to the textual verse, no musical-linguistic\npattern can be identiﬁed as reference. In HOM, this indi-\ncates that a madrigalism starts and ﬁnishes in homorhythm\nbut in its central area, the voices present rhythmic varia-\ntions that disrupt their perfect vertical alignment; see, e. g.,\nFronti’s madrigal at measure 71.\nDifferent motifs : When a verse of the text is musicalised\nby different musical motifs within the same madrigal-\nism; this has been identiﬁed with a different number, e. g.,\nCON1 and CON2 (cf. Figure 4).\nDiminution (dim) : When a musical-linguistic pattern is\nperformed in rhythmic diminution, i. e., the rhythm dis-\nplayed is divided by half w. r. t. the reference. See, e. g., the\nlast madrigalism of Giovanelli’s madrial at measure 62.\n5.3 Annotation in **kern\nFor the annotation of madrigalisms in **kern, the **harm\nspine has been considered, which visually displays the har-\nmonic annotations below the staff, where the lyrics are lo-\ncated in the presented repertoire. In order to avoid collision\nwith the lyrics, and since our intention is not to annotate\nharmonic content, we have engraved the annotations above\nthe ﬁrst staff from the top, by using the command ‘cdata’,\ni. e., **cdata-harm (cf. Figure 5). For each madrigalism,\nthe starting and ending point has been identiﬁed as ‘ <’\nand ‘>’, respectively. When a madrigalism starts beforethe previous has ﬁnished, i. e., there is a overlap between\nboth, ‘ <<>> ’ has been considered. In addition to these,\nother elements have been indicated:\n(i) The number of voices, i. e., for CON and HOM the\nvoices participating (from 1v to 5v); for ANTIF the al-\nternating entries (e. g., four entries—4v). When in CON\n‘multiple voices’ are involved (cf. Section 5. 2), these were\nalso indicated (e. g., one doubled voice—1doub).\n(ii) The combination among textures: HOM + imit and\nANTIF + imit—when the majority of the voices are ho-\nmorhythmic or antiphonal and one performs imitatively\n(see, e. g., the ﬁrst madrigalism of Perue’s madrigal);\nHOM + CON and ANTIF + CON—when the majority of\nthe voices are homorhythmic or antiphonal and one per-\nforms the same musical–linguistic pattern in counterpoint.\n(iii) The number of motifs considered, when ‘different\nmotifs’ (cf. Section 5. 2) have been used to musicalise a\nverse of the lyrics (e. g., two motives—2mot).\n(iv) The repetitions of a madrigalism are indicated as\n<CONrep, <HOMrep, and <ANTIFrep (cf. Section 5. 2).\n6. ANNOTATIONS ASSESSMENT\n6.1 Musical Evaluation\nIn the presented repertoire, we identiﬁed a total of 437\nmadrigalisms across the 30 madrigals (mean of 14.5, and\nstandard deviation (std) 3.7): 199 CON (mean of 6.6, std\n2.9); 139 HOM (mean of 4.6, std 3); 59 ANTIF (mean\nof 1.9, std 1.9); 40 combination between the previous—\ncomb (mean of 1.3, std 1). In Table 1, the distribution of\nmadrigalisms across the 30 madrigals displays the typical\nalternation between contrapunctal and homorhythmic tex-\ntures, which is shown by almost all the madrigals present-\ning both CON and HOM. Even those in which HOM has\nnot been considered, i. e., Correggio’s and Strigio’s madri-\ngals, present a high number of multiple voices, which de-\ncreases the sensation of movement typical of CON; this isProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 465Alberti\nBardi\nBelli\nBertani\nCorreggio\nDa Locca\nEremita\nFiorino\nFronti\nGabrielli\nGiovanelli\nIngegneri\nIsnardi\nLuzzaschi\nMacque\nManara\nMarenzio\nMassaino\nMilleville\nMosto\nPerue\nPigna\nPorta\nSpontone\nStabile\nStrigio\nVecchi\nVirchi\nWert\nZoilo\nCON 11 858 8 9234588914 33 810 710 463 7 411 5655\nHOM 2481\u0000 77515 5524 3 33 7 587913 6 3\u0000 3553\nANTIF 1\u0000 1\u0000\u0000 43\u0000\u0000 3\u0000 13 2 16 7 412\u0000 22 2 7\u0000 2122\ncomb 22\u0000 1 1 133\u0000\u0000 221\u0000 34 2\u0000 11112 2\u0000 2 1\u0000 2\u0000\nTOTAL 16141410 92115111913151317 19 1016 24 191720141010 17 14 13 11121410\nvoice mul 3643 5 3\u0000 112\u0000 26 5\u0000 1 6 836\u0000 53 4 1 5\u0000 212\n<<>> 2\u0000\u0000 1 2 3\u0000 111\u0000 41 2 12\u0000 21331\u0000\u0000 1 5\u0000\u0000 1\u0000\nrep \u0000\u0000 3\u0000 2 43251411 6 12 2\u0000 6\u0000\u0000\u0000\u0000 3 2 1\u0000\u0000 3\u0000\nmot dif 1\u0000\u0000 1\u0000\u0000\u0000\u0000\u0000 3122 3 1\u0000\u0000 1\u0000 2\u0000\u0000\u0000\u0000\u0000\u0000 231\u0000\nmel 2\u0000 55 1\u0000 42\u0000\u0000 5\u000010 22 13\u0000 30 2\u0000\u0000\u000014 3 3 113 34\u0000\u0000\n# ms 77616675 71 8469638765828199130 7171100 968192507563 69 72 96 70799666\nTable 1 : Occurrence for each madrigal of CON, HOM, ANTIF, and textural combinations (comb). Total number of madrigalisms,\noverlap between these ( <<>> ), their repetitions, and length of the madrigals in measures (# ms). The use within madrigalisms of\nmultiple voices (voice mul), different motifs (mot dif), and melisma (mel), is also given.\nalso observed in madrigals with more CON than HOM (see\ne. g., Luzzaschi’s madrigal amongst others).\nThe madrigals with more HOM than CON are rare, and\npresent the opposite tendency, i. e., a low number of mul-\ntiple voices, as e. g., those from Fronti and Perue. The use\nof ANTIF, even less typical than the other madrigalisms, is\ncharacteristic in the musical writing of Marenzio, Stabile,\nand Manara. As it would be expected, the use of repetitions\nis mostly related to longer madrigals, with the exception of\nthe one by Belli that—only 66 measures long—presents\nthree repetitions of a madrigalism. However, this is re-\nlated to the fact that Belli’s madrigal presents a majority\nof HOM, which commonly are shorter than CON. This is\nclear in Perue’s madrigal, i. e., the shortest (50 measures),\npresenting 14 madrigalisms (9 of them HOM), whose com-\npactness is increased by the use of 3 overlaps between\nmadrigalisms. For general statistics of the dataset, such\nas total number of notes or accidentals, see [21].\n6.2 Linguistic Evaluation: Melismas\nOne of the most interesting musical-linguistic synergies\nwithin madrigals is the use of melisma (cf. Section 5.2).\nBy annotating the presented repertoire, we have identiﬁed\n142 melismas, which usually are displayed within CON.\nIndeed, apart from Macque’s madrigal, which presents 13\nmelismas and only 3 CON, all the other madrigals with\na high number of melismas are also characterised by pre-\nsenting a high number of CON. Yet, we should also con-\nsider that in Macque’s madrigal, there are 3 combined\nmadrigalisms, which implicitly present contrapunctal tex-\nture. Furthermore, the relationship between counterpoint\nand melismatic writing should not be taken as a rule but\nonly as a tendency, as shown by Mosto’s madrigal, with\n10 CON and no melismas. The purpose of a melisma is to\nhighlight a word, thus this rhetoric ‘artifact’ relates most\nof the times to linguistic concepts that have an important\nmeaning within a madrigal.\nThe evaluation of the melisma in the presented reper-\ntoire makes the unity of the Il Lauro Secco anthology ev-\nident, whose madrigals have been composed expressively\nfor the creation of the anthology itself. The majority of thelinguistic concepts highlighted through melisma are there-\nfore mostly the same across the whole anthology, and can\nbe clustered into three categories: (i) Nature, i. e., words\nsuch as leaf orgreen , making often a meaning game with\nthe name of the addressee of the anthology—‘Laura’ and\n‘lauro’ ( laurel in Italian); (ii) Emotion, i. e., words such\naslove,happiness , orrage; (iii) Elements of nature, i. e.,\nwords such as ﬁreorwind . Out of the 142 melisma, 46\nrelate to nature and are displayed across 11 madrigals, the\nmost recurrent words being lauro (laurel), verde (green),\nfoglie (leaves), and rami (branch), as well as synonyms\nof those; 42 relate to emotions, displayed across 8 madri-\ngals through recurrent words such as lieto (happy), amore\n(love), and ira(ire), and synonyms of those; 34 relate\nto elements, displayed across 9 madrigals through recur-\nrent words such as venti (winds), acqua (water), and fuoco\n(ﬁre), as well as synonyms and other related words.\n7. CONCLUSIONS AND FUTURE WORK\nOur study presents symbolically codiﬁed scores and anno-\ntations, in **kern and MEI format, of 30 madrigals of the\nanthology Il Lauro Secco . The evaluation of the annota-\ntions conﬁrms the unity of the presented repertoire, by dis-\nplaying similarities across the different madrigals, related\nin a particular way to musical-linguistic synergies, such as\nthe use of melisma to highlight speciﬁc concepts. The re-\nlationships between poetry and music inherent in the pre-\nsented repertoire, and consistently presented across pieces\nby different composers, make it promising for the applica-\ntion of machine learning techniques aimed at the detection\nof similarities among composers. Our future goals include\nto continue the annotation of the anthology by other ex-\nperts, in order to offer an appropriate ‘gold standard’ to re-\nfer to. We also plan to further evaluate the presented reper-\ntoire through available toolkits for automatic music anal-\nysis, as e. g., music21. In addition, we will also work on\nsymbolic annotations of similar repertoires, in order to pro-\nmote the advancement of algorithms for automatic analysis\nof scores in early music, especially considering the auto-\nmatic recognition of music-linguistic synergies.466 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20188. ACKNOWLEDGEMENT\nThis work was supported by the European\nUnions’s Seventh Framework and Horizon\n2020 Program under grant agreement No.\n338164 (ERC StG iHEARu).\n9. REFERENCES\n[1] Willi Apel. The notation of polyphonic music, 900-\n1600 . The Mediaeval Academy of America, Cam-\nbridge, MA, USA, 1961.\n[2] Aristotle and George Alexander Kennedy. On rhetoric:\nA theory of civic discourse . Oxford University Press,\nNew York, NY , USA, 2006.\n[3] Denis Arnold and Nigel Fortune. The new Monteverdi\ncompanion . Faber & Faber, London, UK, 1985.\n[4] Manuel Burghardt and Sebastian Spanner. Allegro:\nUser-centered design of a tool for the crowdsourced\ntranscription of handwritten music scores. In Proc. of\nDATeCH , pages 15–20, Göttingen, Germany, 2017.\nACM.\n[5] Joachim Burmeister. Musical poetics . Yale University\nPress, New Haven, CT, USA, 1993.\n[6] Jorge Calvo-Zaragoza, Gabriel Vigliensoni, and Ichiro\nFujinaga. One-step detection of background, staff\nlines, and symbols in medieval music manuscripts\nwith convolutional neural networks. In Proc. of ISMIR ,\npages 724–730, Suzhou, P. R. China, 2017. ISMIR.\n[7] Michael S. Cuthbert and Christopher Ariza. Music21:\nA toolkit for computer-aided musicology and symbolic\nmusic data. In Proc. of ISMIR , pages 637–642, Utrecht,\nNetherlands, 2010. ISMIR.\n[8] Johanna Devaney, Claire Arthur, Nathaniel Condit-\nSchultz, and Kirsten Nisula. Theme and variation en-\ncodings with roman numerals (TA VERN): A new data\nset for symbolic music analysis. In Proc. of ISMIR ,\npages 728–734, Málaga, Spain, 2015. ISMIR.\n[9] Anhai Doan, Raghu Ramakrishnan, and Alon Y\nHalevy. Crowdsourcing systems on the world-wide\nweb. Communications of the ACM , 54(4):86–96, 2011.\n[10] Elio Durante and Anna Martellotti. Madrigali segreti\nper le dame di Ferrara: Il manoscritto musicale F.\n1358 della biblioteca estense di modena . Studio per\nedizioni scelte (SPES), Florence, Italy, 2000.\n[11] Elio Durante and Anna Martellotti. \"Giovinetta pere-\ngrina\": La vera storia di Laura Peperara e Torquato\nTasso . Leo S. Olschki, Firenze, Italia, 2010.\n[12] Alfred Einstein. The Italian madrigal . Princeton Uni-\nversity Press, Princeton, NJ, USA, 1971.\n[13] Anthony Goodman and Angus MacKay. The impact of\nhumanism on Western Europe . Taylor & Francis, Lon-\ndon, UK, 2013.\n[14] Donald J. Grout and Claude V . Palisca. A history of\nWestern music . Norton, New York, NY , USA, 2001.\n[15] David Huron. Music information processing using theHumdrum Toolkit: Concepts, examples, and lessons.\nComputer Music Journal , 26(2):11–26, 2002.\n[16] David Brian Huron. The humdrum toolkit: Reference\nmanual . Center for Computer Assisted Research in the\nHumanities, Menlo Park, CA, USA, 1994.\n[17] Athanasius Kircher. Musurgia universalis sive ars\nmagna consoni et dissoni in X. libros digesta , vol-\nume 2. Georg Olms, Hildesheim, Germany, 1999.\n[18] François Lesure. Recueils imprimés XVIe-XVIIe siè-\ncles. Henle, Munich, Germany, 1960.\n[19] Anthony Newcomb. The three anthologies for Laura\nPeverara, 1580–1583. Rivista Italiana di Musicologia ,\n10:329–345, 1975.\n[20] Anthony Newcomb. The madrigal at Ferrara: 1579-\n1597 , volume 1. Princeton University Press, Princeton,\nNJ, USA, 1980.\n[21] Emilia Parada-Cabaleiro, Anton Batliner, Alice E.\nBaird, and Björn Schuller. The SEILS dataset: Sym-\nbolically encoded scores in modern-early notation for\ncomputational musicology. In Proc. of ISMIR , pages\n575–581, Suzhou, P. R. China, 2017. ISMIR.\n[22] Pavel Pecina and Jan Haji ˇc. In search of a dataset\nfor handwritten optical music recognition: Introduc-\ning MUSCIMA++. arXiv preprint arXiv:1703.04824 ,\n1:1–16, 2017.\n[23] Laurent Pugin, Rodolfo Zitellini, and Perry Roland.\nVerovio: A library for engraving MEI music notation\ninto SVG. In Proc. ISMIR , pages 107–112, Taipei, Tai-\nwan, 2014. ISMIR.\n[24] Don Michael Randel. The Harvard dictionary of mu-\nsic. Harvard University Press, Cambridge, MA, USA,\n4 edition, 2003.\n[25] Emiliano Ricciardi. The Tasso in music project. Early\nMusic , 43(4):667–671, 2015.\n[26] Philippe Rigaux, Lylia Abrouk, Hervé Audéon, Nadine\nCullot, Cécile Davy-Rigaux, Zoé Faget, Elisabeth Gav-\nignet, David Gross-Amblard, Alice Tacaille, and Vir-\nginie Thion-Goasdoué. The design and implementation\nof Neuma, a collaborative digital scores library. Inter-\nnational Journal on Digital Libraries , 12(2-3):73–88,\n2012.\n[27] Perry Roland, Andrew Hankinson, and Laurent Pugin.\nEarly music and the music encoding initiative. Early\nMusic , 42(4):605–611, 2014.\n[28] Anja V olk, Peter Van Kranenburg, Jörg Garbers, Frans\nWiering, Remco C. Veltkamp, and Louis P. Grijp. A\nmanual annotation method for melodic similarity and\nthe study of melody feature sets. In Proc. of ISMIR ,\npages 101–106, Philadelphia, PA, USA, 2008. ISMIR.\n[29] James Anderson Winn. Unsuspected eloquence: A his-\ntory of the relations between poetry and music . Yale\nUniversity Press, New Haven, CT, USA, 1981.\n[30] Gioseffo Zarlino. Le istituzioni harmoniche . Gregg\nPress, Ridgewood, NJ, USA, 1966.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 467"
    },
    {
        "title": "Music Source Separation Using Stacked Hourglass Networks.",
        "author": [
            "Sungheon Park",
            "Taehoon Kim",
            "Kyogu Lee",
            "Nojun Kwak"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492405",
        "url": "https://doi.org/10.5281/zenodo.1492405",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/138_Paper.pdf",
        "abstract": "In this paper, we propose a simple yet effective method for multiple music source separation using convolutional neural networks. Stacked hourglass network, which was originally designed for human pose estimation in natural images, is applied to a music source separation task. The network learns features from a spectrogram image across multiple scales and generates masks for each music source. The estimated mask is refined as it passes over stacked hourglass modules. The proposed framework is able to separate multiple music sources using a single network. Experimental results on MIR-1K and DSD100 datasets validate that the proposed method achieves competitive results comparable to the state-of-the-art methods in multiple music source separation and singing voice separation tasks.",
        "zenodo_id": 1492405,
        "dblp_key": "conf/ismir/ParkKLK18",
        "keywords": [
            "convolutional neural networks",
            "multiple music source separation",
            "stacked hourglass network",
            "spectrogram image",
            "music source separation task",
            "singing voice separation",
            "experimental results",
            "MIR-1K dataset",
            "DSD100 dataset",
            "competitive results"
        ],
        "content": "MUSIC SOURCE SEPARATION USING STACKED HOURGLASS\nNETWORKS\nSungheon Park Taehoon Kim Kyogu Lee Nojun Kwak\nGraduate School of Convergence Science and Technology, Seoul National University, Korea\nfsungheonpark, kcjs55, kglee, nojunk g@snu.ac.kr\nABSTRACT\nIn this paper, we propose a simple yet effective method\nfor multiple music source separation using convolutional\nneural networks. Stacked hourglass network, which was\noriginally designed for human pose estimation in natural\nimages, is applied to a music source separation task. The\nnetwork learns features from a spectrogram image across\nmultiple scales and generates masks for each music source.\nThe estimated mask is reﬁned as it passes over stacked\nhourglass modules. The proposed framework is able to\nseparate multiple music sources using a single network.\nExperimental results on MIR-1K and DSD100 datasets\nvalidate that the proposed method achieves competitive re-\nsults comparable to the state-of-the-art methods in multi-\nple music source separation and singing voice separation\ntasks.\n1. INTRODUCTION\nMusic source separation is one of the fundamental research\nareas for music information retrieval. Separating singing\nvoice or sounds of individual instruments from a mixture\nhas grabbed a lot of attention in recent years. The separated\nsources can be further used for applications such as auto-\nmatic music transcription, instrument identiﬁcation, lyrics\nrecognition, and so on.\nRecent improvements on deep neural networks (DNNs)\nhave been blurring the boundaries between many applica-\ntion domains, including computer vision and audio sig-\nnal processing. Due to its end-to-end learning character-\nistic, deep neural networks that are used in computer vi-\nsion research can be directly applied to audio signal pro-\ncessing area with minor modiﬁcations. Since the magni-\ntude spectrogram of an audio signal can be treated as a\n2D single-channel image, convolutional neural networks\n(CNNs) have been successfully used in various music\napplications, including the source separation task [1, 8].\nWhile very deep CNNs are typically used in computer vi-\nsion literature with very large datasets [4, 25], CNNs used\nc\rSungheon Park, Taehoon Kim, Kyogu Lee, Nojun\nKwak. Licensed under a Creative Commons Attribution 4.0 Interna-\ntional License (CC BY 4.0). Attribution: Sungheon Park, Taehoon\nKim, Kyogu Lee, Nojun Kwak. “Music Source Separation Using Stacked\nHourglass Networks”, 19th International Society for Music Information\nRetrieval Conference, Paris, France, 2018.for audio source separation so far have relatively shallow\narchitectures.\nIn this paper, we propose a novel music source separa-\ntion framework using CNNs. We used stacked hourglass\nnetwork [18] which was originally proposed to solve hu-\nman pose estimation in natural images. The CNNs take\nspectrogram images of a music signal as inputs, and gener-\nate masks for each music source to separate. An hourglass\nmodule captures both holistic features from low resolution\nfeature maps and ﬁne details from high resolution feature\nmaps. The module outputs 3D volumetric data which has\nthe same width and height as those of the input spectro-\ngram. The number of output channels equals the number\nof music sources to separate. The module is stacked for\nmultiple times by taking the results of the previous mod-\nule. As passing multiple modules, the results are reﬁned\nand intermediate supervision helps faster learning in the\ninitial state. We used a single network to separate multiple\nmusic sources, which reduces both time and space com-\nplexity for training as well as testing.\nWe evaluated our framework on a couple of source sep-\naration tasks: 1) separating singing voice and accompa-\nniments, and 2) separating bass, drum, vocal, and other\nsounds from music. The results show that our method\noutperforms existing methods on MIR-1K dataset [5] and\nachieves competitive results comparable to state-of-the-art\nmethods on DSD100 dataset [30] despite its simplicity.\nThe rest of the paper is organized as follows. In Sec-\ntion 2, we brieﬂy review the literature of audio source sep-\naration focusing on DNN based methods. The proposed\nsource separation framework and the architecture of the\nnetwork are explained in Section 3. Experimental results\nare provided in Section 4, and the paper is concluded in\nSection 5.\n2. RELATED WORK\nNon-negative matrix factrization (NMF) [12] is one of the\nmost widely-used algorithms for audio source separation.\nIt has been successfully applied to monaural source sepa-\nrtion [32] and singing voice separation [29, 38]. However,\ndespite its generality and ﬂexibility, NMF is inferior to re-\ncently proposed DNN-based methods in terms of perfor-\nmance and time complexity.\nSimple deep feed-forward networks consisting of multi-\nple fully-connected layers showed reasonable performance\nfor supervised audio source separation tasks [27]. Wang et289Figure 1 . Structure of the hourglass module used in this paper. We follow the structure proposed in [17] except that the\nnumber of feature maps are set to 256 for all convolutional layers.\nal. [34] used DNNs to learn an ideal binary mask which\nboils the source separation problem down to a binary clas-\nsiﬁcation problem. Simpson et al. [24] proposed a con-\nvolutional DNN to predict a probabilistic binary mask for\nsinging voice separation. Recently, a fully complex-valued\nDNN [13] is proposed to integrate phase information into\nthe magnitude spectrograms. Deep NMF [11] combined\nDNN and NMF by designing non-negative deep network\nand its back-propagation algorithm.\nSince an audio signal is time series data, it is natural\nto use a sequence model like recurrent neural networks\n(RNNs) for music source separation tasks to learn tempo-\nral information. Huang et al. [6] proposed an RNN frame-\nwork that jointly optimizes masks of foreground and back-\nground sources, which showed promising results for var-\nious source separation tasks. Other approaches include\na recurrent encoder-decoder that exploits gated recurrent\nunit [15] or discriminative RNN [33].\nCNNs are also an effective tool for audio signal anal-\nysis when the magnitude spectrogram is used as an input.\nFully convolutional networks (FCNs) [14] are initially pro-\nposed for semantic segmentation in the computer vision\narea, which is also effective for solving human pose esti-\nmation [18,35] or super-resolution [2]. FCNs usually con-\ntain downsampling and upsampling layers to learn mean-\ningful features at multiple scales. Strided convolution or\npooling is used for downsampling, while transposed con-\nvolution or nearest neighbor interpolation is mainly used\nfor upsampling. It is proven that FCNs are also effective\nin signal processing. Chandna et al. [1] proposed encoder-\ndecoder style FCN for monoaural audio source separation.\nRecently, singing voice separation using an U-Net archi-\ntecture [8] showed impressive performance. U-Net [22]\nis a FCN which consists of a series of convolutional lay-\ners and upsampling layers. There is a skip connection\nwhich connects the convolutional layers of the same res-\nolution. They trained vocal and accompaniment parts sep-\narately on different networks. Miron et al. [16] proposed\nthe method that separates multiple sources using a single\nCNN. They used score-ﬁltered spectrograms as inputs and\ngenerated masks for each source via an encoder-decoder\nCNN. Multi-resolution FCN [3] was proposed for monau-\nral audio source separation. Recently proposed CNN ar-\nchitecture [26] based on DenseNet [7] achieved state-of-\nthe-art performance on DSD100 dataset.3. METHOD\n3.1 Network Architecture\nThe stacked hourglass network [18] was originally pro-\nposed to solve human pose estimation in RGB images. It\nis an FCN consisting of multiple hourglass modules. The\nhourglass module is similar to U-Net [22], of which feature\nmaps at lower (coarse) resolution are obtained by repeat-\nedly applying convolution and pooling operations. Then,\nthe feature maps at the lowest resolution are upsampled\nvia nearest neighbor interpolation with a preceding con-\nvolutional layer. Feature maps at the same resolution in\nthe downsampling and the upsampling steps are connected\nwith an additional convolutional layer. The hourglass mod-\nule captures features at different scales by repeating pool-\ning and upsampling with convolutional layers at each reso-\nlution. In addition, multiple hourglass modules are stacked\nto make the network deeper. As more hourglass modules\nare stacked, the network learns more powerful and infor-\nmative features which reﬁne the estimation results. Loss\nfunctions are applied at the end of each module. This in-\ntermediate supervision improves training speed and perfor-\nmance of the network.\nThe structure of a single hourglass module used in this\npaper is illustrated in Fig 1. Considering the efﬁciency and\nthe size of the network, we adopt the hourglass module\nused in [17] which is a smaller network than the origi-\nnally proposed one in [18]. A notable difference is that\nthe residual blocks [4] used in [18] are replaced with a sin-\ngle convolutional layer. This light-weight structure showed\ncompetitive performance to the original network in human\npose estimation with much smaller number of parameters.\nIn the module, there are four downsampling and upsam-\npling steps. All convolutional layers in downsampling and\nupsampling steps have ﬁlter size of 3\u00023. The 2\u00022max\npooling is used to halve the size of the feature maps, and\nthe nearest neighbor interpolation is used to double the size\nof the feature maps in the upsampling steps. We ﬁxed the\nsize of the maximum feature maps in convolutional layers\nto 256 which is different from [17]. After the last upsam-\npling layer, a single 3\u00023convolution and two 1\u00021con-\nvolution is performed to generate network outputs. Then,\nan1\u00021convolution is applied to the outputs to match the\nnumber of channels to that of the input feature maps. An-\nother 1\u00021convolution is also applied to the feature maps290 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 2 . Overall music source separation framework proposed in this paper. Multiple hourglass modules are stacked, and\neach module outputs masks for each music source. The masks are multiplied with the input spectrogram to generate pre-\ndicted spectrograms. Differences between the estimated spectrograms and the ground truth ones are used as loss functions\nof the network.\nwhich used for output generation. Finally, the two feature\nmaps that passed the respective 1\u00021convolution and the\ninput of the hourglass module is added together, and the re-\nsulting feature map is used as an input to the next hourglass\nmodule.\nIn the network used in this paper, input image ﬁrstly\npasses through initial convolutional layers that consist of a\n7\u00027convolutional layer and four 3\u00023convolutional lay-\ners where the number of output feature maps for each layer\nis 64, 128, 128, 128, and 256 respectively. To make the\noutput mask and the input spectrogram have the same size,\nwe did not use the pooling operations in the initial convo-\nlutional layers before the hourglass module. The feature\nmaps generated from the initial layers are fed to the ﬁrst\nhourglass module. The proposed overall music source sep-\naration framework is depicted in Fig. 2.\n3.2 Music Source Separation\nAs shown in Fig. 2, to apply the stacked hourglass network\nto music source separation, we aim to train the network to\noutput soft masks for each music source given the magni-\ntude spectrogram of the mixed source. Hence, the output\ndimension of the network is H\u0002W\u0002Cwhere HandW\nare the height and width of the input spectrogram respec-\ntively, and Cis the number of music sources to separate.\nThe magnitude spectrogram of separated music source is\nobtained by multiplying the mask and the input spectro-\ngram. Our framework is scalable in that it requires almost\nno additional operation as the number of sources increases.\nThe input for the network is the magnitude of spectro-\ngram obtained from Short-Time Fourier Transform (STFT)\nwith a window size of 1024 and a hop size of 256. The\ninput source is downsampled to 8kHz to increase the du-\nration of spectrograms in a batch and to speed up training.\nFor each sample, magnitude spectrograms of mixed andseparated sources are generated, which are divided by the\nmaximum value of the mixed spectrogram for data normal-\nization. The spectrograms have 512 frequency bins and the\nwidth of the spectrogram depends on the duration of the\nmusic sources. For all the music sources, the width of the\nspectrogram is at least 64. Thus, we ﬁx the size of an input\nspectrogram to 512\u000264. Hence, the size of the feature\nmaps at the lowest resolution is 32\u00024. Starting time index\nis randomly chosen when the input batches are created.\nFollowing [22], we designed the loss function as an\nL1;1norm of the difference between the ground truth spec-\ntrogram and the estimated spectrogram. More concretely,\ngiven an input spectrogram X,ith ground truth music\nsource Yi, and the generated mask for the ith source in\nthejth hourglass module ^Mij, the loss for the ith source\nis deﬁned as\nJ(i; j) =kYi\u0000X\f^Mijk1;1; (1)\nwhere\fdenotes element-wise multiplication of the ma-\ntrix.L1;1norm is calculated as the sum of absolute values\nof matrix elements. The loss function of the network be-\ncomes\nJ=CX\ni=1DX\nj=1J(i; j); (2)\nwhere Dis the number of hourglass modules stacked in the\nnetwork. We directly used the output of the last 1\u00021con-\nvolutional layer as the mask, which is different from [22]\nwhere they used the sigmoid activation to generate masks.\nWhile it is natural to use the sigmoid function to restrict\nthe value of the mask to [0,1], we empirically found that\nnot applying the sigmoid function boosts the training speed\nand improves the performance. Since sigmoid activations\nvanish the gradient of the inputs that have large absolute\nvalues, they may diminish the effect of intermediate super-\nvision.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 291We have stacked hourglass modules up to four and pro-\nvide analysis of the effect of stacking multiple modules\nin Section 4. The network is trained using Adam opti-\nmizer [10] with a starting learning rate of 10\u00004and a batch\nsize of 4. We trained the network for 15,000 and 150,000\niterations for MIR-1K dataset and DSD100 dataset respec-\ntively, and the learning rate is decreased to 2\u000210\u00005when\n80% of the training is ﬁnished. No data augmentation is\napplied during training. The training took 3 hours for MIR-\n1K dataset and 31 hours for DSD100 dataset using a single\nGPU when the biggest model is used. For the singing voice\nseparation task, Cis set to 2 which corresponds to vocal\nandaccompaniments . For the music source separation task\nin DSD100 dataset, C= 4is used where each output mask\ncorresponds to drum ,bass,vocal , and others . While it can\nbe advantageous in terms of performance to train a net-\nwork for a single source individually, it is computationally\nexpensive to train a deep CNN for each source. Therefore,\nwe trained a single network for each task.\nIn the test phase, the magnitude spectrogram of the in-\nput source is cropped to network input size and fed to\nthe network sequentially. The output of the last hourglass\nmodule is used for testing. We set the negative values of\noutput masks to 0 in order to avoid negative magnitude\nvalues. The masks are multiplied by the normalized mag-\nnitude spectrogram of the test source and unnormalized to\ngenerate spectrograms of separated sources. We did not\nchange the phase spectrogram of the input source, and it\nis combined with the estimated magnitude spectrogram to\nretrieve signals for separated sources via inverse STFT.\n4. EXPERIMENTS\nWe evaluated performance of the proposed method on\nMIR-1K and DSD100 datasets. For quantitative evalua-\ntion, we measured signal-to-distortion ratio (SDR), source-\nto-interference ratio (SIR), and source-to-artifacts ratio\n(SAR) based on BSS-EV AL metrics [31]. Normalized\nSDR (NSDR) [20] is also measured for the singing voice\nseparation task which measures improvement between the\nmixture and the separated source. The values are obtained\nusing mir-eval toolbox [21]. Global NSDR (GNSDR),\nglobal SIR (GSIR), and global SAR (GSAR) are calcu-\nlated as a weighted mean of NSDR, SIR, and SAR respec-\ntively whose weights are length of the source. The sepa-\nrated sources generated from the network are upsampled\nto the original sampling rate of the dataset and compared\nwith ground truth sources for all experiments.\n4.1 MIR-1K dataset\nMIR-1K dataset is designed for singing voice separation\nresearch. It contains a thousand song clips extracted from\n110 Chinese karaoke songs at a sampling rate of 16kHz.\nFollowing the previous works [6, 37], we used one male\nand one female ( abjones andamy) as a training set which\ncontains 175 clips in total. The remaining 825 clips are\nused for evaluation. For the baseline CNN, we trained the\nFCN that has U-Net [22]-like structure and evaluated itsSinging voice\nMethod GNSDR GSIR GSAR\nMLRR [37] 3.85 5.63 10.70\nDRNN [6] 7.45 13.08 9.68\nModGD [23] 7.50 13.73 9.45\nU-Net [8] 7.43 11.79 10.42\nSH-1stack 10.29 15.51 12.46\nSH-2stack 10.45 15.89 12.49\nSH-4stack 10.51 16.01 12.53\nAccompaniments\nMethod GNSDR GSIR GSAR\nMLRR [37] 4.19 7.80 8.22\nU-Net [8] 7.45 11.43 10.41\nSH-1stack 9.65 13.90 12.27\nSH-2stack 9.64 13.69 12.39\nSH-4stack 9.88 14.24 12.36\nTable 1 . Quantitative evaluation of singing voice separa-\ntion on MIR-1K dataset.\nperformance. We followed the structure of [8], in which\nsinging voice and accompaniments are trained on differ-\nent networks. For the stacked hourglass networks, both\nsinging voice and accompaniments are obtained from a sin-\ngle network.\nThe evaluation results on test sets are shown in Table 1.\nWe trained the networks with varying number of stacked\nhourglass modules 1, 2, and 4. It is proven that our stacked\nhourglass network (SH) signiﬁcantly outperforms existing\nmethods in all evaluation criteria. Our method gains 3.01\ndB in GNSDR, 2.28 dB in GSIR, and 1.83 dB in GSAR\ncompared to the best results of the existing methods. It\nis also proven that the structure of the stacked hourglass\nmodule is more efﬁcient and beneﬁcial than U-Net [8] for\nmusic source separation. U-Net has 9.82 million parame-\nters while single stack hourglass network has 8.99 million\nparameters considering only convolutional layers. Even\nwith the absence of batch normalization, smaller number\nof parameters, and multi-source separation in a single net-\nwork, the stacked hourglass network showed superior per-\nformance to U-Net. While the network with a single hour-\nglass module shows outstanding source separation per-\nformance, even better results are provided when multiple\nhourglass modules are stacked. This indicates that SH net-\nwork does not overﬁt even when the network gets deeper\ndespite small amount of the training data. Our method pro-\nvides good performance on separating both singing voice\nand accompaniments with a single forward step.\nQualitative results of our method and comparison with\nU-Net are shown in Fig. 3. The estimated log spectrograms\nof singing voice and accompaniments from SH-4stack and\nU-Net and the ground truth log spectrograms are provided.\nIt can be seen that our method captures ﬁne details and\nharmonics compared to the U-Net. The voice spectrogram\nfrom U-Net has more artifacts in the time slot of 0\u00181and\n4\u00185compared to the result of SH-4stack. On the other292 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20180 1 2 3 4 5\nTime01000200030004000HzGround truth - Voice\n0 1 2 3 4 5\nTime01000200030004000HzSH-4stack - Voice\n0 1 2 3 4 5\nTime01000200030004000HzU-Net - Voice\n0 1 2 3 4 5\nTime01000200030004000HzGround truth - Acccompaniments\n0 1 2 3 4 5\nTime01000200030004000HzSH-4stack - Acccompaniments\n0 1 2 3 4 5\nTime01000200030004000HzU-Net - AcccompanimentsFigure 3 . Qualitative comparison of our method (SH-4stack) and U-Net for singing voice and accompaniments separation\nonannar 305in MIR-1K dataset. Ground truth and estimated spectrograms are displayed in a log-scale. Our method is\nsuperior in capturing ﬁne details compared to U-Net.\nhand, harmonics from voice signals can be clearly seen in\nthe spectrogram of SH-4stack. For accompaniments spec-\ntrogram, it is observed that U-Net contains voice signals\naround the time slot of 3.\n4.2 DSD100 dataset\nDSD100 dataset consists of 100 songs that are divided into\n50 training sets and 50 test sets. For each song, four differ-\nent music sources, bass, drums, vocals, and other as well as\ntheir mixtures are provided. The sources are stereophonic\nsound with a sampling rate of 44.1kHz. We converted\nall sources to monophonic and performed single channel\nsource separation using stacked hourglass networks. We\nused a 4-stacked hourglass network (SH-4stack) for the ex-\nperiments.\nThe performance of music source separation using\nstacked hourglass network is provided in Table 2. We mea-\nsured SDR of the separated sources for all test songs and\nreport median values for comparison with existing meth-\nods. The methods that use single channel inputs are com-\npared to our method. While the stacked hourglass network\ngives second-best performance following the state-of-the-\nart methods [26] for drums and vocals, it shows poor per-\nformance for separating bass and other. This is mainly\ndue to the similarity between bass and guitar sound in\nother sources, which confuses the network especially when\ntrained together in a single network. Since the losses for all\nsources are summed up with equal weights, the network\ntends to be trained to improve the separation performance\nof vocal and drum, which is easier than separating bass and\nother sources.\nNext, we trained the stacked hourglass network for aMethod Bass Drums Other V ocals\ndNMF [36] 0.91 1.87 2.43 2.56\nDeepNMF [11] 1.88 2.11 2.64 2.75\nBLEND [28] 2.76 3.93 3.37 5.13\nMM-DenseNet [26] 3.91 5.37 3.81 6.00\nSH-4stack 1.77 4.11 2.36 5.16\nTable 2 . Median SDR values for music source separation\non DSD100 dataset.\nMethod V ocals Accompaniments\nDeepNMF [11] 2.75 8.90\nwRPCA [9] 3.92 9.45\nNUG [19] 4.55 10.29\nBLEND [28] 5.23 11.70\nMM-DenseNet [26] 6.00 12.10\nSH-4stack 5.45 12.14\nTable 3 . Median SDR values for singing voice separation\non DSD100 dataset.\nsinging voice separation task. The three sources except vo-\ncals are mixed together to form accompaniments source.\nThe median SDR values for each source are reported in\nTable 3. Our method achieved best result for accompa-\nniments separation and second-best for vocal separation.\nSeparation performance of vocals is improved compared to\nthe music source separation setting. It can be inferred that\nthe stacked hourglass network provides better results as\nnumber of sources are smaller and the separating sources\nare more distinguishable from each other.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 2930 0.5 1 1.5\nTime01000200030004000HzGround truth - Voice\n0 0.5 1 1.5\nTime01000200030004000Hz1st SH - Voice\n0 0.5 1 1.5\nTime01000200030004000Hz2nd SH - Voice\n0 0.5 1 1.5\nTime01000200030004000Hz4th SH - VoiceFigure 4 . Examples showing the effectiveness of stacking multiple hourglass modules. Ground truth and estimated spec-\ntrograms of the part of the song Schoolboy Fascination in DSD100 dataset are shown. SDR values of the source generated\nfrom the spectrograms obtained from ﬁrst, second, fourth hourglass module are 10.90, 12.50, 13.30 respectively. Espe-\ncially, it is observed that the estimated spectrogram captures ﬁne details of spectrogram at low frequency range ( 0\u0018500Hz)\nas more hourglass modules are stacked.\nLastly, we investigate how the stacked hourglass net-\nwork improves the output masks as they pass through the\nhourglass modules within the network. The example illus-\ntrated in Fig. 4 shows the estimated voice spectrogram of\nﬁrst, second, and fourth hourglass module with the ground\ntruth spectrogram from one of the test sets of DSD 100\ndataset. It is observed that the estimated spectrogram be-\ncomes more similar to the ground truth as it is generated\nfrom a deeper side of the network. In the result of the\nfourth hourglass module, spectrograms at low frequency\nare clearly recovered compared to the result of the ﬁrst\nhourglass module. The artifacts in the range of 2000\u00183000\nHz are also removed. Although it is hard to recognize the\ndifference in the spectrogram image, the difference of SDR\nbetween the source estimated from the ﬁrst hourglass mod-\nule and the last hourglass module is about 2.4dB which is\na signiﬁcant performance gain.\n5. CONCLUSION\nIn this paper, we proposed music source separation algo-\nrithm using stacked hourglass networks. The network suc-cessfully captures features at both coarse and ﬁne resolu-\ntion, and it produces masks that are applied to the input\nspectrograms. Multiple hourglass modules reﬁnes the esti-\nmation results and outputs the better results. Experimental\nresults has proven the effectiveness of the proposed frame-\nwork for music source separation. We implemented the\nframework in its simplest form, and there is a lot of room\nfor performance improvements including data augmenta-\ntion, regularization of CNNs, and ensemble learning of\nmultiple models. Designing a loss function that consid-\ners correlation of different sources may further improves\nthe performance.\n6. ACKNOWLEDGEMENT\nThis work was supported by Next-Generation Information\nComputing Development Program through the National\nResearch Foundation of Korea (2017M3C4A7077582).\n7. REFERENCES\n[1] Pritish Chandna, Marius Miron, Jordi Janer, and Emilia\nG´omez. Monoaural audio source separation using deep294 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018convolutional neural networks. In International Con-\nference on Latent Variable Analysis and Signal Sepa-\nration , pages 258–266. Springer, 2017.\n[2] Chao Dong, Chen Change Loy, Kaiming He, and Xi-\naoou Tang. Image super-resolution using deep convo-\nlutional networks. IEEE transactions on pattern anal-\nysis and machine intelligence , 38(2):295–307, 2016.\n[3] Emad M Grais, Hagen Wierstorf, Dominic Ward, and\nMark D Plumbley. Multi-resolution fully convolutional\nneural networks for monaural audio source separation.\narXiv preprint arXiv:1710.11473 , 2017.\n[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition , pages 770–778, 2016.\n[5] C. L. Hsu and J. S. R. Jang. On the improvement\nof singing voice separation for monaural recordings\nusing the mir-1k dataset. IEEE Transactions on Au-\ndio, Speech, and Language Processing , 18(2):310–\n319, Feb 2010.\n[6] Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson,\nand Paris Smaragdis. Joint optimization of masks and\ndeep recurrent neural networks for monaural source\nseparation. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing , 23(12):2136–2147, 2015.\n[7] Forrest Iandola, Matt Moskewicz, Sergey Karayev,\nRoss Girshick, Trevor Darrell, and Kurt Keutzer.\nDensenet: Implementing efﬁcient convnet descriptor\npyramids. arXiv preprint arXiv:1404.1869 , 2014.\n[8] Andreas Jansson, Eric Humphrey, Nicola Montecchio,\nRachel Bittner, Aparna Kumar, and Tillman Weyde.\nSinging voice separation with deep u-net convolutional\nnetworks. 18th International Society for Music Infor-\nmation Retrieval Conferenceng, Suzhou, China , 2017.\n[9] Il-Young Jeong and Kyogu Lee. Singing voice separa-\ntion using rpca with weighted l f1g-norm. In Inter-\nnational Conference on Latent Variable Analysis and\nSignal Separation , pages 553–562. Springer, 2017.\n[10] Diederik P Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n[11] Jonathan Le Roux, John R Hershey, and Felix\nWeninger. Deep nmf for speech separation. In Acous-\ntics, Speech and Signal Processing (ICASSP), 2015\nIEEE International Conference on , pages 66–70.\nIEEE, 2015.\n[12] Daniel D Lee and H Sebastian Seung. Algorithms for\nnon-negative matrix factorization. In Advances in neu-\nral information processing systems , pages 556–562,\n2001.[13] Yuan-Shan Lee, Chien-Yao Wang, Shu-Fan Wang, Jia-\nChing Wang, and Chung-Hsien Wu. Fully complex\ndeep neural network for phase-incorporating monau-\nral source separation. In Acoustics, Speech and Signal\nProcessing (ICASSP), 2017 IEEE International Con-\nference on , pages 281–285. IEEE, 2017.\n[14] Jonathan Long, Evan Shelhamer, and Trevor Darrell.\nFully convolutional networks for semantic segmenta-\ntion. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition , pages 3431–\n3440, 2015.\n[15] Stylianos Ioannis Mimilakis, Konstantinos Drossos,\nTuomas Virtanen, and Gerald Schuller. A recurrent\nencoder-decoder approach with skip-ﬁltering connec-\ntions for monaural singing voice separation. CoRR ,\nabs/1709.00611, 2017.\n[16] Marius Miron, Jordi Janer, and Emilia G ´omez. Monau-\nral score-informed source separation for classical mu-\nsic using convolutional neural networks. In 18th Inter-\nnational Society for Music Information Retrieval Con-\nference, Suzhou, China , 2017.\n[17] Alejandro Newell, Zhiao Huang, and Jia Deng. Asso-\nciative embedding: End-to-end learning for joint detec-\ntion and grouping. In Advances in Neural Information\nProcessing Systems , pages 2274–2284, 2017.\n[18] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked\nhourglass networks for human pose estimation. In Eu-\nropean Conference on Computer Vision , pages 483–\n499. Springer, 2016.\n[19] Aditya Arie Nugraha, Antoine Liutkus, and Em-\nmanuel Vincent. Multichannel music separation with\ndeep neural networks. In Signal Processing Conference\n(EUSIPCO), 2016 24th European , pages 1748–1752.\nIEEE, 2016.\n[20] Alexey Ozerov, Pierrick Philippe, Frdric Bimbot, and\nRmi Gribonval. Adaptation of bayesian models for\nsingle-channel source separation and its application to\nvoice/music separation in popular songs. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n15(5):1564–1578, 2007.\n[21] Colin Raffel, Brian McFee, Eric J Humphrey, Justin\nSalamon, Oriol Nieto, Dawen Liang, Daniel PW Ellis,\nand C Colin Raffel. mir eval: A transparent implemen-\ntation of common mir metrics. In In Proceedings of the\n15th International Society for Music Information Re-\ntrieval Conference, ISMIR . Citeseer, 2014.\n[22] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\nU-net: Convolutional networks for biomedical image\nsegmentation. In International Conference on Medical\nimage computing and computer-assisted intervention ,\npages 234–241. Springer, 2015.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 295[23] Jilt Sebastian and Hema A Murthy. Group delay based\nmusic source separation using deep recurrent neural\nnetworks. In Signal Processing and Communications\n(SPCOM), 2016 International Conference on , pages 1–\n5. IEEE, 2016.\n[24] Andrew JR Simpson, Gerard Roma, and Mark D\nPlumbley. Deep karaoke: Extracting vocals from mu-\nsical mixtures using a convolutional deep neural net-\nwork. In International Conference on Latent Vari-\nable Analysis and Signal Separation , pages 429–436.\nSpringer, 2015.\n[25] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke,\nand Alexander A Alemi. Inception-v4, inception-resnet\nand the impact of residual connections on learning. In\nAAAI , volume 4, page 12, 2017.\n[26] Naoya Takahashi and Yuki Mitsufuji. Multi-scale\nmulti-band densenets for audio source separation. In\nApplications of Signal Processing to Audio and Acous-\ntics (WASPAA), 2017 IEEE Workshop on , pages 21–25.\nIEEE, 2017.\n[27] Stefan Uhlich, Franck Giron, and Yuki Mitsufuji.\nDeep neural network based instrument extraction from\nmusic. In Acoustics, Speech and Signal Processing\n(ICASSP), 2015 IEEE International Conference on ,\npages 2135–2139. IEEE, 2015.\n[28] Stefan Uhlich, Marcello Porcu, Franck Giron, Michael\nEnenkl, Thomas Kemp, Naoya Takahashi, and Yuki\nMitsufuji. Improving music source separation based\non deep neural networks through data augmentation\nand network blending. In Acoustics, Speech and Signal\nProcessing (ICASSP), 2017 IEEE International Con-\nference on , pages 261–265. IEEE, 2017.\n[29] Shankar Vembu and Stephan Baumann. Separation of\nvocals from polyphonic audio recordings. In ISMIR ,\npages 337–344. Citeseer, 2005.\n[30] Emmanuel Vincent, Shoko Araki, Fabian Theis, Guido\nNolte, Pau Boﬁll, Hiroshi Sawada, Alexey Oze-\nrov, Vikrham Gowreesunker, Dominik Lutter, and\nNgoc QK Duong. The signal separation evaluation\ncampaign (2007–2010): Achievements and remain-\ning challenges. Signal Processing , 92(8):1928–1936,\n2012.\n[31] Emmanuel Vincent, R ´emi Gribonval, and C ´edric\nF´evotte. Performance measurement in blind audio\nsource separation. IEEE transactions on audio, speech,\nand language processing , 14(4):1462–1469, 2006.\n[32] Tuomas Virtanen. Monaural sound source separation\nby nonnegative matrix factorization with temporal con-\ntinuity and sparseness criteria. IEEE transactions on\naudio, speech, and language processing , 15(3):1066–\n1074, 2007.[33] Guan-Xiang Wang, Chung-Chien Hsu, and Jen-Tzung\nChien. Discriminative deep recurrent neural networks\nfor monaural speech separation. In Acoustics, Speech\nand Signal Processing (ICASSP), 2016 IEEE Interna-\ntional Conference on , pages 2544–2548. IEEE, 2016.\n[34] Yuxuan Wang, Arun Narayanan, and DeLiang Wang.\nOn training targets for supervised speech separation.\nIEEE/ACM Transactions on Audio, Speech and Lan-\nguage Processing (TASLP) , 22(12):1849–1858, 2014.\n[35] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and\nYaser Sheikh. Convolutional pose machines. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pages 4724–4732, 2016.\n[36] Felix Weninger, Jonathan Le Roux, John R Hershey,\nand Shinji Watanabe. Discriminative nmf and its appli-\ncation to single-channel source separation. In Fifteenth\nAnnual Conference of the International Speech Com-\nmunication Association , 2014.\n[37] Yi-Hsuan Yang. Low-rank representation of both\nsinging voice and music accompaniment via learned\ndictionaries. In ISMIR , pages 427–432, 2013.\n[38] Xiu Zhang, Wei Li, and Bilei Zhu. Latent time-\nfrequency component analysis: A novel pitch-based\napproach for singing voice separation. In Acoustics,\nSpeech and Signal Processing (ICASSP), 2015 IEEE\nInternational Conference on , pages 131–135. IEEE,\n2015.296 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Representation Learning of Music Using Artist Labels.",
        "author": [
            "Jiyoung Park",
            "Jongpil Lee",
            "Jangyeon Park",
            "Jung-Woo Ha",
            "Juhan Nam"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492517",
        "url": "https://doi.org/10.5281/zenodo.1492517",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/168_Paper.pdf",
        "abstract": "In music domain, feature learning has been conducted mainly in two ways: unsupervised learning based on sparse representations or supervised learning by semantic labels such as music genre. However, finding discriminative features in an unsupervised way is challenging and supervised feature learning using semantic labels may involve noisy or expensive annotation. In this paper, we present a supervised feature learning approach using artist labels annotated in every single track as objective meta data. We propose two deep convolutional neural networks (DCNN) to learn the deep artist features. One is a plain DCNN trained with the whole artist labels simultaneously, and the other is a Siamese DCNN trained with a subset of the artist labels based on the artist identity. We apply the trained models to music classification and retrieval tasks in transfer learning settings. The results show that our approach is comparable to previous state-of-the-art methods, indicating that the proposed approach captures general music audio features as much as the models learned with semantic labels. Also, we discuss the advantages and disadvantages of the two models.",
        "zenodo_id": 1492517,
        "dblp_key": "conf/ismir/ParkLPHN18",
        "keywords": [
            "unsupervised learning",
            "sparse representations",
            "supervised learning",
            "semantic labels",
            "discriminative features",
            "artist labels",
            "objective meta data",
            "deep convolutional neural networks",
            "music classification",
            "music retrieval"
        ],
        "content": "REPRESENTATION LEARNING OF MUSIC USING ARTIST LABELS\nJiyoung Park1\u0003Jongpil Lee2\u0003Jangyeon Park1Jung-Woo Ha1Juhan Nam2\n1NA VER Corp.\n2Graduate School of Culture Technology, KAIST\nfj.y.park, jangyeon.park, jungwoo.ha g@navercorp.com, frichter, juhannam g@kaist.ac.kr\nABSTRACT\nIn music domain, feature learning has been conducted\nmainly in two ways: unsupervised learning based on sparse\nrepresentations or supervised learning by semantic labels\nsuch as music genre. However, ﬁnding discriminative fea-\ntures in an unsupervised way is challenging and supervised\nfeature learning using semantic labels may involve noisy\nor expensive annotation. In this paper, we present a super-\nvised feature learning approach using artist labels anno-\ntated in every single track as objective meta data. We pro-\npose two deep convolutional neural networks (DCNN) to\nlearn the deep artist features. One is a plain DCNN trained\nwith the whole artist labels simultaneously, and the other is\na Siamese DCNN trained with a subset of the artist labels\nbased on the artist identity. We apply the trained models to\nmusic classiﬁcation and retrieval tasks in transfer learning\nsettings. The results show that our approach is compara-\nble to previous state-of-the-art methods, indicating that the\nproposed approach captures general music audio features\nas much as the models learned with semantic labels. Also,\nwe discuss the advantages and disadvantages of the two\nmodels.\n1. INTRODUCTION\nRepresentation learning or feature learning has been ac-\ntively explored in recent years as an alternative to feature\nengineering [1]. The data-driven approach, particularly us-\ning deep neural networks, has been applied to the area of\nmusic information retrieval (MIR) as well [14]. In this pa-\nper, we propose a novel audio feature learning method us-\ning deep convolutional neural networks and artist labels.\nEarly feature learning approaches are mainly based on\nunsupervised learning algorithms. Lee et al. used convolu-\ntional deep belief network to learn structured acoustic pat-\nterns from spectrogram [19]. They showed that the learned\nfeatures achieve higher performance than Mel-Frequency\nCepstral Coefﬁcients (MFCC) in genre and artist clas-\nsiﬁcation. Since then, researchers have applied various\n* Equally contributing authors.\nc\rJiyoung Park, Jongpil Lee, Jangyeon Park, Jung-Woo\nHa, Juhan Nam. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: Jiyoung Park, Jongpil\nLee, Jangyeon Park, Jung-Woo Ha, Juhan Nam. “Representation Learn-\ning of Music Using Artist Labels”, 19th International Society for Music\nInformation Retrieval Conference, Paris, France, 2018.unsupervised learning algorithms such as sparse coding\n[12, 24, 29, 31], K-means [8, 24, 30] and restricted Boltz-\nmann machine [24, 26]. Most of them focused on learn-\ning a meaningful dictionary on spectrogram by exploiting\nsparsity. While these unsupervised learning approaches are\npromising in that it can exploit abundant unlabeled audio\ndata, most of them are limited to single or dual layers,\nwhich are not sufﬁcient to represent complicated feature\nhierarchy in music.\nOn the other hand, supervised feature learning has been\nprogressively more explored. An early approach was map-\nping a single frame of spectrogram to genre or mood labels\nvia pre-trained deep neural networks and using the hidden-\nunit activations as audio features [11, 27]. More recently,\nthis approach was handled in the context of transfer learn-\ning using deep convolutional neural networks (DCNN)\n[6, 20]. Leveraging large-scaled datasets and recent ad-\nvances in deep learning, they showed that the hierarchi-\ncally learned features can be effective for diverse music\nclassiﬁcation tasks. However, the semantic labels that they\nuse such as genre, mood or other timbre descriptions tend\nto be noisy as they are sometimes ambiguous to annotate\nor tagged from the crowd. Also, high-quality annotation\nby music experts is known to be highly time-consuming\nand expensive.\nMeanwhile, artist labels are the meta data annotated to\nsongs naturally from the album release. They are objective\ninformation with no disagreement. Furthermore, consid-\nering every artist has his/her own style of music, artist la-\nbels may be regarded as terms that describe diverse styles\nof music. Thus, if we have a model that can discriminate\ndifferent artists from music, the model can be assumed to\nexplain various characteristics of the music.\nIn this paper, we verify the hypothesis using two DCNN\nmodels that are trained to identify the artist from an audio\ntrack. One is the basic DCNN model where the softmax\noutput units corresponds to each of artist. The other is the\nSiamese DCNN trained with a subset of the artist labels to\nmitigate the excessive size of the output layer in the plain\nDCNN when a large-scale dataset is used. After training\nthe two models, we regard them as a feature extractor and\napply artist features to three different genre datasets in two\nexperiment settings. First, we directly ﬁnd similar songs\nusing the artist features and K-nearest neighbors. Second,\nwe conduct transfer learning to further adapter the features\nto each of the datasets. The results show that proposed ap-\nproach captures useful features for unseen audio datasets717(a) The Basic Model\n (b) The Siamese Model\nFigure 1 . The proposed architectures for the model using artist labels.\nand the propose models are comparable to those trained\nwith semantic labels in performance. In addition, we dis-\ncuss the advantages and disadvantages of the two proposed\nDCNN models.\n2. LEARNING MODELS\nFigure 1 shows the two proposed DCNN models to learn\naudio features using artist labels. The basic model is\ntrained as a standard classiﬁcation problem. The Siamese\nmodel is trained using pair-wise similarity between an an-\nchor artist and other artists. In this section, we describe\nthem in detail.\n2.1 Basic Model\nThis is a widely used 1D-CNN model for music classiﬁca-\ntion [5, 9, 20, 25]. The model uses mel-spectrogram with\n128 bins in the input layer. We conﬁgured the DCNN such\nthat one-dimensional convolution layers slide over only a\nsingle temporal dimension. The model is composed of 5\nconvolution and max pooling layers as illustrated in Fig-\nure 1(a). Batch normalization [15] and rectiﬁed linear unit\n(ReLU) activation layer are used after every convolution\nlayer. Finally, we used categorical cross entropy loss in the\nprediction layer.\nWe train the model to classify artists instead of semantic\nlabels used in many music classiﬁcation tasks. For exam-\nple, if the number of artists used is 1,000, this becomes\na classiﬁcation problem that identiﬁes one of the 1,000\nartists. After training, the extracted 256-dimensional fea-\nture vector in the last hidden layer is used as the ﬁnal audio\nfeature learned using artist labels. Since this is the repre-\nsentation from which the identity is predicted by the lin-\near softmax classiﬁer, we can regard it as the highest-level\nartist feature.\n2.2 Siamese Model\nWhile the basic model is simple to train, it has two main\nlimitations. One is that the output layer can be excessively\nlarge if the dataset has numerous artists. For example, if adataset has 10,000 artists and the last hidden layer size is\n100, the number of parameters to learn in the last weight\nmatrix will reach 1M. Second, whenever new artists are\nadded to the dataset, the model must be trained again en-\ntirely. We solve the limitations using the Siamese DCNN\nmodel.\nA Siamese neural network consists of twin networks\nthat share weights and conﬁguration. It then provides\nunique inputs to the network and optimizes similarity\nscores [3, 18, 22]. This architecture can be extended to use\nboth positive and negative examples at one optimization\nstep. It is set up to take three examples: anchor item (query\nsong), positive item (relevant song to the query) and nega-\ntive item (different song to the query). This model is often\ncalled triplet networks and has been successfully applied to\nmusic metric learning when the relative similarity scores of\nsong triplets are available [21]. This model can be further\nextended to use several negative samples instead of just one\nnegative in the triplet network. This technique is called\nnegative sampling and has been popularly used in word\nembedding [23] and latent semantic model [13]. By using\nthis technique, they could effectively approximate the full\nsoftmax function when the output class is extremely large\n(i.e. 10,000 classes).\nWe approximate the full softmax output in the basic\nmodel with the Siamese neural networks using negative\nsampling technique. Regarding the artist labels, we set up\nthe negative sampling by treating identical artist’s song to\nthe anchor song as positive sample and other artists’ songs\nas negative samples. This method is illustrated in Figure\n1(b). Following [13], the relevance score between the an-\nchor song feature and other song feature is measured as:\nR(A; O) = cos( yA; yO) =yT\nAyO\njyAjjyOj(1)\nwhere yAandyOare the feature vectors of the anchor song\nand other song, respectively.\nMeanwhile, the choice of loss function is important in\nthis setting. We tested two loss functions. One is the soft-\nmax function with categorical cross-entropy loss to max-718 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018imize the positive relationships. The other is the max-\nmargin hinge loss to set only margins between positive and\nnegative examples [10]. In our preliminary experiments,\nthe Siamese model with negative sampling was success-\nfully trained only with the max-margin loss function be-\ntween the two objectives, which is deﬁned as follows:\nloss(A; O) =X\nO\u0000max[0 ;\u0001\u0000R(A; O+) +R(A; O\u0000)]\n(2)\nwhere \u0001is the margin, O+andO\u0000denotes positive ex-\nample and negative examples, respectively. We also grid-\nsearched the number of negative samples and the margin,\nand ﬁnally set the number of negative samples to 4 and\nthe margin value \u0001to 0.4. The shared audio model used in\nthis approach is exactly the same conﬁguration as the basic\nmodel.\n2.3 Compared Model\nIn order to verify the usefulness of the artist labels and the\npresented models, we constructed another model that has\nthe same architecture as the basic model but using semantic\ntags. In this model, the output layer size corresponds to\nthe number of the tag labels. Hereafter, we categorize all\nof them into artist-label model andtag-label model , and\ncompare the performance.\n3. EXPERIMENTS\nIn this section, we describe source datasets to train the two\nartist-label models and one tag-label model. We also in-\ntroduce target datasets for evaluating the three models. Fi-\nnally, the training details are explained.\n3.1 Source Tasks\nAll models are trained with the Million Song Dataset\n(MSD) [2] along with 30-second 7digital1preview clips.\nArtist labels are naturally annotated onto every song, thus\nwe simply used them. For the tag label, we used the\nLast.fm dataset augmented on MSD. This dataset contains\ntag annotation that matches the ID of the MSD.\n3.1.1 Artist-label Model\nThe number of songs that belongs to each artist may be ex-\ntremely skewed and this can make fair comparison among\nthe three models difﬁcult. Thus, we selected 20 songs for\neach artist evenly and ﬁltered out the artists who have less\nthan this. Also, we conﬁgured several sets of the artist lists\nto see the effect of the number of artists on the model per-\nformances (500, 1,000, 2,000, 5,000 and 10,000 artists).\nWe then divided them into 15, 3 and 2 songs for training,\nvalidation and testing, respectively for the sets contain less\nthan 10,000 artists. For the 10,000 artist sets, we parti-\ntioned them in 17, 1 and 2 songs because once the artists\nreach 10,000, the validation set already become 10,000\nsongs even when we only use 1 song from each artist which\nis already sufﬁcient for validating the model performance.\n1https://www.7digital.com/We also should note that the testing set is actually not used\nin the whole experiments in this paper because we used the\nsource dataset only for training the models to use them as\nfeature extractors. The reason we ﬁltered and split the data\nin this way is for future work2.\n3.1.2 Tag-label Model\nWe used 5,000 artists set as a baseline experiment setting.\nThis contains total 90,000 songs in the training and valida-\ntion set with a split of 75,000 and 15,000. We thus con-\nstructed the same size set for tagging dataset to compare\nthe artist-label models and the tag-label model. The tags\nand songs are ﬁrst ﬁltered in the same way as the previous\nworks [4, 20]. Among the list with the ﬁltered top 50 used\ntags, we randomly selected 90,000 songs and split them\ninto the same size as the 5,000 artist set.\n3.2 Target Tasks\nWe used 3 different datasets for genre classiﬁcation.\n\u000fGTZAN (fault-ﬁltered version) [17, 28]: 930 songs,\n10 genres. We used a “fault-ﬁltered” version of\nGTZAN [17] where the dataset was divided to pre-\nvent artist repetition in training/validation/test sets.\n\u000fFMA small [7]: 8,000 songs, 8 balanced genres.\n\u000fNA VER Music3dataset with only Korean artists:\n8,000 songs, 8 balanced genres. We ﬁltered songs\nwith only have one genre to clarify the genre char-\nacteristic.\n3.3 Training Details\nFor the preprocessing, we computed the spectrogram using\n1024 samples for FFT with a Hanning window, 512 sam-\nples for hop size and 22050 Hz as sampling rate. We then\nconverted it to mel-spectrogram with 128 bins along with\na log magnitude compression.\nWe chose 3 seconds as a context window of the DCNN\ninput after a set of experiments to ﬁnd an optimal length\nthat works well in music classiﬁcation task. Out of the 30-\nsecond long audio, we randomly extracted the context size\naudio and put them into the networks as a single exam-\nple. The input normalization was performed by dividing\nstandard deviation after subtracting mean value across the\ntraining data.\nWe optimized the loss using stochastic gradient descent\nwith 0.9 Nesterov momentum with 1e\u00006learning rate de-\ncay. Dropout 0.5 is applied to the output of the last ac-\ntivation layer for all the models. We reduce the learning\nrate when a valid loss has stopped decreasing with the ini-\ntial learning rate 0.015 for the basic models (both artist-\nlabel and tag-label) and 0.1 for the Siamese model. Zero-\npadding is applied to each convolution layer to maintain its\nsize.\n2All the data splits of the source tasks are available at the link for re-\nproducible research https://github.com/jiyoungpark527/\nmsd-artist-split .\n3http://music.naver.comProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 719Our system was implemented in Python 2.7, Keras 2.1.1\nand Tensorﬂow-gpu 1.4.0 for the back-end of Keras. We\nused NVIDIA Tesla M40 GPU machines for training our\nmodels. Code and models are available at the link for re-\nproducible research4.\n4. FEATURE EVALUATION\nWe apply the learned audio features to genre classiﬁca-\ntion as a target task in two different approaches: feature\nsimilarity-based retrieval and transfer learning. In this sec-\ntion, we describe feature extraction and feature evaluation\nmethods.\n4.1 Feature Extraction Using the DCNN Models\nIn this work, the models are evaluated in three song-level\ngenre classiﬁcation tasks. Thus, we divided 30-second au-\ndio clip into 10 segments to match up with the model input\nsize and the 256-dimension features from the last hidden\nlayer are averaged into a single song-level feature vector\nand used for the following tasks. For the tasks that require\nsong-to-song distances, cosine similarity is used to match\nup with the Siamese model’s relevance score.\n4.2 Feature Similarity-based Song Retrieval\nWe ﬁrst evaluated the models using mean average preci-\nsion (MAP) considering genre labels as relevant items. Af-\nter obtaining a ranked list for each song based on cosine\nsimilarity, we measured the MAP as following:\nAP=P\nk2relprecision k\nnumber of relevant items(3)\nMAP =PQ\nq=1AP(q)\nQ(4)\nwhere Q is the number of queries. precision kmeasures\nthe fraction of correct items among ﬁrst kretrieved list.\nThe purpose of this experiment is to directly verify\nhow similar feature vectors with the same genre are in the\nlearned feature space.\n4.3 Transfer Learning\nWe classiﬁed audio examples using the k-nearest neigh-\nbors (k-NN) classiﬁer and linear softmax classiﬁer. The\nevaluation metric for this experiment is classiﬁcation ac-\ncuracy. We ﬁrst classiﬁed audio examples using k-NN to\nclassify the input audio into the largest number of genres\namong k nearest to features from the training set. The num-\nber of k is set to 20 in this experiment. This method can be\nregarded as a similarity-based classiﬁcation. We also clas-\nsiﬁed audio using a linear softmax classiﬁer. The purpose\nof this experiment is to verify how much the audio features\nof unseen datasets are linearly separable in the learned fea-\nture space.\n4https://github.com/jongpillee/\nismir2018-artist .MAPArtist-label\nBasic ModelArtist-label\nSiamese ModelTag-label\nModel\nGTZAN\n(fault-ﬁltered)0.4968 0.5510 0.5508\nFMA small 0.2441 0.3203 0.3019\nNA VER Korean 0.3152 0.3577 0.3576\nTable 1 . MAP results on feature similarity-based retrieval.\nKNNArtist-label\nBasic ModelArtist-label\nSiamese ModelTag-label\nModel\nGTZAN\n(fault-ﬁltered)0.6655 0.6966 0.6759\nFMA small 0.5269 0.5732 0.5332\nNA VER Korean 0.6671 0.6393 0.6898\nTable 2 . KNN similarity-based classiﬁcation accuracy.\nLinear SoftmaxArtist-label\nBasic ModelArtist-label\nSiamese ModelTag-label\nModel\nGTZAN\n(fault-ﬁltered)0.6721 0.6993 0.7072\nFMA small 0.5791 0.5483 0.5641\nNA VER Korean 0.6696 0.6623 0.6755\nTable 3 . Classiﬁcation accuracy of a linear softmax.\n5. RESULTS AND DISCUSSION\n5.1 Tag-label Model vs. Artist-label Model\nWe ﬁrst compare the artist-label models to the tag-label\nmodel when they are trained with the same dataset size\n(90,000 songs). The results are shown in Table 1, 2 and\n3. In feature similarity-based retrieval using MAP (Table\n1), the artist-based Siamese model outperforms the rest on\nall target datasets. In the genre classiﬁcation tasks (Table 2\nand 3), Tag-label model works slightly better than the rest\non some datasets and the trend becomes stronger in the\nclassiﬁcation using the linear softmax. Considering that\nthe source task in the tag-based model (trained with the\nLast.fm tags) contains genre labels mainly, this result may\nattribute to the similarity of labels in both source and target\ntasks. Therefore, we can draw two conclusions from this\nexperiment. First, the artist-label model is more effective\nin similarity-based tasks (1 and 2) when it is trained with\nthe proposed Siamese networks, and thus it may be more\nuseful for music retrieval. Second, the semantic-based\nmodel is more effective in genre or other semantic label\ntasks and thus it may be more useful for human-friendly\nmusic content organization.\n5.2 Basic Model vs. Siamese Model\nNow we focus on the comparison of the two artist-label\nmodels. From Table 1, 2 and 3, we can see that the Siamese\nmodel generally outperforms the basic model. However,\nthe difference become attenuated in classiﬁcation tasks and\nthe Siamese model is even worse on some datasets. Among\nthem, it is notable that the Siamese model is signiﬁcantly\nworse than the basic model on the NA VER Music dataset720 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 2 . MAP results with regard to different number of artists in the feature models.\nFigure 3 . Genre classiﬁcation accuracy using k-NN with regard to different number of artists in the feature models.\nFigure 4 . Genre classiﬁcation accuracy using linear softmax with regard to different number of artists in the feature models.\nin the genre classiﬁcation using k-NN even though they are\nbased on feature similarity. We dissected the result to see\nwhether it is related to the cultural difference between the\ntraining data (MSD, mostly Western) and the target data\n(the NA VER set, only Korean). Figure 5 shows the de-\ntailed classiﬁcation accuracy for each genre of the NA VER\ndataset. In three genres, ‘Trot’,‘K-pop Ballad’ and ‘Kids’\nthat do not exist in the training dataset, we can see that the\nbasic model outperforms the Siamese model whereas the\nresults are opposite in the other genres. This indicates that\nthe basic model is more robust to unseen genres of music.\nOn the other hand, the Siamese model slightly over-ﬁts to\nthe training set, although it effectively captures the artist\nfeatures.\n5.3 Effect of the Number of Artists\nWe further analyze the artist-label models by investigat-\ning how the number of artists in training the DCNN af-\nfects the performance. Figure 2, 3 and 4 are the results\nthat show similarity-based retrieval (MAP) and genre clas-\nsiﬁcation (accuracy) using k-NN and linear softmax, re-\nspectively, according to the increasing number of training\nartists. They show that the performance is generally pro-\nportional to the number of artists but the trends are quite\ndifferent between the two models. In the similarity-based\nretrieval, the MAP of the Siamese model is signiﬁcantly\nhigher than that of the basic model when the number of\nFigure 5 . The classiﬁcation results of each genre for the\nNA VER dataset with only Korean music.\nartists is greater than 1,000. Also, as the number of artists\nincreases, the MAP of the Siamese model consistently\ngoes up with a slight lower speed whereas that of the ba-\nsic model saturates at 2,000 or 5,000 artists. On the other\nhand, the performance gap changes in the two classiﬁca-\ntion tasks. On the GTZAN dataset, while the basic model\nis better for 500 and 1,000 artists, the Siamese model\nreverses it for 2,000 and more artists. On the NA VER\ndataset, the basic model is consistently better. On the FMA\nsmall, the results are mixed in two classiﬁers. Again, the\nresults may be explained by our interpretation of the mod-\nels in Section 5.2. In summary, the Siamese model seemsProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 721ModelsGTZAN\n(fault-ﬁltered)FMA small\n2-D CNN [17] 0.6320 -\nTemporal features [16] 0.6590 -\nMulti-level Multi-scale [20] 0.7200 -\nSVM [7] - 0.5482y\nArtist-label Basic model 0.7076 0.5687\nArtist-label Siamese model 0.7203 0.5673\nTable 4 . Comparison with previous state-of-the-art mod-\nels: classiﬁcation accuracy results. Linear softmax classi-\nﬁer is used and features are extracted from the artist-label\nmodels trained with 10,000 artists. yThis result was ob-\ntained using the provided code and dataset in [7].\nto work better in similarity-based tasks and the basic model\nis more robust to different genres of music. In addition,\nthe Siamese model is more capable of being trained with a\nlarge number of artists.\n5.4 Comparison with State-of-the-arts\nThe effectiveness of artist labels is also supported by com-\nparison with previous state-of-the-art models in Table 4.\nFor this result, we report two artist-label models trained\nwith 10,000 artists using linear softmax classiﬁer. In this\ntable, we can see that the proposed models are comparable\nto the previous state-of-the-art methods.\n6. VISUALIZATION\nWe visualize the extracted feature to provide better insight\non the discriminative power of learned features using artist\nlabels. We used the DCNN trained to classify 5,000 artists\nas a feature extractor. After collecting the feature vec-\ntors, we embedded them into 2-dimensional vectors using\nt-distributed stochastic neighbor embedding (t-SNE).\nFor artist visualization, we collect a subset of MSD\n(apart from the training data for the DCNN) from well-\nknown artists. Figure 6 shows that artists’ songs are appro-\npriately distributed based on genre, vocal style and gender.\nFor example, artists with similar genre of music are closely\nlocated and female pop singers are close to each other ex-\ncept Maria Callas who is a classical opera singer. Interest-\ningly, some songs by Michael Jackson are close to female\nvocals because of his distinctive high-pitched tone.\nFigure 7 shows the visualization of features extracted\nfrom the GTZAN dataset. Even though the DCNN was\ntrained to discriminate artist labels, they are well clustered\nby genre. Also, we can observe that some genres such\nas disco, rock and hip-hop are divided into two or more\ngroups that might belong to different sub-genres.\n7. CONCLUSION AND FUTURE WORK\nIn this work, we presented the models to learn audio fea-\nture representation using artist labels instead of semantic\nlabels. We compared two artist-label models and one tag-\nlabel model. The ﬁrst is a basic DCNN consisting of a\nsoftmax output layer to predict which artist they belong to\nout of all artists used. The second is a Siamese-style ar-\nchitecture that maximizes the relative similarity score be-\nFigure 6 . Feature visualization by artist. Total 22 artists\nare used and, among them, 15 artists are represented in\ncolor.\nFigure 7 . Feature visualization by genre. Total 10 genres\nfrom the GTZAN dataset are used.\ntween a small subset of the artist labels based on the artist\nidentity. The last is a model optimized using tag labels\nwith the same architecture as the ﬁrst model. After the\nmodels are trained, we used them as feature extractors and\nvalidated the models on song retrieval and genre classiﬁ-\ncation tasks on three different datasets. Three interesting\nresults were found during the experiments. First, the artist-\nlabel models, particularly the Siamese model, is compa-\nrable to or outperform the tag-label model. This indicates\nthat the cost-free artist-label is as effective as the expensive\nand possibly noisy tag-label. Second, the Siamese model\nshowed the best performances on song retrieval task in all\ndatasets tested. This can indicate that the pair-wise rele-\nvance score loss in the Siamese model helps the feature\nsimilarity-based search. Third, the use of a large number\nof artists increases the model performance. This result is\nalso useful because the artists can be easily increased to a\nvery large number.\nAs future work, we will investigate the artist-label\nSiamese model more thoroughly. First, we plan to in-\nvestigate advanced audio model architecture and diverse\nloss and pair-wise relevance score functions. Second, the\nmodel can easily be re-trained using new added artists be-\ncause the model does not have ﬁxed output layer. This\nproperty will be evaluated using cross-cultural data or us-\ning extremely small data (i.e. one-shot learning [18]).722 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20188. ACKNOWLEDGEMENT\nThis work was supported by Basic Science Research Pro-\ngram through the National Research Foundation of Korea\nfunded by the Ministry of Science, ICT & Future Planning\n(2015R1C1A1A02036962) and by NA VER Corp.\n9. REFERENCES\n[1] Yoshua Bengio, Aaron Courville, and Pascal Vincent.\nRepresentation learning: A review and new perspec-\ntives. IEEE transactions on pattern analysis and ma-\nchine intelligence , 35(8):1798–1828, 2013.\n[2] Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whit-\nman, and Paul Lamere. The million song dataset. In\nProc. of the International Society for Music Informa-\ntion Retrieval Conference (ISMIR) , volume 2, pages\n591–596, 2011.\n[3] Jane Bromley, Isabelle Guyon, Yann LeCun, Ed-\nuard S ¨ackinger, and Roopak Shah. Signature veriﬁca-\ntion using a “siamese” time delay neural network. In\nAdvances in Neural Information Processing Systems\n(NIPS) , pages 737–744, 1994.\n[4] Keunwoo Choi, George Fazekas, and Mark Sandler.\nAutomatic tagging using deep convolutional neural\nnetworks. In Proc. of the International Society for Mu-\nsic Information Retrieval Conference (ISMIR) , pages\n805–811, 2016.\n[5] Keunwoo Choi, Gy ¨orgy Fazekas, Mark Sandler, and\nKyunghyun Cho. Convolutional recurrent neural net-\nworks for music classiﬁcation. In Proc. of the IEEE In-\nternational Conference on Acoustics, Speech, and Sig-\nnal Processing (ICASSP) , pages 2392–2396, 2017.\n[6] Keunwoo Choi, Gy ¨orgy Fazekas, Mark Sandler, and\nKyunghyun Cho. Transfer learning for music classi-\nﬁcation and regression tasks. In Proc. of the Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR) , pages 141–149, 2017.\n[7] Micha ¨el Defferrard, Kirell Benzi, Pierre Van-\ndergheynst, and Xavier Bresson. Fma: A dataset for\nmusic analysis. In Proc. of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 316–323, 2017.\n[8] Sander Dieleman and Benjamin Schrauwen. Multi-\nscale approaches to music audio feature learning. In\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 116–121, 2013.\n[9] Sander Dieleman and Benjamin Schrauwen. End-to-\nend learning for music audio. In Proc. of the IEEE In-\nternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP) , pages 6964–6968, 2014.\n[10] Andrea Frome, Greg S Corrado, Jon Shlens, Samy\nBengio, Jeff Dean, Tomas Mikolov, et al. Devise: A\ndeep visual-semantic embedding model. In Advancesin neural information processing systems (NIPS) ,\npages 2121–2129, 2013.\n[11] Philippe Hamel and Douglas Eck. Learning features\nfrom music audio with deep belief networks. In Proc.\nof the International Conference on Music Information\nRetrieval (ISMIR) , pages 339–344, 2010.\n[12] Mikael Henaff, Kevin Jarrett, Koray Kavukcuoglu, and\nYann LeCun. Unsupervised learning of sparse features\nfor scalable audio classiﬁcation. In Proc. of the Inter-\nnational Conference on Music Information Retrieval\n(ISMIR) , pages 681–686, 2011.\n[13] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry Heck. Learning deep structured\nsemantic models for web search using clickthrough\ndata. In Proc. of the 22nd ACM international confer-\nence on Conference on information & knowledge man-\nagement , pages 2333–2338. ACM, 2013.\n[14] Eric Humphrey, Juan Bello, and Yann LeCun. Feature\nlearning and deep architectures: new directions for mu-\nsic informatics. Journal of Intelligent Information Sys-\ntems, 41(3):461–481, Dec 2013.\n[15] Sergey Ioffe and Christian Szegedy. Batch normaliza-\ntion: Accelerating deep network training by reducing\ninternal covariate shift. In International Conference on\nMachine Learning (ICML) , pages 448–456, 2015.\n[16] Il-Young Jeong and Kyogu Lee. Learning temporal fea-\ntures using a deep neural network and its application\nto music genre classiﬁcation. In Proc. of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 434–440, 2016.\n[17] Corey Kereliuk, Bob L Sturm, and Jan Larsen. Deep\nlearning and music adversaries. IEEE Transactions on\nMultimedia , 17(11):2059–2071, 2015.\n[18] Gregory Koch, Richard Zemel, and Ruslan Salakhut-\ndinov. Siamese neural networks for one-shot image\nrecognition. In ICML Deep Learning Workshop , vol-\nume 2, 2015.\n[19] Honglak Lee, Peter Pham, Yan Largman, and An-\ndrew Y Ng. Unsupervised feature learning for au-\ndio classiﬁcation using convolutional deep belief net-\nworks. In Advances in neural information processing\nsystems (NIPS) , pages 1096–1104, 2009.\n[20] Jongpil Lee and Juhan Nam. Multi-level and multi-\nscale feature aggregation using pretrained convolu-\ntional neural networks for music auto-tagging. IEEE\nSignal Processing Letters , 24(8):1208–1212, 2017.\n[21] Rui Lu, Kailun Wu, Zhiyao Duan, and Changshui\nZhang. Deep ranking: Triplet matchnet for music met-\nric learning. In Proc. of the IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 121–125, 2017.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 723[22] Pranay Manocha, Rohan Badlani, Anurag Kumar,\nAnkit Shah, Benjamin Elizalde, and Bhiksha Raj.\nContent-based representations of audio using siamese\nneural networks. In Proc. of the IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , 2018.\n[23] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S\nCorrado, and Jeff Dean. Distributed representations\nof words and phrases and their compositionality. In\nAdvances in neural information processing systems\n(NIPS) , pages 3111–3119, 2013.\n[24] Juhan Nam, Jorge Herrera, Malcolm Slaney, and\nJulius O. Smith. Learning sparse feature representa-\ntions for music annotation and retrieval. In Proc. of\nthe International Conference on Music Information Re-\ntrieval (ISMIR) , pages 565–570, 2012.\n[25] Jordi Pons, Thomas Lidy, and Xavier Serra. Experi-\nmenting with musically motivated convolutional neu-\nral networks. In Proc. of the International Workshop\non Content-Based Multimedia Indexing (CBMI) , pages\n1–6, 2016.\n[26] Jan Schl ¨uter and Christian Osendorfer. Music Simi-\nlarity Estimation with the Mean-Covariance Restricted\nBoltzmann Machine. In Proc. of the International Con-\nference on Machine Learning and Applications , pages\n118–123, 2011.\n[27] Erik M. Schmidt and Youngmoo E. Kim. Learning\nemotion-based acoustic features with deep belief net-\nworks. In Proc. of the IEEE Workshop on Applications\nof Signal Processing to Audio and Acoustics (WAS-\nPAA) , pages 65–68, 2011.\n[28] George Tzanetakis and Perry Cook. Musical genre\nclassiﬁcation of audio signals. IEEE Transactions on\nspeech and audio processing , 10(5):293–302, 2002.\n[29] Yonatan Vaizman, Brian McFee, and Gert Lanckriet.\nCodebook-based audio feature representation for mu-\nsic information retrieval. IEEE/ACM Transactions on\nAudio, Speech and Language Processing (TASLP) ,\n22(10):1483–1493, 2014.\n[30] Jan W ¨ulﬁng and Martin Riedmiller. Unsupervised\nlearning of local features for music classiﬁcation. In\nProc. of the International Society for Music Informa-\ntion Retrieval Conference (ISMIR) , pages 139–144,\n2012.\n[31] Chin-Chia Yeh, Li Su, and Yi-Hsuan Yang. Dual-layer\nbag-of-frames model for music genre classiﬁcation. In\nProc. of the IEEE International Conference on Acous-\ntics, Speech, and Signal Processing (ICASSP) , pages\n246–250, 2013.724 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "End-to-end Learning for Music Audio Tagging at Scale.",
        "author": [
            "Jordi Pons",
            "Oriol Nieto",
            "Matthew Prockup",
            "Erik M. Schmidt",
            "Andreas F. Ehmann",
            "Xavier Serra"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492497",
        "url": "https://doi.org/10.5281/zenodo.1492497",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/191_Paper.pdf",
        "abstract": "The lack of data tends to limit the outcomes of deep learning research, particularly when dealing with end-to-end learning stacks processing raw data such as waveforms. In this study, 1.2M tracks annotated with musical labels are available to train our end-to-end models. This large amount of data allows us to unrestrictedly explore two different design paradigms for music auto-tagging: assumption-free models – using waveforms as input with very small convolutional filters; and models that rely on domain knowledge – log-mel spectrograms with a convolutional neural network designed to learn timbral and temporal features. Our work focuses on studying how these two types of deep architectures perform when datasets of variable size are available for training: the MagnaTagATune (25k songs), the Million Song Dataset (240k songs), and a private dataset of 1.2M songs. Our experiments suggest that music domain assumptions are relevant when not enough training data are available, thus showing how waveform-based models outperform spectrogrambased ones in large-scale data scenarios.",
        "zenodo_id": 1492497,
        "dblp_key": "conf/ismir/PonsNPSES18",
        "keywords": [
            "end-to-end learning",
            "raw data",
            "waveforms",
            "music auto-tagging",
            "large amount of data",
            "two different design paradigms",
            "domain knowledge",
            "log-mel spectrograms",
            "variable size datasets",
            "training scenarios"
        ],
        "content": "END-TO-END LEARNING FOR MUSIC AUDIO TAGGING AT SCALE\nJordi Pons?yOriol NietoyMatthew ProckupyErik SchmidtyAndreas EhmannyXavier Serra?\n?Music Technology Group, Universitat Pompeu Fabra, Barcelona\nyPandora Media Inc., Oakland, CA\nABSTRACT\nThe lack of data tends to limit the outcomes of deep learn-\ning research, particularly when dealing with end-to-end\nlearning stacks processing raw data such as waveforms.\nIn this study, 1.2M tracks annotated with musical la-\nbels are available to train our end-to-end models. This\nlarge amount of data allows us to unrestrictedly explore\ntwo different design paradigms for music auto-tagging:\nassumption-free models – using waveforms as input with\nvery small convolutional ﬁlters; and models that rely on\ndomain knowledge – log-mel spectrograms with a convo-\nlutional neural network designed to learn timbral and tem-\nporal features. Our work focuses on studying how these\ntwo types of deep architectures perform when datasets\nof variable size are available for training: the MagnaTa-\ngATune (25k songs), the Million Song Dataset (240k\nsongs), and a private dataset of 1.2M songs. Our experi-\nments suggest that music domain assumptions are relevant\nwhen not enough training data are available, thus show-\ning how waveform-based models outperform spectrogram-\nbased ones in large-scale data scenarios.\n1. INTRODUCTION\nOne fundamental goal in music informatics research is to\nautomatically structure large music collections. The music\naudio tagging task consists of automatically estimating the\nmusical attributes of a song – including: moods, language\nof the lyrics, year of composition, genres, instruments, har-\nmony, or rhythmic traits. Thus, tag estimates may be use-\nful to deﬁne a semantic space that can be advantageous for\nautomatically organizing musical libraries.\nMany approaches have been considered for this task\n(mostly based on feature extraction + model [1, 22, 26]),\nwith recent publications showing promising results using\ndeep architectures [5, 9, 14, 21]. In this work we conﬁrm\nthis trend by studying how two deep architectures con-\nceived considering opposite design strategies (using do-\nmain knowledge or not) perform for several datasets – with\none of the datasets being of an unprecedented size: 1.2M\nsongs. Provided that a sizable amount of data is avail-\nable for that study, we investigate the learning capabili-\nc\rJordi Pons, Oriol Nieto, Matthew Prockup, Erik\nSchmidt, Andreas Ehmann, Xavier Serra. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Jordi Pons, Oriol Nieto, Matthew Prockup, Erik Schmidt, Andreas\nEhmann, Xavier Serra. “END-TO-END LEARNING FOR MUSIC AU-\nDIO TAGGING AT SCALE”, 19th International Society for Music Infor-\nmation Retrieval Conference, Paris, France, 2018.ties of these two architectures. Speciﬁcally, we investi-\ngate whether the architectures based on domain knowledge\noverly constrain the solution space for cases where large\ntraining data are available – in essence, we study if certain\narchitectural choices (e.g., using log-mel spectrograms as\ninput) can limit the model’s capabilities to learn from data.\nThe main contribution of this work is to show that little to\nno model assumptions are required for music auto-tagging\nwhen operating with large amounts of data.\nSection 2 discusses the main deep architectures we\nidentiﬁed in the audio literature, section 3 describes the\ndatasets used for this work, section 4 presents the architec-\ntures we study, and section 5 provides discussion about the\nresults with conclusions drawn in section 6.\n2. CURRENT DEEP ARCHITECTURES\nIn order to facilitate the discussion around the current au-\ndio architectures, we divide deep learning models into two\nparts: front-end and back-end – see Figure 1. The front-\nend is the part of the model that interacts with the input\nsignal in order to map it into a latent-space, and the back-\nend predicts the output given the representation obtained\nby the front-end. In the following, we present the main\nfront- and back-ends we identiﬁed in the literature.\nFigure 1 . Deep learning pipeline.\nFront-ends. These are generally comprised of con-\nvolutional neural networks (CNNs) [5, 9, 20, 21, 27],\nsince these can learn efﬁcient representations by sharing\nweights1along the signal. Front-ends can be divided into\ntwo groups depending on the used input signal: wave-\nforms [9, 14, 27] or spectrograms [5, 20, 21]. Further, the\ndesign of the ﬁlters can be either based on domain knowl-\nedge or not. For example, one leverages domain knowl-\nedge when a front-end for waveforms is designed so that\nthe length of the ﬁlter is set to be as the window length of a\nSTFT [9]. Or for a spectrogram front-end, it is used verti-\ncal ﬁlters to learn timbral representations [12] or horizontal\nﬁlters to learn longer temporal cues [25]. Generally, a sin-\ngle ﬁlter shape is used in the ﬁrst CNN layer [5, 9, 12, 25],\nbut some recent works report performance gains when us-\ning several ﬁlter shapes in the ﬁrst layer [4, 18–21, 27].\nUsing many ﬁlters promotes a richer feature extraction in\nthe ﬁrst layer, and facilitates leveraging domain knowl-\nedge for designing the ﬁlters’ shape. For example: a\n1Which determine the learned feature representations.637waveform front-end using many long ﬁlters (of different\nlengths) can be motivated from the perspective of a multi-\nresolution time-frequency transform2[27]; or since it is\nknown that some patterns in spectrograms are occurring\nat different time-frequency scales, one can intuitively in-\ncorporate many (different) vertical and/or horizontal ﬁlters\nin a spectrogram front-end [18–21]. To summarize, using\ndomain knowledge when designing models allows us to\nnaturally connect the deep learning literature with previ-\nous signal processing work. On the other hand, when do-\nmain knowledge is not used, it is common to employ a deep\nstack of small ﬁlters, e.g.: 3 \u00021 as in the sample-level front-\nend used for waveforms [14], or 3 \u00023 ﬁlters used for spec-\ntrograms [5]. These models based on small ﬁlters make\nminimal assumptions over the local stationarities of the\nsignal, so that any structure can be learned via hierarchi-\ncally combining small-context representations. These ar-\nchitectures with small ﬁlters are ﬂexible models able to po-\ntentially learn any structure given enough depth and data.\nBack-ends. Among the different back-ends used in the\naudio literature, we identiﬁed two main groups: (i)ﬁxed-\nlength input back-end, and (ii)variable-length input back-\nend. The generally convolutional nature of the front-end\nallows it to process different input lengths. Therefore, the\nback-end unit can adapt a variable-length feature map to a\nﬁx-sized output. The former group of models ( i) assume\nthat the input will be kept constant – examples of those\nare front-ends based on feed-forward neural-networks or\nfully-convolutional stacks [5,9]. The second group ( ii) can\ndeal with different input-lengths since the model is ﬂexi-\nble in at least one of its input dimensions – examples of\nthose are back-ends using temporal-aggregation strategies\nsuch as max-pooling, average-pooling, attention models or\nrecurrent neural networks [23]. Given that songs are gen-\nerally of different lengths, these types of back-ends are\nideal candidates for music processing. However, despite\nthe different-length nature of music, many works employ\nﬁxed-length input back-ends (group i) since these architec-\ntures tend to be simpler and perform well [5, 9, 21].\n3. DATASETS\nWe study how different deep architectures for music auto-\ntagging perform for 3 music collections of different sizes:\n1) The MagnaTagATune (MTT) dataset is of \u001926k mu-\nsic audio clips of 30s [11]. Predicting the top-50 tags of\nthis dataset is a popular benchmark for auto-tagging.\n2) Although the Million Song Dataset (MSD) name in-\ndicates that 1M songs are available [2], audio ﬁles with\nproper tag annotations (top-50 tags) are only available\nfor\u0019240k previews of 30s. This dataset constitutes the\nbiggest public dataset available for music auto-tagging,\nmaking these data highly appropriate for benchmarking.\n3) A private dataset consisting of 1M songs for training,\n100k for validation, and 100k for test3is available for this\nstudy. The 1.2M-songs dataset has 139 track-level human-\nexpert annotations that can be summarized as follows:\n2The Constant-Q Transform [3] is an example of such transform.\n3Test & validation sets are kept the same throughout the experiments\nfor a fair evaluation. All used partitions are stratiﬁed and artist-ﬁltered.\u0001Meter tags denote different sorts of musical meters\n(e.g., triple-meter, cut-time, compound-duple, odd).\n\u0001Rhythmic feel tags denote rhythmic interpretation\n(e.g., swing, shufﬂe, back-beat strength) and elements of\nrhythmic perception ( e.g., syncopation, danceability).\n\u0001Harmonic tags : major, minor, chromatic, etc.\n\u0001Mood tags express the sentiment of a music audio clip\n(e.g., if the music is angry, sad, joyful).\n\u0001Vocal tags denote the presence of vocals and timbral\ncharacteristics of it (e.g., male, female, vocal grittiness).\n\u0001Instrumentation tags denote the presence of instru-\nments (e.g., piano) and their timbre (e.g., guitar distortion).\n\u0001Sonority tags detail production techniques (e.g., stu-\ndio, live) and overall sound (e.g., acoustic, synthesized).\n\u0001Basic genre tags : jazz, rock, rap, latin, disco, etc.\n\u0001Subgenre tags : jazz (e.g., cool, fusion, hard bop),\nrock (e.g., light, hard, punk), rap (e.g., east coast, old\nschool), world music (e.g., cajun, indian), classical music\n(e.g., baroque period, classical period), etc.\nOther large (music) audio datasets exist: the Free Music\nArchive (FMA:\u0019106k songs) [8] and Audioset ( \u00192.1M\naudios) [10]. Since previous works mainly used the MTT\nand MSD [5, 14], we employ these datasets to assess the\nstudied models with public data. Despite our interest in\nusing FMA, for brevity, we restrict our study to 3 datasets\nthat already cover a wide range of different sizes. Finally,\nAudioset is not used since most of its content is not music.\n4. THE ARCHITECTURES UNDER STUDY\nAfter an initial exploration of the different architectures in-\ntroduced in section 2, we select two models based on op-\nposite design paradigms: one for processing waveforms,\nwith a design that does minimal assumptions over the task\nat hand; and another for spectrograms, with a design that\nheavily relies on musical domain knowledge. Our goal\nis to compare these two models for providing insights in\nwhether domain knowledge is required (or not) for design-\ning deep learning models. This section provides discussion\naround our architectural choices and introduces the basic\nconﬁguration setup – which is also accessible online.4\nThe waveform model was selected after observing that\nthe sample-level front-end (using a deep stack of 3 \u00021 ﬁl-\nters) was remarkably superior to the other waveform-based\nfront ends – as shown in the original paper [14]. This re-\nsult is particularly compelling because this front-end does\nnot rely on domain-knowledge for its design. Note that\nraw waveforms are fed to the model without any pre-\nprocessing, and the small ﬁlters considered for its design\nmake no strong assumptions over the most informative lo-\ncal stationarities in waveforms. Therefore, the sample-\nlevel can be seen as a problem agnostic front-end that has\nthe potential to learn any audio task provided that enough\ndepth and data are available. Given that a large amount\ndata is available for this study, the sample-level front-end\nis of particular interest due to its strong learning potential:\nits solution space is not constrained by severe architectural\nchoices relying on domain knowledge.\n4https://github.com/jordipons/music-audio-tagging-at-scale-models638 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 2 .Bottom-left – back-end. Top-left – waveform front-end. Right – spectrogram front-end. Deﬁnitions –M’stands\nfor the feature map’s vertical axis, BNfor batch norm, and MPfor max-pool.\nOn the other hand, when experimenting with spectro-\ngram front-ends, we found domain knowledge intuitions\nto be valid guides for designing deep architectures. For\nexample, front-ends based on (i)many vertical and hori-\nzontal ﬁlters in the ﬁrst layer were consistently superior to\nfront-ends based on (ii)a single vertical ﬁlter – as shown in\nrecent publications [4, 18–20]. Note that the former front-\nends (i)can learn spectral and (long) temporal represen-\ntations already in the ﬁrst layer – which are known to be\nimportant musical cues; while the latter (ii)can only learn\nspectral representations. Moreover, we observed that front-\nends based on a deep stack of 3 \u00023 ﬁlters were achieving\nequivalent performances to the former front-end (i)when\ninput segments were shorter than 10s – as noted in the lit-\nerature [21]. But when considering longer inputs (which\nyielded better performance), the computational price of\nthis deeper model increases: longer inputs implies hav-\ning larger feature maps in every layer and therefore, more\nGPU memory consumption. For that reason, we refrained\nfrom using a deep stack of 3 \u00023 ﬁlters as a front-end – be-\ncause our 12GBs of VRAM were not enough to input 15s\nof audio when using a back-end. Hence, making use of\ndomain knowledge also provides guidance for minimizing\nthe computational cost of the model – since by using a sin-\ngle layer with many vertical and horizontal ﬁlters, one can\nefﬁciently capture the same receptive ﬁeld without paying\nthe cost of going deep. Finally, note that front-ends using\nmany vertical and horizontal ﬁlters in the ﬁrst layer are an\nexample of deep architectures relying on (musical) domain\nknowledge for their design.\nAfter considering the previous discussion, we select the\nsample-level front-end as main part of our assumption-free\nmodel for waveforms; and we use a spectrogram front-end\nwith many vertical and horizontal (ﬁrst-layer) ﬁlters for the\nmodel designed considering domain knowledge. Experi-\nments below share the same back-end, which enables a fair\ncomparison among the previously selected front-ends. Un-\nless otherwise stated, the following speciﬁcations are the\nones used for the experiments – throughout the document,\nwe refer to these speciﬁcations as the basic conﬁguration:\nShared back-end. It consists of three CNN layers (with\n512 ﬁlters each and two residual connections), two pooling\nlayers and a dense layer – see Figure 2 ( Bottom-left ). We\nintroduced residual connections in our model to explore\nvery deep architectures, such that we can take advantageof the large data available. Although adding more residual\nlayers did not drastically improve our results, we observed\nthat adding these residual connections stabilized learning\nwhile slightly improving performance [16]. The used 1D-\nCNN ﬁlters [9] are computationally efﬁcient and shaped\nsuch that all extracted features are considered across a rea-\nsonable amount of temporal context (note the 7 \u0002M’ ﬁlter\nshapes, representing time\u0002all features ). We also make a\ndrastic use of temporal pooling: ﬁrstly, down-sampling x2\nthe temporal dimensionality of the feature maps; and sec-\nondly, by making use of global pooling with mean and max\nstatistics. The global pooling strategy allows for variable\nlength inputs to the network and therefore, such a model\ncan be classiﬁed as a “variable-length input” back-end. Fi-\nnally, a dense layer with 500 units connects the pooled fea-\ntures to a sigmoidal output.\nWaveform front-end. It is based on a sample-level\nfront-end [14] composed of seven: 1D-CNN (3 \u00021 ﬁlters),\nbatch norm, and max pool layers – see Figure 2 ( Top-left ).\nEach layer has 64, 64, 64, 128, 128, 128 and 256 ﬁlters.\nFor the 1.2M-songs dataset, we use a model with more ca-\npacity having nine layers with 64, 64, 64, 128, 128, 128,\n128, 128, 256 ﬁlters. By hierarchically combining small-\ncontext representations and making use of max pooling,\nthe sample-level front-end yields a feature map for an au-\ndio segment of 15s (down-sampled to 16kHz) which is fur-\nther processed by the previously described back-end.\nSpectrogram front-end. Firstly, audio segments are\nconverted to log-mel magnitude spectrograms (15 seconds\nand 96 mel bins [17]) and normalized to have zero-mean\nand unit-var. Secondly, we use vertical and horizontal ﬁl-\nters explicitly designed to facilitate learning the timbral\nand temporal patterns present in spectrograms [19–21].\nNote in Figure 2 ( Right ) that the spectrogram front-end\nis a single-layer CNN with many ﬁlter shapes that are\ngrouped into two branches [19]: (i) top branch – tim-\nbral features [21]; and (ii)lower branch – temporal fea-\ntures [20]. The top branch is designed to capture pitch-\ninvariant timbral features that are occurring at different\ntime-frequency scales in the spectrogram. Pitch invariance\nis enforced via enabling CNN ﬁlters to convolve through\nthe frequency domain, and via max-pooling the feature\nmap across its vertical axis [21]. Note that several ﬁl-\nter shapes are used to efﬁciently capture many different\ntime-frequency patterns: 7 \u000286, 3\u000286, 1\u000286, 7\u000238, 3\u000238Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 6391.2M-songs train ROC PR\nModels size AUC AUCp\nMSE\nBaseline 1.2M 91.61% 54.27% 0.1569\nWaveform 1M 92.50 %61.20 % 0.1465\nSpectrogram 1M 92.17% 59.92% 0.1473\nWaveform 500k 91.16% 56.42% 0.1504\nSpectrogram 500k 91.61% 58.18% 0.1493\nWaveform 100k 90.27% 52.76% 0.1554\nSpectrogram 100k 90.14% 52.67% 0.1542\nTable 1 . 1.2M-songs average results (3 runs) when using\ndifferent training-set sizes. Baseline: GBTs+features [22].\nand 1\u0002385– to facilitate learning, e.g.: kick-drums (with\nsmall-rectangular ﬁlters of 7 \u000238 capturing sub-band in-\nformation for a short period of time), or string ensemble\ninstruments (with long vertical ﬁlters of 1 \u000286 which are\ncapturing timbral patterns spread in the frequency axis).\nThe lower branch is meant to learn temporal features, and\nis designed to efﬁciently capture different time-scale repre-\nsentations by using several long ﬁlter shapes [20]: 165 \u00021,\n128\u00021, 64\u00021 and 32\u00021.6These ﬁlters operate over an en-\nergy envelope (not directly over the spectrogram) obtained\nvia mean-pooling the frequency-axis of the spectrogram.\nBy computing the energy envelope in that way, we are\nconsidering high and low frequencies together while min-\nimizing the computations of the model – note that no fre-\nquency/vertical convolutions are performed, but 1D (tem-\nporal) convolutions. Thus, domain knowledge is also pro-\nviding guidance to minimize the computational cost of the\nmodel. The output of these two branches is merged, and\nthe previously described back-end is used for going deeper.\nFor further details, see its online implementation.4\nParameters. 50% dropout before every dense layer,\nReLUs as non-linearities, and our models are trained\nwith SGD employing Adam (with an initial learning rate\nof 0.001) as optimizer. We minimize the MSE for the\n1.2M-songs dataset, but we minimize the cross entropy for\nthe other datasets. During training our data are converted\nto audio patches of 15s, but during prediction one aims to\nconsider the whole song. To this end, several predictions\nare computed for a song (by a moving window of 15s) and\nthen averaged. Although our models are capable of pre-\ndicting tags for variable-length inputs, we use ﬁxed length\npatches since in preliminary experiments we observed that\npredicting the whole song at once yielded worse results\nthan averaging several patch predictions. In future work\nwe aim to further study this behavior, to ﬁnd ways to ex-\nploit the fact that the whole song is generally available.\n5. EXPERIMENTAL RESULTS\n5.1 1.2M-songs dataset\nExperimental setup. As a baseline, we use a system\nconsisting of a music feature extractor (in essence: tim-\nbre, rhythm, and harmony descriptors) and a model based\non gradient boosted trees (GBT) for predicting each of the\ntags [22]. By predicting each tag individually, one aims\n5Each ﬁlter shape has 16, 32, 64, 16, 32 and 64 ﬁlters, respectively.\n6Each ﬁlter shape has 16, 32, 64 and 128 ﬁlters, respectively.\nFigure 3 . Linear regression ﬁt on the 1.2M-songs results.\nto turn a hard problem into multiple (hopefully simpler )\nproblems. A careful inspection of the dataset reveals that,\namong tags, two different data distributions dominate the\nannotations: (i)tags with bi-modal distributions, where\nmost of the annotations are zero, which can be classiﬁed;\nand(ii)tags with pseudo-uniform distributions that can be\nregressed.7A regression tag example is acoustic , which in-\ndicates how acoustic a song is – from zero to one, zero\nbeing an electronic music song and one a string quartet.\nAnd a classiﬁcation tag example can be any genre – for\nexample, most songs will not be cataloged as rapsince\nthe dataset is large and its taxonomy contains dozens of\ngenres. We use two sets of performance measurements:\nROC-AUC8and PR-AUC8for the classiﬁcation tags, and\nerror (p\nMSE8) for the regression tags. ROC-AUC can\nlead to over-optimistic scores in cases where data are un-\nbalanced [7]; given that classiﬁcation tags are highly un-\nbalanced, we also consider the PR-AUC metric since it is\nmore indicative than ROC-AUC in these cases [7]. For\nROC-AUC and PR-AUC, the higher the score the better –\nbut forp\nMSE , the lower the better. Studied spectrogram\nand waveform models are set following the basic conﬁgu-\nration – and are composed of 5.9M and 5.5M parameters,\nrespectively. Given the unprecedented size of the dataset,\nwe focus on how these models scale when trained with dif-\nferent amounts of data: 100k, 500k, or 1M songs. Average\nresults (across 3 runs) are shown in Table 1 and Figure 3.\nQuantitative results. Training the models with 100k\nsongs took a few days, with 500k songs one week, and\nwith 1M songs less than two weeks. The deep learning\nmodels trained with 1M tracks achieve better results than\nthe baseline in every metric. However, the deep learning\nmodels trained with 100k tracks perform worse than the\nbaseline. This result conﬁrms that deep learning models\nrequire large datasets to clearly outperform strong methods\nbased on feature-design – although note that large datasets\nare generally not available for most audio tasks. Moreover,\nthe biggest performance improvement w.r.t. the baseline is\nseen for PR-AUC, which provides a more informative pic-\nture of the performance when the dataset is unbalanced [7].\nIn addition, the best performing model is based on the\nwaveform front-end – being capable of outperforming the\nspectrogram model in every metric when trained with 1M\nsongs. This result conﬁrms that waveform sample-level\nfront-ends have a great potential to learn from large data,\nsince their solution space is not constrained by any se-\n7Note that all output nodes are sigmoidal – i.e., we treat classiﬁcation\ntags as regression tags for simplicity’s sake.\n8ROC: Receiver Operating Characteristic. PR: Precision Recall.\nAUC: Area Under the Curve. MSE: Mean Squared Error.640 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018vere architectural choice. On the other hand, the architec-\ntural choices deﬁning the spectrogram front-end might be\nconstraining the solution space. While these architectural\nconstraints are not harmful when training data are scarce\n(as for the 100k/500k songs results or in prior works [24]),\nsuch a strong regularization of the solution space may limit\nthe learning capacity of the model in scenarios where large\ntraining data are available – as for the 1M songs results.\nOne can observe this in Figure 3, where we ﬁt linear mod-\nels to the obtained results to further study this behavior.\nWhen 100k training songs are available: trend lines show\nthat spectrogram models tend to perform better. However,\nwhen 1M training songs are available: the lines show that\nwaveform models outperform the spectrogram ones. It is\nworth mentioning that the observed trends are consistent\nthroughout metrics: ROC-AUC, PR-AUC, andp\nMSE .\nFinally, note that there is room for improving the models\nunder study – e.g.: one could address the data imbalance\nproblem during training, or improve the back-end via ex-\nploring alternative temporal aggregation strategies.\nQualitative results. Since it is the ﬁrst report of a deep\nmusic tagging model trained with such a large dataset, we\nalso perceptually assess the quality of the estimates. To this\nend, we compared the predictions of one of our best per-\nforming models to the predictions of the baseline, and to\nthe human-annotated ground-truth tags. Some interesting\nexamples identiﬁed during this qualitative experiment are\navailable online.9First, we observed that the deep learning\nmodel is biased towards predicting the popular tags (such\naslead vocals ,English ormale vocals ). Note that this is\nexpected since we are not addressing the data unbalanc-\ning issue during training. And second, we observe that the\nbaseline model (which predicts the probability of each tag\nwith an independent GBT model) predicts mutually exclu-\nsive tags with high conﬁdence – e.g., it predicted with high\nscores: East Coast andWest Coast for an East Cost rap\nsong, or baroque period andclassic period for a Bach aria.\nHowever, the deep learning model (predicting the proba-\nbility of all tags together) was able to better differentiate\nthese similar but mutually exclusive tags. This suggests\nthat deep learning has an advantage when compared to tra-\nditional approaches, since these mutually exclusive rela-\ntions can be jointly encoded within the model.\n5.2 MagnaTagATune (MTT) dataset\nExperimental setup. State-of-the-art models are set\nas baselines, and we use the same (classiﬁcation) per-\nformance metrics as for the 1.2M-songs dataset: ROC-\nAUC and PR-AUC – note that the MTT labels are binary.\nOne of the baseline results (the SampleCNN [14] with\n90.55 ROC-AUC) was computed using a slightly different\nversion of the MTT dataset – which only includes songs\nhaving more than 1 tag and lasting more than 29.1 sec-\nonds. As a result, this cleaner version of the MTT dataset\nis of\u001921k songs instead of \u001926k. Although this dataset\ncleans out potential noisy annotations, we decided to use\nthe original dataset to easily compare our results with for-\nmer works. Thus, to fairly compare our models with\n9http://www.jordipons.me/apps/music-audio-tagging-at-scale-demothe SampleCNN, we reproduce their work considering the\noriginal dataset – achieving a score of 88.56 ROC-AUC.\nGiven that less noise is present in the SampleCNN dataset,\nit seems reasonable that their performance is higher than\nthe one obtained by our implementation.\nThe MTT experiments can be divided in two parts:\nwaveform and spectrogram models – see Tables 2 and 3.\nDue to the amenable size of the dataset (every MTT ex-\nperiment lasts <5h), it is feasible to run a comprehen-\nsive study investigating different architectural conﬁgura-\ntions. Speciﬁcally, we study how waveform and spectro-\ngram architectures behave when modifying the capacity of\ntheir front- and back-ends. For example, the experiment\n“# ﬁlters\u00021/2” in Table 2 consists of dividing the num-\nber of ﬁlters available in the waveform front-end by two.\nThis means having 32, 32, 32, 64, 64, 64 and 128 ﬁlters,\ninstead of the 64, 64, 64, 128, 128, 128 and 256 ﬁlters\nin the basic conﬁguration. We also apply this method-\nology to the spectrogram front-ends, and we add/remove\ncapacity to them by increasing/decreasing the number of\navailable ﬁlters. After running the front-end experiments\nwith a ﬁxed back-end (following the basic conﬁguration:\n512 CNN ﬁlters, 500 output units), we select the most\npromising ones to proceed with the back-end study –\nfor waveforms: “# ﬁlters \u00022”,10and for spectrograms:\n“# ﬁlters\u00021/2”. Having now a ﬁxed front-end for every\nexperiment, we modify the capacity of the back-end via\nchanging the number of ﬁlters in every CNN layer (512,\n256, 128, 64) and changing the number of output units\n(500, 200). Since the basic conﬁguration leads to relatively\nbig models for the size of the dataset, these experiments ex-\nplore smaller back-ends. The inputs for the MTT are set to\nbe of 3s, since longer inputs yield worse results [15, 21].\nQuantitative results. The waveform and spectrogram\nmodels we study outperform the proposed baselines –\nwhich represent the current state-of-the-art. Further, per-\nformance is quite robust to the number of parameters of\nthe model. Although the best results are achieved by mod-\nels having higher capacity, the performance difference be-\ntween small and large models is minor – what means that\nrelatively small models (which are easier to deploy) can do\na reasonable job when tagging the MTT music. Finally:\nspectrogram models perform better than waveform models\nfor this small public dataset – which aligns with previous\nworks using datasets of similar size [20,21]. Consequently,\nthese results conﬁrm that domain knowledge intuitions are\nvalid guides for designing deep architectures in scenarios\nwhere training data are scarce.\n5.3 Million Song Dataset (MSD)\nExperimental setup. State-of-the-art models are set\nas baselines, and we use the same (classiﬁcation) perfor-\nmance metrics as for the 1.2M-songs dataset: ROC-AUC\nand PR-AUC – note that the MSD labels are binary. These\nexperiments aim to validate the studied models with the\nbiggest public dataset available. Models are set following\nthebasic conﬁguration, and results are shown in Table 4.\n10“# ﬁlters\u00022” front-end was selected instead of “# ﬁlters \u00024”, be-\ncause it performs similarly with less parameters.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 641MTT dataset ROC PR #\nWaveform models AUC AUC param\nState-of-the-art results – with our own implementations\nSampleCNN [14]1390.55 - 2.4M\nSampleCNN (reproduced) 88.56 34.38 2.4M\nDieleman et al. [9] 84.87 - -\nDieleman et al. (reproduced) 85.58 29.59 194k\nHow much capacity is required for the front-end?\n# ﬁlters\u00024 89.05 34.92 11.8M\n# ﬁlters\u00022 (selected) 88.96 34.74 7M\n# ﬁlters\u00021 88.9 34.18 5.3M\n# ﬁlters\u00021/2 88.69 33.97 4.7M\n# ﬁlters\u00021/4 88.47 33.89 4.4M\nHow much capacity is required for the back-end?\n# ﬁlters in every CNN layer - # units in dense layer\n64 CNN ﬁlters - 500 units 88.57 33.99 1.3M\n- 200 units 88.94 34.47 1.3M\n128 CNN ﬁlters - 500 units 88.82 34.62 1.8M\n- 200 units 88.81 34.6 1.7M\n256 CNN ﬁlters - 500 units 88.95 34.27 3.1 M\n- 200 units 88.59 34.39 2.9M\n512 CNN ﬁlters - 500 units 88.96 34.74 7M\n- 200 units 88.3 34.05 6.7M\nTable 2 . MTT results: waveform models.\nQuantitative results. The spectrogram model outper-\nforms the waveform model for this public dataset – hav-\ning\u0019200k training songs. Furthermore, the spectro-\ngram model performs equivalently to ‘Multi-level & multi-\nscale’ [13], which is the best performing method in the lit-\nerature – denoting that musical knowledge can be of utility\nto design models for the MSD. Additionally, the waveform\nmodel performs worse than other waveform-based mod-\nels that also employ sample-level front-ends. Such perfor-\nmance decrease could be caused because (i)SampleCNN\nmethods [14,15] average ten11estimates for the same song\nto compensate for possible faults in song-level predictions,\nwhile our method only averages two – predicting con-\nsecutive patches of 15s; or (ii)because the major differ-\nence between SampleCNN and the waveform model is that\nthe latter employs a global pooling strategy that could re-\nmove potentially useful information for the model. Be-\nsides, the best performing waveform-based model (‘Sam-\npleCNN multi-level & multi-scale’ [15]) also achieves\nlower scores than the best performing spectrogram-based\nones. Considering the outstanding results we report when\nthe waveform model is trained with 1M songs, one could\nargue that the lack of larger public datasets is limiting\nthe outcomes of deep learning research for music auto-\ntagging – particularly when dealing with end-to-end learn-\ning stacks processing raw data such as waveforms.\n11Since MSD audios are of 30s, ten tag estimates per song can be\nobtained via running the model with consecutive patches of 3s.\n13Result computed with a different MTT version, see section 5.2.\n14Reproduced using 96 mel bands instead of 128 as in [21].MTT dataset ROC PR #\nSpectrogram models AUC AUC param\nState-of-the-art results – with our own implementations\nVGG - Choi et al. [5] 89.40 - 22M\nVGG (reproduced) 89.99 37.56 450k\nTimbre CNN [21] 89.30 - 191k\nTimbre CNN (reproduced)1489.07 34.92 220k\nHow much capacity is required for the front-end?\n# ﬁlters\u00021/8 90.08 37.18 4.4M\n# ﬁlters\u00021/4 90.12 37.69 4.6M\n# ﬁlters\u00021/2 (selected) 90.40 38.11 5M\n# ﬁlters\u00021 90.31 37.79 5.9\n# ﬁlters\u00022 90.07 37.29 7.6M\nHow much capacity is required for the back-end?\n# ﬁlters in every CNN layer - # units in dense layer\n64 CNN ﬁlters - 500 units 90.03 36.98 277k\n- 200 units 90.28 37.55 222k\n128 CNN ﬁlters - 500 units 90.16 37.61 617k\n- 200 units 90.28 37.69 524k\n256 CNN ﬁlters - 500 units 90.18 37.98 1.6M\n- 200 units 90.06 37.16 1.4M\n512 CNN ﬁlters - 500 units 90.40 38.11 5M\n- 200 units 89.98 37.05 4.7M\nTable 3 . MTT results: spectrogram models.\n6. CONCLUSIONS\nThis study presents the ﬁrst work describing how different\ndeep music auto-tagging architectures perform depending\non the amount of available training data. We also present\ntwo architectures that yield results on par with the state-\nof-the-art. These architectures are based on two concep-\ntually different design principles: one is based on a wave-\nform front-end, and no domain knowledge inspired its de-\nsign; and the other, with a spectrogram front-end, makes\nuse of (musical) domain knowledge to justify its architec-\ntural choices. While our results suggest that models rely-\ning on domain knowledge play a relevant role in scenar-\nios where no sizable datasets are available, we have shown\nthat, given enough data, assumption-free models process-\ning waveforms outperform those that rely on musical do-\nmain knowledge.\nMSD ROC PR #\nModels AUC AUC param\nWaveform (ours) 87.41 28.53 5.3M\nSampleCNN [14] 88.12 - 2.4M\nSampleCNN multi-level88.42 - -& multi-scale [15]\nSpectrogram (ours) 88.75 31.24 5.9M\nVGG + RNN [6] 86.2 - 3M\nMulti-level &88.78 - -multi-scale [13]\nTable 4 . MSD results. Top– waveform-based models.\nBottom – spectrogram-based models.642 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20187. ACKNOWLEDGMENTS\nThis work was partially supported by the Maria de Maeztu\nUnits of Excellence Programme (MDM-2015-0502) – and\nwe are grateful for the GPUs donated by NVidia.\n8. REFERENCES\n[1] Yann Bayle, Pierre Hanna, and Matthias Robine.\nRevisiting autotagging toward faultless instrumental\nplaylists generation. arXiv preprint arXiv:1706.07613 ,\n2017.\n[2] Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whit-\nman, and Paul Lamere. The million song dataset. In\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2011.\n[3] Judith C Brown. Calculation of a constant q spectral\ntransform. The Journal of the Acoustical Society of\nAmerica , 89(1):425–434, 1991.\n[4] Ning Chen and Shijun Wang. High-level music de-\nscriptor extraction algorithm based on combination of\nmulti-channel cnns and lstm. In International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2017.\n[5] Keunwoo Choi, George Fazekas, and Mark Sandler.\nAutomatic tagging using deep convolutional neural\nnetworks. International Society for Music Information\nRetrieval Conference (ISMIR) , 2016.\n[6] Keunwoo Choi, Gy ¨orgy Fazekas, Mark Sandler, and\nKyunghyun Cho. Convolutional recurrent neural net-\nworks for music classiﬁcation. In IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , 2017.\n[7] Jesse Davis and Mark Goadrich. The relationship\nbetween precision-recall and roc curves. In Inter-\nnational Conference on Machine Learning (ICML) .\nACM, 2006.\n[8] Micha ¨el Defferrard, Kirell Benzi, Pierre Van-\ndergheynst, and Xavier Bresson. Fma: A dataset for\nmusic analysis. In International Society for Music\nInformation Retrieval Conference (ISMIR) , 2017.\n[9] Sander Dieleman and Benjamin Schrauwen. End-to-\nend learning for music audio. In IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , 2014.\n[10] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman,\nAren Jansen, Wade Lawrence, R Channing Moore,\nManoj Plakal, and Marvin Ritter. Audio set: An on-\ntology and human-labeled dataset for audio events. In\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , 2017.[11] Edith Law, Kris West, Michael I Mandel, Mert Bay,\nand J Stephen Downie. Evaluation of algorithms using\ngames: The case of music tagging. In International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , 2009.\n[12] Honglak Lee, Peter Pham, Yan Largman, and An-\ndrew Y Ng. Unsupervised feature learning for au-\ndio classiﬁcation using convolutional deep belief net-\nworks. In Advances in Neural Information Processing\nSystems , 2009.\n[13] Jongpil Lee and Juhan Nam. Multi-level and multi-\nscale feature aggregation using pretrained convolu-\ntional neural networks for music auto-tagging. IEEE\nsignal processing letters , 24(8):1208–1212, 2017.\n[14] Jongpil Lee, Jiyoung Park, Keunhyoung Luke Kim,\nand Juhan Nam. Sample-level deep convolutional\nneural networks for music auto-tagging using raw\nwaveforms. Sound and Music Computing Conference\n(SMC) , 2017.\n[15] Jongpil Lee, Jiyoung Park, Keunhyoung Luke Kim,\nand Juhan Nam. Samplecnn: End-to-end deep convolu-\ntional neural networks using very small ﬁlters for mu-\nsic classiﬁcation. Applied Sciences , 8(1):150, 2018.\n[16] Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein.\nVisualizing the loss landscape of neural nets. arXiv\npreprint arXiv:1712.09913 , 2017.\n[17] Geoffroy Peeters. A large set of audio features for\nsound description (similarity and classiﬁcation) in the\ncuidado project. 2004.\n[18] Huy Phan, Lars Hertel, Marco Maass, and Alfred\nMertins. Robust audio event recognition with 1-max\npooling convolutional neural networks. arXiv preprint\narXiv:1604.06338 , 2016.\n[19] Jordi Pons, Thomas Lidy, and Xavier Serra. Experi-\nmenting with musically motivated convolutional neural\nnetworks. In 14th International Workshop on Content-\nBased Multimedia Indexing (CBMI) , pages 1–6. IEEE,\n2016.\n[20] Jordi Pons and Xavier Serra. Designing efﬁcient archi-\ntectures for modeling temporal features with convo-\nlutional neural networks. In IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , 2017.\n[21] Jordi Pons, Olga Slizovskaia, Rong Gong, Emilia\nG´omez, and Xavier Serra. Timbre analysis of music au-\ndio signals with convolutional neural networks. Euro-\npean Signal Processing Conference (EUSIPCO2017) ,\n2017.\n[22] Matthew Prockup, Andrew J. Asman, Fabian Gouyon,\nErik M. Schmidt, Oscar Celma, and Youngmoo E.\nKim. Modeling rhythm using tree ensembles and the\nmusic genome project. Machine Learning for MusicProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 643Discovery Workshop at the International Conference\non Machine Learning (ICML) , 2015.\n[23] Colin Raffel. Learning-Based Methods for Comparing\nSequences, with Applications to Audio-to-MIDI Align-\nment and Matching . PhD thesis, 2016.\n[24] Tara N Sainath, Ron J Weiss, Andrew Senior, Kevin W\nWilson, and Oriol Vinyals. Learning the speech front-\nend with raw waveform cldnns. In Sixteenth Annual\nConference of the International Speech Communica-\ntion Association , 2015.\n[25] Jan Schluter and Sebastian Bock. Improved musical\nonset detection with convolutional neural networks. In\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , 2014.\n[26] Mohamed Sordo, Cyril Laurier, and Oscar Celma. An-\nnotating music collections: How content-based simi-\nlarity helps to propagate labels. In International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , 2007.\n[27] Zhenyao Zhu, Jesse H Engel, and Awni Hannun.\nLearning multiscale features directly from waveforms.\narXiv preprint arXiv:1603.09509 , 2016.644 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Analysis by Classification: A Comparative Study of Annotated and Algorithmically Extracted Patterns in Symbolic Music Data.",
        "author": [
            "Iris Yuping Ren",
            "Anja Volk",
            "Wouter Swierstra",
            "Remco C. Veltkamp"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492471",
        "url": "https://doi.org/10.5281/zenodo.1492471",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/75_Paper.pdf",
        "abstract": "Musical patterns are salient passages that repeatedly appear in music. Such passages are vital for compression, classification and prediction tasks in MIR, and algorithms employing different techniques have been proposed to find musical patterns automatically. Human-annotated patterns have been collected and used to evaluate pattern discovery algorithms, e.g., in the Discovery of Repeated Themes  Sections MIREX task. However, state-of-the-art algorithms are not yet able to reproduce human-annotated patterns. To understand what gives rise to the discrepancy between algorithmically extracted patterns and human-annotated patterns, we use jSymbolic to extract features from patterns, visualise the feature space using PCA and perform a comparative analysis using classification techniques. We show that it is possible to classify algorithmically extracted patterns, human-annotated patterns and randomly sampled passages. This implies: (a) Algorithmically extracted patterns possess different properties than human-annotated patterns (b) Algorithmically extracted patterns have different structures than randomly sampled passages (c) Human-annotated patterns contain more information than randomly sampled passages despite subjectivity involved in the annotation process. We further discover that rhythmic features are of high importance in the classification process, which should influence future research on automatic pattern discovery.",
        "zenodo_id": 1492471,
        "dblp_key": "conf/ismir/RenVSV18",
        "keywords": [
            "Musical patterns",
            "Salient passages",
            "Repetitive motifs",
            "Automatic pattern discovery",
            "Human-annotated patterns",
            "Classification techniques",
            "Rhythmic features",
            "Pattern properties",
            "Pattern structures",
            "Information content"
        ],
        "content": "ANALYSIS BY CLASSIFICATION: A COMPARATIVE STUDY OF\nANNOTATED AND ALGORITHMICALLY EXTRACTED PATTERNS IN\nSYMBOLIC MUSIC DATA\nIris Yuping Ren\nUtrecht University\ny.ren@uu.nlAnja Volk\nUtrecht University\na.volk@uu.nlWouter Swierstra\nUtrecht University\nw.s.swierstra@uu.nlRemco C. Veltkamp\nUtrecht University\nr.c.veltkamp@uu.nl\nABSTRACT\nMusical patterns are salient passages that repeatedly ap-\npear in music. Such passages are vital for compression,\nclassiﬁcation and prediction tasks in MIR, and algorithms\nemploying different techniques have been proposed to ﬁnd\nmusical patterns automatically. Human-annotated patterns\nhave been collected and used to evaluate pattern discovery\nalgorithms, e.g., in the Discovery of Repeated Themes &\nSections MIREX task. However, state-of-the-art algorithms\nare not yet able to reproduce human-annotated patterns.\nTo understand what gives rise to the discrepancy between\nalgorithmically extracted patterns and human-annotated pat-\nterns, we use j Symbolic 2to extract features from patterns,\nvisualise the feature space using PCA and perform a compar-\native analysis using classiﬁcation techniques. We show that\nit is possible to classify algorithmically extracted patterns,\nhuman-annotated patterns and randomly sampled passages.\nThis implies: (a) Algorithmically extracted patterns possess\ndifferent properties than human-annotated patterns (b) Algo-\nrithmically extracted patterns have different structures than\nrandomly sampled passages (c) Human-annotated patterns\ncontain more information than randomly sampled passages\ndespite subjectivity involved in the annotation process. We\nfurther discover that rhythmic features are of high impor-\ntance in the classiﬁcation process, which should inﬂuence\nfuture research on automatic pattern discovery.\n1. INTRODUCTION\nPatterns occur in many dimensions of life: we constantly\nlook for patterns to classify and predict based on our ex-\nperience [40]. In music, composers employ patterns to\ninduce structures to their music [14]; listeners look for pat-\nterns while they listen attentively [16, 19]; performers learn\npatterns to better memorise, perform and improvise [39];\nmusicologists use patterns as evidence for categorisation\nand theorisation [1, 23]. In this paper, we work mainly with\nrepeated patterns which characterise and categorise folk\nsongs.\nc\rIris Yuping Ren, Anja V olk, Wouter Swierstra, Remco C.\nVeltkamp. Licensed under a Creative Commons Attribution 4.0 Interna-\ntional License (CC BY 4.0). Attribution: Iris Yuping Ren, Anja V olk,\nWouter Swierstra, Remco C. Veltkamp. “Analysis by classiﬁcation: A\ncomparative study of annotated and algorithmically extracted patterns in\nsymbolic music data”, 19th International Society for Music Information\nRetrieval Conference, Paris, France, 2018.Because of the many potential applications of musical\npatterns, algorithms that can automatically identify patterns\nare useful in many contexts. Automatic pattern discovery\nis an active research area in which many different meth-\nods have been developed, such as string-based approaches\n[5, 8, 17, 21, 22, 32], geometric approaches [4, 7, 29, 41],\ndata mining approaches [6, 36], and machine learning ap-\nproaches [34, 46].\nOne open question is how one should evaluate the qual-\nity of algorithmically extracted patterns. One common\napproach is to compare the extracted patterns with human-\nannotated patterns [2,11,15]. However, because of the afore-\nmentioned versatile application possibilities and diverse\ndeﬁnitions of musical patterns, we face several challenges\nusing human-annotated patterns to evaluate the algorithms.\nFirst, there is a lack of human-annotated pattern datasets in\ngeneral [37]. Second, subjectivity and irreducible human\nerrors could be introduced in the annotation process [27].\nThird, it is not straightforward to see what metrics one\nshould compute to compare the human-annotated patterns\nwith automatically extracted patterns.\nPrevious research has addressed these challenges to a\ncertain extent. Historically, algorithms have been tested\non unassociated datasets with disparate metrics [15]. One\nattempt to standardise the evaluation of algorithms is the\nMIREX Discovery of Repeated Themes & Sections task\ninitiated in 2014. In the task, a pattern is deﬁned as a\nset of time-pitch pairs that occurs at least twice in a piece\nof music and the JKU-PDD dataset was introduced [11].\nAccording to the evaluation metrics in this task, the state-\nof-the-art algorithms perform acceptably well in precision,\nrecall, and F1-scores, although they cannot reproduce the\nhuman-annotated patterns yet. Another pattern annotation\ndataset which has been used for evaluating the algorithms\nis the MTC -ANN Dutch Folk Song dataset [43]: human-\nannotations have been compared with algorithmically ex-\ntracted patterns by their performance in a classiﬁcation\ntask [2] showing the annotated patterns perform better. Fur-\nthermore, a large disagreement between annotated and com-\nputationally extracted patterns has been shown in both the\nJKU-PDD and MTC -ANN dataset in [37].\nThe aim of this paper is to identify and analyse the dis-\ncrepancy between human annotations and algorithmically\nextracted patterns. To achieve this goal, we extract charac-\nteristic features from human-annotated and automatically\nextracted patterns, and conduct a comparative study on the539Data\nAnnotated \nPatterns\nAutomatically \nExtracted \nPatterns\nRandom \npassages\nAlgorithms\nSampling\nExperts\nFeatures\nExamining \nAlgorithms\nJsymbolic2\nPCA/Visualisation\nExamining \nAnnotations\nAnno/Alg/Ran \nCompareFigure 1 . Pipeline of our experiments. Given the music\ndata, experts annotate patterns, algorithms extract patterns,\nand we randomly sample passages in the corpus. Tasks are\nshown in rounded boxes. Diamond boxes are transformed\ndata/features. Section 2 gives a detailed description.\npattern features using classiﬁcation methods. To establish a\nbaseline, we randomly sample passages that have the same\nlengths as human annotations. By performing a ternary\nclassiﬁcation task amongst the human-annotated patterns,\nalgorithmically extracted patterns and random passages,\nwe provide evidence that they are separable by classiﬁers.\nDespite taking musical patterns out of context and only\nconsidering the local structures annotated by humans and\nextracted by algorithms, the result of the experiment shows\npreliminary implications for the future design and evalua-\ntion of pattern discovery algorithms.\nContribution Using the monophonic MTC -ANN Dutch\nFolk Song dataset [43], our main contributions are: (a)\nBy calculating features of human-annotated, automatically\nextracted and sampled passages, we summarise and visu-\nalise the distributions of patterns in the feature space using\nPrincipal Component Analysis ( PCA) (b) Our classiﬁers suc-\ncessfully discriminate between human-annotated patterns\nand algorithmically extracted patterns above random chance\nlevel, which enables us to analyse what characterises the dif-\nferences between human-annotated patterns, automatically\nextracted patterns and random passages (c) Based on the\nanalysis of features and classiﬁcation results, we propose\nseveral ways to improve pattern discovery algorithms.\nFigure 1 is the pipeline of our experiments to be detailed\nin the next section. Abbreviations such as Anno (Annota-\ntions), Ran (Random passages), and Alg(Algorithmically\nextracted patterns) are used in tables and ﬁgures.\n2. DATA PREPARATION\nWe use the MTC-ANN Dutch Folk Song dataset [43], which\ncontains an exceptionally large number of annotated pat-\nterns and is therefore suitable for a classiﬁcation experiment.\nIn this section, we examine groups of patterns, random pas-\nsages, and their features in this dataset.Algorithm #Pattern #Occurrences Incl.\n(Annotation) 153 1657 3\nSIAR 893 5576 3\nSIAP 250 3650 3\nSIAF1 822 5308 3\nVM 182 25679 3\nVM2 159 4658 3\nSC 126 355 3\nSCFP 200 724 3\nPatMinr 105663 182306 7\nME 3339951 5651956 7\nMDGP 3543940 5457210 7\nCOSIATEC 61499 99501 7\nTable 1 . Algorithms and the count of extracted pat-\nterns. Abbreviation correspondence and details are given in\nSection 2.1 . The counts of PatMinr, ME,MDGP , and COSI -\nATEC are larger by serveral magnitude because we include\na parameter sweep, while other algorithms use a parameter\nsetting preset by authors of the algorithms. A comprehen-\nsive investigation into parameter settings of algorithms is\nnot conducted in this paper.\n2.1 Pattern groups in MTC -ANN\nAnnotated patterns During the making of MTC -ANN,\nthree experts have been asked to annotate the prominent\npatterns in each song which best classify the song into one\nof 26 tune families. Tune family is a concept in ethnomusi-\ncology that groups together tunes sharing the same ancestor\nin the process of oral transmission [9]. The dataset consists\nof 360 Dutch folk songs with 1657 annotated pattern occur-\nrences. In an annotation study on what inﬂuences human\njudgements when categorising melodies belonging to the\nsame tune family, repeated patterns turned out to play the\nmost important role [45]. It is, therefore, reasonable to use\nrepeated pattern discovery algorithms on this dataset.\nPatterns from algorithms Table 1 shows the number of\nextracted patterns from state-of-the-art musical pattern dis-\ncovery algorithms that have been used and compared in pre-\nvious research [2, 37]. The count numbers for PatMinr [22],\nMotives Extractor ( ME) [32], MDGP [8], and COSIATEC [26]\ninclude different parameter settings of the algorithm and are\ntherefore several magnitudes larger than other entries. We\ndo not include these patterns because a comprehensive pa-\nrameter search of the algorithms would be out of the scope\nof this paper. For the same reason, although algorithms such\nasSIATECC ompress - TLP(SIAP ),SIATECC ompress - TLF1\n(SIAF1),SIATECC ompress - TLR (SIAR ) are not optimised\nforMTC -ANN, a parameter search is not conducted.\nWe use the seven pattern discovery algorithms and ex-\ntract the patterns from the MTC-ANN dataset using the same\nsetup as in [2, 37]. The extracted patterns from each algo-\nrithm form a subgroup under the umbrella of the extracted\npattern group. The seven algorithms were submitted to\ntheMIREX task during 2014-2017: SIATECC ompress - TLP\n(SIAP ),SIATECC ompress - TLF1(SIAF 1),SIATECC ompress\n-TLR (SIAR ) [28], VM&VM2[44], SYMCHM (SC) [35],\nand SIARCT -CFP(SIACFP ) [7].540 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Sampling random passages We compare annotated\nand extracted patterns with randomly sampled passages\nas a baseline in order to potentially support or refuse the\nsigniﬁcance of musical patterns. In more detail, taking the\nannotated patterns from MTC -ANN, random passages are\nsampled with the following procedures: for each annotated\npattern, we ﬁnd the corresponding song where the anno-\ntation appears. We then ﬁnd a random starting point and\ntake an excerpt of the same length as the pattern to con-\nstruct a candidate excerpt. Finally, we repeat the sampling\nprocedures ﬁve times to prevent accidental results.\n2.2 Compute features\nMuch work of research exists on how to design and com-\npute musical features. As we are concerned with repeated\npatterns, and there are many possibilities as to what features\nmake a pattern repetitive [42], we hence adopt a standard-\nised feature extraction process as described below.\nFeature Calculation We calculate features from the\npatterns by using a common feature extraction tool: the\njSymbolic 2toolbox in the j MIR toolset [25]. j Symbolic 2\ntakes MIDI ﬁles as input and computes 155 musically mean-\ningful features in six categories: texture, rhythm, dynamics,\npitch, melody and chords. After computing all the features\nfor all the patterns, we have a feature vector of 155 dimen-\nsions associated with each pattern. Another well-known\nfeature extraction package, the FANTASTIC toolbox [30] is\nnot used because it cannot process input of short length,\nwhich excludes valuable annotated patterns from contribut-\ning to subsequent classiﬁcation tasks.\nFeature Selection We perform a feature selection step\nand retain 63 features by ﬁrst eliminating the features which\nare constant across all patterns, such as Vibrato Prevalence,\nAverage Range of Glissandos, and so forth. Next, we elimi-\nnate the features which are not relevant to the music content\nof time and pitch, such as the dynamics features and arte-\nfacts introduced by MIDI conversion.\nPCA and Visualisation PCA is known to be a practical\npreprocessing step and visualisation tool for classiﬁcation\nproblems. PCA produces linear combinations of features\nwhich maximise variances in a given dataset and are suitable\nfor visualising differences in data.\nInFigure 2 , we plot different groups and subgroups of\npatterns in a two-dimensional1PCA embedding of the fea-\nture space. We make four cross-group comparisons to show\ntypical cases of how musical patterns distribute in the fea-\nture space spanned by the ﬁrst two components of the PCA\ndecomposition. The visualisation is generated by using the\nannotated patterns as training data to obtain the PCA em-\nbedding, then project random passages and patterns from\ndifferent algorithms onto this PCA embedding space.\nFrom the four snapshots we take from the musical pattern\nPCA feature space as shown in Figure 2 , we make several\nobservations: (1) Annotated patterns and random passages\nhave an extensive area of overlap, which makes it impos-\nsible to ﬁnd a linear classiﬁer using the ﬁrst two principal\ncomponents of the annotated patterns, which in turn makes\n1More visualisations can be found at https://goo.gl/qmyxdhit nontrivial to differentiate the two groups of patterns as\nshown in the upper left ﬁgure. (2) SIAR patterns exhibit\nvery different distribution from the annotated patterns and\nrandom passages as shown in the top right subﬁgure. No-\ntice the annotated patterns concentrate at the top left corner.\nIn this case, it is relatively easy to separate the long-tail\narea of the extracted patterns from the annotation area. By\napplying this observation and designing a ﬁltering process,\nit could substantially improve the performance of the SIAR\nalgorithm on MTC-ANN. (3) The overlap between the anno-\ntated patterns and extracted patterns is small in the bottom\nleft ﬁgure. A linear classiﬁer can be devised to separate\nthe two groups of data using the ﬁrst two principal dimen-\nsions of the annotated patterns. The extracted patterns of\ntheSCalgorithm have different features than the annotated\npatterns. (4) In the bottom right ﬁgure, we show all the het-\nerogeneous patterns as extracted by algorithms, annotated\nby humans or randomly sampled in the same PCA embed-\nding. Patterns extracted by algorithms of the same family,\nnamely SIATECC ompress - TLP(SIAP ),SIATECC ompress -\nTLF1(SIAF1),SIATECC ompress - TLR (SIAR ), and SIACFP\ntend to share the same long-tail property, and therefore their\nperformance on MTC -ANN can be improved by an extra\nﬁltering step as described above.\nIn summary, setting out from the visual examination and\nour observations above, it is promising to apply classiﬁ-\ncation techniques to discriminate the features of different\ngroups of patterns. We commence on the classiﬁcation task\nand conduct a comparative analysis using the classiﬁcation\nresults in the next section.\n3. METHOD CONFIGURATION\nIn this section, we introduce the classiﬁers and evaluation\nmetrics we use for the classiﬁcation task.\n3.1 Classiﬁcation\nSupervised classiﬁcation methods have been used exten-\nsively in MIR tasks such as genre classiﬁcation and classi-\nfying geographically different corpora. In addition, com-\nparative analyses using classiﬁcation methods have been\nperformed in many areas of research [10, 33]. To the best\nof our knowledge, using supervised classiﬁcation for con-\nducting comparative analyses have not been used with sym-\nbolic musical patterns. In this paper, we use supervised\nclassiﬁcation methods to differentiate human-annotated, al-\ngorithmically extracted and randomly sampled passages in\nMTC -ANN. By putting patterns into groups (the group of\nalgorithmically extracted patterns, the group of annotations,\nand the group of random passages) and observing whether\nthere are systematic differences on the group level, we gain\na different perspective than using the metrics based on indi-\nvidual patterns, such as the precision, recall, and F1-score\nused in MIREX .\nTo prevent the results to be classiﬁer-speciﬁc, we use a\nmixture of simple and more sophisticated, linear and non-\nlinear classiﬁers to perform the ternary classiﬁcation task.\nWe also use standard machine learning techniques to train\nand test classiﬁers: ﬁrst, scaling and centering preprocess-\ning steps are performed on all the features and PCA input;Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 541−50510\n−5 0 5 10\nPC1PC2AnnoRan\n−50510\n0 10\nPC1PC2AnnoSC−75−50−250\n0 50 100 150\nPC1PC2AnnoSIAR\n−75−50−250\n0 50 100 150\nPC1PC2Anno\nRanSC\nSIACFPSIAF1\nSIAPSIAR\nVMVM2Figure 2 . Visualisation of different groups of patterns using the space spanned by the ﬁrst two principal components of\nthe annotated pattern features. The legend denoting the colour correspondence with algorithms/annotated patterns/random\npassages is on the top of each subﬁgure. Notice that the scopes of ﬁgures in the left column are subregions of ﬁgures on\nthe right. Notes on each subﬁgure: (1) Upper left: Random passages and annotated patterns. The overlap between the two\ngroups is large, and it is nontrivial to separate them in this two-dimensional PCA embedding. (2) Upper right: SIAR patterns\nand the annotated patterns. SIAR patterns exhibit a long-tail behaviour which is not shared by the annotated patterns. (3)\nBottom left: SCpatterns and the annotated patterns. The overlap of the data points is small, which makes it easier to separate\nthe two groups in this embedding. (4) Bottom right: Random passages, annotated patterns and patterns from all algorithms.\nWe see some of the algorithmically extracted patterns are very different from the annotated patterns, and the algorithms\nbelonging to the same family exhibit the same long-tail behaviour.\nadditionally, to avoid overﬁtting, for all experiments, we\nuse a 10-fold cross-validation 3-times repetition scheme.\nThe PCA projection and parameter search of each classiﬁer\nare performed separately on each fold. The six statistical\nclassiﬁers we use are:\nGBM [13] (Gradient Boosting Machine) produces a pre-\ndiction model consisting of an ensemble of decision trees.\nThe parameters we search through are the learning rate, the\ncomplexity of trees, the minimum number of samples to\ncommence splitting and the number of iterations.\nLVQ [18] (Linear Vector Quantisation) applies a winner-\ntakes-all Hebbian learning-based approach. We search\nthrough two parameters in this classiﬁer: the codebook\nsize and the number of prototypes.\nLDA [38] (Linear Discriminant Analysis) produces a\nlinear classiﬁer which ﬁnds a linear combination of fea-\ntures that best separates different classes in datasets. This\nclassiﬁer does not contain parameters.\nNB[31] (Naive Bayes) computes the conditional a-posterior probabilities of a categorical class variable given\nindependent predictor variables using the Bayes rule. Three\nparameters are tuned for this classiﬁer: the Laplace smooth-\ning, kernel bandwidth and distribution type.\nRF[3] (Random Forest) operates by constructing a mul-\ntitude of decision tree. The parameter we consider is the\nnumber of variables per level.\nSVM [12] (Support Vector Machine) calculates a map\nfrom data to a new representation so that the data points\nof the separate categories are divided by a gap that is as\nwide as possible. We use the radial basis function kernel\nand consider two parameters: the smoothing factor and the\nweight of training examples.\nThe experiments have been performed using R. The\ntask takes about 2 hours on an i7 CPU with a maximum\nmemory usage of 2Gb. For reproducibility, the data and\ncode to replicate the experiments can be downloaded2.\n2https://github.com/irisyupingren/patdisISMIR2018542 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20180.50.60.70.8\nGBM LDA LVQ NB RF SVM\nClassifierAccuracyPCA/RAW: PCARawFigure 3 . Accuracy values of classiﬁers in thirty experi-\nments (10-fold cross-validation repeated three times) using\nsix classiﬁers with j Symbolic 2features and features after\nPCA decomposition.\nOther schemes with different parameters and with a new\ntest set split were used, too, but are omitted because they\ngive similar results to our analysis.\n3.2 Evaluation\nWe mainly use accuracy and its variance as a measure of the\nperformance of classiﬁers. To further interpret the results of\nthe classiﬁcation task, we compute confusion matrices and\nfeature importance measures. Ten other metrics for each\nclassiﬁer are provided for further inspection3. In the next\nsection, we report the most relevant results.\n4. RESULTS AND DISCUSSION\nIn this section, we ﬁrst report the model metrics of classi-\nﬁers. By comparing the metrics, we identify Random Forest\nas the best classiﬁer. Then we interpret the performance\nof the Random Forest classiﬁer using the confusion matrix.\nLast, we examine important features in our best model.\n4.1 Model metrics\nInFigure 3 , we show the accuracy and variance of different\nclassiﬁers using two groups of features: the raw features and\nfeatures after PCA decomposition. The baseline accuracy\nis1\n#group\u001833% because the number of patterns in each\ngroup is the same = 1657 , as ensured by uniform sampling.\nWe see that all the classiﬁers give a result higher than the\nbaseline accuracy. PCA improves the performance of the\nclassiﬁer NB; for three classiﬁers, LVQ,SVM and LDA, using\nPCA or raw input does not make a signiﬁcant difference\non the performance; the performance of other classiﬁers\nis worse when using the PCA input. PCA has different\ninﬂuences on the performance of classiﬁers because there\nare different internal feature transformation mechanisms in\neach classiﬁer. Overall, the random forest classiﬁer gives\nthe best results with the raw feature input and the parameter\n#variables = 32 .\nThe high accuracy and the fact that we can construct a\nclassiﬁer to differentiate the three groups of data imply that:\nﬁrst, algorithmically extracted patterns possess different\nproperties than human-annotated patterns, which suggests\nan extra consideration to features of patterns when trying\nto discover patterns automatically; second, algorithmically\n3https://goo.gl/ezuTCTOriginal!\nClassiﬁed#Alg Ran Anno\nAlg 1595(\u00067:4) 17.2(\u00064:6) 24.8(\u00068:4)\nRan 8.3(\u00062:7) 1597(\u00062:8) 5.0(\u00062:2)\nAnno 54.1(\u00069:9) 42.6(\u00062:7) 1627(\u000610:0)\nTable 2 . Confusion matrix results from the ternary classiﬁ-\ncation experiment using the Random Forest classiﬁer: mean\nand variance (in parenthesis) of ten experiments. The row\nnames indicate the patterns are classiﬁed into the group of\nthis name by the classiﬁer; the column names indicate the\npatterns are orginally from the group of this name. Three\ngroups of data are classiﬁed with high accuracies and sig-\nniﬁcant p-values\u001c0.05.\nextracted patterns have different structures than random\npassages, which means the extracted patterns cannot be re-\nplaced by sampled passages and could be more useful than\nsampled passages for various applications that employ mu-\nsical patterns; last, human-annotated patterns contain more\ninformation than randomness despite subjectivity involved\nin the annotation process, which is in agreement with the\ncarefully designed annotation acquiring process [43] and\nthe previous ﬁndings that the annotations are useful for\nclassifying tune families [2].\n4.2 Confusion Matrix\nInTable 2 , we give the confusion matrix results calculated\nfrom the classiﬁer which has the best classiﬁcation results:\nRandom Forest. We perform the repeated cross-validation\nexperiment ten times and take the average and variance\nof the resulting ten confusion matrices. The results show\nus on the individual patterns level how different groups\nof data are separable to one another. The sum of each\ncolumn is roughly 1657, which is the group size of our data.\nThe row sums do not have this constraint because we do\nnot put restrictions on the group size as determined by the\nclassiﬁer. To read the table, for example, the number 24.8\nin the right top corner of the table is the mean number of\npatterns classiﬁed as algorithmically extracted patterns but\nare actually annotations.\nWe see the classiﬁer can differentiate the three groups\nwith few misclassiﬁed instances. Although it would indicate\na good performance of the algorithms if the count in the\nconfusion matrix is larger in the algorithm pattern group and\nthe annotation pattern group, we come to the conclusion that\nthe algorithmically extracted patterns, annotated patterns\nand random passages all possess their own traits and are not\nsimilar enough for the classiﬁer to fail. This is in accordance\nwith previous research that the extracted patterns are not yet\nindistinguishable from the human annotations [2, 37]. On\nthe positive side, we establish that neither annotated patterns\nnor extracted patterns are as meaningless as random data.\n4.3 Feature Importance\nInFigure 4 , we show the individual importance value of the\nfeatures in the classiﬁcation process by using the Boruta\nalgorithm [20]. The Boruta algorithm randomly duplicatesProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 543Importance\nStrength Ratio of Two Strongest Rhythmic Pulses\nAverage Time Between Attacks\nVariability of Note Durations\nAverage Note Duration\nComplete Rests\nStrength of Second Strongest Rhythmic Pulse\nStrength of Strongest Rhythmic Pulse\nRhythmic Variability\nLongest Complete Rest\nDuration\nMean Melodic Interval\nMaximum Note Duration\nCombined Strength of Two Strongest Rhythmic Pulses\nDirection of Melodic Motion\nVariability of Time Between Attacks\nRange\nNote Density\nStepwise Motion\nInterval Between Most Prevalent Pitches\nMean Pitch\nAmount of Arpeggiation\nInterval Between Most Prevalent Pitch Classes\nMost Common Pitch Class\nPrevalence of Most Common Pitch\nMost Common Pitch\nPrevalence of Most Common Pitch Class\nHarmonicity of Two Strongest Rhythmic Pulses\nDistance Between Most Prevalent Melodic Intervals\nMelodic Pitch Variety\nMost Common Melodic Interval\nPrevalence of Most Common Melodic Interval\nMelodic Thirds\nStrongest Rhythmic Pulse\nPitch Variety\nImportance of High Register\nImportance of Middle Register\nPitch Class Variety\nAverage Interval Spanned by Melodic Arcs\nStrong Tonal Centres\nMinimum Note Duration\nPolyrhythms\nDominant Spread\nSecond Strongest Rhythmic Pulse\nChromatic Motion\nNote Density Variability\nMelodic Perfect Fourths\nRepeated Notes\nRelative Prevalence of Top Pitch Classes\nAverage Length of Melodic Arcs\nNumber of Moderate Rhythmic Pulses\nRelative Prevalence of Top Pitches\nNumber of Common Melodic Intervals\nNumber of Relatively Strong Rhythmic Pulses\nNumber of Common Pitches\nNumber of Strong Rhythmic Pulses\nMinor Major Melodic Third Ratio\nRelative Prevalence of Most Common Melodic Intervals\nMelodic Perfect Fifths\nMelodic Embellishments\nMelodic Sixths\nrandomMax\nMelodic Sevenths\nMelodic Tritones\nMelodic Octaves\nrandomMean\nrandomMinFigure 4 . Feature importance in classifying annotated patterns, extracted patterns and randomly sampled passages using a\nrandom forest classiﬁer. The boxplot shows the mean and variance (interquartile ranges) of the feature importance value [20].\nThe features are ranked by their importance. We omit the y-axis label because the absolute importance values are not relevant\nfor our analysis. The colour green indicates features that are more important than the random features and are therefore\nconﬁrmed to be important; blue entries show the performance of the random features; red and yellow indicate unimportant\nand tentative features respectively.\nand shufﬂes the values in the original features. The algo-\nrithm then employs the random features together with the\noriginal features in classiﬁcation tasks. During the classiﬁ-\ncation process, the algorithm calculates and compares the\nMean Decrease Impurity importance value [24].\nAlthough we have 23 rhythmic features out of 63 fea-\ntures in total, all top ten most important features are rhyth-\nmic features. This suggests that these rhythmic features are\nrelatively more important than other features in constructing\nthe random forest classiﬁer. The prominent features give\nhints on potential improvements to current existing pattern\ndiscovery algorithms. String-based and data mining algo-\nrithms translate pitch and duration pairs into a list of sym-\nbols and do not take into account metric structures imposed\nby musical punctuations such as bar lines and measures.\nOther known algorithms also seldom explicitly consider\nmetric features in patterns. The feature importance values\nsend the message that, in designing and evaluating pattern\ndiscovery algorithms, at least for the MTC-ANN dataset, we\nshould take metric structures into considerations as well as\nthe repetitions and pitch related features in the patterns.\nIn addition, the importance of other j Symbolic 2features\nis conﬁrmed with the exception of three features which\nperformed worse or at the same level as random features,\nas shown in Figure 4 . For example, the Melodic Octaves\nfeature is conﬁrmed to be unimportant and the MelodicSevenths and Melodic Tritones feature are marked to be a\ntentative attribute. They are unessential features because\nsuch intervals rarely happen in the MTC -ANN dataset.\n5. CONCLUSIONS AND FUTURE WORK\nWe visualised and successfully classiﬁed human-annotated\npatterns, algorithmically extracted patterns and random pas-\nsages in MTC-ANN. An analysis of the classiﬁcation results\nsuggests that the automatically extracted patterns are not\nyet indistinguishable from the human-annotated patterns,\nand both extracted and annotated patterns show different\ntraits than randomly sampled passages. Using classiﬁca-\ntion methods for comparative analysis of pattern groups\nprovides a new perspective on examining the output of pat-\ntern discovery algorithms than the comparison of individual\npatterns in the MIREX task. In this way, we discover that\nrhythmic features play an important role in distinguishing\nthe groups of patterns in MTC -ANN.\nFuture research needs to consider different contexts of\npatterns, such as within a melody, within a tune family and\nwithin the corpus, in order to investigate the inﬂuence of\nthe context on what establishes a musical pattern. Expand-\ning our research to other datasets once pattern annotations\nbecome available will allow us to verify whether the impor-\ntance of rhythmic features is speciﬁc to MTC -ANN.544 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20186. REFERENCES\n[1]Koﬁ Agawu. Music as discourse: Semiotic adventures\nin romantic music . Oxford University Press, 2014.\n[2]Peter Boot, Anja V olk, and W. Bas de Haas. Evaluating\nthe role of repeated patterns in folk song classiﬁca-\ntion and compression. Journal of New Music Research ,\n45(3):223–238, 2016.\n[3]Leo Breiman. Random forests. Machine learning ,\n45(1):5–32, 2001.\n[4]Chantal Buteau and Guerino Mazzola. Motivic anal-\nysis according to Rudolph Reti: Formalization by a\ntopological model. Journal of Mathematics and Music ,\n2(3):117–134, 2008.\n[5]Emilios Cambouropoulos. Musical parallelism and\nmelodic segmentation. Music Perception , 23(3):249–\n268, 2006.\n[6]Tsung-Ping Chen and Li Su. Discovery of repeated\nthemes and sections with pattern clustering. Music Infor-\nmation Retrieval Evaluation eXchange (MIREX 2017) ,\n2017.\n[7]Tom Collins, Andreas Arzt, Sebastian Flossmann, and\nGerhard Widmer. SIARCT-CFP: Improving precision\nand the discovery of inexact musical patterns in point-\nset representations. Proceedings of the 14th Interna-\ntional Society for Music Information Retrieval Confer-\nence, pages 549–554, 2013.\n[8]Darrell Conklin. Discovery of distinctive patterns in\nmusic. Intelligent Data Analysis , 14(5):547–554, 2010.\n[9]James R. Cowdery. A fresh look at the concept of tune\nfamily. Ethnomusicology , 28(3):495–504, 1984.\n[10] Luciano da Fontoura Costa and Roberto Marcondes\nCesar Jr. Shape classiﬁcation and analysis: theory and\npractice . CRC Press, Inc., 2009.\n[11] Music Information Retrieval Evaluation eXchange\n(MIREX) 2013. Discovery of Repeated Themes &\nSections. http://www.musicir.org/mirex/\nwiki/2013 .\n[12] V ojtech Franc and V ´aclav Hlav ´ac. Multi-class support\nvector machine. In Proceedings of the 16th Interna-\ntional Conference on Pattern Recognition , volume 2,\npages 236–239. IEEE, 2002.\n[13] Jerome H. Friedman. Greedy function approximation:\na gradient boosting machine. Annals of statistics , pages\n1189–1232, 2001.\n[14] Jia-Lien Hsu, Arbee LP Chen, and C-C Liu. Efﬁcient\nrepeating pattern ﬁnding in music databases. In Proceed-\nings of the 7th International Conference on Information\nand Knowledge Management , pages 281–288. ACM,\n1998.[15] Berit Janssen, W. Bas de Haas, Anja V olk, and Peter van\nKranenburg. Finding repeated patterns in music: State\nof knowledge, challenges, perspectives. Proceedings of\nthe 10th International Symposium on Computer Music\nModeling and Retrieval , pages 277–297, 2013.\n[16] Mari Riess Jones. Dynamic pattern structure in mu-\nsic: Recent theory and research. Perception & psy-\nchophysics , 41(6):621–634, 1987.\n[17] Ian Knopke and Frauke J ¨urgensen. A system for identi-\nfying common melodic phrases in the masses of Palest-\nrina. Journal of New Music Research , 38(2):171–181,\n2009.\n[18] Teuvo Kohonen. Improved versions of learning vector\nquantization. In Proceedings of the 3rd International\nJoint Conference on Neural Networks , pages 545–550.\nIEEE, 1990.\n[19] Gerhard Kubik. Pattern perception and recognition in\nafrican music. The performing arts: music and dance ,\npages 221–49, 1979.\n[20] Miron B. Kursa and Witold R. Rudnicki. Feature se-\nlection with the boruta package. Journal of Statistical\nSoftware , 36(11):1–13, 2010.\n[21] Olivier Lartillot. Multi-dimensional motivic pattern ex-\ntraction founded on adaptive redundancy ﬁltering. Jour-\nnal of New Music Research , 34(4):375–393, 2005.\n[22] Olivier Lartillot. PatMinr: In-depth motivic analysis of\nsymbolic monophonic sequences. Music Information\nRetrieval Evaluation eXchange (MIREX 2014) , 2014.\n[23] Fred Lerdahl and Ray S. Jackendoff. A generative the-\nory of tonal music . MIT press, 1985.\n[24] Gilles Louppe, Louis Wehenkel, Antonio Sutera, and\nPierre Geurts. Understanding variable importances in\nforests of randomized trees. In Advances in neural in-\nformation processing systems , pages 431–439, 2013.\n[25] Cory McKay. Automatic Music Classiﬁcation with jMIR .\nPhD thesis, McGill University, 2010.\n[26] David Meredith. COSIATEC and SIATECCompress:\nPattern discovery by geometric compression. Music\nInformation Retrieval Evaluation Exchange (MIREX\n2013) , 2013.\n[27] David Meredith. Music analysis and point-set compres-\nsion. Journal of New Music Research , 44(3):245–270,\n2015.\n[28] David Meredith. Using SIATECCompress to discover\nrepeated themes and sections in polyphonic music. Mu-\nsic Information Retrieval Evaluation Exchange (MIREX\n2016) , 2016.\n[29] David Meredith, Kjell Lemstr ¨om, and Geraint A. Wig-\ngins. Algorithms for discovering repeated patterns in\nmultidimensional representations of polyphonic music.\nJournal of New Music Research , 31(4):321–345, 2002.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 545[30] Daniel M ¨ullensiefen. Fantastic: Feature analysis tech-\nnology accessing statistics (in a corpus): Technical re-\nport. Goldsmiths University of London , 2009.\n[31] Andrew Y Ng and Michael I Jordan. On discriminative\nvs. generative classiﬁers: A comparison of logistic re-\ngression and naive bayes. In Proceedings of the 15th\nAdvances in Neural Information Processing Systems ,\npages 841–848, 2002.\n[32] Oriol Nieto and Morwaread M. Farbood. Identifying\npolyphonic patterns from audio recordings using mu-\nsic segmentation techniques. Proceedings of the 15th\nInternational Society for Music Information Retrieval\nConference , pages 411–416, 2014.\n[33] Sandeep Patel and A James Barkovich. Analysis and\nclassiﬁcation of cerebellar malformations. American\nJournal of Neuroradiology , 23(7):1074–1087, 2002.\n[34] Matev ˇz Pesek, Ale ˇs Leonardis, and Matija Marolt. Sym-\nchman unsupervised approach for pattern discovery\nin symbolic music with a compositional hierarchical\nmodel. Applied Sciences , 7(11):1135, 2017.\n[35] Matevz Pesek, Ur ˇsa Medve ˇsek, Ale ˇs Leonardis, and\nMatija Marolt. Symchm: a compositional hierarchical\nmodel for pattern discovery in symbolic music repre-\nsentations. 11th Annual Music Information Retrieval\neXchange (MIREX15). Malaga , pages 1–3, 2015.\n[36] Iris Yuping Ren. Closed patterns in folk music and other\ngenres. Proceedings of the 6th International Workshop\non Folk Music Analysis , pages 56–58, 2016.\n[37] Iris Yuping Ren, Hendrik Vincent Koops, Anja V olk,\nand Wouter Swierstra. In search of the consensus among\nmusical pattern discovery algorithms. Proceedings of\nthe 18th International Society for Music Information\nRetrieval , pages 671–680, 2017.\n[38] Brian D Ripley. Pattern recognition and neural net-\nworks . Cambridge university press, 2007.\n[39] Grace Rubin-Rabson. Studies in the psychology of\nmemorizing piano music: A comparison of the whole\nand the part approach. Journal of Educational Psychol-\nogy, 31(6):460, 1940.\n[40] Herbert A. Simon and Richard K. Sumner. Machine\nmodels of music. chapter Pattern in Music, pages 83–\n110. MIT Press, Cambridge, MA, USA, 1992.\n[41] Wai Man Szeto and Man Hon Wong. A graph-\ntheoretical approach for pattern matching in post-\ntonal music analysis. Journal of New Music Research ,\n35(4):307–321, 2006.\n[42] Jan Van Balen. Audio description and corpus analysis\nof popular music . PhD thesis, Utrecht University, 2016.\n[43] Peter van Kranenburg, Berit Janssen, and Anja V olk.\nThe Meertens Tune Collections: The Annotated Corpus\n(MTC-ANN) versions 1.1 and 2.0.1. Meertens Online\nReports , 2016(1), 2016.[44] Gissel Velarde and David Meredith. A wavelet-based\napproach to the discovery of themes and sections in\nmonophonic melodies. Music Information Retrieval\nEvaluation Exchange (MIREX 2014) , 2014.\n[45] Anja V olk and Peter Van Kranenburg. Melodic sim-\nilarity among folk songs: An annotation study on\nsimilarity-based categorization in music. Musicæ Scien-\ntiæ, 16(3):317–339, 2012.\n[46] Cheng-i Wang, Jennifer Hsu, and Shlomo Dubnov. Mu-\nsic pattern discovery with variable markov oracle: A\nuniﬁed approach to symbolic and audio representations.\nInProceedings of the 16th International Society for Mu-\nsic Information Retrieval Conference , pages 176–182,\n2015.546 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "An End-to-end Framework for Audio-to-Score Music Transcription on Monophonic Excerpts.",
        "author": [
            "Miguel A. Román",
            "Antonio Pertusa",
            "Jorge Calvo-Zaragoza"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492337",
        "url": "https://doi.org/10.5281/zenodo.1492337",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/87_Paper.pdf",
        "abstract": "In this work, we present an end-to-end framework for audio-to-score transcription. To the best of our knowledge, this is the first automatic music transcription approach which obtains directly a symbolic score from audio, instead of performing separate stages for piano-roll estimation (pitch detection and note tracking), meter detection or key estimation. The proposed method is based on a Convolutional Recurrent Neural Network architecture directly trained with pairs of spectrograms and their corresponding symbolic scores in Western notation. Unlike standard pitch estimation methods, the proposed architecture does not need the music symbols to be aligned with their audio frames thanks to a Connectionist Temporal Classification loss function. Training and evaluation were performed using a large dataset of short monophonic scores (incipits) from the RISM collection, that were synthesized to get the ground-truth data. Although there is still room for improvement, most musical symbols were correctly detected and the evaluation results validate the proposed approach. We believe that this end-to-end framework opens new avenues for automatic music transcription.",
        "zenodo_id": 1492337,
        "dblp_key": "conf/ismir/RomanPC18",
        "keywords": [
            "end-to-end",
            "audio-to-score",
            "transcription",
            "Convolutional Recurrent Neural Network",
            "spectrograms",
            "symbolic score",
            "Western notation",
            "Connectionist Temporal Classification",
            "RISM collection",
            "incipits"
        ],
        "content": "AN END-TO-END FRAMEWORK FOR AUDIO-TO-SCORE MUSIC\nTRANSCRIPTION ON MONOPHONIC EXCERPTS\nMiguel A. Rom ´an\nU.I. for Computing Research\nUniversity of Alicante\nAlicante, Spain\nmroman@dlsi.ua.esAntonio Pertusa\nU.I. for Computing Research\nUniversity of Alicante\nAlicante, Spain\npertusa@ua.esJorge Calvo-Zaragoza\nPRHLT Research Center\nUniversitat Polit `ecnica de Val `encia\nValencia, Spain\njcalvo@prhlt.upv.es\nABSTRACT\nIn this work, we present an end-to-end framework for\naudio-to-score transcription. To the best of our knowl-\nedge, this is the ﬁrst automatic music transcription ap-\nproach which obtains directly a symbolic score from audio,\ninstead of performing separate stages for piano-roll estima-\ntion (pitch detection and note tracking), meter detection or\nkey estimation. The proposed method is based on a Con-\nvolutional Recurrent Neural Network architecture directly\ntrained with pairs of spectrograms and their correspond-\ning symbolic scores in Western notation. Unlike standard\npitch estimation methods, the proposed architecture does\nnot need the music symbols to be aligned with their au-\ndio frames thanks to a Connectionist Temporal Classiﬁca-\ntion loss function. Training and evaluation were performed\nusing a large dataset of short monophonic scores (incip-\nits) from the RISM collection, that were synthesized to get\nthe ground-truth data. Although there is still room for im-\nprovement, most musical symbols were correctly detected\nand the evaluation results validate the proposed approach.\nWe believe that this end-to-end framework opens new av-\nenues for automatic music transcription.\n1. INTRODUCTION\nAutomatic Music Transcription (AMT) is a very relevant\nﬁeld within the Music Information Retrieval (MIR) com-\nmunity. This task can be deﬁned as the automated pro-\ncess of converting an audio recording into any kind of\nmusically-meaningful structured format. The usefulness of\nthis process is very broad, especially for MIR algorithms\nsuch as content-based music search, symbolic music simi-\nlarity, or symbolic musicological analysis.\nHowever, this is a challenging task and state-of-the-art\nmethods currently obtain a performance signiﬁcantly be-\nlow a human expert. In order to obtain a complete score\nc\rMiguel A. Rom ´an, Antonio Pertusa, Jorge Calvo-\nZaragoza. Licensed under a Creative Commons Attribution 4.0 Inter-\nnational License (CC BY 4.0). Attribution: Miguel A. Rom ´an, An-\ntonio Pertusa, Jorge Calvo-Zaragoza. “An End-to-End framework for\nAudio-to-Score Music Transcription on monophonic excerpts”, 19th In-\nternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.from a waveform, it is necessary to perform pitch detec-\ntion, note onset/offset detection, loudness estimation and\nquantization, instrument recognition, extraction of rhyth-\nmic information, and time quantization [2].\nMost music transcription systems focus on two of these\nstages: pitch detection, where pitches at each time frame\nof the audio are estimated, and note tracking [32], where\nthe estimations of the previous step are discretized into\nsequences of 3-tuples (onset, offset, pitch). The output\nin this case is a piano-roll, that is, a two-dimensional\nrepresentation of notes across time [2]. Multiple pitch\nestimation techniques include spectrogram factorization\nmethods [1, 3, 28] and discriminative approaches, which\nperform frame-by-frame pitch estimation using statistical\nmodels [10], signal processing methods [23, 35], or ma-\nchine learning techniques [4] including deep neural net-\nworks [17,27,30]. Some works also integrate musical lan-\nguage models into the pitch estimation process to resolve\noutput ambiguities [27, 34].\nSupervised learning approaches for piano-roll estima-\ntion require the ground truth to be aligned for training.\nMatching pitches frame by frame with their corresponding\nwaveform samples is a time-consuming task and, although\nthere are some efforts in this direction with datasets such as\nMAPS [10], RWC [11] or MusicNet [29], currently there\nare no very large AMT corpora. Beyond the difﬁculty of\nperforming an accurate annotation, frame-by-frame esti-\nmation has some additional issues to be taken into account.\nFor example, when a whole note is played using a plucked\nstring instrument such as a guitar, the quick decay of its\nharmonic amplitudes produces frames with a very low in-\ntensity at the end of the note, causing ambiguities when\nlabeling the offset frames.\nIn addition, as pointed out in [2], AMT algorithms are\nusually developed independently to carry out individual\ntasks such as multiple pitch detection, beat tracking and in-\nstrument recognition. Some existing AMT methods, such\nas the ones proposed in [19–21], also include rhythm esti-\nmation and time quantization. Still, the challenge remains\nto combine the outputs of the individual tasks to perform\njoint estimation of all parameters, in order to avoid the cas-\ncading of errors when algorithms are combined sequen-\ntially.\nIn this work we intend to open a new framework to ad-\ndress the AMT task. Our proposal is to consider end-to-34end machine learning strategies, with which this task can\nbe carried out holistically. In other words, we aim at us-\ning a waveform as input, and directly obtaining a music\nscore at the output taking into account all its components\n(pitches, note durations, time signature, key signature, etc.)\njointly.\nThe task of directly estimating a symbolic score from\naudio is certainly different from that of estimating a piano-\nroll. While piano-roll estimation aims to extract what has\nbeen played from the audio as exact as possible, in the\nscore transcription task the goal is to obtain a symbolic rep-\nresentation from what the musician read, which includes\nabstracting away some information such as loudness.\nFor this, we address score estimation using Deep Neural\nNetworks. We speciﬁcally consider the use of a Convolu-\ntional Recurrent Neural Network, which is responsible of\nboth processing the input spectrogram to extract meaning-\nful features and predict an output sequence that represents\nthe music contained in a given audio recording. Thanks\nto the Connectionist Temporal Classiﬁcation (CTC) loss\nfunction, this kind of networks can be trained in terms\nof pairs (input, output), without needing of dividing the\nprocess into smaller stages or providing framewise annota-\ntions. The idea is that the prediction is forced to be encoded\nin terms of actual music-notation elements.\nIt is important to emphasize that the objective of this\nwork is not to outperform the accuracy of previous ap-\nproaches, but to propose a framework with which to ad-\ndress the AMT task. In order to demonstrate the feasibil-\nity of this formulation, our experiments are restricted to a\nconstrained scenario, using audio recordings from mono-\nphonic scores that were synthesized using a piano. We are\naware that the main challenge in AMT is to deal with poly-\nphonic real music. In a future work we plan to extend the\nproposed approach to detect polyphonic scores, although\nits effectiveness with sound mixtures is yet to be studied.\nThe evaluation results in this constrained scenario val-\nidates the proposed framework and show that the the pro-\nposed approach obtains reliable results, correctly detecting\nmost musical symbols.\nThe rest of the paper is organized as follows: the corpus\nused for evaluation is described in Section 2; the holistic\nneural framework proposed for the AMT task is described\nin Section 3; the series of experiments carried out are de-\ntailed in Section 4; and ﬁnally, the conclusions of the cur-\nrent work are summarized in Section 5, pointing out some\ninteresting avenues for future work as well.\n2. DATASET\nIn order to get the ground truth for our framework, we\nused the RISM1collection [26], which currently contains\nmore than one million incipits (short monophonic music\nexcerpts). This corpus is very useful for music retrieval\ntasks because of its size and the fact that it contains real\nmusic written by human composers [31]. Spectrograms\nfrom synthesized incipits are the inputs to our method, and\n1The complete set of RISM incipits can be downloaded from https:\n//opac.rism.info/index.php?id=8&L=1&id=8\nRISM dataset\n(PAE)Audio synthesisSymbolic EncodingSTFTEnd-to-end\nCRNN FrameworkCorpusPreprocessingTrainingyxFigure 1 : Data acquisition for training. RISM incipits are\nconverted into our music notation format and magnitude\nspectrograms (Short-Time Fourier Transform, STFT) are\nalso computed from synthesized versions of the incipits.\nThe inputs of the proposed framework ( x) are the symbolic\ndata and the outputs are the spectrograms ( y). Frame-by-\nframe alignment is not necessary.\ntheir corresponding symbolic scores are the outputs. The\nscheme of the proposed method can be seen in Figure 1.\n2.1 Preprocessing\nRISM incipits are formatted in Plaine & Easie Code (PAE).\nWe randomly selected a subset of 71,400 incipits in West-\nern notation and converted them into the music notation\nformat that can be seen in Table 1, where each symbol is\nencoded using a single character. This notation is oriented\nto represent the music as a language, similarly to what a\nspeech recognition system does. Following this analogy,\nwe consider a music note as a word (for example, C]4 \u0007 \u0010)\ncontaining several characters from an alphabet set \u0006that\ncan be seen in Table 1, and which is separated to other\nwords by blank spaces. Rests are represented in the same\nway, with a word consisting of the rest symbol and its du-\nration. In addition to notes and rests, the alphabet set in-\ncludes clefs, key and time signatures, measure bars and\nnote ties. Every musical symbol in Table 1 is encoded for\nour framework using a single element (one character).\nIn order to get the audio ﬁles, the RISM PAE incipits\nwere converted into Music Encoding Initiative (MEI) for-\nmat, and then translated again into MIDI using Meico2,\nwhich unlike Verovio3takes into account the key signa-\nture.\nThe synthesis from MIDI ﬁles was performed using\ntimidity with the piano program of the default soundfont,\nobtaining monoaural audio ﬁles at 16kHz. Then, mag-\nnitude spectrograms were calculated using a 64ms (1024\nsamples) Hamming window with a 16ms hop (256 sam-\nples). All incipits were synthesized using random tempo\nvalues in the range [96-144] bpm in order to make the net-\nwork work with different speeds.\n3. FRAMEWORK\nWe describe in this section the neural model that allows\nus to face the AMT task directly from an audio signal to a\nsequence of meaningful symbols.\n2https://github.com/cemfi/meico\n3http://www.verovio.org/index.xhtmlProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 35Class Symbol Count Histogram\nGlobal Blank 1,526,051\nClef G2 39,337\nF4 4,414\nC1 22,468\nC3 1,981\nC4 3,200\nKey D [M 112\nA[M 1,065\nE[M 6,815\nB[M 8,950\nFM 11,599\nCM 15,488\nGM 10,309\nDM 10,861\nAM 4,933\nEM 1,216\nBM 52\nPitch A 87,323\nB 88,004\nC 95,190\nD 100,014\nE 80,780\nF 75,579\nG 84,953\n[ 70,557\n] 55,471\nRest 89,635\nOctave 2 2,937\n3 44,170\n4 274,590\n5 284,081\n6 6,065\nDuration \t 8,686\n\b \u001052,541\n\u0007 \u0010172,933\n\u0007 \u0010( 226,395\n\u0007 \u0010) 72,124\n\u0007 \u0010* 6,711\n\u0018 72,453\nTie 10,393\nTime 4/4 27,855\n2/2 13,848\n3/4 11,595\n2/4 7,569\n6/8 4,950\n3/8 2,916\n3/2 1,199\n12/8 592\n6/4 417\n4/2 305\n9/8 154\nBarline 245,239\nTable 1 : Symbols of the alphabet \u0006. Notes are encoded\nusing words of three to ﬁve symbols (for example, C]4 \u0007 \u0010\u0018).\n$qcF]4ˇ“(.G 4ˇ“)|A 4ˇ“a4ˇ“(.A 4ˇ“)A4ˇ“(D5ˇ“(C]5ˇ“(B4ˇ“(|A 4ˇ“A4ˇ“)B4ˇ“)A4ˇ“)G4ˇ“)F]4ˇ“.A 4ˇ“(|G 4ˇ“(.F]4ˇ“)G4ˇ“(A4ˇ“(F]4ˇ“(.E 4ˇ“)F]4ˇ“(G4ˇ“(\n1$qcF]4ˇ“(.G 4ˇ“)|A 4ˇ“a4ˇ“(.A 4ˇ“)A4ˇ“(D5ˇ“(C]5ˇ“(B4ˇ“(|A 4ˇ“A4ˇ“)B4ˇ“)A4ˇ“)G4ˇ“)F]4ˇ“.A 4ˇ“(|G 4ˇ“(.F]4ˇ“)G4ˇ“(A4ˇ“(F]4ˇ“(.E 4ˇ“)F]4ˇ“(G4ˇ“(\n1Figure 2 : Example of a magnitude spectrogram ( x) syn-\nthesized from a RISM incipit (center). The symbolic en-\ncoding representation used for the CRNN ( y) is shown be-\nlow, where the character ‘$’ is the G2 clef, ‘q’ is the key\nsignature DM, the symbol ‘c’ is used to encode 4/4 and\n‘|’ represents the barline. Similarly to speech recognition,\nwords are separated by blank spaces.\nFormally, letX=f(x1; y1);(x2; y2); :::gbe our end-\nto-end application domain, where xiis an audio recording\nrepresented by its magnitude spectrogram, and yidenotes\nits corresponding ground-truth sequence from a ﬁxed al-\nphabet set \u0006.\nThe problem of AMT can be reformulated as retriev-\ning the most likely sequence of symbols ^ygiven an input\nspectrogram x. That is:\n^y= arg max\ny2\u0006\u0003P(yjx) (1)\nWe formulate this statistical framework by means of Re-\ncurrent Neural Networks (RNN), as they allow handling\nsequences [12]. Ultimately, therefore, the RNN will be re-\nsponsible of producing the sequence of musical symbols\nthat fulﬁlls Eq. 1. Nevertheless, on top of it, we add a Con-\nvolutional Neural Network (CNN), which learns how to\nprocess the input signal to represent it in a meaningful way\nfor the task at issue [36]. Since both types of networks\nconsist of feed-forward operations, the training stage can\nbe carried out jointly by simply connecting the output of\nthe last layer of the CNN with the input of the ﬁrst layer of\nthe RNN, which leads to a Convolutional Recurrent Neu-\nral Network (CRNN). A similar topology was previously\napplied to drum transcription in [33], although not in an\nend-to-end fashion.\nOur work is conducted over a supervised learning sce-\nnario. Therefore, it is assumed that we can make use of\na setT \u001aX with which to train the model. Initially, the\ntraditional training mechanism for a CRNN needs to be\nprovided with the expected output for each frame of the in-\nput. As introduced above, for each recording the training\nset only contains its corresponding sequence of expected36 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018symbols, without any kind of explicit information about\ntheir location within the input signal. This scenario can be\nsolved by means of the so-called Connectionist Temporal\nClassiﬁcation (CTC) loss function [13].\nGiven an input x, CTC provides a means to optimize the\nCRNN parameters in order to directly output its correct se-\nquence y. In other works, CTC directly optimizes P(yjx).\nSince the ground-truth is not aligned at the frame level,\nthat is, it is unknown the alignment between the frames of\nthe recurrent part and the output symbols, CTC integrates\nover all possible alignments. It only considers monotonic\nalignments (left-to-right constraint), which is a valid as-\nsumption in our task.\nAlthough optimizing the aforementioned probability\nis computationally expensive, CTC performs a local op-\ntimization using an Expectation-Maximization algorithm\nsimilar to that used for training Hidden Markov Models\n[24]. However, given that CTC integrates over all possible\nalignments, its main limitation is that the cost of the op-\ntimization procedure grows rapidly with the length of the\nsequences.\nNote that CTC is used only for training. At the in-\nference stage, the CRNN still predicts a symbol for each\nframe of the recurrent block. To indicate a separation be-\ntween symbols, or to handle those frames in which there\nis no symbol, CTC considers an additional symbol in the\nalphabet that indicates this situation ( blank symbol).\n3.1 Implementation details\nFinding the best instantiation of a CRNN for the case of\nAMT is out of the scope of this work, but we are inspired\nby the Deep Speech 2 [8] topology, which was especially\ndesigned for the task of Automatic Speech Recognition\n(ASR). Although ASR and AMT are different tasks they\nare related, and so the use of this architecture allows us to\nobtain valuable results without having to make an exhaus-\ntive search of the best neural topology.\nNonetheless, we made small modiﬁcations to the orig-\ninal architecture in order to adjust its behavior to AMT.\nThe speciﬁcation of our neural topology is detailed in\nTable 2. It consists of 2convolutional layers and 3recur-\nrent layers. Convolutional layers are composed of convo-\nlutional ﬁlters followed by Batch Normalization [16], and\nthe non-linear hard hyperbolic tangent (HardTanh) activa-\ntion function [14]. Furthermore, bi-directional recurrent\nlayers are conﬁgured as Gated Recurrent Units (GRU) [7],\nwith Batch Normalization as well. On top of the last recur-\nrent output, a fully-connected layer is placed with as many\nneurons as symbols of the vocabulary (plus 1, because of\ntheblank symbol). The use of the softmax activation al-\nlows us to interpret the output of this last layer as a poste-\nrior probability over the vocabulary [6].\nThe training stage is carried out by providing pairs of\nspectrograms with their corresponding unaligned sequence\nof musical symbols. The optimization procedure follows\nstochastic gradient descent (SGD) [5] with Nesterov mo-\nmentum of 0:9, gradient L2 Norm clipping of 400, and a\nmini-batch size of 20samples, which modiﬁes the networkInput (1024\u0002T)\nConvolutional block\nConv (32;41\u000211;2\u00022), BatchNorm(), HardTanh()\nConv (32;21\u000211;2\u00021), BatchNorm(), HardTanh()\nRecurrent block\nB-GRU (1024) , BatchNorm()\nB-GRU (1024) , BatchNorm()\nB-GRU (1024) , BatchNorm()\nDense (j\u0006j+ 1) , Softmax ()\nTable 2 : Instantiation of the CRNN used in this work for\naudio-to-score AMT, consisting of 2convolutional layers\nand3recurrent layers. Notation: Input (h\u0002w)means an\ninput spectrogram of height hand width w; Conv (n; kh\u0002\nkw; sh\u0002sw)denotes a convolution operator of nﬁlters,\nkernel size of kh\u0002kw, and stride of sh\u0002sw; BatchNorm()\ndenotes a batch normalization procedure; HardTanh() rep-\nresents the hard hyperbolic tangent activation; B-GRU (n)\nmeans a bi-directional Gated Recurrent Units of nneurons;\nDense (n)denotes a fully-connected layer of nneurons;\nandSoftmax ()represents the softmax activation function.\n\u0006denotes the character-wise alphabet considered.\nweights to minimize the CTC loss function through back-\npropagation. The learning rate was initially set to 0:0003 ,\nbut it was annealed by a factor of 1:1after each epoch to fa-\nvor convergence. The model was trained during 20epochs,\nﬁxing the weights according to the best result over the val-\nidation set.\nOnce the CRNN is trained with the previous procedure,\nit can be used to output a discrete symbol sequence from a\ngiven spectrogram. The model yields character-level pre-\ndictions in each frame. In order to provide an actual sym-\nbol sequence, it is necessary to both collapse repeating\ncharacters and discarding blank characters. Since there\ncould be several frame-level sequences that result in the\nsame sequence of musical symbols, the ﬁnal decoding is\nconducted by a beam search procedure [37], with a beam\nwidth set to 10.\n4. EXPERIMENTS\n4.1 Setup\nThe proposed framework is evaluated using the corpus de-\nscribed in Section 2.1.\nExperiments are performed dividing the available data\ninto three independent partitions: 49;980samples ( 118:03\nhours) for training, 10;710samples ( 25:34hours) for val-\nidation, and 10;710samples ( 25:36hours) for the test set,\nwhich is used to evaluate the actual performance.\nGiven the differences with existing AMT approaches,\nour results are not directly comparable with any previous\nwork. Likewise, there are no standard evaluation metrics\nwith which to evaluate this framework.\nHere, we propose a series of metrics especially consid-\nered for evaluating the presented approach. In particular,Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 37we are inspired by other tasks, like ASR or Optical Charac-\nter Recognition (OCR), that are also formulated expecting\na sequence of symbols as output. Analogously to these\ntasks, we also assume that the output consists of individual\ncharacters (pitches, durations, alterations, ...) that build\ncomplete words (such as notes). Therefore, the perfor-\nmance can be evaluated in terms of Character Error Rate\n(CER) and Word Error Rate (WER). These metrics are de-\nﬁned as the number of elementary editing operations (in-\nsertion, deletion, or substitution) to convert the hypotheses\nof the system into the ground-truth sequences, at the char-\nacter and word level, respectively. They compute this cost\nin a normalized way according to the length of the ground-\ntruth sequences. Even assuming that these metrics are not\noptimal for the task of AMT, we hope that they allow us\nto validate the approach and draw reasonable conclusions\nfrom our experimental results.\nIn order to get some baseline results that can be com-\npared to other works, we also applied the evaluation me-\ntric used in [19] for piano-roll alignment tasks. The total\nnumber of notes in the ground truth is denoted by NGT,\nthat of estimated notes by Nest. The number of notes with\npitch errors is denoted by Np, that of extra notes by Ne,\nand that of missing notes by Nm. The number of matched\nnotes is deﬁned as Nmatch =NGT\u0000Nm=Nest\u0000Ne.\nThen we deﬁne the pitch error rate as Ep=Np=NGT, ex-\ntra note rate as Ee=Ne=Nest, and missing note rate as\nEm=Nm=NGT. Onset/offsets errors are also reported\nin [19]. As we are dealing with note durations instead of\nonsets/offsets, we include an alternative error metric Ed\nwhich is calculated similarly to the pitch error Epbut us-\ning note duration errors, denoted by Nd. Thus, we deﬁne\nthe duration error rate as Ed=Nd=NGT.\n4.2 Results\nFigure 3 shows the evolution of the errors during the train-\ning process. As can be seen, the convergence is fast and\nthe best results on the validation set are obtained at epoch\n18, reporting a CER of 5.53 and a WER of 15.98. In the\ntest set, a CER of 5.36 and a WER of 15.67 are obtained.\nThese results are very similar to those from the validation\nset, thus proving that there is no over-ﬁtting and the model\ngeneralizes well.\nAfter an in-depth analysis of the test set transcriptions\nobtained, we observed that the majority of errors are due\nto wrong time signatures, barline locations, and clefs. This\nresult was expected in our prior analysis, as even for a hu-\nman it would be difﬁcult to identify them based on the\nshort audio excerpts we provide to our model (the average\nnumber of music measures of the audio excerpts is 4:4).\nFurthermore, there are some time signatures that contain\nthe same number of notes per measure and therefore they\nrequire more musical context to identify them correctly\n(e.g. 4/4 and 2/2 time signatures), as shown in Figure 4.\nIn other cases, one of these speciﬁc errors causes the ap-\npearance of many others, as seem to happen with the time\nsignature in the example of Figure 5. In order to address\nthese ambiguities, normalization techniques could be em-\nFigure 3 : Evolution curves of the CTC loss, CER, and\nWER over the validation set with respect to the training\nepoch. The lowest WER (15.98) and CER (5.53) ﬁgures\nare obtained at epoch 18.\n(a) Original score.\n(b) Transcribed score.\nFigure 4 : Example of transcription performance. Note that\nthe two mistakes made (clef and time signature) belong to\nmusic notation ambiguities.\nployed (for instance, changing all 2/2 by their equivalent\nnotation in 4/4).\nIn spite of all these difﬁculties, some samples are per-\nfectly recognized, as the one depicted in Figure 6.\nWe provide the results of the evaluation metric proposed\nin [19] for estimated notes, and for estimated notes and\nrests combined (in this case, Epdoes not change). As\ncan be seen in Table 3, the error rates are quite low com-\npared to [19], but this is due to the fact that our audio ﬁles\nare monophonic and synthesized. In addition, most tran-\nscription errors are due to wrong estimations of time sig-\nnatures, subsequently yielding wrong barline locations as\npreviously explained.\n5. CONCLUSIONS\nIn this work, we propose a new formulation of AMT in the\nform of an audio-to-score task. In summary, the advan-\ntages of this formulation over piano-roll estimation are:\n1) it is not required to have a frame-by-frame annotation\naligned with the audio, therefore potentially more data\nTable 3 : Note pitch error rate ( Ep), missing symbol rate\n(Em), extra symbol rate ( Ee) and symbol duration error\nrate (Ed) considering only notes and notes plus rests.\nEp Em Ee Ed\nNotes 0.99% 2.63% 1.81% 0.71%\nNotes+Rests 0.99% 4.94% 2.51% 1.23%38 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018(a) Original score.\n(b) Transcribed score.\nFigure 5 : Example of a transcription with several mis-\ntakes. Here, the unusual time signature 3/2 (wrongly de-\ntected) propagates the errors to the notes.\nFigure 6 : Example of a correctly transcribed score.\ncould be acquired for training; 2) the obtained outputs are\nmusically meaningful; 3) the frame-by-frame annotation\nambiguities are avoided, although on the other hand there\nare music notation ambiguities to deal with; 4) the task\nis addressed holistically instead of using a pipeline of in-\ndividual processes, avoiding the cascading of errors when\nthey are combined sequentially, and 5) musical models are\nimplicitly inferred as it occurs with language models in\nspeech recognition.\nWe validated the proposed framework using a CRNN\nwith a CTC loss function trained on RISM incipits, cor-\nrectly predicting around 84% of symbols for monophonic\nscores synthesized with a piano sound at different tempos.\nIt is important to note that some symbols such as barlines,\nrests, ties, time signatures or key signatures were no ex-\nplicitly present in spectrograms but they were correctly in-\nferred from the context.\nA qualitative analysis of the performance reported that\nmany errors occurred because of music notation ambigui-\nties. Although they decreased the WER and CER ﬁgures,\n”wrong” outputs are musically correct and equivalent to\nthe ground-truth scores in most cases.\nAs a future work, we are planning ﬁrst to extend it for\npolyphonic sources, and also to perform instrument recog-\nnition. In order to deal with polyphony, a chord could be\nconsidered as a “word” containing “syllabus” (the individ-\nual notes), for example: C4 \b \u0010E4 \b \u0010G4 \b \u0010. An additional sym-\nbol could be added to indicate the instrument (for example,\nPC4 \u0007 \u0010could represent a quarter note of pitch C4 played on\nPiano).\nAs pointed out in [18], previous experiments on deep\nneural networks dealing with framewise multiple pitch de-\ntection showed that unseen combinations are hard to de-\ntect. A partial solution to this problem might involve a\nmodiﬁcation of the loss function for the network to disen-\ntangle individual notes explicitly and learn to decompose\na (nonlinear) mixture of signals into its constituent parts.\nWe believe that, unlike what happens in this framewise de-\ntection, CTC loss may be able to break the observed glass-\nceiling, given that ASR methods using this architecture are\ncapable of generalizing to detect unseen words from its\nconstituent (character) elements. Nonetheless, additional\nexperiments on AMT should conﬁrm this hypothesis.Synthesized scores were used to perform the experi-\nments, although ideally real data should be evaluated. For\nthis, we are planning to use datasets such as Lakh [25],\nwhich contains audio ﬁles with their corresponding MIDIs.\nGiven the computational cost of CTC, the proposed frame-\nwork needs to use short segments. Therefore, it is neces-\nsary to have aligned barlines to split both the audio and the\ncorresponding score ground truth into smaller pieces. This\ncould be done using a score following method [9,22]. This\npreprocessing could introduce some errors due to wrong\nalignments, but there is a more suitable alternative: to\ntrain the CRNN using full scores along with their complete\nreal audio ﬁles, which is the ultimate goal of the proposed\nframework. This is possible and could be done by con-\nsidering the recently proposed online CTC [15] function,\nwhich efﬁciently adapts to any sequence length.\nAnother obvious future work is to ﬁnd a more ade-\nquate network architecture and evaluate alternative hyper-\nparameters to increase the accuracy. CNN and RNN\ntopologies evaluated in previous AMT works [17, 27]\nshould be investigated for this task.\nIn conclusion, in this work we show that it is feasible\nto perform end-to-end transcription from monophonic au-\ndio ﬁles to scores. We are fully aware that experiments\nwere made in a very controlled and simpliﬁed environ-\nment and there is still much work to do in order to per-\nform a complete transcription. But we believe that the pro-\nposed framework opens a new exciting research area given\nthe huge amount of data that could potentially be used for\ntraining, and its practical utility for musicians who could\nobtain directly a score from audio.\n6. ACKNOWLEDGEMENT\nThis work was supported by the University of Alicante\nthrough grant GRE-16-04 and its University Institute for\nComputing Research (IUII), and by the Spanish Ministe-\nrio de Econom ´ıa, Industria y Competitividad through His-\npaMus project (TIN2017-86576-R) and Juan de la Cierva\n- Formaci ´on grant (Ref. FJCI-2016-27873).\n7. REFERENCES\n[1] E. Benetos and S. Dixon. A shift-invariant latent vari-\nable model for automatic music transcription. Com-\nputer Music Journal , 36(4):81–94, 2012.\n[2] E. Benetos, S. Dixon, D. Giannoulis, H. Kirchhoff,\nand A. Klapuri. Automatic Music Transcription: Chal-\nlenges and Future Directions. Journal of Intelligent In-\nformation Systems , 41(3):407–434, 2013.\n[3] E. Benetos and T. Weyde. An efﬁcient temporally-\nconstrained probabilistic model for multiple-\ninstrument music transcription. In 16th International\nConference on Music Information Retrieval , pages\n701–707, 2015.\n[4] S. B ¨ock and M. Schedl. Polyphonic piano note tran-\nscription with recurrent neural networks. In IEEE In-Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 39ternational Conference on Acoustics, Speech and Sig-\nnal Processing , pages 121–124, 2012.\n[5] L. Bottou. Large-scale machine learning with stochas-\ntic gradient descent. In Proceedings of COMP-\nSTAT’2010 , pages 177–186. Springer, 2010.\n[6] H. Bourlard and C. Wellekens. Links Between Markov\nModels and Multilayer Perceptrons. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence ,\n12(11):1167–1178, 1990.\n[7] K. Cho, B. van Merrienboer, D. Bahdanau, and Y .\nBengio. On the properties of neural machine transla-\ntion: Encoder-decoder approaches. In Proceedings of\nthe Eighth Workshop on Syntax, Semantic sand Struc-\nture in Statistical Translation , pages 103–111, 2014.\n[8] D. Amodei et al. Deep Speech 2 : End-to-End Speech\nRecognition in English and Mandarin. In 33nd Inter-\nnational Conference on Machine Learning , pages 173–\n182, 2016.\n[9] R. B. Dannenberg and C. Raphael. Music score align-\nment and computer accompaniment. Communications\nof the ACM , 49(8):38–43, 2006.\n[10] V . Emiya, R. Badeau, and B. David. Multipitch estima-\ntion of piano sounds using a new probabilistic spectral\nsmoothness principle. IEEE Trans. on Audio, Speech,\nand Language Processing , 18(6):1643–1654, 2010.\n[11] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC Music Database: Popular, Classical and Jazz\nMusic Databases. In 3rd International Conference on\nMusic Information Retrieval , pages 287–288, 2002.\n[12] A. Graves. Supervised Sequence Labelling with recur-\nrent neural networks . PhD thesis, Technical University\nMunich, 2008.\n[13] A. Graves, S. Fern ´andez, F. Gomez, and J. Schmidhu-\nber. Connectionist Temporal Classiﬁcation: Labelling\nUnsegmented Sequence Data with Recurrent Neural\nNetworks. In 23rd International Conference on Ma-\nchine Learning , International Conference on Machine\nLearning, pages 369–376. ACM, 2006.\n[14] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv,\nand Y . Bengio. Binarized neural networks. In Advances\nin Neural Information Processing Systems 29 , pages\n4107–4115, 2016.\n[15] K. Hwang and W. Sung. Online Sequence Training of\nRecurrent Neural Networks with Connectionist Tem-\nporal Classiﬁcation. CoRR , abs/1511.06841, 2015.\n[16] S. Ioffe and C. Szegedy. Batch normalization: accel-\nerating deep network training by reducing internal co-\nvariate shift. In Proceedings of the 32nd International\nConference on Machine Learning, ICML 2015, Lille,\nFrance, 6-11 July , pages 448–456, 2015.[17] R. Kelz, M. Dorfer, F. Korzeniowski, S. B ¨ock, A. Arzt,\nand G. Widmer. On the Potential of Simple Frame-\nwise Approaches to Piano Transcription. In 17th Inter-\nnational Conference on Music Information Retrieval ,\n2016.\n[18] R. Kelz and G. Widmer. An experimental analy-\nsis of the entanglement problem in neural-network-\nbased music transcription systems. arXiv preprint\narXiv:1702.00025 , 2017.\n[19] E. Nakamura, E. Benetos, K. Yoshii, and S. Dixon.\nTowards complete polyphonic music transcription: In-\ntegrating multi-pitch detection and rhythm quantiza-\ntion. In IEEE International Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP) . IEEE, 2018.\n[20] E. Nakamura, K. Yoshii, and S. Dixon. Note Value\nRecognition for Piano Transcription Using Markov\nRandom Fields. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing , 25:1542–1554,\n2017.\n[21] E. Nakamura, K. Yoshii, and S. Sagayama. Rhythm\nTranscription of Polyphonic Piano Music Based on\nMerged-Output HMM for Multiple V oices. IEEE/ACM\nTransactions on Audio, Speech, and Language Pro-\ncessing , 25:794–806, 2017.\n[22] N. Orio, S. Lemouton, and D. Schwarz. Score follow-\ning: State of the art and new developments. In Proc.\nof the 2003 Conference on New interfaces for Musical\nExpression , pages 36–41. National University of Sin-\ngapore, 2003.\n[23] A. Pertusa and J. M. I ˜nesta. Efﬁcient methods for joint\nestimation of multiple fundamental frequencies in mu-\nsic signals. EURASIP Journal on Advances in Signal\nProcessing , 2012(1):27, Feb 2012.\n[24] L. Rabiner and B.-H. Juang. Fundamentals of speech\nrecognition . Prentice hall, 1993.\n[25] C. Raffel. Learning-Based Methods for Comparing Se-\nquences, with Applications to Audio-to-MIDI Align-\nment and Matching . PhD thesis, Columbia University,\n2016.\n[26] RISM. R ´epertoire International des Sources Musicales,\n2017.\n[27] S. Sigtia, E. Benetos, and S. Dixon. An End-to-end\nNeural Network for polyphonic piano music transcrip-\ntion. IEEE Transactions on Audio, Speech and Lan-\nguage Processing , 24(5):927–939, 2016.\n[28] P. Smaragdis and J. C. Brown. Non-negative Matrix\nFactorization for Polyphonic Music Transcription. In\nIEEE Workshop on Applications of Signal Processing\nto Audio and Acoustics , pages 177–180, 2003.\n[29] J. Thickstun, Z. Harchaoui, and S. Kakade. Learning\nfeatures of music from scratch. In International Con-\nference on Learning Representations (ICLR) , 2017.40 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[30] J. Thickstun, Z. Harchaoui, D. Foster, and S. M.\nKakade. Invariances and data augmentation for\nsupervised music transcription. arXiv preprint\narXiv:1711.04845 , 2017.\n[31] R. Typke, M. Den Hoed, J. De Nooijer, F. Wiering,\nand R. C. Veltkamp. A ground truth for half a million\nmusical incipits. Journal of Digital Information Man-\nagement , pages 34–39, 2005.\n[32] J. J. Valero-Mas, E. Benetos, and J. M. I ˜nesta. A super-\nvised classiﬁcation approach for note tracking in poly-\nphonic piano transcription. Journal of New Music Re-\nsearch , pages 1–15, 2018.\n[33] R. V ogl, M. Dorfer, G. Widmer, and P. Knees. Drum\ntranscription via joint beat and drum modeling using\nconvolutional recurrent neural networks. In 18th Inter-\nnational Conference on Music Information Retrieval ,\npages 150–157, 2017.\n[34] Q. Wang, R. Zhou, and Y . Yan. Polyphonic Piano Tran-\nscription with a Note-Based Music Language Model.\nApplied Sciences , 8(3), 2018.\n[35] C. Yeh, A. Robel, and X. Rodet. Multiple fundamental\nfrequency estimation of polyphonic music signals. In\nIEEE International Conference on Acoustics, Speech,\nand Signal Processing , volume 3. IEEE, 2005.\n[36] M. D. Zeiler and R. Fergus. Visualizing and Un-\nderstanding Convolutional Networks. In 13th Euro-\npean Conference on Computer Vision , pages 818–833,\nSeptember 2014.\n[37] T. Zenkel, R. Sanabria, F. Metze, J. Niehues, M. Sper-\nber, S. St ¨uker, and A. Waibel. Comparison of decod-\ning strategies for CTC acoustic models. In 18th Annual\nConference of the International Speech Communica-\ntion Association , pages 513–517, 2017.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 41"
    },
    {
        "title": "Disambiguating Music Artists at Scale with Audio Metric Learning.",
        "author": [
            "Jimena Royo-Letelier",
            "Romain Hennequin",
            "Viet-Anh Tran",
            "Manuel Moussallam"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492493",
        "url": "https://doi.org/10.5281/zenodo.1492493",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/211_Paper.pdf",
        "abstract": "We address the problem of disambiguating large scale catalogs through the definition of an unknown artist clustering task. We explore the use of metric learning techniques to learn artist embeddings directly from audio, and using a dedicated homonym artists dataset, we compare our method with a recent approach that learn similar embeddings using artist classifiers. While both systems have the ability to disambiguate unknown artists relying exclusively on audio, we show that our system is more suitable in the case when enough audio data is available for each artist in the train dataset. We also propose a new negative sampling method for metric learning that takes advantage of side information such as music genre during the learning phase and shows promising results for the artist clustering task.",
        "zenodo_id": 1492493,
        "dblp_key": "conf/ismir/Royo-LetelierHT18",
        "keywords": [
            "disambiguating large scale catalogs",
            "unknown artist clustering task",
            "metric learning techniques",
            "artist embeddings",
            "homonym artists dataset",
            "recent approach",
            "audio data",
            "artist classifiers",
            "negative sampling method",
            "music genre"
        ],
        "content": "DISAMBIGUATING MUSIC ARTISTS AT SCALE WITH AUDIO METRIC\nLEARNING\nJimena Royo-Letelier Romain Hennequin Viet-Anh Tran Manuel Moussallam\nDeezer, 12 rue d’Ath `enes, 75009 Paris, France\nresearch@deezer.com\nABSTRACT\nWe address the problem of disambiguating large scale\ncatalogs through the deﬁnition of an unknown artist clus-\ntering task. We explore the use of metric learning tech-\nniques to learn artist embeddings directly from audio, and\nusing a dedicated homonym artists dataset, we compare\nour method with a recent approach that learn similar em-\nbeddings using artist classiﬁers. While both systems have\nthe ability to disambiguate unknown artists relying exclu-\nsively on audio, we show that our system is more suitable\nin the case when enough audio data is available for each\nartist in the train dataset. We also propose a new negative\nsampling method for metric learning that takes advantage\nof side information such as music genre during the learning\nphase and shows promising results for the artist clustering\ntask.\n1. INTRODUCTION\n1.1 Motivation\nWith contemporary on-line music catalogs typically\nproposing dozens of millions of recordings, a major prob-\nlem is the lack of an universal and reliable mean to identify\nmusic artists. Contrarily to albums’ and tracks’ ISRC1,\nand despite some initiative such as ISNI2, there exist no\nunique standardized identiﬁer for artists in the industry. As\na direct consequence, the name of an artist remains its de-\nfacto identiﬁer in practice although it results in common\nambiguity issues. For example, name artist collisions (e.g.\nBill Evans is the name of a jazz pianist but also the name\nof a jazz saxophonist and the name of a blackgrass banjo\nplayer) or artist aliases (e.g. Youssou N’Dour vs.Youssou\nNdour ,Simon & Garfunkel vsPaul Simon and Art Gar-\nfunkel ,Cat Stevens vsYusuf Islam ) are usual. Relying on\nhuman resources to clean or verify all artists in the database\nis practically impossible, although a major issue resulting\nfrom these ambiguities is the difﬁculty to correctly credit\n1http://isrc.ifpi.org\n2http://www.isni.org\nc\rFirst author, Second author, Third author, Fourth author.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: First author, Second author, Third\nauthor, Fourth author. “Disambiguating Music Artists at Scale with Au-\ndio Metric Learning”, 19th International Society for Music Information\nRetrieval Conference, Paris, France, 2018.artists for their work and the confusion that may arise for\nend users while exploring catalogs.\nAutomatically distinguishing between artists is a com-\nplicated task, even for human specialists, since there is no\none to one relation between a track and an artist. Tracks\ncan be performed by several artists (e.g. duets andfeatur-\nings). Albums may contain tracks from different artists\n(e.g. in compilations ). Even artist denominations may\ndrastically evolve during their careers. In this work, our\ngoal is, given a set of recordings, to ﬁnd a partition of this\nset for which all tracks from a given subset are associated\nto the same artist.\nIn the MIR literature, problems dealing with artist assig-\nnation from a recording are most of the time addressed as a\nclassiﬁcation problem [3, 9, 10], where a set of predeﬁned\nartists is known. This is not a real case scenario for evolv-\ning large music catalogs, since the number of artists can\nbe huge (several millions) and new artists are added ev-\nery day. In this paper, we propose a new task of unknown\nartists clustering from audio, without having any ground\ntruth data about the identities of the artist nor a prior in-\nformation about the number of different artists present in\na cluster. To the best of our knowledge, there are no prior\nwork addressing this. Disambiguating homonym artists is\na practical application of this task, where a set of tracks\nmust be split into an unknown number of clusters, each\ncorresponding to a different artist entity. We believe that\naccurately solving this task could results in a major im-\nprovement in the quality of large sized catalogs.\nTo tackle this new task, we propose to use metric learn-\ning methods to train a system that outputs artist embed-\ndings from audio. Indeed, as we will explain later, metric\nlearning objective function is primarily designed to ensure\nthat embeddings of samples from the same entity are clus-\ntered together. Metric learning also offers the interesting\npossibility of controlling what an embedding system learns\nby means of the sampling necessary to feed its loss. Here\nwe also suggest to leverage musical relationships among\naudio tracks as source of information to strengthen the rep-\nresentation learning, allowing to incorporate music side\ninformation -such as genre, mood or release date- to the\ntraining process.\n1.2 Overview\nThe paper is organized as follows. In Section 2 we expose\nhow this paper relates to prior work. In Section 3, we detail\nthe metric learning system used to learn artist embeddings622from audio. In Section 4, we introduce the newly proposed\nartist disambiguation task and the datasets used for exper-\niments. In Section 5, we show our results and compare\nto the previous systems. Finally, we draw conclusions in\nSection 6.\n2. PREVIOUS WORKS\nIn this paper we propose a method to learn artist embed-\ndings from audio. This approach falls in a general cate-\ngory of methods in identity disambiguation problems that\ntry to learn a parametric map from content samples (for in-\nstance face pictures [18] or speaker speech recordings [5])\nto a metric space, so that same identity samples are closely\nlocated and different identity samples are far away. The\nsame idea has been exploited to address item retrieval in\nlarge image datasets (cars, birds, on-line products) [21,25]\nand to learn audio representations for sound event classiﬁ-\ncation [12].\nFor music artist, the embedding approach has been ad-\ndressed previously in [3], where a representation space is\nconstructed using the output probabilities of a multi-class\nclassiﬁcation system with Mel-Frequency Cepstral Coefﬁ-\ncients (MFCC) as input. The space is only used to address\nthe classiﬁcation of known artists, but would also be suit-\nable to unknown ones. Convolutional deep belief networks\nwere used to learn features that were afterwards used for\nartist classiﬁcation [14]: while the evaluation is done on\nonly four artists, the approach learns a representation from\nunlabeled data which can generalize to unknown artists.\nIn [28] the authors train a linear system that attempts to\ncapture the semantic similarities between items in a large\ndatabase by modeling audio, artist names and tags in a sin-\ngle space. The system is trained with multi-task ranking\nlosses, which highly resembles metric learning methods:\neach ranking loss takes as input triplets of samples from\npossibly different kind of sources. Although this approach\nis very promising, both for the objective function and the\nuse of side information, the same artists are used for train\nand evaluation. Unfortunately, direct comparison is hard\nsince little details are given about how datasets are ob-\ntained. In [16], artist embeddings are learned using 1d con-\nvolutional neural networks trained as mono-label classiﬁer\nused afterwards as general features extractors. Their ap-\nproach is able to deal with artists not seen during the train-\ning phase. Information is given on how train databases are\nobtained from the Million Song Dataset (MSD) [4] using\ntheartist73labels.\nSeveral other works address directly the artist classiﬁ-\ncation problem. The current state of the art is inspired by\nspeaker recognition system and makes use of I-vectors to\nseparate artists [9–11]. In [20], an artist ﬁngerprint based\non MFCC is proposed to tackle the problem of retrieving\nan artist from an audio track at scale. In [13], multivariate\nsimilarity kernel methods are proposed to tackle (among\nother tasks) artist classiﬁcation. In [22], the authors fo-\ncus on the main vocalist, and then vocal separation is used\n3http://developer.7digital.comas a preprocessing for artist classiﬁcation. In [2], a multi-\nmodal approach taking advantage of both lyrics and audio\nis proposed to perform artist classiﬁcation. In [7], the au-\nthors use a convolutional neural network to perform artist\nrecognition on a 50artists dataset. While the techniques\nemployed in these works are of interest for their potential\nuse in extracting representations of unknown artists, they\nusually only consider at the classiﬁcation of known artists\nand give no results on the generalization to new artist not\nseen during training phase, nor address the extraction of\nrepresentations useful for unknown artists.\n3. PROPOSED METHOD\n3.1 Metric learning\nThe main idea in metric learning is to learn a metric pre-\nserving map ffrom one metric space into another. Using\na discrete metric in the ﬁrst space, this can be exploited to\nlearn embedding spaces where distances correspond with\nmembership to some category. Here we use the discrete\nmetric deﬁned by artist membership in the space of musi-\ncal audio recordings.\nIn this membership context, the learning of fcan be\nachieved using the triplet loss mechanism [27]. This relies\non using triplets X= (xa;x+;x\u0000), where (xa;x+)is a\npositive pair (samples with same artist membership) and\n(xa;x\u0000)is a negative pair (samples with different artist\nmembership). The triplet loss `contains a term that tries\nto bring closer the images by fof samples in a positive\npairs, and another term that tries to separate the images by\nfof samples in a negative pair. It writes\n`(X) =\f\fkf(xa)\u0000f(x+)k2\n2\u0000kf(xa)\u0000f(x\u0000)k2\n2+\u000b\f\f\n+\n(1)\nwherejsj+= max (0;s). The parameter \u000bhere before is\na positive constant used to ensure a margin between dis-\ntances of points from positive and negative pairs. In order\nto prevent the system to tend to unsuitable states, where all\npoints are mapped to 0, or where the map ftakes arbitrary\nlarge values, we constraint the embedding to lie in the unit\nsphere by imposing kf(x)k2= 1.\n3.2 Training and sampling strategies\nWe train our system using Stochastic Gradient Descent\nover batches of triplets. Since optimizing over triplets that\nare already well separated by the system (i.e. `(X) = 0 ) is\nunnecessary and costly, the system is fed only with triplets\nthat are not yet separated. These are called hard triplets if\nthe value of\nkf(xa)\u0000f(x+)k2\n2\u0000kf(xa)\u0000f(x\u0000)k2\n2\nis negative, and semi-hard triplets if this value is in [0;\u000b].\nWe set\u000b= 0:2as in [5, 18]. Notice that during triplets se-\nlection process the i-th state of the system is used to com-\npute embeddings to ﬁlter data for the ( i+ 1)-th parameters\nupdate.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 623At each iteration, nsamples are chosen from Ndiffer-\nent artists to form positive pairs. For each positive pair one\nnegative sample from the N\u00001left artists is taken to form\na triplet. This leave us with Nn(n\u00001)=2triplets per iter-\nation that are given to the system for optimization only if\nthey are labeled as hard or semi-hard.\nNotice from the expression (1) that the gradients of `\nare close to zero when the system maps all entries to very\nclose points in the space, so in practice learning can fail\nwith the system being stuck in a “collapsed” state where\nf(x) = ^ffor everyx. We observe in experiments that\ncorrect triplet sampling strategies are crucial to avoid this\nphenomenon: taking large batches and enforcing the pres-\nence of as many different artists as possible in each batch\nprevents the system from collapsing.\nIn order to strengthen the artist representations learned\nwe propose to make use of side information related to mu-\nsic artists. Suppose that we are given tags for artists of the\ntraining database. The fact that two artists have a same tag\nt, indicates that these artists share some characteristic. If\nwe want our system to distinguish between similar but not\nequal artists, an interesting possibility is to train the sys-\ntem to not rely on these characteristics. To implement this\nidea, we deﬁne a probability pto prefer a negative sample\nx\u0000with the same tag as the anchor sample xawhen creat-\ning a triplet X. This is done at each iteration after the ﬁrst\nsampling of the Nn(n\u00001)=2triplets. If for some anchor\nxathere is no negative sample x\u0000with a different tag, or\nif we do not dispose of tags for the anchor sample, then we\nfallback to the previous triplet method creation, that is, we\nchoose as negative any sample from another artist. This al-\nlows us to make use of side information even if some may\nbe missing in the database, setting thus a ﬂexible sampling\nframework.\n4. EXPERIMENTS AND EVALUATION\nIn this section, we ﬁrst present our main artist clustering\ntask, the two auxiliary tasks that we use to compare to pre-\nvious works and validate our metric learning approach, and\nthe datasets used for evaluation. Then, we describe the ar-\nchitecture of the neural network that we use to learn artist\nrepresentations. Finally, we detail the datasets used during\nthe training of the systems.\n4.1 Tasks and Evaluations\nThe systems studied in this paper output vector embed-\ndings from audio excerpts. In order to perform evalua-\ntions, we create track-level embeddings by simply aver-\naging embeddings over 10linearly spaced segments of the\nsame track. For the metric learning based system, we also\nproject back the mean embeddings on the unit sphere.\n4.1.1 Artist classiﬁcation and veriﬁcation\nDataset: evaluation is made over a dataset of 467\nartists not seen during training of the embedding systems.\nThese artists are taken from the MSD as explained in Sec-\ntion 4.3.1. For each artist we extract 20tracks, 15tracksare used as ground truth to build artist models and we re-\nport the results for 5tracks as test cases.\nClassiﬁcation task: we attribute to each test case the\nartist identity of its nearest neighbor for the euclidean dis-\ntance among all the ground truth artist models in the em-\nbedding space, and we report the classiﬁcation accuracy\nobtained with this procedure.\nVeriﬁcation task: this is a binary classiﬁcation task were\ngiven any two track-level embeddings (ei;ej)of a test case\niand artist model j, we decide whether they have the same\nartist membership. This is achieved by thresholding the\neuclidean distance between the two embeddings. We may\ndo two types of errors in this task: a false positive error\nwhen two embeddings from two different artists are incor-\nrectly classiﬁed as sharing the same membership, and a\nfalse negative error when two embeddings from the same\nartist are classiﬁed as having different memberships. The\nhigher the decision threshold is, the higher the false neg-\native rate (FNR). Respectively, lowering the threshold re-\nsults in an increase of the false positive rate (FPR). We\nreport the Equal Error Rate (EER), i.e.: the value of FPR\nand FNR for the threshold at which they are equal.\n4.1.2 Homonym artist clustering\nDataset: we built an homonym artists database gather-\ning artists which share exactly the same name with manual\ncleaning. This results in a database of 122groups of 2to\n4homonym artists ( 102groups of 2artists, 17groups of 3\nartists and 3groups of 4). Each artist has a total number of\n2to14albums and 8to168tracks.\nTask: the problem to solve is to discriminate between\nartists that share the same name. From a set of tracks by\ndifferent artists (with the same name), the task is to retrieve\nthe actual clusters of tracks having the same artist member-\nship. It is worth noting that this task may be used in a real\nlife scenario since there is no need of previous knowledge\nof the identities nor the number of artists present in a group\nof same named artists.\nWe use the Adjusted Rand Index (ARI) [17] and the\nAdjusted Mutual Information (AMI) [24] to measure the\nsimilarity of the obtained clusters with the ground truth\nclusters given by the artist memberships. We choose these\ncorrected for chance information theoretic measures since\nthe data size is relatively small compared to the number of\nclusters present therein [24].\nWe present the results on a 5-fold cross-validation of\nthe homonym artists datasets. We used an agglomerative\nhierarchical clustering algorithm [15] to group tracks from\nthe same named artists. During the linkage phase we use\nward method [26] with euclidean metric for calculating the\ndistance between newly formed clusters, and we apply dis-\ntance criterion to get the ﬂat clusters. We select the optimal\nparameters based only on the AMI performance on the de-\nvelopment set, given that we observe a strong correlation\nwith the ARI performance. Finally we report the ARI and\nthe AMI, averaged over the test dataset.624 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018layer size-out kernel activation\ndyn. comp. 1x388x128 - log(1 + 104\u0001)\nconv 16x388x128 3x3x16,1 relu\nmaxpool 16x194x64 3x3x16,2 -\nconv 32x194x64 3x3x32,1 relu\nmaxpool 32x64x21 3x3x32,2 -\nconv 64x64x21 3x3x64,1 relu\nmaxpool 64x21x7 3x3x64,2 -\nﬂatten 1x64x147 - -\naverage pool 1x1x147 - -\ng dropout 1x1x147 - -\ndense 1x1xd - tanh\n`2const. 1x1xd -\u0001\u0002k\u0001k\u00001\n2\nTable 1 : Neural network map details for 9slong audio\nsegments. The output size is described in stacks x rows x\ncolumns. The kernel is speciﬁed as rows x columns x nb.\nﬁlters, stride. The parameter dis the size of the embedding.\n4.2 Embedding systems\nOur embedding map fconsists in a convolutional deep\nneural network that takes features computed from either\n3s or9s long audio samples as input xand outputs points\ninS1\u001aRd. The audio is down-sampled to 22050 kH and\nwe use a mel-spectrogram as features, with 128mel-ﬁlters\nand46ms long Hann window with 50% of overlapping.\nWe apply dynamic compression at the ﬁrst layer of the net-\nwork. Temporal pooling is performed inside the network\nafter the convolution and maxpooling layers, by ﬂattening\ntogether the stack and features tensor dimensions and then\nperforming a global average pooling in the time dimen-\nsion. The output of the network is a d-dimensional vector\nwithd= 32 ord= 256 . Further details of the structure\nof the network are described in Table 1. We use the RM-\nSProp optimizer [23] with a learning rate of 10\u00003,\u001a= 0:9,\n\u000f= 10\u00008and no decay. We compare our metric learn-\ning based system to the system in [16] since it is the only\nprevious work that deals with unknown artists. This sys-\ntem computes artist embeddings of dimension 256using\nthe last hidden layer of a 1D convolutional neural network\ntrained as an artist classiﬁer. We refer to [16] for further\ndetails. Both systems were implemented using Keras [6]\nframework with Tensorﬂow [1] as back-end.\n4.3 Datasets for training embedding systems\nFor each of the datasets described in this section, audio was\nprovided by Deezer music streaming company. At the web\naddress4are provided the ﬁles containing the ids for the\ntracks as well as the artists names.\nWe build 3datasets with different characteristics in\nterms of available audio data per artist and statistical dis-\ntribution of tracks per artist. Our main goal is to test on\nthe homonym artists clustering task the embedding sys-\ntems obtained in different scenarios. One of the datasets\nis created to match real life catalog conditions.\n4https://github.com/deezer/Disambiguating-Music-Artists-at-Scale-\nwith-Audio-Metric-Learning4.3.1 MSD small dataset.\nThe ﬁrst dataset is a sub-sampling of the MSD. The inter-\nest of this dataset is to compare the two studied systems\nwhen a small amount of audio data is available for each\nartist. Following [16] for the dataset construction, we use\nthe7digitalid artist labels delivered with the MSD, but in\naddition we take care of cleaning the dataset from poten-\ntial duplicate artists: we drop MSD artist labels associated\nwith more than one 7digitalid artist labels and viceversa\n(6:3%and5:8%of artist labels). From this cleaned dataset\nwe use the 7digitalid labels to choose a number of artists\nbetween 100 and 2000, and then select 17tracks for each\nartist. These are respectively split into 15for training and 2\nto perform early stopping. We extract a 30s-long excerpts\nfor each track (the position of the excerpts were sampled at\nrandom between the beginning and the end of each track)\nthat are further subdivided in 3slong segments with no\noverlapping used to feed the system. This results in 7:5\nminutes of audio per artist in the training sets.\n4.3.2 Balanced dataset.\nThe interest of this second dataset is to compare the two\nstudied systems when access is granted to larger amounts\nof audio. We choose artists among the 1000 most pop-\nular in the music streaming service that provided the au-\ndio. We keep artist that have at least 4albums with more\nthan3tracks. From the remaining artists, we ﬁrst choose\na number of them between 25 and 600, and then select\n1000 9slong samples for each artist linearly spaced in\neach track. From these samples we pick 1=3for training\nand1=6to perform early stopping. The split is done at\nthe album level, meaning that two tracks from the same al-\nbum cannot appear in the same split. This results in 100\nminutes of audio per artist for the training set. We remark\nthat this dataset is balanced in terms of number of sam-\nples per artist, a common approach to prevent bias towards\ncommon classes in classiﬁer systems. Also, the samples\nare taken from any region of the track, resulting in dataset\nwith more variability than the MSD small dataset.\n4.3.3 Unbalanced dataset.\nIn this third dataset we reproduce the statistical distribution\nof large music catalogs. We use a match from the Discogs\n[8] dataset onto the music streaming company artist dataset\nand we keep the genre tags. We drop artists with less than\n4tracks. We do not perform any further cleaning, so the\nresulting dataset is heavily unbalanced and presents typical\nlong tail behavior. Distributions of samples by artist for\nthis dataset are shown in Figure 1: the unbalanced dataset\nexhibit a long-tailed distribution.\nFrom this dataset we select 10 9slong samples for each\ntrack, which are respectively split into train, evaluation and\ntest. The split is done at the artist level, meaning that two\ntracks from the same artist cannot appear in the same split.\nThere are 1749 different artists in the training set, each of\none has between 1:5and45minutes of audio.\nThe interest of this dataset is to study the abilities of\neach system to make use of all the available audio. Classiﬁ-Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 625Figure 1 : Count distribution of samples by artist in the un-\nbalanced side information negative sampling andunbal-\nanced datasets for metric learning, and the balanced ver-\nsions for classiﬁer systems of the unbalanced dataset.\ncation systems are usually trained with a balanced dataset.\nIf we have access to a dataset that is not already balanced\nin terms of classes, we have two options in other to balance\nit: (A) either cut down samples from the most represented\nclasses or (B) repeat samples of the less represented ones.\nThe former option implies losing data that could have po-\ntentially improved the training of the system, while in the\nsecond option there is a risk that the classiﬁcation system\nover-ﬁt the repeated samples. On the contrary, the triplet\nsampling of a metric learning system permits to make use\nof all accessible training data, since the system has the abil-\nity of dynamically choosing which data is more relevant for\ntraining. To study this, we train the metric learning system\nwith the unbalanced dataset and the classiﬁer system with\ntwo balanced versions of it as explained in here before. As\nwe see in Figure 1, the (A) dataset contains 1002 artists\nand the (B) dataset contains 1749 artists, each with 2000\nsamples.\nFinally, we study the inﬂuence of the new proposed neg-\native sampling method using genre tags with a last unbal-\nanced side information negative sampling dataset: from\nthe raw match Discogs we retain 3023 artists, we keep the\ngenre tags and we take only one 9slong sample for each\ntrack.\n5. RESULTS\nWe ﬁrst present in Figures 2 and 3 the results of the ver-\niﬁcation and classiﬁcation tasks on the MSD small and\nbalanced datasets. We observe that the classiﬁer sys-\ntem performs better than the metric learning based sys-\ntem (d= 256 ) when few audio samples are available for\neach artist ( MSD small ), and that inversely, our system out-\nperforms the classiﬁer system when we are provided with\nlarger amounts for each artist ( balanced ). Indeed, metric\nlearning system are generally difﬁcult to optimize, so large\nquantities of data are needed to make them learn correctly.\nWhen this data is accessible for each artist in train datasets,\nthe metric learning system seems to learn better, which val-\nidates our approach.\nIn Figure 4 we present the results of the homonym\nartists clustering task. We ﬁrst remark that both ARI and\nFigure 2 : Equal Error Rate results of metric learning (ml\n- # artists) and classiﬁcation (cl - # artists) embedding sys-\ntems on the artist veriﬁcation task for different training\ndatasets and number of artists in the dataset. Lower is bet-\nter.\nFigure 3 : Accuracy results of metric learning (ml - #\nartists) and classiﬁcation (cl - # artists) embedding systems\non the artist veriﬁcation task for different training datasets\nand number of artists in the dataset. Higher is better.\nAMI measures take values between \u00001and1, with 0as ex-\npected value for a random clustering. The obtained results\nare thus satisfactory, showing the feasibility of the task\nand making it a compelling candidate to disambiguate un-\nknown artists relying exclusively on audio, for large sized\ncatalogs.\nAs we observed for the veriﬁcation and classiﬁcation\ntasks on the MSD small andbalanced datasets, the metric\nlearning system generally takes better advantage of larger\ntraining datasets. Moreover, the experiments with the un-\nbalanced dataset and its balanced versions ( A) and ( B) in-626 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 4 : Mean AMI and mean ARI performances of the\nmetric learning (ml - # artists) and classiﬁcation (cl - #\nartists) embedding systems on the artist clustering task (5-\nfold cross-validation) for the different training datasets and\nnumber of artists in the dataset. Higher is better.\ndicate that the metric learning system (d = 32) takes full ad-\nvantage of all available data, at least when considering the\nbalancing strategies that we proposed. This is of great in-\nterest since being able to use all the data in train databases\ncould be beneﬁcial in many other settings.\nFinally, we study the results obtained with the side neg-\native sampling strategy that use the genre tags in the un-balanced side information negative sampling dataset. We\nexperiment with setting the probability pas explained in\nSection 3.2 to different values. Although a linear hierarchy\nbetween different values of pis not completely observed,\nwe remark that the best result is obtained with the proba-\nbilityp= 0:5. These promising results show the potential\nof using music side information to strengthen the learned\nartist representations.\n6. CONCLUSION AND FUTURE WORK\n6.1 Synthesis\nWe present a new task of unknown artists clustering to help\ndisambiguating large scale catalogs, show the interest of it\nregarding the current problems of artists identiﬁcation in\nthe music industry, and demonstrate its feasibility with two\ndifferent artist embeddings methods. Regarding different\ntraining datasets conditions (size, amount of audio avail-\nable, distribution of tracks per artist) one or another could\nbe of better help. We showed that the characteristics of\nartists learned by the system can generalize to other artists\nnot seen during the learning phase. We prove that metric\nlearning based method is an interesting choice for learning\nartist representations, in particular by the ﬂexibility of the\ntriplet loss mechanism that allows to better exploit avail-\nable audio data or to incorporate music side information\nduring training. To this extend, we proposed a new neg-\native sampling method that takes advantage of side infor-\nmation during learning phase and show its relevance when\nusing artist genre tags.\n6.2 Future work\nAn interesting question for subsequent work is to under-\nstand the differences between the classiﬁcation based artist\nrepresentation and the metric learning based one, and if\nthey learned similar high level characteristics of audio. To\nthis extent a simple concatenation followed by a dimension\nreduction could be a ﬁrst solution. More interestingly, we\ncould try to train an embedding system with a linear com-\nbination of both losses. Since metric learning loss is dif-\nﬁcult to optimize, for instance due to the collapsing prob-\nlems, classiﬁcation loss could act as a regularization term.\nAnother interesting research direction will be to explore\nother artist embeddings methods that also can incorporate\nside information, such as multi-task ranking losses from\n[28] or task driven non negative matrix factorization with\ngroup constraints as in [19].\nFinally, we plan to further investigate the use of side in-\nformation in negative sampling, and to explore the use of\nother kinds of sources such as mood or release date. In-\nversely, an interesting idea would be to investigate how to\nameliorate genre representations learning by using artists\nlabels as side information.\n7. REFERENCES\n[1] Mart ´ın Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 627Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay\nGhemawat, Ian Goodfellow, Andrew Harp, Geoffrey\nIrving, Michael Isard, Yangqing Jia, Rafal Jozefow-\nicz, Lukasz Kaiser, Manjunath Kudlur, Josh Leven-\nberg, Dan Man ´e, Rajat Monga, Sherry Moore, Derek\nMurray, Chris Olah, Mike Schuster, Jonathon Shlens,\nBenoit Steiner, Ilya Sutskever, Kunal Talwar, Paul\nTucker, Vincent Vanhoucke, Vijay Vasudevan, Fer-\nnanda Vi ´egas, Oriol Vinyals, Pete Warden, Martin\nWattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang\nZheng. TensorFlow: Large-scale machine learning on\nheterogeneous systems, 2015. Software available from\ntensorﬂow.org.\n[2] Kamelia Aryafar and Ali Shokoufandeh. Multimodal\nmusic and lyrics fusion classiﬁer for artist identiﬁca-\ntion. In ICMLA , pages 506–509, 2014.\n[3] A. Berenzweig, D. P. W. Ellis, and S. Lawrence. An-\nchor space for classiﬁcation and similarity measure-\nment of music. In International Conference on Mul-\ntimedia and Expo (ICME) , volume 1, pages I–29–32,\n2003.\n[4] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian\nWhitman, and Paul Lamere. The million song dataset.\nInternational Society for Music Information Retrieval\nConference , 2011.\n[5] Herv ´e Bredin. TristouNet: Triplet Loss for Speaker\nTurn Embedding. In IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\npages 5430–5434, 2017.\n[6] Franc ¸ois Chollet et al. Keras. https://github.\ncom/fchollet/keras , 2015.\n[7] Sander Dieleman, Phil ´emon Brakel, and Benjamin\nSchrauwen. Audio-based music classiﬁcation with a\npretrained convolutional network. In ISMIR , pages\n669–674, 2011.\n[8] Discogs. Monthly dumps of discogs release, artist, la-\nbel, and master release data, 2018.\n[9] Hamid Eghbal-zadeh, Bernhard Lehner, Markus\nSchedl, and Gerhard Widmer. I-vectors for timbre-\nbased music similarity and music artist classiﬁcation.\nInISMIR , pages 554–560, 2015.\n[10] Hamid Eghbal-Zadeh, Markus Schedl, and Gerhard\nWidmer. Timbral modeling for music artist recognition\nusing i-vectors. In European Signal Processing Con-\nference (EUSIPCO) , pages 1286–1290. IEEE, 2015.\n[11] Hamid Eghbal-zadeh and Gerhard Widmer. Noise ro-\nbust music artist recognition using i-vector. In ISMIR ,\npages 709–715, 2016.\n[12] Aren Jansen, Manoj Plakal, Ratheet Pandya, Dan El-\nlis, Shawn Hershey, Jiayang Liu, Channing Moore, and\nRif A. Saurous. Unsupervised learning of semantic au-\ndio representations. In ICASSP , 2018.[13] Pavel P. Kuksa. Efﬁcient multivariate sequence classi-\nﬁcation. arXiv:1409.8211 [cs] , 2014.\n[14] Honglak Lee, Peter Pham, Yan Largman, and An-\ndrew Y . Ng. Unsupervised feature learning for au-\ndio classiﬁcation using convolutional deep belief net-\nworks. In Advances in Neural Information Processing\nSystems , pages 1096–1104. 2009.\n[15] Daniel M ¨ullner. Modern hierarchical, agglomerative\nclustering algorithms. CoRR , abs/1109.2378, 2011.\n[16] Jiyoung Park, Jongpil Lee, Jangyeon Park, Jung-Woo\nHa, and Juhan Nam. Representation learning of music\nusing artist labels. CoRR , abs/1710.06648, 2017.\n[17] William M. Rand. Objective criteria for the evaluation\nof clustering methods. Journal of the American Statis-\ntical Association , 66(336):846, 1971.\n[18] Florian Schroff, Dmitry Kalenichenko, and James\nPhilbin. Facenet: A uniﬁed embedding for face recog-\nnition and clustering. In CVPR , pages 815–823. IEEE\nComputer Society, 2015.\n[19] Romain Serizel, Slim Essid, and Ga ¨el Richard. Group\nNon-negative Matrix Factorisation With Speaker And\nSession Similarity Constraints For Speaker Identiﬁca-\ntion. In IEEE International Conference on Acoustics,\nSpeech, and Signal Processing , Shangai, China, March\n2016.\n[20] Sajad Shirali-Shahreza, Hassan Abolhassani, and\nM. Hassan Shirali-Shahreza. Fast and scalable system\nfor automatic artist identiﬁcation. IEEE Transactions\non Consumer Electronics , 55(3), 2009.\n[21] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Sil-\nvio Savarese. Deep metric learning via lifted structured\nfeature embedding. In Computer Vision and Pattern\nRecognition (CVPR) , 2016.\n[22] Li Su and Yi-Hsuan Yang. Sparse modeling for artist\nidentiﬁcation: Exploiting phase information and vocal\nseparation. In ISMIR , pages 349–354, 2013.\n[23] T. Tieleman and G. Hinton. Lecture 6.5—RmsProp:\nDivide the gradient by a running average of its recent\nmagnitude. COURSERA: Neural Networks for Ma-\nchine Learning, 2012.\n[24] Nguyen Xuan Vinh, Julien Epps, and James Bailey. In-\nformation theoretic measures for clusterings compari-\nson: Variants, properties, normalization and correction\nfor chance. Journal of the American Statistical Associ-\nation , (301):236–244.\n[25] Jian Wang, Feng Zhou, Shilei Wen, Xiao Liu, and\nYuanqing Lin. Deep metric learning with angular loss.\n08 2017.\n[26] Joe H. Ward. Hierarchical grouping to optimize an ob-\njective function. Journal of the American Statistical\nAssociation , 58(301):236–244, 1963.628 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[27] Kilian Q. Weinberger and Lawrence K. Saul. Dis-\ntance metric learning for large margin nearest neighbor\nclassiﬁcation. J. Mach. Learn. Res. , 10:207–244, June\n2009.\n[28] Jason Weston, Samy Bengio, and Philippe Hamel.\nLarge-scale music annotation and retrieval: Learning\nto rank in joint semantic spaces. CoRR , abs/1105.5196,\n2011.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 629"
    },
    {
        "title": "Zero-Mean Convolutions for Level-Invariant Singing Voice Detection.",
        "author": [
            "Jan Schlüter",
            "Bernhard Lehner"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492413",
        "url": "https://doi.org/10.5281/zenodo.1492413",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/189_Paper.pdf",
        "abstract": "State-of-the-art singing voice detectors are based on classifiers trained on annotated examples. As recently shown, such detectors have an important weakness: Since singing voice is correlated with sound level in training data, classifiers learn to become sensitive to input magnitude, and give different predictions for the same signal at different sound levels. Starting from a Convolutional Neural Network (CNN) trained on logarithmic-magnitude mel spectrogram excerpts, we eliminate this dependency by forcing each first-layer convolutional filter to be zero-mean – that is, to have its coefficients sum to zero. In contrast to four other methods – data augmentation, instance normalization, spectral delta features, and per-channel energy normalization (PCEN) – that we evaluated on a largescale public dataset, zero-mean convolutions achieve perfect sound level invariance without any impact on prediction accuracy or computational requirements. We assume that zero-mean convolutions would be useful for other machine listening tasks requiring robustness to level changes.",
        "zenodo_id": 1492413,
        "dblp_key": "conf/ismir/SchluterL18",
        "keywords": [
            "state-of-the-art",
            "singing voice detectors",
            "classifier trained",
            "annotated examples",
            "correlated with sound level",
            "sound level invariance",
            "zero-mean convolutions",
            "perfect sound level invariance",
            "prediction accuracy",
            "computational requirements"
        ],
        "content": "ZERO-MEAN CONVOLUTIONS FOR\nLEVEL-INVARIANT SINGING VOICE DETECTION\nJan Schlüter\nAustrian Research Institute for\nArtiﬁcial Intelligence, Vienna\njan.schlueter@ofai.atBernhard Lehner\nInstitute of Computational Perception,\nJohannes Kepler University Linz, Austria\nbernhard.lehner@jku.at\nABSTRACT\nState-of-the-art singing voice detectors are based on clas-\nsiﬁers trained on annotated examples. As recently shown,\nsuch detectors have an important weakness: Since singing\nvoice is correlated with sound level in training data, clas-\nsiﬁers learn to become sensitive to input magnitude, and\ngive different predictions for the same signal at different\nsound levels. Starting from a Convolutional Neural Net-\nwork (CNN) trained on logarithmic-magnitude mel spec-\ntrogram excerpts, we eliminate this dependency by forc-\ning each ﬁrst-layer convolutional ﬁlter to be zero-mean\n– that is, to have its coefﬁcients sum to zero. In con-\ntrast to four other methods – data augmentation, instance\nnormalization, spectral delta features, and per-channel en-\nergy normalization (PCEN) – that we evaluated on a large-\nscale public dataset, zero-mean convolutions achieve per-\nfect sound level invariance without any impact on predic-\ntion accuracy or computational requirements. We assume\nthat zero-mean convolutions would be useful for other ma-\nchine listening tasks requiring robustness to level changes.\n1. INTRODUCTION\nAutomatically annotating the presence of singing voice in a\nmusic recording is a challenging task, as singing voice cov-\ners a wide range of notes and expressions, is often accom-\npanied by several other instruments, and may be confused\nwith instruments capable of producing similar melody con-\ntours. Recent approaches try to capture this variability by\ntraining strong classiﬁers such as deep neural networks\non annotated data [9, 12, 14, 20, 22]. While they achieve\nhigh accuracies on standard benchmark datasets, classiﬁers\nmay exploit correlations between inputs and targets that are\npresent in both the training and test data, but are not seman-\ntically meaningful (such a classiﬁer is sometimes called a\nhorse [24]) or unwanted (leading to algorithmic bias [6]).\nIn [13], we demonstrated that three state-of-the-art singing\nvoice detectors – both with hand-designed and learned fea-\ntures – exploit a dependency between singing voice and\nsound level present in common datasets.\nc\rJan Schlüter, Bernhard Lehner. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Jan Schlüter, Bernhard Lehner. “Zero-Mean Convolutions for\nLevel-Invariant Singing V oice Detection”, 19th International Society for\nMusic Information Retrieval Conference, Paris, France, 2018.\u000050 \u000040 \u000030 \u000020 \u000010 0\nmagnitude (dB FS)010k20k30knumber of framesthreshold singing\nno singing\nFigure 1 : Spectrogram frames of the Jamendo training set\ncontaining singing voice tend to have larger magnitudes. A\nsimple threshold allows distinguishing the classes with an\naccuracy of 61% (8.5 percent points above the baseline).\nWe can reveal this dependency in a simple experiment:\nWe compute spectrograms for all ﬁles of the Jamendo data-\nset [18] and sum up the linear magnitudes for each frame.\nThe distribution of magnitudes in the training set is clearly\nskewed towards larger values for frames containing singing\nvoice (Figure 1). Choosing an optimal threshold, we can\ndistinguish vocal from nonvocal frames at an accuracy of\n61.1%. With the same threshold, we correctly classify\n59.0% of the validation and 68.7% of the test set frames.\nThis is a strong enough improvement over predicting the\nmajority class (52.6%, 51.4% and 53.7%, respectively)\nthat any classiﬁer will pick up this cue. Note that for clar-\nity of presentation, we omitted typical preprocessing steps\nsuch as mel scaling, logarithmic magnitude compression\nor bandwise standardization, but results hardly differ (0.3\npercent points improved) with these steps included.\nOf course this confound does not stem from inherent\ncharacteristics of singing voice, but from production habits\nin commercial music – if a track contains vocals, those are\nmixed to stand out. Thus, it affects many other Western-\nmusic datasets (we veriﬁed this for RWC [8,16], MSD100\n[17], and tracks containing vocals in MedleyDB [3]) that\nare commonly used for singing voice detection research.\nIn [13], we argue that to avoid this, datasets should in-\nclude a sufﬁcient number of instrumental tracks, which\ncannot feature vocals as the most prominent instrument.\nAnd indeed, for the enlarged dataset in [13], there is hardly\nany linear correlation between input magnitude and class\n(Figure 2). However, there is still a strong statistical depen-\ndency, with vocal frames exhibiting a different magnitude\ndistribution from nonvocal frames, enabling a better-than-\nchance prediction of the class from the input magnitude.321\u000050 \u000040 \u000030 \u000020 \u000010 0\nmagnitude (dB FS)0100k200knumber of framessingingno singing\nFigure 2 : For a dataset including many purely-\ninstrumental tracks, input magnitude and class are not lin-\nearly correlated, but still show a clear statistical depen-\ndency exploitable by a classiﬁer.\n7:34 7:38 7:42+6 dB0 dBpredictions-6 dBsong A\n(a) correlated1:45 1:49 1:53predictionssong B\n(b) anti-correlated0:45 0:49 0:53predictionssong C\n(c) uncorrelated\nFigure 3 : Presenting a state-of-the-art classiﬁer with the\nsame music excerpt at altered sound levels reveals a strong\nsound level dependency. (a) For some songs, increasing\nthe level by 6 dB increases the classiﬁer’s output. (b) For\nsome, this dependency is inverted. (c) For some, vocals are\nonly detected at the original sound level (second row).\nWhen training a state-of-the-art network on this dataset,\nit develops a complex sound level dependency: for some\ntest ﬁles, predictions are correlated with input magnitude\n(Fig. 3a), for others, they behave conversely (Fig. 3b) or\ndecrease for any deviation from the original level (Fig. 3c).\nIf and which of these cases applies to a given input seems\nto depend on the content, not only the original sound level,\nand sometimes varies from model to model, but the effect\nappears reliably.\nWhile a closer investigation of the underlying reasons\nwould be highly interesting, for now we content ourselves\nwith stating that this effect is unwanted. As changing the\nsound level of a music recording does not change the pres-\nence of singing voice, we would like a singing voice detec-\ntor to be invariant to the scale of the input signal. In [13],\nwe show how to achieve this for a system based on hand-\ndesigned features. In this work, we propose and evalu-\nate different ways to achieve the same for a Convolutional\nNeural Network (CNN) trained on mel spectrograms, out-\nperforming the hand-designed system.\nThe remaining paper is structured as follows: In the\nnext section, we review related work on singing voice de-\ntection and level invariance. Section 3 explains the CNN-\nbased baseline system as well as ﬁve methods to improve\nits robustness to level changes, and Section 4 evaluates\nthese methods experimentally. Finally, Section 5 summa-\nrizes our ﬁndings and their implications.2. RELATED WORK\nFrom early approaches [2] to recent ones [9,12,14,20,22],\nsinging voice detection has mostly been addressed with\nclassiﬁers trained on audio features. Berenzweig et al. [2]\nbased their system on an existing speech recognizer, com-\nbined with cepstral coefﬁcients and classiﬁed with a simple\nGaussian model. Leglaive et al. [12] trained a bidirectional\nRecurrent Neural Network (RNN) on preprocessed mel\nspectra, Lehner et al. [14] trained a unidirectional RNN on\na set of hand-designed features. Schlüter et al. [22] deﬁne\nthe current state of the art using a CNN on logarithmic-\nmagnitude mel spectrograms trained with data augmenta-\ntion; we will use their public implementation as a starting\npoint. More recent work uses CNNs in attempts to lower\nannotation effort by learning from song-wise labels [20],\nor by deriving labels from pairing songs with instrumental\nversions [9]. The related tasks of auto-tagging (i.e., deter-\nmining song-wise labels) and singing voice separation are\nalso tackled with CNNs, but will not be considered here.\nApart from our work [13], to the best of our knowl-\nedge, invariance to the sound level has not been addressed\nin the context of singing voice detection, but at least Mauch\net al. [15] and Sturm [24, Sec. III.B] recognized it as a pos-\nsible confounding factor for music information retrieval\nsystems. In speech recognition, early approaches based\non Mel-Frequency Cepstral Coefﬁcients (MFCCs) discard\nthe 0thcoefﬁcient [4, Eq. 1], effectively becoming invariant\nto the scale of the input signal. Modern CNN-based sys-\ntems processing spectrograms or raw signals achieve ro-\nbustness by using large networks and datasets (e.g., 38 mil-\nlion parameters and 7000 hours in [1]). For smaller CNNs,\nWang et al. [26] recently proposed to process spectrograms\nwith an automatic gain control of learnable parameters,\ntermed per-channel energy normalization (PCEN). We will\ninclude this method in our experiments.\n3. METHOD\nIn the following, we will describe the state-of-the-art\nmethod we used as a starting point, and ﬁve modiﬁcations\naiming to reduce its sound level dependency (which was\ndemonstrated in Figure 3).\n3.1 Baseline\nWe base our work on the system of Schlüter et al. [22],\nin the variant they made available online1and described\nin [21, Sec. 9.8]. From monophonic input signals sampled\nat 22 kHz, it computes magnitude spectrograms (frame\nlength 1024, hop size 315 samples), applies a mel ﬁlter-\nbank (80 bands from 27.5 Hz to 8 kHz) and scales mag-\nnitudes as log(max(10\u00007;x)). A CNN classiﬁes 115-\nframe excerpts of these spectrograms into vocal/nonvocal.\nIt starts with batch normalization [10] across the batch and\ntime axis without learned scale and bias – this effectively\nstandardizes each mel band over the training set as in [22],\nbut can adapt to changes to the frontend during training,\n1https://github.com/f0k/ismir2015/tree/phd_\nextra , accessed 2018-03-30322 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018which we need for PCEN. This is followed by two convo-\nlutional layers of 64and32 3\u00023ﬁlters, respectively, 3\u00023\nmax-pooling, 128and64 3\u00023convolutions, 128 3\u000218\nconvolutions, 4\u00021pooling, and three dense layers of 256,\n64and1units, respectively. Each convolutional and dense\nlayer is followed by batch normalization and leaky recti-\nﬁcation max(x=100;x)except for the ﬁnal layer, which\nuses a sigmoid unit for binary classiﬁcation.\nDuring training, 50% dropout is applied before each\nfully-connected layer, and inputs are augmented with pitch\nshifting and time stretching up to \u000630%, and random fre-\nquency band ﬁlters of up to \u000610dB, before mel scaling.\nAt test time, we turn the CNN into a fully-convolutional\nnet, replacing dense layers by convolutions and adding di-\nlations as described in [23]. This allows computing pre-\ndictions over a full spectrogram without redundant com-\nputations that would occur when feeding overlapping 115-\nframe excerpts. All batch normalizations use statistics col-\nlected during training, not statistics from test examples.\n3.2 Data Augmentation\nA sure way to prevent classiﬁers from exploiting particular\ncorrelations in the training data is to remove these corre-\nlations from the data. Data augmentation attempts to re-\nmove or reduce correlations by varying the training exam-\nples along the confounding dimension. In our case, to re-\nduce the dependency between input magnitude and target\nshown in Figures 1, 2, we scale input signals randomly by\nup to\u000610dB in addition to the existing augmentations.\n3.3 Instance Normalization\nAs a more drastic measure, we replace the initial batch nor-\nmalization with instance normalization [25], i.e., we sep-\narately standardize each 115-frame excerpt to zero mean\nand unit variance per mel band, both at training and at test\ntime. This is in contrast to batch normalization, which uses\nbatch-wise rather than excerpt-wise statistics during train-\ning, and ﬁxed dataset-wise statistics2for testing.\nInstance normalization trivially results in a representa-\ntion that is fully invariant to scaling the input signal. How-\never, it prevents using the CNN as a fully-convolutional\nnet at test time, since every excerpt needs to be processed\nseparately. In Section 4.4, we will see how this affects\ncomputation time.\n3.4 Spectral Delta Features\nScaling the input signal results in a shift of the logarithmic-\nmagnitude mel spectrogram. Delta features, i.e., the\nelementwise difference between a frame and its predeces-\nsor, are invariant to such an offset. They are commonly\nused as supporting features to include temporal informa-\ntion in frame-wise classiﬁcation, but have also been used\nsuccessfully as the only input for RNN-based musical on-\nset detection (albeit in a rectiﬁed form, [5]) and might be\nsufﬁcient for singing voice detection.\n2For simplicity, an exponential moving average of batch-wise statis-\ntics collected during training, as suggested for validation in [10, Sec. 3.1].\nImportantly, the normalization is independent of the input at test time.3.5 PCEN\nProposed by Wang et al. [26], per-channel energy nor-\nmalization (PCEN) processes a mel spectrogram of linear\nmagnitudes (i.e., replacing the logarithmic scaling) as\nYt;f=\u0012Xt;f\n(\u000f+Mt;f)\u000bf+\u000ef\u0013rf\n\u0000\u000erf\nf; (1)\nwhereMis an estimate of the local magnitude per time\nstep and frequency band computed using a simple inﬁnite\nimpulse response (IIR) ﬁlter:\nMt;f= (1\u0000sf)Mt\u00001;f+sfXt;f (2)\nThe division by Mimplements an automatic gain control,\nwhich is followed by root compression (for 0< rf<1).\nWang et al. parameterize \u000bf:= exp(^\u000bf),\u000ef:= exp( ^\u000ef),\nrf:= exp(^rf)and learn ^\u000b,^\u000e,^ras part of a neural net-\nwork. Learning the logarithms ensures that \u000b,\u000e,rremain\npositive. Instead of learning s, Wang et al. replace Mwith\na convex combination of precomputed IIR ﬁlters of differ-\nent smoothing factors sand learn the combination weights.\nWe deviate from their approach in two respects:\n1. We ﬁx\u000bf:= 1 , as any other choice will make Y\ndependent on the scale of X.\n2. We parameterize sf:= exp(^sf)and learn ^sdirectly\nas part of the neural network.3Wang et al. noted\nthat option in [26, Sec. 3], but did not explore it.\nThe IIR ﬁlter must process the input sequentially, and\nthus is not a good ﬁt for massively parallel computation\ndevices such as Graphical Processing Units (GPUs). We\nwill see how this affects computation time in Section 4.4.\n3.6 Zero-Mean Convolution\nSpectral delta features are just one of many ways to com-\npute differences in the spectrogram that are invariant to\nadding a constant to the input. For example, we could\njust as well compute differences between neighbouring fre-\nquencies. More generally, any cross-correlation with a\nzero-mean ﬁlter Wwill remove a global offset cfromX:\n((X+c)\u0003W)t;f=X\ni;j(Xt+i;f+j+c)Wi;j\n=X\ni;jXt+i;f+jWi;j+cX\ni;jWi;j= (X\u0003W)t;f\nThe last step uses our assumption of a zero-mean ﬁlter,P\ni;jWi;j= 0. The ﬁrst convolutional layer of our CNN\nalready computes 64separate cross-correlations of the in-\nput with learnable ﬁlters W(k), wherekindexes the 64\nﬁlters. We enforce these to be zero-mean by parameteriz-\ningW(k)\ni;j:=^W(k)\ni;j\u00001\nMNP\ni;j^W(k)\ni;jand learning ^W(k),\nwhereM=N= 3is the ﬁlter size.\n3We could also use a sigmoid function to ensure 0<sf<1, but in\npractice, the bound sf<1was not at a risk to be broken during learning.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 3230:02:55:07:5classiﬁcation error (%)\nbaseline-9-6-30+3+6+95:80\naugmentation-9-6-30+3+6+95:74\ninstance norm.-9-6-30+3+6+96:24\ndelta features-9-6-30+3+6+96:64\nPCEN (\u000b= 1)-9-6-30+3+6+96:22\nzero-mean conv.-9-6-30+3+6+95:52\nFigure 4 : Classiﬁcation error on our test set for each method with modiﬁed input gain between -9 dB to +9 dB. Error bars\nindicate the standard deviation over ﬁve networks. To facilitate comparison, the result at 0 dB is printed at the top.\n4. EXPERIMENTS\nTo compare the ﬁve methods and the baseline, we trained\nand tested each of them on a large public singing voice de-\ntection dataset, comparing the quality of their predictions,\nrobustness to level changes, and computational demands.\n4.1 Dataset\nFor our previous work [13], we curated a dataset combin-\ning data from Jamendo [18], RWC [8, 16], MSD100 [17],\na music video game, YouTube and several instrumental al-\nbums. Compared to existing corpora, it is larger and more\ndiverse, both in terms of music genres and by including\npurely instrumental music pieces – it can be insightful to\ntest a singing voice detection system on music that does not\nfeature vocals as the predominant instrument (for example,\nFigures 3a,b show excerpts of two instrumental pieces).\nIn total, the dataset contains almost 80 h of music, split\nup (without artist overlaps) into 20 h for training, 17.5 h for\nvalidation, and 42 h for testing. For a more detailed listing,\nwe refer the reader to [13, Table I].\n4.2 Training\nNetworks are trained to minimize cross-entropy loss on\nmini-batches of 32excerpts with ADAM [11]. Weights are\ninitialized following Saxe et al. [19], PCEN parameters ^\u000ef\nand^rfto zeros, ^sftolog(0:025) , when used. Compared to\nthe public implementation of the baseline system, we use\nan adaptive learning rate schedule to cope with the larger\ndataset. We start at a learning rate of 0:001and drop it to\na tenth whenever the training loss4did not reach a new\nminimum for 10 consecutive mini-epochs of 1000 updates\neach. At each drop, we reset the network weights to the\nprevious minimum. On the third drop, we stop training.\n4.3 Evaluation\nAfter training, we compute framewise predictions (net-\nwork outputs between 0.0 and 1.0) for all validation and\ntest recordings at their original sound level as well as\nall test recordings at gains of -9 dB, -6 dB, -3 dB, +3 dB,\n+6 dB, +9 dB.5Each sequence of predictions is smoothed\n4We did not run into any overﬁtting, possibly because the network was\noriginally designed for a much smaller dataset, and found it beneﬁcial to\nbase the schedule on the training loss rather than the validation loss.\n5Gains are applied to the input signal expressed as ﬂoating-point sam-\nples, so positive gains cannot result in clipping.Nvidia Nvidia Intel\nTitan Xp GTX 970 i7-4770S\nbaseline 1.7 s 3.0 s 15.2 s\naugmentation 1.7 s 3.0 s 15.2 s\ninstance norm. 42.5 s 103.1 s 643.1 s\ndelta features 1.7 s 3.0 s 15.2 s\nPCEN 6.9 s 9.0 s 15.5 s\nzero-mean conv. 1.7 s 3.0 s 15.2 s\nTable 1 : Computation time required for predicting singing\nvoice in one hour of audio with each method, for two GPUs\nand a CPU (using a single core).\nin time with a sliding median ﬁlter of 800 ms. We deter-\nmine the optimal classiﬁcation threshold for the smoothed\npredictions of the validation set at its original sound level,\nand apply this threshold to all other predictions. Finally,\nwe compute the classiﬁcation error for the test recordings,\nseparately for each applied gain.\n4.4 Results\nFigure 4 depicts our results. The leftmost group of bars\nshows the classiﬁcation error of the baseline system: It\nreaches 5:8% error for the original recordings, but per-\nforms worse when scaling the input signals, up to an error\nof7:6% for -9 dB (a scale factor of 10\u00009=10\u00190:126).\nTraining with examples of modiﬁed gain apparently\ndoes not help: Results at original sound level are compa-\nrable to the baseline, and the sound level dependency is as\nstrong as before. Apparently, the augmentation does not\nsufﬁciently weaken the dependency between input magni-\ntude and target label. Furthermore, it may not add anything\nover the existing frequency ﬁltering augmentation, which\napplies a random gain to a random frequency range.\nAll remaining methods are invariant to an input gain by\nconstruction, so they achieve the same classiﬁcation er-\nror regardless of the gain.6In terms of accuracy, spec-\ntral delta features perform worst, at an error of 6:6%. In-\nstance Normalization and PCEN (with ﬁxed \u000bfparameters\nas explained in Section 3.5) are noticeably better, but still\nfall signiﬁcantly behind the baseline system at 6:2% error.\n6Note that the converse is not true: a system achieving the same classi-\nﬁcation error for altered inputs may still be level-dependent, by improving\nfor some examples and failing on others. In [13], we propose an evalua-\ntion scheme to rule out this case, but it is not needed here.324 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018When not ﬁxing \u000b, PCEN reaches an error of 5:9%at\n0 dB, but is as level-dependent as the baseline, with learned\n\u000bfbetween 0.5 and 0.8 (results not included in Figure 4).\nFinally, zero-mean convolutions slightly exceed the classi-\nﬁcation accuracy of the baseline system while still being\nrobust to level changes.\nAs an additional criterion, Table 1 compares the test-\ntime computational demands of the different variants. Us-\ning the baseline system, computing framewise singing\nvoice predictions for one hour of audio (with spectrograms\nalready computed) takes 1.7 seconds with a high-end GPU,\n3 seconds with a consumer GPU, and 15 seconds on a sin-\ngle CPU core. Since data augmentation and zero-mean\nconvolutions only affect training, and since spectral delta\nfeatures are cheap to compute, all three are just as fast\nas the baseline. The IIR ﬁlter of PCEN is inherently se-\nrial, hindering parallelization. This is not a problem in\nsingle-threaded CPU computation, but up to 4 \u0002slower\nthan the baseline on GPU. Finally, Instance Normalization\nrequires processing each 115-frame network input sepa-\nrately, preventing reuse of computation in overlapping ex-\ncerpts. While still fast enough for real-time processing,\nthis poses a huge disadvantage, and is up to 42 \u0002slower\nthan the baseline.\n5. CONCLUSION\nAfter demonstrating that singing voice detectors are sus-\nceptible to partly base their prediction on the absolute mag-\nnitude of the input signal, we explore ﬁve different ways\nto reduce or eliminate this dependency in a CNN-based\nstate-of-the-art system. They have different strengths and\nweaknesses, but one method turned out to be optimal in\nterms of classiﬁcation error, robustness to level changes\nand computational overhead: parameterizing the ﬁlters of\nthe ﬁrst convolutional layer to be zero-mean. When pro-\ncessing logarithmic-magnitude spectrograms, this removes\nany constant offset resulting from changing the input gain.\nIntroducing level invariance with zero-mean convolu-\ntions is easy and does not measurably affect training time.\nThis might be useful in other machine listening tasks that\nshould not take the sound level into account – either to sta-\nbilize predictions against changes in the input gain, as in\nour case, or even to improve learning from data of varying\nloudness. To facilitate reuse, our implementation of all ﬁve\nmethods is available online.7\nA dissatisfying aspect of our solution is that it required\nunderstanding the problem and introducing a constraint in\nthe parameter space of the neural network. While this is\na reasonable way to make progress, it would be helpful to\nﬁnd a method that forces the network to learn this con-\nstraint from data. A possible candidate would be Unsuper-\nvised Domain Adaptation [7], although initial experiments\ndid not turn out successful. Level-invariant singing voice\ndetection might be a useful test bed, since we already know\nwhat a level-invariant CNN can look like.\n7https://github.com/f0k/ismir2018 or\nhttp://jan-schlueter.de/pubs/2018_ismir.zipIn the broader context of the discussion on horses [24]\n(systems that rely on confounding factors for their pre-\ndictions), our work identiﬁed a system to be a horse, and\nfound a way to ﬁx the aspect it identiﬁed. Most probably,\nthe system is still partly using the wrong cues, and future\nwork could iteratively ﬁnd and ﬁx this. However, this may\nnot be the best road to follow: both ﬁnding and avoiding\nconfounds is difﬁcult. We discovered the loudness con-\nfound after noticing that including the 0thMFCC in the\nfeature set of a classiﬁer unexpectedly improved results,\nfollowing this trail by testing classiﬁers with altered ex-\namples. Avoiding it required very different approaches for\na hand-designed feature set [13] and the CNN addressed\nhere. Another confound, a hypersensitivity of our system\nto sloped lines in a spectrogram, was discovered by look-\ning at false negatives and false positives, but attempts to\navoid it were fruitless [21, p. 190]. A different angle of at-\ntack on horses would be to research ways to constrain the\nlearning system to mimic human perception, such that it\ncannot use cues that humans would not consider in the ﬁrst\nplace.\n6. ACKNOWLEDGEMENTS\nThis research is supported by the Vienna Science and\nTechnology Fund (WWTF) under grants NXT17-004 and\nMA14-018. We also gratefully acknowledge the support of\nNVIDIA Corporation with the donation of two Tesla K40\nGPUs and a Titan Xp GPU used for this research. Last, but\nnot least, we would like to thank the anonymous reviewers\nfor their valuable input.\n7. REFERENCES\n[1] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J.\nCasper, B. C. Catanzaro, et al. Deep Speech 2: End-to-\nend speech recognition in english and mandarin. arXiv\ne-prints , abs/1512.02595, 2015.\n[2] A. L. Berenzweig and D. P. W. Ellis. Locating singing\nvoice segments within music signals. In IEEE Work-\nshop on the Applications of Signal Processing to Audio\nand Acoustics (WASPAA) , pages 119–122, New Paltz,\nNY , USA, October 2001.\n[3] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Can-\nnam, and J. P. Bello. MedleyDB: A multitrack dataset\nfor annotation-intensive MIR research. In Proceedings\nof the 15th International Society for Music Information\nRetrieval Conference (ISMIR) , Taipei, Taiwan, October\n2014.\n[4] S. B. Davis and P. Mermelstein. Comparison of para-\nmetric representations for monosyllabic word recog-\nnition in continuously spoken sentences. IEEE Trans-\nactions on Acoustics, Speech and Signal Processing ,\n28(4):357–366, August 1980.\n[5] F. Eyben, S. Böck, B. Schuller, and A. Graves. Univer-\nsal onset detection with bidirectional long short-term\nmemory neural networks. In Proceedings of the 11thProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 325International Society for Music Information Retrieval\nConference (ISMIR) , pages 589–594, Utrecht, Nether-\nlands, August 2010.\n[6] B. Friedman and H. Nissenbaum. Bias in computer\nsystems. ACM Transactions on Information Systems ,\n14(3):330–347, July 1996.\n[7] Y . Ganin and V . Lempitsky. Unsupervised domain\nadaptation by backpropagation. In F. Bach and D. Blei,\neditors, Proceedings of the 32nd International Confer-\nence on Machine Learning (ICML) , volume 37 of Pro-\nceedings of Machine Learning Research , pages 1180–\n1189, Lille, France, July 2015. PMLR.\n[8] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Popular, classical, and jazz mu-\nsic databases. In Proceedings of the 3rd International\nConference on Music Information Retrieval (ISMIR) ,\npages 287–288, Paris, France, October 2002.\n[9] E. J. Humphrey, N. Montecchio, R. Bittner, A. Jans-\nson, and T. Jehan. Mining labeled data from web-\nscale collections for vocal activity detection in mu-\nsic. In Proceedings of the 18th International Society\nfor Music Information Retrieval Conference (ISMIR) ,\nSuzhou, China, October 2017.\n[10] S. Ioffe and C. Szegedy. Batch normalization: Ac-\ncelerating deep network training by reducing internal\ncovariate shift. In F. Bach and D. Blei, editors, Pro-\nceedings of the 32nd International Conference on Ma-\nchine Learning (ICML) , volume 37 of Proceedings\nof Machine Learning Research , pages 448–456, Lille,\nFrance, July 2015. PMLR.\n[11] D. P. Kingma and J. Ba. Adam: A method for\nstochastic optimization. In Proceedings of the 3rd In-\nternational Conference on Learning Representations\n(ICLR) , San Diego, CA, USA, May 2015.\n[12] S. Leglaive, R. Hennequin, and R. Badeau. Singing\nvoice detection with deep recurrent neural networks.\nInProceedings of the 40th IEEE International Con-\nference on Acoustics, Speech, and Signal Processing\n(ICASSP) , pages 121–125, Brisbane, Australia, April\n2015.\n[13] B. Lehner, J. Schlüter, and G. Widmer. Online,\nloudness-invariant vocal detection in mixed music sig-\nnals. IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing , 26(8):1369–1380, August 2018.\n[14] B. Lehner, G. Widmer, and S. Böck. A low-latency,\nreal-time-capable singing voice detection method with\nLSTM recurrent neural networks. In Proceedings of\nthe 23rd European Signal Processing Conference (EU-\nSIPCO) , pages 21–25, Nice, France, August 2015.\n[15] M. Mauch and S. Ewert. The audio degradation tool-\nbox and its application to robustness evaluation. In Pro-\nceedings of the 14th International Society for Music In-\nformation Retrieval Conference (ISMIR) , pages 83–88,\nCuritiba, Brazil, November 2013.[16] M. Mauch, H. Fujihara, K. Yoshii, and M. Goto. Tim-\nbre and melody features for the recognition of vocal\nactivity and instrumental solos in polyphonic music. In\nProceedings of the 12th International Society for Mu-\nsic Information Retrieval Conference (ISMIR) , pages\n233–238, Miami, FL, USA, October 2011.\n[17] N. Ono, Z. Raﬁi, D. Kitamura, N. Ito, and A. Liutkus.\nThe 2015 signal separation evaluation campaign. In\nInternational Conference on Latent Variable Analy-\nsis and Signal Separation (LVA/ICA) , pages 387–395,\nLiberec, France, August 2015.\n[18] M. Ramona, G. Richard, and B. David. V ocal detection\nin music with support vector machines. In Proceedings\nof the 33rd IEEE International Conference on Acous-\ntics, Speech, and Signal Processing (ICASSP) , pages\n1885–1888, Las Vegas, NV , USA, March 2008.\n[19] A. M. Saxe, J. L. McClelland, and S. Ganguli. Ex-\nact solutions to the nonlinear dynamics of learning in\ndeep linear neural networks. In Proceedings of the 2nd\nInternational Conference on Learning Representations\n(ICLR) , Banff, Canada, April 2014.\n[20] J. Schlüter. Learning to pinpoint singing voice from\nweakly labeled examples. In Proceedings of the 17th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , New York City, NY , USA, Au-\ngust 2016.\n[21] J. Schlüter. Deep Learning for Event Detection, Se-\nquence Labelling and Similarity Estimation in Music\nSignals . PhD thesis, Johannes Kepler University Linz,\nAustria, July 2017.\n[22] J. Schlüter and T. Grill. Exploring data augmentation\nfor improved singing voice detection with neural net-\nworks. In Proceedings of the 16th International Society\nfor Music Information Retrieval Conference (ISMIR) ,\nMálaga, Spain, October 2015.\n[23] T. Sercu and V . Goel. Dense prediction on sequences\nwith time-dilated convolutions for speech recognition.\nInNIPS Workshop on End-to-end Learning for Speech\nand Audio Processing , Barcelona, Spain, November\n2016.\n[24] B. L. Sturm. A simple method to determine if a mu-\nsic information retrieval system is a “horse”. IEEE\nTransactions on Multimedia , 16(6):1636–1644, Octo-\nber 2014.\n[25] D. Ulyanov, A. Vedaldi, and V . S. Lempitsky. Instance\nnormalization: The missing ingredient for fast styliza-\ntion. arXiv e-prints , abs/1607.08022, July 2016.\n[26] Y . Wang, P. Getreuer, T. Hughes, R. F. Lyon, and R. A.\nSaurous. Trainable frontend for robust and far-ﬁeld\nkeyword spotting. In Proceedings of the 42nd IEEE\nInternational Conference on Acoustics, Speech, and\nSignal Processing (ICASSP) , pages 5670–5674, March\n2017. arXiv:1607.05666.326 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "A Single-Step Approach to Musical Tempo Estimation Using a Convolutional Neural Network.",
        "author": [
            "Hendrik Schreiber 0001",
            "Meinard Müller"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.3553592",
        "url": "https://doi.org/10.5281/zenodo.3553592",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/141_Paper.pdf",
        "abstract": "Global tempo annotations used for training of the tempo estimation CNN presented inA Single-step Approach to Musical Tempo Estimation using a Convolutional Neural Network.",
        "zenodo_id": 3553592,
        "dblp_key": "conf/ismir/SchreiberM18",
        "keywords": [
            "Tempo Estimation",
            "CNN",
            "Dataset",
            "Tempo Annotations"
        ],
        "content": "A SINGLE-STEP APPROACH TO MUSICAL TEMPO ESTIMATION USING\nA CONVOLUTIONAL NEURAL NETWORK\nHendrik Schreiber\ntagtraum industries incorporated\nhs@tagtraum.comMeinard M ¨uller\nInternational Audio Laboratories Erlangen\nmeinard.mueller@audiolabs-erlangen.de\nABSTRACT\nWe present a single-step musical tempo estimation system\nbased solely on a convolutional neural network (CNN).\nContrary to existing systems, which typically ﬁrst iden-\ntify onsets or beats and then derive a tempo, our sys-\ntem estimates the tempo directly from a conventional mel-\nspectrogram in a single step. This is achieved by fram-\ning tempo estimation as a multi-class classiﬁcation prob-\nlem using a network architecture that is inspired by con-\nventional approaches. The system’s CNN has been trained\nwith the union of three datasets covering a large variety of\ngenres and tempi using problem-speciﬁc data augmenta-\ntion techniques. Two of the three ground-truths are novel\nand will be released for research purposes. As input the\nsystem requires only 11:9 sof audio and is therefore suit-\nable for local as well as global tempo estimation. When\nused as a global estimator, it performs as well as or better\nthan other state-of-the-art algorithms. Especially the ex-\nact estimation of tempo without tempo octave confusion is\nsigniﬁcantly improved. As local estimator it can be used to\nidentify and visualize tempo drift in musical performances.\n1. INTRODUCTION\nUndoubtedly, the tempo of a musical piece is one of its\nmain characteristics. Its estimation is often deﬁned as mea-\nsuring the frequency with which humans “tap” along to the\nbeat. This is notably different from beat tracking , which\naims at determining individual beat positions. If the tempo\nof a musical piece stays constant throughout the whole per-\nformance, it is called global tempo . It can be represented\nby a single number usually speciﬁed in beats per minute\n(BPM). Global tempi often occur in genres like Rock, Pop,\nand Dance music. The method proposed in this paper was\nprimarily developed for estimating the tempo of short ex-\ncerpts, but can also be applied to global tempo estimation.\nMany different approaches to tempo estimation have\nbeen taken in the past. Gouyon et al. [11] provided a\ncomparative evaluation of the systems that participated in\nthe ISMIR 2004 contest, the ﬁrst large-scale evaluation of\n© Hendrik Schreiber, Meinard M ¨uller. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Hendrik Schreiber, Meinard M ¨uller. “A single-step ap-\nproach to musical tempo estimation using a convolutional neural net-\nwork”, 19th International Society for Music Information Retrieval Con-\nference, Paris, France, 2018.tempo induction algorithms. Five years later, Zapata and\nG´omez gave an updated overview [39]. To our knowledge,\nthe most recent comprehensive evaluations are presented\nin [2, 25, 31]. For a textbook-style introduction see [22].\nEarly tempo estimation methods often combined sig-\nnal processing with heuristics. Scheirer [28] for example\nused bandpass ﬁlters, followed by parallel comb ﬁlters, fol-\nlowed by peak picking. Klapuri et al. [17] replaced the\nconventional bandpass approach with STFTs, producing\n36band spectra. By differentiating and then half-wave rec-\ntifying the power in each band, they created band-speciﬁc\nonset strength signals ( OSS ), which were then combined\ninto four accent signals and fed into comb ﬁlters in or-\nder to detect periodicities. Instead of processing an OSS\nwith comb ﬁlters, several other methods have been pro-\nposed. Among them autocorrelation [1, 22], clustering of\ninter-onset intervals (IOI) [5, 33], and the discrete Fourier\ntransform (DFT) [22, 23].\nRecent approaches put emphasis on ﬁnding not just a\nperiodicity, but on ﬁnding one corresponding to the per-\nceived tempo, trying to avoid common errors by a factor\nof2or3, so-called octave errors [11, 31]. The meth-\nods used range from genre classiﬁcation (e.g., obtained\nby a genre classiﬁcation component) [14, 32], secondary\ntempo estimation [30], and the discrete cosine transform\nof IOI histograms [7], to machine learning approaches\nlike Gaussian mixture models (GMM) [24], support vec-\ntor machines (SVM) [9, 25], k-nearest neighbor classiﬁca-\ntion (k-NNC) [37, 38], neural networks [6], and random\nforests [31].\nAnother area of active research aims at creating a bet-\nterOSS through the use of neural networks. Elowsson [6]\nuses harmonic/percussive source separation and two differ-\nent feedforward neural networks to classify a frame as beat\nor non-beat. B ¨ock et al. [2] use a bidirectional long short-\nterm memory (BLSTM) recurrent neural network (RNN)\nto map spectral magnitude frames and their ﬁrst order dif-\nferences to beat activation values. These are then pro-\ncessed further with comb ﬁlters. For their dancing robot\napplication, Gkiokas et al. [10] use a convolutional neural\nnetwork (CNN) to derive a beat activation function, which\nis then used for beat tracking and tempo estimation.\nWhat all these methods have in common is the multi-\nstep approach of decomposing the signal into sub-bands,\nderiving some kind of OSS , detecting periodicities, and\nthen trying to pick the best one. As Humphrey et al. [15]\npoint out, this can be described as a deep architecture con-9840–50\n50–60\n60–70\n70–80\n80–90\n90–100\n100 –110\n110 –120\n120 –130\n130 –140\n140 –150\n150 –160\n160 –170\n170 –180\n180 –190\n190 –200\n200 –210\n210 –2200102030\nTempo intervals in BPM% of tracks\u0016= 121:32;\u001b= 30:52\nN= 8;596\nFigure 1 : Tempo distribution for the Train dataset con-\nsisting of LMD Tempo ,MTG Tempo , andEBall .\nsisting of multiple components (“layers”) that has evolved\nnaturally. But to the best of our knowledge, nobody has\nreplaced the traditional multi-component architecture with\na single deep neural network (DNN) yet. In this paper we\ndescribe a CNN-based approach that estimates the local\ntempo of a short musical piece or excerpt based on mel-\nscaled spectrograms in a single step, i.e., without explicitly\ncreating mid-level features like an OSS or a beat activation\nfunction that need to be processed further by another, sepa-\nrate system component. Using averaging, we can combine\nmultiple local tempi into a global tempo.\nThe remainder of this paper is structured as follows:\nSection 2 introduces our training datasets. Then Section 3\ndescribes the signal representation, network architecture,\nnetwork training, and how we combine multiple local esti-\nmates into a global estimate. In Section 4 we evaluate our\nglobal tempo estimation approach quantitatively by bench-\nmarking against known datasets and state-of-the-art algo-\nrithms. Then we discuss local tempo estimation qualita-\ntively using samples from different genres and eras. Fi-\nnally, in Section 5 we present our conclusions.\n2. TRAINING DATASETS\nOur goal is to create a general purpose system that does not\nsuffer from strong genre-bias. Therefore we avoid cross-\nvalidation on small datasets and instead created a large,\nmulti-genre training dataset, consisting of three smaller\ndatasets: One derived from a subset of the Lakh MIDI\ndataset ( LMD) [27], a subset of the GiantSteps MTG key\ndataset ( MTG Key ) [8]1, and a subset of the Extended\nBallroom [20] dataset. Two of the derived ground-truths\nhave been newly created for this paper.\n2.1 LMD Tempo\nLMD is a dataset containing MIDI ﬁles that have been\nmatched to 30 saudio excerpts. While some of the MIDI\nﬁles contain tempo information, none of the audio ﬁles are\nannotated, and there is no guarantee that associated MIDI\nand audio ﬁles have the same tempo. Our idea is to cre-\nate a sub-dataset, called LMD Tempo , that can be used for\ntraining supervised tempo induction algorithms. To this\n1https://github.com/GiantSteps/\nGiantSteps-mtg-key-datasetend, we estimated the tempo of the matched audio pre-\nviews using the algorithm from [31]. Then the associated\nMIDI ﬁles were parsed for tempo change messages. If the\nvalue of more than half the tempo messages for a given\npreview were within 2%of the estimated tempo, we as-\nsumed the estimated tempo of the audio excerpts to be cor-\nrect and added it to LMD Tempo . This resulted in 3;611\naudio tracks. We were able to match more than 76% of the\ntracks to the Million Song Dataset (MSD) genre annota-\ntions from [29]. Of the matched tracks 29% were labeled\nrock ,27%pop,5%r&b,5%dance ,5%country ,\n4%latin , and 3%electronic . Less than 2%of the\ntracks were labeled jazz ,soundtrack ,world and\nothers. Thus it is fair to characterize LMD Tempo as a\ngood cross-section of popular music.\n2.2 MTG Tempo\nTheMTG Key dataset was created by Faraldo [8] as a\nground-truth for key estimation of electronic dance mu-\nsic (edm), a genre that is very much underrepresented in\nLMD Tempo . Each two-minute track in MTG Key is an-\nnotated with one or more keys and a conﬁdence value\nc2f0;1;2gfor the key annotation. We annotated those\ntracks that have an unambiguous key and a conﬁdence of\nc= 2 with a manually tapped tempo, which makes it one\nof the very few datasets that is suitable for key andtempo\nestimation. The resulting dataset size is 1;159tracks. In\nthe following we will refer to this new ground-truth as MTG\nTempo .\n2.3 Extended Ballroom\nThe original Ballroom dataset [11] is still used as test\ndataset today, which is why we exclude it from train-\ning. Better suited is the recently released and much\nlarger Extended Ballroom dataset. Because it con-\ntains some songs also occurring in Ballroom , we use\nthe complement Extended Ballroom nBallroom .\nWe refer to the resulting dataset as EBall . It contains\n3;826 tracks with 30 s length each. EBall contributes\ntracks from genres that are underrepresented or simply ab-\nsent from both MTG Tempo andLMD Tempo .\n2.4 Combined Training Dataset\nCombined, LMD Tempo ,MTG Tempo , andEBall have\na size of 8;596 tracks with tempi ranging from 44to\n216 BPM (Figure 1). In the following we will call it\nTrain . The sweet octave (i.e., the tempo interval [\u001c;2\u001c)\nthat contains the most tracks [31]) for Train is77\u0000\n154 BPM , covering 84:4%of the items. The shortest in-\nterval that covers 99% of the items is 65\u0000204 BPM .\nEven though many different tempi are represented, Train\nis not tempo-balanced. More than 30% of its tracks have\ntempi in the [120;130) interval. Its mean is \u0016= 121:32\nand the standard deviation \u001b= 30:52. And while cover-\ning many different genres, Train is not genre-balanced,\neither. Genres like jazz andworld only have rela-\ntively few representatives. But despite these shortcomings,Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 99mel-spectrogram\t\nBN,\tCONV\t16x1x5,\tELU\tshort\tfilter\tconv\tlayers\t1x40x256\t\navg-pooling\t5x1\t\nBN,\tCONV\t16x1x5,\tELU\tBN,\tCONV\t16x1x5,\tELU\t\navg-pooling\t2x1\t\navg-pooling\t2x1\t\navg-pooling\t2x1\t\nmf_mod\tmf_mod\t\nmf_mod\tmf_mod\tmulti\tfilter\tmodules\t\tdense\tlayers\t\nstepwise\tpooling\talong\t\tthe\tfrequency\taxis\tFC\t64,\tELU\tBN,\tFC\t64,\tELU\tBN,\tFC\t256,\tsoftmax\t\nBN,\tDO\t0.5\t\nInput\tOutput\t\n\tBN\t\t\tBatch\tNormalization\t\t\tCONV\tConvolutional\tLayer\t\tFC\t\t\tFully\tConnected\tLayer\t\t\t\t\t\t\t\tDO\t\t\t\t\tDropout\t\nFigure 2 : Schematic overview of the network architecture.\nThree convolutional layers are followed by four mfmod\nmodules, which in turn are followed by four dense layers.\nTrain is a very rich, multi-faceted dataset and completely\nindependent from the test datasets we are going to use for\nevaluation in Section 4.1.\n3. METHOD\nOur proposed method for estimating a local tempo consists\nof a single step. Using a suitable representation we classify\nthe signal with a CNN, which produces a BPM value. We\nextend the system for global tempo estimation by averag-\ning the softmax activation function over different parts of\na full track.\n3.1 Signal Representation\nAlthough we believe that it is possible to build a system\nlike ours with raw audio as input [4,19], we choose to rep-\nresent the signal as mel-scaled magnitude spectrogram to\nreduce the amount of data that needs to be processed by\nthe CNN. The mel-scale as opposed to a linear scale was\nchosen for its relation to human perception and instrument\nfrequency ranges.\nTo create the spectrogram, we convert the signal to\nmono, downsample to 11;025 Hz and use half-overlapping\nwindows of 1;024 samples. This is equivalent to a\navg-pooling\t\nBN\t\nCONV\t24x1x32\t\nCONV\t24x1x64\t\nCONV\t24x1x96\t\nCONV\t24x1x128\t\nCONV\t24x1x192\t\nCONV\t24x1x256\t\nconcatenation\t\nmf_mod\t\nCONV\t36x1x1\t\nnext\tlayer\t\nprevious\tlayer\t\nFigure 3 : Each multi-ﬁlter module mfmod consists of a\npooling layer, batch normalization, six different convolu-\ntional layers, a concatenation layer and a bottleneck layer.\nThe activation function for all convolutional layers is ELU .\nframe rate of 21:5 Hz , which (according to the Nyquist-\nShannon sampling theorem) sufﬁces to represent tempi up\nto646 BPM —well above the tempi we usually ﬁnd in mu-\nsic. Each window is transformed into a 40band mel-scaled\nmagnitude spectrum covering 20\u00005;000 Hz by applying\na Hamming window, the DFT, and a suitable ﬁlterbank.\nSince musical tempo is not an instantaneous quantity, we\nrequire a spectrogram of a musically sufﬁcient length. As\nsuch we choose 256frames, equivalent to \u001911:9 s.\n3.2 Network Architecture\nEven though tempo estimation appears to be a regression\nproblem, we are approaching it as a classiﬁcation prob-\nlem for two reasons. First, a probability distribution over\nmultiple classes allows us to judge how reliable a given es-\ntimate is. Additionally, such a distribution is naturally ca-\npable of representing tempo ambiguities [21], allowing for\nthe estimation of a second best tempo. Second, in infor-\nmal experiments we found that a classiﬁcation-based ap-\nproach led to more stable results compared to a regression-\nbased approach. So instead of attempting to estimate a\nBPM value as decimal number, we are choosing one of\n256tempo classes, covering the integer tempo values from\n30to285 BPM .\nThe proposed network architecture (Figure 2) is in-\nspired by the traditional approach of ﬁrst creating an OSS ,\nwhich is then analyzed for periodicities. In our approach,\nwe ﬁrst process the input with three convolutional layers\nwith 16 (1\u00025)ﬁlters each. All ﬁlters are oriented along\nthe time axis using padding and a stride of 1. Using these\nfairly short ﬁlters, we hope to match onsets in the signal.\nThese three layers are followed by four almost identical\nmulti-ﬁlter modules ( mfmod, Figure 3) each consisting of\nan average pooling layer (m\u00021), parallel convolutional\nlayers with different ﬁlter lengths ranging from (1\u000232)\nto(1\u0002256) , a concatenation layer and a (1\u00021)bottle-\nneck layer for dimensionality reduction. With each of these100 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 4 : Scale-&-crop data augmentation. During train-\ning, the mel-spectrogram is ﬁrst stretched or compressed\nalong the time axis, which requires an adjustment of the\nground-truth label, and then cropped to 256 frames at a\nrandomly chosen offset.\nmodules we are trying to achieve two goals: 1) Pooling\nalong the frequency axis to summarize mel-bands, and 2)\nmatching the signal with a variety of ﬁlters that are capable\nof detecting long temporal dependencies. Using parallel\nconvolutional layers with different ﬁlter lengths has been\ninspired by [26, 35]. In a traditional system, this could be\nregarded as some sort of comb ﬁlterbank\nTo classify the features delivered by the convolutional\nlayers, we add two fully connected layers ( 64units each)\nfollowed by an output layer with 256 units. The output\nlayer uses softmax as activation function, while all other\nlayers use ELU [3]. Each convolutional or fully connected\nlayer is preceded by batch normalization [16]. The ﬁrst\nfully connected layer is additionally preceded by a dropout\nlayer withp= 0:5to counter overﬁtting. As loss function\nwe use categorical cross-entropy. Overall, the network has\n2;921;042trainable parameters.\n3.3 Network Training\nWe use 90% ofTrain for training and 10% for valida-\ntion. To counter the tempo class imbalance and, at the same\ntime, augment the dataset during training, for each epoch,\nwe use a scale-&-crop-approach borrowed from image\nrecognition systems (see e.g., [34]). Contrary to regular\nimages, the two dimensions of spectrograms have very dif-\nferent meaning, which is why we cannot simply scale-&-\ncrop indiscriminately. Instead, we have to be careful to ei-\nther not change the labeled meaning of a sample or change\nits label suitably (Figure 4). In our case this means that\nwe have to preserve the properties of the frequency axis,\nbut may manipulate the time axis. Concretely, we scale\nthe time axis of the samples’ mel-spectrograms with a ran-\ndomly chosen factor 2f0:8;0:84;0:88;:::; 1:16;1:2gus-\ning spline interpolation and adjust the ground-truth tempo\nlabels accordingly. This substantially increases the number\n(a) “Honky Tonk Women” by The Rolling Stones\n(b) “Rolling in the Deep” by Adele\n(c) “Typhoon” by Foreign Beggars/Chasing Shadows\nFigure 5 : Tempo class probabilities for tracks from differ-\nent genres and eras. (a) The tempo drift of the performance\nis clearly visible: the track starts with 108 BPM and ends\nwith 125 BPM. (b) Very stable tempo of a modern pop\nmusic production. (c) Dubstep track with several no beat\npassages, a very active middle section, and halve tempo\nintro and outro.\nof different samples we can present to the network. Since\nthe full mel-spectrogram for a sample is longer than the\nnetwork input layer (e.g., covering 60 svs.11:9 s), we crop\neach scaled sample at a randomly chosen time axis offset\nto ﬁt the input layer. This again drastically increases the\nnumber of different samples we can offer to the network.\nAfter scaling and cropping, the values of the resulting sub-\nspectrogram are rescaled to [0;1]. In order to ensure com-\nparability, time-axis augmentations are skipped during val-\nidation.\nWe deﬁne Accuracy0 as the fraction of estimates that\nare correct when rounding decimal ground-truth labels to\nthe nearest integer. To avoid overﬁtting, we train un-\ntilAccuracy0 for the validation set has not improved for\n20epochs using Adam (with a learning rate of 0:001,\n\f1= 0:9;\f2= 0:999;\u000f= 1e\u00008) as optimizer, and then\nkeep the model that achieved the highest validation Accu-\nracy0 (early stopping).Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 101Dataset schr b ¨ock new\nACM Mirum 38.3 29.4- 40.6\nISMIR04 37.7 27.2- 34.1\nBallroom 46.8- 33.8- 67.9\nHainsworth 43.7 33.8 43.2\nGTzan 38.8 32.2- 36.9\nSMC 14.3 17.1 12.4\nGiantSteps 53.5- 37.2- 59.8\nCombined 40.9- 31.2- 44.8\nDS Average 39.0 30.1 42.1\n(a)Accuracy0Dataset schr b ¨ock new\nACM Mirum 72.3- 74.0- 79.5\nISMIR04 63.4 55.0 60.6\nBallroom 64.6- 84.0- 92.0\nHainsworth 65.8- 80.6 77.0\nGTzan 71.0 69.7 69.4\nSMC 31.8 44.7+ 33.6\nGiantSteps 63.1- 58.9- 73.0\nCombined 66.5- 69.5- 74.2\nDS Average 61.7 66.7 69.3\n(b)Accuracy1Dataset schr b ¨ock new\nACM Mirum 97.3 97.7 97.4\nISMIR04 92.2 95.0 92.2\nBallroom 97.0 98.7 98.4\nHainsworth 85.6 89.2+ 84.2\nGTzan 93.3 95.0+ 92.6\nSMC 55.3 67.3+ 50.2\nGiantSteps 88.7 86.4- 89.3\nCombined 92.2 93.6+ 92.1\nDS Average 87.1 89.9 86.4\n(c)Accuracy2\nTable 1 : Accuracies in percent. The ‘ +’ and ‘\u0000’ signs indicate a statistically signiﬁcant difference between either schr\norb¨ock, andnew. Bold numbers mark the best-performing algorithm(s) for a dataset. DS Average is the mean of the\nalgorithms’ results for each dataset.\n3.4 Global Tempo Estimation\nSince the input layer is usually shorter than the mel-\nspectrogram of a whole track, it estimates merely a local\ntempo. To estimate the global tempo for a track, we cal-\nculate multiple output activations using a sliding window\nwith half-overlap, i.e., a hop size of 128frames\u00195:96 s.\nThe activations are averaged class-wise and then—just like\nin the local approach—the tempo class with the greatest\nactivation is picked as the result.\n4. EVALUATION\nFor evaluation, we trained three models and chose the one\nwith the highest Accuracy0 measured against the valida-\ntion set as our ﬁnal model. As metrics we used Accuracy0\nas well as Accuracy1 andAccuracy2 , which are typically\nused for evaluating tempo estimation systems. Accuracy1\nis deﬁned as the fraction of estimates identical to reference\nvalues while allowing a 4%tolerance. Accuracy2 is the\npercentage of correct estimates allowing for octave errors\n2and3again using a 4%tolerance.\n4.1 Global Tempo Benchmarking\nIt has become customary to benchmark tempo estimation\nmethods with results reported for a small set of datasets:\nACM Mirum [24], Ballroom [11], GTzan [36],\nHainsworth [12], ISMIR04 [11], GiantSteps\nTempo [18], and SMC [13]. The latter was speciﬁcally de-\nsigned to be difﬁcult for beat trackers. Where applicable,\nwe used the corrected annotations from [25]. A detailed\ndescription of the datasets is given in [31]. We refer to the\nunion of these seven datasets as Combined . Unweighted\naverages of results for all seven datasets will be referred\nto as DS Average . We benchmarked our approach\nnew with the algorithms by B ¨ock et al. ( b¨ock) [2]2and\nSchreiber ( schr ) [31]. Table 1 shows the results.\nOverall, new achieves the highest results when tested\nagainst Combined with the strict metrics Accuracy0\n(44:8 %) and Accuracy1 (74:2 %). Both accuracy values\nare slightly lower when summarized as DS Average .\n2madmom-0.15.1, default options, available at https://github.\ncom/CPJKU/madmomWhen testing with octave-error tolerance, i.e., Accuracy2 ,\nb¨ock reaches 93:6 % forCombined , versus 92:2 %\nreached by schr , and 92:1 %reached by new. In essence,\nnew is better than b¨ock at estimating the tempo octave\ncorrectly, while b¨ock—and to a lesser degree schr —\nachieve a slightly higher accuracy when ignoring the met-\nrical level. This may be due to the fact that both b¨ock and\nschr use a traditional periodicity analysis (DFT and comb\nﬁlters, respectively) that tends to be prone to octave errors,\nwhile new does not use a comparable isolated component.\nWhen inspecting the dataset-speciﬁc results, we\nﬁnd that new’sAccuracy1 is particularly high for\nBallroom (92:0 %),GiantSteps (73:0 %), and\nACM Mirum (79:5 %). In fact, they are signiﬁ-\ncantly higher than b¨ock’s (+8:0 pp/+14:1 pp/+5:5 pp)\norschr ’s (+27:4 pp/+9:9 pp/+7:2 pp) results. Both the\nBallroom andGiantSteps values can be explained\nthrough our training dataset. They clearly correspond to\nEBall andMTG Tempo , therefore high values are not\nsurprising. We believe the same is true for ACM Mirum\nandLMD Tempo . To us these results indicate that a genre-\ncomplete training set may lead to better results for the other\ndatasets as well. This hypothesis is supported by the fact\nthatGTzan contains genres like reggae ,classical ,\nblues , and jazz , and Hainsworth contains the gen-\nreschoral ,classical ,folk , and jazz —none of\nwhich are well represented in Train . For both datasets\nnew performs worse than b¨ock orschr . A similar con-\nnection may exist for b¨ock andGiantSteps —as far as\nwe know, b¨ock has not been trained on edm.\n4.2 Local Tempo Visualization\nTo illustrate the system’s performance for continuous lo-\ncal tempo estimation, we analyzed several tracks from dif-\nferent genres using overlapping windows with a relatively\nsmall hop size of 32frames, i.e.,\u00191:5seconds. For clar-\nity, we cropped the images at 50 and 150 BPM. Figure 5a\nbeautifully reveals the tempo drift in The Rolling Stone’s\n1969 performance of “Honky Tonk Women”, starting out\nat 108 BPM and ending in 125 BPM. In contrast, Adele’s\nrecent studio production “Rolling in the Deep” (Figure 5b)\nstays very stable at 105 BPM. A more complicated picture\nis presented by the dubstep track “Typhoon” by Foreign102 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Beggars/Chasing Shadows (Figure 5c). After several sec-\nonds of weather noises, the intro starts with 70 BPM. The\nmain part’s tempo is clearly 140 BPM interrupted by two\nsections with no beat. The outro again feels like 70 BPM\nfollowed by a fade out.\n5. CONCLUSIONS\nWe have presented a single-step tempo estimation system\nconsisting of a convolutional neural network (CNN). With\na conventional mel-spectrogram as input, the system is ca-\npable of estimating the musical tempo using multi-class\nclassiﬁcation. The network’s architecture consolidates tra-\nditional multi-step approaches into a single CNN, avoid-\ning explicit mid-level features such as onset strength sig-\nnals (OSS) or beat activation functions. Consequently and\ncontrary to many other systems, our approach does not\nrely on handcrafted features or ad-hoc heuristics, but is\ncompletely data-driven. The system was trained with sam-\nples from the union of several large datasets, two of which\nwere newly created. To aid training, we applied problem-\nspeciﬁc data augmentation techniques. For global tempo\nestimation, we have shown that our single network, data-\ndriven approach performs as well as or better than other\nmore complicated state-of-the-art systems, especially w.r.t.\nAccuracy1 . Furthermore, by visualizing examples for lo-\ncal tempo estimations, we have demonstrated qualitatively\nhow the system can aid music analysis, e.g., to identify\ntempo drift.\nWe believe that the system can be improved even fur-\nther by training with a more balanced dataset that con-\ntains tracks for all tested genres. Notably missing from the\ncurrent training set are jazz ,classical , orreggae\ntracks. Another area of potential improvement is the net-\nwork architecture. Shorter ﬁlters, dilated convolutions,\nresidual connections, and a suitable replacement for the\nfully connected layers might be used to reduce the number\nof parameters and thus the number of operations needed\nfor training and estimation.\nAdditional Material\nDatasets are available at http://www.tagtraum.\ncom/tempo_estimation.html . Code to estimate\ntempi and create tempograms is available at https://\ngithub.com/hendriks73/tempo-cnn .\nAcknowledgments\nThe International Audio Laboratories Erlangen are a\njoint institution of the Friedrich-Alexander-Universit ¨at\nErlangen-N ¨urnberg (FAU) and Fraunhofer Institute for In-\ntegrated Circuits IIS. Meinard M ¨uller is supported by the\nGerman Research Foundation (DFG MU 2686/11-1).\n6. REFERENCES\n[1] Miguel Alonso, Bertrand David, and Ga ¨el Richard.\nTempo and beat estimation of musical signals. In Pro-\nceedings of the International Conference on Music In-\nformation Retrieval (ISMIR) , Barcelona, Spain, 2004.\n[2] Sebastian B ¨ock, Florian Krebs, and Gerhard Widmer.Accurate tempo estimation based on recurrent neu-\nral networks and resonating comb ﬁlters. In Proceed-\nings of the 16th International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , pages 625–631,\nM´alaga, Spain, 2015.\n[3] Djork-Arn ´e Clevert, Thomas Unterthiner, and Sepp\nHochreiter. Fast and accurate deep network learning by\nexponential linear units (elus). In International Confer-\nence on Learning Representations (ICLR) , San Juan,\nPuerto Rico, Feb. 2015.\n[4] Sander Dieleman and Benjamin Schrauwen. End-to-\nend learning for music audio. In Proceedings of the\nIEEE International Conference on Acoustics, Speech,\nand Signal Processing (ICASSP) , pages 7014–7018,\nFlorence, Italy, 2014. IEEE.\n[5] Simon Dixon. Automatic extraction of tempo and beat\nfrom expressive performances. Journal of New Music\nResearch , 30:39–58, 2001.\n[6] Anders Elowsson. Beat tracking with a cepstroid in-\nvariant neural network. In Proceedings of the 17th In-\nternational Society for Music Information Retrieval\nConference (ISMIR) , pages 351–357, New York, NY ,\nUSA, 2016.\n[7] Anders Elowsson and Anders Friberg. Modeling the\nperception of tempo. The Journal of the Acoustical So-\nciety of America , 137(6):3163–3177, 2015.\n[8]´Angel Faraldo, Sergi Jord `a, and Perfecto Herrera. A\nmulti-proﬁle method for key estimation in EDM. In\nProceedings of the AES International Conference on\nSemantic Audio , Erlangen, Germany, June 2017. Au-\ndio Engineering Society.\n[9] Aggelos Gkiokas, Vassilios Katsouros, and George\nCarayannis. Reducing tempo octave errors by periodic-\nity vector coding and svm learning. In Proceedings of\nthe 13th International Society for Music Information\nRetrieval Conference (ISMIR) , pages 301–306, Porto,\nPortugal, 2012.\n[10] Aggelos Gkiokas and Vassilis Katsouros. Convolu-\ntional neural networks for real-time beat tracking: A\ndancing robot application. In Proceedings of the 18th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 286–293, Suzhou, China,\nOctober 2017.\n[11] Fabien Gouyon, Anssi P. Klapuri, Simon Dixon,\nMiguel Alonso, George Tzanetakis, Christian Uhle,\nand Pedro Cano. An experimental comparison of au-\ndio tempo induction algorithms. IEEE Transactions on\nAudio, Speech, and Language Processing , 14(5):1832–\n1844, 2006.\n[12] Stephen Webley Hainsworth. Techniques for the Auto-\nmated Analysis of Musical Audio . PhD thesis, Univer-\nsity of Cambridge, UK, September 2004.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 103[13] Andre Holzapfel, Matthew E.P. Davies, Jos ´e R. Zap-\nata, Jo ˜ao Lobato Oliveira, and Fabien Gouyon. Selec-\ntive sampling for beat tracking evaluation. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n20(9):2539–2548, 2012.\n[14] Florian H ¨orschl ¨ager, Richard V ogl, Sebastian B ¨ock,\nand Peter Knees. Addressing tempo estimation octave\nerrors in electronic music by incorporating style in-\nformation extracted from wikipedia. In Proceedings of\nthe Sound and Music Computing Conference (SMC) ,\nMaynooth, Ireland, 2015.\n[15] Eric J. Humphrey, Juan Pablo Bello, and Yann Le-\nCun. Moving beyond feature design: Deep architec-\ntures and automatic feature learning in music informat-\nics. In Proceedings of the 13th International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 403–408, Porto, Portugal, 2012.\n[16] Sergey Ioffe and Christian Szegedy. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint\narXiv:1502.03167 , 2015.\n[17] Anssi P. Klapuri, Antti J. Eronen, and Jaakko Astola.\nAnalysis of the meter of acoustic musical signals. IEEE\nTransactions on Audio, Speech and Language Process-\ning, 14(1):342–355, 2006.\n[18] Peter Knees, ´Angel Faraldo, Perfecto Herrera, Richard\nV ogl, Sebastian B ¨ock, Florian H ¨orschl ¨ager, and Mick-\nael Le Goff. Two data sets for tempo estimation and\nkey detection in electronic dance music annotated from\nuser corrections. In Proceedings of the 16th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 364–370, M ´alaga, Spain, October\n2015.\n[19] Jongpil Lee, Jiyoung Park, Keunhyoung Luke Kim,\nand Juhan Nam. Sample-level deep convolutional neu-\nral networks for music auto-tagging using raw wave-\nforms. In Proceedings of the Sound and Music Com-\nputing Conference (SMC) , pages 220–226, Espoo, Fin-\nland, July 2017.\n[20] Ugo Marchand and Geoffroy Peeters. The extended\nballroom dataset. In Late Breaking Demo of the Inter-\nnational Conference on Music Information Retrieval\n(ISMIR) , New York, NY , USA, 2016.\n[21] Martin F. McKinney and Dirk Moelants. Deviations\nfrom the resonance theory of tempo induction. In Pro-\nceedings of the Conference on Interdisciplinary Musi-\ncology , Graz, Austria, 2004.\n[22] Meinard M ¨uller. Fundamentals of Music Processing\n– Audio, Analysis, Algorithms, Applications . Springer\nVerlag, 2015.\n[23] Geoffroy Peeters. Template-based estimation of time-\nvarying tempo. EURASIP Journal on Advances in Sig-\nnal Processing , 2007(1):158–158, 2007.[24] Geoffroy Peeters and Joachim Flocon-Cholet. Percep-\ntual tempo estimation using GMM-regression. In Pro-\nceedings of the second international ACM workshop\non Music information retrieval with user-centered and\nmultimodal strategies (MIRUM) , pages 45–50, New\nYork, NY , USA, 2012. ACM.\n[25] Graham Percival and George Tzanetakis. Streamlined\ntempo estimation based on autocorrelation and cross-\ncorrelation with pulses. IEEE/ACM Transactions on\nAudio, Speech and Language Processing (TASLP) ,\n22(12):1765–1776, 2014.\n[26] Jordi Pons and Xavier Serra. Designing efﬁcient archi-\ntectures for modeling temporal features with convolu-\ntional neural networks. In Proceedings of the IEEE In-\nternational Conference on Acoustics, Speech, and Sig-\nnal Processing (ICASSP) , pages 2472–2476, New Or-\nleans, USA, March 2017. IEEE.\n[27] Colin Raffel. Learning-Based Methods for Comparing\nSequences, with Applications to Audio-to-MIDI Align-\nment and Matching . PhD thesis, Columbia University,\n2016.\n[28] Eric D. Scheirer. Tempo and beat analysis of acoustical\nmusical signals. Journal of the Acoustical Society of\nAmerica , 103(1):588–601, 1998.\n[29] Hendrik Schreiber. Improving genre annotations for\nthe million song dataset. In Proceedings of the 16th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 241–247, M ´alaga, Spain,\n2015.\n[30] Hendrik Schreiber and Meinard M ¨uller. Exploiting\nglobal features for tempo octave correction. In Pro-\nceedings of the IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\npages 639–643, Florence, Italy, 2014.\n[31] Hendrik Schreiber and Meinard M ¨uller. A post-\nprocessing procedure for improving music tempo esti-\nmates using supervised learning. In Proceedings of the\n18th International Society for Music Information Re-\ntrieval Conference (ISMIR) , pages 235–242, Suzhou,\nChina, October 2017.\n[32] Bj ¨orn Schuller, Florian Eyben, and Gerhard Rigoll.\nTango or waltz?: Putting ballroom dance style into\ntempo detection. EURASIP Journal on Audio, Speech,\nand Music Processing , 2008:12, 2008.\n[33] Jarno Sepp ¨anen. Tatum grid analysis of musical sig-\nnals. In Proceedings of the IEEE Workshop on Appli-\ncations of Signal Processing to Audio and Acoustics\n(WASPAA) , pages 131–134, 2001.\n[34] Patrice Y . Simard, David Steinkraus, John C. Platt,\net al. Best practices for convolutional neural networks\napplied to visual document analysis. In Proceedings of104 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018the 7th International Conference on Document Anal-\nysis and Recognition (ICDAR) , volume 3, pages 958–\n962, August 2003.\n[35] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-\nmanet, Scott Reed, Dragomir Anguelov, Dumitru Er-\nhan, Vincent Vanhoucke, Andrew Rabinovich, et al.\nGoing deeper with convolutions. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) ,\npages 1–9, Boston, MA, USA, June 2015.\n[36] George Tzanetakis and Perry Cook. Musical genre\nclassiﬁcation of audio signals. IEEE Transactions on\nSpeech and Audio Processing , 10(5):293–302, 2002.\n[37] Fu-Hai Frank Wu. Musical tempo octave error re-\nducing based on the statistics of tempogram. In 23th\nMediterranean Conference on Control and Automation\n(MED) , pages 993–998, Torremolinos, Spain, 2015.\nIEEE.\n[38] Fu-Hai Frank Wu and Jyh-Shing Roger Jang. A super-\nvised learning method for tempo estimation of musi-\ncal audio. In 22nd Mediterranean Conference of Con-\ntrol and Automation (MED) , pages 599–604, Palermo,\nItaly, 2014. IEEE.\n[39] Jose R. Zapata and Emilia G ´omez. Comparative eval-\nuation and combination of audio tempo estimation ap-\nproaches. In 42nd AES Conference on Semantic Audio ,\nIlmenau, Germany, 2011.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 105"
    },
    {
        "title": "A Crowdsourced Experiment for Tempo Estimation of Electronic Dance Music.",
        "author": [
            "Hendrik Schreiber 0001",
            "Meinard Müller"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.3553625",
        "url": "https://doi.org/10.5281/zenodo.3553625",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/220_Paper.pdf",
        "abstract": "Raw beat and derived tempo annotationsfromA Crowdsourced Experiment for Tempo Estimation of Electronic Dance Music.",
        "zenodo_id": 3553625,
        "dblp_key": "conf/ismir/SchreiberM18a",
        "keywords": [
            "Crowdsourced",
            "Experiment",
            "Tempo",
            "Electronic",
            "Music",
            "Raw",
            "Beat",
            "Derived",
            "Tempo",
            "Annotations"
        ],
        "content": "A CROWDSOURCED EXPERIMENT FOR TEMPO ESTIMATION OF\nELECTRONIC DANCE MUSIC\nHendrik Schreiber\ntagtraum industries incorporated\nhs@tagtraum.comMeinard M ¨uller\nInternational Audio Laboratories Erlangen\nmeinard.mueller@audiolabs-erlangen.de\nABSTRACT\nRelative to other datasets, state-of-the-art tempo estima-\ntion algorithms perform poorly on the GiantSteps Tempo\ndataset for electronic dance music (EDM). In order to in-\nvestigate why, we conducted a large-scale, crowdsourced\nexperiment involving 266 participants from two distinct\ngroups. The quality of the collected data was evaluated\nwith regard to the participants’ input devices and back-\nground. In the data itself we observed signiﬁcant tempo\nambiguities, which we attribute to annotator subjectivity\nand tempo instability. As a further contribution, we then\nconstructed new annotations consisting of tempo distri-\nbutions for each track. Using these annotations, we re-\nevaluated two recent state-of-the-art tempo estimation sys-\ntems achieving signiﬁcantly improved results. The main\nconclusions of this investigation are that current tempo es-\ntimation systems perform better than previously thought\nand that evaluation quality needs to be improved. The new\ncrowdsourced annotations will be released for evaluation\npurposes.\n1. INTRODUCTION\nEstimation of a music piece’s global tempo is a classic mu-\nsic information retrieval (MIR) task. It is often deﬁned as\nestimating the frequency with which humans tap along to\nthe beat. A necessary precondition for successful global\ntempo estimation is the existence of a stable tempo as it\noften occurs in rock, pop, or dance music. To evaluate\na tempo estimation system one needs the system itself, a\ndataset with suitable tempo annotations, and one or more\nmetrics. One such dataset, named GiantSteps Tempo , has\nbeen released by Knees et al. in 2015 [6]. It was created by\nscraping a forum that let listeners discuss Beatport1songs\nwith wrong tempo labels. Scraping was done via a script\nand15% of the labels were manually veriﬁed. All 664\ntracks in the dataset belong to the umbrella genre electronic\ndance music (EDM) with its subgenres trance, drum-and-\nbass, techno, etc. Since its release, several academic and\n1http://www.beatport.com/ , an online music store\n© Hendrik Schreiber, Meinard M ¨uller. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Hendrik Schreiber, Meinard M ¨uller. “A Crowdsourced\nExperiment for Tempo Estimation of Electronic Dance Music”, 19th In-\nternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.commercial tempo estimation systems have been tested\nagainst the dataset (e.g. [12]). As is common for datasets\nannotated with only a single tempo per track, the two met-\nricsAccuracy1 andAccuracy2 were used. Accuracy1 is\ndeﬁned as the fraction of correct estimates while allowing\na tolerance of 4%.Accuracy2 additionally allows estimates\nto be wrong by a factor of 2,3,1=2or1=3(so-called octave\nerrors ). The highest results reported for the GiantSteps\ndataset are 77:0%Accuracy1 by the applications NI Trak-\ntor Pro 22(with octave bias 88\u0000175) and 90:2%Accu-\nracy2 by CrossDJ3(with octave bias 75\u0000150).4These\nresults are surprisingly low—the highest reported Accu-\nracy2 values for other commonly used datasets like ACM\nMirum [10], Ballroom [4], and GTzan [13] are greater than\n95% [1]. Since EDM is often associated with repeating\nbass drum patterns and steady tempi [2, 7], it should be\ncomparatively easy to estimate the tempo for this genre.\nWe hypothesize that relatively low accuracy values were\nachieved for multiple possible reasons. Since the annota-\ntions were scraped off a forum for disputed tempo labels,\nthe dataset may contain many tracks that are especially\nhard to annotate for humans. And if not difﬁcult for hu-\nmans to annotate, it is conceivable that the tracks are par-\nticularly hard for algorithms to analyze. Lastly, if neither\nhumans nor algorithms fail, perhaps some of the scraped\nannotations are simply wrong.\nIn this paper we investigate why tempo estimation sys-\ntems perform so poorly for GiantSteps Tempo . To this end,\nwe conducted a large, crowdsourced experiment to collect\nnew tempo data for GiantSteps Tempo from human partic-\nipants. The experiment is described in detail in Section 2.\nThe data is analyzed in Section 3 and used to create a new\nground-truth. This ground-truth is then compared to the\noriginal ground-truth and used to evaluate two recent al-\ngorithms. The results are discussed in Section 4. Finally,\nin Section 5, we summarize our ﬁndings and draw conclu-\nsions.\n2. EXPERIMENT\nIn order to generate a new ground-truth for the GiantSteps\nTempo dataset, we set up a web-based experiment in which\n2https://www.native-instruments.com/en/\nproducts/traktor/dj-software/traktor-pro-2/\n3http://www.mixvibes.com/\ncross-dj-software-mac-pc/\n4More benchmark results are available at http://www.cp.jku.\nat/datasets/giantsteps/409we asked participants to tap along to audio excerpts using\ntheir keyboard or touchscreen. The user interface for this\nexperiment is depicted in Figure 1. Since most tracks from\nthe dataset are 2 min long and tapping for the full dura-\ntion is difﬁcult, we split each track into half-overlapping\n30 s segments. Out of the 664 tracks we created 4;640\nsuch segments (in most cases 7per track). To measure\ntempo, it is not important for tap and beat to occur at the\nsame time. In contrast to experiments for beat tracking,\nphase shifts, input method latencies, or anticipatory early\ntapping—known as negative mean asynchrony (NMA)—\nare irrelevant, as long as they stay constant (see [11] for an\noverview of tapping and [3,5] for beat tracking). Therefore\nparticipants were asked to tap along to randomly chosen\nsegments as steadily as possible , over the entire duration of\n30 swithout skipping beats. To encourage steady tapping,\nthe user interface gave immediate feedback in the form\nof the mean tempo \u0016in BPM, the median tempo med in\nBPM, the standard deviation of the inter-tap-intervals (ITI)\n\u001bin milliseconds, as well as textual messages and emo-\njis (Figure 1). When calculating the standard deviation,\nthe ﬁrst three taps were ignored, as those are typically of\nlow quality (users have to ﬁnd their way into the groove).\nWhen the standard deviation \u001bstayed very low, smilies,\nthumbs up and textual praise were shown. When \u001bclimbed\nabove a certain threshold, the user was shown sad faces\nand messages like “Did you miss a beat? Try to tap more\nsteadily.” To prevent low quality submissions, users were\nonly allowed to proceed to the next track, once four condi-\ntions were met:\n1. 20 or more taps\n2. Taps cover at least 15 s\n3. ITI standard deviation: \u001b<50 ms\n4. Median tempo: 50\u0014med\u0014210 BPM\nWhile the ﬁrst three conditions were not explicitly com-\nmunicated, the instructions made participants aware that\nthe target tempo lies between 50and210 BPM . Once all\nfour conditions were met, a large red bar turned green and\ntheNext button became enabled. For situations in which\nthe user was not able to fulﬁll all conditions, the user inter-\nface offered a No Beat checkbox. Once checked, it allowed\nusers to bypass the quality check and proceed to the next\nsong. It must be noted that there is a tradeoff between en-\ncouraging participants to tap well (i.e. steadily) and a bias\ntowards stable tempi. We opted for this design for two rea-\nsons. 1) tempo in EDM is usually is very steady [2, 7].\n2) the bias is limited to individual tapping sessions at the\nsegment level, i.e. we can still detect tempo stability prob-\nlems on the track level by aggregating segment level anno-\ntations.\nParticipants were recruited from two distinct groups:\nAcademics and people interested in the consumer-level\nmusic library management system beaTunes5. We refer to\nthe former group as academics and the latter as beaTunes .\nWhile members of the academics group were asked to help\n5https://www.beatunes.com/\nFigure 1 : Illustration of the web-based interface used in\nour experimental user study.\nin this experiment via relevant mailing lists without offer-\ning any beneﬁts, members of the beaTunes group were in-\ncentivized by promising a reward license for the beaTunes\nsoftware, if they submitted 110valid annotations. While\nit was not explicitly speciﬁed what a “valid annotation” is,\nwe attempted to steer people in the right direction using in-\nstructions and the instant feedback mechanisms described\nabove (Figure 1).\n3. DATA ANALYSIS\nOver a period of 21=2months 266 persons participated\nin the experiment, 217 (81:6%) belonging to beaTunes\nand49 (18:4%) toacademics . Together they submitted\n18;684segment annotations ( avg = 4:03=segment ). We\nmade sure that all segments were annotated at least twice.\nSince some segments are harder to annotate than others,\nwe monitored submissions and ensured that segments an-\nnotated by participants as very different from the origi-\nnal ground-truth—exceeding a tolerance of 4%—were pre-\nsented to participants more often than others. The vast ma-\njority of annotations was submitted by the beaTunes group\n(95:1%). Overall 7:5%of all submissions were marked\nwith No Beat . With 7:6%theNo Beat -rate was slightly\nhigher among members of the beaTunes group. Members\nofacademics checked No Beat only for 5:2%of their sub-\nmissions. Since the experiment was run in the participant’s\nweb-browser, the browser’s user-agent for each submission\nwas logged by the web-server. Among other information\nthe user-agent contains the name of the participant’s op-\nerating system. 17;012(91:1%) of the submissions were\nsent from desktop operating systems that are typically con-\nnected to a physical keyboard. 1;672(8:9%) were from\nmobile operating systems that are usually associated with\ntouchscreens. Participants interested in a reward license,\nalso had to enter name and email. Both datapoints have410 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Dataset Split + -p-value\n\u0006academics 0:0074 0:0090 3:11e\u000029\n\u0006keyboard 0:0088 0:0095 9:74e\u00007\nbeaTunes \u0006keyboard 0:0089 0:0099 6:71e\u000010\nacademics \u0006keyboard 0:0074 0:0073 8:68e\u00001\nTable 1 : Average coefﬁcients of variation cvfor dataset\nsplits, academics or not, keyboard or not, and keyboard or\nnot for either beaTunes oracademics . The lowp-values\nindicate a signiﬁcant difference between the dataset splits.\nbeen removed from the collected data to ensure anonymity.\nWe analyzed the submitted data to ﬁnd out whether we\ncan ﬁnd quality differences between submissions from dif-\nferent participant groups (Section 3.1). Section 3.2 intro-\nduces metrics for ambiguity and stability. In Section 3.3,\nwe measure to which extent participants agree on one or\nmultiple tempi for the same segment. Then, in Section 3.4,\nwe take a look at segment annotations aggregated on the\ntrack-level. Finally, in Section 3.5, we investigate whether\ntempo ambiguity is a genre-dependent phenomenon.\n3.1 Submission Quality\nWe wondered how steadily participants tapped and\nwhether some groups of participants tapped more steadily\nthan others. Speciﬁcally, are the beaTunes submissions as\ngood as the academics submissions? We can use the co-\nefﬁcient of variation cv=\u001b\n\u0016of each submission’s ITIs\nas a normalized indicator for how steadily a participant\ntapped. To remove tapping outliers within a segment, we\nsort each submission’s ITIs and only keep the central 10\nbefore calculating the cv. This has the effect of reducing cv\nfor all submissions. The average cvfor all submissions is\ncv= 0:0089 . Assuming a normal distribution, this means\nthat on average 99:7%of all central 10ITIs lie within\n\u00062:67% (\u00113\u001b) of their submission’s mean value. Us-\ningcvas a measure for the submission quality of different\ndataset splits, we found that members of academics tapped\nsigniﬁcantly more steadily ( cv= 0:0074 ) than members\nofbeaTunes (cv= 0:0090 ) (Table 1). To test for signif-\nicance we used Welch’s t-test. Also, submissions from\ndesktop operating systems that are typically installed on\ndevices connected to a physical keyboard (i.e., no touch-\nscreen) are of signiﬁcantly higher quality ( cv= 0:0088 )\nthan submissions from devices using iOS or Android as\noperating system ( cv= 0:0095 ). Despite the differences,\nwe found that even the ITIs from the group with the high-\nestcv, i.e., beaTunes without keyboard, still lie within only\n\u00062:97% (\u00113\u001b) of their mean value 99:7%of the time—\nagain assuming a normal distribution. This is well below\nthe tolerance of 4%allowed by Accuracy1 .\nWe conclude that the data submitted by academics with\nkeyboard is of the highest quality with regard to tempo sta-\nbility, but ﬁnd that the data submitted by members of bea-\nTunes without keyboard is still acceptable, because the dif-\nference incvis not very large. This may be a direct result\nof the experiment’s design which did not permit partici-\npants to submit highly irregular taps.50 100 150 20000:10:20:3\nBPMsaliencesegment 6\nwhole track\nFigure 2 : Tempo salience distribution for segment\n6of track ‘Neoteric D&B Mix’ by Polex (Beatport\nid4397469 ). Measured values are: P(Ttrack) = 4 ,\nP(Tseg6) = 2 ,A(Ttrack) = 0:30,A(Tseg6) = 0:40, and\nJSD = 0:24.\n3.2 Tempo Distribution Metrics\nHow steadily participants tapped does not say anything\nabout whether they tapped along to the true tempo. But\nsince the purpose of the experiment is to create a new\nground-truth, we cannot easily verify submissions for cor-\nrectness. What we can do though, is to measure annotator\n(dis)agreement both for a segment and for all segments be-\nlonging to the same track. To this end, we deﬁne some\nmetrics based on tapped tempo distributions. To create\nsuch a tapped tempo distribution for a segment, we com-\nbine the 10central ITIs from each of its submissions in\na histogram Twith a bin width of 1 BPM and then nor-\nmalize so thatPn\ni=1T(xi) = 1 , withnas the number\nof bins and xias the corresponding BPM values. For T\nwe deﬁne local peaks as the highest non-zero T(xi)for\nall intervals [xi\u00005;xi+ 5]. This may include very small\npeaks. We interpret the BPM values xiof the histogram’s\nlocal peaks as the perceptually strongest tempi and their\nheights equivalent to their saliences. Per-track tempo dis-\ntributions are created simply by averaging the 7 segment\nhistograms belonging to a given track. For an example,\nplease see Figure 2.\nAs a ﬁrst, very simple indicator for annotator disagree-\nment, we deﬁne P(T)as the number of histogram peaks\nwe ﬁnd in a given tempo distribution T. A high peak count\nfor a single segment P(Tseg)indicates annotator disagree-\nment for that segment. This is not necessarily true for the\npeak count for a track P(Ttrack), since it may also be a\nsign of tempo instability, i.e., tempo changes or no-beat-\nsections. Because the peak count Pdoes not say any-\nthing about the peaks’ height or salience, it is a relatively\ncrude measure. Therefore we deﬁne as second metric the\nsalience ratio between the most salient and the second most\nsalient peak as a measure for ambiguity. More formally, if\ns1is the salience of the highest peak and s2the salience\nof the second highest peak, then the ambiguity A(T)is de-\nﬁned as:\nA(T) :=8\n><\n>:1; forP(T) = 0\n0; forP(T) = 1\ns2=s1;forP(T)>1(1)\nA value close to 0indicates low and a value close to 1Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 411high ambiguity. This deﬁnition is inspired by McKinney\net al. [8] approach to ambiguity, but not identical. Just\nlikeP, we can use Afor both segment and track tempo\ndistributions. Again, for tracks we cannot be sure of the\nambiguity’s source.\nFinally, we introduce a third metric that focuses more\non tempo instability within tracks. Obvious indicators for\ninstabilities are large differences between the tempo distri-\nbutions of segments belonging to one track. Since we cre-\nate tapped tempo distributions for each segment in a way\nthat lets us interpret them as probability distributions, we\ncan use the Jensen-Shannon Divergence ( JSD) for this pur-\npose, which is based on the Shannon entropy H. With the\nJSD we measure the difference between the tempo distri-\nbution’s entropy for the whole track and the average of the\nthe individual segment tempo distributions’ entropies.\nH(T) :=\u0000nX\ni=1T(xi)logbT(xi) (2)\nJSD(T1;:::;Tm) := H mX\nj=11\nmTj!\n\u0000mX\nj=11\nmH(Tj)(3)\nTo allow an easy interpretation of JSD-values, we\nchoose an unusual base for the entropy’s logarithm. By\nsettingb=nin (2), we ensure that 0\u0014JSD\u00141. This\nmeans, that a JSD-value near 0indicates a small difference\nbetween the tempo distributions for a track’s segments.\nCorrespondingly, a JSD-value closer to 1means that the\ntempo distributions of a track’s segments are very different.\nTo avoid detecting small tempo changes due to annotator\ndisagreements, we convert the segment tempo distributions\nTto a bin width of 10 BPM before calculating JSD.\n3.3 Segment Annotator Agreement\nHow much do participants agree on a tempo for a given\nsegment? Recall that we have 4;640segments (and 18;684\nannotations for these segments) coming from 664tracks.\nAs depicted in Figure 3 top, the submissions for more than\nhalf the segments ( 2;500or53:9%) have just one peak, i.e.,\nP(Tseg) = 1 . For 1;514or32:6%of all segments we were\nable to ﬁnd two peaks, indicating some ambiguity. For\n432segments ( 9:3%) we found 3 peaks and for 184seg-\nments ( 4:0%) 4 peaks or more. 10segments have no peak\nat all, because they have been marked as No Beat in all their\nsubmissions. When interpreting these numbers one has to\nkeep in mind that some segments have been annotated by\nvery few participants (Figure 3 bottom). To give an exam-\nple, while the segments annotated with one peak are based\non3:64submissions on average, the segments annotated\nwith6peaks are annotated with 9:42submissions per seg-\nment. This reﬂects the fact that we presented difﬁcult seg-\nments to participants more often, but could also be caused\nby increased variability introduced by a higher number of\nsubmissions. Because submissions marked as No Beat do\nnot show up in this overview unless all submissions for a\nsegment were No Beat , we counted the segments for which\na majority of submissions were marked with No Beat . That\nwas the case for 118segments ( 2:5%).01234567891001;0002;0003;000\n102;500\n1;514\n432\n129 3512 4301segments\n01234567891005101520\n5:1\n3:64\n4:18\n4:61\n5:74\n6:69\n9:42\n14\n15\n0\n17\nP(Tseg)avg # subs/seg\nFigure 3 : (top) Segments per peak count. (bottom) Aver-\nage number of submissions per segment by peak count.\n123456789101112050100150200\n81178\n155\n101\n5745\n21157301tracks\n1234567891011120204060\n25:43\n26:83\n27:91\n28:56\n28:61\n31:27\n30:52\n37:33\n39:57\n29:33\n0\n34\nP(Ttrack)avg # subs/track\nFigure 4 : (top) Tracks per peak count. (bottom) Average\nnumber of submissions per track by peak count.\nAs mentioned in Section 3.2, the peak count does not\nsay anything about the peaks’ height or salience and is\ntherefore a relatively crude measure. We found that the av-\nerage ambiguity for all segments is A(Tseg) = 0:25(with\nstandard deviation \u001b= 0:32), meaning that on average the\nhighest peak is 4 times more salient than the second high-\nest peak. In other words, we can often observe a peak that\nis much more salient than others. At the same time, there\nmay also be a second peak with considerable salience.\n3.4 Track Annotator Agreement\nJust like for the segments, we looked at the number of\ntracks per peak count. We found only 81tracks ( 12:2%)\nwith one peak and 582tracks ( 87:8%) with two or more\npeaks (Figure 4 top). The largest group among the multi-\npeak tracks are tracks with two peaks ( 178 or26:8%).\nThese numbers are much more reliable than the segment\npeak counts as they are based on at least 25submissions\nper track (Figure 4 bottom). Compared to the segments’\npeak counts we see a larger proportion of tracks with more\nthan one peak. But this does not necessarily mean that the\nambiguityAis much higher than for the segments, because412 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 201850 100 150 20000:20:40:60:8\nBPMsaliencesegments 1-3\nsegment 4\nsegments 5-7\nFigure 5 : Tempo salience distributions for segments of\nthe track ‘Rude Boy feat. Omar LinX Union V ocal Mix’\nby Zeds Dead (Beatport id 1728723 ). The track’s tempo\nchanges in segment 4, leading to four distinct peaks. With\nJSD = 0:44its Jensen-Shannon divergence is high.\npeak counts do not account for salience and even small lo-\ncal peaks are counted. In fact, we measured an average\nambiguity of A(Ttrack) = 0:26(with standard deviation\n\u001b= 0:27)—almost the same average as for the segments.\nTherefore we attribute the shift towards more peaks to the\nmuch higher number of submissions per item and possible\ntempo instabilities in the tracks themselves. By tempo in-\nstability we mean for example a tempo change in the mid-\ndle of the track, a quiet section, or no beat at all. Any of\nthese cases inherently lead to more peaks. A typical exam-\nple for a track with a tempo change is shown in Figure 5.\nIn an attempt to quantify tempo instabilities in the sub-\nmissions we calculated the JSD introduced in Section 3.2.\nThe histogram in Figure 6 shows the distribution of tracks\nperJSD interval with a bin width of 0:05. The average\ndivergence for the whole dataset is \u0016JSD= 0:15, the stan-\ndard deviation is \u001bJSD= 0:11. To test whether a high\nJSD correlates with tempo instabilities, we considered all\ntracks with JSD> \u0016 JSD+ 2\u001bJSD= 0:375, resulting in\n39tracks. Performing an informal listening test on these\ntracks revealed that 3had no beat, 10contained a tempo\nchange (e.g. Figure 5), 7had sections that felt half as fast\nas other sections (metrical ambiguity), 8contained larger\nsections with no discernible beat, 9were difﬁcult to tap,\nand2had a stable tempo through the whole track. From\nthis result one may conclude that a high JSD is connected\nto tempo instabilities, but it may also just indicate that a\ntrack is difﬁcult to tap. Nevertheless, using JSD helped\nus ﬁnd tracks in the GiantSteps Tempo dataset that exhibit\ntempo stability issues. Since 2:5%of the segments were\nannotated most often with No Beat , we wondered whether\nany tracks have a majority of segments that have predomi-\nnantly been annotated with No Beat , hinting at the absence\nof not just a local beat (e.g., a sound effect or a silent sec-\ntion), but the lack of a global beat. This is true for 6tracks,\ni.e.,0:9%of the dataset. All 6of them are among the 39\ntracks with very high JSD and either have no beat, are very\ndifﬁcult to tap or contain large sections without a beat.\n3.5 Ambiguity by Genre\nWe wondered whether we can conﬁrm ﬁndings by McK-\ninney and Moelants [9] that the amount of tempo ambi-0 0:2 0:4 0:6 0:8 10102030\u0016JSD\u0016JSD+ 2\u001bJSD\nJSDtracks in %\nFigure 6 : Distribution of tracks in the dataset per JSD in-\nterval with a bin width of 0:05. The blue line shows \u0016JSD\nand the red line shows \u0016JSD+ 2\u001bJSD:\nGenre A(Tseg)A(Ttrack)\nall 0.25 0.26\ntechno 0.12 0.10\ntrance 0.17 0.12\ndrum-and-bass 0.37 0.39\nelectronica 0.36 0.38\ndubstep 0.35 0.43\nTable 2 : Average ambiguity for the top 5 genres.\nguity depends on the genre or musical style. To ensure\nmeaningful results, we considered only the 5 most often\noccurring genres in the dataset with 54or more tracks\neach. We found that the genres techno and trance do not\nseem to be very affected by ambiguity. More than 65%\nof their segments are annotated with just one peak. In\ncontrast to that, fewer than 38% of all segments in the\ngenres drum-and-bass, dubstep, and electronica are anno-\ntated with just one peak (Figure 7 top). A similar picture\npresents itself when looking at the average segment ambi-\nguityA(Tseg). As shown in Table 2, it is 0:12for techno\nsegments and thus much lower than the overall average of\n0:25. The same is true for trance ( 0:17). Contrary to that,\nthe ambiguity values for drum-and-bass ( 0:37), electron-\nica (0:36) and dubstep ( 0:35) are all well above the av-\nerage. We found similar relations for peak counts on the\ntrack level (Figure 7 bottom) and the average track ambi-\nguityA(Ttrack)(Table 2). This strongly supports McKin-\nney and Moelants’ ﬁnding that tapped tempo ambiguity is\ngenre-dependent. Perhaps it is even an inherent property.\n4. EVALUATION\nThe tempo histograms for tracks can easily be turned into\nsingle tempo per track or two tempi+salience labels. This\nprovides us the opportunity to evaluate the original ground-\ntruth for the GiantSteps Tempo dataset by treating it like an\nalgorithm. Since the original annotations are single tempo\nper track only, we are using Accuracy1 andAccuracy2 as\nmetrics. To obtain one tempo value per track from a distri-\nbution, we are using just the tempo value with the highest\nsalience. The three tracks without a beat have been re-\nmoved. We refer to these new annotations as GSNew and\nto the original ones as GSOrig . Figure 8 shows the ac-\ncuracy results for the comparison of GSOrig withGSNew\nand reveals a large discrepancy between the two. Only 81:5Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 4130 20 40 60 80 100drum-and-bass\ndubstep\nelectronica\ntechno\ntrance\n% of segments\n0 20 40 60 80 100drum-and-bass\ndubstep\nelectronica\ntechno\ntrance\n% of tracks\nno peak 1 peak 2 peaks 3 peaks 4 peaks 5 or more\nFigure 7 : Percentage of segments (top) and tracks (bot-\ntom) with a given number of peaks by genre. Drum-\nand-bass, dubstep, and electronica suffer much more from\ntapped tempo ambiguity than techno and trance.\n0 20 40 60 80 100Accuracy1\nAccuracy281:5\n91:1\nAccuracy in %\nFigure 8 : Accuracies measured when comparing GSNew\nwithGSOrig .\nof the labels match when using Accuracy1 , and only 91:1%\nmatch when using Accuracy2 .\nComing back to the original motivation for this paper—\nthe poor performance of tempo estimation systems for Gi-\nantSteps Tempo —we evaluated the two state-of-the-art al-\ngorithms schreiber [12] and b¨ock [1] with both the\nold and the new annotations. The algorithms were chosen\nfor their proven performance and conceptual dissimilar-\nity. While schreiber implements a conventional onset\ndetection approach followed by an error correction proce-\ndure, b¨ock’s core consists of a bidirectional long short-\nterm memory (BLSTM) recurrent neural network. De-\nspite their conceptual differences, both algorithms reach\nconsiderably higher accuracy values when tested against\nGSNew (Figure 9). Accuracy1 increases for b¨ock by\n5:9 pp (58:9%to64:8%) and for schreiber by7:1 pp\n(63:1%to70:2%).Accuracy2 shows similar increases,\n7:6 pp (86:4%to94:0%) forb¨ock and6:5 pp (88:7%to\n95:2%) for schreiber . Remarkably, both b¨ock and\nschreiber reach higher Accuracy2 values for GSNew\nthan the original annotations reached, when compared with\nGSNew . The increased results for GSNew are much more\nin line with values reported for other tempo datasets. We\ntherefore believe that this increase and the discrepancy be-\ntween GSOrig andGSNew are hardly coincidences, but\nstrong indicators for incorrect annotations in GSOrig .0 20 40 60 80 100b¨ock\nschreiber64:8\n70:258:9\n63:1\nAccuracy1 in %GSNew\nGSOrig\n0 20 40 60 80 100b¨ock\nschreiber94\n95:286:4\n88:7\nAccuracy2 in %\nFigure 9 : Accuracies for the algorithms b¨ock and\nschreiber measured against both GSOrig andGSNew .\n5. DISCUSSION AND CONCLUSIONS\nIn this paper we described a crowdsourced experiment for\ntempo estimation. We collected 18;684tapped annotations\nfrom 266participants for electronic dance music (EDM)\ntracks contained in the GiantSteps Tempo dataset. To an-\nalyze the data, we used multiple metrics and found that\nhalf of the annotated segments and more than half of the\ntracks exhibit some degree of tempo ambiguity, which may\neither stem from annotator disagreement or from intra-\ntrack tempo instability. This refutes the assumption that\nit is always easy to determine a single global tempo for\nEDM. We were able to identify tracks with no tempo at\nall, no-beat-sections or tempo changes, which raises ques-\ntions about the suitability of parts of the dataset for the\nglobal tempo estimation task. Furthermore, we provided\nadditional evidence for genre-dependent tempo ambiguity.\nBased on the user-submitted data we derived the new an-\nnotations GSNew . The relatively low agreement with the\noriginal annotations GSOrig indicates that one of the two\nground-truths contains incorrect annotations for up to 8:9%\nof the tracks (ignoring octave errors). We re-evaluated two\nrecent tempo estimation algorithms against both ground-\ntruths and measured considerably higher accuracies when\ntesting against GSNew . This leads us to the following con-\nclusions: GSOrig contains incorrectly annotated tracks as\nwell as tracks that are not suitable for the global tempo\nestimation task. The accuracy of state-of-the-art tempo es-\ntimations systems is considerably higher than previously\nthought. And last but not least, as a community, we have\nto get better at evaluating tempo algorithms in the sense\nthat we need veriﬁed, high quality datasets that represent\nreality with tempo distributions instead of single value an-\nnotations. If we cannot accurately measure progress, we\nhave no way of knowing when the task is done.\nDatasets\nAll data is available at http://www.tagtraum.com/\ntempo_estimation.html .\nAcknowledgments\nThe International Audio Laboratories Erlangen are a joint insti-\ntution of the Friedrich-Alexander-Universit ¨at Erlangen-N ¨urnberg\n(FAU) and Fraunhofer Institute for Integrated Circuits IIS.\nMeinard M ¨uller is supported by the German Research Founda-\ntion (DFG MU 2686/11-1).414 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20186. REFERENCES\n[1] Sebastian B ¨ock, Florian Krebs, and Gerhard Widmer. Ac-\ncurate tempo estimation based on recurrent neural networks\nand resonating comb ﬁlters. In Proceedings of the 16th Inter-\nnational Society for Music Information Retrieval Conference\n(ISMIR) , pages 625–631, M ´alaga, Spain, 2015.\n[2] Mark J. Butler. Unlocking the Groove: Rhythm, Meter, and\nMusical Design in Electronic Dance Music . Proﬁles in popu-\nlar music. Indiana University Press, 2006.\n[3] Olmo Cornelis, Joren Six, Andre Holzapfel, and Marc Le-\nman. Evaluation and recommendation of pulse and tempo\nannotation in ethnic music. Journal of New Music Research ,\n42(2):131–149, 2013.\n[4] Fabien Gouyon, Anssi P. Klapuri, Simon Dixon, Miguel\nAlonso, George Tzanetakis, Christian Uhle, and Pedro Cano.\nAn experimental comparison of audio tempo induction algo-\nrithms. IEEE Transactions on Audio, Speech, and Language\nProcessing , 14(5):1832–1844, 2006.\n[5] Andre Holzapfel, Matthew EP Davies, Jos ´e R Zapata,\nJo˜ao Lobato Oliveira, and Fabien Gouyon. Selective sam-\npling for beat tracking evaluation. IEEE Transactions on Au-\ndio, Speech, and Language Processing , 20(9):2539–2548,\n2012.\n[6] Peter Knees, ´Angel Faraldo, Perfecto Herrera, Richard V ogl,\nSebastian B ¨ock, Florian H ¨orschl ¨ager, and Mickael Le Goff.\nTwo data sets for tempo estimation and key detection in elec-\ntronic dance music annotated from user corrections. In Pro-\nceedings of the 16th International Society for Music Informa-\ntion Retrieval Conference (ISMIR) , pages 364–370, M ´alaga,\nSpain, October 2015.\n[7] Michaelangelo Matos. Electronic dance music. Encyclopæ-\ndia Britannica https://www.britannica.com/art/\nelectronic-dance-music , last checked 6/9/2018, Au-\ngust 2015.\n[8] Martin F McKinney and Dirk Moelants. Deviations from the\nresonance theory of tempo induction. In Proc. Conference on\nInterdisciplinary Musicology , Graz, Austria, 2004.\n[9] Martin F McKinney and Dirk Moelants. Ambiguity in tempo\nperception: What draws listeners to different metrical levels?\nMusic Perception: An Interdisciplinary Journal , 24(2):155–\n166, 2006.\n[10] Geoffroy Peeters and Joachim Flocon-Cholet. Perceptual\ntempo estimation using GMM-regression. In Proceedings of\nthe second international ACM workshop on Music informa-\ntion retrieval with user-centered and multimodal strategies\n(MIRUM) , pages 45–50, New York, NY , USA, 2012. ACM.\n[11] Bruno H. Repp. Sensorimotor synchronization: A review\nof the tapping literature. Psychonomic Bulletin & Review ,\n12(6):969–992, 2005.\n[12] Hendrik Schreiber and Meinard M ¨uller. A post-processing\nprocedure for improving music tempo estimates using su-\npervised learning. In 18th International Society for Music\nInformation Retrieval Conference (ISMIR) , pages 235–242,\nSuzhou, China, October 2017.\n[13] George Tzanetakis and Perry Cook. Musical genre classiﬁca-\ntion of audio signals. IEEE Transactions on Speech and Audio\nProcessing , 10(5):293–302, 2002.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 415"
    },
    {
        "title": "Evaluating Language Models of Tonal Harmony.",
        "author": [
            "David R. W. Sears",
            "Filip Korzeniowski",
            "Gerhard Widmer"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492385",
        "url": "https://doi.org/10.5281/zenodo.1492385",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/262_Paper.pdf",
        "abstract": "This study borrows and extends probabilistic language models from natural language processing to discover the syntactic properties of tonal harmony. Language models come in many shapes and sizes, but their central purpose is always the same: to predict the next event in a sequence of letters, words, notes, or chords. However, few studies employing such models have evaluated the most stateof-the-art architectures using a large-scale corpus of Western tonal music, instead preferring to use relatively small datasets containing chord annotations from contemporary genres like jazz, pop, and rock. Using symbolic representations of prominent instrumental genres from the common-practice period, this study applies a flexible, data-driven encoding scheme to (1) evaluate Finite Context (or n-gram) models and Recurrent Neural Networks (RNNs) in a chord prediction task; (2) compare predictive accuracy from the best-performing models for chord onsets from each of the selected datasets; and (3) explain differences between the two model architectures in a regression analysis. We find that Finite Context models using the Prediction by Partial Match (PPM) algorithm outperform RNNs, particularly for the piano datasets, with the regression model suggesting that RNNs struggle with particularly rare chord types.",
        "zenodo_id": 1492385,
        "dblp_key": "conf/ismir/SearsKW18",
        "keywords": [
            "probabilistic language models",
            "tonal harmony",
            "syntactic properties",
            "language processing",
            "chord prediction task",
            "data-driven encoding scheme",
            "Finite Context models",
            "Recurrent Neural Networks (RNNs)",
            "regression analysis",
            "common-practice period"
        ],
        "content": "EVALUATING LANGUAGE MODELS OF TONAL HARMONY\nDavid R. W. Sears1Filip Korzeniowski2Gerhard Widmer2\n1College of Visual & Performing Arts, Texas Tech University, Lubbock, USA\n2Institute of Computational Perception, Johannes Kepler University, Linz, Austria\ndavid.sears@ttu.edu\nABSTRACT\nThis study borrows and extends probabilistic language\nmodels from natural language processing to discover the\nsyntactic properties of tonal harmony. Language models\ncome in many shapes and sizes, but their central purpose is\nalways the same: to predict the next event in a sequence\nof letters, words, notes, or chords. However, few stud-\nies employing such models have evaluated the most state-\nof-the-art architectures using a large-scale corpus of West-\nern tonal music, instead preferring to use relatively small\ndatasets containing chord annotations from contemporary\ngenres like jazz, pop, and rock.\nUsing symbolic representations of prominent instru-\nmental genres from the common-practice period, this study\napplies a ﬂexible, data-driven encoding scheme to (1)\nevaluate Finite Context (or n-gram) models and Recur-\nrent Neural Networks (RNNs) in a chord prediction task;\n(2) compare predictive accuracy from the best-performing\nmodels for chord onsets from each of the selected datasets;\nand (3) explain differences between the two model archi-\ntectures in a regression analysis. We ﬁnd that Finite Con-\ntext models using the Prediction by Partial Match (PPM)\nalgorithm outperform RNNs, particularly for the piano\ndatasets, with the regression model suggesting that RNNs\nstruggle with particularly rare chord types.\n1. INTRODUCTION\nFor over two centuries, scholars have observed that tonal\nharmony, like language, is characterized by the logical\nordering of successive events, what has commonly been\ncalled harmonic syntax . In Western music of the common-\npractice period (1700-1900), pitch events group (or co-\nhere) into discrete, primarily tertian sonorities, and the\nsuccession of these sonorities over time produces mean-\ningful syntactic progressions. To characterize the passage\nfrom the ﬁrst two measures of Bach’s “Aus meines Herzens\nGrunde”, for example, theorists and composers developed\na chord typology that speciﬁes both the scale steps on\nwhich tertian sonorities are built ( Stufentheorie ), and the\nc\rSears, Korzeniowski, Widmer. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Sears, Korzeniowski, Widmer. “Evaluating language models\nof tonal harmony”, 19th International Society for Music Information Re-\ntrieval Conference, Paris, France, 2018.I IV6V6I V vi G: –\nFigure 1 . Bach, “Aus meines Herzens Grunde”, mm. 1–2;\nfrom the Riemenschneider edition, No. 1. Key and Roman\nnumeral annotations appear below.\nfunctional (i.e., temporal) relations that bind them ( Funk-\ntionstheorie ). Shown beneath the staff in Figure 1, this Ro-\nman numeral system allows the analyst to recognize and\ndescribe these relations using a simple lexicon of symbols.\nIn the presence of such language-like design features,\nmusic scholars have increasingly turned to string-based\nmethods from the natural language processing (NLP) com-\nmunity for the purposes of pattern discovery [6], classiﬁ-\ncation [7], similarity estimation [18], and prediction [19].\nIn sequential prediction tasks, for example, probabilistic\nlanguage models have been developed to predict the next\nevent in a sequence — whether it consists of letters, words,\nDNA sequences, or in our case, chords.\nAlthough corpus studies of tonal harmony have become\nincreasingly commonplace in the music research commu-\nnity, applications of language models for chord prediction\nremain somewhat rare. This is likely because language\nmodels take as their starting point a sequence of chords,\nbut the musical surface is often a dense web of chordal and\nnonchordal tones, making automatic harmonic analysis a\ntremendous challenge. Indeed, such is the scope of the\ncomputational problem that a number of researchers have\ninstead elected to start with a particular chord typology\nright from the outset (e.g., Roman numerals, ﬁgured bass\nnomenclature, or pop chord symbols), and then identify\nchord events using either human annotators [3], or rule-\nbased computational classiﬁers [25]. As a consequence,\nlanguage models for tonal harmony frequently train on rel-\natively small, heavily curated datasets ( <200;000chords)\n[3], or use data augmentation methods to increase the size\nof the corpus [15]. And since the majority of these corpora\nreﬂect pop, rock, or jazz idioms, vocabulary reduction is\na frequent preliminary step to ensure improved model per-\nformance, with the researcher typically including speciﬁc\nchord types (e.g., major, minor, seventh, etc.), thus ignor-211ing properties of tonal harmony relating to inversion [15]\nor chordal extension [11].\nGiven the state of the annotation bottleneck, we propose\na complementary method for the implementation and eval-\nuation of language models for chord prediction. Rather\nthan assume a particular chord typology a priori and train\nour models on the chord classes found therein, we will in-\nstead propose a data-driven method for the construction of\nharmonic corpora using chord onsets derived from the mu-\nsical surface. It is our hope that such a bottom-up approach\nto chord prediction could provide a springboard for the im-\nplementation of chord class models in future studies [2],\nthe central purpose of which is to use predictive methods\nto reduce the musical surface to a sequence of syntactic\nprogressions by discovering a small vocabulary of chord\ntypes.\nWe begin in Section 2 by describing the datasets used\nin the present research and then present the tonal encod-\ning scheme that reduces the combinatoric explosion of po-\ntential chord types to a vocabulary consisting of roughly\ntwo hundred types for each scale-degree in the lowest in-\nstrumental part. Next, Section 3 describes the two most\nstate-of-the-art architectures employed in the NLP com-\nmunity: Finite Context (or n-gram) models and Recurrent\nNeural Networks (RNNs). Section 4 presents the experi-\nments, which (1) evaluate the two aforementioned model\narchitectures in a chord prediction task; (2) compare pre-\ndictive accuracy from the best-performing models for each\ndataset; (3) attempt to explain the differences between the\ntwo models using a regression analysis. We conclude in\nSection 5 by considering limitations of the present ap-\nproach, and offering avenues for future research.\n2. CORPUS\nThis section presents the datasets used in the present re-\nsearch and then describes the chord representation scheme\nthat permits model comparison across datasets.\n2.1 Datasets\nShown in Table 1, this study includes nine datasets of\nWestern tonal music (1710–1910) featuring symbolic rep-\nresentations of the notated score (e.g., metric position,\nrhythmic duration, pitch, etc.). The Chopin dataset con-\nsists of 155 works for piano that were encoded in Mu-\nsicXML format [10]. The Assorted symphonies dataset\nconsists of symphonic movements by Beethoven, Berlioz,\nBruckner, and Mahler that were encoded in MATCH for-\nmat [26]. All other datasets were downloaded from the\nKernScores database in MIDI format.1In total, the\ncomposite corpus includes the complete catalogues for\nBeethoven’s string quartets and piano sonatas, Joplin’s\nrags, and Chopin’s piano works, and consists of over 1,000\ncompositions containing more than 1 million chord tokens.\n1http://kern.ccarh.org/ .Composer Genre N pieces Ntokens Ntypes\nBach Chorale 370 35,237 786\nHaydn Quartet 210 159,579 1472\nMozart Quartet 82 78,201 1289\nBeethoven Quartet 70* 132,896 1699\nMozart Piano 51 92,279 833\nBeethoven Piano 102* 176,370 1332\nChopin Piano 155* 147,827 1790\nJoplin Piano 47* 43,848 854\nAssorted Symphony 29 147,549 2420\nTotal 1116 1,013,786 2590\nNote . * denotes the complete catalogue.\nTable 1 . Datasets and descriptive statistics for the corpus.\n2.2 Chord Representation Scheme\nTo derive chord progressions from symbolic corpora using\ndata-driven methods, music analysis software frameworks\ntypically perform a full expansion of the symbolic en-\ncoding, which duplicates overlapping note events at every\nunique onset time. Shown in Figure 2, expansion identiﬁes\n9 unique onset times in the ﬁrst two measures of Bach’s\nchorale harmonization, “Aus meines Herzens Grunde.”\nPrevious studies have represented each chord accord-\ning to the simultaneous relations between its note-event\nmembers (e.g., vertical intervals) [23], the sequential re-\nlations between its chord-event neighbors (e.g., melodic\nintervals) [6], or some combination of the two [22]. For\nthe purposes of this study, we have adopted a chord typol-\nogy that models every possible combination of note events\nin the corpus. The encoding scheme consists of an ordered\ntuple (S;I) for each chord onset in the sequence, where S\nis a set of up to three intervals above the bass in semitones\nmodulo the octave (i.e., 12), resulting in 133(or 2197) pos-\nsible combinations;2andIis the chromatic scale degree\n(again modulo the octave) of the bass, where 0 represents\nthe tonic, 7 the dominant, and so on.\nBecause this encoding scheme makes no distinction be-\ntween chord tones and non-chord tones, the syntactic do-\nmain of chord types is still very large. To reduce the\ndomain to a more reasonable number, we have excluded\npitch class repetitions in S(i.e., voice doublings), and we\nhave allowed permutations. Following [22], the assump-\ntion here is that the precise location and repeated appear-\nance of a given interval are inconsequential to the identity\nof the chord. By allowing permutations, the major triads\nh4;7;0iandh7;4;0itherefore reduce to h4;7;?i. Simi-\nlarly, by eliminating repetitions, the chords h4;4;10iand\nh4;10;10ireduce toh4;10;?i. This procedure restricts\nthe domain to 233unique chord types in S(i.e., whenIis\nundeﬁned).\nTo determine the underlying tonal context of each chord\nonset, we employ the key-ﬁnding algorithm in [1], which\ntends to outperform other distributional methods (with an\n2The value of each vertical interval is either undeﬁned (denoted by\n?), or represents one of twelve possible interval classes, where 0 denotes\na perfect unison or octave, 7 denotes a perfect ﬁfth, and so on.212 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018<0,4,7,?>\n<11,3,8,?> < 9,3,7,?>\nFigure 2 . Full expansion of Bach, “Aus meines Herzens\nGrunde”, mm. 1–2. Three chord onsets are shown with the\ntonal encoding scheme described in Section 2.2 for illus-\ntrative purposes.\naccuracy of around 90% for both major and minor keys).\nSince the movements in this dataset typically feature mod-\nulations, we compute the Pearson correlation between the\ndistributional weights in the selected key-ﬁnding algorithm\nand the pitch-class distribution identiﬁed in a moving win-\ndow of 16 quarter-note beats and centered around each\nchord onset in the sequence. The algorithm interprets the\npassage in Figure 2 in G major, for example, so the bass\nnote of the ﬁrst harmony is 0 (i.e., the tonic).\n3. LANGUAGE MODELS\nThe goal of language models is to estimate the probabil-\nity of event eigiven a preceding sequence of events e1\ntoei\u00001, notated here as ei\u00001\n1. In principle, these models\npredicteiby acquiring knowledge through unsupervised\nstatistical learning of a training corpus, with the model\narchitecture determining how this learning process takes\nplace. For this study we examine the two most common\nand best-performing language models in the NLP commu-\nnity: (1) Markovian ﬁnite-context (or n-gram) models us-\ning the PPM algorithm, and (2) recurrent neural networks\n(RNNs) using both long short-term memory (LSTM) lay-\ners and gated recurrent units (GRUs).\n3.1 Finite Context Models\nContext models estimate the probability of each event in a\nsequence by stipulating a global order bound (or determin-\nistic context) such that p(ei)depends only on the previous\nn\u00001events, orp(eijei\u00001\n(i\u0000n)+1). For this reason, context\nmodels are also sometimes called n-gram models, since\nthe sequence ei\n(i\u0000n)+1is an n-gram consisting of a context\nei\u00001\n(i\u0000n)+1, and a single-event prediction ei. These models\nﬁrst acquire the frequency counts for a collection of se-\nquences from a training set, and then apply these counts to\nestimate the probability distribution governing the identity\nofeiin a test sequence using maximum likelihood (ML)\nestimation.\nUnfortunately, the number of potential n-grams de-\ncreases dramatically as the value of nincreases, so high-\norder models often suffer from the zero-frequency prob-\nlem, in whichn-grams encountered in the test set do not\nappear in the training set [27]. The most common solution\nto this problem has been the Prediction by Partial Match\n(PPM) algorithm, which adjusts the ML estimate for eiby\ncombining (or smoothing ) predictions generated at higherorders with less sparsely estimated predictions from lower\norders [5]. Speciﬁcally, PPM assigns some portion of the\nprobability mass to accommodate predictions that do not\nappear in the training set using an escape method . The\nbest-performing smoothing method is called mixtures (or\ninterpolated smoothing ), which computes a weighted com-\nbination of higher order and lower order models for every\nevent in the sequence.\n3.1.1 Model Selection\nTo implement this model architecture, we apply the\nvariable-order Markov model (called IDyOM ) developed\nin [19].3The model accommodates many possible con-\nﬁgurations based on the selected global order bound, es-\ncape method, and training type. Rather than select a global\norder bound, researchers typically prefer an extension to\nPPM called PPM*, which uses simple heuristics to de-\ntermine the optimal high-order context length for ei, and\nwhich has been shown to outperform the traditional PPM\nscheme in several prediction tasks (e.g., [21]), so we ap-\nply that extension here. Regarding the escape method, re-\ncent studies have demonstrated the potential of method C\nto minimize model uncertainty in melodic and harmonic\nprediction tasks [12, 21], so we also employ that method\nhere.\nTo improve model performance, Finite Context mod-\nels often separately estimate and then combine two sub-\nordinate models trained on differed subsets of the corpus:\nalong-term model (LTM+), which is trained on the en-\ntire corpus; and a short-term (orcache ) model (STM),\nwhich is initially empty for each individual composition\nand then is trained incrementally (e.g., [8]). As a result,\nthe LTM+ reﬂects inter-opus statistics from a large corpus\nof compositions, whereas the STM only reﬂects intra-opus\nstatistics, some of which may be speciﬁc to that composi-\ntion. Finally, the model implemented here also includes a\nmodel that combines the LTM+ and STM models using a\nweighted geometric mean (BOTH+) [20]. Thus, we report\nthe LTM+, STM, and BOTH+ models for the analyses that\nfollow.4\n3.2 Recurrent Neural Networks\nRecurrent Neural Networks (RNNs) are powerful models\ndesigned for sequential modelling tasks. RNNs transform\nan input sequence xN\n1to an output sequence oN\n1through\na non-linear projection into a hidden layer hN\n1, parame-\nterised by weight matrices Whx,WhhandWoh:\nhi=\u001bh(Whxxi+Whhhi\u00001) (1)\noi=\u001bo(Wohhi); (2)\nwhere\u001bhand\u001boare the activation functions for the hid-\nden layer (e.g. the sigmoid function), and the output layer\n3The model is available for download: http://code.\nsoundsoftware.ac.uk/projects/idyom-project\n4The models featuring the + symbol represent both the statistics from\nthe training set andthe statistics from that portion of the test set that has\nalready been predicted.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 213......\nFigure 3 . The basic architecture for an RNN-based\nlanguage model. This model can easily accommodate\nmore recurrent hidden layers or include additional skip-\nconnections between the input and each hidden layer or\nthe output. The ﬁrst input, e0, is a dummy symbol without\nan associated chord.\n(e.g. the softmax), respectively. We excluded bias terms\nfor simplicity.\nRNNs have become popular models for natural lan-\nguage processing due to their superior performance com-\npared to Finite Context models [17]. Here, the input at each\ntime stepiis a (learnable) vector representation of the pre-\nceding symbol, v(ei\u00001). The network’s output oi2RNtypes\nis interpreted as the conditional probability over the next\nsymbol,p\u0000\neijei\u00001\n1\u0001\n. As outlined in Figure 3, this proba-\nbility depends on allpreceding symbols through the recur-\nrent connection in the hidden layer.\nDuring training, the categorical cross-entropy between\nthe output oiand the true chord symbol is minimised by\nadapting the weight matrices in Eqs. 1 and 2 using stochas-\ntic gradient descent and back-propagation through time.\nHowever, this training procedure suffers from vanishing\nand exploding gradients because of the recursive dot prod-\nuct in Eq. 1. The latter problem can be averted by clipping\nthe gradient values; the former, however, is trickier to pre-\nvent, and necessitates more complex recurrent structures\nsuch as the long short-term memory unit (LSTM) [13] or\nthe gated recurrent unit (GRU) [4]. These units have be-\ncome standard features of RNN-based language modeling\narchitectures [16].\n3.2.1 Model Selection\nSelecting good hyper-parameters is crucial for neural net-\nworks to perform well. To this end, we performed a num-\nber of preliminary experiments to tune the networks. Our\nﬁnal architecture comprises two layers of 128 recurrent\nunits each (either LSTM or GRU), a learnable input em-\nbedding of 64 dimensions (i.e. v(\u0001)maps each chord class\nto a vector in R64), and skip connections between the input\nand all other layers.\nRNNs are prone to over-ﬁt the training data. We use\nthe network’s performance on held-out data to identify this\nissue. Since we employ 4-fold cross-validation (see Sec. 4\nfor details), we hold out one of the three training folds as\na validation set. If the results on these data do not improve\nfor 10 epochs, we stop training and select the model with\nthe lowest cross-entropy on the validation data.We trained the networks for a maximum of 200 epochs,\nusing stochastic gradient descent with a mini-batch size of\n4. Each of these 4 data points is a sequence of at most 300\nchords. The gradient updates are scaled using the Adam\nupdate rule [14] with standard parameters. To prevent ex-\nploding gradients, we clip gradient values larger than 1.\n4. EXPERIMENTS\n4.1 Evaluation\nTo evaluate performance using a more reﬁned method than\none simply based on the accuracy of the model’s predic-\ntion, we use a statistic called corpus cross-entropy , denoted\nbyHm.\nHm(pm;ej\n1) =\u00001\njjX\ni=1log2pm(eijei\u00001\n1): (3)\nHmrepresents the average information content for the\nmodel probabilities estimated by pmover allein the se-\nquenceej\n1. That is, cross-entropy provides an estimate of\nhow uncertain a model is, on average, when predicting a\ngiven sequence of events [21], regardless of whether the\ncorrect symbol for each event was assigned the highest\nprobability in the distribution.\nFinally, we employ 4-fold cross-validation stratiﬁed by\ndataset for both model architectures, using cross-entropy\nas a measure of performance.\n4.2 Results\nWe ﬁrst compare the average cross-entropy estimates\nacross the entire corpus using Finite Context models and\nRNNs, and then examine the estimates across datasets for\nthe best performing model conﬁguration from each archi-\ntecture. We conclude by examining the differences be-\ntween these models in a regression analysis.\n4.2.1 Comparing Models\nTable 2 presents the average cross-entropy estimates for\neach model conﬁguration. For the purposes of statisti-\ncal inference, we also include the 95% bootstrap conﬁ-\ndence interval using the bias-corrected and accelerated per-\ncentile method [9]. For the Finite Context models, BOTH+\nModel Type Hm CIa\nFinite Context\nLTM+ 4.895 4.811–4.978\nSTM 6.710 6.600–6.820\nBOTH+ 4.893 4.800–4.966\nRecurrent Neural Network\nLSTM 5.583 5.539–5.626\nGRU 5.600 5.551–5.645\naCI refers to the 95% bootstrap conﬁdence interval of Hmusing the\nbias-corrected and accelerated percentile method with 1000 replicates.\nTable 2 . Model comparison using cross-entropy as an eval-\nuation metric.214 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Chopin\nPianoBach\nChoraleMozart\nPianoBeethoven\nPianoJoplin\nPianoHaydn\nQuartetMozart\nQuartetBeethoven\nQuartetAssorted\nSymphonyBOTH+\nLSTM\n012345678\nHm(bits)\nFigure 4 . Bar plots of the best-performing model conﬁgurations from the Finite Context (BOTH+) and RNN (LSTM)\nmodels. Whiskers represent the 95% bootstrap conﬁdence interval of the mean using the bias-corrected and accelerated\npercentile method with 1000 replicates.\nproduced the lowest cross-entropy estimates on average,\nthough the difference between BOTH+ and LTM+ was\nnegligible. STM was the worst performing model over-\nall, which is unsurprising given the restrictions placed on\nthe model’s training parameters (i.e., that it only trains on\nthe already-predicted portion of the test set).\nOf the RNN models, LSTM slightly outperformed\nGRU, but again this difference was negligible. What is\nmore, the long-term Finite Context models (BOTH+ and\nLTM+) signiﬁcantly outperformed both RNNs. This ﬁnd-\ning could suggest that context models are better suited to\nmusic corpora, since the datasets for melodic and harmonic\nprediction are generally miniscule relative to those in the\nNLP community [15]. The encoding scheme for this study\nalso produced a large vocabulary (2590 symbols), so the\nPPM* algorithm might be useful when the model is forced\nto predict particularly rare types in the corpus.\n4.2.2 Comparing Datasets\nTo identify the differences between these models for each\nof the datasets in the corpus, Figure 4 presents the bar\nplots for the best-performing model conﬁgurations from\neach model architecture: BOTH+ from the Finite Context\nmodel, and LSTM from the RNN model. On average,\nBOTH+ produced the lowest cross-entropy estimates for\nthe piano datasets (Mozart, Beethoven, Joplin), but much\nhigher estimates for the other datasets. This effect was not\nobserved for LSTM, however, with the datasets’ genre —\nchorale, piano work, quartet, and symphony — apparently\nplaying no role in the model’s overall performance.\nThe difference between these two model architectures\nfor the Joplin and Mozart piano datasets is particularly\nstriking. Given the degree to which piano works gener-\nally consist of fewer homorhythmic textures relative to the\nother genres in this corpus, it could be the case that the\npiano datasets feature a larger proportion of rare, mono-\nphonic chord types relative to the other datasets. The next\nsection examines this hypothesis using a regression model.\n4.2.3 A Regression Model\nGiven the complexity of the corpus, a number of factors\nmight explain the performance of these models. Thus,we have included the following ﬁve predictors in a mul-\ntiple linear regression (MLR) model to explain the average\ncross-entropy estimates for the compositions in the corpus\n(N= 1136 ):5\nNtokens Cache (i.e., STM) and RNN-based language mod-\nels often beneﬁt from datasets that feature longer se-\nquences by exploiting statistical regularities in the\nportion of the test sequence that was already pre-\ndicted. Thus, Ntokens represents the number of to-\nkens in each sequence. Compositions featuring more\ntokens should receive lower cross-entropy estimates\non average.\nNtypes Language models struggle with data sparsity as n\nincreases (i.e., the zero-frequency problem). One\nsolution is to select corpora for which the vocab-\nulary of possible distinct types is relatively small.\nThus,Ntypesrepresents the number of types in each\nsequence. Compositions with larger vocabularies\nshould receive higher cross-entropy estimates on av-\nerage.\nImprobable Events that occur with low probability in the\nzeroth-order distribution are particularly difﬁcult to\npredict due to the data sparsity problem just men-\ntioned. Thus, Improbable represents the proportion\nof tokens in each sequence that appear in the bottom\n10% of types in the zeroth-order probability distribu-\ntion. Compositions with a large proportion of these\nparticularly rare types should receive higher cross-\nentropy estimates on average.\nMonophonic Chorales feature homorhythmic textures in\nwhich each temporal onset includes multiple coin-\ncident pitch events. The chord types representing\nthese tokens should be particularly common in this\ncorpus, but some genres might also feature poly-\nphonic textures in which the number of coincident\nevents is potentially quite low (e.g., piano). Thus,\n5Four of the 1116 compositions were further subdivided in the se-\nlected datasets, producing an additional 20 sequences in the analyses:\nBeethoven, Quartet No. 6, Op. 18, iv (2); Chopin, Op. 12 (2); Mozart,\nPiano Sonata No. 6, K. 284, iii (13); Mozart, Piano Sonata No. 11, K.\n331, i (7).Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 215Monophonic represents the proportion of tokens in\neach sequence that consist of only one pitch event.\nCompositions with a large proportion of these mono-\nphonic events should receive higher cross-entropy\nestimates on average.\nRepetition Compared to chord-class corpora, data-driven\ncorpora are far more likely to feature adjacent rep-\netitions of tokens. Thus, Repetition represents the\nproportion of tokens in each sequence that feature\nadjacent repetitions. Compositions with a large pro-\nportion of repetitions should receive lower cross-\nentropy estimates on average.\nTable 3 presents the results of a stepwise regression\nanalysis predicting the average cross-entropy estimates\nwith the aforementioned predictors. R2refers to the ﬁt of\nthe model, where a value of 1 indicates that the model ac-\ncounts for all of the variance in the outcome variable (i.e., a\nperfectly linear relationship between the predictors and the\ncross-entropy estimates). The slope of the line measured\nfor each predictor, denoted by \f, represents the change in\nthe outcome resulting from a unit change in the predictor.\nFor the Finite Context model (BOTH+), four of the\nﬁve predictors explained 53% of the variance in the cross-\nentropy estimates. As predicted, cross-entropy decreased\nas the number of tokens increased, suggesting that the\nmodel learned from past tokens in the sequence. What is\nmore, cross-entropy increased as the vocabulary increased,\nas well as when the proportion of monophonic or improb-\nable tokens increased, though the latter two predictors had\nlittle effect on the model.\nFor the RNN model, the effect of these predictors was\nstrikingly different. In this case, cross-entropy increased\nwith the proportion of improbable events. Note that this\npredictor played only a minor role for the Finite Context\nmodel, which suggests PPM* may be responsible for the\nmodel’s superior performance. For the remaining predic-\ntors, cross-entropy estimates decreased when the propor-\ntion of adjacent repeated tokens increased. Like the Finite\nContext model, the RNN model also struggled when the\nproportion of monophonic tokens increased, but beneﬁted\nfrom longer sequences featuring smaller vocabularies.\n5. CONCLUSION\nThis study examined the potential for language models to\npredict chords in a large-scale corpus of tonal compositions\nfrom the common-practice period. To that end, we devel-\noped a ﬂexible chord representation scheme that (1) made\nminimal a priori assumptions about the chord typology un-\nderlying tonal music, and (2) allowed us to create a much\nlarger corpus relative to those based on chord annotations.\nOur ﬁndings demonstrate that Finite Context models out-\nperform RNNs, particularly in piano datasets, which sug-\ngests PPM* is responsible for the superior performance,\nsince it assigns a portion of the probability mass to poten-\ntially rare, as-yet-unseen types. A regression analysis gen-\nerally conﬁrmed this hypothesis, with LSTM struggling to\npredict the improbable types from the piano datasets.Model Predictors \f R2\nBOTH+\nNtokens −2.079 .212\nNtypes 1.860 .506\nMonophoni c 0.233 .506\nImprobable 0.076 .530\nLSTM\nImprobable 0.463 .318\nRepetition −0.558 .375\nNtypes 0.817 .504\nMonophonic 0.452 .568\nNtokens −0.554 .591\nNote . Each predictor appears in the order speciﬁed by stepwise selection,\nwithR2estimated at each step. However, \fpresents the standardized\nbetas estimated in the model’s ﬁnal step.\nTable 3 . Stepwise regression analysis predicting the av-\nerageHmestimated for each composition from the best-\nperforming model conﬁgurations with characteristic fea-\ntures of the corpus.\nTo our knowledge, this is the ﬁrst language-modeling\nstudy to use such a large vocabulary of chord types, though\nthis approach is far more common in the NLP community,\nwhere the selected corpus can sometimes contain millions\nof distinct word types. Our goal in doing so was to bridge\nthe gulf between the most current data-driven methods for\nmelodic and harmonic prediction on the one hand [24], and\napplications of chord typologies for the creation of cor-\npora using expert analysts on the other [3]. Indeed, despite\nrecent efforts to determine the efﬁcacy of language mod-\nels for annotated corpora [11, 15], relatively little has been\ndone to develop unsupervised methods for the discovery of\ntonal harmony in predictive contexts.\nOne serious limitation of the architectures examined\nin this study is their unwavering commitment to the sur-\nface. Rather than skipping seemingly inconsequential on-\nsets, such as those containing embellishing tones or repeti-\ntions, these models predict every onset in their path. As a\nresult, the model conﬁgurations examined here attempted\nto predict tonal (pitch) content rather than tonal harmonic\nprogressions per se. In our view, word class models could\nprovide the necessary bridge between the bottom-up and\ntop-down approaches just described by reducing the vo-\ncabulary of surface simultaneities to its most essential har-\nmonies [2]. Along with prediction tasks, these models\ncould then be adapted for sequence generation and auto-\nmatic harmonic analysis, and in so doing, provide converg-\ning evidence that the statistical regularities characterizing\na tonal corpus also reﬂect the order in which its constituent\nharmonies occur.\n6. ACKNOWLEDGMENTS\nThis project has received funding from the European Re-\nsearch Council (ERC) under the European Union’s Hori-\nzon 2020 research and innovation programme (grant agree-\nment n\u000e670035).216 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20187. REFERENCES\n[1] J. Albrecht and D. Shanahan. The use of large corpora\nto train a new type of key-ﬁnding algorithm: An im-\nproved treatment of the minor mode. Music Perception ,\n31(1):59–67, 2013.\n[2] P. F. Brown, V . J. Della Pietra, P. V . deSouza, J. C. Lai,\nand R. L. Mercer. Class-based n-gram models of nat-\nural language. Computational Linguistics , 18(4):467–\n479, 1992.\n[3] J. A. Burgoyne, J. Wild, and I. Fujinaga. An Expert\nGround Truth Set for Audio Chord Recognition and\nMusic Analysis. In Proceedings of the 12th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , Miami, USA, 2011.\n[4] K. Cho, B. van Merrienboer, D. Bahdanau, and Y . Ben-\ngio. On the Properties of Neural Machine Translation:\nEncoder-Decoder Approaches. arXiv:1409.1259 [cs,\nstat], September 2014.\n[5] J. G. Cleary and I H. Witten. Data compression\nusing adaptive coding and partial string matching.\nIEEE Transactions on Communications , 32(4):396–\n402, 1984.\n[6] D. Conklin. Representation and discovery of vertical\npatterns in music. In C. Anagnostopoulou, M. Fer-\nrand, and A. Smaill, editors, Music and Artiﬁcal Intel-\nligence: Lecture Notes in Artiﬁcial Intelligence 2445 ,\nvolume 2445, pages 32–42. Springer-Verlag, 2002.\n[7] D. Conklin. Multiple viewpoint systems for music clas-\nsiﬁcation. Journal of New Music Research , 42(1):19–\n26, 2013.\n[8] D. Conklin and I. H. Witten. Multiple viewpoint sys-\ntems for music prediction. Journal of New Music Re-\nsearch , 24(1):51–73, 1995.\n[9] T. J. DiCiccio and B. Efron. Bootstrap conﬁdence in-\ntervals. Statistical Science , 11(3):189–228, 1996.\n[10] S. Flossmann, W. Goebl, M. Grachten, B. Nie-\ndermayer, and G. Widmer. The Magaloff project:\nAn interim report. Journal of New Music Research ,\n39(4):363–377, 2010.\n[11] B. Di Giorgi, S. Dixon, M. Zanoni, and A. Sarti. A\ndata-driven model of tonal chord sequence complexity.\nIEEE/ACM Transactions on Audio, Speech and Lan-\nguage Processing , 25(11):2237–2250, 2017.\n[12] T. Hedges and G. A. Wiggins. The prediction of\nmerged attributes with multiple viewpoint systems.\nJournal of New Music Research , 2016.\n[13] S. Hochreiter and J. Schmidhuber. Long Short-Term\nMemory. Neural Computing , 9(8):1735–1780, Novem-\nber 1997.[14] D. Kingma and J. Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980 , 2014.\n[15] F. Korzeniowski, D. R. W. Sears, and G. Widmer. A\nlarge-scale study of language models for chord predic-\ntion. In Proceedings of the International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\nCalgary, Canada, 2018.\n[16] G. Melis, C. Dyer, and P. Blunsom. On the state of the\nart of evaluation in neural language models. In Sixth\nInternational Conference on Learning Representations\n(ICLR) , Vancouver, Canada, April 2018.\n[17] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. Cernock ´y, and\nS. Khudanpur. Recurrent neural network based lan-\nguage model. In INTERSPEECH 2010, 11th Annual\nConference of the International Speech Communica-\ntion Association, Makuhari, Chiba, Japan, September\n26-30, 2010 , pages 1045–1048, Chiba, Japan, 2010.\n[18] D. M ¨ullensiefen and M. Pendzich. Court decisions on\nmusic plagiarism and the predictive value of similar-\nity algorithms. Musicæ Scientiæ , Discussion Forum\n4B:257–295, 2009.\n[19] M. T. Pearce. The Construction and Evaluation of Sta-\ntistical Models of Melodic Structure in Music Per-\nception and Composition . Phd thesis, City University,\nLondon, 2005.\n[20] M. T. Pearce, D. Conklin, and G. A. Wiggins. Methods\nfor Combining Statistical Models of Music , pages 295–\n312. Springer Verlag, Heidelberg, Germany, 2005.\n[21] M. T. Pearce and G. A. Wiggins. Improved methods for\nstatistical modelling of monophonic music. Journal of\nNew Music Research , 33(4):367–385, 2004.\n[22] I. Quinn. Are pitch-class proﬁles really “key for key”?\nZeitschrift der Gesellschaft der Musiktheorie , 7:151–\n163, 2010.\n[23] D. R. W. Sears. The Classical Cadence as a Closing\nSchema: Learning, Memory, and Perception . Phd the-\nsis, McGill University, Montreal, Canada, 2016.\n[24] D. R. W. Sears, M. T. Pearce, W. E. Caplin, and\nS. McAdams. Simulating melodic and harmonic ex-\npectations for tonal cadences using probabilistic mod-\nels. Journal of New Music Research , 47(1):29–52,\n2018.\n[25] D. Temperley and D. Sleator. Modeling meter and har-\nmony: A preference-rule approach. Computer Music\nJournal , 23(1):10–27, 1999.\n[26] G. Widmer. Using AI and machine learning to study\nexpressive music performance: Project survey and ﬁrst\nreport. AI Communications , 14(3):149–162, 2001.\n[27] I. H. Witten and T. C. Bell. The zero-frequency prob-\nlem: Estimating the probabilities of novel events in\nadaptive text compression. IEEE Transactions on In-\nformation Theory , 37(4):1085–1094, 1991.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 217"
    },
    {
        "title": "Automatic, Personalized, and Flexible Playlist Generation using Reinforcement Learning.",
        "author": [
            "Shun-Yao Shih",
            "Heng-Yu Chi"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492371",
        "url": "https://doi.org/10.5281/zenodo.1492371",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/18_Paper.pdf",
        "abstract": "Songs can be well arranged by professional music curators to form a riveting playlist that creates engaging listening experiences. However, it is time-consuming for curators to timely rearrange these playlists for fitting trends in future. By exploiting the techniques of deep learning and reinforcement learning, in this paper, we consider music playlist generation as a language modeling problem and solve it by the proposed attention language model with policy gradient. We develop a systematic and interactive approach so that the resulting playlists can be tuned flexibly according to user preferences. Considering a playlist as a sequence of words, we first train our attention RNN language model on baseline recommended playlists. By optimizing suitable imposed reward functions, the model is thus refined for corresponding preferences. The experimental results demonstrate that our approach not only generates coherent playlists automatically but is also able to flexibly recommend personalized playlists for diversity, novelty and freshness.",
        "zenodo_id": 1492371,
        "dblp_key": "conf/ismir/ShihC18",
        "keywords": [
            "professional music curators",
            "ripping playlist experiences",
            "time-consuming rearrangement",
            "deep learning techniques",
            "reinforcement learning",
            "language modeling problem",
            "attention language model",
            "policy gradient",
            "interactive approach",
            "user preferences"
        ],
        "content": "AUTOMATIC, PERSONALIZED, AND FLEXIBLE PLAYLIST\nGENERATION USING REINFORCEMENT LEARNING\nShun-Yao Shih\nNational Taiwan University\nshunyaoshih@gmail.comHeng-Yu Chi\nKKBOX Inc., Taipei, Taiwan\nhenrychi@kkbox.com\nABSTRACT\nSongs can be well arranged by professional music curators\nto form a riveting playlist that creates engaging listening\nexperiences. However, it is time-consuming for curators\nto timely rearrange these playlists for ﬁtting trends in fu-\nture. By exploiting the techniques of deep learning and\nreinforcement learning, in this paper, we consider music\nplaylist generation as a language modeling problem and\nsolve it by the proposed attention language model with\npolicy gradient. We develop a systematic and interactive\napproach so that the resulting playlists can be tuned ﬂex-\nibly according to user preferences. Considering a playlist\nas a sequence of words, we ﬁrst train our attention RNN\nlanguage model on baseline recommended playlists. By\noptimizing suitable imposed reward functions, the model\nis thus reﬁned for corresponding preferences. The ex-\nperimental results demonstrate that our approach not only\ngenerates coherent playlists automatically but is also able\nto ﬂexibly recommend personalized playlists for diversity,\nnovelty and freshness.\n1. INTRODUCTION\nProfessional music curators or DJs are usually able to care-\nfully select, order, and form a list of songs which can give\nlisteners brilliant listening experiences. For a music radio\nwith a speciﬁc topic, they can collect songs related to the\ntopic and sort in a smooth context. By considering pref-\nerences of users, curators can also ﬁnd what they like and\nrecommend them several lists of songs. However, different\npeople have different preferences toward diversity, popu-\nlarity, and etc. Therefore, it will be great if we can reﬁne\nplaylists based on different preferences of users on the ﬂy.\nBesides, as online music streaming services grow, there\nare more and more demands for efﬁcient and effective mu-\nsic playlist recommendation. Automatic and personalized\nmusic playlist generation thus becomes a critical issue.\nHowever, it is unfeasible and expensive for editors to\ndaily or hourly generate suitable playlists for all users\nbased on their preferences about trends, novelty, diversity,\nc\rShun-Yao Shih, Heng-Yu Chi. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Shun-Yao Shih, Heng-Yu Chi. “automatic, personalized, and\nﬂexible playlist generation using reinforcement learning”, 19th Interna-\ntional Society for Music Information Retrieval Conference, Paris, France,\n2018.etc. Therefore, most of previous works try to deal with\nsuch problems by considering some particular assump-\ntions. McFee et al. [14] consider playlist generation as a\nlanguage modeling problem and solve it by adopting statis-\ntical techniques. Unfortunately, statistical method does not\nperform well on small datasets. Pampalk et al. [16] gen-\nerate playlists by exploiting explicit user behaviors such\nas skipping. However, for implicit user preferences on\nplaylists, they do not provide a systematic way to handle\nit.\nAs a result, for generating personalized playlists auto-\nmatically and ﬂexibly, we develop a novel and scalable\nmusic playlist generation system. The system consists of\nthree main steps. First, we adopt Chen et al. ’s work [4]\nto generate baseline playlists based on the preferences of\nusers about songs. In details, given the relationship be-\ntween users and songs, we construct a corresponding bipar-\ntite graph at ﬁrst. With the users and songs graph, we can\ncalculate embedding features of songs and thus obtain the\nbaseline playlist for each songs by ﬁnding their k-nearest\nneighbors. Second, by formulating baseline playlists as\nsequences of words, we can pretrain RNN language model\n(RNN-LM) to obtain better initial parameters for the fol-\nlowing optimization, using policy gradient reinforcement\nlearning. We adopt RNN-LM because not only RNN-LM\nhas better ability of learning information progresses than\ntraditional statistical methods in many generation tasks,\nbut also neural networks can be combined with reinforce-\nment learning to achieve better performances [10]. Finally,\ngiven preferences from user proﬁles and the pretrained pa-\nrameters, we can generate personalized playlists by ex-\nploiting techniques of policy gradient reinforcement learn-\ning with corresponding reward functions. Combining these\ntraining steps, the experimental results show that we can\ngenerate personalized playlists to satisfy different prefer-\nences of users with ease.\nOur contributions are summarized as follows:\n\u000fWe design an automatic playlist generation frame-\nwork, which is able to provide timely recommended\nplaylists for online music streaming services.\n\u000fWe remodel music playlist generation into a se-\nquence prediction problem using RNN-LM which is\neasily combined with policy gradient reinforcement\nlearning method.\n\u000fThe proposed method can ﬂexibly generate suitable\npersonalized playlists according to user proﬁles us-168ing corresponding optimization goals in policy gra-\ndient.\nThe rest of this paper is organized as follows. In Sec-\ntion 2, we introduce several related works about playlist\ngeneration and recommendation. In Section 3, we provide\nessential prior knowledge of our work related to policy gra-\ndient. In Section 4, we introduce the details of our pro-\nposed model, attention RNN-LM with concatenation (AC-\nRNN-LM). In Section 5, we show the effectiveness of our\nmethod and conclude our work in Section 6.\n2. RELATED WORK\nGiven a list of songs, previous works try to rearrange them\nfor better song sequences [1,3,5,12]. First, they construct a\nsong graph by considering songs in playlist as vertices, and\nrelevance of audio features between songs as edges. Then\nthey ﬁnd a Hamiltonian path with some properties, such as\nsmooth transitions of songs [3], to create new sequencing\nof songs. User feedback is also an important considera-\ntion when we want to generate playlists [6, 7, 13, 16]. By\nconsidering several properties, such as tempo, loudness,\ntopics, and artists, of users’ favorite played songs recently,\nauthors of [6, 7] can thus provide personalized playlist\nfor users based on favorite properties of users. Pampalk\net al. [16] consider skip behaviors as negative signals and\nthe proposed approach can automatically choose the next\nsong according to audio features and avoid skipped songs\nat the same time. Maillet et al. [13] provides a more in-\nteractive way to users. Users can manipulate weights of\ntags to express high-level music characteristics and obtain\ncorresponding playlists they want. To better integrate user\nbehavior into playlist generation, several works are pro-\nposed to combine playlist generation algorithms with the\ntechniques of reinforcement learning [11, 20]. Xing et al.\nﬁrst introduce exploration into traditional collaborative ﬁl-\ntering to learn preferences of users. Liebman et al. take\nthe formulation of Markov Decision Process into playlist\ngeneration framework to design algorithms that learn rep-\nresentations for preferences of users based on hand-crafted\nfeatures. By using these representations, they can generate\npersonalized playlist for users.\nBeyond playlist generation, there are several works\nadopting the concept of playlist generation to facilitate\nrecommendation systems. Given a set of songs, Vargas\net al. [18] propose several scoring functions, such as diver-\nsity and novelty, and retrieve the top-K songs with higher\nscores for each user as the resulting recommended list of\nsongs. Chen et al. [4] propose a query-based music rec-\nommendation system that allow users to select a preferred\nsong as a seed song to obtain related songs as a recom-\nmended playlist.\n3. POLICY GRADIENT REINFORCEMENT\nLEARNING\nReinforcement learning has got a lot of attentions from\npublic since Silver et al. [17] proposed a general reinforce-\nment learning algorithm that could make an agent achievesuperhuman performance in many games. Besides, rein-\nforcement learning has been successfully applied to many\nother problems such as dialogue generation modeled as\nMarkov Decision Process (MDP).\nA Markov Decision Process is usually denoted by a tu-\nple(S;A;P;R;\r), where\n\u000f S is a set of states\n\u000f A is a set of actions\n\u000f P(s;a;s0) = Pr[s0js;a]is the transition probability\nthat actionain stateswill lead to state s0\n\u000f R(s;a) =E[rjs;a]is the expected reward that an\nagent will receive when the agent does action ain\nstates.\n\u000f\r2[0;1]is the discount factor representing the im-\nportance of future rewards\nPolicy gradient is a reinforcement learning algorithm to\nsolve MDP problems. Modeling an agent with parameters\n\u0012, the goal of this algorithm is to ﬁnd the best \u0012of a pol-\nicy\u0019\u0012(s;a) = Pr[ajs;\u0012]measured by average reward per\ntime-step\nJ(\u0012) =X\ns2Sd\u0019\u0012(s)X\na2A\u0019\u0012(s;a)R(s;a) (1)\nwhered\u0019\u0012(s)is stationary distribution of Markov chain for\n\u0019\u0012.\nUsually, we assume that \u0019\u0012(s;a)is differentiable with\nrespect to its parameters \u0012, i.e.,@\u0019\u0012(s;a)\n@\u0012exists, and solve\nthis optimization problem Eqn (1) by gradient ascent. For-\nmally, given a small enough \u000b, we update its parameters \u0012\nby\n\u0012 \u0012+\u000br\u0012J(\u0012) (2)\nwhere\nr\u0012J(\u0012) =X\ns2Sd\u0019\u0012(s)X\na2A\u0019\u0012(s;a)r\u0012\u0019\u0012(s;a)R(s;a)\n=E[r\u0012\u0019\u0012(s;a)R(s;a)]\n(3)\n4. THE PROPOSED MODEL\nThe proposed model consists of two main components. We\nﬁrst introduce the structure of the proposed RNN-based\nmodel in Section 4.1. Then in Section 4.2, we formulate\nthe problem as a Markov Decison Process and solve the\nformulated problem by policy gradient to generate reﬁned\nplaylists.\n4.1 Attention RNN Language Model\nGiven a sequence of tokens fw1;w2;:::;wtg, an RNN-\nLM estimates the probability Pr[wtjw1:t\u00001]with a recur-\nrent function\nht=f(ht\u00001;wt\u00001) (4)\nand an output function, usually softmax,\nPr[wt=vijw1:t\u00001] =exp(W>\nviht+bvi)P\nkexp(W>vkht+bvk)(5)Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 169Figure 1 . The structure of our attention RNN language model with concatenation\nwhere the implementation of the function fdepends on\nwhich kind of RNN cell we use, ht2RD,W2RD\u0002V\nwith the column vector Wvicorresponding to a word vi,\nandb2RVwith the scalar bvicorresponding to a word vi\n(Dis the number of units in RNN, and Vis the number of\nunique tokens in all sequences).\nWe then update the parameters of the RNN-LM by\nmaximizing the log-likelihood on a set of sequences with\nsizeN,fs1;s2;:::;sNg, and the corresponding tokens,\nfwsi\n1;wsi\n2;:::;wsi\njsijg.\nL=1\nNNX\nn=1jsnjX\nt=2log Pr[wsn\ntjwsn\n1:t\u00001] (6)\n4.1.1 Attention in RNN-LM\nAttention mechanism in sequence-to-sequence model has\nbeen proven to be effective in the ﬁelds of image caption\ngeneration, machine translation, dialogue generation, and\netc. Several previous works also indicate that attention is\neven more impressive on RNN-LM [15].\nIn attention RNN language model (A-RNN-LM), given\nthe hidden states from time t\u0000Cwstot, denoted as\nht\u0000Cws:t, whereCwsis the attention window size, we want\nto compute a context vector ctas a weighted sum of hid-\nden statesht\u0000Cws:t\u00001and then encode the context vector\nctinto the original hidden state ht.\n\fi=\u0017>tanh(W1ht+W2ht\u0000Cws+i) (7)\n\u000bi=exp(\fi)PCws\u00001\nk=0exp(\fk)(8)\nct=Cws\u00001X\ni=0\u000biht\u0000Cws+i (9)\nh0\nt=W3\u0014ht\nct\u0015\n(10)\nwhere\fis Bahdanau’s scoring style [2], W1;W22\nRD\u0002D, andW32RD\u00022D.4.1.2 Our Attention RNN-LM with concatenation\nIn our work,fs1;s2;:::;sNgandfwsi\n1;wsi\n2;:::;wsi\njsijg\nare playlists and songs by adopting Chen et al. ’s work [4].\nMore speciﬁcally, given a seed song wsi\n1for a playlist si,\nwe ﬁnd top-k approximate nearest neighbors of wsi\n1to for-\nmulate a list of songs fwsi\n1;wsi\n2;:::;wsi\njsijg.\nThe proposed attention RNN-LM with concatenation\n(AC-RNN-LM) is shown in Figure 1. We pad w1:t\u00001to\nw1:Tand concatenate the corresponding h0\n1:Tas the input\nof our RNN-LM’s output function in Eqn (5), where Tis\nthe maximum number of songs we consider in one playlist.\nTherefore, our output function becomes\nPr[wt=vijw1:t\u00001] =exp(W>\nvih0+bvi)P\nkexp(W>vkh0+bvk)(11)\nwhereW2RDT\u0002V,b2RVand\nh0=2\n6664h0\n1\nh0\n2\n...\nh0\nT3\n77752RDT\u00021(12)\n4.2 Policy Gradient\nWe exploit policy gradient in order to optimize Eqn (1),\nwhich is formulated as follows.\n4.2.1 Action\nAn actionais a song id, which is a unique representation\nof each song, that the model is about to generate. The set\nof actions in our problem is ﬁnite since we would like to\nrecommend limited range of songs.\n4.2.2 State\nA statesis the songs we have already recommended in-\ncluding the seed song, fwsi\n1;wsi\n2;:::;wsi\nt\u00001g.170 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20184.2.3 Policy\nA policy\u0019\u0012(s;a)takes the form of our AC-RNN-LM and\nis deﬁned by its parameters \u0012.\n4.2.4 Reward\nRewardR(s;a)is a weighted sum of several reward func-\ntions, i.e.,Ri:s\u0002a7!R. In the following introductions,\nwe formulate 4 important metrics for playlists generation.\nThe policy of our pretrained AC-RNN-LM is denoted as\n\u0019\u0012RNN(s;a)with parameters \u0012RNN , and the policy of our\nAC-RNN-LM optimized by policy gradient is denoted as\n\u0019\u0012RL(s;a)with parameters \u0012RL.\nDiversity represents the variety in a recommended list of\nsongs. Several generated playlists in Chen et al. ’s\nwork [4] are composed of songs with the same artist\nor album. It is not regarded as a good playlist for\nrecommendation system because of low diversity.\nTherefore, we formulate the measurement of the di-\nversity by the euclidean distance between the em-\nbeddings of the last song in the existing playlist,\nws\njsj, and the predicted song, a.\nR1(s;a) =\u0000log(jd(ws\njsj;a)\u0000Cdistancej)(13)\nwhered(\u0001)is the euclidean distance between the em-\nbeddings of ws\njsjanda, andCdistance is a parameter\nthat represents the euclidean distance that we want\nthe model to learn.\nNovelty is also important for a playlist generation sys-\ntem. We would like to recommend something new\nto users instead of recommend something familiar.\nUnlike previous works, our model can easily gener-\nate playlists with novelty by applying a correspond-\ning reward function. As a result, we model reward of\nnovelty as a weighted sum of normalized playcounts\nin periods of time [19].\nR2(s;a) =\u0000log(X\ntw(t)log(pt(a))\nlog(maxa02Apt(a0)))\n(14)\nwherew(t)is the weight of a time period, t, with a\nconstraintP\ntw(t) = 1 ,pt(a)is playcounts of the\nsonga, andAis the set of actions. Note that songs\nwith less playcounts have higher value of R2, and\nvice versa.\nFreshness is a subjective metric for personalized playlist\ngeneration. For example, latest songs is usually\nmore interesting for young people, while older peo-\nple would prefer old-school songs. Here, we arbi-\ntrarily choose one direction for optimization to the\nagent\u0019\u0012RLto show the feasibility of our approach.\nR3(s;a) =\u0000log(Ya\u00001900\n2017\u00001900) (15)\nwhereYais the release year of the song a.Coherence is the major feature we should consider to\navoid situations that the generated playlists are\nhighly rewarded but lack of cohesive listening ex-\nperiences. We therefore consider the policy of our\npretrained language model, \u0019\u0012RNN(s;a), which is\nwell-trained on coherent playlists, as a good enough\ngenerator of coherent playlists.\nR4(s;a) = log(Pr[ajs;\u0012RNN ]) (16)\nCombining the above reward functions, our ﬁnal reward\nfor the action ais\nR(s;a) =\r1R1(s;a) +\r2R2(s;a)+\n\r3R3(s;a) +\r4R4(s;a)(17)\nwhere the selection of \r1,\r2,\r3, and\r4depends on dif-\nferent applications.\nNote that although we only list four reward functions\nhere, the optimization goal Rcan be easily extended by a\nlinear combination of more reward functions.\n5. EXPERIMENTS AND ANALYSIS\nIn the following experiments, we ﬁrst introduce the details\nof dataset and evaluation metrics in Section 5.1 and train-\ning details in Section 5.2. In Section 5.3, we compare pre-\ntrained RNN-LMs with different mechanism combination\nby perplexity to show our proposed AC-RNN-LM is more\neffectively and efﬁciently than others. In order to demon-\nstrate the effectiveness of our proposed method, AC-RNN-\nLM combined with reinforcement learning, we adopt three\nstandard metrics, diversity, novelty, and freshness (cf. Sec-\ntion 5.1) to validate our models in Section 5.4. More-\nover, we demonstrate that it is effortless to ﬂexibly ma-\nnipulate the properties of resulting generated playlists in\nSection 5.5. Finally, in Section 5.6, we discuss the details\nabout the design of reward functions with given preferred\nproperties.\n5.1 Dataset and Evaluation Metrics\nThe playlist dataset is provided by KKBOX Inc., which is\na regional leading music streaming company. It consists of\n10;000playlists, each of which is composed of 30songs.\nThere are 45;243unique songs in total.\nFor validate our proposed approach, we use the metrics\nas follows.\nPerplexity is calculated based on the song probability dis-\ntributions, which is shown as follows.\nperplexity =e1\nNPN\nn=1P\nx\u0000q(x) logp(x)\nwhereNis the number of training samples, xis a\nsong in our song pool, pis the predicted song prob-\nability distribution, and qis the song probability dis-\ntribution in ground truth.\nDiversity is computed as different unigrams of artists\nscaled by he total length of each playlist, which is\nmeasured by Distinct-1 [9]Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 171Figure 2 . Log-perplexity of different pretrained models on\nthe dataset under different training steps\nNovelty is designed for recommending something new to\nusers [19]. The more the novelty is, the lower the\nmetric is.\nFreshness is directly measured by the average release\nyear of songs in each playlist.\n5.2 Training Details\nIn the pretraining and reinforcement learning stage, we\nuse 4 layers and 64 units per layer in all RNN-LM with\nLSTM units, and we choose T= 30 for all RNN-LM\nwith padding and concatenation. The optimizer we use is\nAdam [8]. The learning rates for pretraining stage and re-\ninforcement learning stage are empirically set as 0.001 and\n0.0001, respectively.\n5.3 Pretrained Model Comparison\nIn this section, we compare the training error of RNN-LM\ncombining with different mechanisms. The RNN-LM with\nattention is denoted as A-RNN-LM, the RNN-LM with\nconcatenation is denoted as C-RNN-LM, and the RNN-\nLM with attention and concatenation is denoted as AC-\nRNN-LM. Figure 2 reports the training error of different\nRNN-LMs as log-perplexity which is equal to negative log-\nlikelihood under the training step from 1to500;000. Here\none training step means that we update our parameters by\none batch. As shown in Figure 2, the training error of\nour proposed model, AC-RNN-LM, can not only decrease\nfaster than the other models but also reach the lowest value\nat the end of training. Therefore, we adopt AC-RNN-LM\nas our pretrained model.\nWorth noting that the pretrained model is developed for\ntwo purposes. One is to provide a good basis for fur-\nther optimization, and the other is to estimate transitionTable 1 . Weights of reward functions for each model\nModel \r1\r2\r3\r4\nRL-DIST 0.5 0.0 0.0 0.5\nRL-NOVELTY 0.0 0.5 0.0 0.5\nRL-YEAR 0.0 0.0 0.5 0.5\nRL-COMBINE 0.2 0.2 0.2 0.4\nTable 2 . Comparison on different metrics for playlist gen-\neration system (The bold text represents the best, and the\nunderline text represents the second)\nModel Diversity Novelty Freshness\nEmbedding [4] 0.32 0.19 2007.97\nAC-RNN-LM 0.39 0.20 2008.41\nRL-DIST 0.44 0.20 2008.37\nRL-NOVELTY 0.42 0.05 2012.89\nRL-YEAR 0.40 0.19 2006.23\nRL-COMBINE 0.49 0.18 2007.64\nprobabilities of songs in the reward function. Therefore,\nwe simply select the model with the lowest training er-\nror to be optimized by policy gradient and an estimator of\nPr[ajs;\u0012RNN ](cf. Eqn (16)).\n5.4 Playlist Generation Results\nAs shown in Table 2, to evaluate our method, we compare\n6 models on 3 important features, diversity, novelty, and\nfreshness (cf. Section 5.1), of playlist generation system.\nThe details of models are described as follows. Embed-\nding represents the model of Chen et al. ’s work [4]. Chen\net al. construct the song embedding by relationships be-\ntween user and song and then ﬁnds approximate knearest\nneighbors for each song. RL-DIST, RL-NOVELTY , RL-\nYEAR, and RL-COMBINE are models that are pretrained\nand optimized by the policy gradient method (cf. Eqn (17))\nwith different weights, respectively, as shown in Table 1.\nThe experimental results show that for single objec-\ntive such as diversity, our models can accurately gener-\nate playlists with corresponding property. For example,\nRL-Year can generate a playlist which consists of songs\nwith earliest release years than Embedding and AC-RNN-\nLM. Besides, even when we impose our model with mul-\ntiple reward functions, we can still obtain a better resulting\nplaylist in comparison with Embedding and AC-RNN-LM.\nSample result is shown in Figure 3.\nFrom Table 2, we demonstrate that by using appropriate\nreward functions, our approach can generate playlists to ﬁt\nthe corresponding needs easily. We can systematically ﬁnd\nmore songs from different artists (RL-DIST), more songs\nheard by fewer people (RL-NOVELTY), or more old songs\nfor some particular groups of users (RL-YEAR).\n5.5 Flexible Manipulating Playlist Properties\nAfter showing that our approach can easily ﬁt several\nneeds, we further investigate the inﬂuence of \rto the re-172 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 3 . Sample playlists generated by our approach. The\nleft one is generated by Embedding [4] and the right one is\ngenerated by RL-COMBINE.\nsulting playlists. In this section, several models are trained\nwith the weight \r2from 0:0to1:0to show the vari-\nances in novelty of the resulting playlists. Here we keep\n\r2+\r4= 1:0and\r1=\r3= 0and ﬁx the training steps\nto10;000.\nAs shown in Figure 4, novelty score generally decreases\nwhen\r2increases from 0:0to1:0but it is also possible\nthat the model may sometimes ﬁnd the optimal policy ear-\nlier than expectation such as the one with \r2= 0:625.\nNevertheless, in general, our approach can not only let the\nmodel generate more novel songs but also make the extent\nof novelty be controllable. Besides automation, this kind\nof ﬂexibility is also important in applications.\nTake online music streaming service as an example,\nwhen the service provider wants to recommend playlists\nto a user who usually listens to non-mainstream but fa-\nmiliar songs (i.e., novelty score is 0:4), it is more suitable\nto generate playlists which consists of songs with novelty\nscores equals to 0:4instead of generating playlists which is\ncomposed of 60% songs with novelty scores equals to 0:0\nand40% songs with novelty scores equals to 1:0. Since\nusers usually have different kinds of preferences on each\nproperty, to automatically generate playlists ﬁtting needs\nof each user, such as novelty, becomes indispensable. The\nexperimental results verify that our proposed approach can\nsatisfy the above application.\n5.6 Limitation on Reward Function Design\nWhen we try to deﬁne a reward function Rifor a prop-\nerty, we should carefully avoid the bias from the state s.\nIn other words, reward functions should be speciﬁc to the\ncorresponding feature we want. One common issue is that\nFigure 4 . Novelty score of playlists generated by different\nimposing weights\nthe reward function may be inﬂuenced by the number of\nsongs in state s. For example, in our experiments, we adopt\nDistinct-1 as the metric for diversity. However, we cannot\nalso adopt Distinct-1 as our reward function directly be-\ncause it is scaled by the length of playlists which results\nin all actions from states with fewer songs will be bene-\nﬁted. Therefore, difference between cR1and Distinct-1 is\nthe reason that RL-DIST does not achieve the best perfor-\nmance in Distinct-1 (cf. Table 1). In summary, we should\nbe careful to design reward functions, and sometimes we\nmay need to formulate another approximation objective\nfunction to avoid biases.\n6. CONCLUSIONS AND FUTURE WORK\nIn this paper, we develop a playlist generation system\nwhich is able to generate personalized playlists automat-\nically and ﬂexibly. We ﬁrst formulate playlist generation\nas a language modeling problem. Then by exploiting the\ntechniques of RNN-LM and reinforcement learning, the\nproposed approach can ﬂexibly generate suitable playlists\nfor different preferences of users.\nIn our future work, we will further investigate the pos-\nsibility to automatically generate playlists by considering\nqualitative feedback. For online music streaming service\nproviders, professional music curators will give qualitative\nfeedback on generated playlists so that research develop-\ners can improve the quality of playlist generation system.\nQualitative feedback such as ‘songs from diverse artists\nbut similar genres’ is easier to be quantitative. We can de-\nsign suitable reward functions and generate corresponding\nplaylists by our approach. However, other feedback such\nas ‘falling in love playlist’ is more difﬁcult to be quantita-\ntive. Therefore, we will further adopt audio features and\nexplicit tags of songs in order to provide a better playlist\ngeneration system.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 1737. REFERENCES\n[1] Masoud Alghoniemy and Ahmed H. Tewﬁk. A net-\nwork ﬂow model for playlist generation. In Proceed-\nings of International Conference on Multimedia and\nExpo , pages 329–332, 2001.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. Neural machine translation by jointly learning to\nalign and translate. In International Conference on\nLearning Representations , 2015.\n[3] Rachel M. Bittner, Minwei Gu, Gandalf Hernandez,\nEric J. Humphrey, Tristan Jehan, P. Hunter McCurry,\nand Nicola Montecchio. Automatic playlist sequenc-\ning and transitions. In Proceedings of the 18th Inter-\nnational Conference on Music Information Retrieval ,\npages 472–478, 2017.\n[4] Chih-Ming Chen, Ming-Feng Tsai, Yu-Ching Lin, and\nYi-Hsuan Yang. Query-based music recommendations\nvia preference embedding. In Proceedings of the ACM\nConference Series on Recommender Systems , pages\n79–82, 2016.\n[5] Arthur Flexer, Dominik Schnitzer, Martin Gasser, and\nGerhard Widmer. Playlist generation using start and\nend songs. In Proceedings of the 9th International\nSociety for Music Information Retrieval Conference ,\npages 173–178, 2008.\n[6] Negar Hariri, Bamshad Mobasher, and Robin Burke.\nContext-aware music recommendation based on latent-\ntopic sequential patterns. In Proceedings of the Sixth\nACM Conference on Recommender Systems , RecSys\n’12, pages 131–138, New York, NY , USA, 2012. ACM.\n[7] Dietmar Jannach, Lukas Lerche, and Iman\nKamehkhosh. Beyond ”hitting the hits”: Gener-\nating coherent music playlist continuations with the\nright tracks. In Proceedings of the 9th ACM Confer-\nence on Recommender Systems , RecSys ’15, pages\n187–194, New York, NY , USA, 2015. ACM.\n[8] Diederik P. Kingma and Jimmy Ba. Adam: A method\nfor stochastic optimization. CoRR , abs/1412.6980,\n2014.\n[9] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng\nGao, and Bill Dolan. A diversity-promoting objec-\ntive function for neural conversation models. CoRR ,\nabs/1510.03055, 2015.\n[10] Jiwei Li, Will Monroe, Alan Ritter, Michel Gal-\nley, Jianfeng Gao, and Dan Jurafsky. Deep rein-\nforcement learning for dialogue generation. CoRR ,\nabs/1606.01541, 2016.\n[11] Elad Liebman and Peter Stone. DJ-MC: A\nreinforcement-learning agent for music playlist\nrecommendation. CoRR , abs/1401.1880, 2014.\n[12] Beth Logan. Content-based playlist generation: Ex-\nploratory experiments. 2002.[13] Franc ¸ois Maillet, Douglas Eck, Guillaume Desjardins,\nand Paul Lamere. Steerable playlist generation by\nlearning song similarity from radio station playlists. In\nProceedings of the 10th International Society for Mu-\nsic Information Retrieval Conference , 2009.\n[14] Brian McFee and Gert Lanckriet. The natural language\nof playlists. In Proceedings of the 12th International\nSociety for Music Information Retrieval Conference ,\npages 537–542, 2011.\n[15] Hongyuan Mei, Mohit Bansal, and Matthew R. Walter.\nCoherent dialogue with attention-based language mod-\nels. In The Thirty-First AAAI Conference on Artiﬁcial\nIntelligence , 2016.\n[16] Elias Pampalk, Tim Pohle, and Gerhard Widmer. Dy-\nnamic playlist generation based on skipping behavior.\nInProceedings of the 6th International Society for Mu-\nsic Information Retrieval Conference , pages 634–637,\n2005.\n[17] David Silver, Thomas Hubert, Julian Schrittwieser,\nIoannis Antonoglou, Matthew Lai, Arthur Guez, Marc\nLanctot, Laurent Sifre, Dharshan Kumaran, Thore\nGraepel, Timothy Lillicrap, Karen Simonyan, and\nDemis Hassabis. Mastering chess and shogi by self-\nplay with a general reinforcement learning algorithm.\n2017.\n[18] Sa ´ul Vargas. New approaches to diversity and novelty\nin recommender systems. In Proceedings of the Fourth\nBCS-IRSG Conference on Future Directions in Infor-\nmation Access , FDIA’11, pages 8–13, Swindon, UK,\n2011. BCS Learning & Development Ltd.\n[19] Yuan Cao Zhang, Diarmuid ´O S ´eaghdha, Daniele\nQuercia, and Tamas Jambor. Auralist: Introducing\nserendipity into music recommendation. In Proceed-\nings of the Fifth ACM International Conference on Web\nSearch and Data Mining , WSDM ’12, pages 13–22,\nNew York, NY , USA, 2012. ACM.\n[20] Xinxi Wang Zhe Xing and Ye Wang. Enhancing col-\nlaborative ﬁltering music recommendation by balanc-\ning exploration and exploitation. In Proceedings of the\n15th International Society for Music Information Re-\ntrieval Conference , pages 445–450, 2014.174 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Summarizing and Comparing Music Data and Its Application on Cover Song Identification.",
        "author": [
            "Diego Furtado Silva",
            "Felipe Falcão",
            "Nazareno Andrade"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492521",
        "url": "https://doi.org/10.5281/zenodo.1492521",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/36_Paper.pdf",
        "abstract": "While there is a multitude of music information retrieval algorithms that have distance functions as their core procedure, comparing the similarity between recordings is a costly procedure. At the same, the recent growth of digital music repositories makes necessary the development of novel time- and memory-efficient algorithms to deal with music data. One particularly interesting idea on the literature is transforming the music data into reduced representations, improving the memory usage and reducing the time necessary to assess the similarity. However, these techniques usually add other issues, such as an expensive preprocessing or a reduced retrieval performance. In this paper, we propose a novel method to summarize a recording in small snippets based on its self-similarity information. Besides, we present a simple way to compare other recordings to these summaries. We demonstrate, in the scenario of cover song identification, that our method is more than one order of magnitude faster than state-of-the-art adversaries, at the same time that the retrieval performance is not affected significantly. Additionally, our method is incremental, which allows the easy and fast update of the database when a new song needs to be inserted into the retrieval system.",
        "zenodo_id": 1492521,
        "dblp_key": "conf/ismir/SilvaFA18",
        "keywords": [
            "distance functions",
            "music data",
            "novel time- and memory-efficient algorithms",
            "reduced representations",
            "self-similarity information",
            "simple way to compare",
            "incremental update",
            "cover song identification",
            "state-of-the-art adversaries",
            "retrieval performance"
        ],
        "content": "SUMMARIZING AND COMPARING MUSIC DATA AND ITS\nAPPLICATION ON COVER SONG IDENTIFICATION\nDiego Furtado Silva\nDepartamento de Computac ¸ ˜ao\nUniversidade Federal de S ˜ao Carlos\nS˜ao Carlos, Brazil\ndiegofs@ufscar.brFelipe Vieira Falc ˜ao, Nazareno Andrade\nDepartamento de Sistemas e Computac ¸ ˜ao\nUniversidade Federal de Campina Grande\nCampina Grande, Brazil\nffelipev,nazareno g@lsd.ufcg.edu.br\nABSTRACT\nWhile there is a multitude of music information retrieval\nalgorithms that have distance functions as their core pro-\ncedure, comparing the similarity between recordings is a\ncostly procedure. At the same, the recent growth of digi-\ntal music repositories makes necessary the development of\nnovel time- and memory-efﬁcient algorithms to deal with\nmusic data. One particularly interesting idea on the lit-\nerature is transforming the music data into reduced rep-\nresentations, improving the memory usage and reducing\nthe time necessary to assess the similarity. However, these\ntechniques usually add other issues, such as an expensive\npreprocessing or a reduced retrieval performance. In this\npaper, we propose a novel method to summarize a record-\ning in small snippets based on its self-similarity informa-\ntion. Besides, we present a simple way to compare other\nrecordings to these summaries. We demonstrate, in the sce-\nnario of cover song identiﬁcation, that our method is more\nthan one order of magnitude faster than state-of-the-art ad-\nversaries, at the same time that the retrieval performance\nis not affected signiﬁcantly. Additionally, our method is\nincremental, which allows the easy and fast update of the\ndatabase when a new song needs to be inserted into the\nretrieval system.\n1. INTRODUCTION\nWith the arising of digital music platforms and the con-\nsequent growth of music data repositories, we have wit-\nnessed an increasing interest in fast methods for mining\nthis kind of data. Organizing, searching, and ﬁnding pat-\nterns in large repositories require algorithms that are ef-\nﬁcient in memory and time while providing an accurate\nperformance.\nSeveral algorithms proposed for different music\nmining and information retrieval rely on comparing\n(dis)similarities among the recordings of interest. One is-\nc\rDiego Furtado Silva, Felipe Vieira Falc ˜ao, Nazareno An-\ndrade. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Diego Furtado Silva, Felipe Vieira\nFalc˜ao, Nazareno Andrade. “Summarizing and Comparing Music Data\nand Its Application on Cover Song Identiﬁcation”, 19th International So-\nciety for Music Information Retrieval Conference, Paris, France, 2018.sue regarding this approach is the scalability of these meth-\nods, since comparing distances is usually a costly proce-\ndure.\nThe cover song identiﬁcation (CSI) is one task that usu-\nally is assessed by similarity-based methods. Most work\non advancing the knowledge in CSI is based on creating\nor adapting new similarity measures and algorithms for\ncomparing the recordings [8, 11, 19] or fusing features or\ndistances to improve the retrieval performance [6, 17, 25].\nWhile some efforts point to the direction of improving\nCSI runtime [4, 10, 18, 24], the majority of papers on\nCSI rely on quadratic algorithms to compare each pair of\nsongs [3,21,26–28], which may difﬁcult its application on\nlarge databases.\nOne particular idea on speeding up the CSI was pre-\nsented by Silva, Souza and Batista [24]. The authors pro-\nposed a training phase to ﬁnd what they called “triplets”,\nwhich are three short excerpts of each original recording\nthatsummarize them, i.e., that represent the songs with a\nreduced amount of data. When comparing a new query\nagainst these summarized data, the runtime for the distance\ncalculations drastically reduces, since it is proportional to\nthe length of the feature vectors under comparison.\nWhile summarizing tracks can signiﬁcantly improve\nretrieval runtime, the triplets technique has some con-\ntrivances that make its use difﬁcult. Most importantly, its\ntraining phase is costly in time and memory. Also, it con-\nsiders that more than one original or authorized version\nof each song is available: the method depends on measur-\ning the distance of each candidate excerpt to several other\nsegments of the same “class label.” Finally, once a new\nrecording is added to the dataset, the training phase must\nbe recomputed, since the method to choose the summaries\nalso relies on comparing songs from different labels to an-\nalyze the class separability.\nIn this work, we also leverage the idea of summariz-\ning recordings for fast retrieval. However, we proposed a\nnew suite of methods for summarizing and comparing mu-\nsic data that makes these two usually costly steps simpler\nand faster. The methods put forward are based on a fast\nsubsequence similarity join algorithm that achieves good\nretrieval performances and can easily and quickly incre-\nment the reference dataset when a new original recording\nis presented.732The suit of methods proposed in this work has the fol-\nlowing contributions:\n\u000fconsiderably higher speed to summarize recordings\ncompared to the state-of-the-art;\n\u000fa reduction of an order of magnitude in the runtime\nof the comparison and retrieval phase compared to\nrecent proposals of scalable algorithms;\n\u000fno signiﬁcant loss of retrieval performance is in-\ncurred in process of speeding up summarization; and\n\u000fbecause our summarization method solely relies on\nthe recording being summarized, the method is nat-\nurally parallelizable and incremental.\nFigure 1 illustrates the pipeline of our method.\nOriginal\nrecordingsChroma-based\nfeaturesSelf-similarity\njoin\n...Reference dataQuerySummaries\nComparing____________Final\nranking\nFigure 1 : The pipeline of our method consists of summa-\nrizing the reference dataset using similarity joins and, for\neach query, comparing it to the summaries to achieve a\nranking by similarity.\nThis paper is organized as follow: Section 2 introduces\na background on the task of cover song recognition and a\nfew related work. Section 3 presents our summarizing and\ncomparing techniques, which composes the proposed suite\nfor fast similarity recover. Section 4 presents the experi-\nmental evaluation of our method. Section 5 discuss some\nideas on how further improve our proposal. Finally, Sec-\ntion 6 concludes this work.\n2. BACKGROUND AND RELATED WORK\nThe main focus of this paper is the cover song identiﬁca-\ntion (CSI) task. A cover song is a generic name to refer for\nany recording that is a new version of an original record-\ning. While it may represent an attempt to make a faithfully\nreproduction of the original work, covers usually widely\ndiffers on many characteristics, such as key, timbre, struc-\nture and tempo, which makes CSI a difﬁcult task.\nTo deal with this variation, several methods provide in-\nvariance to these issues to CSI algorithms. One example\nis the Optimal Transposition Index (OTI) [20], which pro-\nvides key invariance. This algorithm starts by calculating\nand storing a global pitch proﬁle of the recordings under\ncomparison. Using these proﬁles, it estimates the differ-\nence in key of the songs and transpose one of their feature\nvectors so that the tracks have the same (estimated) key.Tempo differences also motivate the need for invari-\nance. Several similarity methods for CSI proposed in the\nliterature are based on a dynamic programming algorithm\nto dynamically align the compared recordings [6,8,21,25].\nThis kind of algorithm provides invariance to tempo at the\ncost of relying on a costly alignment algorithm.\nFor this reason, techniques which are chieﬂy concerned\nwith the runtime of CSI systems usually apply lock-step\nmeasures such as the Euclidean distance. In these cases,\nproviding tempo invariance in the feature level is a com-\nmon approach. One option for that is smoothing the feature\nvectors, an approach that is adopted by some chroma-based\nfeatures deﬁnitions, such as the Chroma Energy Normal-\nized Statistics (CENS) [16].\nMany methods for CSI in the literature, if not all, use\na pipeline that includes techniques to provide invariances\nand a distance measure calculation. One example is the\nalready mentioned Triplets [24]. Speciﬁcally, this method\nuses CENS and OTI in its process.\nTriplets summarizes the CENS from each reference\n(original) recording in three short excerpts that are maxi-\nmally close to excerpts (subsequences) from the same song\nand far from the excerpts from other pieces. Once a query\nis presented to the CSI system, it rotates the query accord-\ning OTI and compares it to the summaries. On the one\nhand, the summarization signiﬁcantly improves the run-\ntime to assess a query. On the other hand, the step of ﬁnd-\ning the triplets is prohibitive thanks to the high number of\ndistance calculations it requires.\nAnother work from the same research group proposes\nto identify covers assessing subsequence similarity joins\nby using the similarity matrix proﬁle, or SiMPle [23]. The\nSiMPle is a representation of the subsequence similarity\njoin, which is the task of ﬁnding the nearest neighbor of\neach subsequence from a frame-level feature vector among\nall the subsequences of another vector. Particularly, this\noperation is called AB-join. The operation of calculating\nof the best match between a subsequence of a song to itself\n(disregarding trivial matches) is referred to as self-join.\nThe join operation returns two pieces of information:\nthe SiMPle and the SiMPle index. While the SiMPle stores\nthe distance of each subsequence to its nearest neigh-\nbor, the SiMPle index indicate which subsequence is such\nneighbor.\nThe main intuition behind SiMPle in the CSI domain is\nthat comparing a query to its corresponding original ver-\nsion tends to return small distances. Conversely, compar-\ning the query to a recording of another song tends to pro-\nduce high subsequence distances. As such, the authors de-\nﬁned the distance between a query Band a candidate orig-\ninal recording Aas:\ndist(A; B) =median (SiMPle (B; A)) (1)\nWhile in the SiMPle paper the SiMPle-based CSI is\nperformed over the AB-join operation, the authors demon-\nstrate other applications relying on the self-join procedure.\nFor instance, it is possible to use the SiMPle and the SiM-\nPle index to ﬁnd music thumbnails (most repeated subse-Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 733quence, given by the neighbors stored in the index) and\npatterns that are faithfully reproduced in different times of\nthe song or are the most different excerpts in the recording\n(small and high values in the SiMPle, respectively).\nHowever, computing the distance between a high num-\nber of subsequence pairs is a costly operation. To speed\nup the SiMPle calculation, the authors used MASS, a\nFast Fourier Transform-based algorithm to perform a fast\nsubsequence similarity search under the Euclidean dis-\ntance [14]. The Euclidean distance (ED) between two vec-\ntors of nelements is calculated as:\nED(A; B) =vuutnX\ni=1(ai)2+nX\ni=1(bi)2\u00002(A\u0001B)(2)\nWhen using the ED to ﬁnd the best match between a\nsubsequence and a long feature vector, we may slide the\nshort sequence along the longer one. The main idea be-\nhind MASS is substituting the required dot product by a\nFFT-based algorithm for calculating the cross-correlation\nbetween the compared vectors, often referred to as sliding\ndot-product. Besides, we can pre-calculate the quadratic\nsums required by the ED and reuse it when necessary.\nConsider nthe length of the long feature vector and m\nthe length of the assessed subsequence. The main advan-\ntage of using MASS is that it reduces the subsequence sim-\nilarity search’s complexity from O(n m)toO(n log n )by\nusing the FFT to ﬁnd the windowed dot-products instead\na brute-force approach [29]. Moreover, these dot-products\ncan be reused to calculate SiMPle even faster [22].\nIn this paper, we propose a new method that sums up\nthe advantages of Triplets and SiMPle in a single solution.\nOur algorithm is described in the next section.\n3. SUMMARIZING AND COMPARING\nRECORDINGS\nWe propose the Summarizing and Comparing (SuCo)\nmethod. As the name suggests, it is split into two main\nprocedures. The ﬁrst one summarizes the reference record-\nings based on SiMPle. To ensure the time efﬁciency of\nour method, we use the faster version of SiMPle’s algo-\nrithm [22]. The second part refers to the way that a query is\ncompared to these summaries to estimate a distance value\nbetween the query and each reference recording.\n3.1 Summarization\nSummarizing music ﬁles is not a novel procedure. Al-\nthough a few algorithm use it as a intermediate step (e.g.\nTriplets for CSI), it is usually the ﬁnal procedure in some\nspeciﬁc tasks, such as thumbnailing [1]. In this work,\nwe use the SiMPle to summarize music data as the ﬁrst\nstep of our algorithm. Using this representation of subse-\nquences similarities, we summarize the music ﬁles in ﬁve\nexcerpts1using two different approaches, which are de-\nscribed in the next sub-sections.\n1The number of summaries per song is a parameter, which we set to\n5. For details, please refer to Section 4.2.The summarization step can be seen as a training phase,\nsimilar to what is done by Triplets. However, while sum-\nmarizing in Triplets depends on comparing each record-\ning with the entire dataset, our approach processes each\nrecording independently. This implies that (i)summariza-\ntion in SuCo is naturally parallelizable, and (ii)once a new\noriginal recording is added, it is only necessary to summa-\nrize it and add this summary to our set of summaries. The\nlatter operation takes only hundredths of a second.\n3.1.1 Thumbnailing\nThumbnailing relies on summarizing a recording in one\nshort segment that best represents it. A thumbnail en-\nables for example a listener to quickly identify a song or\nits marked characteristics.\nA possible deﬁnition of a good thumbnail is the excerpt\nof the song that is most times repeated [1]. Based on this\ndeﬁnition, Silva et al. [22] use the subsequence that most\nappears in the SiMPle index as the thumbnail of a track.\nThis is the subsequence that is most times considered the\nnearest neighbor of other fragments. In practical terms, the\nthumbnail is the mode of the SiMPle index. In case of a tie,\nthe subsequence chosen is the one with lowest mean dis-\ntance to the segments that point at it in the SiMPle index.\nIn SuCo we use this same step as our ﬁrst summary,\nand combine it with four other in a set of ﬁve segments\nthat summarizes each recording. The extra segments are\nconsider that is desirable to extract summaries that faith-\nfully describe the song but they need to be diverse. In other\nwords, we avoid describing music with similar excerpts, as\nthis aggregates little information to the retrieval step.\nThat said, after we choose a summary, we exclude from\nthe choices of next summaries all the subsequences that\nhave it as the nearest neighbor. From a practical stand-\npoint, we keep the count of times that each subsequence is\ndenoted as the nearest neighbor in the SiMPle index and\nuse this information to decide the next summary. When\nwe select a subsequence as a summary, we turn to zero the\ncount regarding each of the subsequences that point at the\ncurrent summary in the SiMPle index.\nSimilarly, we make subsequences around each picked\nsummary also ineligible for next summaries. Let pbe the\nposition of the current thumbnail and wa constant deﬁned\nas one-quarter of the assessed subsequences’ length. We\nturn to zero the neighbor count for all subsequences start-\ning at pi2[p\u0000w; p+w].\n3.1.2 Diverse repeated pattern\nWhile the thumbnail-based summaries rely on the SiMPle\nindex, we also propose a summarization method based on\nthe distances stored by SiMPle. We count on the fact that\nsmall distances mean faithfully repeated patterns. These\nexcerpts are very likely to be more precisely repeated be-\ncause they are more signiﬁcant to describe the song. For\ninstance, a guitar solo is not similar to other points of the\nsong. Also, that is not a good summary for the cover song\nrecognition, since many covers skip, modify or poorly per-\nform these parts of the song.734 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018As in the thumbnail version, this summarization pro-\ncess also aims to pick diverse patterns. For this, we use a\ntechnique based on that proposed by Dau and Keogh [7].\nThe main idea of this process is to, after picking a subse-\nquence as a summary, increase the value in the SiMPle of\nthe subsequences that are similar to the current summary.\nThis procedure reduces the chance that we choose similar\nsummaries to describe a song.\nMore precisely, after choosing a summary, we calculate\nthe distance from it to all the subsequences in the song,\nusing MASS. This provides us a vector of distance which\nhas the same length than SiMPle. Then, we normalize this\nvector in the interval [0;1], being that the position storing\n1represents the most distant subsequence and 0appears at\nthe position of the current summary. Finally, we perform\na point-by-point division of the SiMPle by this vector. As\nsimilar subsequences have a (normalized) distance close to\nzero, the division will make its relative positions in SiMPle\nsigniﬁcantly increase its values. Consequently, they will\nunlikely be chosen as a summary in the next iterations. We\nrefer the reader to the paper that ﬁrst proposed this proce-\ndure [7] for a formal deﬁnition of it.\nIn addition to the described procedure, we also make\nineligible the subsequences that are around the picked\nsummaries. Similarly to the thumbnail, we set a region\npi2[p\u0000w; p+w]of ineligible subsequences. The dif-\nference here is that we set as inﬁnite the values at these\npositions in the SiMPle.\n3.1.3 Pitch proﬁles\nIn addition to the summaries, the global pitch class proﬁle\nare also stored in our procedure. This proﬁle is the normal-\nized sum of each bin of the chroma vectors that describe\nthe recording [20]. This is necessary to apply OTI and,\nconsequently, provide invariance to key differences when\ncalculating the distances for a new query recording.\n3.2 Distance Calculation\nWhen a new query is presented to the CSI system, SuCo\nmust compare it to each song in the reference database and\nreturn a ranking by similarity. In our proposal, we match\nthe query with each summary of each original recording.\nFor this, we again take advantage of the algorithm MASS.\nGiven a query q, the steps to compare qwith the original\nrecordings are:\n1. Calculate the global pitch proﬁle of qand the statis-\ntics required by MASS, i.e., its sliding quadratic\nsums and its FFT (which is used to calculate the slid-\ning dot-product). We only need to calculate these\nvalues once and, then, use them in every posterior\ndistance calculations.\n2. From an original recording r, compare its pitch pro-\nﬁle with the proﬁle obtained from q. Then, rotate the\nchroma vector of each summary of raccordingly.\n3. Using the values calculate in the previous steps, cal-\nculate the distances between qand each summary ofr. Store the lowest distance value, i.e., the distance\nbetween the summary and the subsequence from q\nthat best matches it.\n4. The ﬁnal distance between qandris given by the\ngeometric mean of the distances stored in the previ-\nous step. The geometric mean beneﬁts low values in\nits calculation, favoring the match between qand the\nreference songs with one or more summaries with a\ngood approximate match.\n4. EXPERIMENTAL EVALUATION\nThis section presents an experimental evaluation of the\nproposed methods. For the sake of reproducibility, all code\nand detailed results are provided in a supplementary web-\nsite2.\nThis section is split in distinct topics regarding different\nphases of our evaluation. First, we describe the datasets we\nused. Next, we present the experimental setup regarding\nfeature extraction, parameter setting, evaluation measures,\nand adversary methods. Finally, we present the results of\nexperiments regarding time and retrieval performances.\nFor simplicity, we refer to our summarizing and com-\nparing methods by thumbnails and diverse repeated pat-\nterns as SuCo-thumb and SuCo-repeat, respectively.\n4.1 Datasets\nThe datasets used in our experiments include popular and\nclassical music with different sizes. We opted to use the\nsame data as in the paper that proposed SiMPle, so that\nresults are directly comparable. The datasets are:\n\u000fYouTubeCovers: This dataset is composed of 50\npopular songs of different genres, with seven record-\nings each. The data is split in pre-deﬁned refer-\nence/training and test partitions. The reference set\ncomprises the original (studio) recording and a live\nversion performed by the same artist for each song.\nThe test set is, therefore, composed of ﬁve different\nversions of each song in the dataset.\n\u000fMazurkas: This dataset is a collection of clas-\nsical music. It comprises 2914 distinct record-\nings of 49Chopin’s Mazurkas obtained from the\nMazurka Project3. The number of performances\nof each piece varies between 41and95. Unlike\nthe YouTubeCovers dataset, the Mazurkas is not\nsplit into default partitions. We therefore assess this\ndataset using the leave-one-out approach.\n4.2 Experimental Setup\nTo detail our experimental setup, we next describe the ap-\nplied feature sets, the parameters of our method, and how\nwe compare results.\n2https://sites.google.com/view/sucomusic\n3www.mazurka.org.uk/Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 7354.2.1 Feature Extraction\nAlthough some work explores the combination of differ-\nent features to improve retrieval performance [17, 25], the\nusual procedure in the literature is to apply chroma-based\nfeatures [8, 9, 12, 21, 23] that describe pitch perceived over\ntime. More recently, deep learning-based methods have\nbeen used to extract cleaner chroma features from audio.\nIn this work, we extract deep-chroma features [13] to de-\nscribe the recordings in our datasets, using the Madmom\ntool library [5].\nSince the Euclidean distance is sensitive to tempo dif-\nferences, features are smoothed to provide robustness\nagainst this issue. Speciﬁcally, we used the technique ap-\nplied by CENS, which uses a Hann window to smooth fea-\ntures in the time axis. Moreover, we reduced the dimen-\nsionality of the temporal axis of the chroma vectors by a\nfactor of ﬁve. At the end of this procedure, each record-\ning is represented by a vector containing two (smoothed)\ndeep-chroma values per second of audio.\n4.2.2 Parameter Settings\nThe two parameters of our method are the number and\nlength of subsequences used to describe reference songs.\nWe assessed three values of relative summary length: 10,\n20, and 30seconds (i.e., 20,40, and 60consecutive fea-\ntures). Using 10seconds provide the worst results, and\nthese results are not shown, while they point that using too\nsmall windows hampers performance.\nSimilarly, we assessed results using 1,3and5subse-\nquences as the summaries set. We notice that while vary-\ning the set size does not signiﬁcantly affect runtime, but\nthe retrieval performance is clearly superior when using\nﬁve segments.\nGiven the results of this parameter exploration, we\nhenceforth present the results using ﬁve summaries of\n30seconds for the YouTubeCovers data. Because some\nrecordings in the Mazurkas dataset are too short to apply\nsummarization with 30seconds per summary, we present\nresults on this dataset using 20-second summaries.\n4.2.3 Evaluation Measures and Compared Algorithms\nOur evaluation consider three common evaluation mea-\nsures: mean average precision (MAP); precision at 10\n(P@10); and mean rank of ﬁrst correct match (MR1).\nThese measures allow us to compare SuCo against results\npresented in the literature.\nFor the YouTubeCovers, our experiments compare\nSuCo, Triplets [24], SiMPle [23], and a recent technique\nbased on the 2-D Fourier Transform (which we refer as 2D-\nFT) [19]. The Mazurkas dataset has been less often used\nin the literature, so it is only possible to compare SuCo\nagainst SiMPle with this dataset.\nBecause previous evaluations of SiMPle did not use\ndeep-chroma features, to isolate whether accuracy im-\nprovements in SuCo compared to previous ideas in SiM-\nPle happen due to its use of deep-chromas or algorithmic\nimprovements, we also run SiMPle with the same deep-\nchroma feature vector used by SuCo.4.3 Runtime Performance\nA central goal of SuCo is to create a fast method for\nsimilarity-based music information retrieval. We thus ﬁrst\nfocus on evaluating its runtime in our datasets4.\nNote that this evaluation does not consider the duration\nof feature extraction, since it is common to all methods.\nAlso, although we report the total runtime of SuCo’s sum-\nmarizing and comparing procedure, in practice the sum-\nmarization is only performed once. For the SiMPle-based\nCSI, the reported runtime regards only the retrieval phase,\nas it does not rely on a training phase.\nIn the YouTubeCovers dataset, SuCo-thumb and SuCo-\nrepeat run in 136 and 134 seconds, respectively. On the\nother hand, the SiMPle-based CSI took 4,192 seconds to\nassess the same dataset. That is, while our method takes a\nlittle more than 2 minutes to run, SiMPle (which is consid-\nered a fast algorithm) needs more than one hour. SuCo is\nmore than 30times faster than SiMPle.\nThis difference can be further observed in the Mazurkas\ndataset. While SuCo-thumb and SuCo-repeat take around\n10 and 13 hours, respectively, to run the complete process,\nSiMPle only assess around 240 queries – out of 2914 – in\nthe same runtime. This shows that in this larger dataset,\nSuCo is two order of magnitudes faster than SiMPle. In-\ndeed, we aborted the execution of SiMPle for this dataset.\nTo break down the runtime for summarization and com-\nparison in the SuCo pipeline, we isolate the runtime for\nsummarizing in the YouTubeCovers dataset. This dataset\nhas100reference recordings, and the summarization step\nfor it takes around 20seconds. This means that summa-\nrizing a new reference track takes around 0:2seconds, and\ntherefore that incrementing the training set using SuCo is\nnearly instantaneous.\n4.4 Retrieval Performance\nAfter efﬁciency, our second evaluation criteria is accuracy.\nTable 1 presents the results for our accuracy evaluation\nmeasures in the YouTubeCovers dataset. The SiMPle-deep\nrefers to running regular SiMPle algorithm on the deep-\nchroma features.\nAlgorithm MAP P@10 MR1\nTriplets [24] 0.48 0.13 8.49\nSiMPle [23] 0.59 0.14 7.91\n2D-FT [19] 0.65 0.14 8.27\nSiMPle-Deep 0.78 0.17 3.66\nSuCo-thumb 0.65 0.15 5.13\nSuCo-repeat 0.74 0.17 3.80\nTable 1 : Results on the YouTubeCovers dataset\nThe results of SuCo and SiMPle-deep are a signiﬁcant\nimprovement over previous results presented in the litera-\nture for this dataset. SiMPle-deep presented the best results\n4This version of SuCo is implemented in Matlab. The experiments\nwere carried out in a desktop computer with 16Intel(R) Core(TM) i 7\u0000\n2600KCPU @ 3.20GHz and 64Gb of memory running Ubuntu 16.04.\nAlso, at any time, there was only one process computing SuCO.736 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018in this experiment, while SuCo-repeat achieved a very sim-\nilar performance.\nTable 2 presents the results for the Mazurkas classical\nmusic dataset.\nAlgorithm MAP P@10 MR1\nSiMPle [23] 0.88 0.95 2.33\nSuCo-thumb 0.83 0.93 2.83\nSuCo-repeat 0.85 0.94 2.77\nTable 2 : Results on the Mazurkas dataset\nLike in the YouTubeCovers data, SiMPle displays a\nslightly better performance than SuCo, in this case even\nwithout the deep-learned chroma features.\nTaken together, the results on the two datasets point that\nSuCo is able to attain an accuracy very close to the best\nperforming method while providing a much higher perfor-\nmance, with much lower runtime.\nBesides, we notice that we may spend an extra few\ntime to enhance our distance calculation, improving our re-\ntrieval performance and not signiﬁcantly affecting the run-\ntime. We discuss this topic in the next section.\n5. ON REFINING THE EUCLIDEAN DISTANCE\nThe main purpose of using the Euclidean distance is its\ntime efﬁciency and the possibility of exploring algorithms\nto further speeding up its application. However, we under-\nstand that it has a negative impact on the efﬁcacy, since ED\nis sensitive to different distortions in the data.\nWhile we reduce the impact of tempo variances by\nsmoothing the feature vectors, our method is still sensi-\ntive to major differences. Applying a distance measure\nwhich is more robust to tempo differences in the entire\nSuCo pipeline could completely compromise our runtime\nperformance. However, we believe that “reﬁning” the dis-\ntance calculation by some of these functions can improve\nour results.\nTo assure this argument, we made a subtle modiﬁca-\ntion of the comparison algorithm. Once the best subse-\nquence match is found using ED, we re-calculate the dis-\ntance of the matched pair using the Open-End Dynamic\nTime Warping (DTW) [15] with a relative warping win-\ndow of 10% of the subsequence length. For simplicity, we\nwill refer to this optimization as SuCo-DTW.\nBefore presenting the results, we discuss another char-\nacteristic that may affect the ED calculation: the complex-\nity of the data. Batista et al. [2] show that more complex\ntime series, i.e., with high variations between consecutive\nobservations, tends to present higher distances to its neigh-\nbors. Figure 2 illustrates an example of a simpler and a\nmore complex chroma pattern.\nTo circumvent this issue, we estimate the complexity of\neach summary by the standard deviation of its chroma di-\nmensions. The mean complexity of the twelve dimensions\nis taken as the summary’s complexity estimate. Finally, we\nadjust the distance of a query to each summary by diving it\n(a) Low-complex chroma\n (b) High-complex chroma\nFigure 2 : Examples of two different summaries with\nclearly different complexities\nby the complexity estimate. For simplicity, we refer to this\napproach as SuCo-complexity.\nTo test our assumptions, we ran an experiment using\nthese strategies on the YouTubeCovers dataset. Table 3\npresents the results. In this experiment both SuCo-DTW\nand SuCo-complexity use the diverse repeated patterns as\nthe summarizing method. As previously noted, we did not\nrun the complete SiMPle-deep for the Mazurkas data, due\nto its impracticable runtime.\nAlgorithm MAP P@10 MR1\nSiMPle-Deep 0.78 0.17 3.66\nSuCo-DTW 0.80 0.18 3.42\nSuCo-complexity 0.78 0.17 5.09\nTable 3 : Results on the YouTubeCovers dataset\nFinally, reﬁning the distance calculation does not\nseverely affect the algorithm runtime. For instance, cal-\nculating the estimate complexities and “ﬁx” the whole dis-\ntance matrix in the YouTubeCovers dataset takes only 0.4\nseconds. Also, the total runtime for running SuCo-DTW\ntakes 459 seconds for the entire pipeline. Although it\nis slower than SuCO-thumb and SuCo-repeat, it is still\naround ten times faster than SiMPle and presents better re-\ntrieval performance. Investigating this kind of efﬁciency-\nprecision trade-offs is part of our future works, presented\nwith other concluding remarks in the next section.\n6. CONCLUDING REMARKS\nThis paper presents and evaluates SuCo, a suite of meth-\nods for summarizing and comparing music data for fast\ncontent-based information retrieval. The techniques devel-\noped focus on the identiﬁcation of cover songs. Our results\ndemonstrate that it is possible to achieve results that are\nclose to state-of-the-art algorithms while performing up to\ntwo orders of magnitudes faster depending on the dataset.\nFurther improvements on the precision of SuCo with sim-\nple post-processing methods were also explored.\nFuture work may explore the SuCo pipeline in varied\napplications which rely on similarity comparisons. It also\nseems promising to further investigate methods to be added\nto this pipeline to improve precision. Future work may\nfor example experiment with varied features and similarity\nmeasures, as well as with fusing different approaches to\nimprove the retrieval efﬁcacy [22].Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 7377. ACKNOWLEDGEMENT\nThis work was funded by grants #2013/26151-5\nand #2018/11755-6 S ˜ao Paulo Research Foundation\n(FAPESP).\n8. REFERENCES\n[1] Mark A Bartsch and Gregory H Wakeﬁeld. Audio\nthumbnailing of popular music using chroma-based\nrepresentations. IEEE Transactions on Multimedia ,\n7(1):96–104, 2005.\n[2] Gustavo EAPA Batista, Eamonn J Keogh, Oben Moses\nTataw, and Vinicius MA De Souza. Cid: an efﬁ-\ncient complexity-invariant distance for time series.\nData Mining and Knowledge Discovery , 28(3):634–\n669, 2014.\n[3] Juan Pablo Bello. Audio-based cover song retrieval us-\ning approximate chord sequences: Testing shifts, gaps,\nswaps and beats. In International Society for Music In-\nformation Retrieval Conference , volume 7, pages 239–\n244, 2007.\n[4] Thierry Bertin-Mahieux and Daniel PW Ellis. Large-\nscale cover song recognition using the 2d Fourier trans-\nform magnitude. In International Society for Music In-\nformation Retrieval Conference , pages 241–246, 2012.\n[5] Sebastian B ¨ock, Filip Korzeniowski, Jan Schl ¨uter, Flo-\nrian Krebs, and Gerhard Widmer. Madmom: A new\nPython audio and music signal processing library.\nInACM Multimedia Conference , pages 1174–1178.\nACM, 2016.\n[6] Ning Chen, Wei Li, and Haidong Xiao. Fusing similar-\nity functions for cover song identiﬁcation. Multimedia\nTools and Applications , 77(2):2629–2652, 2018.\n[7] Hoang Anh Dau and Eamonn Keogh. Matrix proﬁle v:\nA generic technique to incorporate domain knowledge\ninto motif discovery. In ACM SIGKDD International\nConference on Knowledge Discovery and Data Min-\ning, pages 125–134. ACM, 2017.\n[8] Daniel PW Ellis and Graham E Poliner. Identifying-\ncover songs’ with chroma features and dynamic pro-\ngramming beat tracking. In IEEE International Con-\nference on Acoustics, Speech and Signal Processing ,\nvolume 4, pages IV–1429. IEEE, 2007.\n[9] Jiunn-Tsair Fang, Yu-Ruey Chang, and Pao-Chi\nChang. Deep learning of chroma representation for\ncover song identiﬁcation in compression domain.\nMultidimensional Systems and Signal Processing ,\n29(3):887–902, 2018.\n[10] Eric J Humphrey, Oriol Nieto, and Juan Pablo Bello.\nData driven and discriminative projections for large-\nscale cover song identiﬁcation. In International Soci-\nety for Music Information Retrieval Conference , pages\n149–154, 2013.[11] Jesper Hojvang Jensen, Mads G Christensen,\nDaniel PW Ellis, and Soren Holdt Jensen. A\ntempo-insensitive distance measure for cover song\nidentiﬁcation based on chroma features. In IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing , pages 2209–2212. IEEE, 2008.\n[12] Maksim Khadkevich and Maurizio Omologo. Large-\nscale cover song identiﬁcation using chord proﬁles. In\nInternational Society for Music Information Retrieval\nConference , pages 233–238, 2013.\n[13] Filip Korzeniowski and Gerhard Widmer. Feature\nlearning for chord recognition: The deep chroma ex-\ntractor. In International Society for Music Information\nRetrieval Conference , pages 37–43, 2016.\n[14] Abdullah Mueen, Yan Zhu, Michael Yeh, Kaveh Kam-\ngar, Krishnamurthy Viswanathan, Chetan Gupta, and\nEamonn Keogh. The fastest similarity search algo-\nrithm for time series subsequences under euclidean dis-\ntance, August 2017. http://www.cs.unm.edu/\n˜mueen/FastestSimilaritySearch.html .\n[15] M Muller. Dynamic time warping (DTW). Information\nRetrieval for Music and Motion , pages 70–83, 2007.\n[16] Meinard Muller, Frank Kurth, and Michael Clausen.\nChroma-based statistical audio features for audio\nmatching. In Applications of Signal Processing to Au-\ndio and Acoustics, 2005. IEEE Workshop on , pages\n275–278. IEEE, 2005.\n[17] Julien Osmalsky, Jean-Jacques Embrechts, Peter Fos-\nter, and Simon Dixon. Combining features for cover\nsong identiﬁcation. In International Society for Mu-\nsic Information Retrieval Conference , pages 462–468,\n2015.\n[18] Julien Osmalskyj, S ´ebastien Pi ´erard, Marc\nVan Droogenbroeck, and Jean-Jacques Embrechts.\nEfﬁcient database pruning for large-scale cover song\nrecognition. In IEEE International Conference on\nAcoustics, Speech and Signal Processing , pages\n714–718. IEEE, 2013.\n[19] Prem Seetharaman and Zafar Raﬁi. Cover song identi-\nﬁcation with 2d Fourier transform sequences. In IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing , pages 616–620. IEEE, 2017.\n[20] Joan Serra, Emilia G ´omez, and Perfecto Herrera.\nTransposing chroma representations to a common key.\nInIEEE CS Conference on The Use of Symbols to Rep-\nresent Music and Multimedia Objects , pages 45–48,\n2008.\n[21] Joan Serra, Emilia G ´omez, Perfecto Herrera, and\nXavier Serra. Chroma binary similarity and local align-\nment applied to cover song identiﬁcation. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n16(6):1138–1151, 2008.738 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[22] D. F. Silva, C. M. Yeh, Y . Zhu, E. Keogh, and G. E. A.\nP. A. Batista. Fast similarity matrix proﬁle for music\nanalysis and exploration. IEEE Transactions on Multi-\nmedia , 2018 (in press).\n[23] Diego F Silva, Chin-Chia M Yeh, Gustavo Enrique\nde Almeida Prado Alves Batista, and Eamonn Keogh.\nSimple: Assessing music similarity using subse-\nquences joins. In International Society for Music In-\nformation Retrieval Conference , pages 23–29, 2016.\n[24] Diego Furtado Silva, Vin ´ıcius Mour ˜ao Alves de Souza,\nand Gustavo Enrique de Almeida Prado Alves Batista.\nMusic shapelets for fast cover song regognition. In\nInternational Society for Music Information Retrieval\nConference , pages 441–447, 2015.\n[25] Christopher J Tralie. Early MFCC and HPCP fusion for\nrobust cover song identiﬁcation. In International Soci-\nety for Music Information Retrieval Conference , pages\n294–301, 2017.\n[26] Christopher J Tralie and Paul Bendich. Cover song\nidentiﬁcation with timbral shape sequences. In Inter-\nnational Society for Music Information Retrieval Con-\nference , pages 38–44, 2015.\n[27] Wei-Ho Tsai, Hung-Ming Yu, Hsin-Min Wang, and\nJorng-Tzong Horng. Using the similarity of main\nmelodies to identify cover versions of popular songs\nfor music document retrieval. Journal of Information\nScience & Engineering , 24(6), 2008.\n[28] Fan Yang and Ning Chen. Cover song identiﬁcation\nbased on cross recurrence plot and local alignment. J.\nEast China Univ. Sci. Technol , 42(2):247–253, 2016.\n[29] Chin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova,\nNurjahan Begum, Yifei Ding, Hoang Anh Dau,\nZachary Zimmerman, Diego Furtado Silva, Abdullah\nMueen, and Eamonn Keogh. Time series joins, motifs,\ndiscords and shapelets: a unifying view that exploits\nthe matrix proﬁle. Data Mining and Knowledge Dis-\ncovery , 32(1):83–123, 2018.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 739"
    },
    {
        "title": "Player Vs Transcriber: A Game Approach To Data Manipulation For Automatic Drum Transcription.",
        "author": [
            "Carl Southall",
            "Ryan Stables",
            "Jason Hockman"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492343",
        "url": "https://doi.org/10.5281/zenodo.1492343",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/24_Paper.pdf",
        "abstract": "State-of-the-art automatic drum transcription (ADT) approaches utilise deep learning methods reliant on timeconsuming manual annotations and require congruence between training and testing data. When these conditions are not held, they often fail to generalise. We propose a game approach to ADT, termed player vs transcriber (PvT), in which a player model aims to reduce transcription accuracy of a transcriber model by manipulating training data in two ways. First, existing data may be augmented, allowing the transcriber to be trained using recordings with modified timbres. Second, additional individual recordings from sample libraries are included to generate rare combinations. We present three versions of the PvT model: AugExist, which augments pre-existing recordings; AugAddExist, which adds additional samples of drum hits to the AugExist system; and Generate, which generates training examples exclusively from individual drum hits from sample libraries. The three versions are evaluated alongside a state-of-the-art deep learning ADT system using two evaluation strategies. The results demonstrate that including the player network improves the ADT performance and suggests that this is due to improved generalisability. The results also indicate that although the Generate model achieves relatively low results, it is a viable choice when annotations are not accessible.",
        "zenodo_id": 1492343,
        "dblp_key": "conf/ismir/SouthallSH18",
        "keywords": [
            "automatic drum transcription",
            "deep learning methods",
            "time-consuming manual annotations",
            "congruence between training and testing data",
            "game approach",
            "player vs transcriber",
            "transcription accuracy",
            "training data manipulation",
            "augmenting existing data",
            "adding additional samples"
        ],
        "content": "PLAYER VS TRANSCRIBER: A GAME APPROACH TO DATA\nMANIPULATION FOR AUTOMATIC DRUM TRANSCRIPTION\nCarl Southall, Ryan Stables and Jason Hockman\nDMT Lab, Birmingham City University, Birmingham, United Kingdom\nfcarl.southall, ryan.stables, jason.hockman g@bcu.ac.uk\nABSTRACT\nState-of-the-art automatic drum transcription (ADT) ap-\nproaches utilise deep learning methods reliant on time-\nconsuming manual annotations and require congruence be-\ntween training and testing data. When these conditions\nare not held, they often fail to generalise. We propose\na game approach to ADT, termed player vs transcriber\n(PvT), in which a player model aims to reduce transcrip-\ntion accuracy of a transcriber model by manipulating train-\ning data in two ways. First, existing data may be aug-\nmented, allowing the transcriber to be trained using record-\nings with modiﬁed timbres. Second, additional individual\nrecordings from sample libraries are included to generate\nrare combinations. We present three versions of the PvT\nmodel: AugExist , which augments pre-existing record-\nings; AugAddExist , which adds additional samples of\ndrum hits to the AugExist system; and Generate , which\ngenerates training examples exclusively from individual\ndrum hits from sample libraries. The three versions are\nevaluated alongside a state-of-the-art deep learning ADT\nsystem using two evaluation strategies. The results demon-\nstrate that including the player network improves the ADT\nperformance and suggests that this is due to improved gen-\neralisability. The results also indicate that although the\nGenerate model achieves relatively low results, it is a vi-\nable choice when annotations are not accessible.\n1. INTRODUCTION\nAutomatic music transcription (AMT) systems generate a\nsymbolic representation of the instrumentation within an\naudio recording. There are multiple educational, analytical\nand creative ﬁelds that would beneﬁt from fast and accu-\nrately produced music notation. Automatic drum transcrip-\ntion (ADT) systems form a subset of AMT systems which\nproduce notation solely focused on drum instrumentation.\n1.1 Background\nAt present, high ADT accuracies have been achieved for\naudio ﬁles containing either just drums or polyphonic\nmixtures [4, 7, 9–11, 18]. Following the comprehensive\nc\rCarl Southall, Ryan Stables and Jason Hockman. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Carl Southall, Ryan Stables and Jason Hock-\nman. “Player Vs Transcriber: A Game Approach To Data Manipulation\nFor Automatic Drum Transcription”, 19th International Society for Music\nInformation Retrieval Conference, Paris, France, 2018.ADT literature review in [22], current state-of-the-art\nADT systems utilise either deep learning (DL) or non-\nnegative matrix factorisation (NMF). NMF approaches\nperform instrument-speciﬁc onset detection through an it-\nerative simultaneous update of basis and activation func-\ntions via factorisation of an input spectrogram. Recent\nNMF approaches have introduced specialised update meth-\nods (i.e., ﬁxed, adaptive and semi-adaptive) based on the\nexpected end-use application of the algorithm [2] or in-\ncorporated additional basis functions to capture harmonic\ncontent within polyphonic recordings [23]. Alternatively,\nDL approaches perform instrument-speciﬁc onset detec-\ntion through supervised frame-based classiﬁcation. The\nﬁrst DL systems to acheive high ADT performance in-\ncorporated recurrent neural networks [14, 19, 20]. More\nrecent approaches now include convolutional neural net-\nworks [21] and soft attention mechanisms [15]. As in\nmany ﬁelds, augmentation of data (i.e., pitch shifting) dur-\ning training has aided performance [12, 20].\n1.2 Motivation\nEvaluations undertaken in [22] and the MIREX 2017 Drum\nTranscription Task1highlight that state-of-the-art ADT\naccuracies are achieved by supervised DL approaches.\nHowever, success of DL systems is reliant on training\ndata achieved through a time-consuming manual annota-\ntion process [24]. Also, if there is a mismatch between\ntraining and testing data, these systems will fail to gener-\nalise [22]. We propose a game approach to ADT inﬂuenced\nby generative adversarial networks [5], termed player vs\ntranscriber (PvT). In an attempt to undermine the accuracy\nof a transcriber model (i.e., an existing supervised ADT\napproach), a player model seeks to exploit poorly deﬁned\nareas of the feature space through a manipulation of train-\ning data. This is achieved through learned data manipu-\nlation variables in the player network, which are used to\ndeﬁne the manipulation coordinates of the transform. Ad-\nditionally, the player model is able to manipulate the data\ndepending on its content, where existing methods for aug-\nmentation typically rely on a set of global variables [8].\nThe remainder of this paper is structured as follows:\nSection 2 presents the PvT model and Section 3 provides\nan overview of the undertaken evaluation. The results and\ndiscussion are presented in Section 4 and the conclusions\nand future work are presented in Section 5.\n1http://www.music-ir.org/mirex/wiki/2017:\nDrum_Transcription_Results58Figure 1 . Overview of the player vs transcriber (PvT) game approach to automatic drum transcription. The PvT model is\nachieved through four stages: feature generation, player model, transcriber model and peak-picking.\n2. METHOD\nFigure 1 provides an overview of the proposed PvT system,\nwhich is achieved through four stages: Feature generation,\ndata manipulation by a player, activation function creation\nby a transcriber, and framewise classiﬁcation by peak pick-\ning. At the core of the system is an iterative process of data\nmanipulation (i.e., data augmentation and sample addition)\nand activation function generation between the player and\ntranscriber. Both models examine the loss functions related\nto the activation functions created by the transcriber. Here,\nthe two models have diametrically opposed goals; while\nthe player seeks to maximize the loss, the transcriber at-\ntempts to minimize the loss. Once the loss function has\nbeen optimized during training, testing data may be evalu-\nated by the transcriber and drum events are found through\npeak picking.\n2.1 Feature Generation\nInput audio (16-bit .wav ﬁle sampled at 44.1kHz) is seg-\nmented into Tframes using a Hanning window of msam-\nples (m= 2048 ) with am\n2hopsize. A logarithmic fre-\nquency representation of each of the frames is created us-\ning a similar process to [21] using the madmom Python\nlibrary [1]. The magnitudes of a discrete Fourier transform\nare converted to a logarithmic scale (20Hz–20kHz) using\ntwelve triangular ﬁlters per octave, resulting in a 84xT\nlogarithmic spectrogram xand corresponding target y.\n2.2 Player\nThe aim of the player is to exploit weaknesses within the\ntranscriber through a manipulation of the training data.\nThis is achieved using two processes as demonstrated in\nFigure 2: data augmentation (Section 2.2.1), which alters\nthe frequential content of existing data; and sample addi-\ntion (Section 2.2.2), which adds recordings of individual\ndrum hits from drum samples (Section 2.2.3) to the pre-\nexisting training examples. To ensure that the process is\nend-to-end and that the player network can be trained us-\ning back propagation, the process must be differentiable.\nTo this end, the entire process is designed around network\ndeﬁned variables \u0012(Sections 2.2.4 and 2.2.5) and avoids\noperations such as argmax .2.2.1 Data Augmentation\nThe data augmentation stage is based on existing data aug-\nmentation approaches [8], however also aims to portray\nchanges in instrumentation and performance techniques by\nmanipulating the frequency content of pre-existing data.\nThis is achieved using three network-generated variables\n(\u0012p,\u0012nand\u0012g), in which \u0012pand\u0012nare used within an\npseudo-equaliser function and \u0012gas an overall gain. Time-\nsteptof the augmented segment xaugis calculated using:\nxt\naug=ReLU (xt+ (s(\u0012p)xtv)\u0000(s(\u0012n)xtv))g; (1)\nwherevands, the softmax functions are used to prevent\nover augmentation by limiting maximum augmentation to\neither 1 or -1. The rectiﬁed linear unit function (ReLU)\nensures non-negativity and gis determined using:\ng=\u0012g(1\u0000ming) +ming; (2)\nwheremingis a hyperparameter that determines the mini-\nmum possible gain.\n2.2.2 Sample Addition\nThe sample addition stage aims to reduce transcription ac-\ncuracy by adding new drum hits to the augmented existing\ntraining data using drum samples c(Section 2.2.3). This\nis achieved by generating a new spectrogram qand cor-\nresponding target uand adding them to the existing aug-\nmented training spectrogram xaugand targety:\nx=xaug+q!; (3)\ny=y+u!: (4)\n!is the sample number and the total sample number hy-\nperparameter \ndetermines how many of each sample class\nare added. Each sample is added in an iterative process\nwith the latest version of yandxaugused in the update\nequations. To create qandu, four network determined\nvariables are used: \u0012ps,\u0012nsand\u0012gswhich are variables\nused to augment each sample using the previously ex-\nplained augmentation process and \u0012l, which is used to de-\ntermine the location of the additional drum samples. For\nall four network-determined variables, if \n>1a different\nvariable is used for each sample (i.e., \u0012l= [\u0012l\n1;\u0012l\n2]). TheProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 59Figure 2 . Overview of player model (Section 2.2) with\na data augmentation stage that alters frequency content of\npre-existing data and a sample addition stage that adds new\ndrum hits based on generated locations.\ngenerated target uis created using two variables: f, an ac-\ntivation function derived from the network determined pa-\nrameter\u0012landk, a variable that ensures there are no over-\nlaps between the same drum class. The activation function\nffor each individual drum sample is derived using:\ni!=\u0012l\n!\nmax(\u0012l!); (5)\nf!=ReLU (i!+\u000f\u0000max(i!))1\n\u000f; (6)\nwhere\u000fis used to prevent undeﬁned numbers. kis calcu-\nlated from the current target yusing a minimum distance\nbetween possible locations hyperparameter d:\nht\n!=mean (yt\u0000d:yt+d); (7)\nn!=max(h!)\nh!(h!\u0000\u000f); (8)\nk!= 1\u0000ReLU (n!\nmax(n!)): (9)\nuis then generated by performing element wise multipli-\ncation on the two activation functions fandk:\nu!=k!\ff!: (10)\nTo calculate the new spectrogram q, a matrix consisting of\nall of the possible spectrograms for all Ttime stepseis\ncreated using:\nz!=pad(caug!;T;T ); (11)et\n!=zt+b\n!:zt+b+T\n!; (12)\nwherecaugis the augmented current sample spectrogram\nandpad(c;1;1)means zero pad cby 1 in both directions in\nthe time-step dimension. The spectrogram with the sample\nin the chosen location is then calculated using:\nq!=TX\nt=1et\n!ut\n!: (13)\nIt is worth noting that the proposed sample addition tech-\nnique is capable of learning to not add any samples by\nputting the samples in locations where they overlap other\nlocations and so will be removed.\n2.2.3 Drum Samples\nFor drum samples (i.e., isolated drum events) to be utilised\nwithin the player model they must be segmented and un-\ndergo the same processing as the input features x. Segmen-\ntation of drum events from within larger audio ﬁles was\nachieved automatically through an automatic drum tran-\nscription method [13], and subsequently veriﬁed manually.\nEach sample is then cut to a pre-determined sample length\nwithbframes before the onset. In this work a sample\nlength of 50 is used with b= 10 . The segmented samples\nare then converted to logarithmic spectrograms cthrough\nthe process presented in Section 2.1. More information re-\ngarding the extraction of samples is given in Section 3.3.\n2.2.4 Variations\nFrom the augmentation and sample addition processes we\ncreate three different versions of the player model. The\nﬁrst version, termed AugExist augments existing training\ndata using only the augmentation stage. The second ver-\nsion, termed AugAddExist uses both stages to augment\nexisting training data and add drum samples to the aug-\nmented data. The ﬁnal version, termed Generate gener-\nates entirely new training data by initializing xandywith\nzeros (i.e., no existing training data).\n2.2.5 Player Network\nThe player neural network generates \u0012using a convolu-\ntional neural network consisting of two 3x3 kernel convo-\nlutional layers with max pooling, dropout [17] and batch\nnormalisation [6], followed by a sigmoid fully connected\noutput layer. The ﬁrst convolutional layer is comprised\nof 5 channels and the second layer contains 10 channels.\nThe size of the max pooling layer is altered depending on\nthe player parameters (i.e., \n,v,ming) so that the total\nnumber of trainable parameters of each model is compa-\nrable. The input features are the existing training data x\nand the different drum instrument samples cconcatenated\nalong the time dimensions. Throughout the remainder of\nthis paper, all trainable player parameters are denoted as \u0010.\n2.3 Transcriber\nThe transcriber model follows the same system outline pro-\nposed in [14]. Input features are fed into a pre-trained\nneural network, which aims to output an activation func-\ntion~ywith spikes in frames where onsets are located. In60 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018this paper, we present a novel system that combines the\nstrength of two recently proposed models. The soft at-\ntention mechanism presented in [15] is combined with the\nconvolutional recurrent neural network proposed in [21] to\ncreate a convolutional recurrent neural network with a soft\nattention mechanism output layer. It contains two convo-\nlutional layers consisting of 3x3 ﬁlters, 3x3 max pooling,\ndropouts [17] and batch normalization [6], with the ﬁrst\nlayer consisting of 32 channels and the second is com-\nprised of 64 channels. This is followed by two 20 neu-\nron bidirectional recurrent neural network layers contain-\ning long short-term memory cells with peephole connec-\ntions and a three-neuron sigmoid soft attention mechanism\noutput layer. The attention number is set to 3 [15] and\nall other unstated variables are the same as the original im-\nplementations [15, 21]. Throughout the remainder of this\npaper, all trainable transcriber parameters are denoted as \u001e.\n2.4 Peak Picking\nOnce the optimisation process is complete, the peak-\npicking stage classiﬁes frames of ~yas either containing or\nnot containing an onset. We use the mean threshold (MT)\npeak-picking technique from [15]. First a threshold \u001ctis\ndetermined as:\n\u001ct=mean (~yt\u0000\r: ~yt+\r)\u0003\u0015 (14)\n\u001ct=\u001atmax; \u001c >tmax\ntmin; \u001c <tmin;(15)\nwhere\u0015is a constant, tmax andtmin are the possible\nmaximum and minimum values and \rsets the number of\nframes used to calculate the mean. The current frame of ~y\nis accepted as an onset if it is the maximum of a surround-\ning number of frames and above the threshold \u001c:\nOt=\u001a1;~yt==max(~yt\u0000\u000e: ~yt+\u000e) & ~yt>\u001ct\n0; otherwise;\n(16)\nwhereO(t)represents an onset at time step tand\u000eis the\nnumber of frames on either side of the current frame tused\nto calculate the maximum.\n2.5 Training\nThe player and transcriber are iteratively trained for one\nepoch each in a two-stage process, in which the player\nis ﬁrst trained by updating \u0010and then the transcriber is\ntrained by updating \u001e. Cross entropy is used to minimise\nthe loss function in both instances with 1\u0000yused as the\nplayer target and yused as the transcriber target. Both sys-\ntems are trained using mini-batch gradient descent with the\nAdam optimiser and an initial learning rate of 0.003. These\nsettings were determined using previous ADT studies [15]\nand also suited the player network well with no instability\nobserved. Each mini-batch consists of 10 randomly cho-\nsen, 100 frame length segments. The data is divided into\ntraining, validation and test sets, with the training data used\nto optimize the systems and the validation used to prevent\nover ﬁtting and to optimize the player and peak-picking pa-\nrameters. Training is stopped if there has been no decrease\nin the transcriber validation loss after 10 epochs.3. EVALUATION\nTo identify whether the PvT approach improves ADT per-\nformance, we compare it with the current state-of-the-art\nsupervised ADT approach in six evaluation conditions,\nconsisting of the three contexts and two evaluation strate-\ngies used in [22]. To determine whether similarity be-\ntween drum samples and existing training data affects per-\nformance, two different sample libraries are utilised.\n3.1 Contexts\nFor the ﬁrst context, termed drum transcription of drum-\nonly recordings (DTD) we utilise the IDMT-SMT-Drums\ndataset [2]. For the second context, termed termed drum\ntranscription in the presence of percussion (DTP) we\nutilise the drum-only tracks within the ENST-Drums mi-\nnus one subset [3] and MDB Drums [16]. The third con-\ntext, termed drum transcription in the presence of melodic\ninstruments (DTM), utilises the full polyphonic audio from\nthe ENST-Drums minus one subset, MDB-Drums and\nRBMA-2013 [21].\n3.2 Evaluation Strategies\nThe ﬁrst strategy, termed random , utilises all of the con-\ntext data and divides the tracks in to 70%, 15% and 15%\ntraining, validation and test subsets with three-fold cross\nvalidation. The second strategy, termed subset , utilises all\ndata for the DTD context and only ENST-Drums for DTP\nand DTM. This strategy aims to test the generalisability\nof the systems by utilising the existing subsets within the\ndatasets so that the training and testing data are unrelated.\n3.3 Sample Usage\nFor drum samples, two sample libraries are used to test for\nthe effect of similarity to existing training data. In addition\nto these, percussive mixtures are also generated using the\ncontent of the unobserved drum samples.\nTraining : The ﬁrst library, termed training , only utilises\nsamples extracted from the training data. For the IDMT-\nSMT-Drums dataset, a single sample for each observed\ndrum instrument is extracted from each of the 104 tracks.\nFor ENST-Drums the samples are extracted from the\nincluded isolated drum ﬁles, resulting in a total of 276\nsamples (21 KD, 146 SD and 109 HH). No samples\nare extracted from the other datasets; in the DTP and\nDTM cases the training sample library only consists of\nENST-Drum samples.\nCollection : The second sample library, termed collection ,\nconsists of drum samples collected from online resources.\nThe collection samples are included as they represent a\ndataset with a wider diversity and are not included in the\nexisting training data. In total, there are 445 samples (101\nKD, 151 SD and 193 HH).\nPolyphonic Instances : For the DTP versions of the\nGenerate system, the player network output is combined\nwith artiﬁcial percussive mixture segments created from\nother percussive samples (i.e., toms and cymbals) extractedProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 61Figure 3 . Results for random (top) and subset (bottom) strategies for DTD (left), DTP (middle) and DTM (right) contexts.\nCrosses denote mean instrument and mean fold F-measure, dashed lines present mean instrument and median fold F-\nmeasure and box plots present F-measure range across folds.\nFigure 4 . Individual and mean instrument F-measure, pre-\ncision and recall scores for existing state-of-the-art sys-\ntem ( S) and highest performing AugAddExist PvT model\n(AAE) using random (top) and subset (bottom) strategies.\nfrom the ENST-Drums isolated ﬁles. For the DTM ver-\nsions of the Generate system the player network output is\ncombined with the artiﬁcial percussive mixtures as well as\nthe accompaniment ﬁles from the ENST-Drums and MDB-\nDrums datasets.\n3.4 Evaluation Methodology\nIn all evaluations, the three proposed PvT models—\nAugExist (AE),AugAddExist (AAE) and Generate (G)—\nare compared with the current state-of-the-art supervised\nADT approach ( S), which consists solely of the transcriber\nmodel. Additionally, two versions of the AAE system are\nevaluated: for the ﬁrst ( AAER ), the\u0012parameters are set to\nthe highest performing random values using a grid searchwith the aim of portraying existing data augmentation tech-\nniques [8], and the second ( AAEC ) uses data from the col-\nlection samples alone (Section 3.3). The standard preci-\nsion, recall and F-measure are used as evaluation metrics,\nwith onset candidates being accepted if they fall within\n30ms of the ground truth annotations. For all PvT systems\ndis set to 3 (approx. 30ms) bis set to 10 and hyperpa-\nrametersv,mingand\nare optimized using grid search.\nTo prevent either networks from overpowering the other,\nthe player max pooling sizes are set so that the number\nof parameters \u0010matches that of the transcriber network \u001e\n(approx. 100,000).\n4. RESULTS AND DISCUSSION\n4.1 Random and Subset\nFigure 3 presents the random and subset results for the six\nimplemented systems in the three contexts. The crosses\nrepresent the mean instrument F-measures and the box\nplots present the median and range across the folds. In\nall cases the trained versions of the AugExist (AE) and\nAugAddExist (AAE) systems achieve a higher mean F-\nmeasure and median F-measure than the existing state-of-\nthe-art supervised method ( S), with the box plots show-\ning that this improvement is consistent in the major-\nity of the folds. Results from t-tests across folds high-\nlight that the improvements made by all AugExist and\nAugAddExist systems in the random DTD evaluation are\nsigniﬁcant (i.e., \u001a < 0:05). Larger improvements are\nseen in the subset evaluation—designed speciﬁcally to test\nthe generalisability of the systems [22]—which suggests\nthat the PvT model does improve generalisability. For all\nPvT variations, the trained AugAddExist player versions\n(AAE) achieve higher accuracies than the grid search-set\nAugAddExist system ( AAER ). This demonstrates the worth\nof utilising a player network to learn the weaknesses of the\ntranscriber model and its ability to manipulate each of the\nsegements based on the content. Although the Generate62 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 5 . Mean fold, mean instrument and mean training strategy F-measure results of different AugAddExist settings for\nPvT model hyperparameters v(left),ming(middle) and \n(right).\nsystems do not achieve a higher F-measure they achieve\na high accuracy relative to the amount of human input re-\nquired within the process. Again, this is more apparent in\nthe subset and easier contexts. The lower improvements\nachieved by the AugAddExist andGenerate systems in\nthe DTP and DTM contexts can be attributed to the limi-\ntation of samples from just the ENST dataset. This high-\nlights that the more diverse the sample library, the larger\nthe improvement in performance. However, the results\nfrom the system trained using the collection sample library\n(AAEC ) show that too much diversity can cause the sys-\ntem to underﬁt, resulting in a slightly lower improvement\nbeing observed. The random- \u0012and collection sample li-\nbrary versions of the AugExist andGenerate versions\nwere also implemented but not included within the results\nas the trends are the same as the AAER andAAEC systems.\n4.2 Individual Instrument\nFigure 4 presents the mean fold, mean context individ-\nual and mean drum instrument F-measure, precision and\nrecall scores for the supervised and highest performing\nAugAddExist systems. In all cases the AugAddExist sys-\ntem achieves higher F-measure precision and recall scores\nfor all of the observed drum instruments with 0.015 in-\ncrease in mean instrument F-measure in random and 0.035\nincrease in subset. The largest relative improvement is\nwithin the snare drum class, which further suggests the in-\ncrease is due to greater generalisability, as it has proven to\nbe the most difﬁcult instrument to generalise [22].\n4.3 Player Settings\nFigure 5 presents mean instrument, mean fold and mean\ntraining strategy F-measure results for the AugAddExist\nsystem with different user deﬁned hyperparameter set-\ntings. The left diagram presents results for different val-\nues ofv, the middle diagram for different values of ming\nand the right diagram different values of \n.v= 0:0005\nachieved the highest accuracies with lower values not al-\nlowing enough manipulation and higher values causing\nclass overlap. ming= 0:9achieved the highest accu-\nracy overall however, within the DTP and DTM contexts\nlower values achieved similar results. This is possibly due\nto the fact that a larger diversity of playing technique is\npresent within those contexts. Adding a maximum of two\nextra drum hits ( \n = 2 ) resulted in the highest accuracies\nwith larger values causing too much overlap between in-\nstruments.4.4 Understanding What The Player Does\nBy observing the player model training it is possible to\ngain an understanding of poorly deﬁned areas of the fea-\nture space within ADT datasets. When the player performs\ndata augmentation, a maximum amount of augmentation is\nselected most of the time (i.e., the output of the ReLU func-\ntion in eq. 1 is close to either 1 or -1). This suggests that\nthere are substantial gaps in coverage of training datasets in\nthe feature space for the different instrumentation. Within\nthe sample addition stage the player model consistently at-\ntempts to overlap drums with both other drum instruments\nand other instrumentation within the existing training data.\nThis suggests that the datasets only contain limited obser-\nvations of overlapping instrument combinations.\n5. CONCLUSIONS AND FUTURE WORK\nTo overcome the requirement of time-consuming manual\nannotation, we proposed PvT, a game approach to auto-\nmatic drum transcription. The player model is trained to\nalter the training data so that the accuracy of the tran-\nscriber model is reduced. The three implemented versions\nof the PvT model— AugExist (AE),AugAddExist (AAE)\nandGenerate (G)—are evaluated alongside the existing\nsupervised state-of-the-art ADT ( S) and a grid search-\nsetAugAddExist approach ( AAER ) using two evaluation\nstrategies and three contexts. The results highlight that the\ntrainable PvT model does improve ADT performance with\nAugAddExist achieving the highest accuracy in all evalu-\nations. The Generate model also provides a viable option\nwhen annotated training data is not accessible. Although\ntwo approaches to alter the training data have been im-\nplemented, more are possible. Future work could explore\ntrainable methods for moving existing drum events or syn-\nthesizing new drum samples within the player network, as\nthere are no structural constraints on what the player can\ndo. For polyphonic cases, the unobserved instrumentation\ncould also be included within the player network so that\nfurther combinations can be generated. Another possible\ndirection is to increase the number of observed drum in-\nstruments (i.e., including toms and crash cymbals), which\nis easily done using the Generate model. Open source\nversions of the PvT model are available online.2\n2https://github.com/CarlSouthall/\nPlayer-Vs-TranscriberProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 636. REFERENCES\n[1] Sebastian B ¨ock, Filip Korzeniowski, Jan Schl ¨uter, Flo-\nrian Krebs, and Gerhard Widmer. madmom: A new\nPython audio and music signal processing library.\nInProceedings of the ACM International Conference\non Multimedia , pages 1174–1178, Amsterdam, The\nNetherlands, 2016.\n[2] Christian Dittmar and Daniel G ¨artner. Real-time tran-\nscription and separation of drum recordings based on\nNMF decomposition. In Proceedings of the Interna-\ntional Conference on Digital Audio Effects (DAFx) ,\npages 187–194, Erlangen, Germany, 2014.\n[3] Olivier Gillet and Ga ¨el Richard. Enst-drums: an exten-\nsive audio-visual database for drum signals processing.\nInProceedings of the 7th International Society for Mu-\nsic Information Retrieval Conference (ISMIR) , 2006.\n[4] Olivier Gillet and Ga ¨el Richard. Transcription and sep-\naration of drum signals from polyphonic music. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 16(3):529–540, 2008.\n[5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial\nnets. In Advances in neural information processing sys-\ntems, pages 2672–2680, 2014.\n[6] Sergey Ioffe and Christian Szegedy. Batch normaliza-\ntion: Accelerating deep network training by reducing\ninternal covariate shift. In International Conference on\nMachine Learning , pages 448–456, 2015.\n[7] Henry Lindsay-Smith, Skot McDonald, and Mark San-\ndler. Drumkit transcription via convolutive NMF. In\nProceedings of the International Conference on Digital\nAudio Effects Conference (DAFx) , York, United King-\ndom, 2012.\n[8] Brian McFee, Eric J Humphrey, and Juan Pablo Bello.\nA software framework for musical data augmentation.\nInISMIR , pages 248–254, 2015.\n[9] Marius Miron, Matthew E. P. Davies, and Fabien\nGouyon. An open-source drum transcription system\nfor pure data and max MSP. In Proceedings of the\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , pages 221–225, Van-\ncouver, Canada, 2013.\n[10] Axel R ¨obel, Jordi Pons, Marco Liuni, and Mathieu La-\ngrange. On automatic drum transcription using non-\nnegative matrix deconvolution and Itakura-Saito diver-\ngence. In Proceedings of the IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 414–418, Brisbane, Australia, 2015.\n[11] Mathias Rossignol, Mathieu Lagrange, Gr ´egoire\nLafay, and Emmanouil Benetos. Alternate level clus-\ntering for drum transcription. In Proceedings of theEuropean Signal Processing Conference (EUSIPCO) ,\npages 2023–2027, Nice, France, August 2015.\n[12] Carl Southall. MIREX 2017 drum transcription sub-\nmissions. In Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\nSuzhou, China, 2017.\n[13] Carl Southall, Nick Jillings, Ryan Stables, and Jason\nHockman. ADTWeb: An open source browser based\nautomatic drum transcription system. In Proceedings\nof the International Society for Music Information Re-\ntrieval Conference (ISMIR) , Suzhou, China, 2017.\n[14] Carl Southall, Ryan Stables, and Jason Hockman. Au-\ntomatic drum transcription using bi-directional recur-\nrent neural networks. In Proceedings of the 17th In-\nternational Society for Music Information Retrieval\nConference (ISMIR) , pages 591–597, New York City,\nUnited States, 2016.\n[15] Carl Southall, Ryan Stables, and Jason Hockman.\nAutomatic drum transcription for polyphonic record-\nings using soft attention mechanisms and convolutional\nneural networks. In Proceedings of the 18th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 606–612, Suzhou, China, 2017.\n[16] Carl Southall, Chih-Wei Wu, Alexander Lerch, and\nJason Hockman. MDB drums an annotated subset of\nmedleyDB for automatic drum transcription. In Pro-\nceedings of the International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , Suzhou, China,\n2017.\n[17] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. Dropout:\nA simple way to prevent neural networks from\noverﬁtting. Journal of Machine Learning Research ,\n15(1):1929–1958, 2014.\n[18] Lucas Thompson, Simon Dixon, and Matthias Mauch.\nDrum transcription via classiﬁcation of bar-level rhyth-\nmic patterns. In Proceedings of the 15th International\nSociety for Music Information Retrieval Conference\n(ISMIR) , pages 187–192, Taipei, Taiwan, 2014.\n[19] Richard V ogl, Matthias Dorfer, and Peter Knees. Re-\ncurrent neural networks for drum transcription. In Pro-\nceedings of the 17th International Society for Music\nInformation Retrieval Conference (ISMIR) , pages 730–\n736, New York City, United States, 2016.\n[20] Richard V ogl, Matthias Dorfer, and Peter Knees. Drum\ntranscription from polyphonic music with recurrent\nneural networks. In Proceedings of the IEEE Interna-\ntional Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP) , pages 201–205, New Orleans,\nUnited States, 2017.\n[21] Richard V ogl, Matthias Dorfer, Gerhard Widmer, and\nPeter Knees. Drum transcription via joint beat and64 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018drum modeling using convolutional recurrent neural\nnetworks. In Proceedings of the 18th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 150–157, Suzhou, China, 2017.\n[22] Chih-Wei Wu, Christian Dittmar, Carl Southall,\nRichard V ogl, Gerhard Widmer, Jason Hockman,\nMeinard M ¨uller, and Alexander Lerch. A Review of\nAutomatic Drum Transcription. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing ,\n26(9):1457–1483, 2018.\n[23] Chih-Wei Wu and Alexander Lerch. Drum transcrip-\ntion using partially ﬁxed non-negative matrix factor-\nization with template adaptation. In Proceedings of the\n16th International Society for Music Information Re-\ntrieval Conference (ISMIR) , pages 257–263, Malaga,\nSpain, 2015.\n[24] Chih-Wei Wu and Alexander Lerch. Automatic\ndrum transcription using the student-teacher learning\nparadigm with unlabeled music data. In Proceedings\nof the 18th International Society for Music Informa-\ntion Retrieval Conference (ISMIR) , pages 613–620,\nSuzhou, China, 2017.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 65"
    },
    {
        "title": "Improving Peak-picking Using Multiple Time-step Loss Functions.",
        "author": [
            "Carl Southall",
            "Ryan Stables",
            "Jason Hockman"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492409",
        "url": "https://doi.org/10.5281/zenodo.1492409",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/25_Paper.pdf",
        "abstract": "The majority of state-of-the-art methods for music information retrieval (MIR) tasks now utilise deep learning methods reliant on minimisation of loss functions such as cross entropy. For tasks that include framewise binary classification (e.g., onset detection, music transcription) classes are derived from output activation functions by identifying points of local maxima, or peaks. However, the operating principles behind peak picking are different to that of the cross entropy loss function, which minimises the absolute difference between the output and target values for a single frame. To generate activation functions more suited to peak-picking, we propose two versions of a new loss function that incorporates information from multiple time-steps: 1) multi-individual, which uses multiple individual time-step cross entropies; and 2) multi-difference, which directly compares the difference between sequential time-step outputs. We evaluate the newly proposed loss functions alongside standard cross entropy in the popular MIR tasks of onset detection and automatic drum transcription. The results highlight the effectiveness of these loss functions in the improvement of overall system accuracies for both MIR tasks. Additionally, directly comparing the output from sequential time-steps in the multidifference approach achieves the highest performance.",
        "zenodo_id": 1492409,
        "dblp_key": "conf/ismir/SouthallSH18a",
        "keywords": [
            "deep learning",
            "loss functions",
            "cross entropy",
            "framewise binary classification",
            "output activation functions",
            "local maxima",
            "operating principles",
            "peak picking",
            "activation functions",
            "MIR tasks"
        ],
        "content": "IMPROVING PEAK-PICKING USING MULTIPLE TIME-STEP LOSS\nFUNCTIONS\nCarl Southall, Ryan Stables and Jason Hockman\nDMT Lab, Birmingham City University, Birmingham, United Kingdom\nfcarl.southall, ryan.stables, jason.hockman g@bcu.ac.uk\nABSTRACT\nThe majority of state-of-the-art methods for music infor-\nmation retrieval (MIR) tasks now utilise deep learning\nmethods reliant on minimisation of loss functions such as\ncross entropy. For tasks that include framewise binary\nclassiﬁcation (e.g., onset detection, music transcription)\nclasses are derived from output activation functions by\nidentifying points of local maxima, or peaks. However, the\noperating principles behind peak picking are different to\nthat of the cross entropy loss function, which minimises the\nabsolute difference between the output and target values\nfor a single frame. To generate activation functions more\nsuited to peak-picking, we propose two versions of a new\nloss function that incorporates information from multiple\ntime-steps: 1) multi-individual , which uses multiple indi-\nvidual time-step cross entropies; and 2) multi-difference ,\nwhich directly compares the difference between sequential\ntime-step outputs. We evaluate the newly proposed loss\nfunctions alongside standard cross entropy in the popular\nMIR tasks of onset detection and automatic drum tran-\nscription. The results highlight the effectiveness of these\nloss functions in the improvement of overall system ac-\ncuracies for both MIR tasks. Additionally, directly com-\nparing the output from sequential time-steps in the multi-\ndifference approach achieves the highest performance.\n1. INTRODUCTION\nAt present, the state-of-the-art systems for many music in-\nformation retrieval (MIR) tasks utilise deep learning mod-\nels. Within the domain of dynamic time-series MIR tasks\nsuch as onset detection and music transcription, solutions\nare achieved through a binary classiﬁcation of each time-\nstept. A binary classiﬁcation output is typically limited to\na range of [ 0,1] using a non-linear function (e.g., sigmoid,\nsoftmax). For classiﬁcation purposes the output is subse-\nquently rounded to either 0or1. However, in framewise\nbinary classiﬁcation tasks using this approach has proven\nto be less effective [7]. In the example presented in Fig-\nure 1, a framewise output activation function ~yis shown in\nc\rCarl Southall, Ryan Stables and Jason Hockman. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: Carl Southall, Ryan Stables and Jason Hock-\nman. “Improving Peak-picking Using Multiple Time-step Loss Func-\ntions”, 19th International Society for Music Information Retrieval Con-\nference, Paris, France, 2018.\nFigure 1 . A true positive is missed using the rounding\napproach, but is successfully selected through peak picking\n(circled point). The solid line denotes the output, the dotted\nline the target, the dashed line the 0.5 rounding threshold\nand the dash-dotted line the peak-picking threshold.\nwhich the values ideally associated with a class label (i.e.,\nvalue) of 1do not exceed 0:5. While ~yclearly shows the\npresence of an event as a peak , it would be identiﬁed as a\nfalse negative ( ~yt<0:5).\n1.1 Peak Picking\nTo overcome the problem posed in Figure 1, the majority of\nframewise binary classiﬁcation systems utilise peak pick-\ning, which differentiates between classes by identifying lo-\ncal maxima. Multiple peak-picking approaches have been\nproposed in the literature [1,4,12,16] and follow a general\nprocess as shown in Figure 1. Here, a point is selected as a\npeak if it is the maximum value within a local window and\nabove a threshold \u001c. In [16] the threshold is determined\nby calculating the mean of a window, controlled using \u000e,\na user determined constant \u0015and maximum and minimum\nvalues (tmax andtmin ).\n\u001ct=mean (~yt\u0000\u000e: ~yt+\u000e)\u0003\u0015 (1)\n\u001ct=\u001atmax; \u001c >tmax\ntmin; \u001c <tmin(2)313An onset classiﬁcation vector Ois achieved by determin-\ning if each time-step of ~yis the maximum value within the\nsurrounding number of frames, set using \n, and above the\nthreshold\u001c:\nOt=\u001a1;~yt==max(~yt\u0000\n: ~yt+\n) & ~yt>\u001ct\n0; otherwise:\n(3)\n1.2 Loss Functions\nThe overall loss (often referred to as the cost)Lrepresents\nthe error of a system within a single value. It is calculated\nby comparing the difference between the desired ground\ntruthyand the actual output ~y[10]. Within audio based\ntime-step classiﬁcation tasks it is calculated by taking the\nmean of the individual time-step losses lt:\nL=1\nTTX\nt=1lt: (4)\nLis a component of back propagation (and truncated back\npropagation) which is used to calculate the gradients G\nused in updating the trainable parameters of the model \u0002\nwith learning rate \u0016.\n\u0002 \u0002\u0000\u0016\u0001G (5)\nCommonly used loss functions for calculating ltinclude\nmean squared error ( MS) (eq. 6) and cross entropy ( CE)\n(eq. 7) [5].\nlt\nmsfyt;~ytg= (yt\u0000~yt)2(6)\nlt\ncefyt;~ytg=ytlog (~yt) + (1\u0000yt) log (1\u0000~yt)(7)\nBoth of these loss functions are suited to differentiating\nbetween binary classes using rounding as they aim to min-\nimise the absolute difference between the targets yand\nthe output ~y. In the majority of MIR tasks CEis more\nsuited than MSdue to its greater penalization of large er-\nrors [14, 16, 22].\n1.3 Motivation\nIn the peak-picking process, multiple frames are utilized\nin both the calculation of a threshold as well as the peak\nselection. However, in the MSandCEcalculations only the\ncurrent time-step tis used in measuring the difference be-\ntween the target yand output ~y. In order for the loss to\nreﬂect peak salience (i.e., the clarity of the local maxima)\nand to ensure that the output activation function is suit-\nable for peak-picking, then multiple time-steps should be\nincluded within the loss function calculation. To this end,\nwe propose two versions of a new loss function which not\nonly measures the absolute difference between yand~y, but\nalso allows for peak salience to be maintained. We then\nevaluate the worth of these functions within the tasks of\nonset detection and automatic drum transcription (ADT).\nThe remainder of this paper is structured as follows:\nSection 2 presents the proposed loss functions and Section3 gives an overview of the evaluation. The results and dis-\ncussion are presented in Section 4 and the conclusions and\nfuture work are presented in Section 5.\n2. METHOD\nFor a loss function to represent an understanding of peak\nsalience, it must include at least three points: ~yt\u00001: ~yt+1.\nWe propose combining CEand a peak salience measure\ninto a single loss function termed peakiness cross entropy\n(PCE):\nlt\npce=1\n2\u0000\n\rlt\ncefyt;~ytg+ (1\u0000\r)(lt\np+lt\nf)\u0001\n; (8)\nwhere the ﬁrst part of the equation is the standard cross\nentropy ( CE) of the current time-step t. The second part\nof the function is a peak salience measure that consists of\ntwo variables: lp, which focuses on the previous time-step\nandlf, which focuses on the future ( t+ 1) time step. \ris\nused to control the weighting between standard CEand the\npeakiness measure. We propose two methods for achieving\nlpandlf: a combination of multiple individual time-step\ncalculations and a direct comparison of the differences be-\ntween multiple time-steps.\n2.1 Multi-individual\nThe multi-individual ( MI) method calculates lpandlfas\nindividual time step cross entropies of previous and future\ntime-steps:\nlt\np=lt\ncefyt\u00001;~yt\u00001g (9)\nlt\nf=lt\ncefyt+1;~yt+1g: (10)\nThis ensures that updates to ~ytdo not cause greater nega-\ntive updates to ~yt\u00001and~yt+1.\n2.2 Multi-difference\nAlthough MIutilizes multiple time-steps it does not com-\npare absolute differences between sequential time-steps.\nTo achieve this, we propose an additional calculation of\nlpandlf, termed multi-difference ( MD), which measures\nthe absolute differences between sequential time-steps of\nthe targetyand the output ~y. The ﬁrst version of MD(MMD),\nutilizes MS. The second version ( WMD) utilizes an updated\nversion of the CEequation, termed weighted cross entropy\n(WCE):\nlt\nwcefyt;~ytg= (1\u0000\u001e)ytlog (~yt)+\u001e(1\u0000yt) log (1\u0000~yt);\n(11)\nwhich allows the strength of each half of the equation to\nbe controlled using the weighting parameter \u001e. The ﬁrst\nhalf of the WCEequation (henceforth referred to as WCE-FN )\naims to reduce false negatives by producing a loss value\nproportional to the difference between sequential time-\nsteps ofytand~yt. The second half of the WCE equation\n(hereafter termed WCE-FP ) aims to suppress false positives314 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 2 . Example activation function scenarios with corresponding loss values output from each loss function. From\nleft to right: a raised true positive (RTP), a ﬂat line false negative (FFN), a large false positive (LFP), a small raised false\npositive (SFP) and a raised ﬂat line (RFL).\nas it outputs a larger value if there is a large undesirable\ndifference between sequential frames. lt\npandlt\nfinMMDand\nWMDare calculated respectively using:\nlt\np=\u001alt\nwcefjyt\u0000yt\u00001j;j~yt\u0000~yt\u00001jg; WMD;\nlt\nmsfjyt\u0000yt\u00001j;j~yt\u0000~yt\u00001jg; MMD;\n(12)\nlt\nf=\u001alt\nwcefjyt\u0000yt+1j;j~yt\u0000~yt+1jg; WMD;\nlt\nmsfjyt\u0000yt+1j;j~yt\u0000~yt+1jg; MMD:\n(13)\nTruncated back propagation is used to calculate the gradi-\nents for all loss functions. The presented implementation\nutilises the automatic differentiation functions built into\nthe Tensorﬂow1library for this purpose.\n2.3 Example Loss Function Scenarios\nFigure 2 presents ﬁve example activation function sce-\nnarios. The loss values achieved by CE,MI,MMD,WMD\nand the two separated halves of WMD:WMD-FN (\u001e= 0)\nandWMD-FP (\u001e= 1), are presented with \r= 0:5. The\ntargets are presented at the top and the output activation\nfunction on the bottom. It is worth noting that all of\nthe loss functions that utilize CEcan be directly com-\npared but MMD is relative to itself (i.e., the MMD values\nmight seem small relative to the other loss values but\nnot relative to other values of MMD). If all frames of the\noutput are correct then all of the loss functions output zero.\n(a)Reduced true positive : The ﬁrst example shows\na reduced true positive where the surrounding frames are\ncorrect. In this case CEandWMD output the largest values\nas this peak could fall below the peak-picking threshold.\n1https://www.tensorflow.org(b) Flat line false negative : The second example\nshows a false negative where the output is a ﬂat line. In\nthis case high relative error values are given by all of\nthe loss functions, however larger error values are given\nbyMMD and especially the FN suppression half of WMD.\nThis example would generally not be selected during\npeak-picking.\n(c)Large false positive : The third example shows a\nfalse positive where the surrounding frames are correct.\nIn this case high values are given by CE,MMD and the\nfalse positive suppression part of WMD, as this would be an\nincorrectly selected peak.\n(d)Small raised false positive : The fourth example\nagain shows a false positive, similar to the previous\nexample, but the surrounding frames are raised resulting\nin a less salient false positive. In this case lower values\nare given by MIandWMD-FP , than CE, as this peak is not\nas salient as the one in example three (i.e., large false\npositive).\n(e)Raised ﬂat line : The ﬁnal example presents a\nraised ﬂat line. In this case the MMDandWMDloss functions\npenalize less than CEandMI. While the absolute values\nare slightly wrong, the difference between the sequential\nframes is correct, resulting in no peaks being correctly\nchosen.\n3. EVALUATION\nTo identify whether the new loss functions improve per-\nformance, we compare the newly proposed loss functions\nagainst standard cross entropy ( CE) in two common MIR\ntasks: onset detection (OD) and automatic drum transcrip-\ntion (ADT). To ensure performance trends are consistent\nwith different systems, we implement four neural networkProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 315Figure 3 . Subset mean system F-measure results for the four implemented cost functions for onset detection and automatic\ndrum transcription. The individual subset F-measure results are on the left and the mean subset F-measure, precision and\nrecall results are on the right. The red error plots display the standard deviations across the folds.\nbased models which have achieved state-of-the-art results\nfor both of the tasks in recent years. Standard F-measure,\nderived from precision and recall, is used as the evalua-\ntion metric with onset candidates being accepted if they\nfall within 30ms of the ground truth annotations (i.e., win-\ndow of acceptance). If onset candidates fall within 30ms\nof each other, they are combined into a single onset at the\nmiddle location (i.e., window of combination).\n3.1 Onset Detection (OD)\nFor the OD evaluation, we utilize the same datasets and\nsubset splits as used in [3], consisting of: complex mix-\ntures (CM), pitched percussion (PP), non-pitched percus-\nsion (NPP), wind instruments (WI), bowed strings (BS)\nand vocals (VO). As OD is a binary classiﬁcation task, all\nsystems are implemented with a two neuron softmax out-\nput layer, one neuron corresponds to an onset and the other\nneuron corresponds to the absence of an onset.\n3.2 Automatic Drum Transcription (ADT)\nFor the ADT evaluation, we utilize four ADT datasets:\nIDMT-SMT-Drums [6], ENST-Drums minus one subset\n[8], MDB-Drums [18] and RBMA-2013 [21]. To observe\ntrends between contexts, the datasets are divided into the\nthree groups proposed in [23]: 1) drum only (DTD) con-\nsisting of IDMT-SMT-Drums, 2) drums in the presence of\nextra percussion (DTP) consisting of the drum-only ver-\nsions ENST-d and MDB-d and 3) drums in the presence of\nextra percussion and melodic instruments (DTM), which\nconsist of the polyphonic versions ENST-m, MDB-m and\nRBMA-2013. ENST-m is created by combining the ENST\ndrum tracks and the accompaniment ﬁles using ratios of2\n3\nand1\n3respectively, as done in [6, 9, 15, 20, 24]. A three-\nneuron sigmoid output layer is used for all implemented\nADT systems, with the neurons corresponding to the three\nobserved drum instruments (i.e., KD, SD and HH).3.3 Systems\nFour different neural network based systems are imple-\nmented. All systems consist of the same overlying struc-\nture: First, input features are fed into a pre-trained neural\nnetwork model. Peak-picking is then performed to deter-\nmine the locations of the onset candidates using the algo-\nrithm from [16] (eq.1:3).\n3.3.1 Input Features\nFor both tasks we use the same framewise logarithmic\nspectral input features xgenerated using the madmom\nPython library [2]. The input audio (16-bit .wav ﬁle sam-\npled at 44.1 kHz) is segmented into Tframes using a Han-\nning window of msamples (m= 2048 ) with am\n2hop-\nsize. A logarithmic frequency representation of each of the\nframes is created using a similar process to [22]. The mag-\nnitudes of a discrete Fourier are transformed into a loga-\nrithmic scale (20Hz–20kHz) using twelve triangular ﬁlters\nper octave. This results in a 84 x Tlogarithmic spectro-\ngram.\n3.3.2 lstmpB\nThelstmpB system is based on the system presented in\n[23] and the baseline system used in [16]. It consists of two\n50-neuron hidden layers containing long short-term mem-\nory cells with peephole connections. The input features are\nprocessed in a framewise manner.\n3.3.3 lstmpSA3B\nThelstmpSA3B system is based on the SA3 system pro-\nposed in [16]. It is the same as the lstmpB system other\nthan it contains a soft attention mechanism in the output\nlayer. As in the original implementation the attention num-\nberacontrols the number of attention connections, and is\nset to three.316 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 4 . Individual system mean subset F-measure re-\nsults for the proposed cost functions in OD and ADT tasks.\nRed bars denote standard deviations across folds.\nFigure 5 . Mean system and mean subset results for differ-\nent values of WMDparameters ( \rand\u001e) in onset detection\n(OD) and automatic drum transcription (ADT) evaluations.\n3.3.4 lstmpSA3B-F5\nThelstmpSA3B-F5 system is identical to the lstmpSA3B\nsystem with a larger number of input features used. A total\nof 11 frames (5 either side of the current frame (xt\u00005:\nxt+5)) are used for each time-step.\n3.3.5 cnnSA3B-F5\nThecnnSA3B-F5 [17] combines the convolutional recur-\nrent neural network proposed in [22] and the soft atten-\ntion mechanism proposed in [16]. It contains two convo-\nlutional layers consisting of 3x3 ﬁlters, 3x3 max pooling,\ndropouts [19] and batch normalization [11], with the ﬁrst\nlayer consisting of 32 channels and the second 64 chan-\nnels. It contains the same soft attention mechanism outputlayer and the same input feature size as lstmpSA3B-F5 .\n3.3.6 Training\nAll systems are trained using mini-batch gradient descent\nwith the Adam optimizer [13]. An initial learning rate\nof 0.003 is used and three-fold cross validation is per-\nformed. Each mini-batch consists of 10 randomly chosen,\n100 time-step segments and the data is divided by track\ninto 70% training, 15% validation and 15% testing sets.\nThe training data is used to optimize the systems and the\nvalidation data is used to prevent overﬁtting and to opti-\nmize the peak-picking parameters. For datasets contain-\ning subsets (i.e., IDMT-SMT Drums and ENST Drums)\nthe splits are performed evenly across the subsets.\n4. RESULTS AND DISCUSSION\n4.1 Subset Performance\nFigure 3 presents the subset results for all cost functions in\nboth evaluations. The red error bars represent standard de-\nviation across folds. The OD results are derived from the\nmean of the systems and the ADT results are derived from\nthe mean of the systems and the mean of the observed drum\ninstruments (i.e., KD, SD and HH). The left part of the ﬁg-\nure presents the individual subset F-measures and the right\npart of the ﬁgure presents the mean subset F-measure, pre-\ncision and recall. In both MIR tasks, all three of the newly\nproposed cost functions achieved a higher mean subset F-\nmeasure than standard CE, with WMDperforming the best in\nboth. Within the ADT evaluation a higher performance\nis achieved for all three observed drum instruments. A\nslightly larger increase in performance was witnessed in\nthe ADT task and both versions of the MDcost function\nachieve higher mean subset F-measures than MI. This high-\nlights that measuring the absolute differences between se-\nquential frames does improve performance. The mean sub-\nset precision and recall results highlight that in all cases\nthe newly proposed cost functions achieve higher precision\nand recall scores than standard CE. In the OD evaluation the\nhighest increase in performance between WMDandCEis in\nthe NPP subset. In the ADT evaluation the largest increase\nis seen within the DTP subsets (ENST-d and MDB-d). For\nall subsets in both evaluations the highest F-measure is\nachieved by one of the three newly proposed cost functions\nand the error bars show that this improvement occurs in all\nof the folds. Results from t-tests highlight that the WMDsys-\ntems improvement over CEwithin the BS and mean recall\nOD categories and MDB-d, mean F-measure and precision\nADT categories are signiﬁcant ( \u001a<0:05).\n4.2 Individual System Performance\nFigure 4 presents the individual system, mean subset F-\nmeasure results for both MIR evaluations. In all cases the\nhighest F-measure is achieved by one of the newly pro-\nposed cost functions, with the WMD cost function achiev-\ning the highest F-measure in ﬁve of the eight cases. This\nreinforces that using multiple framed cost functions doesProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 317improve performance and that this increase is not just as-\nsociated with one system. The highest F-measure and\nthe largest increase relative to CEis achieved by the\ncnnSA3B-F5 system using the WMDcost function.\n4.3 WMD Parameters\nFigure 5 presents the mean system, mean subset F-measure\nresults for different parameter settings of the WMDcost func-\ntion. Plots of six \u001evalues for\r= 0:25and\r= 0:5are\npresented for both evaluations. For any \u001evalues less than\none, there is a dramatic decrease in performance which\nsuggests that the false negative suppression half of the WCE\nfunction has a negative effect on performance. This is pos-\nsibly due to the extremely high value given to ﬂat parts of\nthe activation function (see Figure 2), causing these parts\nof the activation function to become noisy. This suggests\nthat the improvement is due to the false positive suppres-\nsion half of the WMDsystem. As this alone achieves higher\nF-measures than the other proposed cost functions, then\nit also suggests that their improvement is also due to the\nsuppression of false positives. However, the false nega-\ntive suppression in those cost functions do not cause reper-\ncussions. As \r(i.e., weighting of peak salience measure)\nincreases (0.5 = even weighting with standard CE) then\nthe performance decreases. This trend continues with all\nvalues below 0.5 and with the other two proposed loss\nfunctions. The highest F-measures were achieved with\n\r=0.25 (Standard CEis weighted twice as much as the peak\nsalience measure) for all three proposed loss functions. To\ncategorically identify ideal parameter settings for a partic-\nular scenario, a grid search would be required. However,\nthe results suggest that \r= 0:25and\u001e= 1would always\nbe optimal.\n4.4 Understanding the Improvement\nAfter visual comparison of the output activation functions\na common situation in which the newly proposed loss func-\ntions achieve higher performance was observed. Figure 6\npresents an example of this situation, with the top diagram\nshowing the output activation function using CEand the\nbottom diagram showing the activation function when us-\ning the highest performing version of WMD. In the CEdia-\ngram, there are two spikes to the right that are wrongly de-\ntected as peaks but in the WMDversion these peaks are less\nsalient, resulting in no false positives. The consequence of\nthis is that the actual true positive within in the WMD ver-\nsion has a lower amplitude than the one in the CEversion.\nHowever, this has no effect on performance as the true pos-\nitive is still a clear peak and correctly chosen within both\nCEandWMDversions. We believe this situation occurs be-\ncause within CEa higher error is given to the true positive\nthan the combination of the two smaller false positive er-\nrors. This causes the true positive to be closer to the tar-\ngetybut consequentially causes the false positive spikes.\nWithin the WMD version, the false positive suppression as-\nsigns a greater loss value to the two false positive spikes\nthan the reduced true positive, ensuring that the spikes are\nFigure 6 . Example of WMDloss function reducing the num-\nber of false positives by suppressing false spikes. CEoutput\nactivation function (top) and WMDoutput activation function\n(bottom) with output ~y(solid lines), target y(dotted lines)\nand the peak-picking threshold (dashed lines). Circles de-\nnote selected peaks and arrowed lines show windows of\nacceptance and combination.\nnot selected by the peak-picking algorithm. This reduc-\ntion of noise in the activation function results in less false\npositives but also enables the peak-picking threshold to be\nlower, enabling more true positives to be selected. This\neffect could likely explain both the increase in recall and\nprecision.\n5. CONCLUSIONS AND FUTURE WORK\nWe have developed three new loss functions in an at-\ntempt to generate activation functions more suited to peak-\npicking. The new loss functions utilise information from\nmultiple time-steps which allow them to measure both the\nabsolute values and to maintain peak salience by compar-\ning sequential time-steps. We evaluated the newly pro-\nposed loss functions against standard CEusing four neural\nnetwork-based systems in the MIR tasks of onset detection\nand ADT. The results highlight that all three of the newly\nproposed cost functions do improve performance, with the\nWMD loss function achieving the highest accuracy. This\nwork focuses on the inclusion of a single frame on either\nside of the current time-step. Future work could explore\nthe potential beneﬁt of using a greater number of frames\nand a version of the WMDequation in which the false nega-\ntive suppression component does not negatively inﬂuence\nthe outcome. Additionally, to make the system end-to-end,\nthe evaluation methodology (i.e., F-measure and tolerance\nwindows) could also be incorporated within the loss func-\ntions. Open source implementations of the new loss func-\ntions are available online.2\n2https://github.com/CarlSouthall/PP_loss_\nfunctions318 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20186. REFERENCES\n[1] Juan Pablo Bello, Laurent Daudet, Samer Abdallah,\nChris Duxbury, Mike Davies, and Mark B. Sandler.\nA tutorial on onset detection in music signals. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 13(5):1–13, 2005.\n[2] Sebastian B ¨ock, Filip Korzeniowski, Jan Schl ¨uter, Flo-\nrian Krebs, and Gerhard Widmer. madmom: A new\nPython audio and music signal processing library.\nInProceedings of the ACM International Conference\non Multimedia , pages 1174–1178, Amsterdam, The\nNetherlands, 2016.\n[3] Sebastian B ¨ock, Florian Krebs, and Markus Schedl.\nEvaluating the online capabilities of onset detection\nmethods. In Proceedings of the 13th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , Porto, Portugal, 2012.\n[4] Sebastian B ¨ock, Jan Schl ¨uter, and Gerhard Widmer.\nEnhanced peak picking for onset detection with recur-\nrent neural networks. In Proceedings of the 6th Inter-\nnational Workshop on Machine Learning and Music\n(MML) , pages 15–18, Prague, Czech Republic, 2013.\n[5] Pieter-Tjerk de Boer, Dirk Kroese, Shie Mannor, and\nReuven Y . Rubinstein. A tutorial on the cross-entropy\nmethod. Annals of operations research , 134(1):19–67,\n1 2005.\n[6] Christian Dittmar and Daniel G ¨artner. Real-time tran-\nscription and separation of drum recordings based on\nNMF decomposition. In Proceedings of the Interna-\ntional Conference on Digital Audio Effects (DAFx) ,\npages 187–194, Erlangen, Germany, 2014.\n[7] Florian Eyben, Sebastian B ¨ock, Bj ¨orn Schuller, and\nAlex Graves. Universal onset detection with bidirec-\ntional long-short term memory neural networks. In\nProceedings of the 11th International Society for Mu-\nsic Information Retrieval Conference (ISMIR) , pages\n589–594, Utrecht, The Netherlands, 2010.\n[8] Olivier Gillet and Ga ¨el Richard. Enst-drums: an exten-\nsive audio-visual database for drum signals processing.\nInProceedings of the 7th International Society for Mu-\nsic Information Retrieval Conference (ISMIR) , pages\n156–159, Victoria, Canada, 2006.\n[9] Olivier Gillet and Ga ¨el Richard. Transcription and sep-\naration of drum signals from polyphonic music. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 16(3):529–540, 2008.\n[10] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\nDeep Learning . MIT Press, 2016.\n[11] Sergey Ioffe and Christian Szegedy. Batch normaliza-\ntion: Accelerating deep network training by reducing\ninternal covariate shift. In International Conference on\nMachine Learning , pages 448–456, 2015.[12] Ismo Kauppinen. Methods for detecting impulsive\nnoise in speech and audio signals,. In Proceedings\nof the 14th International Conference on Digital Sig-\nnal Processing (DSP2002) , pages 967–970, Santorini,\nGreece, 2002.\n[13] Diederik P. Kingma and Jimmy Ba. Adam: A method\nfor stochastic optimization. CoRR , abs/1412.6980,\n2014.\n[14] Jan Schluter and Sebastian Bock. Improved musical\nonset detection with convolutional neural networks.\nInProceedings of the 2014 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 6979–6983, 2014.\n[15] Carl Southall, Ryan Stables, and Jason Hockman. Au-\ntomatic drum transcription using bi-directional recur-\nrent neural networks. In Proceedings of the 17th In-\nternational Society for Music Information Retrieval\nConference (ISMIR) , pages 591–597, New York City,\nUnited States, 2016.\n[16] Carl Southall, Ryan Stables, and Jason Hockman.\nAutomatic drum transcription for polyphonic record-\nings using soft attention mechanisms and convolutional\nneural networks. In Proceedings of the 18th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , pages 606–612, Suzhou, China, 2017.\n[17] Carl Southall, Ryan Stables, and Jason Hockman.\nPlayer vs transcriber: A game approach to data manip-\nulation for automatic drum transcription. In Proceed-\nings of the 19th International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , Paris, France,\n2018.\n[18] Carl Southall, Chih-Wei Wu, Alexander Lerch, and\nJason Hockman. MDB drums an annotated subset of\nmedleyDB for automatic drum transcription. In Pro-\nceedings of the 18th International Society for Music\nInformation Retrieval Conference (ISMIR) , Suzhou,\nChina, 2017.\n[19] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. Dropout:\nA simple way to prevent neural networks from\noverﬁtting. Journal of Machine Learning Research ,\n15(1):1929–1958, 2014.\n[20] Richard V ogl, Matthias Dorfer, and Peter Knees. Re-\ncurrent neural networks for drum transcription. In Pro-\nceedings of the 17th International Society for Music\nInformation Retrieval Conference (ISMIR) , pages 730–\n736, New York City, United States, 2016.\n[21] Richard V ogl, Matthias Dorfer, and Peter Knees. Drum\ntranscription from polyphonic music with recurrent\nneural networks. In Proceedings of the IEEE Interna-\ntional Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP) , pages 201–205, New Orleans,\nUnited States, 2017.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 319[22] Richard V ogl, Matthias Dorfer, Gerhard Widmer, and\nPeter Knees. Drum transcription via joint beat and\ndrum modeling using convolutional recurrent neural\nnetworks. In Proceedings of the 18th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 150–157, Suzhou, China, 2017.\n[23] Chih-Wei Wu, Christian Dittmar, Carl Southall,\nRichard V ogl, Gerhard Widmer, Jason Hockman,\nMeinard M ¨uller, and Alexander Lerch. A Review of\nAutomatic Drum Transcription. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing ,\n26(9):1457–1483, 2018.\n[24] Chih-Wei Wu and Alexander Lerch. Drum transcrip-\ntion using partially ﬁxed non-negative matrix factor-\nization with template adaptation. In Proceedings of the\n16th International Society for Music Information Re-\ntrieval Conference (ISMIR) , pages 257–263, Malaga,\nSpain, 2015.320 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Influences on the Social Practices Surrounding Commercial Music Services: A Model for Rich Interactions.",
        "author": [
            "Louis Spinelli",
            "Josephine Lau",
            "Liz Pritchard",
            "Jin Ha Lee 0001"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492505",
        "url": "https://doi.org/10.5281/zenodo.1492505",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/52_Paper.pdf",
        "abstract": "Music can play an important role in social experiences and interactions. Technologies in-use affect these experiences and interactions and as they continue to evolve, social behaviors and norms surrounding them also evolve. In this paper, we explore the social aspects of commercial music services through focus group observation and interview data. We seek to better understand how existing services are used for social music practices and can be improved. We identified 9 social practices and 24 influences surrounding commercial music services. Based on the user data, we created a model of these practices and influences that provides a lens through which social experiences surrounding commercial music services can be understood. An understanding of these social practices within their contextual ecosystem help inform what influences should be considered when designing new technologies. Our findings include the identification of: the underlying relationships between practices and their influences; practices and influences that inform the weight of relationships in social networks; social norms to be considered when designing social features; influences that add additional insight to previously observed behaviors; and a detailed explanation of how music selection and listening practices can be supported by commercial music services.",
        "zenodo_id": 1492505,
        "dblp_key": "conf/ismir/SpinelliLPL18",
        "keywords": [
            "Music",
            "social experiences",
            "interactions",
            "technologies",
            "evolution",
            "social behaviors",
            "norms",
            "commercial music services",
            "focus group observation",
            "interview data"
        ],
        "content": "INFLUENCES ON THE SOCIAL PRACTICES SURROUNDING COMMERCIAL MUSIC\nSERVICES: A MODEL FOR RICH INTERACTIONS\nLouis Spinelli Josephine Lau Liz Pritchard Jin Ha Lee\nInformation School, University of Washington, Seattle\nspinelli@uw.edu, jolau@uw.edu, epritch@uw.edu, jinhalee@uw.edu\nABSTRACT\nMusic can play an important role in social experiences\nand interactions. Technologies in-use affect these expe-\nriences and interactions and as they continue to evolve, so-\ncial behaviors and norms surrounding them also evolve.\nIn this paper, we explore the social aspects of commer-\ncial music services through focus group observation and\ninterview data. We seek to better understand how exist-\ning services are used for social music practices and can\nbe improved. We identiﬁed 9 social practices and 24 in-\nﬂuences surrounding commercial music services. Based\non the user data, we created a model of these practices\nand inﬂuences that provides a lens through which social\nexperiences surrounding commercial music services can\nbe understood. An understanding of these social prac-\ntices within their contextual ecosystem help inform what\ninﬂuences should be considered when designing new tech-\nnologies. Our ﬁndings include the identiﬁcation of: the\nunderlying relationships between practices and their inﬂu-\nences; practices and inﬂuences that inform the weight of\nrelationships in social networks; social norms to be con-\nsidered when designing social features; inﬂuences that add\nadditional insight to previously observed behaviors; and a\ndetailed explanation of how music selection and listening\npractices can be supported by commercial music services.\n1. INTRODUCTION\nMusic plays a role in social experience and social cohesion\n[3, 18]. It can function as an icebreaker, to facilitate infor-\nmal interactions, to initiate friendships, and to strengthen\nrelationships [18]. Different music media and technology\nsuch as tapes, CDs, and digital ﬁles offer different affor-\ndances that inﬂuence the activities surrounding music [4].\nThese music-related activities are also inﬂuenced by the\nphysical and social context of the technology [5]. In this\npaper, we investigate music-related “social practices,” de-\nﬁned as activities a person carries out on a regular basis\ninvolving others or in the presence of other people. Social\nc\rLouis Spinelli, Josephine Lau, Liz Pritchard, Jin Ha Lee.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Louis Spinelli, Josephine Lau, Liz\nPritchard, Jin Ha Lee. “Inﬂuences on the Social Practices Surrounding\nCommercial Music Services: A Model for Rich Interactions”, 19th In-\nternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.practices surrounding music previously studied include lis-\ntening, discovering [12], sharing [4, 12], exploring and\npeeping [20].\nIn 2006, O’Hara and Brown’s work [20] on social and\ncollaborative aspects of music consumption technologies\nprovided an up-to-date and comprehensive foundation for\nresearch into the social practices surrounding music at that\ntime. Since then, Komulainen et al. [16] explored mu-\nsic sharing of youth in the context of new social media\nand mobile devices. Leong and Wright [18] explored so-\ncial practices surrounding music in households. Cunning-\nham et al. also explored social music practices in speciﬁc\nplaces and situations like parties [8] and cars [9]. Hagen\nand Luders [12] explored sharing and following behav-\niors with commercial music services. However, existing\nresearch on social practices surrounding music provides\nonly a glimpse of the social behaviors surrounding music\nrespective of that time and then-current technology.\nTechnological aspects related music practices have also\nbeen explored. For instance, Chamberlain and Crabtree [6]\nexplored the workﬂows and technologies involved in the\ndiscovery, identiﬁcation, acquisition, and organization of\nmusic in domestic settings. Goto [11] has also explored\nthe inﬂuence of new “intelligent” interfaces on music prac-\ntices. Several researchers including Barrington et al. [1],\nZhu et al. [23], and Kamehkhosh et al. [15] also explored\ndifferent factors, dimensionality of music, or automated\nrecommendations that affect playlist generation/evaluation\nand music search. Although their research did not specif-\nically focus on social practices surrounding commercial\nmusic services, our research adds new depth and context\nto their ﬁndings.\nStreaming music technologies have grown in preva-\nlence [19] and new features such as app integrations,\nand auto generated playlists as well as new interactive\nmechanisms like voice control have become more ubiqui-\ntous. The majority of music listeners in America are now\nstreaming music [19]. Because of the role music plays in\nsociety and the inﬂuence of technology on social activities,\nit is important to gain an updated understanding of the in-\nﬂuences on social practices surrounding commercial music\nservices.\nBuilding upon earlier research, our work looks specif-\nically at commercial music services such as Spotify, Pan-\ndora, and Google Play, which have become pervasive in\nrecent years. As social practices surrounding music have\nco-evolved with commercial music services, it is important671to understand the implications these new technologies have\non social practices, both positive and negative. However,\nupdated literature on social practices surrounding commer-\ncial music services — as well as inﬂuencers on social prac-\ntices — is lacking. To address this gap in knowledge we\nconducted six focus groups on two university campuses.\nWe found that social practices surrounding commercial\nmusic services exist within a rich ecosystem of inﬂuences.\nWe contribute a codebook deﬁning 24 inﬂuences identi-\nﬁed from user data as well as a model that provides a lens\nthrough which these experiences can be viewed and under-\nstood. This codebook provides insight into what inﬂuences\nneed to be taken into consideration when designing tech-\nnologies that are used socially. Notable ﬁndings based on\nour codebook and model include the identiﬁcation of: the\nunderlying relationships between practices and their inﬂu-\nences; practices and inﬂuences that inform the weight of\nrelationships in social networks; social norms to be con-\nsidered when designing social features; inﬂuences that add\nadditional insight to previously observed behaviors; and a\ndetailed explanation of how music selection and listening\npractices can be supported by commercial music services.\n2. RELATED WORK\nSocial practices were previously observed and described\nin context of the technologies of the day. When O’Hara\nand Brown published their book in 2006, MP3 sharing\nplatforms like Gnutella, Kazzaa, and Soulseek had just\nreplaced Napster, and iTunes was a new legal addition\nto the market. In 2013, Leong and Wright observed\nchanges in social behaviors around the exploration, dis-\ncovery, and sharing of music in relation to changes in tech-\nnologies including streaming internet and bluetooth on mo-\nbile phones. They noted “an emergence of new sociality\nand new forms of social practices around music” as well as\nnew social tensions emerging around music selection and\nlistening in shared settings [18].\nPrevious research in social practices surrounding music\nhighlights the important role evolving technologies play in\nunderstanding the social practices surrounding music. A\nbody of related work focuses on technology in shared en-\nvironments. Brush and Inkpen [5] examine the use and\nsharing of technology in domestic environments. They\nfound two common models for sharing devices: the ap-\npliance model and the proﬁle model. Sharing of devices\nusing the appliance model is mediated through social pro-\ntocols. In contrast, sharing of devices using the proﬁle\nmodel is mediated by allowing users to have individual\nproﬁles. Brush and Inkpen [5] also looked at the ownership\nmodels of devices - individual ownership versus shared\nownership - within domestic environments. The physical\nlocations of technology, privacy, and capability for per-\nsonalization of technologies all inﬂuenced social behav-\niors [5]. They found that video game systems exempliﬁed\ndevices with shared ownership whereas mobile music play-\ners exempliﬁed devices with individual ownership [5]. Ja-\ncobs, Cramer, and Barkhuus found four types of behaviors\nwhen studying the sharing practices of personal devicesbetween cohabiting couples: “intentional sharing, explic-\nitly not sharing, unintentional access and unintentionally\ninhibiting access” [14]. They also observed that couples\nsupport sharing behaviors by “hacking” the intended use\nof the technologies [14].\nThese studies highlight that practices surrounding tech-\nnology are inﬂuenced by the relationships of the individ-\nuals involved as well as the environment. Not only are\nsocial practices inﬂuenced byindividual relationships, pre-\nvious research also has shown social music practices also\ninﬂuence relationships and individual behavior. Boer and\nAbubakar [3] described the beneﬁts of music listening in\nfamilies and peer groups in which they found “beneﬁts for\nyoung people’s social cohesion and emotional well-being”.\nYang, Wang, and Mourali describe the inﬂuence of peers\non unauthorized music downloading and sharing [22]. The\nphysical context (environment) of previous research into\nsocial music practices also includes cars, public locations,\nworkspaces, and dance clubs [20].\nResearch has also focused on individuals and their prac-\ntices surrounding music and commercial music services.\nIn 2013, Belcher and Haridakis [2] explored the motives\npeople have for listening to music. Their results included\nthe identiﬁcation of social motivations inﬂuencing music\nlistening and selection behaviors. Lee and Price [17] devel-\noped seven personas based on empirical music user data.\nThese personas provide greater insight into design impli-\ncations for users than user groups based on demograph-\nics. Our work further expands understanding of commer-\ncial music service users and their behaviors in social situa-\ntions. Finally, while Hagen and L ¨uders [12] have looked at\nthe sharing and following behaviors of commercial music\nservice users, we take a broader look at the social prac-\ntices surrounding commercial music services and their in-\nﬂuences.\n3. STUDY DESIGN AND METHODS\nSix focus groups were held at Eckerd College and at the\nUniversity of Washington, Seattle (UW). Participants were\ncommercial music service users who were aged eighteen\nto thirty-four and lived with roommates. Results were ana-\nlyzed by qualitative content analysis using a constant com-\nparative method. Focus groups were selected to enable\nrich conversations where participants could prompt and re-\nmind one another of social situations they may have en-\ncountered. Participants were often prompted by situations\nsimilar to what they had encountered but described han-\ndling them in different ways. Participants often contrasted\ntheir social behaviors with individual behaviors.\n3.1 Participants\nRecruitment activities for the focus groups consisted of\ndisplaying ﬂyers, posting to listservs, and posting on so-\ncial media. Physical ﬂyers were placed on boards around\nthe Eckerd College Campus, the UW Campus, and in busi-\nnesses surrounding each campus. Posts were also made\nto additional listservs and social media outlets known to672 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018the researchers that included student afﬁliated groups and\ngroups of people not afﬁliated with either university. Par-\nticipants were compensated with an Amazon gift card for\nbeing part of two additional activities for subsequent re-\nsearch not discussed in this paper. All recruiting activities\ndirected potential participants to a screener survey.\nThe screener survey was used to ensure all participants\nwere between the ages of 18 and 34, currently living with\na roommate or roommates, and using at least one commer-\ncial music service. Of the 80 potential participants who\nﬁlled out the survey, 61 were eligible and available to at-\ntend the focus group on one of the preselected dates. Focus\ngroups were ﬁlled on a ﬁrst come ﬁrst serve basis.\nIn total, 6 participants from the screener for Eckerd Col-\nlege and 20 participants from the screener for UW took\npart in six focus sessions - 2 held at Eckerd College and 4\nheld at UW. Of the 25 participants who reported a gender\nidentity, 16 were female and 9 were male. 24 participants\nwere between the ages 18 to 24, and 2 others were ages 25\nto 34. Participants reported using a diverse array of com-\nmercial music services currently on the market including\nSpotify, Pandora, Google Play Music, YouTube, Sound-\ncloud, Apple Music, Online radio services (e.g. NPR mu-\nsic, iHeartRadio, etc.), Indieshufﬂe, Tidal, and Amazon\nMusic.\n3.2 Procedures\nEach of the six focus group sessions were approximately\nan hour long, with 3 to 6 participants per session. Each\nsession had a facilitator and note-taker. Each session was\nalso recorded and transcribed to ensure accurate analysis.\nTwo pilot focus groups were held - one at each location -\nto test the focus group script, although reﬁnement of the\nscript continued throughout the study.\nThe script1was designed to prompt participants to\nhave open-ended dialogues with each other about their\nsocial practices surrounding commercial music services.\nGenerally, each focus group began with a warm-up activ-\nity where each participant introduced themselves and de-\nscribed the commercial music services they use, then the\ngroup brainstormed different locations where they listen to\nmusic identifying the social situations. Participants were\nthen prompted to talk about their social practices in co-\nlistening situations, when sharing, and with technology.\n3.3 Analysis\nAfter transcribing each focus group session, we\nanonymized participant information and analyzed the\nresults in a two-part qualitative content analysis process.\nFor the ﬁrst part, we separated each participant com-\nment into individual post-it notes and conducted constant\ncomparative analysis including afﬁnity diagraming [21].\nWe opted for this method because it allows the creation\nof categories to be driven by the raw data and not estab-\nlished a priori [21]. When building an afﬁnity diagram,\n1Documented here: https://perma.cc/Z58H-XQJUnot only did social practices surrounding commercial mu-\nsic services emerge from the data, but inﬂuencers to these\npractices also emerged. Reﬂecting on previous work de-\nveloping personas of commercial music service users [17],\nwe recognized the importance of understanding these in-\nﬂuences on individual behavior in social situations.\nFor the second part, a codebook with social practices\nand inﬂuences was developed following an iterative coding\nprocess using Dedoose, custom Python code, and Google\nSheets. An initial version of the codebook was produced\nby a single team member who coded all transcripts asking\ntwo questions about each excerpt: 1) Does this describe\na social practice surrounding a commercial music service?\nand 2) Does this describe something that inﬂuences a social\npractice surrounding a commercial music service?\nThe codebook was then revised and reﬁned through\nan iterative team coding process, following a consensus\nmodel [13]. During each test, each excerpt was coded with\napplicable codes for social practices and inﬂuences. Each\nexcerpt describing a social situation could be coded with\nmultiple social practices and inﬂuences. Codes applied to\nexcerpts by different independent coders were then com-\npared. When applied codes differed, the team members\nwho coded the excerpts discussed their reasoning leading\nto an agreement that one coder erred, that an update to the\ncodebook was needed, or that a third team member was\nneeded as a tie-breaker.\n4. RESULTS\n4.1 Social Music Practice Codebook and Model\nDuring analysis, 9 distinct social practices and 24 inﬂu-\nences emerged1(Table 1). These inﬂuences were grouped\ninto three categories: group/social inﬂuences, external in-\nﬂuences, and internal inﬂuences.\nMany of the practices and inﬂuences that emerged could\nbe applied to – and have been observed with – other tech-\nnologies. For example, Music Technology Management\ncould have been described as Technology Management –\nsomething that has been studied in households with all\ntechnologies [5].\nCo-occurrence of social practices and inﬂuences when\napplying codes to the transcript led to the insight that\neach social practice surrounding commercial music ser-\nvices happens in an ecosystem. The ecosystem’s complex-\nity is captured in our model of the inﬂuences on the social\npractices surrounding commercial music services, as ex-\nplained in more detail in the following sub-sections.\n4.1.1 A Flexible Model of a Rich Ecosystem\nOur model of the social practices surrounding commercial\nmusic services and the inﬂuences thereof represents a rich\ncommunity of practices and inﬂuences. Inﬂuences affect\nother inﬂuences. Social practices affect other social prac-\ntices as well as their own inﬂuences. This meant that a\nsimple situation would likely capture multiple social prac-\ntices and multiple inﬂuences, as illustrated by the sample\nquote below.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 673Figure 1 . A model of the inﬂuences on the social practices\nsurrounding commercial music service.\n“. . . my roommate also likes Phish so I was at a show and they\nplayed her favorite song, so I like sent it to her... Like I sent a\nvideo recording of it and she was so excited. And we bonded over\nthat. ” (P22)\nWe coded this sample quote of a participant describ-\ning sharing a video of a song with her roommate with\ntwo social practices ( Sharing Music (Information) and So-\ncial Interaction/Navigation ) as well as three inﬂuences -\ntwo external inﬂuences ( Technology/Music Collection and\nEvent/Activity ) and one group/social inﬂuence ( Level of\nGroup Intimacy ).Level of Group Intimacy andSocial In-\nteraction/ Navigation demonstrate the bidirectional nature\nof the inﬂuences and social practices. The practice here\n- sharing music - is inﬂuenced by an existing relationship\nwhich it also strengthened.\n4.1.2 A Non-Linear Model\nOur model does not deﬁne the sequence in which social\npractices occur. Similar to Fosters’ nonlinear model of in-\nformation seeking behavior, our model of social practices\nis nonlinear [10]. Practices precipitate other practices, co-\noccur, and can be repeated. While a practice like Music\nIdentiﬁcation can occur after listening to music, it can also\nhappen during listening or before listening.\n“Yeah, for me, they need to be vetted as friend ﬁrst, and then I’ll\nsee if I’ll take their music recommendations. ” (P2)\nAs described by participant 2, for a social practice like\nsharing ( Sharing Music (Information) ) to be successful it\nmay need to be preceded by social interactions (Social\nInteraction/Navigation) that build trust and develop inti-\nmacy. These preceding practices could also include shar-\ning (Music (Information) Sharing ).\n4.2 The Full Spectrum of Each Inﬂuence\nEach code refers to the full spectrum of that inﬂuence, so\nusers could describe an inﬂuence as being very important\nto them or not at all. For instance, the Privacy and Secu-\nrity Considerations code also captures the lack of privacyconcerns when a participant described sharing their phone\nwith friends:\n“You can never be too sure, but I just don’t care. If they see any-\nthing, they see it. I guess I trust them not to snoop around on the\nphone. ” (P15)\n5. DISCUSSION\nTo illustrate the utility of the codebook and model, we dis-\ncuss ﬁndings that emerged from its application, adding to\nthe overall understanding of music services in social con-\ntexts.\n5.1 The Unequal Weight of Inﬂuences\nUnderstanding the meaning and underlying weight of in-\nﬂuences supports quantitative data and adds depth to de-\nsign considerations. Quantitative network analysis is bol-\nstered by an understanding of both the strength of connec-\ntions and what is driving each relationship. Understanding\nwhich inﬂuences will hinder the adoption of a new feature\nor technology can save time and money, and identify neg-\native externalities that may not have been considered.\n5.1.1 Weight of Relationships in Social Networks\nSocial network analysis is used to study the “connection[s]\nand interaction[s] between social actors” [7]. Crossley et\nal.’s 2014 collection of essays explore different applica-\ntions of social network analysis for understanding “music\nworlds” - a phrase they use to describe collective actions\nthat are similar to social movements [7]. Rather than look-\ning at large scale social movements, we focused on what\npractices surrounded commercial music services and why.\nUnderstanding why users share music and what sharing in-\ndicates about their relationships provides insight into the\nstrengths of connections between social actors.\n“Sometimes I will share [playlists] ... but usually just [with] close\nfriends . . .” (P18)\n“She knows music and knows when it sounds good and when it\ndoesn’t. So, if she’s like ‘listen to this because it sounds good’,\nthen I take every opinion that she has .” (P23)\nOur ﬁndings indicate that a person is more likely to\nshare a playlist with close friends. In addition, participants\nreported that after receiving music, they were more likely\nto listen to songs shared by vetted friends. With this ﬁnd-\ning in mind, quantitative data showing social actors that\nshare playlists and listen to one another’s recommenda-\ntions could indicate users that have a high Level of Inti-\nmacy .\n5.1.2 Social Norms as Design Considerations\nWe found social norms exist around vehicles and resi-\ndences, and have strong inﬂuences on social practices. Our\nparticipants indicate the location and the type of relation-\nship inﬂuence music sharing in social situations. Partic-\nipants were unlikely to share a request with a stranger, a\nhost, or a driver in a social situation because they did not\nbelieve it was socially appropriate. On the other hand, the674 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018A Social Practices Surrounding Music Any activity\ninvolving two or more participants engaging with mu-\nsic, or situations where an individual changes his/her\nmusic-related behavior based on social inﬂuences. Oth-\nerwise, does not apply to individual settings.\nA.1 Music Discovery When one ﬁnds new music\nwith someone, from someone else, or from a social\ninteraction/environment.\nA.2 Music Identiﬁcation When one takes action to\ndetermine the title of a song, artist, or album in a so-\ncial interaction/environment.\nA.3 Music Listening When a group (two or more\nparticipants) listens to music together.\nA.4 Music Management When one actively engages\nwith music and music metadata (e.g., adding an al-\nbum, adding a song to a playlist, browsing another’s\ncollection, using a storing practice to remember a\nsong for later).\nA.5 Music Selection When one selects music for co-\nlistening, sharing, and for collaborative playlists.\nA.6 Music Technology Management When one reg-\nulates how one’s music technologies and accounts are\nprotected, shared, accessed, or determined for their\nuses in a social interaction/environment.\nA.7 Navigating Space/Setting When one adjusts\ntheir behavior depending on the setting (e.g., leaving\na room when roommate is playing bad music or pass-\ning the phone around in a car).\nA.8 Music (Information) Sharing When one shares\nor receives music/ music metadata.\nA.9 Social Interaction/Navigation Bonding activi-\nties, group dynamic formation activities, group norm-\ning activities.\nB Internal Inﬂuences Any activity where a social mu-\nsic practice is inﬂuenced by how an individual engages\nwith the practice to a varying degree.\nB.1 Assertiveness Level The degree an individual in-\nﬂuences or confronts others, or vice versa (e.g., tak-\ning over music selection for the group or not feelingcomfortable suggesting a song be changed when one\ndoes not like it).\nB.2 Considerateness The degree to which an indi-\nvidual cares about bothering others.\nB.3 Current State The current emotional state,\nmood, or preference of an individual. NOT the cur-\nrent event or activity, for this USE: External Inﬂuence\n>Event/Activity.\nB.4 Effort/Engagement Level of effort or engage-\nment an individual is willing to put forth or take on\nresponsibility.\nB.5 Expertise/Knowledge of Music Having an ex-\npertise/knowledge about music, such as an awareness\nof new music, or lack thereof.\nB.6 Impression Management Considerations\nWhen others’ perceived reception of music choices,\nsuggestions, or tastes affect an individual’s actions in\na social music practice.\nB.7 Openness Willingness to explore new music or\nothers’ recommendations.\nB.8 Privacy and Security Considerations Consid-\nerations relating to privacy and/or security that inﬂu-\nence an individual’s actions in a social music practice.\nB.9 Social Driver Social need or purpose for interac-\ntion (e.g., a bonding experience, a desire to share the\nsame space with another person).\nB.10 Technology Knowledge/Considerations\nKnowledge and consideration, or lack thereof, of\nfeatures of technology, preferences for technology,\npersonal attachment to technology.\nB.11 Tolerance The degree an individual endures\nmusic or music-related behavior/situation they do not\nlike.\nB.12 Trust/Reliability When group members have\nvarying degree of conﬁdence in another member’s\nability to undertake social music practices (e.g., mu-\nsic selection/sharing) or when a member has varying\ndegree of conﬁdence from other group members.B.13 Willingness The degree of willingness an indi-\nvidual exhibits to take part in a practice.\nC External Inﬂuence Any activity where a social music\npractice is inﬂuenced by something outside of individ-\nual, group, or social traits.\nC.1 Event/Activity Attributes of an event or activity\nincluding the goals and the situational context.\nC.2 Norm/Expectation Societal norms including for\nplaces, events, and gatherings.\nC.3 Ownership and/or Control of Service or Tech-\nnology Possession of technology (e.g., speakers,\nChromecast) or access (e.g., subscription) to a com-\nmercial music service.\nC.4 Popularity/Reception of Music The wider soci-\netal and cultural reputation of a song, artist, or genre\nas well as the prevalence of this knowledge.\nC.5 Technology/Music Collection Technology, or\nthe attributes/features thereof, being used (e.g., com-\nmercial music service or physical collections likes\nvinyl or CDs).\nC.6 Temporal/Spatial Physical space, physical prox-\nimity, or temporality.\nD Group/Social Inﬂuence Any activity where a social\nmusic practice is inﬂuenced by the social aspect of a sit-\nuation or setting to a varying degree.\nD.1 First Mover When someone else being in a set-\nting ﬁrst affects the social situation.\nD.2 Group Dynamic When a group’s shared prefer-\nences or norms affect how they generally engage with\nmusic (e.g., a group’s preferences for songs, genres,\ntechnologies, or a group member to play music).\nD.3 Group Size When the number of people in the\nsocial situation affects how the group engages with\nmusic.\nD.4 Level of Group Intimacy When the level of fa-\nmiliarity between group members affects how they\nengage with music (e.g., perceived knowledge of an-\nother’s taste or opinion in music).\nTable 1 . Codebook of Social Practices and Inﬂuences.\nmore intimate the relationship, the less these social norms\nstood in the way.\n“If someone’s playing music, it’s usually the driver’s call .” (P1)\n“I’ve never even thought about asking an Uber driver to play\nmusic , like I don’t even know like, well I guess like because a\ndriver’s a stranger I’d feel kind of weird. ” (P12)\n“It depends on the people you are with , if you’re with friends, it’s\nﬁne, if you’re with siblings it’s possible you can compete, but if\nyou’re with people who you don’t know much, you would rather\nlisten to what’s going on rather than insist on playing something\nor maybe just plug in own earphones and not notice it. ” (P9)\nParticipants discussed situations where they would or\nwould not ask music to be changed or request a song to\nbe played. The ownership of the space seemed to matter\nsigniﬁcantly as they talked about respecting the host of the\nparty or the driver being in control of the music. Partici-\npants indicated that they would be uncomfortable request-\ning a song or a song change if they were not the driver\nunless with a group of friends. Most participants reported\nthat they would be unlikely to do either of these behaviors\nin a rideshare vehicle.\n5.2 Insight into Invisible Inﬂuences\nLeong and Wright observed recent technologies support-\ning social practices, but also contributing to social ten-\nsions [18]. They observed nuanced situations that involvedcontrol ( Assertiveness ), rituals ( Group Dynamic ), cul-\ntural/linguistic elements ( Social Norms, Group Dynamic ),\nrelationship ( Level of Group Intimacy ),Considerateness ,\nand setting ( Temporal/Spatial, Event/Activity ). While in-\ndependently developing our codebook we captured simi-\nlarly nuanced situations. In addition, we identiﬁed an ad-\nditional inﬂuence not explained in the previous research,\n“Social Driver ”, to describe situations where participants\nsimply wanted to be collocated with others.\n“Sometimes it’s nice with my roommate, we’ll go to our rooms. . .\nand just listen to our own individual music and do our own thing\nwhich can be nice , but you know when we want to be social . . .\nthen it’s kind of nice to listen to music together ... ” (P17)\n“[Headphones allow] me to immerse myself, like, in myself\nwhile still being in public .” (P23)\nThis need to be social has implications for social mu-\nsic practices and what technologies should consider. So-\ncial needs may drive people with disparate music tastes\nto use a commercial music service together to select mu-\nsic or for people to wear headphones in a shared space.\nEvent/Activity ,Temporal/Spatial , and Level of Group In-\ntimacy played a role in what participants did in these sit-\nuations. For instance, when studying, participants would\noften use headphones, but when taking a break from study-\ning, participants would more likely select music with their\nroommates. This ﬁnding — that an interaction between So-\ncial Driver andEvent/Activity affects Music Selection —Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 675has implications for playlist development and how com-\nmercial music services can support individual and social\nlistening (described in Section 5.3).\n5.3 Supporting Selection and Listening Practices\nthrough Playlist Generation\nMusic Selection practices varied depending on a num-\nber of inﬂuences including Group Size ,Group Dynamic ,\nEvent/Activity , and Effort/Engagement . The broadness of\na music collection and playlist content ( Technology/Music\nCollection ) played a role in what technology was used and\nwhat music was selected during different experiences.\n“Depending on the activity, most of the time when I’m alone,\nI’ll just put on a playlist that’s already curated . Like running,\nor at the gym, that way I don’t have to skip anything. It’s usually\npretty tailored to that speciﬁc activity. Or like studying, I usually\ndon’t really listen stuff with a lot of lyrics, because that can be\ndistracting. But when I’m with friends, there’s more skipping,\nwe’ll have a pretty broad variety of songs. ” (P16)\nFor many social situations, participants described either\ndeveloping or selecting playlists that included a larger vari-\nety of music enabling more skipping of songs. This behav-\nior can be supported by anticipating this skipping behavior\nwith playlists - both pre-made and auto-generated - by in-\ncluding a larger selection of songs knowing that some will\nbe skipped.\n“You might choose to play some common songs which generally\npeople would like and not very exotic choices so that most of\nthem enjoy - not all - but maybe most of them would like to have\nit there. ” (P9)\n“If I really want to listen to something, I’ll usually just listen to\nit by myself , because I can just focus on the music or focus on\nthe task that I’m doing rather than having to socialize with other\npeople, which is kind of, where you get lost in the conversation\nand miss out on the music. ” (P15)\nFor social situations, participants described choosing\nplaylists and songs “commonly” popular amongst their so-\ncial groups. Many participants described selecting music\nto set the mood, but recognized the music would not be the\nsole focus of the social situation. They would choose to\nlisten to music on their own if they really wanted to focus\non it. The described playlists for social situations included\ngo-to songs for the group, current hits, classics, and other\nmusic already likely to be know my group members.\n“If we’re pre-gaming, we can use a playlist and just use that and\nthat way no one has to touch the phone . Everyone can be talking\nwhile the music is just in the background. ” (P16)\n“It is annoying to like constantly be adding songs and also to\nlisten to three people’s songs that you may not like their music\nas much. ” (P7)\n“I do the queue thing a lot, like taking requests , if not just queue-\ning up stuff yourself. ” (P6)\nEffort and engagement also played a role in music selec-\ntion in social situations. While most participants preferred\nplaylists in social situations, some participants described\nbehaviors that required more effort – selecting and queu-ing songs was one of these behaviors.\n6. CONCLUSION AND FUTURE WORK\nIn this work, we created a model of these practices and\ninﬂuences that provides a lens through which social ex-\nperiences surrounding commercial music services can be\nunderstood as technology continues to evolve and affect\nthem. Our model, building on previous work, provides in-\nsight into the social practices and inﬂuences that should be\ntaken into consideration when designing commercial mu-\nsic services. Applying our codebook on qualitative data\npertaining to speciﬁc technologies and user groups enables\nresearchers to gather design considerations for social prac-\ntices speciﬁc to their own technology and context.\nDesign implications for music services based on the\nuser data include:\n1.Invitations to break social norms : If a music ser-\nvice wanted to support music sharing in social situ-\nations, a push notiﬁcation sent from a host or driver\ninviting the guest or passenger to make a request\nthrough a selected music service could help over-\ncome inhibitive social norms.\n2.Customization of playlists and stations for social\ninﬂuences : Playlists and stations are currently or-\nganized by genre, activity, and mood. Designing to\nsupport social listening would involve allowing par-\nticipants to select, customize, or generate playlists\nand stations based on group size. Recognition of\ngroup consumption should increase the amount of\nsongs and their level of popularity/broad appeal.\nThe addition of individual interviews in future studies\nwould likely lead to further insight into privacy consid-\nerations and impression management behaviors, although\nparticipants seemed to speak freely about both. Addi-\ntional studies with different methodologies and different\nsegments of the population will allow the model and code-\nbook to be revised, enriched, updated, and validated. Also,\nfurther research is currently underway exploring the Q-\nmethod as a way to understand the personal signiﬁcance\nof different inﬂuences for individual participants.\nWhile this model was based on the social practices sur-\nrounding commercial music services, it can apply to other\ntechnologies. A good example might be the streaming\nvideo services that were often described analogously by\nparticipants. It is likely that many of the inﬂuences will be\nsimilar, although the technologies differ.\n7. ACKNOWLEDGEMENTS\nThe authors would like to thank Katie O’Leary, Briana\nKeller, Pandora, Spotify, and Google Play Music.\n8. REFERENCES\n[1] L. Barrington, R. Oda, and G.R.G. Lanckriet. Smarter\nthan Genius? Human Evaluation of Music Recom-\nmender Systems. In Proc. ISMIR , pages 357–362,\n2009.676 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[2] J.D. Belcher and P. Haridakis. The Role of Background\nCharacteristics, Music-Listening Motives, and Music\nSelection on Music Discussion. Communication Quar-\nterly, 61(4):375–396, 2013.\n[3] D. Boer and A. Abubakar. Music listening in families\nand peer groups: Beneﬁts for young people’s social co-\nhesion and emotional well-being across four cultures.\nFrontiers in Psychology , 5, 2014.\n[4] B. Brown and A. Sellen. Sharing and Listening to\nMusic. In Consuming Music Together , pages 37–56.\nSpringer, Dordrecht, 2006.\n[5] A.J.B. Brush and K.M. Inkpen. Yours, Mine and Ours?\nSharing and Use of Technology in Domestic Envi-\nronments. In UbiComp 2007: Ubiquitous Computing ,\npages 109–126, 2007.\n[6] A. Chamberlain and A. Crabtree. Searching for music:\nUnderstanding the discovery, acquisition, processing\nand organization of music in a domestic setting for de-\nsign. Personal and Ubiquitous Computing , 20(4):559–\n571, 2016.\n[7] N. Crossley, S. McAndrew, and P. Widdop. Social Net-\nworks and Music Worlds . Routledge, New York, 2014.\n[8] S.J. Cunningham and D.M. Nichols. Exploring social\nmusic behaviour: An investigation of music selection\nat parties. In Proc. ISMIR , pages 747–752, 2009.\n[9] S.J. Cunningham, D.M. Nichols, D. Bainbridge, and\nH. Ali. Social music in cars. In Proc. ISMIR , pages\n457–462, 2014.\n[10] A. Foster. A Nonlinear Model of Information-Seeking\nBehavior. Journal of the American Society for Informa-\ntion Science and Technology , 55(3):228–237, 2004.\n[11] M. Goto. Intelligent Music Interfaces. In Proc. IUI ,\npages 3–4, 2018.\n[12] A.N. Hagen and M. L ¨uders. Social streaming? Nav-\nigating music as personal and social. Convergence:\nThe International Journal of Research into New Media\nTechnologies , 23(6):643–659, 2017.[13] C.E. Hill, S. Knox, B.J. Thompson, E.N. Williams,\nS.A. Hess, and N. Ladany. Consensual qualitative re-\nsearch: An update. Journal of Counseling Psychology ,\n52(2):196–205, 2005.\n[14] M. Jacobs, H. Cramer, and L. Barkhuus. Caring About\nSharing: Couples’ Practices in Single User Device Ac-\ncess. In Proc. of the 19th International Conference on\nSupporting Group Work , pages 235–243, 2016.\n[15] I. Kamehkhosh, D. Jannach, and G. Bonnin. How Au-\ntomated Recommendations Affect the Playlist Creation\nBehavior of Users. In MILC , 2018.\n[16] S. Komulainen, M. Karukka, and J. H ¨akkil ¨a. Social\nMusic Services in Teenage Life: A Case Study. In\nProc. OZCHI , pages 364–367, 2010.\n[17] J.H. Lee and R. Price. Understanding Users of Com-\nmercial Music Services Through Personas: Design Im-\nplications. In Proc. ISMIR , pages 476–482, 2015.\n[18] T.W. Leong and P.C. Wright. Revisiting Social Prac-\ntices Surrounding Music. In Proc. CHI , pages 951–\n960, 2013.\n[19] Everyone Listens to Music, But How We Lis-\nten is Changing. Nielson Newswire , 2015,\nhttps://perma.cc/C5DM-26P5.\n[20] K. O’Hara and B. Brown. Consuming Music Together:\nSocial and Collaborative Aspects of Music Consump-\ntion Technologies . Springer, Dordrecht, 2006.\n[21] A.J. Pickard. Research Methods in Information . Neal-\nSchuman, Chicago, 2nd ed. edition, 2013.\n[22] Z. Yang, J. Wang, and M. Mourali. Effect of peer inﬂu-\nence on unauthorized music downloading and sharing:\nThe moderating role of self-construal. Journal of Busi-\nness Research , 68(3):516–525, 2015.\n[23] S. Zhu, J. Cai, J. Zhang, Z. Li, J.C. Wang, and Y . Wang.\nBridging the User Intention Gap: An Intelligent and\nInteractive Multidimensional Music Search Engine. In\nProc. WISMM , pages 59–64, 2014.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 677"
    },
    {
        "title": "Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation.",
        "author": [
            "Daniel Stoller",
            "Sebastian Ewert",
            "Simon Dixon"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492417",
        "url": "https://doi.org/10.5281/zenodo.1492417",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/205_Paper.pdf",
        "abstract": "Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyperparameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a stateof-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem.",
        "zenodo_id": 1492417,
        "dblp_key": "conf/ismir/StollerED18",
        "keywords": [
            "end-to-end source separation",
            "phase information",
            "high sampling rates",
            "long-range temporal correlations",
            "Wave-U-Net",
            "resampling feature maps",
            "output layer",
            "upsampling technique",
            "context-aware prediction framework",
            "SDR evaluation metrics"
        ],
        "content": "WA VE-U-NET: A MULTI-SCALE NEURAL NETWORK FOR\nEND-TO-END AUDIO SOURCE SEPARATION\nDaniel Stoller\nQueen Mary University of London\nd.stoller@qmul.ac.ukSebastian Ewert\nSpotify\nsewert@spotify.comSimon Dixon\nQueen Mary University of London\ns.e.dixon@qmul.ac.uk\nABSTRACT\nModels for audio source separation usually operate on\nthe magnitude spectrum, which ignores phase information\nand makes separation performance dependant on hyper-\nparameters for the spectral front-end. Therefore, we in-\nvestigate end-to-end source separation in the time-domain,\nwhich allows modelling phase information and avoids ﬁxed\nspectral transformations. Due to high sampling rates for\naudio, employing a long temporal input context on the sam-\nple level is difﬁcult, but required for high quality separation\nresults because of long-range temporal correlations. In\nthis context, we propose the Wave-U-Net, an adaptation\nof the U-Net to the one-dimensional time domain, which\nrepeatedly resamples feature maps to compute and com-\nbine features at different time scales. We introduce further\narchitectural improvements, including an output layer that\nenforces source additivity, an upsampling technique and a\ncontext-aware prediction framework to reduce output arti-\nfacts. Experiments for singing voice separation indicate that\nour architecture yields a performance comparable to a state-\nof-the-art spectrogram-based U-Net architecture, given the\nsame data. Finally, we reveal a problem with outliers in the\ncurrently used SDR evaluation metrics and suggest report-\ning rank-based statistics to alleviate this problem.\n1. INTRODUCTION\nCurrent methods for audio source separation almost exclu-\nsively operate on spectrogram representations of the audio\nsignals [6, 7], as they allow for direct access to compo-\nnents in time and frequency. In particular, after applying a\nshort-time Fourier transform (STFT) to the input mixture\nsignal, the complex-valued spectrogram is split into its mag-\nnitude and phase components. Then only the magnitudes\nare input to a parametric model, which returns estimated\nspectrogram magnitudes for the individual sound sources.\nTo generate corresponding audio signals, these magnitudes\nare combined with the mixture phase and then converted\nwith an inverse STFT to the time domain. Optionally, the\nphase can be recovered for each source individually using\nthe Grifﬁn-Lim algorithm [5].\nc\rDaniel Stoller, Sebastian Ewert, Simon Dixon. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Daniel Stoller, Sebastian Ewert, Simon Dixon. “Wave-\nU-Net: A Multi-Scale Neural Network for End-to-End Audio Source\nSeparation”, 19th International Society for Music Information Retrieval\nConference, Paris, France, 2018.This approach has several limitations. Firstly, the STFT\noutput depends on many parameters, such as the size and\noverlap of audio frames, which can affect the time and\nfrequency resolution. Ideally, these parameters should be\noptimised in conjunction with the parameters of the sep-\naration model to maximise performance for a particular\nseparation task. In practice, however, the transform pa-\nrameters are ﬁxed to speciﬁc values. Secondly, since the\nseparation model does not estimate the source phase, it is\noften assumed to be equal to the mixture phase, which is\nincorrect for overlapping partials. Alternatively, the Grifﬁn-\nLim algorithm can be applied to ﬁnd an approximation to a\nsignal whose magnitudes are equal to the estimated ones,\nbut this is slow and often no such signal exists [8]. Lastly,\nthe mixture phase is ignored in the estimation of sources,\nwhich can potentially limit the performance. Thus, it would\nbe desirable for the separation model to learn to estimate\nthe source signals including their phase directly.\nAs an approach to tackle the above problems, several\naudio processing models were recently proposed that oper-\nate directly on time-domain audio signals, including speech\ndenoising as a task related to general audio source separa-\ntion [1,16,18]. Inspired by these ﬁrst results, we investigate\nin this paper the potential of fully end-to-end time-domain\nseparation systems in the face of unresolved challenges. In\nparticular, it is not clear if such a system will be able to deal\neffectively with the very long-range temporal dependencies\npresent in audio due to its high sampling rate. Further, it is\nnot obvious upfront whether the additional phase informa-\ntion will indeed be beneﬁcial for the task, or whether the\nnoisy phase might be detrimental for the learning dynamics\nin such a system. Overall, our contributions in this paper\ncan be summarised as follows.\n\u000fWe propose the Wave-U-Net, a one-dimensional\nadaptation of the U-Net architecture [7, 19], which\nseparates sources directly in the time domain and can\ntake large temporal contexts into account.\n\u000fWe show a way to provide the model with additional\ninput context to avoid artifacts at the boundaries of\noutput windows, in contrast to previous work [7, 16].\n\u000fWe replace strided transposed convolution used in\nprevious work [7, 16] for upsampling feature maps\nwith linear interpolation followed by a normal convo-\nlution to avoid artifacts.\nThis work was partially funded by EPSRC grant EP/L01632X/1.\nImplementation available at https://github.com/f90/\nWave-U-Net334Upsampling block 1 Downsampling block 1\nSource 1 output\nCrop and concat\nMixture audio\nCrop and concatDownsampling block 2\nCrop and concatDownsampling block L...Upsampling block 2\nUpsampling block L...1D Convolution, Size 15\nDownsampling1D Convolution, Size 5\nUpsampling\n1D Convolution, Size 151D Convolution, Size 1\nSource K-1 output\n...\nCrop and concatFigure 1 . Our proposed Wave-U-Net with Ksources and Llayers.\nWith our difference output layer, the K-th source prediction is the\ndifference between the mixture and the sum of the other sources.\n\u000fThe Wave-U-Net achieves good multi-instrument and\nsinging voice separation, the latter of which compares\nfavourably to our re-implementation of the state-of-\nthe-art network architecture [7], which we train under\ncomparable settings.\n\u000fSince the Wave-U-Net can process multi-channel au-\ndio, we compare stereo with mono source separation\nperformance\n\u000fWe highlight an issue with the commonly used Signal-\nto-Distortion ratio evaluation metric, and propose a\nwork-around.\nIt should be noted that we expect the current state of\nthe art model as presented in [7] to yield higher separation\nquality than what we report here, as the training dataset used\nin [7] is well-designed, highly unbiased and considerably\nlarger. However, we believe that our comparison with a\nre-implementation trained under similar conditions might\nbe indicative of relative performance improvements.\n2. RELATED WORK\nTo alleviate the problem of ﬁxed spectral representations\nwidely used in previous work [6, 11, 13, 14, 20, 23], an\nadaptive front-end for spectrogram computation was devel-\noped [24] that is trained jointly with the separation network,\nwhich operates on the resulting magnitude spectrogram. De-\nspite comparatively increased performance, the model does\nnot exploit the mixture phase for better source magnitude\npredictions and also does not output the source phase, so\nthe mixture phase has to be used for source signal recon-\nstruction, both of which limit performance.\nTo our knowledge, only the TasNet [12] and MRCAE [4]\nsystems tackle the general problem of audio source separa-\ntion in the time domain. The TasNet performs a decompo-\nsition of the signal into a set of basis signals and weights,and then creates a mask over the weights which are ﬁnally\nused to reconstruct the source signals. The model is shown\nto work for a speech separation task. However, the work\nmakes conceptual trade-offs to allow for low-latency appli-\ncations, while we focus on ofﬂine application, allowing us\nto exploit a large amount of contextual information.\nThe multi-resolution convolutional auto-encoder (MR-\nCAE) [4] uses two layers of convolution and transposed\nconvolution each. The authors argue the different convo-\nlutional ﬁlter sizes detect audio frequencies with different\nresolutions, but they work only on one time resolution (that\nof the input), since the network does not perform any resam-\npling. Since input and output consist of only 1025 audio\nsamples (equivalent to 23 ms), it can only exploit very lit-\ntle context information. Furthermore, at test time, output\nsegments are overlapped using a regular spacing and then\ncombined, which differs from how the network is trained.\nThis mismatch and the small context could hurt perfor-\nmance and also explain why the provided sound examples\nexhibit many artifacts.\nFor the purpose of speech enhancement and denoising,\nthe SEGAN [16] was developed, employing a neural net-\nwork with an encoder and decoder pathway that succes-\nsively halves and doubles the resolution of feature maps\nin each layer, respectively, and features skip connections\nbetween encoder and decoder layers. While we use a simi-\nlar architecture, we rectify the issue of aliasing artifacts in\nthe generated output when using strided transposed convo-\nlutions as shown by [15]. Furthermore, the model cannot\npredict audio samples close to its border output well since\nit is given no additional input context, which is an issue we\naddress using convolutions with proper padding. It is also\nnot clear if the model’s performance can transfer to other\nand more challenging audio source separation tasks.\nThe Wavenet [1] was adapted for speech denoising [18]\nto have a non-causal conditional input and a parallel output\nof samples for each prediction and is based on the repeated\napplication of dilated convolutions with exponentially in-\ncreasing dilation factors to factor in context information.\nWhile this architecture is very parameter-efﬁcient, memory\nconsumption is high since each feature map resulting from\na dilated convolution still has the original audio’s sampling\nrate as resolution.\nIn contrast, our approach calculates the longer-term de-\npendencies based on feature maps with more features and\nincreasingly lower resolution. This saves memory and en-\nables a large number of high-level features, which arguably\ndo not need sample-level resolution to be useful, such as\ninstrument activity, or the position in the current measure.\n3. THE WA VE-U-NET MODEL\nOur goal is to separate a mixture waveform\nM2[\u00001;1]Lm\u0002CintoKsource waveforms S1;:::;SK\nwithSk2[\u00001;1]Ls\u0002Cfor allk2f1;:::;Kg,Cas the\nnumber of audio channels and LmandLsas the respective\nnumbers of audio samples. For model variants with extra\ninput context, we have Lm>Lsand make predictions for\nthe centre part of the input.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 335Block Operation Shape\nInput (16384 ;1)\nDS, repeated for\ni= 1; : : : ; LConv1D (Fc\u0001i,fd)\nDecimate (4;288)\nConv1D (Fc\u0001(L+ 1) ,fd) (4;312)\nUS, repeated for\ni=L; : : : ; 1Upsample\nConcat (DS block i)\nConv1D (Fc\u0001i,fu) (16834 ;24)\nConcat (Input) (16834 ;25)\nConv1D (K, 1) (16834 ;2)\nTable 1 . Block diagram of the base architecture. Shapes describe\nthe ﬁnal output after potential repeated application of blocks, for\nthe example of model M1, and denote the number of time steps\nand feature channels, in that order. DS block irefers to the output\nbefore decimation. Note that the US blocks are applied in reverse\norder, from level Lto 1.\n3.1 The base architecture\nA diagram of the Wave-U-Net architecture is shown in Fig-\nure 1. It computes an increasing number of higher-level\nfeatures on coarser time scales using downsampling (DS)\nblocks. These features are combined with the earlier com-\nputed local, high-resolution features using upsampling (US)\nblocks, yielding multi-scale features which are used for\nmaking predictions. The network has Llevels in total, with\neach successive level operating at half the time resolution\nas the previous one. For Ksources to be estimated, the\nmodel returns predictions in the interval (\u00001;1), one for\neach source audio sample.\nThe detailed architecture is shown in Table 1.\nConv1D (x,y) denotes a 1D convolution with xﬁlters of\nsizey. It includes zero-padding for the base architecture,\nand is followed by a LeakyReLU activation (except for\nthe ﬁnal one, which uses tanh ).Decimate discards fea-\ntures for every other time step to halve the time resolution.\nUpsample performs upsampling in the time direction by a\nfactor of two, for which we use linear interpolation (see Sec-\ntion 3.1.1 for details). Concat (x) concatenates the current,\nhigh-level features with more local features x. In extensions\nof the base architecture (see below), where Conv1D does\nnot involve zero-padding, x is centre-cropped ﬁrst so it has\nthe same number of time steps as the current layer.\n3.1.1 Avoiding aliasing artifacts due to upsampling\nMany related approaches use transposed convolutions with\nstrides to upsample feature maps [7,16]. This can introduce\naliasing effects in the output, as shown for the case of image\ngeneration networks [15]. In initial tests, we also found ar-\ntifacts when using such convolutions as upsampling blocks\nin our Wave-U-Net model in the form of high-frequency\nbuzzing noise.\nTransposed convolutions with a ﬁlter size of kand a\nstride ofx >1can be viewed as convolutions applied to\nfeature maps padded with x\u00001zeros between each original\nvalue [2]. We suspect that the interleaving with zeros with-\nout subsequent low-pass ﬁltering introduces high-frequency\npatterns into the feature maps, shown symbolically in Fig-\nure 2, which leads to high-frequency noise in the ﬁnal out-\nput as well. Instead of transposed strided convolutions, we\nthus perform linear interpolation for upsampling, which\nensures temporal continuity in the feature space, followed\nby a normal convolution. In initial tests, we did not observe\nConvolutionDecimation\nUpsampling\n?Convolutiona) b)Figure 2 . a) Common model (e.g. [7]) with an even number of\ninputs (grey) which are zero-padded (black) before convolving,\ncreating artifacts at the borders (dark colours). After decimation,\na transposed convolution with stride 2 is shown here as upsam-\npling by zero-padding intermediate and border values followed\nby normal convolution, which likely creates high-frequency arti-\nfacts in the output. b) Our model with proper input context and\nlinear interpolation for upsampling from Section 3.2.2 does not\nuse zero-padding. The number of features is kept uneven, so\nthat upsampling does not require extrapolating values (red arrow).\nAlthough the output is smaller, artifacts are avoided.\nany high-frequency sound artifacts in the output with this\ntechnique and achieved very similar performance.\n3.2 Architectural improvements\nThe previous Section described the baseline variant of the\nWave-U-Net. In the following, we will describe a set of\narchitectural improvements for the Wave-U-Net designed\nto increase model performance.\n3.2.1 Difference output layer\nOur baseline model outputs one source estimate for each\nofKsources by independently applying Kconvolutional\nﬁlters followed by a tanh non-linearity to the last feature\nmap. In the separation tasks we consider, the mixture signal\nis the sum of its source signal components: M\u0019PK\nj=1Sj.\nSince our baseline model is not constrained in this fashion, it\nhas to learn this rule approximately to avoid highly improb-\nable outputs, which could slow down learning and reduce\nperformance. Therefore, we use a difference output layer to\nconstrain the outputs ^Sj, enforcingPK\nj=1^Sj=M: only\nK\u00001convolutional ﬁlters with a size of 1 are applied to\nthe last feature map of the network, followed by a tanh non-\nlinearity, to estimate the ﬁrst K\u00001source signals. The last\nsource is then simply computed as ^SK=M\u0000PK\u00001\nj=1^Sj.\nThis type of output was also used for speech denois-\ning in [18] as part of an “energy-conserving” loss, and a\nsimilar idea can be found very commonly in spectrogram-\nbased source separation in the form of masks that distribute\nthe energy of the input mixture magnitudes to the output\nsources. We investigate the impact of introducing this layer\nand its additivity assumption, since it depends on the extent\nto which this additivity property is satisﬁed by the data.336 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20183.2.2 Prediction with proper input context and resampling\nIn previous work [4,7,16], the input and the feature maps are\npadded with zeros before convolving, so that the resulting\nfeature map does not change in its dimension, as shown in\nFigure 2a. This simpliﬁes the network’s implementation,\nsince the input and output dimensions are the same. Zero-\npadding audio or spectrogram input this way effectively\nextends the input using silence at the beginning and end.\nHowever, taken from a random position in a full audio\nsignal, the information at the boundary becomes artiﬁcial,\ni.e. the temporal context for this excerpt is given in the\nfull audio signal but is ignored and assumed to be silent.\nWithout proper context information, the network thus has\ndifﬁculty predicting output values near the beginning and\nend of the sequence. As a result, simply concatenating the\noutputs as non-overlapping segments at test time to obtain\nthe prediction for a full audio signal can create audible\nartifacts at the segment borders, as neighbouring outputs\ncan be inconsistent when they are generated without correct\ncontext information. In Section 5.2, we investigate this\nbehaviour in practice.\nAs a solution, we employ convolutions without implicit\npadding and instead provide a mixture input larger than\nthe size of the output prediction, so that the convolutions\nare computed on the correct audio context (see Figure 2b).\nSince this reduces the feature map sizes, we constrain the\npossible output sizes of the network so that feature maps\nare always large enough for the following convolution.\nFurther, when resampling feature maps, feature dimen-\nsions are often exactly halved or doubled [7, 16], as shown\nin Figure 2a for transposed strided convolution. However,\nthis necessarily involves extrapolating at least one value at\na border, which can again introduce artifacts. Instead, we\ninterpolate only between known neighbouring values and\nkeep the very ﬁrst and last entries, producing 2n\u00001entries\nfromnor vice versa, as shown in Figure 2b. To recover\nthe intermediate values after decimation, while keeping bor-\nder values the same, we ensure that feature maps have odd\ndimensionality.\n3.2.3 Stereo channels\nTo accommodate for multi-channel input with Cchannels,\nwe simply change the input Mfrom anLm\u00021to an\nLm\u0002Cmatrix. Since the second dimension is treated\nas a feature channel, the ﬁrst convolution of the network\ntakes into account all input channels. For multi-channel\noutput withCchannels, we modify the output component\nto haveKindependent convolutional layers with ﬁlter size\n1 andCﬁlters each. With a difference output layer, we\nonly useK\u00001such convolutional layers. We use this\nsimple approach with C= 2to perform experiments with\nstereo recordings and investigate the degree of improvement\nin source separation metrics when using stereo instead of\nmono estimation.\n3.2.4 Learned upsampling for Wave-U-Net\nLinear interpolation for upsampling is simple, parameter-\nless and encourages feature continuity. However, it may\nbe restricting the network capacity too much. Perhaps, the\nfeature spaces used in these feature maps are not structuredso that a linear interpolation between two points in feature\nspace is a useful point on its own, so that a learned upsam-\npling could further enhance performance. To this end, we\npropose the learned upsampling layer. For a given F\u0002n\nfeature map with ntime steps, we compute an interpolated\nfeatureft+0:52RFfor pairs of neighbouring features\nft;ft+12RFusing parameters w2RFand the sigmoid\nfunction\u001bto constrain each wi2wto the [0;1]interval:\nft+0:5=\u001b(w)\fft+ (1\u0000\u001b(w))\fft+1 (1)\nThis can be implemented as a 1D convolution across time\nwithFﬁlters of size two and no padding with a properly\nconstrained matrix. The learned interpolation layer can\nbe viewed as a generalisation of simple linear interpola-\ntion, since it allows convex combinations of features with\nweights other than 0:5.\n4. EXPERIMENTS\nWe evaluate the performance of our models on two tasks:\nSinging voice separation and music separation with bass,\ndrums, guitar, vocals and “other” instruments as categories,\nas deﬁned by the SiSec separation campaign [10].\n4.1 Datasets\n75tracks from the training partition of the MUSDB [17]\nmulti-track database are randomly assigned to our training\nset, and the remaining 25tracks form the validation set,\nwhich is used for early stopping. Final performance is\nevaluated on the MUSDB test partition comprised of 50\nsongs. For singing voice separation, we also add the whole\nCCMixter database [9] to the training set.\nAs data augmentation for both tasks, we multiply source\nsignals with a factor chosen uniformly from the interval\n[0:7;1:0]and set the input mixture as the sum of source\nsignals. No further data preprocessing is performed, only a\nconversion to mono (except for stereo models) and down-\nsampling to 22050 Hz.\n4.2 Training procedure\nDuring training, audio excerpts are sampled randomly and\ninputs padded accordingly for models with input context.\nAs loss, we use the mean squared error (MSE) over all\nsource output samples in a batch. We use the ADAM op-\ntimizer with learning rate 0:0001 , decay rates \f1= 0:9\nand\f2= 0:999and a batch size of 16. We deﬁne 2000\niterations as one epoch, and perform early stopping after 20\nepochs of no improvement on the validation set, measured\nby the MSE loss. Afterwards, the last model is ﬁne-tuned\nfurther, with the batch size doubled and the learning rate\nlowered to 0:00001 , again until 20 epochs without improve-\nment in validation loss. Finally, the model with the best\nvalidation loss is selected.\n4.3 Model settings and variants\nFor our baseline model, we use Lm=Ls= 16384 input\nand output samples, L= 12 layers,Fc= 24 extra ﬁlters\nper layer and ﬁlter sizes fd= 15 andfu= 5.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 337To determine the impact of the model improvements\ndescribed in Section 3.2, we train a baseline model M1 as\ndescribed in Section 3.1 and models M2 to M5 which add\nthe difference output layer from Section 3.2.1 (M2), the in-\nput context and resampling from Section 3.2.2 (M3), stereo\nchannels from Section 3.2.3 (M4), and learned upsampling\nfrom Section 3.2.4 (M5), and also contain all features of the\nrespectively previous model. We apply the best model of\nthe above (M4) to multi-instrument separation (M6). Mod-\nels with input context (M3 to M6) have Lm= 147443 input\nandLs= 16389 output samples.\nFor comparison with previous work, we also train\nthe spectrogram-based U-Net architecture [7] (U7) that\nachieved state-of-the-art vocal separation performance, and\na Wave-U-Net comparison model (M7) under the same con-\nditions, both using the audio-based MSE loss and mono\nsignals downsampled to 8192 Hz. M7 is based on the best\nmodel M4, but is set to Lm= 233459 andLs= 102405 to\nhave very similar output size compared to U7 ( Ls= 98650\nsamples),Fc= 34 to bring our network to the same size\nas U7 (20M param.), and the initial batch size is set to four\ndue to the high amount of memory needed per sample. To\ntrain U7, we backpropagate the error through the inverse\nSTFT operation that is used to construct the source audio\nsignal from the estimated spectrogram magnitudes and the\nmixture phase. We also train the same model with an L1\nloss on the spectral magnitudes (U7a), following [7]. Since\nthe training procedure and loss are exactly the same for\nnetworks U7 and M7, we can fairly compare both architec-\ntures by ensuring that performance differences do not arise\nsimply because of the amount of training data or the type of\nloss function used, and also compare with a spectrogram-\nbased loss (U7a). Despite our effort to enable an overall\nmodel comparison, note that some training settings such as\nlearning rates used in [7] might differ from ours (and are\npartly unknown) and could provide better performance with\nU7 and U7a than shown here, even with the same dataset.\n5. RESULTS\n5.1 Quantitative results\n5.1.1 Evaluation metrics\nThe signal-to-distortion (SDR) metric is commonly used\nto evaluate source separation performance [25]. An audio\ntrack is usually partitioned into non-overlapping audio seg-\nments multiple seconds in length, and segment-wise metrics\nare then averaged over each audio track or the whole dataset\nto evaluate model performance. Following the procedure\nused for the SiSec separation campaign 2018 [17], these\nsegments are one second long.\n5.1.2 Issues with current evaluation metrics\nThe SDR computation is problematic when the true source\nis silent or near-silent. In case of silence, the SDR is unde-\nﬁned ( log(0) ), which happens often for vocal tracks. Such\nsegments are excluded from the results, so performance on\nthese segments is ignored. For near-silent parts, the SDR\nis typically very low when the separator output is quiet,\nbut not silent, although such an output is arguably not a\n100\n 80\n 60\n 40\n 20\n 0 20 40AccompanimentVocalsSegment-wise SDR distributionFigure 3 . Violin plot of the segment-wise SDR values in the\nMUSDB test set for model M5. Black points show medians, dark\nblue lines the means.\ngrave error perceptually. These outliers are visualised using\nmodel M5 in Figure 3. Since the mean over segments is\nusually used to obtain overall performance measures, these\noutliers greatly affect evaluation results.\nSince the collection of segment-wise vocal SDR values\nacross the dataset is not normally distributed (compare Fig-\nure 3 for vocals), the mean and standard deviation are not\nsufﬁcient to adequately summarise it. As a workaround,\nwe take the median over segments, as it is robust against\noutliers and intuitively describes the minimum performance\nthat is achieved 50% of the time. To describe the spread\nof the distribution, we use the median absolute deviation\n(MAD) as a rank-based equivalent to the standard deviation\n(SD). It is deﬁned as the median of the absolute deviations\nfrom the overall median and is easily interpretable, since\na value ofxmeans that 50% of values have an absolute\ndifference from the median that is lower than x.\nWe also note that increasing the duration of segments\nbeyond one second alleviates this issue by removing many,\nbut not all outliers. This is more memory-intensive and\npresumably still punishes errors during silent sections most.\n5.1.3 Model comparison\nTable 2 shows the evaluation results for singing voice sepa-\nration. The low vocal SDR means and high medians for all\nmodels again demonstrate the outlier problem discussed in\nSection 5.1.2. The difference output layer does not notice-\nably change performance, as model M2 appears to be only\nvery slightly better than model M1. Initial experiments with-\nout ﬁne-tuning showed a larger difference, which may indi-\ncate that a ﬁner adjustment of weights makes constrained\noutputs less important, but they could still enable the us-\nage of faster learning rates. Introducing context noticeably\nimproves performance, as model M3 shows, likely due to\nbetter predictions at output borders. The stereo modeling in\nmodel M4 yields improvements especially for accompani-\nment, which may be because its sounds are panned more to\nthe left or right channels than vocals. The learned upsam-\npling (M5) slightly improves the median, but slightly de-\ncreases the mean vocal SDR. The small differences could be\nexplained by the low number of weights in learned upsam-\npling layers, considering that we also experimented with\nunconstrained convolutions, which brought more improve-\nments but also high-frequency sound artifacts. We therefore\nconsider M4 as our best model. For multi-instrument sepa-\nration, we achieve slightly lower but moderate performance\n(M6), as shown in Table 3, in part due to less training data.338 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018M1 M2 M3 M4 M5 M7 U7 U7a\nV oc.Med. 3.90 3.92 3.96 4.46 4.58 3.49 2.76 2.74\nMAD 3.04 3.01 3.00 3.21 3.28 2.71 2.46 2.54\nMean -0.12 0.05 0.31 0.65 0.55 -0.23 -0.66 0.51\nSD 14.00 13.63 13.25 13.67 13.84 13.00 12.38 10.82\nAcc.Med. 7.45 7.46 7.53 10.69 10.66 7.12 6.76 6.68\nMAD 2.08 2.10 2.11 3.15 3.10 2.04 2.00 2.04\nMean 7.62 7.68 7.66 11.85 11.74 7.15 6.90 6.85\nSD 3.93 3.84 3.90 7.03 7.05 4.10 3.67 3.60\nTable 2 . Test set performance metrics (SDR statistics, in dB) for\neach singing voice separation model. Best performances overall\nand among comparison models are shown in bold.\nV ocals Other\nMed. MAD Mean SD Med. MAD Mean SD\nM6 3.0 2.76 -2.10 15.41 2.03 1.64 1.68 6.14\nBass Drums\nMed. MAD Mean SD Med. MAD Mean SD\nM6 2.91 2.47 -0.30 13.50 4.15 1.99 2.88 7.68\nTable 3 . Test performance metrics (SDR statistics, in dB) for our\nmulti-instrument model\nU7 performs worse than our comparison model M7, sug-\ngesting that our network architecture compares favourably\nto the state-of-the-art architecture since all else is kept con-\nstant during the experiments. However, U7 stopped improv-\ning on the training set unexpectedly early, perhaps because\nit was not designed for minimising an audio-based MSE\nloss or because of effects related to backpropagating gra-\ndients through the inverse STFT. In contrast, U7a showed\nexpected training behaviour using the magnitude-based loss.\nOur model also outperforms U7a, yielding considerably\nhigher mean and median SDR scores. The mean vocal SDR\nis the only exception, arising since our model has more\noutlier segments, but better output the majority of the time.\nModels M4 and M6 were submitted as STL1 and STL2\nto the SiSec campaign [22]. For vocals, M4 performs bet-\nter or as well as almost all other systems. Although it is\nsigniﬁcantly outperformed by submissions UHL3, TAK1-3\nand TAU1, all of these except TAK1 used an additional 800\nsongs for training and thus have a large advantage. M4 also\nseparates accompaniment well, although slightly less so\nthan the vocals. We refer to [22] for more details.\n5.2 Qualitative results and observations\nAs an example of problems occurring when not using a\nproper temporal context, we generated a vocal source es-\ntimate for a song with the baseline model M1, and visu-\nalised an excerpt using a spectrogram in Figure 4. Since\nthe model’s input and output are of equal length and the\ntotal output is created by concatenating predictions for non-\noverlapping consecutive audio segments, inconsistencies\nemerge at the borders shown in red: the loudness abruptly\ndecreases at 1:2seconds, and a beginning vocal melisma\nis suddenly cut off at 2:8seconds, leaving only quiet noise,\nbefore the vocals reappear at 4:2seconds. A vocal melisma\nwith only the vowel “a” can sound similar to a non-vocal\ninstrument and presumably was mistaken for one because\nno further temporal context was available.\nIn conclusion, these models suffer not only from incon-\nsistencies at such segment borders, but are also less capable\nof performing separation there whenever information from\na temporal context is required. Larger input and output\nsizes alleviate the issue somewhat, but the problems at the\n0 1 2 3 4 5 6\nt (s)0\n2\n4\n6\n8\n10f (KHz)\n50\n40\n30\n20\n10\n01020Figure 4 . Power spectrogram (dB) of a vocal estimate excerpt gen-\nerated by a model without additional input context. Red markers\nshow boundaries between independent segment-wise predictions.\nborders remain. Blending the predictions for overlapping\nsegments [4] is an ad-hoc solution, since the average of\nmultiple predicted audio signals might not be a realistic\nprediction itself. For example, two sinusoids with equal\namplitude and frequency, but opposite phase would cancel\neach other out. Blending should thus be avoided in favour\nof our context-aware prediction framework.\n6. DISCUSSION AND CONCLUSION\nIn this paper, we proposed the Wave-U-Net for end-to-end\naudio source separation without any pre- or postprocessing,\nand applied it to singing voice and multi-instrument sepa-\nration. A long temporal context is processed by repeated\ndownsampling and convolution of feature maps to com-\nbine high- and low-level features at different time-scales.\nAs indicated by our experiments, it outperforms the state-\nof-the-art spectrogram-based U-Net architecture [7] when\ntrained under comparable settings. Since our data is quite\nlimited in size however, it would be interesting to train our\nmodel on datasets comparable in size to the one used in [7]\nto better assess respective advantages and disadvantages.\nWe highlight the lack of a proper temporal input context\nin recent separation and enhancement models, which can\nhurt performance and create artifacts, and propose a simple\nchange to the padding of convolutions as a solution. Simi-\nlarly, artifacts resulting from upsampling by zero-padding\nas part of strided transposed convolutions can be addressed\nwith a linear upsampling with a ﬁxed or learned weight to\navoid high-frequency artifacts.\nFinally, we identify a problem in current SDR-based\nevaluation frameworks that produces outliers for quiet parts\nof sources and propose additionally reporting rank-based\nmetrics as a simple workaround. However, the underlying\nproblem of perceptual evaluation of sound separation results\nusing SDR metrics still remains and should be tackled at its\nroot in the future.\nFor future work, we could investigate to which extent our\nmodel performs a spectral analysis, and how to incorporate\ncomputations similar to those in a multi-scale ﬁlterbank, or\nto explicitly compute a decomposition of the input signal\ninto a hierarchical set of basis signals and weightings on\nwhich to perform the separation, similar to the TasNet [12].\nFurthermore, better loss functions for raw audio prediction\nshould be investigated such as the ones provided by genera-\ntive adversarial networks [3, 21], since the MSE might not\nreﬂect the perceived loss of quality well.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 3397. REFERENCES\n[1]Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol\nVinyals, Alex Graves, Nal Kalchbrenner, Andrew Se-\nnior, Koray Kavukcuoglu, et al. Wavenet: A generative\nmodel for raw audio. arXiv preprint arXiv:1609.03499 ,\n2016.\n[2]Vincent Dumoulin and Francesco Visin. A guide to\nconvolution arithmetic for deep learning. arXiv preprint\narXiv:1603.07285 , 2016.\n[3]Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial\nnets. In Advances in Neural Information Processing\nSystems , pages 2672–2680, 2014.\n[4]Emad M Grais, Dominic Ward, and Mark D Plumbley.\nRaw multi-channel audio source separation using multi-\nresolution convolutional auto-encoders. arXiv preprint\narXiv:1803.00702 , 2018.\n[5]D. Grifﬁn and Jae Lim. Signal estimation from modi-\nﬁed short-time fourier transform. IEEE Transactions on\nAcoustics, Speech, and Signal Processing , 32(2):236–\n243, 1984.\n[6]Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson,\nand Paris Smaragdis. Singing-voice separation from\nmonaural recordings using deep recurrent neural net-\nworks. In International Society for Music Information\nRetrieval (ISMIR) , pages 477–482, 2014.\n[7]Andreas Jansson, Eric J. Humphrey, Nicola Montecchio,\nRachel Bittner, Aparna Kumar, and Tillman Weyde.\nSinging voice separation with deep U-Net convolutional\nnetworks. In Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 323–332, 2017.\n[8]Jonathan Le Roux, Nobutaka Ono, and Shigeki\nSagayama. Explicit consistency constraints for STFT\nspectrograms and their application to phase reconstruc-\ntion. In SAPA@ INTERSPEECH , pages 23–28, 2008.\n[9]Antoine Liutkus, Derry Fitzgerald, and Zafar Raﬁi.\nScalable audio separation with light kernel additive\nmodelling. In 2015 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\npages 76–80. IEEE, 2015.\n[10] Antoine Liutkus, Fabian-Robert St ¨oter, Zafar Raﬁi,\nDaichi Kitamura, Bertrand Rivet, Nobutaka Ito, Nobu-\ntaka Ono, and Julie Fontecave. The 2016 signal separa-\ntion evaluation campaign. In Proceedings of the Inter-\nnational Conference on Latent Variable Analysis and\nSignal Separation (LVA/ICA) , pages 323–332, 2017.\n[11] Y . Luo, Z. Chen, J. R. Hershey, J. Le Roux, and N. Mes-\ngarani. Deep clustering and conventional networks for\nmusic separation: Stronger together. In IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP) , pages 61–65, 2017.[12] Yi Luo and Nima Mesgarani. Tasnet: time-domain au-\ndio separation network for real-time, single-channel\nspeech separation. CoRR , abs/1711.00541, 2017.\n[13] Marius Miron, Jordi Janer Mestres, and Emilia\nG´omez Guti ´errez. Generating data to train convolutional\nneural networks for classical music source separation.\nInProceedings of the 14th Sound and Music Computing\nConference . Aalto University, 2017.\n[14] Aditya Arie Nugraha, Antoine Liutkus, and Emmanuel\nVincent. Multichannel audio source separation with\ndeep neural networks . PhD thesis, Inria, 2015.\n[15] Augustus Odena, Vincent Dumoulin, and Chris Olah.\nDeconvolution and checkerboard artifacts. Distill , 2016.\n[16] Santiago Pascual, Antonio Bonafonte, and Joan Serra.\nSegan: Speech enhancement generative adversarial net-\nwork. arXiv preprint arXiv:1703.09452 , 2017.\n[17] Zafar Raﬁi, Antoine Liutkus, Fabian-Robert Stter,\nStylianos Ioannis Mimilakis, and Rachel Bittner. The\nMUSDB18 corpus for music separation, 2017.\n[18] Dario Rethage, Jordi Pons, and Xavier Serra. A wavenet\nfor speech denoising. CoRR , abs/1706.07162, 2017.\n[19] O. Ronneberger, P. Fischer, and T. Brox. U-net: Con-\nvolutional networks for biomedical image segmenta-\ntion. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention , pages\n234–241. Springer, 2015.\n[20] Andrew JR Simpson, Gerard Roma, and Mark D Plumb-\nley. Deep karaoke: Extracting vocals from musical mix-\ntures using a convolutional deep neural network. In\nInternational Conference on Latent Variable Analysis\nand Signal Separation , pages 429–436. Springer, 2015.\n[21] Daniel Stoller, Sebastian Ewert, and Simon Dixon. Ad-\nversarial semi-supervised audio source separation ap-\nplied to singing voice extraction. In Proceedings of the\nIEEE International Conference on Acoustics, Speech,\nand Signal Processing (ICASSP) , pages 2391–2395,\nCalgary, Canada, 2018. IEEE.\n[22] F.-R. St ¨oter, A. Liutkus, and N. Ito. The 2018 Signal\nSeparation Evaluation Campaign. ArXiv e-prints , 2018.\n[23] S. Uhlich, M. Porcu, F. Giron, M. Enenkl, T. Kemp,\nN. Takahashi, and Y . Mitsufuji. Improving music source\nseparation based on deep neural networks through data\naugmentation and network blending. In 2017 IEEE In-\nternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 261–265, March 2017.\n[24] Shrikant Venkataramani and Paris Smaragdis. End-to-\nend source separation with adaptive front-ends. CoRR ,\nabs/1705.02514, 2017.\n[25] E. Vincent, R. Gribonval, and C. Fevotte. Performance\nmeasurement in blind audio source separation. IEEE\nTransactions on Audio, Speech, and Language Process-\ning, 14(4):1462–1469, 2006.340 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Concert Stitch: Organization and Synchronization of Crowd Sourced Recordings.",
        "author": [
            "Vinod Subramanian",
            "Alexander Lerch 0001"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492489",
        "url": "https://doi.org/10.5281/zenodo.1492489",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/182_Paper.pdf",
        "abstract": "The number of audience recordings of concerts on the internet has exploded with the advent of smartphones. This paper proposes a method to organize and align these recordings in order to create one or more complete renderings of the concert. The process comprises two steps: first, using audio fingerprints to represent the recordings, identify overlapping segments, and compute an approximate alignment using a modified Dynamic Time Warping (DTW) algorithm and second, applying a cross-correlation around the approximate alignment points in order to improve the accuracy of the alignment. The proposed method is compared to two baseline systems using approaches previously proposed for similar tasks. One baseline cross-correlates the audio fingerprints directly without DTW. The second baseline replaces the audio fingerprints with pitch chroma in the DTW algorithm. A new dataset annotating real-world data obtained from the Live Music Archive is presented and used for evaluation of the three systems.",
        "zenodo_id": 1492489,
        "dblp_key": "conf/ismir/SubramanianL18",
        "keywords": [
            "Smartphones",
            "concerts",
            "internet",
            "audience recordings",
            "organizing",
            "aligning",
            "renderings",
            "complete concert",
            "DTW algorithm",
            "cross-correlation"
        ],
        "content": "CONCERT STITCH: ORGANIZATION AND SYNCHRONIZATION OF\nCROWD-SOURCED RECORDINGS\nVinod Subramanian\nCenter for Music Technology\nGeorgia Institute of Technology\nvsubramanian32@gatech.eduAlexander Lerch\nCenter for Music Technology\nGeorgia Institute of Technology\nalexander.lerch@gatech.edu\nABSTRACT\nThe number of audience recordings of concerts on the in-\nternet has exploded with the advent of smartphones. This\npaper proposes a method to organize and align these record-\nings in order to create one or more complete renderings\nof the concert. The process comprises two steps: ﬁrst,\nusing audio ﬁngerprints to represent the recordings, iden-\ntify overlapping segments, and compute an approximate\nalignment using a modiﬁed Dynamic Time Warping (DTW)\nalgorithm and second, applying a cross-correlation around\nthe approximate alignment points in order to improve the\naccuracy of the alignment. The proposed method is com-\npared to two baseline systems using approaches previously\nproposed for similar tasks. One baseline cross-correlates\nthe audio ﬁngerprints directly without DTW. The second\nbaseline replaces the audio ﬁngerprints with pitch chroma\nin the DTW algorithm. A new dataset annotating real-world\ndata obtained from the Live Music Archive is presented and\nused for evaluation of the three systems.\n1. INTRODUCTION\nCrowd-sourcing is the concept of presenting a problem to\na large group of people and utilizing the best combination\nof the solutions received [12]. Although a large group of\npeople can be used to obtain data, the data needs to be\norganized and labeled in a logical way to be useful. For\ninstance, there has been an explosion in the number of\naudio and video recordings available online in the last few\nyears. For large events such as concerts, speeches, and\nsports events, there are many recordings of (parts of) the\nsame event. These recordings, however, are not annotated\nin a way that would allow a reconstruction of the complete\ntimeline of the event. The focus of this research is, therefore,\non the automatic organization and synchronization of the\nmultiple recordings available of the same event.\nMarshall and Shipman [16] analyze the people’s reasons\nfor recording events and report personal memorabilia, shar-\ning on social platforms, creation of remixes, and online\nc\rVinod Subramanian, Alexander Lerch. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Vinod Subramanian, Alexander Lerch. “Concert Stitch:\nOrganization and Synchronization of Crowd-Sourced Recordings”, 19th\nInternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.republishing as the main reasons. This indicates that there\nis value attached to these recordings. Vihavainen et al. [24]\nshowed in their work that a human-computer collaborative\napproach to remixing concerts is of interest to a concert au-\ndience. Although the subjects favored the manually edited\nconcerts in this instance, it still emphasizes the value of\nrecombining audience recordings\nWhile recombining audience recordings creates a better\naudience experience beyond the concert, a tool for auto-\nmatic concert “stitching”, faces several challenges. For\nexample, each recording will have different audio quality\ndue to different recording devices, distance from the stage,\nlocal disturbances etc.\nAfter meeting these challenges, the application of this\nresearch enables (a) improved audience experience through\npersonalized, collaborative, or theme-driven reconstruction\nof the event thus creating a platform for derivative work,\n(b) analysis and improvement of stage setups by venues and\nperformers through audience videos from a large variety of\nrecording angles, and, more generally, (c) audio forensics\nto reconstruct a scene by synchronizing multiple recordings\nfor surveillance and investigation.\nThe goal of this study is to present a method that can\n(a) reliably identify if multiple recordings from an event\nhave common audio content and (b) provide a precise align-\nment between all pairs of recordings. In the hope of en-\ncouraging more research on this task, we also present a new\ndataset for training and evaluation.\n2. RELATED WORK\nThe task of aligning multiple recordings of an event can\nbe divided into two steps: ﬁrst, using a representation the\nrecordings to identify overlapping segments, and compute\nan approximate alignment and second, applying a cross-\ncorrelation around the approximate alignment points in\norder to improve the accuracy of the alignment.\nIn tasks such as speech recognition [6,11] and music sim-\nilarity [1,8], Mel-Frequency Cepstral Coefﬁcients (MFCCs)\nare widely used to measure similarity between audio ﬁles.\nThe Mel-Cepstrum captures timbral information and the\nspectral shape of the audio ﬁle [3]. However, MFCCs do not\ncontain musically meaningful information such as melody\nor rhythm which could be argued to be crucial for comput-\ning music similarity.\nMusic Information Retrieval tasks such as cover song608Audio \nfingerprints Dynamic \nTime \nWarping Feature \nExtraction Candidate \nPaths SVM \nClassifier Audio \nrecording 1 \nBest\npath\nAudio \nfingerprints Audio \nrecording 2 (a) First part of algorithm for approximate alignment\nBest\nPathFine tuned \nAlignment Compute \nFeature Cross \nCorrelation Find Peak \n(b) Second part of algorithm for ﬁne tuning alignment using cross-correlation\nFigure 1 : Proposed method block diagram\ndetection [19], audio thumbnailing [2], and genre classiﬁ-\ncation [23] use pitch-based features such as a pitch chroma\nto compute a measure of similarity. The pitch chroma [15]\nis an octave-invariant representation of the tonal content of\nan audio ﬁle and is usually computed in intervals of approx.\n10 ms. A useful property of the pitch chroma is its robust-\nness to timbre variations, allowing it to compare the pitch\ncontent of two different versions of the same song without\nbeing strongly inﬂuenced by timbre variations.\nDetermining the similarity of two recordings is closely\nrelated to audio ﬁngerprinting, which aims at identifying a\nrecording from a large database of recordings. An audio ﬁn-\ngerprint is a highly compressed and unique representation\nof a (part of a) song [10, 25]. Wang [25] introduced an au-\ndio ﬁngerprinting technique based on so-called landmarks.\nA landmark is identiﬁed as the spatial relationship of the\nsalient spectral peaks. This representation is also used for\nthe task of audio alignment of concert recordings [4, 13].\nMost audio ﬁngerprinting methods are temporally sensitive,\nmeaning that they are not designed to handle variations in\nplayback speed — a scenario that is likely in the case of\nanalog recordings of concerts. The audio ﬁngerprinting\nmethod introduced by Haitsma and Kalker calculates a 32\nbit sub-ﬁngerprint for every block of audio by looking at\nthe energy differences along the frequency and time axes.\nThis ﬁngerprint method is used by Shrestha et al. [21] in\ntheir work on alignment of concert recordings. Alternately,\nWilmering et al. [26] use high-level audio features such\nas tempo and chords in combination with low-level audio\nfeatures such as MFCCs and pitch chroma to detect au-\ndio similarity for audio alignment of different versions of\nconcerts.\nIdentifying and aligning overlapping segments requires\nthe computation of a similarity or distance measure across\na sequence of signal descriptors. One way of doing this\nis cross-correlation. Most of the research in aligning con-\ncert recordings apply this approach [4, 5, 13, 21, 22]. One\nconstraint of cross-correlation is that the two sequences are\nassumed to be at the same speed. It is apparent that cross-\ncorrelation cannot be easily applied to the task of aligninganalog recordings because there may be tempo variations\nand temporal ﬂuctuations in the data. Another issue with\ncross-correlation is that a threshold needs to be set for what\nconstitutes an alignment. To set the threshold some publica-\ntions use heuristic methods based on their data [4, 5, 13, 22],\nwhile others [21] use a threshold determined by Haitsma\nand Kalker [10]. Using ﬁxed thresholds bears the risk of\nerrors when applying the system to unseen data.\nAnother method for computing overlaps is the use of\nDynamic Time Warping (DTW), as it is able to handle tem-\nporal ﬂuctuations between the signals [14]. Wilmering et\nal. apply DTW twice, the ﬁrst time for aligning a recording\nto a reference audio ﬁle in order to identify the different\nplayback speeds. Based on the result, the audio ﬁles are\nprocessed to mirror the playback speed of the reference.\nThe second alignment is then applied to improve the accu-\nracy of the ﬁrst alignment. DTW is also used in the related\ntask of sample detection, where it can help to identify the\nlocation of a sample in a song [9].\n3. ALGORITHM DESCRIPTION\nThe ﬁrst part of the algorithm, as shown in Figure 1a, com-\nputes audio ﬁngerprints for each recording and uses these\nﬁngerprints to compute pairwise distance matrices. For\neach distance matrix, a DTW algorithm determines mul-\ntiple possible path candidates representing the potentially\noverlapping region between that pair of recordings. For\neach of these candidates, features are extracted and an SVM\nclassiﬁer determines which path is the most likely. In the\ncase that the pair is not overlapping, no path should be\nselected from the candidates. The second part of the algo-\nrithm as shown in Figure 1b takes the most likely path and\ncomputes a cross-correlation of the overlapping regions to\ndetermine the exact alignment of the pairs and to improve\naccuracy.\n3.1 Audio Fingerprint Computation\nThe motivation for using audio ﬁngerprints is that it is\na representation of audio robust to noise and timbre [10,Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 609Figure 2 : The top row shows the ﬁngerprints from two\nrecordings of the same 5 second snippet. The second row\nshows the ﬁngerprints from two recordings of different 5\nsecond snippets. For the Bit Error, the black regions indicate\nthe ﬁngerprints match and the white regions indicate the\nﬁngerprints are different.\n25]. The audio ﬁngerprinting technique utilized here is the\nHaitsma and Kalker algorithm [10]. The audio ﬁngerprints\nare computed at a sampling rate of 5 kHz with a block size\n2048 and a hop size of 512.\nFigure 2 visualizes the robustness of audio ﬁngerprints\nto noise distortion with an example. The upper row shows\nthe bit error (in white) between the ﬁngerprints of two\nmatching but distorted recordings, the lower row shows the\nsame for two different recordings. We can clearly see how\nthe ﬁngerprints retain the essential information even in the\ncase of heavy distortion.\n3.2 Modiﬁed Dynamic Time Warping\nDynamic Time Warping (DTW) is designed to align se-\nquences with similar content but are temporally different.\nIn the case of aligning concert recordings, the temporal ﬂuc-\ntuations might occur due to inaccuracies in the sampling\nrate; in the case of analog recordings, the temporal ﬂuctua-\ntions might be caused due to varying playback speeds.\nThe classical DTW algorithm introduced by Sakoe and\nChiba works under the assumption that the start and end\npoints of the two sequences are aligned [20]. A modiﬁcation\nof the standard approach allows the algorithm to detect sub-\nsequences [18]; however, in the case of real life recordings,\nthe most likely scenario is that a pair of recordings might\nhave overlapping regions. Therefore, a pair of recordings\nwill neither have the same start and end points, nor will\none recording necessarily be a subsequence of the other.\nTo address this issue, the subsequence DTW algorithm\nis modiﬁed to look for overlapping regions by doing the\ntraceback from all possible end points.\nThe distance matrix is computed as the pairwise dis-\ntance of two audio ﬁngerprint matrices corresponding to\ntwo recordings. The dimension of one ﬁngerprint matrix is\n32\u0002Mand of the second is 32\u0002NwhereMandNcorre-\nspond to the number of blocks of audio that each recording\nwas divided into. Using the Hamming distance, the result is\nFigure 3 : Distance matrix examples. The dark line indi-\ncates high similarity. For Distance matrix 4, there is no\noverlap, so there is no high similarity region\nFigure 4 : Different candidate path examples. The straight-\nest line in the image represents the correct path.\na distance matrix Dwith the dimensions M\u0002N. Figure 3\nshows examples of the distance matrix for different pairs of\nrecordings; the top left matrix shows a standard DTW case\nwith start and end points of both sequences aligned, the top\nright and bottom left are computed from pairs of record-\nings with overlapping regions and the bottom right matrix\ncorresponds to a pair of recordings without overlapping\nregions.\nA cost matrix is computed from the distance matrix as is\ndone for the subsequence DTW algorithm [18]. In short, the\ninitialization of the cost matrix computation is modiﬁed– as\nopposed to accumulating the distance across both the ﬁrst\nrow and ﬁrst column, only the ﬁrst column is accumulated.\nWe use the standard DTW technique to traceback the\npath; however, instead of doing this on just the minimum\ncost point, the traceback is performed on all possible path\nend points from the last row and last column. This results\nin multiple paths. Figure 4 illustrates a few paths that are\ncomputed for an example cost matrix.\n3.3 Feature Extraction\nTo identify the most likely candidate path, we extract fea-\ntures from each path. Each possible path has three features:610 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018(a) the DTW cost normalized by path length, (b) the slope\nof the line connecting the starting and ending points, and\n(c) the deviation of the path from the line connecting the\nstart and end points. These paths are then clustered such that\neach cluster contains paths that share a start point; the end\npoint for each cluster is the path with the lowest normalized\ncost. From each cluster, the minimum, mean, and standard\ndeviation of the three path features are taken along with the\nnumber of paths in the cluster. These cluster features are\nsimilar to the ones proposed by Gururani and Lerch in the\ncontext of sample detection [9]. The extracted features per\ncluster have a dimensionality of 1\u000210per cluster and are\nthe input of a classiﬁer estimating whether a path candidate\nrepresents a true overlap or not.\n3.4 Classiﬁer\nA binary classiﬁer is trained to determine which of the\ncandidate paths is the most likely path for the alignment.\nA Support Vector Machine algorithm (SVM) with a linear\nkernel is used as this classiﬁer. In the event that the classiﬁer\ndoesn’t identify any of the candidate paths as a path for\nalignment, it is assumed that that pair of recordings do not\nhave overlapping content. In the case of two or more paths\nbeing classiﬁed as true overlapping paths, the classiﬁer’s\noutput probability is used to choose the most probable path.\n3.5 Sample-Accurate Alignment\nThe audio ﬁngerprinting technique used [10] downsamples\nthe audio to 5000 Hz and blocks the audio by 1024 samples\nso the DTW alignment has a low resolution. As a more ac-\ncurate result is desirable to reconstruct the timeline artifact-\nfree (without ’jumps’) when splicing two recordings to-\ngether, a post-processing step is applied. One audio ﬁle\nis resampled based on the approximate alignment; then,\nthe cross-correlation of overlapping regions of the pair of\nrecordings is computed for 5 seconds around the detected\nstart point. The result should then provide a synchronization\npoint with improved accuracy.\n3.6 Baseline\nWe compare the results of the proposed method to two\nbaseline systems– one looking at the audio features and the\nother looking at the alignment stage.\n3.6.1 Pitch chroma baseline\nIn order to investigate the effect of audio descriptors on\nthe alignment accuracy, the pitch chroma is used as the\naudio representation instead of audio ﬁngerprints. For the\npitch chroma, a euclidean distance is used instead of the\nHamming distance for calculating the distance matrix. Pitch\nchroma is a feature of interest as it is a typical feature used\nfor audio similarity [2, 19, 23]. It has also been used in\nprevious work on aligning concert recordings [26]. The\npitch chroma is computed at a sampling rate of 11 kHz with\na block size of 4096 and a hop size of 1024.3.6.2 Cross-correlation baseline\nThe cross-correlation on audio ﬁngerprints is the most estab-\nlished approach in the ﬁeld of aligning noisy concert record-\nings [4, 13, 21]. For this process, the Hamming distance is\ncomputed at different levels of overlap and a threshold of\n0.35 Bit Error Rate (BER) is set according to the recom-\nmendation by Haitsma and Kalker [10]. If the distance falls\nbelow the threshold then the pair of recordings are aligned\nat that overlap.\n4. EXPERIMENTS\nWe run several experiments to investigate our algorithm.\nWe evaluate the audio (feature) representation, approaches\nto alignment, and alignment accuracy.\n4.1 Dataset\nTwo datasets are used in this study– a synthetic dataset\nand a real world dataset. The synthetic dataset created for\nsimulating a real world scenario; the advantage is a sample-\naccurate ground truth. The synthetic dataset will be used as\nthe training and validation set, as well as to provide some\npreliminary results with high accuracy. The real-world\ndataset is manually annotated from existing recordings and\nis used to test the overall performance of the algorithm.\n4.1.1 Synthetic Dataset\nThe synthetic dataset is a collection of audio recordings\ndownloaded from YouTube1consisting of live recordings\nof concerts. There are a 100 songs available in this dataset.\nIn order to create training data for the classiﬁer, each\nsong of the dataset is divided into 17 (can be varied) record-\nings with the constraint that each recording is longer than\n20 s and the entire song is covered. Each recording is\nmodiﬁed by (a) resampling randomly between 42.9 kHz\nto 45.2 kHz, (b) either low pass ﬁltering with a cutoff be-\ntween 5000 Hz to 11600 Hz or high pass ﬁltering with a\ncutoff between 200 Hz to 5000 Hz, (c) adding crowd sounds\nobtained from freesound.org [7], and (d) adding distortion\nusing the ’live recording’ and ’smart phone recording’ sim-\nulations in the audio degradation toolbox [17]. The code\nfor generating the synthetic dataset is available online2.\n4.1.2 Real World Dataset\nThe real world dataset consists of 5 audience recordings of a\nGrateful Dead concert performed on 1977-05-08. The audio\ndata was obtained from the Live Music Archive3. The ﬁrst\n5 songs from the concert were selected and each of the 5\nversions of the 5 songs were annotated. The annotations\nindicate the start and end points of the song. In case a\npart of the song is missing, the duration and location of the\nmissing location is indicated. Since these recordings were\nmade on analog devices, the data is prone to tempo and\nplayback speed variation in addition to the usual ﬁltering\nand distortion heard in audience recordings. The real world\n1https://www.youtube.com/ accessed March 1st 2018\n2https://github.com/VinodS7/ConcertStitch-datasetProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 611precision recall f-measure\nFingerprints 0.9697 0.6732 0.8145\nPitch Chroma 0.6753 0.3191 0.4335\nTable 1 : Experiment 1: Overlap detection for audio ﬁnger-\nprints vs. pitch chroma on real world data\ndataset is augmented by splitting each version of a song into\n10 recordings, resulting in 50 simulated audience recordings\nper song. The songs are split in the same way as for the\nsynthetic dataset.\n4.2 Metrics\nThere are two metrics used for the evaluation of this task.\nThe ﬁrst metric is using the precision, recall, and f-measure\nto provide an understanding of whether an alignment is cor-\nrectly detected for a pair of recordings. The second metric\nis the statistical analysis of the alignment accuracy in sec-\nonds where the median, standard deviation, and maximum\nvalues are used to measure how accurate the alignment is.\n4.3 Experiment 1: Audio ﬁngerprints vs. pitch chroma\nThe aim of this experiment is to compare the audio repre-\nsentation on which the distance computation is based. We\ninvestigate audio ﬁngerprints and pitch chroma for the task\nof aligning noisy recordings.\nTo train the SVM classiﬁer for the algorithm, the above-\nmentioned cluster features are extracted from the synthetic\ndataset for 25 songs. To extract the features for each pair of\nrecordings, the DTW algorithm computes multiple possible\npaths corresponding to all unique starting points. All paths\nare labeled incorrect except the path that is closest to the\nground truth in the case of overlapping recordings. The\nextracted feature matrix thus consists of the cluster features\nalong with a label of whether those features correspond to\nan overlap or not. This process is applied to both audio\nﬁngerprints and pitch chromas.\nOnce the feature matrix is available, it is divided into\nan 80-20 split for training and validation, respectively. As\neach pair of recordings has multiple candidate paths with\na maximum of only one being correct, there are far more\nnegative observations in the feature matrix than positive\nobservations. To counteract the high number of negative\nobservations, the training data is sampled to reduce the\nnumber of negative observations. The ratio of negative\nto positive observations is 50:1 for the audio ﬁngerprints\nclassiﬁer and 30:1 for the pitch chroma classiﬁer.\nThe evaluation is performed on the real world dataset.\nOnly the start points of the alignment are taken into account\nbecause the audio ﬁles are not modiﬁed or resampled based\non the end points.\n4.3.1 Results\nTable 1 reports the precision, recall, and f-measure of the\naudio ﬁngerprints and the pitch chroma. The ﬁngerprint out-\nperforms the pitch chroma considerably for all metrics. This\n3https://archive.org/details/GratefulDead accessed January 15th 2018Real World precision recall f-measure\nDTW 0.9697 0.6732 0.8145\ncross-correlation 0.4132 0.2534 0.3141\nSynthetic precision recall f-measure\nDTW 0.9570 0.9319 0.9443\ncross-correlation 0.6936 0.8956 0.7818\nTable 2 : Experiment 2: DTW vs. cross-correlation using\naudio ﬁngerprints for real world and synthetic data\nresult is expected as the ﬁngerprint is speciﬁcally designed\nto work in conditions with severe quality impairments. The\npoor performance of the pitch chroma can be traced back to\ncomputing the candidate paths in the DTW algorithm. Due\nto the noise, the candidate paths frequently do not contain\nthe correct path for the pitch chroma. This adversely affects\nthe training process for the SVM classiﬁer and subsequently\nthe performance on the real world data.\n4.4 Experiment 2: DTW vs. cross-correlation\nThe aim of this experiment is to compare the performance of\nthe DTW and the cross-correlation techniques when audio\nﬁngerprints are used as the audio representation. The audio\nﬁngerprints for the cross-correlation method are almost the\nsame as for the DTW algorithm, the only difference is that\nthe hop size is now 64 instead of 512.\nThe classiﬁer for the DTW algorithm is set up the same\nway as in Experiment 1. The evaluation is performed on\nboth the synthetic dataset with no temporal ﬂuctuations and\nthe real world dataset.\n4.4.1 Results\nTable 2 reports the precision, recall, and f-measure of the\nDTW method and the cross-correlation method on the two\ndatasets. We observe that the DTW method clearly outper-\nforms the cross-correlation method. This is especially true\nfor the real-world data because the DTW is designed to han-\ndle temporal ﬂuctuations while cross-correlation is not. On\nthe synthetic dataset containing no temporal ﬂuctuations,\nthe cross-correlation method performs much better; how-\never, it still does not perform as well as the DTW method.\nOne possible reason might be that the cross-correlation\nmethod uses a strict threshold to identify alignment so the\ncross-correlation method does not scale well to different\ntypes of noise.\n4.5 Experiment 3: DTW performance analysis\nThe goal of this experiment is to understand the strengths\nand weaknesses of the proposed algorithm.\nFor the ﬁrst part of the experiment, the precision, recall,\nand f-measure are reported for difference tolerance thresh-\nolds on the real world dataset. The tolerance threshold gives\nmaximum allowable deviation of the alignment provided\nby the algorithm from the ground truth. If the alignment\nexceeds the threshold then it means the algorithm predicted\nthe alignment incorrectly.612 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018precision\nrecall\nf-measureFigure 5 : Experiment 3: Analyzing the performance of the\nproposed method\nThe second part of the experiment tests how robust to\ntime stretching and pitch shifting the algorithm is. First,\nthe sample rates for each of the recordings are identiﬁed\nusing the same technique as Wilmering et al. [26]. Then,\nthe alignment is calculated between each pair of recordings.\nFinally, the f-measure of the alignment is compared to the\nratio of sample rates( or version lengths).\n4.5.1 Results\nThe ﬁrst part of Figure 5 shows the precision, recall, and\nf-measure at different tolerance thresholds. The plot shows\nthat the performance decreases drastically for tolerances be-\nlow 2 s. These results indicate a need to reﬁne the alignment\nin order to provide a more accurate measure of alignment.\nFor the second part of Figure 5, we expect the algorithm\nto perform better if the ratio of lengths is closer to 1 and\nthe performance to get worse the further away from 1. The\nreason is that if the ratio of sample rates is further away\nfrom one the pitch shifting becomes more signiﬁcant which\nthis algorithm is not designed to handle. However, the plot\ndoes not reﬂect this hypothesis because the pitch shifts in\ncertain audio ﬁles is greater than expected. In addition to\nresampling there is more pitch shifting which causes the al-\ngorithm to fail since both the pitch chroma and ﬁngerprints\nare sensitive to pitch shifting.\n4.6 Experiment 4: Analysis of improved alignment\naccuracy\nFor a pair of recordings using the alignment, a resam-\npling factor is calculated using a ratio of the length of\nthe two paths. One recording is resampled so it has the\nsame length as the other. We investigate and compare the\nspectral ﬂux, spectral centroid, and time-domain raw audio\nfor their ability to improve the alignment accuracy when\ncross-correlating a small segment around the previously es-\ntimated alignment points. For reference, the same features\nare computed without resampling the audio. The spectral\nﬂux and spectral centroid are calculated at a block size of\n128 with a hop size of 32. The alignment accuracy for the\nraw audio, spectral ﬂux, and spectral centroid for the origi-\nnal and resampled audio are compared against the originalmedian std max\nDTW alignment 5240 11503 125221\nRaw audio 9043 49091 823906\nSpectral Flux 7073 12554 108950\nSpectral Centroid 7161 9919 54695\nRes. Raw Audio 5801 23185 227631\nRes. Spec. Flux 5078 13092 125846\nRes. Spec. Centroid 5006 11128 100165\nTable 3 : Raw Audio vs Spectral Flux to improve alignment\naccuracy. The results are reported as deviation in samples\nat 44.1 kHz\nDTW algorithm to evaluate the accuracy improvement. The\nevaluation for this task is done on the synthetic dataset be-\ncause the annotations are more accurate than for the real\nworld data.\n4.6.1 Results\nThe results of Experiment 4 are reported in Table 3. The\nnumbers indicate how close to the ground truth alignment\nthe algorithm performs in samples at a sample rate of\n44100 Hz None of the ﬁner alignment algorithms are able to\nsigniﬁcantly improve the alignment of the algorithm. How-\never, it is important to note that by using the approximate\nalignment to resample the audio ﬁles, the results are much\nbetter than without resampling. One explanation for the\nlimited improvement in performance is that the spectral\ncentroid and spectral ﬂux might not be too susceptible to\nnoise.\n5. CONCLUSION\nThis paper presented a method for accurately aligning\nrecordings of a concert event given that these recordings are\nnoisy snippets. The results show that audio ﬁngerprints are\nbetter suited than pitch chroma for the task of representing\nnoisy audio and that dynamic time warping performs bet-\nter than cross-correlation for the alignment. Using a ﬁner\nalignment on the resampled audio shows promise; however,\nthe results are still unsatisfactory. The real world data has\nbeen made publicly available, and the used modiﬁcations\nof the data is published online4.\nThe biggest drawback of the algorithm is its inability to\nhandle pitch shifts in audio recordings very well– a known\nissue with many ﬁngerprinting systems. If the current audio\nﬁngerprinting algorithm is replaced with an algorithm that\nis robust to noise as well as to pitch shifts, we expect the\nperformance of the system would improve considerably on\nour real world dataset.\nFuture work on this task will focus on the actual rendi-\ntion of the complete event once the alignment is known and\npossibly combine audio with video. Selecting the segments,\ndetermining fade points, durations, and type in the overlap-\nping regions, are all interesting and challenging tasks that\nhave not been researched in depth yet.\n4https://github.com/VinodS7/ConcertStitch-datasetProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 6136. REFERENCES\n[1]Jean-Julien Aucouturier, Francois Pachet, et al. Music\nsimilarity measures: What’s the use? In Proc. of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 13–17, 2002.\n[2]M. A. Bartsch and G. H. Wakeﬁeld. Audio thumbnailing\nof popular music using chroma-based representations.\nIEEE Transactions on Multimedia , 7(1):96–104, 2005.\n[3]A. Berenzweig, B. Logan, D. P. W. Ellis, and B. Whit-\nman. A large-scale evaluation of acoustic and subjective\nmusic-similarity measures. Computer Music Journal ,\n28(2):63–76, 2004.\n[4]N. J. Bryan, P. Smaragdis, and G. J. Mysore. Clustering\nand synchronizing multi-camera video via landmark\ncross-correlation. In IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\npages 2389–2392, 2012.\n[5]C. V . Cotton and D. P. W. Ellis. Audio ﬁngerprinting to\nidentify multiple videos of an event. In 2010 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing , pages 2386–2389, March 2010.\n[6]Steven B Davis and Paul Mermelstein. Comparison\nof parametric representations for monosyllabic word\nrecognition in continuously spoken sentences. In Read-\nings in speech recognition , pages 65–74. Elsevier, 1990.\n[7]Frederic Font, Gerard Roma, and Xavier Serra.\nFreesound technical demo. In ACM International\nConference on Multimedia (MM’13) , pages 411–412,\nBarcelona, Spain, 21/10/2013 2013.\n[8]Jonathan T. Foote. Content-based retrieval of music and\naudio, 1997.\n[9]Siddharth Gururani and Alexander Lerch. Automatic\nSample Detection in Polyphonic Music. In Proceed-\nings of the International Society for Music Information\nRetrieval Conference (ISMIR) , Suzhou, 2017.\n[10] Jaap Haitsma and Ton Kalker. A highly robust audio ﬁn-\ngerprinting system. In Proc. of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 107–115, 2002.\n[11] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. r. Mohamed,\nN. Jaitly, A. Senior, V . Vanhoucke, P. Nguyen, T. N.\nSainath, and B. Kingsbury. Deep neural networks for\nacoustic modeling in speech recognition: The shared\nviews of four research groups. IEEE Signal Processing\nMagazine , 29(6):82–97, Nov 2012.\n[12] Jeff Howe. The rise of crowdsourcing. Wired magazine ,\n14:1–4, 2006.\n[13] Lyndon Kennedy and Mor Naaman. Less talk,\nmore rock: Automated organization of community-\ncontributed collections of concert videos. In Proc. of\nthe 18th International Conference on World Wide Web ,\npages 311–320, New York, 2009.[14] Holger Kirchhoff and Alexander Lerch. Evaluation of\nFeatures for Audio-to-Audio Alignment. Journal of\nNew Music Research , 40(1):27–41, 2011.\n[15] Alexander Lerch. An Introduction to Audio Content\nAnalysis: Applications in Signal Processing and Music\nInformatics . Wiley-IEEE Press, Hoboken, 2012.\n[16] Catherine C. Marshall and Frank M. Shipman. Saving,\nreusing, and remixing web video: Using attitudes and\npractices to reveal social norms. In Proceedings of the\n22Nd International Conference on World Wide Web ,\nWWW ’13, pages 885–896, New York, NY , USA, 2013.\n[17] Matthias Mauch, Sebastian Ewert, et al. The audio\ndegradation toolbox and its application to robustness\nevaluation. 2013.\n[18] Meinard M ¨uller. Information Retrieval for Music and\nMotion . Springer-Verlag New York, Inc., Secaucus, NJ,\nUSA, 2007.\n[19] S. Ravuri and D. P. W. Ellis. Cover song detection: From\nhigh scores to general classiﬁcation. In 2010 IEEE In-\nternational Conference on Acoustics, Speech and Signal\nProcessing , pages 65–68, March 2010.\n[20] H. Sakoe and S. Chiba. Dynamic programming algo-\nrithm optimization for spoken word recognition. IEEE\nTransactions on Acoustics, Speech, and Signal Process-\ning, 26(1):43–49, Feb 1978.\n[21] Prarthana Shrestha, Peter H.N. de With, Hans Weda,\nMauro Barbieri, and Emile H.L. Aarts. Automatic\nmashup generation from multiple-camera concert\nrecordings. In Proc. of the 18th ACM International\nConference on Multimedia , pages 541–550, New York,\n2010.\n[22] Joren Six and Marc Leman. Synchronizing multimodal\nrecordings using audio-to-audio alignment. Journal on\nMultimodal User Interfaces , 9(3):223–229, Sep 2015.\n[23] George Tzanetakis, Andrey Ermolinskyi, and Perry\nCook. Pitch histograms in audio and symbolic music\ninformation retrieval. Journal of New Music Research ,\n32(2):143–152, 2003.\n[24] Sami Vihavainen, Sujeet Mate, Lassi Sepp ¨al¨a,\nFrancesco Cricri, and Igor D.D. Curcio. We want more:\nHuman-computer collaboration in mobile social video\nremixing of music concerts. In Proc. of the SIGCHI\nConference on Human Factors in Computing Systems ,\npages 287–296, New York, 2011.\n[25] Avery Wang. An industrial strength audio search algo-\nrithm. In Proc. of the International Society for Music In-\nformation Retrieval Conference (ISMIR) , volume 2003,\npages 7–13. Washington, D.C., 2003.\n[26] Thomas Wilmering, Florian Thalmann, and Mark B.\nSandler. Grateful live: Mixing multiple recordings of\na dead performance into an immersive experience. In\nAudio Engineering Society Convention 141 , 2016.614 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Instrudive: A Music Visualization System Based on Automatically Recognized Instrumentation.",
        "author": [
            "Takumi Takahashi",
            "Satoru Fukayama",
            "Masataka Goto"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492477",
        "url": "https://doi.org/10.5281/zenodo.1492477",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/63_Paper.pdf",
        "abstract": "A music visualization system called Instrudive is presented that enables users to interactively browse and listen to musical pieces by focusing on instrumentation. Instrumentation is a key factor in determining musical sound characteristics. For example, a musical piece performed with vocals, electric guitar, electric bass, and drums can generally be associated with pop/rock music but not with classical or electronic. Therefore, visualizing instrumentation can help listeners browse music more efficiently. Instrudive visualizes musical pieces by illustrating instrumentation with multi-colored pie charts and displays them on a map in accordance with the similarity in instrumentation. Users can utilize three functions. First, they can browse musical pieces on a map by referring to the visualized instrumentation. Second, they can interactively edit a playlist that showing the items to be played later. Finally, they can discern the temporal changes in instrumentation and skip to a preferable part of a piece with a multi-colored graph. The instruments are identified using a deep convolutional neural network that has four convolutional layers with different filter shapes. Evaluation of the proposed model against conventional and state-of-the-art methods showed that it has the best performance.",
        "zenodo_id": 1492477,
        "dblp_key": "conf/ismir/TakahashiFG18",
        "keywords": [
            "music visualization",
            "interactive browsing",
            "instrumentation",
            "sound characteristics",
            "pop/rock music",
            "instrumentation visualization",
            "multi-colored pie charts",
            "map",
            "playlist editing",
            "temporal changes"
        ],
        "content": "INSTRUDIVE: A MUSIC VISUALIZATION SYSTEM BASED ON\nAUTOMATICALLY RECOGNIZED INSTRUMENTATION\nTakumi Takahashi1;2Satoru Fukayama2Masataka Goto2\n1University of Tsukuba, Japan\n2National Institute of Advanced Industrial Science and Technology (AIST), Japan\ns1720822@s.tsukuba.ac.jp, fs.fukayama, m.goto g@aist.go.jp\nABSTRACT\nA music visualization system called Instrudive is presented\nthat enables users to interactively browse and listen to mu-\nsical pieces by focusing on instrumentation. Instrumenta-\ntion is a key factor in determining musical sound charac-\nteristics. For example, a musical piece performed with vo-\ncals, electric guitar, electric bass, and drums can generally\nbe associated with pop/rock music but not with classical or\nelectronic . Therefore, visualizing instrumentation can help\nlisteners browse music more efﬁciently. Instrudive visu-\nalizes musical pieces by illustrating instrumentation with\nmulti-colored pie charts and displays them on a map in\naccordance with the similarity in instrumentation. Users\ncan utilize three functions. First, they can browse musical\npieces on a map by referring to the visualized instrumen-\ntation. Second, they can interactively edit a playlist that\nshowing the items to be played later. Finally, they can dis-\ncern the temporal changes in instrumentation and skip to a\npreferable part of a piece with a multi-colored graph. The\ninstruments are identiﬁed using a deep convolutional neu-\nral network that has four convolutional layers with differ-\nent ﬁlter shapes. Evaluation of the proposed model against\nconventional and state-of-the-art methods showed that it\nhas the best performance.\n1 INTRODUCTION\nSince multiple musical instruments having different tim-\nbres are generally used in musical pieces, instrumentation\n(combination or selection of musical instruments) is a key\nfactor in determining musical sound characteristics. For\nexample, a song consisting of vocals, electric guitar, elec-\ntric bass, and drums may sound like pop/rock ormetal but\nnotclassical orelectronic . Consider, for example, a lis-\ntener who appreciates gypsy jazz (featuring violin, acoustic\nguitar, clarinet, and double bass). How can he/she discover\nsimilar-sounding music? Searching by instrumentation can\nreveal musical pieces played with the same, slightly differ-\nc⃝Takumi Takahashi, Satoru Fukayama, Masataka Goto.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Takumi Takahashi, Satoru Fukayama,\nMasataka Goto. “Instrudive: A Music Visualization System Based on\nAutomatically Recognized Instrumentation”, 19th International Society\nfor Music Information Retrieval Conference, Paris, France, 2018.\nFigure 1 : Overview of Instrudive music visualization system.\nent, or completely different instrumentation, correspond-\ning to his/her preferences.\nInstrumentation is strongly connected with musical\nsound and genres but is not restricted to a speciﬁc genre.\nFor example, pop/rock ,funk, and fusion are sometimes\nplayed with similar instrumentation. Therefore, it can be\nhelpful for listeners to overcome the conﬁnements of a\ngenre by focusing on sound characteristics when search-\ning for similar-sounding music.\nTo let users ﬁnd musical pieces that they prefer, various\nmethods and interfaces for retrieving and recommending\nmusic have been proposed. They are generally categorized\ninto three approaches: bibliographic retrieval based on the\nmetadata of musical pieces, such as artist, album, year of\nrelease, genres, and tags [2], music recommendation based\non collaborative ﬁltering using playlogs [5, 38], and music\nrecommendation/retrieval based on content-based ﬁltering\nusing music analysis, such as genre classiﬁcation [14, 30]\nand auto-tagging [4, 14, 20]. Music interfaces leveraging\nautomatic instrument recognition [22] have received less\nattention from researchers.\nWe have developed a music visualization system called\nInstrudive that automatically recognizes the instruments\nused in each musical piece of a music collection, visualizes\nthe instrumentations of the collection, and enables users to\nbrowse for music that they prefer by using the visualized\ninstrumentation as a guide (Figure 1). Instrudive visual-\nizes each musical piece as a pie-chart icon representing the\nduration ratio of each instrument that appears. This en-\nables a user to see which instruments are used and their\nrelative amount of usage before listening. The icons of561Figure 2 :Instrudive interface consists of four parts.\nall musical pieces in a collection are arranged in a two-\ndimensional space with similar-instrumentation pieces po-\nsitioned in close proximity. This helps the user listen to\npieces having similar instrumentation. Furthermore, the\nuser can create a playlist by entering a pie-chart query to\nretrieve pieces having instrumentation similar to the query\nand listen to a musical piece while looking at a timeline\ninterface representing when each instrument appears in the\npiece.\nIn the following section, we describe previous studies\non music visualization and instrument recognition. We\nthen introduce the usage and functions of Instrudive in Sec-\ntion 3 and explain its implementation in Section 4. Since\nthe main contributions of this work are not only the In-\nstrudive interface but also a method for automatically rec-\nognizing instruments on the basis of a deep convolutional\nneural network (CNN), we explain the recognition method\nand experimental results in Section 5. After discussing the\nusefulness of the system in Section 6, we summerize the\nkey points and describe future work in Section 7.\n2 RELATED WORK\n2.1 Music Visualization\nVisualization of music by using audio signal processing\nhas been studied by many researchers.\nGiven a large collection of musical pieces, a commonly\nused approach is to visualize those pieces to make it easy\nto gain an overview of the collection [11, 13, 23, 24, 31, 32,\n37, 40]. The collection is usually visualized so that simi-\nlar pieces are closely arranged [13, 23, 24, 31, 32, 37]. The\nvisualization helps listeners to ﬁnd and listen to musical\npieces they may prefer by browsing the collection. Instru-\nmentation is not focused on in this approach, whereas In-\nstrudive visualizes the instrumentations of the pieces in the\ncollection by displaying pie-chart icons for the pieces in a\ntwo-dimensional space as shown in Figure 2.\nGiven a musical piece, a commonly used approach is to\nvisualize the content of the piece by analyzing the musi-\ncal elements [3, 9, 10, 12, 18, 29]. For example, a repetitive\nmusic structure is often visualized [3,9,10,12,29]. This en-\nhances the listening experience by making listeners aware\nof the visualized musical elements. Our Instrudive inter-\nface also takes this approach. After a user selects a musical\nFigure 3 : Multi-colored pie charts depict instrumentation.\npiece, Instrudive displays a timeline interface representing\nwhen each musical instrument appears in the piece. This\nhelps the listener focus on the instrumentation while listen-\ning to music.\n2.2 Instrument Recognition\nThe difﬁculty in recognizing instruments depends on the\nnumber of instruments used in the piece. The greater the\nnumber of instruments, the greater the difﬁculty. When a\nsingle instrument is used in a monophonic recording, many\nmethods achieve good performance [6, 8, 19, 41, 42].\nOn the other hand, when many instruments are used in\na polyphonic recording, which is typical in popular music\nproduced using multitrack recording, it is more difﬁcult to\nrecognize the instruments. Most previous studies [7, 15,\n22,26] used machine learning techniques to overcome this\ndifﬁculty. In Section 5, we compare our proposed model of\ninstrument recognition with one that uses a support vector\nmachine (SVM).\nA more recent approach to recognizing instruments is\nto use a deep learning method, especially a CNN [16, 27,\n28, 34]. Methods using this approach have outperformed\nconventional and other state-of-the-art methods, but their\nperformances cannot be easily compared due to the use\nof different databases and instrument labels. Despite their\nhigh performance, there is room for improvement in their\naccuracy. We aim to improve accuracy by proposing and\nimplementing an improved CNN-based method.\n3 INSTRUDIVE\nInstrudive enables users to browse musical pieces by fo-\ncusing on instrumentation. The key idea of visualizing the\ninstrumentation is to use a multi-colored pie chart in which\ndifferent colors denote the different instruments used in a\nmusical piece. The ratios of the colors indicate relative\ndurations in which the corresponding instruments appear.\nFigure 3 shows example charts created using ground truth\nannotations from the multitrack MedleyDB dataset [1].\nThe charts representing different genres have different ap-\npearances due to the differences in instrumentation among\ngenres.\nThese multi-colored pie charts help a user browsing a\ncollection of musical pieces to understand the instrumen-\ntations before listening to the pieces. Moreover, during\nthe playing of a musical piece, Instrudive displays a multi-\ncolored graph that indicates the temporal changes in instru-\nmentation.\nInstrudive can recognize 11 categories of instruments:\nacoustic guitar, clean electric guitar, distorted electric gui-562 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 4 : Menu appears\nafter right-clicking chart.\nFigure 5 : Scattering mode en-\nables playlist to be created by\ndrawing curve.\nFigure 6 : Visual player helps listener understanding instrumen-\ntation and its temporal changes.\ntar, drums, electric bass, fx/processed sound (sound with\neffects), piano, synthesizer, violin, voice, and other (instru-\nments not included in the 10 categories). The categories\ndepend on this dataset and are deﬁned on the basis of [27].\nAs shown in Figure 2, the interface of Instrudive con-\nsists of four parts: an instrumentation map for browsing\nmusical pieces, a visual player for enhancing the listening\nexperience, a search function for ﬁnding musical pieces\nby using the pie-chart icons as queries, and an interactive\nplaylist for controlling the order of play.\n3.1 Instrumentation Map\nThe instrumentation map visualizes the musical pieces in\na collection. Each piece is represented by a multi-colored\npie chart. Similar pie charts are closely located in a two-\ndimensional space. As shown in Figure 9, this map sup-\nports visualization modes, circular andscattering .\nWhen a user right-clicks on a pie chart, a menu appears\nas shown in Figure 4. The user can play the piece or use\nthe piece as a query for the search function. By using the\ncircular mode, which arranges the pie charts in a circu-\nlar path, the user can automatically play the pieces with\nsimilar instrumentation one after another along the path.\nBy switching to the scattering mode, the user can draw a\ncurve to create a playlist consisting of pieces on the curve\nas shown in Figure 5.\n3.2 Visual Player\nThe visual player (Figure 6) visualizes the temporal\nchanges in instrumentation in the selected musical piece\nas it is played. It shows a graph along the timeline inter-\nface consisting of a number of colored rectangular tiles,\neach of which denotes activity (i.e., presence) of the corre-\nsponding instrument. As the musical piece is played, this\nactivity graph (covering a 60-s window) is automatically\nscrolled to continue showing the current play position.\nFigure 7 : Interfaces for search\nmenu and playlist.\nFigure 8 : Simpliﬁed in-\nterface for novice users.\nThe user can interactively change the play position by\nleft-clicking on another position on the graph. The graph\nenables the user to anticipate how the instrumentation will\nchange. For example, a signiﬁcant change in instrumenta-\ntion can be anticipated, as shown in Figure 6\nThe pie chart on the right side of Figure 6 represents\nthe instruments currently being played and changes in syn-\nchronization with the playing of the piece. The instrument\nicons shown below the chart are consistently shown in the\nsame color, enabling the user to easily distinguish them.\nBy hovering the mouse over an icon, the user can see the\nname of the instrument.\n3.3 Search Function\nThe search function (left side of Figure 7) enables the\nuser to retrieve pieces by entering a query. Pressing an\ninstrument-icon button intensiﬁes its color, so the selected\nbutton is clearly evident. The ratio of instruments in the\nquery can be adjusted by moving the sliders.\nWhen the search button is pressed, the system retrieves\nmusical pieces with instrumentation similar to that of the\nquery by using the search algorithm described in Section\n4.3. The retrieved pieces are not only highlighted on the\nmap as shown in Figure 10 but also instantly added to the\nplaylist.\n3.4 Interactive Playlist\nThe interactive playlist (right side of Figure 7) shows a list\nof the retrieved or selected musical pieces along with their\npie charts, titles, and artist names. The user can change\ntheir order, add or delete a piece, and play a piece.\nA musical piece disappears from the playlist after it has\nbeen played. If no piece is in the list, the next piece is se-\nlected automatically. In circular mode, the available play\nstrategies are clockwise (pieces are played in clockwise or-\nder), and shufﬂe (pieces are played randomly). In scat-\ntering mode, the available play strategies are shufﬂe and\nnearest (pieces nearby are played). The user can thus play\npieces having similar or different instrumentation.\n3.5 Simpliﬁed Interface\nWe also prepared a simpliﬁed interface for novice users\nwho are not familiar with music instrumentation. As\nshown in Figure 8, the visual player, the search function,Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 563Figure 9 : Two algorithms are used to create maps. Map on left is\nused in circular mode; map on right is used in scattering mode.\nand the interactive playlist can be folded to the side to let\nthe user concentrate on simple interaction using the instru-\nmentation map.\n4 IMPLEMENTATION OF INSTRUDIVE\nThe Instrudive interfaces were mainly programmed using\na Python library Tkinter and executed on Mac OS X. After\nthe instruments were recognized, as described in Section\n5, the results were stored and used for the interfaces.\n4.1 Iconic Representation\nA multi-colored pie chart of a musical piece with length T\ns is displayed by computing the absolute appearance ratio\n(AAR) and the relative appearance ratio (RAR) for each\ninstrument i(2I: recognized instrument categories ).\nThe result of recognizing an instrument iis converted\nintoAAR i:\nAAR i=ti\nT; (1)\nwhere ti(\u0014T) s is the total of all durations in which in-\nstrument iis played. AAR represents the ratio of this total\ntime against the length of the musical piece.\nRAR i=AAR i∑\niAAR i(2)\nrepresents the ratio of this total time against the total time\nof the appearances of all instruments. After RAR iis com-\nputed for all instruments, an jIj-dimensional vector (11-\ndimensional vector in the current implementation) summa-\nrizing the instrumentation of the piece is obtained. The pie\nchart is a visual representation of this vector: RAR iis used\nas an area ratio in the circle for the corresponding instru-\nment.\n4.2 Mapping Algorithms\nTo visualize musical pieces in circular mode (Figure 9), we\nuse an jIj-dimensional vector (11-dimensional vector in\nthe current implementation) of AAR. The AAR vectors for\nall the pieces are arranged on a circular path obtained by\nsolving the traveling salesman problem (TSP) [25] to ﬁnd\nthe shortest route for visiting all pieces. After assigning all\nthe pieces on the path, we scatter them randomly towards\nand away from the center of the circle so that the pie charts\nare not located too close together.\nFigure 10 : Top ten search results are highlighted and added to\nplaylist. Users can check contents of results before listening.\nLayer Output size\nMagnitude spectrogram 1024\u000287\u00021\nConv (4\u00021) 1024\u000287\u000232\nPool (5\u00023) 204\u000229\u000232\nConv (16\u00021) 204\u000229\u000264\nPool (4\u00023) 51\u00029\u000264\nConv (1\u00024) 51\u00029\u000264\nPool (3\u00023) 17\u00023\u000264\nConv (1\u000216) 17\u00023\u0002128\nPool (2\u00022) 8\u00021\u0002128\nDropout (0:5) 1024\nDense 1024\nDense 121\nDense 11\nTable 1 : Proposed CNN architecture.\nTo visualize musical pieces in scattering mode, the\n11-dimensional AAR vectors are projected onto a two-\ndimensional space by using t-distributed stochastic neigh-\nbor embedding (t-SNE) [39], which is an algorithm for di-\nmensionality reduction frequently used to visualize high-\ndimensional data. Since similar pie charts are often located\ntoo close together, we slightly adjust their positions one by\none by randomly moving them until all the charts have a\ncertain distance from each other.\n4.3 Search Algorithms\nSince both a query and a musical piece can be represented\nas 11-dimensional AAR vectors, we can simply compute\nthe cosine similarity between the query and each musical\npiece in the collection. In Figure 10, for example, given a\nquery containing acoustic guitar, violin, and others, the re-\ntrieved pieces ranked higher have similar pie charts. As the\nrank gets lower, the charts gradually becomes less similar.\n5 INSTRUMENT RECOGNITION\n5.1 Pre-processing\nEach musical piece was converted into a monaural audio\nsignal with a sampling rate of 44100 Hz and then divided\ninto one-second fragments. To obtain a one-second magni-\ntude spectrogram, we applied short-time Fourier transform\n(STFT) with a window length of 2048 and a hop size of\n512. We then standardized each spectrogram to have zero\nmean and unit variance. As a result, each one-second spec-\ntrogram had 1024 frequency bins and 87 time frames.564 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20185.2 CNN Architecture\nWe compared several CNN models; the one that showed\nthe best performance is summerized in Table 1. The model\nmainly consists of four convolutional layers with max-\npooling and ReLU activation. A spectrogram represents\nthe structure of frequencies with one axis and its tempo-\nral changes against the other axis, which is unlike an im-\nage that represents spatial information with both axes. We\nset the shape of each layer to have length along only one\naxis (frequency or time). For convolutions, feature maps\nwere padded with zeros so that dimensionality reduction\nwas done only by using max-pooling layers. By doing this,\nwe could use various shapes of layers and their combina-\ntions without modifying the shapes of other layers. After a\n50% dropout was applied to prevent overﬁtting, two dense\nlayers with ReLU and an output dense layer with a sig-\nmoid function were used to output an 11-dimensional vec-\ntor. Batch normalization [17] was applied to each of the\nconvolutional and dense layers. In training, we used the\nAdam algorithm [21] as the optimizer and binary cross-\nentropy as the loss function. The mini-batch size was 128,\nand the number of epochs was 1000.\nThis proposed CNN model outputs 1-s instrument la-\nbels as a vector. By gathering the vectors corresponding to\neach musical piece, we can represent each musical piece\nas a sequence of 11-dimensional vectors (instrument la-\nbels/activations), which are used to calculate the instru-\nmentation described in Section 4.\n5.3 Dataset\nTo evaluate the proposed CNN model and apply it to In-\nstrudive , we used the MedleyDB dataset [1]. This dataset\nhas 122 multitrack recordings of various genres and instru-\nment activations representing the sound energy for each\nstem (a group of audio sources mixed together), individu-\nally calculated along with time frames with a hop size of\n46.4 ms.\nWe generated instrument labels and split the data on\nthe basis of the source code published online [27]. We\nused the 11 categories listed in Section 3 based on the\nground truth annotations from the multitrack MedleyDB\ndataset [1]. Since our system does not depend on these\ncategories, it can be generalized to any set of categories\ngiven any dataset.\nThe 122 musical pieces were divided into ﬁve groups\nby using the algorithm in [35] so that the instrument labels\nwere evenly distributed among the ﬁve groups. Four of the\ngroups were used for training, and the ﬁfth was used for\nevaluation. All the musical pieces that appear in Instrudive\nwere included in the data used for evaluation, and their\ninstrumentations were predicted using cross validation.\n5.4 Baseline\nFor comparison with our model, we used a conventional\nbag-of-features method, a state-of-the-art deep learning\nmethod with mel-spectrogram input, and a state-of-the-art\ndeep learning method with raw wave input.Layer Output size\nMel-spectrogram 128\u000243\u00021\nConv (3\u00023) 130\u000245\u000232\nConv (3\u00023) 132\u000247\u000232\nPool (2\u00022) 44\u000215\u000232\nDropout (0:25) 44\u000215\u000232\nConv (3\u00023) 46\u000217\u000264\nConv (3\u00023) 48\u000219\u000264\nPool (2\u00022) 16\u00026\u000264\nDropout (0:25) 16\u00026\u000264\nConv (3\u00023) 18\u00028\u0002128\nConv (3\u00023) 20\u000210\u0002128\nPool (2\u00022) 6\u00023\u0002128\nDropout (0:25) 6\u00023\u0002128\nConv (3\u00023) 8\u00025\u0002256\nConv (3\u00023) 10\u00027\u0002256\nGlobal pool 1\u00021\u0002256\nDense 1024\nDropout (0:5) 1024\nDense 11\nTable 2 : Han’s architecture.Layer Output size\nRaw wave 44100\u00021\nConv (3101) 41000\u0002256\nPool (40) 2049\u0002256\nConv (300) 1750\u0002384\nPool (30) 87\u0002384\nConv (20) 68\u0002384\nPool (8) 16\u0002384\nDropout (0:5) 16\u0002384\nDense 400\nDense 11\nTable 3 : Li’s architecture.\n5.4.1 Bag-of-features\nFor the bag-of-features method, we used the features de-\nscribed by [15], consisting of 120 features obtained by\ncomputing the mel-frequency cepstral coefﬁcients and 16\nspectral features [33]. We trained an SVM with a radial ba-\nsis function (RBF) kernel by feeding it these 136 features.\n5.4.2 Mel-spectrogram (Han’s CNN model)\nFor the deep learning method with mel-spectrogram input,\nwe used Han’s CNN architecture [16] (Table 2). This ar-\nchitecture is based on VGGNet [36], a commonly used\nmodel in the image processing ﬁeld. Each one-second\nfragment of the audio signal was resampled into 22050 Hz,\nconverted into a mel-spectrogram, and standardized. Every\nactivation function was LReLU ( \u000b= 0:33) except the out-\nput sigmoid.\nIn preliminary experiments, training this model failed\nin almost 700 epochs due to a gradient loss. Therefore,\nwe applied batch normalization to each of the convolu-\ntional and dense layers, enabling us to successfully com-\nplete 1000 epochs of training. We also used 500 epochs,\nbut the performance was worse than for 1000.\n5.4.3 Raw Waveform (Li’s CNN model)\nFor the deep learning method with raw wave input, we used\nLi’s CNN model in [27] (Table 3). This model performs\nend-to-end learning using a raw waveform. We standard-\nized each one-second fragment of the monaural audio sig-\nnal obtained in pre-processing. Every activation function\nwas ReLU except the output sigmoid. Batch normalization\nwas again applied to each layer. We trained the model with\n1000 epochs.\n5.5 Metrics\nWe evaluated each model using four metrics: accuracy ,F-\nmicro ,F-macro , and AUC .\nAccuracy was deﬁned as the ratio of predicted labels\nthat exactly matched the ground truth. Each label predicted\nby the CNN at every one-second fragment in all pieces wasProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 565Figure 11 : Proposed model showed best performance for F-\nmicro, F-macro, and AUC but took ﬁve times longer to com-\nplete training than Han’s model, which showed second-best per-\nformance.\nan 11-dimensional vector of likelihoods. Since each likeli-\nhood ranged between 0 and 1, we rounded it to an integer\n(0 or 1) before matching.\nThe F-micro was deﬁned as the micro average of the F1\nmeasure for all predicted labels over the 11 categories. The\nF1 measure is deﬁned as the harmonic mean of recall and\nprecision and is widely used in multi-label classiﬁcation\ntasks. Since it is calculated immediately without consider-\ning the categories, if some instruments frequently appear,\ntheir predicted labels considerably affect the F-micro.\nThe F-macro was deﬁned as the macro average with\neach instrument equally considered. For each of the 11\ncategories, the F1 measure of the predicted labels was ﬁrst\ncalculated. Then, the average of the resulting 11 values\nwas calculated as the F-macro.\nThe area under the curve (AUC) of the receiver oper-\nating characteristic was ﬁrst calculated for each category.\nThen, the macro average of the resulting 11 values was\nused as the AUC in our multi-label task.\n5.6 Results\nAs shown in Figure 11, the proposed model outperformed\nthe other models in terms of AUC, F-micro, and especially\nF-macro, which was about 8% better than the next-best\nmodel (Han’s model). This indicates that our model has\nhigher generic performance and is more powerful in deal-\ning with various kinds of instruments.\nInterestingly, all of the deep learning methods showed\nsigniﬁcantly higher accuracy than the bag-of-features\nmethod. Since the accuracy cannot be increased with\npredictions made through guesswork, such as predicting\nclasses that frequently appear, the deep learning methods\nare more capable of capturing the sound characteristics of\ninstruments in sound mixtures.\nThe proposed model took ﬁve times longer to complete\ntraining than Han’s model. This is because Han’s model\ntook advantage of using a more compact mel-spectrogram\n(128\u000287) than the raw spectrogram (1024 \u000287) used\nfor the proposed model. ɹSince using a mel-spectrogram\nresults in losing more information, the performance was\nworse.\nFigure 12 : Maps created using ground truth data.\n6 DISCUSSION\n6.1 Smoothing Transitions Between Listening States\nOur observations during testing showed that the use of\nInstrudive helped smooth the transition between listening\nstates. Although the music was often passively listened to,\nthe listeners sometimes suddenly became active when the\ntime came to choose the next piece. In the circular mode\nof Instrudive, for example, the clockwise player played a\npiece that had instrumentation similar to the previous one.\nSince the sound characteristics were changing gradually, a\nuser was able to listen to various genres in a passive state.\nIf non-preferred music started playing, the user skipped to\na different type of music by using the shufﬂe player . In ad-\ndition, the user actively used the search function to access\npieces with similar instrumentation and enjoyed looking at\nthe temporal changes in the activity graph.\n6.2 Studies from Ground Truth Data\nWe compared maps created using the automatically recog-\nnized (predicted) data (Figure 9) with maps created using\nthe ground truth data (Figure 12). Although they are sim-\nilar to some extent, the contrast of the color distributions\nis much more vivid for the ground truth data, suggesting\nthat the performance of our CNN model still has room for\nimprovement. Since the proposed Instrudive interface is\nindependent of the method used for instrument recogni-\ntion, we can simply incorporate an improved model in the\nfuture.\n7 CONCLUSION\nOur Instrudive system visualizes the instrumentations of\nthe musical pieces in a collection for music discovery and\nactive music listening. The ﬁrst main contribution of this\nwork is showing how instrumentation can be effectively\nused in browsing musical pieces and in enhancing the lis-\ntening experience during playing of a musical piece. The\nsecond main contribution is proposing a CNN model for\nrecognizing instruments appearing in polyphonic sound\nmixtures that achieves better performance than other state-\nof-the-art models.\nWe plan to conduct user studies of Instrudive to analyze\nits nature in more detail and to test different shapes of ﬁl-\nters to analyze the reasons for the superior performance of\nour CNN model. We are also interested in investigating\nthe scalability of our approach by increasing the number\nof musical pieces and allowing a greater variety of instru-\nments.566 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20188 ACKNOWLEDGMENTS\nThis work was supported in part by JST ACCEL Grant\nNumber JPMJAC1602, Japan.\n9 REFERENCES\n[1]Rachel M. Bittner, Justin Salamon, Mike Tierney,\nMatthias Mauch, Chris Cannam, and Juan Pablo\nBello. MedleyDB: A multitrack dataset for annotation-\nintensive MIR research. In Proceedings of the 15th\nInternational Society for Music Information Retrieval\nConference (ISMIR 2014) , pages 155–160, 2014.\n[2]Dmitry Bogdanov and Perfecto Herrera. How much\nmetadata do we need in music recommendation? A\nsubjective evaluation using preference sets. In Pro-\nceedings of the 12th International Society for Music\nInformation Retrieval Conference (ISMIR 2011) , pages\n97–102, 2011.\n[3]Mattew Cooper and Jonathan Foote. Automatic music\nsummarization via similarity analysis. In Proceedings\nof the 3rd International Conference on Music Informa-\ntion Retrieval (ISMIR 2002) , 2002.\n[4]Sander Dieleman and Benjamin Schrauwen. End-to-\nend learning for music audio. In Proceedings of the\nIEEE International Conference on Acoustics, Speech,\nand Signal Processing (IEEE ICASSP 2014) , pages\n6964–6968, 2014.\n[5]Michael D. Ekstrand, John T. Riedl, and Joseph A.\nKonstan. Collaborative ﬁltering recommender systems.\nFoundations and Trends in Human-Computer Interac-\ntion, 4(2):81–173, 2010.\n[6]Antti Eronen and Aussi Klapuri. Musical instrument\nrecognition using cepstral coefﬁcients and temporal\nfeatures. In Proceedings of the IEEE International\nConference on Acoustics, Speech, and Signal Process-\ning (IEEE ICASSP 2000) , volume 2, pages 753–756,\n2000.\n[7]Slim Essid, Ga ¨el Richard, and Bertrand David. In-\nstrument recognition in polyphonic music based on\nautomatic taxonomies. IEEE Transactions on Audio,\nSpeech, and Language Processing , 14(1):68–80, 2006.\n[8]Slim Essid, Ga ¨el Richard, and Bertrand David. Mu-\nsical instrument recognition by pairwise classiﬁcation\nstrategies. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 14(4):1401–1412, 2006.\n[9]Jonathan Foote. Visualizing music and audio using\nself-similarity. In Proceedings of the Seventh ACM In-\nternational Conference on Multimedia (ACM Multime-\ndia 1999) , pages 77–80, 1999.\n[10] Masataka Goto. A chorus section detection method for\nmusical audio signals and its application to a music lis-\ntening station. IEEE Transactions on Audio, Speech,\nand Language Processing , 14(5):1783–1794, 2006.[11] Masataka Goto and Takayuki Goto. Musicream: Inte-\ngrated music-listening interface for active, ﬂexible, and\nunexpected encounters with musical pieces. IPSJ Jour-\nnal, 50(12):2923–2936, 2009.\n[12] Masataka Goto, Kazuyoshi Yoshii, Hiromasa Fujihara,\nMatthias Mauch, and Tomoyasu Nakano. Songle: A\nweb service for active music listening improved by\nuser contributions. In Proceedings of the 12th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR 2011) , pages 311–316, 2011.\n[13] Masahiro Hamasaki and Masataka Goto. Songrium: A\nmusic browsing assistance service based on visualiza-\ntion of massive open collaboration within music con-\ntent creation community. In Proceedings of the 9th In-\nternational Symposium on Open Collaboration (ACM\nWikiSym + OpenSym 2013) , pages 1–10, 2013.\n[14] Philippe Hamel and Douglas Eck. Learning features\nfrom music audio with deep belief networks. In Pro-\nceedings of the 11th International Society for Music\nInformation Retrieval Conference (ISMIR 2010) , pages\n339–344, 2010.\n[15] Philippe Hamel, Sean Wood, and Douglas Eck. Auto-\nmatic identiﬁcation of instrument classes in polyphonic\nand poly-instrument audio. In Proceedings of the 10th\nInternational Society for Music Information Retrieval\nConference (ISMIR 2009) , pages 399–404, 2009.\n[16] Yoonchang Han, Jaehun Kim, and Kyogu Lee. Deep\nconvolutional neural networks for predominant instru-\nment recognition in polyphonic music. IEEE/ACM\nTransactions on Audio, Speech, and Language Pro-\ncessing , 25(1):208–221, 2017.\n[17] Sergey Ioffe and Christian Szegedy. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint\narXiv:1502.03167 , 2015.\n[18] Dasaem Jeong and Juhan Nam. Visualizing music in\nits entirety using acoustic features: Music ﬂowgram. In\nProceedings of the International Conference on Tech-\nnologies for Music Notation and Representation , pages\n25–32, 2016.\n[19] Ian Kaminskyj and Tadeusz Czaszejko. Automatic\nrecognition of isolated monophonic musical instru-\nment sounds using kNNC. Journal of Intelligent Infor-\nmation Systems , 24(2):199–221, 2005.\n[20] Taejun Kim, Jongpil Lee, and Juhan Nam. Sample-\nlevel cnn architectures for music auto-tagging using\nraw waveforms. In Processings of the 14th Sound and\nMusic Computing Conference (SMC 2017) , 2017.\n[21] Diederik P. Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 567[22] Tetsuro Kitahara, Masataka Goto, Kazunori Komatani,\nTetsuya Ogata, and Hiroshi G. Okuno. Instrogram:\nProbabilistic representation of instrument existence for\npolyphonic music. IPSJ Journal , 2(1):279–291, 2007.\n[23] Peter Knees, Markus Schedl, Tim Pohle, and Ger-\nhard Widmer. An innovative three-dimensional user in-\nterface for exploring music collections enriched with\nmeta-information from the web. In Proceedings of the\n14th ACM International Conference on Multimedia\n(ACM Multimedia 2006) , pages 17–24, 2006.\n[24] Paul Lamere and Douglas Eck. Using 3D visualiza-\ntions to explore and discover music. In Proceedings of\nthe 8th International Conference on Music Information\nRetrieval (ISMIR 2007) , pages 173–174, 2007.\n[25] Gilbert Laporte. The traveling salesman problem: An\noverview of exact and approximate algorithms. Euro-\npean Journal of Operational Research , 59(2):231–247,\n1992.\n[26] Pierre Leveau, David Sodoyer, and Laurent Daudet.\nAutomatic instrument recognition in a polyphonic mix-\nture using sparse representations. In Proceedings of\nthe 8th International Conference on Music Information\nRetrieval (ISMIR 2007) , pages 233–236, 2007.\n[27] Peter Li, Jiyuan Qian, and Tian Wang. Auto-\nmatic instrument recognition in polyphonic music\nusing convolutional neural networks. arXiv preprint\narXiv:1511.05520 , 2015.\n[28] Vincent Lostanlen and Carmine-Emanuele Cella. Deep\nconvolutional networks on the pitch spiral for music in-\nstrument recognition. In Proceedings of the 17th Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR 2016) , pages 612–618, 2016.\n[29] Meinard M ¨uller and Nanzhu Jiang. A scape plot rep-\nresentation for visualizing repetitive structures of mu-\nsic recordings. In Proceedings of the 13th International\nSociety for Music Information Retrieval Conference\n(ISMIR 2012) , pages 97–102, 2012.\n[30] Sergio Oramas, Oriol Nieto, Francesco Barbieri, and\nXavier Serra. Multi-label music genre classiﬁcation\nfrom audio, text and images using deep features. In\nProceedings of the 18th International Society for Mu-\nsic Information Retrieval Conference (ISMIR 2017) ,\npages 23–30, 2017.\n[31] Elias Pampalk, Simon Dixon, and Gerhard Widmer.\nExploring music collections by browsing different\nviews. In Proceedings of the 4th International Con-\nference on Music Information Retrieval (ISMIR 2003) ,\n2003.\n[32] Elias Pampalk and Masataka Goto. MusicRainbow: A\nnew user interface to discover artists using audio-based\nsimilarity and web-based labeling. In Proceedings of\nthe 7th International Conference on Music Information\nRetrieval (ISMIR 2006) , pages 367–370, 2006.[33] Geoffroy Peeters. A large set of audio features for\nsound description (similarity and classiﬁcation) in the\nCUIDADO project. Technical report, IRCAM, 2004.\n[34] Jordi Pons, Olga Slizovskaia, Rong Gong, Emilia\nG´omez, and Xavier Serra. Timbre analysis of music au-\ndio signals with convolutional neural networks. In Pro-\nceedings of the 25th European Signal Processing Con-\nference (EUSIPCO 2017) , pages 2744–2748, 2017.\n[35] Konstantinos Sechidis, Grigorios Tsoumakas, and\nIoannis Vlahava. On the stratiﬁcation of multi-label\ndata. In Machine Learning and Knowledge Discovery\nin Databases , pages 145–158, 2011.\n[36] Karen Simonyan and Andrew Zisserman. Very deep\nconvolutional networks for large-scale image recogni-\ntion. arXiv preprint arXiv:1409.1556 , 2014.\n[37] Marc Torrens, Patrick Hertzog, and Josep-Lluis Arcos.\nVisualizing and exploring personal music libraries. In\nProceedings of the 5th International Conference on\nMusic Information Retrieval (ISMIR 2004) , 2004.\n[38] A¨aron van den Oord, Sander Dieleman, and Benjamin\nSchrauwen. Deep content-based music recommenda-\ntion. In Proceedings of the 26th International Confer-\nence on Neural Information Processing Systems (NIPS\n2013) , pages 2643–2651, 2013.\n[39] Laurens van der Maaten and Geoffrey Hinton. Visual-\nizing data using t-SNE. Journal of Machine Learning\nResearch , 9:2579–2605, 2008.\n[40] Kazuyoshi Yoshii and Masataka Goto. Music Thumb-\nnailer: Visualizing musical pieces in thumbnail images\nbased on acoustic features. In Proceedings of the 9th\nInternational Conference on Music Information Re-\ntrieval (ISMIR 2008) , pages 211–216, 2008.\n[41] Guoshen Yu and Jean-Jacques Slotine. Audio classi-\nﬁcation from time-frequency texture. In Proceedings\nof the IEEE International Conference on Acoustics,\nSpeech, and Signal Processing (IEEE ICASSP 2014) ,\npages 1677–1680, 2009.\n[42] Xin Zhang and Zbigniew W. Ras. Differentiated har-\nmonic feature analysis on music information retrieval\nfor instrument recognition. In Proceedings of the\nIEEE International Conference on Granular Comput-\ning, pages 578–581, 2006.568 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Cover Song Synthesis by Analogy.",
        "author": [
            "Christopher J. Tralie"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492381",
        "url": "https://doi.org/10.5281/zenodo.1492381",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/103_Paper.pdf",
        "abstract": "In this work, we pose and address the following \"cover song analogies\" problem: given a song A by artist 1 and a cover song A' of this song by artist 2, and given a different song B by artist 1, synthesize a song B' which is a cover of B in the style of artist 2. Normally, such a polyphonic style transfer problem would be quite challenging, but we show how the cover songs example constrains the problem, making it easier to solve. First, we extract the longest common beat-synchronous subsequence between A and A', and we time stretch the corresponding beat intervals in A' so that they align with A. We then derive a version of joint 2D convolutional NMF, which we apply to the constant-Q spectrograms of the synchronized segments to learn a translation dictionary of sound templates from A to A'. Finally, we apply the learned templates as filters to the song B, and we mash up the translated filtered components into the synthesized song B' using audio mosaicing. We showcase our algorithm on several examples, including a synthesized cover version of Michael Jackson's \"Bad\" by Alien Ant Farm, learned from the latter's \"Smooth Criminal\" cover.",
        "zenodo_id": 1492381,
        "dblp_key": "conf/ismir/Tralie18",
        "keywords": [
            "cover song analogies",
            "polyphonic style transfer",
            "longest common beat-synchronous subsequence",
            "time stretching",
            "joint 2D convolutional NMF",
            "constant-Q spectrograms",
            "translation dictionary",
            "sound templates",
            "audio mosaicing",
            "synthesized cover version"
        ],
        "content": "COVER SONG SYNTHESIS BY ANALOGY\nChristopher J. Tralie\nDuke University Department of Mathematics\nctralie@alumni.princeton.edu\nABSTRACT\nIn this work, we pose and address the following “cover\nsong analogies” problem: given a song A by artist 1 and\na cover song A’ of this song by artist 2, and given a dif-\nferent song B by artist 1, synthesize a song B’ which is a\ncover of B in the style of artist 2. Normally, such a poly-\nphonic style transfer problem would be quite challenging,\nbut we show how the cover songs example constrains the\nproblem, making it easier to solve. First, we extract the\nlongest common beat-synchronous subsequence between\nA and A’, and we time stretch the corresponding beat in-\ntervals in A’ so that they align with A. We then derive a\nversion of joint 2D convolutional NMF, which we apply to\nthe constant-Q spectrograms of the synchronized segments\nto learn a translation dictionary of sound templates from A\nto A’. Finally, we apply the learned templates as ﬁlters to\nthe song B, and we mash up the translated ﬁltered compo-\nnents into the synthesized song B’ using audio mosaicing.\nWe showcase our algorithm on several examples, including\na synthesized cover version of Michael Jackson’s “Bad” by\nAlien Ant Farm, learned from the latter’s “Smooth Crimi-\nnal” cover.\n1. INTRODUCTION\nThe rock group Alien Ant Farm has a famous cover of\nMichael Jackson’s “Smooth Criminal” which is faithful\nto but stylistically unique from the original song. How-\never, to our knowledge, they never released a cover of any\nother Michael Jackson songs. What if we instead wanted\nto know how they would have covered Michael Jackson’s\n“Bad”? That is, we seek a song which is identiﬁable\nas MJ’s “Bad,” but which also sounds as if it’s in Alien\nAnt Farm’s style, including timbral characteristics, relative\ntempo, and instrument types.\nIn general, multimedia style transfer is a challenging\ntask in computer aided creativity applications. When an\nexample of the stylistic transformation is available, as in\nthe “Smooth Criminal” example above, this problem can\nbe phrased in the language of analogies; given an object\nAand a differently stylized version of this object A0, and\ngiven an object Bin the style of A, synthesize an ob-\nc\rChristopher J. Tralie. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution:\nChristopher J. Tralie. “Cover Song Synthesis by Analogy”, 19th Interna-\ntional Society for Music Information Retrieval Conference, Paris, France,\n2018.\nFigure 1 . A demonstration of the “shape analogies” tech-\nnique in [22]. In the language of our work, the statue head\nwith a neutral expression ( A0) is a “cover” of the low reso-\nlution mesh Awith a neutral expression, and this is used to\nsynthesize the surprised statue “cover” face B0from a low\nresolution surprised face mesh B.\njectB0which has the properties of Bbut the style of\nA0. One of the earliest works using this vocabulary is the\n“image analogies” work [12], which showed it was pos-\nsible to transfer both linear ﬁlters (e.g. blurring, emboss-\ning) and nonlinear ﬁlters (e.g. watercolors) in the same\nsimple framework. More recent work with convolutional\nnetworks has shown even better results for images [10].\nThere has also been some work on “shape analogies” for\n3D meshes [22], in which nonrigid deformations between\ntriangle meshes AandA0are used to induce a correspond-\ning deformation B0from an object B, which can be used\nfor motion transfer (Figure 1).\nIn the audio realm, most style transfer works are based\non mashing up sounds from examples in a target style us-\ning “audio mosaicing,” usually after manually specifying\nsome desired path through a space of sound grains [16].\nA more automated audio moscaicing technique, known\nas “audio analogies” [19], uses correspondences between\na MIDI score and audio to drive concatenated synthesis,\nwhich leads to impressive results on monophonic audio,\nsuch as stylized synthesis of jazz recordings of a trumpet.\nMore recently, this has evolved into the audio to musical\naudio setting with audio “musaicing,” in which the timbre\nof an audio source is transferred onto a target by means of\na modiﬁed NMF algorithm [7], such as bees buzzing The\nBeatles’ “Let It Be.” A slightly closer step to the poly-\nphonic (multi source) musical audio to musical audio case\nhas been shown to work for drum audio cross-synthesis\nwith the aid of a musical score [5], and some very recent\ninitial work has extended this to the general musical audio\nto musical audio case [9] using 2D nonnegative matrix fac-\ntorization, though this still remains open. Finally, there is\nsome recent work on converting polyphonic audio of gui-\ntar songs to musical scores of varying difﬁculties so users197Figure 2 . An ideal cartoon example of our joint 2DNMF\nand ﬁltering process, where M= 20 ,N= 60 ,T= 10 ,\nF= 10 , andK= 3. In this example, vertical time-\nfrequency blocks are “covered” by horizontal blocks, di-\nagonal lines with negative slopes are covered by diagonal\nlines with positive slopes, and squares are covered by cir-\ncles. When presented with a new song B, our goal is to\nsynthesize a song B0whose CQT is shown in the lower\nright green box.\ncan play their own covers [1].\nIn this work, we constrain the polyphonic musical au-\ndio to musical audio style transfer problem by using cover\nsong pairs, or songs which are the same but in a differ-\nent style, and which act as our ground truth AandA0\nexamples in the analogies framework. Since small scale\nautomatic cover song identiﬁcation has matured in recent\nyears [4, 17, 18, 23], we can accurately synchronize Aand\nA0(Section 2.1), even if they are in very different styles.\nOnce they are synchronized, the problem becomes more\nstraightforward, as we can blindly factorize AandA0into\ndifferent instruments which are in correspondence, turn-\ning the problem into a series of monophonic style transfer\nproblems. To do this, we perform NMF2D factorization of\nAandA0(Section 2.2). We then ﬁlter Bby the learned\nNMF templates and mash up audio grains to create B0, us-\ning the aforementioned “musaicing” techniques [7] (Sec-\ntion 2.3). We demonstrate our techniques on snippets of\nA,A0, andBwhich are about 20 seconds long, and we\nshow qualitatively how the instruments of A0transfer onto\nthe music of Bin the ﬁnal result B0(Section 3).\n2. ALGORITHM DETAILS\nIn this section, we will describe the steps of our algorithm\nin more detail.1\n2.1 Cover Song Alignment And Synchronization\nAs with the original image analogies algorithm [12], we\nﬁnd it helpful if AandA0are in direct correspondence\nat the sample level. Since cover song pairs generally dif-\nfer in tempo, we need to align them ﬁrst. To accomplish\nthis, we draw upon the state of the art “early fusion” cover\nsong alignment technique presented by the authors of [23].\nBrieﬂy, we extract beat onsets for AandA0using either\n1Note that all audio is mono and sampled at 22050hz.\nFigure 3 . An example feature fused cross-similarity ma-\ntrixDfor the ﬁrst 80 beats of Michael Jackson’s “Smooth\nCriminal,” compared to the cover version by Alien Ant\nFarm. We threshold the matrix and extract 20 seconds of\nthe longest common subsequence, as measured by Smith\nWaterman. The alignment path is shown in red.\na simple dynamic programming beat tracker [8] or slower\nbut more accurate RNN + Bayesian beat trackers [13], de-\npending on the complexity of the audio. We then com-\npute beat-synchronous sliding window HPCP and MFCC\nfeatures, and we fuse them using similarity network fu-\nsion [25, 26]. The result is a M\u0002Ncross-similarity ma-\ntrixD, whereMis the number of beats in AandNis the\nnumber of beats in A0, andDijis directly proportional to\nthe similarity between beat iofAand beatjinA0. Please\nrefer to [23] for more details.\nOnce we have the matrix D, we can then extract an\nalignment between AandA0by performing Smith Water-\nman [20] on a binary thresholded version of D, as in [23].\nWe make one crucial modiﬁcation, however. To allow for\nmore permissive alignments with missing beats for identi-\nﬁcation purposes, the original cover songs algorithm cre-\nates a binary thresholded version of Dusing 10% mutual\nbinary nearest neighbors. On the other hand, in this appli-\ncation, we seek shorter snippets from each song which are\nas well aligned as possible. Therefore, we create a stricter\nbinary thresholded version B, whereBij= 1 only if it is\nin the top 3p\nMN distances over all MN distances in D.\nThis means that many rows of Bijwill be all zeros, but\nwe will hone in on the best matching segments. Figure 3\nshows such a thresholding of the cross-similarity matrix for\ntwo versions of the song “Smooth Criminal,” which is an\nexample we will use throughout this section. Once Bhas\nbeen computed, we compute a X-length alignment path\nPby back-tracing through the Smith Waterman alignment\nmatrix, as shown in Figure 3.\nLet the beat onset times for Ain the path Pbe\nt1;t2;:::;tXand the beat times for A0bes1;s2;:::;sX.\nWe use the rubberband library [3] to time stretch A0beat\nby beat, so that interval [si;si+ 1] is stretched by a fac-\ntor(ti+1\u0000ti)=(si+1\u0000si). The result is a snippet of A0\nwhich is the same length as the corresponding snippet in A.\nHenceforth, we will abuse notation and refer to these snip-\npets asAandA0. We also extract a smaller snippet from B\nof the same length for reasons of memory efﬁciency, which\nwe will henceforth refer to as B.198 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20182.2 NMF2D for Joint Blind Factorization / Filtering\nOnce we have synchronized the snippets AandA0, we\nblindly factorize and ﬁlter them into Kcorresponding\ntracksA1;A2;:::;AKandA0\n1;A0\n2;:::;A0\nK. The goal is for\neach track to contain a different instrument. For instance,\nA1may contain an acoustic guitar, which is covered by an\nelectric guitar contained in A0\n1, andA2may contain drums\nwhich are covered by a drum machine in A0\n2. This will\nmake it possible to reduce the synthesis problem to Kin-\ndependent monophonic analogy problems in Section 2.3.\nTo accomplish this, the main tool we use is 2D convo-\nlutional nonnegative matrix factorization (2DNMF) [15].\nWe apply this algorithm to the magnitude of the constant-Q\ntransforms (CQTs) [2] CA,CA0andCBofA,A0, andB,\nrespectively. Intuitively, we seek Ktime-frequency tem-\nplates inAandA0which represent small, characteristic\nsnippets of the Kinstruments we believe to be present in\nthe audio. We then approximate the magnitude constant-\nQ transform of AandA0by convolving these templates\nin both time and frequency. The constant-Q transform is\nnecessary in this framework, since a pitch shift can be ap-\nproximated by a linear shift of all CQT bins by the same\namount. Frequency shifts can then be represented as con-\nvolutions in the vertical direction, which is not possible\nwith the ordinary STFT. Though it is more complicated,\n2DNMF is a more compact representation than 1D con-\nvolutional NMF (in time only), in which it is necessary\nto store a different template for each pitch shift of each\ninstrument. We note that pitch shifts in real instruments\nare more complicated than shifting all frequency bins by\nthe same perceptual amount [14], but the basic version of\n2DNMF is ﬁne for our purposes.\nMore concretely, deﬁne a K-component, F-frequency,\nT-time lag 2D convolutional NMF decomposition for a\nmatrix X2RM\u0002Nas follows\nX\u0019\u0003W;H=TX\n\u001c=1FX\n\u001e=1#\u001e\nW\u001c!\u001c\nH\u001e(1)\nwhere W\u001c2RM\u0002KandH\u001e2RK\u0002Nstore a\u001c-\nshifted time template and \u001e-frequency shifted coefﬁcients,\nrespectively. By#\u001e\nA, we mean down shift the rows of Aby\n\u001e, so that row iof#\u001e\nAis rowi\u0000\u001eofA, and the ﬁrst \u001erows\nof#\u001e\nAare all zeros. And \u001c\nAmeans left-shift A, so that col-\numnjof \u001c\nAis columnj\u0000\u001cofA, and the ﬁrst \u001ccolumns\nof \u001c\nAare all zeros.\nIn our problem, we deﬁne an extension of 2DNMF to\njointly factorize the 2 songs, AandA0, which each have M\nCQT coefﬁcients. In particular, given matrices CA;CA02\nCM\u0002N1representing the complex CQT frames in each\nsong over time, we seek W\u001c\n1;W\u001c\n22RM\u0002KandH\u001e\n12\nRK\u0002N1that minimize the sum of the Kullback-Leibler di-\nvergences between the magnitude CQT coefﬁcients and the\nconvolutions:\nD(jCAjjj\u0003W1;H1) +D(jCA0jjj\u0003W2;H1)(2)where the Kullback-Leibler divergence D(XjjY)is de-\nﬁned as\nD(XjjY) =X\niX\njXi;jlogXi;j\nYi;j\u0000Xi;j+Yi;j (3)\nThat is, we share H1between the factorizations of\njCAjandjCA0jso that we can discover shared structure\nbetween the covers. Following similar computations to\nthose of ordinary 2DNMF [15], it can be shown that Equa-\ntion 2 is non-decreasing under the alternating update rules:\nW\u001c\n1 W\u001c\n1\fPF\n\u001e=1\"\u001e\u0010\njCAj\n\u0003W1;H1\u0011!\u001c\nH\u001e\n1T\nPF\n\u001e=11\u0001!\u001c\nH\u001e\n1T(4)\nW\u001c\n2 W\u001c\n2\fPF\n\u001e=1\"\u001e\u0010\njCA0j\n\u0003W2;H1\u0011!\u001c\nH\u001e\n1T\nPF\n\u001e=11\u0001!\u001c\nH\u001e\n1T(5)\nH\u001e\n1 H\u001e\n1\f0\nBB@PT\n\u001c=1#\u001e\nW\u001c\n1T \u001c\u0010\njCAj\n\u0003W1;H1\u0011\n+#\u001e\nW\u001c\n2T \u001c\u0010\njCA0j\n\u0003W2;H1\u0011\nPT\n\u001c=1#\u001e\nW\u001c\n1T \u001c\n1+#\u001e\nW\u001c\n2T \u001c\n11\nCCA\n(6)\nwhere 1is a column vector of all 1s of appropriate di-\nmension. We need an invertible CQT to go back to au-\ndio templates, so we use the non-stationary Gabor Trans-\nform (NSGT) implementation of the CQT [24] to compute\nCA;CA0, and CB. We use 24 bins per octave between\n50hz and 11.7kHz, for a total of 189 CQT bins. We also\nuseF= 14 in most of our examples, allowing 7 halfstep\nshifts, and we use T= 20 on temporally downsampled\nCQTs to cover a timespan of 130 milliseconds. Finally, we\niterate through Equations 4, 5,and 6 in sequence 300 times.\nNote that a naive implementation of the above equations\ncan be very computationally intensive. To ameliorate this,\nwe implemented GPU versions of Equations 1, 4, 5,and 6.\nEquation 1 in particular is well-suited for a parallel imple-\nmentation, as the shifted convolutional blocks overlap each\nother heavily and can be carefully ofﬂoaded into shared\nmemory to exploit this2. In practice, we witnessed a 30x\nspeedup of our GPU implementation over our CPU imple-\nmentation for 20 second audio clips for A,A0, andB.\nFigure 2 shows a synthetic example with an exact solu-\ntion, and Figure 4 shows a local min which is the result\nof running Equations 4, 5,and 6 on real audio from the\n“Smooth Criminal” example. It is evident from H1that\nthe ﬁrst component is percussive (activations at regular in-\ntervals in H1\n1, and no pitch shifts), while the second com-\nponent corresponds to the guitar melody ( H2\n1appears like\na “musical score” of sorts). Furthermore, W1\n1andW1\n2\n2In the interest of space, we omit more details of our GPU-based\nNMFD in this paper, but a documented implementation can be found\nathttps://github.com/ctralie/CoverSongSynthesis/ ,\nand we plan to release more details in a companion paper later.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 199Figure 4 . Joint 2DNMF on the magnitude CQTs of “Smooth Criminal” by Michael Jackson and Alien Ant Farm, with\nF= 14 (7 halfsteps at 24 bins per octave), T= 20 (130ms in time), and K= 2 components. In this case, W1\n1andW1\n2\nhold percussive components, and W2\n1andW2\n2hold guitar components.\nhave broadband frequency content consistent with percus-\nsive events, while W2\n1andW2\n2have visible harmonics\nconsistent with vibrating strings. Note that we generally\nuse more than K= 2, which allows ﬁner granularity than\nharmonic/percussive, but even for K= 2 in this exam-\nple, we observe qualitatively better separation than off-the-\nshelf harmonic/percussive separation algorithms [6].\nOnce we have W1,W2, andH1, we can recover the\naudio templates A1;A2;:::;AKandA0\n1;A0\n2;:::;A0\nKby us-\ning the components of W1andW2as ﬁlters. First, deﬁne\n\u0003W;H;kas\n\u0003W;H;k=TX\n\u001c=1FX\n\u001e=1#\u001e\nWk\u001c!\u001c\nHk\u001e(7)\nwhere Wkis thekthcolumn of WandHkis thekth\nrow of H. Now, deﬁne the ﬁltered CQTs by using soft\nmasks derived from W1andH1:\nCAk=CA\f \n\u0003p\nW1;H1;kPK\nm=1\u0003p\nW1;H1;m!\n(8)\nCA0\nk=CA0\f \n\u0003p\nW2;H1;kPK\nm=1\u0003p\nW2;H1;m!\n(9)\nwherepis some positive integer applied element-wise\n(we choosep= 2), and the above multiplications and di-\nvisions are also applied element-wise. It is now possible to\ninvert the CQTs to uncover the audio templates, using the\ninverse NSGT [24]. Thus, if the separation was good, we\nare left with Kindependent monophonic pairs of sounds\nbetweenAandA0.\nIn addition to inverting the sounds after these masks are\napplied, we can also listen to the components of W1and\nW2themselves to gain insight into how they are behav-\ning as ﬁlters. Since W1andW2are magnitude only, we\napply the Grifﬁn Lim algorithm [11] to perform phase re-\ntrieval, and then we invert them as before to obtain 130\nmillisecond sounds for each k.2.3 Musaicing And Mixing\nWe now describe how to use the corresponding audio tem-\nplates we learned in Section 2.2 to perform style transfer\non a new piece of audio, B.\n2.3.1 Separating Tracks in B\nFirst, we compute the CQT of B,CB2CM\u0002N2. We\nthen represent its magnitude using W1as a basis, so that\nwe ﬁlterBinto the same set of instruments into which A\nwas separated. That is, we solve for H2so thatjCBj\u0019\n\u0003W1;H2. This can be performed with ordinary 2DNMF,\nholding W1ﬁxed; that is, repeating the following update\nuntil convergence\nH\u001e\n2 H\u001e\n2\f0\nBB@PT\n\u001c=1#\u001e\nW\u001c\n1T \u001c\u0010\njCBj\n\u0003W1;H2\u0011\nPT\n\u001c=1#\u001e\nW\u001c\n1T \u001c\n11\nCCA(10)\nAs withAandA0, we can now ﬁlter Binto a set of au-\ndio tracksB1;B2;:::;BKby ﬁrst computing ﬁltered CQTs\nas follows\nCBk=CB\f \n\u0003p\nW1;H2;kPK\nm=1\u0003p\nW1;H2;m!\n(11)\nand then inverting them.\n2.3.2 Constructing B0Track by Track\nAt this point, we could use H2and let our cover song CQT\nmagnitudesjCB0j=\u0003W2;H2, followed by Grifﬁn Lim\nto recover the phase. However, we have found that the\nresulting sounds are too “blurry,” as they lack all but re-\narranged low rank detail from A0. Instead, we choose to\ndraw sound grains from the inverted, ﬁltered tracks from\nA0, which contain all of the detail of the original song.\nFor this, we ﬁrst reconstruct each track of Busing audio\ngrains from the corresponding tracks in A, and then we200 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018replace each track with the corresponding track in B. To\naccomplish this, we apply the audio musaicing technique\nof Driedger [7] to each track in B, using source audio A.\nFor computational and memory reasons, we now aban-\ndon the CQT and switch to the STFT with hop size h=\n256and window size w= 2048 . More speciﬁcally, let N1\nbe the number of STFT frames in AandA0and letN2be\nthe number of STFT frames in B. For eachAk, we create\nan STFT matrix SAkwhich holds the STFT of Akcon-\ncatenated to pitch shifted versions of Ak, so that pitches\nbeyond what were in Acan be represented, but by the same\ninstruments. We use \u00066 halfsteps, so SAk2Cw\u000213N1.\nWe do the same for A0\nkto create SA0\nk2Cw\u000213N1. Fi-\nnally, we compute the STFT of Bkwithout any shifts:\nSBk2Cw\u0002N2.\nNow, we apply Driedger’s technique, using jSAkjas a\nspectral dictionary to reconstruct jSBkj(SAkis analogous\nto the buzzing bees in [7]). That is, we seek an Hk2\nR13N1\u0002N2so that\njSBkj\u0019jSAkjH (12)\nFor completeness, we brieﬂy re-state the iterations in\nDriedger’s technique [7]. For Literations total, at the `th\niteration, compute the following 4 updates in order. First,\nwe restrict the number of repeated activations by ﬁltering\nin a maximum horizontal neighborhood in time\nR(`)\nkm=(\nH(`)\nkmifH(`)\nkm=\u0016r;(`)\nkm\nH(`)\nkm(1\u0000n+1\nN) otherwise)\n(13)\nwhere\u0016r;(`)\nkmholds the maximum in a neighborhood\nHk;m\u0000r:m+rfor some parameter r(we choose r= 3 in\nour examples). Next, we restrict the number of simultane-\nous activations by shrinking all of the values in each col-\numn that are less than the top pvalues in that column:\nP(`)\nkm=(\nR(`)\nkmifR(`)\nkm\u0015Mp(n)\nR(`)\nkm(1\u0000n+1\nN) otherwise)\n(14)\nwhere Mp(`)is a row vector holding the pthlargest\nvalue of each column of R(`)(we choose p= 10 in our\nexamples). After this, we promote time-continuous struc-\ntures by convolving along all of the diagonals of H:\nC(`)\nkm=cX\ni=\u0000cP(`)\n(k+i);(m+i)(15)\nWe choosec= 3in our examples. Finally, we perform\nthe ordinary KL-based NMF update:\nH(`+1) H(`)\fjSAkjTjSBkj\njSAkjC(`)\njSAkj\u00011(16)\nWe perform 100 such iterations ( L= 100 ). Once we\nhave our ﬁnal H, we can use this to create B0\nkas follows:\nSB0\nk=SA0\nkH (17)In other words, we use the learned activations to cre-\nateSBkusing SAk, but we instead use these activations\nwith the dictionary from SA0k.This is the key step in\nthe style transfer , and it is done for the same reason that\nH1is shared between AandA0. Figure 5 shows an ex-\nample for the guitar track on Michael Jackson’s “Bad,”\ntranslating to Alien Ant Farm using AandA0templates\nfrom Michael Jackson’s and Alien Ant Farms’ versions of\n“Smooth Criminal,” respectively. It is visually apparent\nthatjSAkjH\u0019SBk, it is also apparent that SB0\nk=SA0\nkH\nis similar to SBk, except it has more energy in the higher\nfrequencies. This is consistent with the fact that Alien Ant\nFarm uses a more distorted electric guitar, which has more\nbroadband energy.\nFigure 5 . Driedger’s technique [7] using audio grains from\npitch shifted versions of Ak(thekthtrack in Michael Jack-\nson’s “Smooth Criminal”) to create Bk(thekthtrack in\nMichael Jackson’s “Bad”), and using the activations to\ncreateB0\nk(thekthsynthesized track in Alien Ant Farm’s\n“Bad”).\nTo create our ﬁnal B0, we simply add all of the STFTs\nSB0\nktogether for each k, and we perform and inverse STFT\nto go back to audio.\n2.4 A Note About Tempos\nThe algorithm we have described so far assumes that the\ntempostA;tA0, andtBofA,A0, andB, respectively are\nsimilar. This is certainly not true in more interesting cov-\ners. Section 2.1 took care of the disparity between Aand\nA0during the synchronization. However, we also need to\nperform a tempo scaling on Bby a factor of tA=tBbe-\nfore running our algorithm. Once we have computed B0,\nwhose tempo is initially tA, we scale its tempo back by\n(tB=tA)\u0001(tA0=tA). For instance, suppose that tA= 60 ,\ntA0= 80 , andtB= 120 . Then the ﬁnal tempo of B0will\nbe60\u0002(120=60)\u0002(80=60) = 160 bpm.\n3. EXPERIMENTAL EXAMPLES\nWe now qualitatively explore our technique on several ex-\namples3. In all of our examples, we use K= 3 sources.\n3Please listen to our results at http://www.covers1000.net/\nanalogies.htmlProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 201This is a hyperparameter that should be chosen with care\nin general, since we would like each component to cor-\nrespond to a single instrument or group of related instru-\nments. However, our initial examples are simple enough\nthat we expect basically a harmonic source, a percussive\nsource, and one other source or sub-separation between ei-\nther harmonic and percussive.\nFirst, we follow through on the example we have been\nexploring and we synthesize Alien Ant Farm’s “Bad” from\nMichael Jackson’s “Bad” ( B), using “Smooth Criminal”\nas an example. The translation of the guitar from synth\nto electric is clearly audible in the ﬁnal result. Further-\nmore, a track which was exclusively drums in Aincluded\nsome extra screams in A0that Alien Ant Farm performed\nas embellishments. These embellishments transferred over\ntoB0in the “Bad” example, further reinforcing Alien Ant\nFarm’s style. Note that these screams would not have been\npreserved had we simply used inverted the CQT \u0003W2;H2,\nbut they are present in one of the ﬁltered tracks and avail-\nable as audio grains during musaicing for that track.\nIn addition to the “Bad” example, we also synthesize\nAlien Ant Farm’s version of “Wanna Be Startin Some-\nthing,” using “Smooth Criminal” as an example for Aand\nA0once again. In this example, Alien Ant Farm’s screams\noccur consistently with the fast drum beat every measure.\nFinally, we explore an example with a more extreme\ntempo shift between AandA0(tA0< tA). We use Mari-\nlyn Manson’s cover of “Sweet Dreams” by the Eurythmics\nto synthesize a Marilyn Manson cover of “Who’s That\nGirl” by the Eurythmics. We found that in this particular\nexample, we obtained better results when we performed\n2DNMF onjCAjby itself ﬁrst, and then we performed the\noptimizations in Equation 5 and Equation 6, holding W1\nﬁxed. This is a minor tweak that can be left to the discre-\ntion of the user at runtime.\nOverall, our technique works well for instrument trans-\nlation in these three examples. However, we did notice that\nthe vocals did not carry over at all in any of them. This\nis to be expected, since singing voice separation often as-\nsumes that the instruments are low rank and the voice is\nhigh rank [21], and our ﬁlters and ﬁnal mosaicing are both\nderived from low rank NMF models.\n4. DISCUSSION / FUTURE DIRECTIONS\nIn this work, we demonstrated a proof of concept, fully\nautomated end to end system which can synthesize a cover\nsong snippet of a song Bgiven an example cover AandA0,\nwhereAis by the same artist as B, andB0should sound\nlikeBbut in the style of A0. We showed some promising\ninitial results on a few examples, which is, to our knowl-\nedge, one of the ﬁrst steps in the challenging direction of\nautomatic polyphonic audio musaicing.\nOur technique does have some limitations, however.\nSince we use W1for bothAandB, we are limited to\nexamples in which AandBhave similar instruments. It\nwould be interesting to explore how far one could push\nthis technique with different instruments between Aand\nB, which happens quite often even within a corpus by thesame artist.\nWe have also noticed that in addition to singing voice,\nother “high rank” instruments, such as the ﬁddle, cannot be\nproperly translated. We believe that that translating such\ninstruments and voices would be an interesting and chal-\nlenging future direction of research, and it would likely\nneed a completely different approach to the one we pre-\nsented here.\nFinally, out of the three main steps of our pipeline, syn-\nchronization (Section 2.1), blind joint factorization/source\nseparation (Section 2.2), and ﬁltering/musaicing (Sec-\ntion 2.3), the weakest step by far is the blind source separa-\ntion. The single channel source separation problem is still\nfar from solved in general even without the complication of\ncover songs, so that will likely remain the weakest step for\nsome time. If one has access to the unmixed studio tracks\nforA,A0, andB, though, that step can be entirely cir-\ncumvented; the algorithm would remain the same, and one\nwould expect higher quality results. Unfortunately, such\ntracks are difﬁcult to ﬁnd in general for those who do not\nwork in a music studio, which is why blind source separa-\ntion also remains an important problem in its own right.\n5. ACKNOWLEDGEMENTS\nChristopher Tralie was partially supported by an NSF big\ndata grant DKA-1447491 and an NSF Research Training\nGrant NSF-DMS 1045133. We also thank Brian McFee\nfor helpful discussions about invertible CQTs.\n6. REFERENCES\n[1] Shunya Ariga, Satoru Fukayama, and Masataka Goto.\nSong2guitar: A difﬁculty-aware arrangement system\nfor generating guitar solor covers from polyphonic au-\ndio of popular music. In 18th International Society for\nMusic Information Retrieval (ISMIR) , 2017.\n[2] Judith C Brown. Calculation of a constant q spectral\ntransform. The Journal of the Acoustical Society of\nAmerica , 89(1):425–434, 1991.\n[3] C Cannam. Rubber band library. Software released\nunder GNU General Public License (version 1.8. 1) ,\n2012.\n[4] Ning Chen, Wei Li, and Haidong Xiao. Fusing similar-\nity functions for cover song identiﬁcation. Multimedia\nTools and Applications , pages 1–24, 2017.\n[5] Christian Dittmar and Meinard M ¨uller. Reverse En-\ngineering the Amen Break – Score-informed Sepa-\nration and Restoration applied to Drum Recordings.\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , 24(9):1531–1543, 2016.\n[6] Jonathan Driedger, Meinard M ¨uller, and Sascha Disch.\nExtending harmonic-percussive separation of audio\nsignals. In ISMIR , pages 611–616, 2014.202 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[7] Jonathan Driedger, Thomas Pr ¨atzlich, and Meinard\nM¨uller. Let it bee-towards nmf-inspired audio mosaic-\ning. In ISMIR , pages 350–356, 2015.\n[8] Daniel PW Ellis. Beat tracking by dynamic program-\nming. Journal of New Music Research , 36(1):51–60,\n2007.\n[9] Hadrien Foroughmand and Geoffroy Peeters. Multi-\nsource musaicing using non-negative matrix factor 2-d\ndeconvolution. In 18th International Society for Music\nInformation Retrieval (ISMIR), Late Breaking Session ,\n2017.\n[10] Leon A Gatys, Alexander S Ecker, and Matthias\nBethge. A neural algorithm of artistic style. arXiv\npreprint arXiv:1508.06576 , 2015.\n[11] Daniel Grifﬁn and Jae Lim. Signal estimation from\nmodiﬁed short-time fourier transform. IEEE Transac-\ntions on Acoustics, Speech, and Signal Processing ,\n32(2):236–243, 1984.\n[12] Aaron Hertzmann, Charles E Jacobs, Nuria Oliver,\nBrian Curless, and David H Salesin. Image analogies.\nInProceedings of the 28th annual conference on Com-\nputer graphics and interactive techniques , pages 327–\n340. ACM, 2001.\n[13] Florian Krebs, Sebastian B ¨ock, and Gerhard Widmer.\nAn efﬁcient state-space model for joint tempo and me-\nter tracking. In ISMIR , pages 72–78, 2015.\n[14] Tomohiko Nakamura and Hirokazu Kameoka. Shifted\nand convolutive source-ﬁlter non-negative matrix fac-\ntorization for monaural audio source separation. In\nAcoustics, Speech and Signal Processing (ICASSP),\n2016 IEEE International Conference on , pages 489–\n493. IEEE, 2016.\n[15] Mikkel N Schmidt and Morten Mørup. Nonnegative\nmatrix factor 2-d deconvolution for blind single chan-\nnel source separation. In International Conference on\nIndependent Component Analysis and Signal Separa-\ntion, pages 700–707. Springer, 2006.\n[16] Diemo Schwarz, Roland Cahen, and Sam Britton. Prin-\nciples and applications of interactive corpus-based con-\ncatenative synthesis. In Journ ´ees d’Informatique Musi-\ncale (JIM) , pages 1–1, 2008.\n[17] Joan Serra, Xavier Serra, and Ralph G Andrzejak.\nCross recurrence quantiﬁcation for cover song identi-\nﬁcation. New Journal of Physics , 11(9):093017, 2009.\n[18] Diego F Silva, Chin-Chin M Yeh, Gustavo Enrique de\nAlmeida Prado Alves Batista, Eamonn Keogh, et al.\nSimple: assessing music similarity using subsequences\njoins. In International Society for Music Information\nRetrieval Conference, XVII . International Society for\nMusic Information Retrieval-ISMIR, 2016.[19] Ian Simon, Sumit Basu, David Salesin, and Maneesh\nAgrawala. Audio analogies: Creating new music from\nan existing performance by concatenative synthesis. In\nICMC . Citeseer, 2005.\n[20] Temple F Smith and Michael S Waterman. Identiﬁca-\ntion of common molecular subsequences. Journal of\nmolecular biology , 147(1):195–197, 1981.\n[21] Pablo Sprechmann, Alexander M Bronstein, and\nGuillermo Sapiro. Real-time online singing voice sepa-\nration from monaural recordings using robust low-rank\nmodeling. In ISMIR , pages 67–72, 2012.\n[22] Robert W Sumner and Jovan Popovi ´c. Deformation\ntransfer for triangle meshes. In ACM Transactions on\nGraphics (TOG) , volume 23, pages 399–405. ACM,\n2004.\n[23] Christopher J Tralie. Mfcc and hpcp fusion for robust\ncover song identiﬁcation. In 18th International Society\nfor Music Information Retrieval (ISMIR) , 2017.\n[24] Gino Angelo Velasco, Nicki Holighaus, Monika\nD¨orﬂer, and Thomas Grill. Constructing an invertible\nconstant-q transform with non-stationary gabor frames.\nProceedings of DAFX11, Paris , pages 93–99, 2011.\n[25] Bo Wang, Jiayan Jiang, Wei Wang, Zhi-Hua Zhou, and\nZhuowen Tu. Unsupervised metric fusion by cross dif-\nfusion. In Computer Vision and Pattern Recognition\n(CVPR), 2012 IEEE Conference on , pages 2997–3004.\nIEEE, 2012.\n[26] Bo Wang, Aziz M Mezlini, Feyyaz Demir, Marc Fi-\nume, Zhuowen Tu, Michael Brudno, Benjamin Haibe-\nKains, and Anna Goldenberg. Similarity network fu-\nsion for aggregating data types on a genomic scale. Na-\nture methods , 11(3):333–337, 2014.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 203"
    },
    {
        "title": "Listener Anonymizer: Camouflaging Play Logs to Preserve User&apos;s Demographic Anonymity.",
        "author": [
            "Kosetsu Tsukuda",
            "Satoru Fukayama",
            "Masataka Goto"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492509",
        "url": "https://doi.org/10.5281/zenodo.1492509",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/78_Paper.pdf",
        "abstract": "When a user signs up with an online music service, she is often requested to register her demographic attributes such as age, gender, and nationality. Even if she does not input such information, it has been reported that user attributes can be predicted with high accuracy by using her play log. How can users enjoy music when using an online music service while preserving their demographic anonymity? To solve this problem, we propose a system called Listener Anonymizer. Listener Anonymizer monitors the user's play log. When it detects that her confidential attributes can be predicted, it selects songs that can decrease the prediction accuracy and recommends them to her. The user can camouflage her play logs by playing these songs to preserve her demographic anonymity. Since such songs do not always match her music taste, selecting as few songs as possible that can effectively anonymize her attributes is required. Listener Anonymizer realizes this by selecting songs based on feature ablation analysis. Our experimental results using Last.fm play logs showed that Listener Anonymizer was able to preserve anonymity with fewer songs than a method that randomly selected songs.",
        "zenodo_id": 1492509,
        "dblp_key": "conf/ismir/TsukudaFG18",
        "keywords": [
            "demographic attributes",
            "play log",
            "user attributes",
            "online music service",
            "demographic anonymity",
            "Listener Anonymizer",
            "user play logs",
            "music taste",
            "feature ablation analysis",
            "Last.fm play logs"
        ],
        "content": "LISTENER ANONYMIZER: CAMOUFLAGING PLAY LOGS\nTO PRESERVE USER’S DEMOGRAPHIC ANONYMITY\nKosetsu Tsukuda Satoru Fukayama Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST), Japan\nfk.tsukuda, s.fukayama, m.gotog@aist.go.jp\nABSTRACT\nWhen a user signs up with an online music service, she\nis often requested to register her demographic attributes\nsuch as age, gender, and nationality. Even if she does\nnot input such information, it has been reported that user\nattributes can be predicted with high accuracy by using\nher play log. How can users enjoy music when using an\nonline music service while preserving their demographic\nanonymity? To solve this problem, we propose a system\ncalled Listener Anonymizer. Listener Anonymizer moni-\ntors the user’s play log. When it detects that her conﬁ-\ndential attributes can be predicted, it selects songs that can\ndecrease the prediction accuracy and recommends them to\nher. The user can camouﬂage her play logs by playing\nthese songs to preserve her demographic anonymity. Since\nsuch songs do not always match her music taste, selecting\nas few songs as possible that can effectively anonymize her\nattributes is required. Listener Anonymizer realizes this\nby selecting songs based on feature ablation analysis. Our\nexperimental results using Last.fm play logs showed that\nListener Anonymizer was able to preserve anonymity with\nfewer songs than a method that randomly selected songs.\n1. INTRODUCTION\nWhen a user signs up with an online music service (e.g.,\nLast.fm1and Spotify2), it is common for the user to be\nasked to input her demographic attributes such as age and\ngender. Registering such demographic attributes is bene-\nﬁcial for her because various songs are recommended to\nher by the service according to her attributes. In addition,\nshe can follow another user who has similar demographic\nattributes, and they can communicate with each other. De-\nspite such beneﬁts, many users conceal their demographic\nattributes because they would be concerned about privacy.\nAs shown in Section 5.1, as many as 49.3% of Last.fm\nusers do not register any of the age, gender, and national-\nity attributes. If a user does not register her demographic\nattributes, is her privacy fully protected?\nSeveral studies have aimed to predict users’ demo-\ngraphic attributes from their music play logs [10, 12, 25].\n1http://www.last.fm\n2http://www.spotify.com\nc\rKosetsu Tsukuda, Satoru Fukayama, Masataka Goto.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: Kosetsu Tsukuda, Satoru Fukayama,\nMasataka Goto. “Listener Anonymizer: Camouﬂaging Play Logs\nto Preserve User’s Demographic Anonymity”, 19th International Society\nfor Music Information Retrieval Conference, Paris, France, 2018.They have tackled the problem because it had been re-\nported that the attributes contribute to improving music\nrecommendation accuracy [21, 23, 27]. However, users\nwho do not register their attributes might not want re-\nsearchers or companies to predict their demographic at-\ntributes. It is not only a psychological problem; if a user’s\ndemographic attributes are predicted, she may suffer dam-\nage. For example, suppose one day the email address and\nmusic play logs of a female user who has not input her gen-\nder on an online music service are leaked from the website,\nand a malicious company obtains the data. If the malicious\ncompany can predict her gender with high accuracy from\nthe logs, it can send her spam e-mails that target females.\nWhat should we do to enable users to enjoy music when\nusing an online music service while preserving their de-\nmographic anonymity? In this paper, we propose a system\ncalled Listener Anonymizer to solve this problem. Listener\nAnonymizer camouﬂages the user’s play log and preserves\nthe anonymity of her conﬁdential attributes. To be more\nspeciﬁc, when a user plays a song using an online music\nservice, Listener Anonymizer monitors the songs that are\nplayed. If Listener Anonymizer detects that the user’s con-\nﬁdential attributes can be predicted with an accuracy above\na certain level, the system selects songs that can decrease\nthe prediction accuracy and recommends them to her. The\nuser can camouﬂage her play log by playing them and pre-\nserve her anonymity. However, since such selected songs\ndo not always match her music taste, selecting as few songs\nas possible that can effectively anonymize her attributes is\nrequired. To achieve this, we propose a method for select-\ning songs according to the user’s conﬁdential attributes.\nOur contributions in this paper are as follows.\n\u000fTo the best of our knowledge, this is the ﬁrst\nstudy that introduces the concept of preserving\nthe anonymity of the users’ demographic attributes\nwhile they play songs using an online music service.\n\u000fWe propose an approach that camouﬂages the user’s\nplay log to preserve her anonymity. We marshaled\nfactors to consider for selecting songs from ﬁve\nviewpoints: the deﬁnition of anonymity, method for\npredicting demographics, timing of camouﬂaging\nplay logs, user’s true demographics, and anonymiza-\ntion of multiple demographics.\n\u000fTo examine the effectiveness of the proposed\nmethod, we carried out experiments using Last.fm\nplay logs. Our experimental results showed that our\nproposed method was able to preserve anonymity\nwith fewer songs than a method that randomly se-\nlected songs. Based on the experiments, we dis-687cuss four important considerations: impact on rec-\nommendation accuracy, user’s taste in music, simu-\nlation of multiple prediction methods, and real-time\nmonitoring of songs that are played.\n2. RELATED WORK\nSince predicting the user’s demographic attributes can be\nused in many applications such as content recommendation\nand user behavior analysis, studies of demographic predic-\ntion have been conducted in various domains. One of the\nmost popular domains is social media such as Twitter3and\nFacebook4. It is known that language use on social media\nvaries according to demographic attributes such as age [9]\nand gender [7]. Hence, most studies have used text data\nposted to social media and utilized machine learning tech-\nniques to predict users’ demographic attributes [15,18,24].\nAlthough it was thought that predicting demographics was\na difﬁcult task [15, 16], recent studies reported a high pre-\ndiction accuracy. For example, age and gender can be pre-\ndicted with mean absolute error (MAE) of 3.40 and a bi-\nnary classiﬁcation accuracy of 91.9%, respectively [18].\nIn addition to social media, demographic prediction has\nbeen conducted in the ﬁelds of blogs [2,5] and web search\nqueries [8].\nIn the ﬁeld of music information retrieval (MIR), too,\nusers’ demographic attributes on an online music service\nplay an important role mainly for music recommendation.\nAs reported by Uitdenbogerd and Schnydel [22], music\npreference is affected by individual factors including age\nand ethnicity. In fact, it was revealed that music recom-\nmendation accuracy was improved by considering demo-\ngraphic attributes [21, 23, 27]. Motivated by these results,\nseveral studies in MIR have aimed to predict demographic\nattributes. The main way to perform this task is to use\nplay logs obtained from an online music service and su-\npervised machine learning techniques. Liu and Yang [12]\npredicted age and gender by using timestamps, song/artist\nmetadata, and acoustic features of music signals. Wu et\nal.[25] also proposed methods to predict age and gender\nbased on music metadata. They created two kinds of fea-\ntures: a TF-IDF-based one and a GSV(Gaussian super vec-\ntor [3])-based one. They applied support vector machine\n(SVM) to them. Krismayer et al. [10] predicted national-\nity in addition to age and gender based on music metadata\n(artist names and artist’s tags). The details of their method\nare described in Section 4.2. By using their method, it was\nreported that demographic attributes can be predicted with\nhigh accuracy. The age was predicted with MAE of 4.13,\nand the gender and nationality were predicted with a clas-\nsiﬁcation accuracy of 81.36% and 69.37%, respectively.\nUnlike these studies, our goal is to preserve users’ de-\nmographic anonymity since some users do not want re-\nsearchers or companies to predict their demographic at-\ntributes. Although several studies have discussed privacy\nproblems (e.g., the release of a user query log can lead\nto loss of privacy [8], conﬁdential information such as\nmedical conditions can be inferred from tweets [14], and\n3https://twitter.com\n4https://facebook.comhow should researchers deal with personal information in\nMIR [19]), our study is different from these studies in that\nwe propose a concrete anonymization system and carried\nout experiments to evaluate how well it works.\n3. FACTORS FOR REALIZING LISTENER\nANONYMIZER\nAs we described in Section 1, we propose an approach that\nselects songs and camouﬂages the user’s play log by play-\ning these songs so that the user can preserve demographic\nanonymity. To enable an intuitive understanding of our\nidea, we give the following example story.\nEmma is a 22-year-old French female. She is a Last.fm\nuser and concealed her nationality when she signed up.\nShe also uses Listener Anonymizer, which monitors the\nmusic she plays using Last.fm. One day, when Emma is\nlistening to music using Last.fm with her smartphone, Lis-\ntener Anonymizer detects that her nationality can be pre-\ndicted as French with high accuracy from her play logs.\nThus, Listener Anonymizer shows an alert message stating\n“your nationality can be predicted as French with a proba-\nbility of 67%” on her smartphone screen and recommends\nthree songs to her. Emma plays the songs to preserve the\nanonymity of her nationality.\nAlthough this is just an example story, we need to con-\nsider various factors to realize Listener Anonymizer. Be-\nlow, we marshal the factors from ﬁve viewpoints.\n3.1 Deﬁnition of Anonymity\nFirst, we deﬁne the anonymity of demographic attributes.\nIn this paper, we propose two kinds of concepts for\nanonymity: not-ﬁrst-anonymity andk-ﬂat-anonymity. Sup-\npose a demographic attribute dhasnattribute values rep-\nresented by Ad=fa1;a2;\u0001\u0001\u0001;ang. For example, when\ndis nationality, ai2Adcan be French, Japanese, etc.\nWhen useruhas an attribute value au2Adand conceals\nthe attribute, given her music play log, we can compute\nthe probability p(ai)for each attribute value in Adby us-\ning an attribute prediction method (0 \u0014p(ai)\u00141andPn\ni=1p(ai) = 1). In this case, not-ﬁrst-anonymity is sat-\nisﬁed if the following condition is met: the rank of p(au)\namong all attribute values is not the highest. In the case of\nEmma, not-ﬁrst-anonymity is satisﬁed when the probabil-\nity of French is not the highest.\nIn the case of k-ﬂat-anonymity, the anonymity is sat-\nisﬁed if the following condition is met. Given the top k\nattribute values in terms of the probability, auis included\nin the topkattribute values and the probability gap be-\ntween any two attribute values is lower than \u0012. Ink-ﬂat-\nanonymity, user’s demographic attributes may be regarded\nas unpredictable because the top kattribute values have\nalmost the same probabilities. In the example of Emma,\nsupposekand\u0012are set to 3 and 0.05, respectively, and\nthe probabilities of French, Spanish, and German are 0.32,\n0.28, and 0.29, respectively. In this case, because the prob-\nabilities of other nationalities are lower than those of the\nthree nationalities and the probability gap between any\ntwo nationalities out of the three nationalities is lower than\n0.05,k-ﬂat-anonymity is satisﬁed even though French has\nthe highest probability.688 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20183.2 Method for Predicting Demographics\nTo realize Listener Anonymizer, simulating the method\nused in a demographics prediction system is required so\nthat we can show an alert at the right time. However, since\nit is generally impossible to know the prediction method,\nwe have to assume some prediction methods and propose a\nsong selection method according to them. It is common to\nuse music metadata extracted from the user’s play log for\npredicting demographic attributes [10, 12, 25]. If we pro-\npose a song selection method that works well for state-of-\nthe-art methods that are based on music metadata, we can\nsay that our proposed method is robust to a certain extent.\nIn light of the above, in this paper, we propose a method for\nselecting songs in Section 4.3 and show the effectiveness\nof this method through experiments in Section 5.\n3.3 Timing of Camouﬂaging Play Logs\nListener Anonymizer selects songs and camouﬂages play\nlogs in two main situations. One is when not-ﬁrst-\nanonymity (or k-ﬂat-anonymity) is no longer satisﬁed as\nwe described in the example story at the beginning of this\nsection. The other is when a user does not use a smart-\nphone such as when she is sleeping or taking a bath. In the\nformer case, since the user listens to her favorite songs be-\nfore the songs are recommended by Listener Anonymizer,\nit should select as few songs as possible so that the user can\nsoon resume listening to her favorite songs. In the latter\ncase, since the user has enough time to play recommended\nsongs and does not need to listen to them, a method that\nrandomly selects many songs might be enough for recov-\nering not-ﬁrst-anonymity. Some users still hope to play as\nfew recommended songs as possible to save on the packet\ncommunication fee.\n3.4 User’s True Demographics\nIn the preceding sections, we assumed that our anonymiza-\ntion system knows the user’s true attribute values (e.g.,\nEmma’s nationality is French). That is, the user has to in-\nput the true demographic attributes before starting to use\nListener Anonymizer. However, some users would not\nwant to tell even the system their true demographics. When\nthe system does not know the user’s true demographic\nattribute, not-ﬁrst-anonymity can be deﬁned as follows:\nwhen the difference between the highest probability and\nthe second highest probability is lower than \u0012. In this case,\nnot-ﬁrst-anonymity will not often be satisﬁed, and songs\nwill be more frequently recommended to the user than the\ncase where the system knows the true demographic at-\ntribute. In the example of Emma, suppose she does not\ntell her nationality to Listener Anonymizer. If she wants to\npreserve complete anonymity, she must play recommended\nsongs at every alert, but this is a heavy burden for her. She\ncould ignore an alert if the predicted nationality is wrong.\nHowever, if she plays only the recommended songs when\nthe predicted nationality is French, Listener Anonymizer\ncan estimate that Emma’s nationality is French.\nWhen the anonymization system knows a user’s true\nattribute, the alert is displayed only when the true demo-\ngraphic can be predicted, which reduces the user’s burden.\nIn addition, if we can implement Listener Anonymizer as astand-alone smartphone application, the user’s true demo-\ngraphics are stored only in the smartphone and are not sent\nto a server. In this case, users do not need to worry about\nleakage of demographic information from the server.\n3.5 Anonymization of Multiple Demographics\nWe need to consider a situation where a user wants to pre-\nserve anonymity of more than one demographic attribute.\nFor example, in the example of Emma, she anonymized\nonly her nationality; now suppose she did not register her\nnationality, age, and gender on Last.fm. She may think that\nit does not matter if her age is predicted but may think it is a\nbig problem if her nationality and gender are predicted. In\nsuch a case, she tells Listener Anonymizer the two demo-\ngraphic attributes that she wants to preserve the anonymity\nof. The system shows an alert and recommends songs\nwhen at least one demographic does not satisfy not-ﬁrst-\nanonymity. If more than one demographic attribute does\nnot satisfy not-ﬁrst-anonymity at the same time, the system\nneeds to select songs that can recover not-ﬁrst-anonymity\nfor all of the demographic attributes by playing recom-\nmended the songs. When a user tells the system many de-\nmographics that she wants to preserve the anonymity of,\nalerts may frequently be displayed, and this makes it difﬁ-\ncult for the user to enjoy listening to her favorite songs.\nTherefore, the user has to select demographic attributes\nthat she really wants to preserve the anonymity of.\n4. CAMOUFLAGING PLAY LOGS\nIn Section 3, we described various factors to be considered\nto realize Listener Anonymizer. In this section, based on\nthese factors, we discuss the situation dealt with in this pa-\nper, give the problem deﬁnition, and propose a method for\nselecting songs for camouﬂaging play logs.\n4.1 Problem Deﬁnition\nIn terms of the type of anonymity, we use not-ﬁrst-\nanonymity because of its simplicity. If we can show the ef-\nfectiveness of our proposed method in not-ﬁrst-anonymity,\nwe will deal with k-ﬂat-anonymity in future work. As for\nthe timing of selecting songs and camouﬂaging play logs,\nwe camouﬂage the user’s play log with as few songs as\npossible. That is, given user u’s play log Luthat con-\nsists ofmsongs (L u=fs1;s2;\u0001\u0001\u0001;smgwheresirep-\nresents a song), we aim to anonymize u’s conﬁdential de-\nmographic attribute by selecting as few songs as possi-\nble. We assume our system knows the user’s true de-\nmographic attributes. This assumption is reasonable be-\ncause users will not hesitate to tell their demographics to\nthe system if it is implemented as a stand-alone applica-\ntion as we described in Section 3.4. Finally, for preserving\nthe anonymity of multiple demographics, since this paper\ndeals with a new research problem, we consider single de-\nmographic anonymity as a ﬁrst step. We are fully aware\nof the issue of multiple demographic anonymity; we leave\nthis for future work.\nBased on the above assumptions, our problem is deﬁned\nas follows: “User uconceals an attribute value auin de-\nmographicdand wants to preserve not-ﬁrst-anonymity re-\ngardingau. Givenu’s play log consisting of msongs, weProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 689verify ifausatisﬁes not-ﬁrst-anonymity. If it does not, we\nselect as few songs as possible so that aucan satisfy not-\nﬁrst-anonymity by playing them.”\n4.2 Demographic Prediction Method\nTo the best of our knowledge, the state-of-the-art method\nfor prediction of users’ demographic attributes of an on-\nline music service is the method proposed by Krismayer\net al. [10]. They proposed a feature modeling approach.\nMore speciﬁcally, given the users’ play logs in the train-\ning data, they extract an artist name and the artist’s tags\nas features for each song in each user’s play log. Here,\nonly the top 10,000 artists and top 10,000 tags in terms of\nthe popularity in the training data are used to create fea-\nture vectors. They compute the weight for each feature in\nthe form of TF-IDF values and create a feature vector for\neach user. The feature vectors, each of which has 20,000\ndimensions, are reduced to 500 dimensions by principal\ncomponent analysis (PCA) [6]. Finally, the classiﬁer is\nbuilt by using SVM. Since their evaluation results showed\nthat the polynomial kernel achieved high prediction accu-\nracy on average, we assume that using the SVM with the\npolynomial kernel is the state-of-the-art method. More de-\ntails can be found in Krismayer et al. [10]. Once a classiﬁer\nis built, given a user’s play log, the classiﬁer computes the\nprobability distribution over demographic attribute values\nand outputs the attribute value that has the highest prob-\nability as the user’s predicted attribute value. We imple-\nmented this prediction method by ourselves with reference\nto Krismayer et al. [10].\n4.3 Song Selection Method\nWhen Emma’s nationality is predicted as French, Listener\nAnonymizer needs to select as few songs as possible that\ncan anonymize her nationality. To achieve this, we aim\nto ﬁnd songs that can largely increase the probability of\nthe second-highest nationality (in this example, suppose\nItalian has the second highest probability). Since the fea-\nture vector corresponding to a song is compressed and\nthe compressed vector is projected onto a new coordinate\nspace by a polynomial kernel of SVM, it is difﬁcult to\nﬁnd such songs based on the characteristics of an orig-\ninal 20,000-dimension vector. Instead, we assume such\nsongs are played by users who are in the training dataset\nand are classiﬁed as Italian with high probabilities. From\nthese users’ play logs, we extract effective songs by using\nfeature ablation analysis [1]. More formally, given Lu, we\nﬁrst compute the probability distribution over nattribute\nvalues by using the method in Section 4.2. If p(au)is not\nthe highest among them, we do not need to do anything. If\np(au)is the highest, we select songs as follows.\nSupposeaj(6=au)has the second-highest probability\nafterau. LetU=fut\n1;ut\n2;\u0001\u0001\u0001;ut\nqgbe a set of users in\nthe training data. By developing the SVM classiﬁer, user\nut\ni2Uhas the probability p(aj;ut\ni)that represents the\nprobability of ut\nionaj. From all users in U, we collect the\ntoprusers in terms of p(aj;ut\ni). Each user has her play\nlog that consists of msongs. Suppose we remove the lth\nsong fromut\ni’s play log and compute the new probability\nofp(aj;ut\ni)by applying the SVM to the remaining m\u00001Table 1. Percentage of users who anonymize their demo-\ngraphic attributes. “X” represents anonymization.\nAge Gender Nationality No.\nof users %\nX X X 59,350 49.3\nX X 2,345 1.95\nX X 2,713 2.25\nX X 454 0.377\nX 9,794 8.14\nX 2,402 2.00\nX 2,615 2.17\n4,0649 33.8\nsongs (let the new probability be p0(aj;ut\ni)). If the score\nofp(aj;ut\ni)\u0000p0(aj;ut\ni)is large, we assume that the lth\nsong is essential to increase the probability of aj. Based\non this idea, we compute the score for each of the r\u0002m\nsongs and collect the top ccorresponding artists based on\nthe score. After collecting the top cartists, we randomly\nselect one artist; then we randomly select one song of the\nartist’s songs. By adding the song to Lu, we generate a\ncamouﬂaged play log consisting of m+ 1songs. We re-\npeatedly select a song and add it to Luuntil the camou-\nﬂaged play log satisﬁes not-ﬁrst-anonymity.\n5. EXPERIMENTS\nIn this section, we carry out experiments to evaluate the\neffectiveness of our proposed method.\n5.1 Dataset\nWe used the Last.fm dataset provided by Schedl [20]. As\nfor the user’s demographic attributes, this dataset includes\nage, gender, and nationality. Table 1 shows the num-\nbers of users and the percentages for each of the combi-\nnations of conﬁdential attributes, where “X” indicates a\nconﬁdential attribute. It can be observed that as many as\n49.3% of users do not register any of their attributes, and\n66.2% of them conceal at least one attribute. These statis-\ntics suggest the importance of preserving the user’s demo-\ngraphic anonymity, though there might be other reasons.\nThe dataset also includes users’ play logs, each of which\nconsists of the user ID, artist ID, track ID, and timestamp.\nFollowing Krismayer et al. [10], we selected users who\nregistered all of the three attributes, had equal to or more\nthan 500 play logs, and had a nationality that was one of\nthe 25 most common nationalities in terms of the number\nof users in the dataset. This gave us 32,991 users. We used\n70% of them as training data and developed an SVM clas-\nsiﬁer. The remaining 30% of them were used as test data.\nArtists’ tags were collected by using the Last.fm API5.\nThe number of classes of each attribute is as follows. The\nnationality consists of 25 classes that correspond to the top\n25 most common nationalities, the gender has two classes\nthat are male and female, and following Schedl et al. [21],\nthe age was divided into seven age groups ([6 - 17], [18 -\n21], [22 - 25], [26 - 30], [31 - 40], [41 - 50], and [51 - 60]).\n5.2 Methods Comparison\n5.2.1 Settings\nOur ﬁrst research question is “Is our proposed method able\nto camouﬂage play logs with fewer songs than a base-\n5https://www.last.fm/api690 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018line method that randomly selects songs?” To answer this\nquestion, we count the number of songs selected by each\nmethod to preserve anonymity as follows. In this evalua-\ntion, for each user in the test dataset, we use the ﬁrst 30\nsongs from the oldest songs in the play logs (i.e., m=\n30). For example, given the demographic attribute “na-\ntionality,” we ﬁrst compute each user’s probability distri-\nbution over 25 nationalities when the 30 songs are played.\nWe then sample 50 users whose nationality does not sat-\nisfy not-ﬁrst-anonymity (i.e., the user’s nationality has the\nhighest probability among 25 nationalities). Note that in\nthis case, each user’s play log in the training data also con-\nsists of 30 songs. Given a sampled user’s play log consist-\ning of 30 songs, we add a song selected by our proposed\nmethod and compute the new probability distribution for\nthe 31 songs. If the probability of the user’s nationality is\nnot the highest among 25 nationalities, it means her nation-\nality is anonymized and the song selection process ends;\notherwise, we add a new song selected by our method and\ncompute the probability distribution for the 32 songs. If\nthe user’s nationality is not anonymized even after select-\ning the additional 30 songs, we stop the song selection pro-\ncess. In this way, we count the number of selected songs\nfor all of the 50 users. In this evaluation, the values of r\nandcwere set to 3 and 1, respectively. Note that ris the\nnumber of users used for selecting candidate songs and c\nis the number of artists used for recommending songs as\nwe described in Section 4.3. The random baseline method\n(hereafter, the random method) randomly selects a song\nfrom all the songs in the dataset and counts the number of\nsongs in the same manner as described above.\nIn addition to the proposed method and the random\nmethod, we use a popularity-based baseline method (here-\nafter, the popularity method). Intuitively, in this method, if\nwe want to decrease the probability of France, for exam-\nple, we select a song that is not popular in France but is\npopular in the other 24 nationalities. To achieve this, we\nrank all artists in each nationality where an artist’s score is\nthe number of users who have listened to one of the artist’s\nsongs at least once. The artists are ranked in descending\norder of their score. When a user u’s nationality auis not\nanonymized, the popularity method ﬁrst selects an artist b\u0003\nwhere\nb\u0003= arg max\nb2BP\nai2Adnfaug(rank(a i;b)\u0000rank(a u;b))\njAdnfaugj:\nIn the equation, Bis the set of all artists and rank(a i;b)\nrepresents the rank of artist bin nationality ai. Finally, a\nsong ofb\u0003is selected and added to u’s play log.\nNote that in this evaluation, we used the same 50 users\nfor all of the three methods for a fair comparison.\n5.2.2 Results\nThe results are shown in Figure 1 where each bar repre-\nsents the average number of selected songs over 50 users.\nIt can be observed that our proposed method outperformed\nthe other two methods in all attributes. In the “gender”\nattribute, even the proposed method selected as many as\n19.68 songs on average. Since the “gender” attribute has\nonly two classes (male and female), the probability tended\n0102030\nAge Gender Na�onalityRandom Popularity ProposedFigure 1. Comparison results between three methods. The\ny-axis is the average number of selected songs for camou-\nﬂaging play logs. Error bars indicates the standard error.\nto be strongly biased to one class. Thus, we presume that\nmany songs were needed to ﬁll the large gap. In the “na-\ntionality” attribute, the proposed method was especially ef-\nfective: it selected less than one third of the songs selected\nby the random method.\nThe results of the popularity method were worse than\nthose of the random method. This is because of the com-\nplexity of the demographic prediction method as described\nin Section 4.3. These results indicate that songs selected by\nthe popularity method are rarely plotted to ideal points in\nthe coordinate space created by an SVM polynomial ker-\nnel. Moreover, in the random method, a song that largely\ndecreases the probability of the user’s conﬁdential attribute\ncan be selected by chance. Because of these reasons, the\nrandom method outperformed the popularity method.\n5.3 Parameter Effect\n5.3.1 Settings\nRemind that our method has a parameter cthat determines\nhow many artists we use from the result of the feature abla-\ntion, although we set cto 1 in Section 5.2. Our second re-\nsearch question is “What is the relation between the value\nofcin the proposed method and the number of selected\nsongs?” To answer the question, we change the value of\ncfrom 1 to 10 and count the number of selected songs for\neachc. In each of the three demographic attributes, the\nsame 50 sampled users were used for all of the cvalues.\n5.3.2 Results\nFigure 2 shows the results. In the “age” and “nationality”\nattributes, the number of selected songs decreases when c\nchanges from 1 to 2 and the number is at a minimum when\ncis 2 or 3; then the number increases with the increase\nofc. In particular, in the “age” attribute, when cis 2, only\n3.22 songs are required on average to camouﬂage play logs\nconsisting of 30 songs. In the “gender” attribute, although\nthe number of selected songs decreases when cchanges\nfrom 1 to 2, the minimum score was 9.28 when cis 10.\nFrom these results, we can say that selecting songs only\nfrom the best artist in terms of feature ablation analysis\ndoes not lead to the best result.\nIn addition to the decrease of the number of selected\nsongs for large c, the increase of chas another advan-\ntage. When cis 1, songs are always selected from one\nartist to anonymize an attribute value. This may enable\na company that wants to predict users’ demographic at-\ntributes to easily detect the camouﬂaged logs and predict\nthe true attributes by removing the camouﬂaged logs. In\ncontrast, when cis large, it becomes difﬁcult to detect theProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 6913813\n1 3 5 7 99121518\n1 3 5 7 944.555.5\n1 3 5 7 9Age Gender Na�onalityFigure 2. The relation between the value of cin the proposed method (x-axis) and the average number of selected songs\nfor camouﬂaging play logs (y-axis).\n00.10.20.30.4\nGE CZ AM00.10.20.30.4\nGE CZ AM00.10.20.30.4\nCZ GE AM\n00.20.40.60.81\nPL BR BZ00.20.40.60.81\nPL SP BR00.20.40.60.81\nPL SP BR00.20.40.60.81\nSP PL BRGE: German    PL: Polish \nCZ: Czech      BR: British\nAM: American   BZ: Brazilian\nSP: Spanish\nFigure 3. Examples of transition of probability distribu-\ntion when two songs (top) and three songs (bottom) are\nselected for camouﬂaging play logs (y-axis: probability).\ncamouﬂaged parts in play logs. Moreover, by selecting\nsongs from various artists, Listener Anonymizer may be\nable to expand the user’s music taste while preserving her\nanonymity. Figure 2 shows that even when cis 10, our\nmethod can anonymize an attribute with much fewer songs\nthan the random method in all attributes. It would be useful\nto enable a user to select the value of cwhile thinking about\na trade-off between the number of songs and the diversity\nof selected songs.\nFigure 3 shows examples of the transition of probabil-\nity distribution when two or three songs are selected by our\nproposed method in the “nationality” attribute. The value\nofcwas set to 3. For visibility, we show only the top three\nnationalities in terms of the probability. In the top exam-\nple, the user’s attribute is German. Listener Anonymizer\ncan anonymize the user’s attribute by selecting two songs.\nIn the bottom example, although the initial probability dis-\ntribution is strongly biased to the user’s nationality (PL),\nthis user can camouﬂage the play log by playing only three\nsongs recommended by Listener Anonymizer.\n6. DISCUSSION\nIn Section 5, we showed the effectiveness of the pro-\nposed method. However, since preserving demographic\nanonymity by camouﬂaging play logs is a quite new re-\nsearch theme, we discuss four important considerations.\n6.1 Impact on Recommendation Accuracy\nSince Listener Anonymizer camouﬂages play logs, it\nmight degrade recommendation accuracy of music ser-\nvices. Although this paper dared to propose this controver-\nsial topic of research to give users an option of increasing\nthe privacy and raise privacy issues in the MIR commu-\nnity, we are fully aware of the importance and usefulness of\nmusic recommendation to improve the user’s music experi-\nence. We hope that our paper could contribute to discuss a\ndiversity of options for music experiences while balancing\nprivacy versus accuracy in music recommendation.\n6.2 User’s Taste in Music\nIn our method, the selected songs do not always match her\ntaste in music. Even if those songs can camouﬂage the play\nlogs, she might be reluctant to keep listening to the songs.\nHence, it is beneﬁcial to select songs by considering theuser’s taste in music. Many studies about song recommen-\ndation [11, 26, 28] and playlist generation [4, 13, 17] that\ncan reﬂect the user’s taste in music have been conducted.\nBy introducing the methods proposed in these studies, we\nplan to propose a song selection method that can balance\ncamouﬂaging the play logs and taste. That would also be\nbeneﬁcial to satisfy both anonymization and good recom-\nmendation.\nConsidering the user’s taste has another advantage. If\nour method to camouﬂage play logs gains in popularity,\ncompanies that want to know the user’s demographic at-\ntributes will try to predict them by removing songs that\ncamouﬂage her play log. By selecting songs that match\nthe user’s taste, it becomes more difﬁcult to detect songs\nthat are played for camouﬂage.\n6.3 Simulation of Multiple Prediction Methods\nIn our experiments, we assumed that the system knew that\nthe method by Krismayer et al. [10] is used to predict the\nuser’s demographic attributes. However, we cannot always\nknow the prediction method in advance. When we do not\nknow it, one strategy is to prepare multiple possible pre-\ndiction methods and simulate them one at a time. An alert\nis issued when more than vmethods detect that the user’s\ndemographic attribute can be predicted with high accuracy.\nFor smallv, the degree of anonymity preservation is high\nbut alerts are often issued and vice versa for large v. It\nwould be useful for a user to be able to set the value of v\naccording to the degree of anonymity she requires.\n6.4 Real-time Monitoring of a Play Log\nIn our experiments, the number of songs in a given play log\nwas set to 30. Hence, all logs in training data also consisted\nof 30 songs. However, this assumption is not sufﬁcient\nto monitor the user’s played songs and recommend songs\nat the right time as we described in Section 3.3. This is\nbecause, when a user plays her ﬁrst song, there is no play\nlog in the training data consisting of only one song and\nwe cannot correctly compute the probability distribution\nfor the song. To solve this problem, we need to develop\nclassiﬁers for various values of l, wherelis the number of\nsongs included in a play log.\n7. CONCLUSION\nIn this paper, we proposed Listener Anonymizer that can\npreserve the user’s demographic anonymity by camouﬂag-\ning her play log. Our experimental results show the effec-\ntiveness of our proposed method to select as few songs as\npossible. For example, in the “age” attribute, 15.3 songs\nwere selected by the random method, while only 3.22\nsongs were selected by our method. Since this paper pro-\nposed a new concept, there are many remaining issues to\nbe addressed as we discussed in Section 3 and 6. We plan\nto tackle them one by one and make Listener Anonymizer\nmore ﬂexible and useful.692 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20188. ACKNOWLEDGMENTS\nThis work was supported in part by JST ACCEL Grant\nNumber JPMJAC1602, Japan.\n9. REFERENCES\n[1] Eugene Agichtein, Ryen W. White, Susan T. Dumais,\nand Paul N. Bennet. Search, interrupted: Understand-\ning and predicting search task continuation. In Pro-\nceedings of the 35th International ACM SIGIR Con-\nference on Research and Development in Information\nRetrieval, SIGIR’12, pages 315–324, 2012.\n[2] John D. Burger and John C. Henderson. An explo-\nration of observable features related to blogger age. In\nIn Computational Approaches to Analyzing Weblogs:\nPapers from the 2006 AAAI Spring Symposium, pages\n15–20, 2006.\n[3] William M. Campbell, Douglas E. Sturim, and Dou-\nglas A. Reynolds. Support vector machines using gmm\nsupervectors for speaker veriﬁcation. IEEE Signal Pro-\ncessing Letters, 13(5):308–311, 2006.\n[4] Arthur Flexer, Dominik Schnitzer, Martin Gasser,\nand Gerhard Widmer. Playlist generation using start\nand end songs. In Proceedings of the 9th Interna-\ntional Conference on Music Information Retrieval, IS-\nMIR’08, pages 173–178, 2008.\n[5] Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.\nStylometric analysis of bloggers’ age and gender. In\nProceedings of the 3rd International AAAI Conference\non Weblogs and Social Media, ICWSM’09.\n[6] Harold Hotelling. Analysis of a complex of statistical\nvariables with principal components. Journal of Edu-\ncational Psychology, 24:417–441 and 498–520, 1933.\n[7] David Huffaker and Sandra Calvert. Gender, iden-\ntity, and language use in teenage blogs. Journal of\nComputer-Mediated Communication, 10(2), 2005.\n[8] Rosie Jones, Ravi Kumar, Bo Pang, and Andrew\nTomkins. “I know what you did last summer”: Query\nlogs and user privacy. In Proceedings of the Sixteenth\nACM Conference on Conference on Information and\nKnowledge Management, CIKM’07, pages 909–914,\n2007.\n[9] Margaret L. Kern, Johannes C. Eichstaedt, H. An-\ndrew Schwartz, Gregory Park, Lyle H. Ungar andDavid\nJ. Stillwell, Michal Kosinski, Lukasz Dziurzynski, and\nMartin E. P. Seligman. From “sooo excited!!!” to “so-\nproud”: Using language to study development. Devel-\nopmental psychology, 50(1):178–188, 2014.\n[10] Thomas Krismayer, Markus Schedl, Peter Knees, and\nRick Rabiser. Prediction of user demographics from\nmusic listening habits. In Proceedings of the 15th Inter-\nnational Workshop on Content-Based Multimedia In-\ndexing, CBMI’17, pages 8:1–8:7, 2017.[11] Dawen Liang, Minshu Zhan, and Daniel P. W. El-\nlis. Content-aware collaborative music recommenda-\ntion using pre-trained neural networks. In Proceedings\nof the 16th International Society for Music Informa-\ntion Retrieval Conference, ISMIR’15, pages 295–301,\n2015.\n[12] Jen-Yu Liu and Yi-Hsuan Yang. Inferring personal\ntraits from music listening history. In Proceedings of\nthe 2nd International ACM Workshop on Music Infor-\nmation Retrieval with User-centered and Multimodal\nStrategies, MIRUM’12, pages 31–36, 2012.\n[13] Beth Logan. Content-based playlist generation: Ex-\nploratory experiments. In Proceedings of the 3rd Inter-\nnational Conference on Music Information Retrieval,\nISMIR’02, pages 295–296, 2002.\n[14] Huina Mao, Xin Shuai, and Apu Kapadia. Loose\ntweets: An analysis of privacy leaks on twitter. In Pro-\nceedings of the 10th Annual ACM Workshop on Pri-\nvacy in the Electronic Society, WPES’11, pages 1–12,\n2011.\n[15] Dong-Phuong Nguyen, Rilana Gravel, Rudolf Berend\nTrieschnigg, and Theo Meder. “How old do you think\nI am?”: A study of language and age in twitter. In Pro-\nceedings of the 7th International AAAI Conference on\nWeblogs and Social Media, ICWSM’13, pages 439–\n448, 2013.\n[16] Dong-Phuong Nguyen, Rudolf Berend Trieschnigg,\nA. Seza Dogruoz, Rilana Gravel, Mariet Theune, Theo\nMeder, and Franciska M.G. de Jong. Why gender and\nage prediction from tweets is hard: Lessons from a\ncrowdsourcing experiment. In Proceedings of the 25th\nInternational Conference on Computational Linguis-\ntics, COLING’14, pages 1950–1961, 2014.\n[17] Elias Pampalk, Tim Pohle, and Gerhard Widmer. Dy-\nnamic playlist generation based on skipping behavior.\nInProceedings of the 6th International Conference on\nMusic Information Retrieval, ISMIR’05, pages 634–\n637, 2005.\n[18] Maarten Sap, Gregory Park, Johannes C. Eichstaedt,\nMargaret L. Kern, David Stillwell, Michal Kosin-\nski, Lyle H. Ungar, and H. Andrew Schwartz. De-\nveloping age and gender predictive lexica over so-\ncial media. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP’14, pages 1146–1151, 2014.\n[19] Pierre Saurel, Francis Rousseaux, and Marc Danger.\nOn the changing regulations of privacy and personal\ninformation in mir. In Proceedings of the 15th Interna-\ntional Society for Music Information Retrieval Confer-\nence, ISMIR’14, pages 597–602, 2014.\n[20] Markus Schedl. The lfm-1b dataset for music retrieval\nand recommendation. In Proceedings of the 2016 ACM\non International Conference on Multimedia Retrieval,\nICMR’16, pages 103–110, 2016.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 693[21] Markus Schedl, David Hauger, Katayoun Farrahi, and\nMarko Tkal ˇciˇc. On the inﬂuence of user characteristics\non music recommendation algorithms. In Proceedings\nof the 37th European Conference on Information Re-\ntrieval, ECIR’15, pages 339–345, 2015.\n[22] Alexandra L. Uitdenbogerd and Ron G. van Schyn-\ndel. A review of factors affecting music recommender\nsuccess. In Proceedings of the 3rd International So-\nciety for Music Information Retrieval Conference, IS-\nMIR’02, pages 204–208, 2002.\n[23] Gabriel Vigliensoni and Ichiro Fujinaga. Automatic\nmusic recommendation systems: Do demographic,\nproﬁling, and contextual features improve their perfor-\nmance? In Proceedings of the 17th International So-\nciety for Music Information Retrieval Conference, IS-\nMIR’16, pages 94–100, 2016.\n[24] Yuan Wang, Yang Xiao, Chao Ma, and Zhen Xiao. Im-\nproving users’ demographic prediction via the videos\nthey talk about. In Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP’16, pages 1359–1368, 2016.\n[25] Ming-Ju Wu, Jyh-Shing Roger Jang, and Chun-Hung\nLu. Gender identiﬁcation and age estimation of users\nbased on music metadata. In Proceedings of the 15th\nInternational Society for Music Information Retrieval\nConference, ISMIR’14, pages 555–560, 2014.\n[26] Zhe Xing, Xinxi Wang, and Ye Wang. Enhancing col-\nlaborative ﬁltering music recommendation by balanc-\ning exploration and exploitation. In Proceedings of the\n15th International Society for Music Information Re-\ntrieval Conference, ISMIR’14, pages 445–450, 2014.\n[27] Billy Yapriady and Alexandra L. Uitdenbogerd. Com-\nbining demographic data with collaborative ﬁltering\nfor automatic music recommendation. In Knowledge-\nBased Intelligent Information and Engineering Sys-\ntems, volume 3684 of LNCS, pages 201–207. 2005.\n[28] Kazuyoshi Yoshii, Masataka Goto, Kazunori Ko-\nmatani, Tetsuya Ogata, and Hiroshi G. Okuno. Hybrid\ncollaborative and content-based music recommenda-\ntion using probabilistic model with latent user prefer-\nences. In Proceedings of the 7th International Confer-\nence on Music Information Retrieval, ISMIR’06, pages\n296–301, 2006.694 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Interactive Arrangement of Chords and Melodies Based on a Tree-Structured Generative Model.",
        "author": [
            "Hiroaki Tsushima",
            "Eita Nakamura",
            "Katsutoshi Itoyama",
            "Kazuyoshi Yoshii"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492365",
        "url": "https://doi.org/10.5281/zenodo.1492365",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/1_Paper.pdf",
        "abstract": "We describe an interactive music composition system that assists a user in refining chords and melodies by generating chords for melodies (harmonization) and vice versa (melodization). Since these two tasks have been dealt with independently, it is difficult to jointly estimate chords and melodies that are optimal in both tasks. Another problem is developing an interactive GUI that enables a user to partially update chords and melodies by considering the latent tree structure of music. To solve these problems, we propose a hierarchical generative model consisting of (1) a probabilistic context-free grammar (PCFG) for chord symbols, (2) a metrical Markov model for chord boundaries, (3) a Markov model for melody pitches, and (4) a metrical Markov model for melody onsets. The harmonic functions (syntactic roles) and repetitive structure of chords are learned by the PCFG. Any variables specified by a user can be optimized or sampled in a principled manner according to a unified posterior distribution. For improved melodization, a long short-term memory (LSTM) network can also be used. The subjective experimental result showed the effectiveness of the proposed system.",
        "zenodo_id": 1492365,
        "dblp_key": "conf/ismir/TsushimaNIY18",
        "keywords": [
            "interactive music composition system",
            "refining chords and melodies",
            "harmonization and melodization",
            "probabilistic context-free grammar",
            "metrical Markov model",
            "Markov model for melody pitches",
            "metrical Markov model for melody onsets",
            "hierarchical generative model",
            "PCFG for chord symbols",
            "metrical Markov model for chord boundaries"
        ],
        "content": "INTERACTIVE ARRANGEMENT OF CHORDS AND MELODIES\nBASED ON A TREE-STRUCTURED GENERATIVE MODEL\nHiroaki Tsushima Eita Nakamura Katsutoshi Itoyama Kazuyoshi Yoshii\nGraduate School of Informatics, Kyoto University, Japan\nftsushima, enakamura g@sap.ist.i.kyoto-u.ac.jp, fitoyama, yoshii g@kuis.kyoto-u.ac.jp\nABSTRACT\nWe describe an interactive music composition system that\nassists a user in reﬁning chords and melodies by gener-\nating chords for melodies (harmonization) and vice versa\n(melodization). Since these two tasks have been dealt with\nindependently, it is difﬁcult to jointly estimate chords and\nmelodies that are optimal in both tasks. Another problem\nis developing an interactive GUI that enables a user to par-\ntially update chords and melodies by considering the la-\ntent tree structure of music. To solve these problems, we\npropose a hierarchical generative model consisting of (1) a\nprobabilistic context-free grammar (PCFG) for chord sym-\nbols, (2) a metrical Markov model for chord boundaries,\n(3) a Markov model for melody pitches, and (4) a metri-\ncal Markov model for melody onsets. The harmonic func-\ntions (syntactic roles) and repetitive structure of chords are\nlearned by the PCFG. Any variables speciﬁed by a user can\nbe optimized or sampled in a principled manner according\nto a uniﬁed posterior distribution. For improved melodiza-\ntion, a long short-term memory (LSTM) network can also\nbe used. The subjective experimental result showed the ef-\nfectiveness of the proposed system.\n1. INTRODUCTION\nMusic composition is a highly intelligent task that has been\nconsidered to be done only by musically trained people.\nTo help musically untrained people create their own musi-\ncal pieces, automatic music composition has actively been\nstudied ( e.g., [4, 8, 19, 31]). While conventional studies\nhave aimed at full automation of music composition, in\nthe process of music composition, melodies (sequences of\nmusical notes) and chord sequences are partially and incre-\nmentally reﬁned by trial and error until the resulting musi-\ncal piece has musically appropriate structure. Our aim is to\ndevelop an interactive arrangement system that can assist\nunskillful people to take such a process for reﬂecting their\npreference in creating melodies and chord sequences.\nIt is non-trivial to reﬂect user’s preference to a musical\npiece in a consistent and uniﬁed framework of statistical\nc⃝Hiroaki Tsushima, Katsutoshi Itoyama, Eita Nakamura,\nKazuyoshi Yoshii. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: Hiroaki Tsushima,\nKatsutoshi Itoyama, Eita Nakamura, Kazuyoshi Yoshii. “An Interactive\nSystem for Generating Chords and Melodies Based on a Tree-Structured\nModel”, 19th International Society for Music Information Retrieval Con-\nference, Paris, France, 2018.\nFigure 1 : Our interactive music arrangement system based\non a tree-structured generative model.\nmodeling. This problem is hard to solve especially when a\nblack-box method ( e.g., neural end-to-end learning) is used\nfor music generation. To incrementally reﬁne a musical\npiece, one may iteratively use a harmonization method for\ngenerating a chord sequence from a melody [4, 19, 24, 28]\nand a melodization method for generating a melody from a\nchord sequence [3,7,8,15,22,30,31]. This approach, how-\never, cannot enable a user to partially and incrementally re-\nﬁne melodies and chords in consideration of the optimality\nof the whole musical piece because each task has a unique\nevaluation criterion.\nSince music is typically well-characterized by chords\nand melodies, it is important to be aware of complicated\nstructures within and between chords and melodies. when\ncomposing a musical piece. To generate a musically ap-\npropriate sequence of chords, the harmonic functions of\nchords, which typically consist of three categories, i.e.,\ntonic (T), dominant (D), and subdominant (SD), should\nbe considered because such functions represent syntactic\nroles in the same way as parts of speech in written texts. In\naddition, a sequence of harmonic functions of chords has a\ntree structure [21, 26]. For example, a chord sequence (C,\nDm, G, Am, C, F, G, C) can be interpreted as (((T, SD),\n(D, T)), ((T, SD), (D, T))), where subtrees such as (T, SD),\n(D, T), and ((T, SD), (D, T)) appear repeatedly in a hierar-\nchical manner. Therefore, it is desirable to consider such\nthe hierarchical tree structure of chord sequences when we\ncomputationally help people to create a new music.\nIn this paper we propose an interactive music arrange-\nment system that enables musically untrained users to cre-\nate a melody and a chord sequence (Fig. 1). To partially\nand incrementally reﬁne the piece, users can choose sev-\neral types of operations that are often exploited by mu-\nsically trained people. Speciﬁcally, the entire chord se-145quence and the corresponding tree structure can be reﬁned\njointly for a melody; the onset time of a speciﬁed chord can\nbe reﬁned; two adjacent chords forming a subtree can be\nmerged into a single chord or a chord can be split into two\nchords; and melody notes in the region of a speciﬁed chord\ncan be reﬁned. All a user needs to do is to specify where\nto update the piece and it is not necessary to manually edit\nindividual musical elements.\nTo optimize a chord sequence and a melody in a uniﬁed\ncriterion, we propose a tree-structured hierarchical gen-\nerative model that consists of (i) a probabilistic context-\nfree grammar (PCFG) generating chord symbols [28], (ii) a\nmetrical Markov model generating chord rhythms, and (iii)\na Markov model generating melody pitches conditionally\non the chord sequence, and (iv) a metrical Markov model\ngenerating melody rhythms (Fig. 2). The rule probabili-\nties of the PCFG are learned from chord sequences, with\nthe expectation that the syntactic roles of chords are cap-\ntured by the non-terminal symbols [29]. The other mod-\nels are also learned from chord and/or note sequences. To\nimprove the melodization process, a long short-term mem-\nory (LSTM) network can be used instead of the Markov\nmodels (iii) and (iv) for capturing the long-term character-\nistic of a melody. Using the generative model trained in\nadvance, we can estimate any “missing” variables, i.e., an\nunpleasant part of chords or musical notes speciﬁed by the\nuser, in a statistical manner.\nThe major contribution of this study is the realization of\na directability-aware music composition/arrangement sys-\ntem based on a uniﬁed probabilistic model. This system\nprovides a user with an easy-to-use GUI that shows other\npossibilities for an unpleasant part of the piece and all op-\nerations on the GUI are implemented as posterior inference\nbased on the probabilistic model. Our contribution lies in\nthe marriage of AI and human creativity.\n2. RELATED WORK\nThis section reviews related studies on automatic harmo-\nnization and melodization.\n2.1 Automatic Harmonization\nMany studies have been conducted for automatic harmo-\nnization for given melodies. Some studies aim to gener-\nate a sequence of chord symbols (as in this paper), and\nothers aim to generate several (typically four) voices of\nmusical notes. In the former type of research, Chuan and\nChew [4] proposed a method consisting of three processes:\nselecting musical notes that might form chords from given\nmelodies with a support vector machine (SVM), construct-\ning triad chords from the selected notes, and generating\nchord progressions by using a rule-base method. Simon\net al. [24] proposed a commercial system MySong based\non hidden Markov models (HMMs) with Markovian chord\ntransitions. Raczy ´nski et al. [20] proposed similar Markov\nmodels in which chords are conditioned by melodies and\ntime-varying keys. Tsushima et al. [28] proposed a har-\nmonization method that considers the hierarchical repeti-\ntive structure of sequences of chord symbols obtained by\n/c0f/c22/c4e /c27 /c28 /c24\n/c22/c4e /c24 /c27 /c28/c31/c53/c50/c43/c42/c43/c4a/c4d/c4a/c54/c55/c4a/c44/c01/c44/c50/c4f/c55/c46/c59/c55/c0e/c47/c53/c46/c46/c01\n/c48/c53/c42/c4e/c4e/c42/c53/c01/c09/c31/c24/c27/c28/c0a\n/c47/c50/c53/c01/c44/c49/c50/c53/c45/c01/c54/c5a/c4e/c43/c50/c4d/c54\n/c2e/c46/c55/c53/c4a/c44/c42/c4d/c01/c2e/c42/c53/c4c/c50/c57/c01/c4e/c50/c45/c46/c4d\n/c47/c50/c53/c01/c44/c49/c50/c53/c45/c01/c50/c4f/c54/c46/c55/c54\n/c2e/c42/c53/c4c/c50/c57/c01/c4e/c50/c45/c46/c4d\n/c47/c50/c53/c01/c4e/c46/c4d/c50/c45/c5a/c01/c51/c4a/c55/c44/c49/c46/c54 /c0744/c2e/c46/c55/c53/c4a/c44/c42/c4d/c01/c2e/c42/c53/c4c/c50/c57/c01/c4e/c50/c45/c46/c4d\n/c47/c50/c53/c01/c4e/c46/c4d/c50/c45/c5a/c01/c50/c4f/c54/c46/c55/c54\n/c07/c0fFigure 2 : A tree-structured hierarchical generative model\nfor chord symbols and melodies.\nPCFGs and pitch transitions conditioned by chord symbols\nwith Markov models. De Prisco et al. [19] proposed a har-\nmonization method for only a base line of the input with\na distinctive network that models the dependencies among\nbass notes, the previous chord, and the current chord.\nIn the latter type of research, Ebcio ˘glu [6] proposed\na rule-based method for generating four-part chorales in\nBach’s style. Several methods of using variants of genetic\nalgorithms (GAs) based on music theories have also been\nproposed [17, 18, 27]. Allan and Williams [2] proposed\nan HMM-based method that represents chords as hidden\nstates and musical notes as observed outputs. A hidden\nsemi-Markov model (HSMM) [11] has been used for ex-\nplicitly representing the durations of chords. Paiement et\nal. [16] proposed a hierarchical tree-structured model that\ndescribes chord movements from the viewpoint of hierar-\nchical time scales by dividing the notations of chords. To\ngenerate highly convicting four-part chorales, a deep re-\ncurrent neural network has also been used for capturing the\nlong-term characteristic of a melody and a harmony [12].\n2.2 Automatic Melodization\nThere have been many studies on automatic melodization\n[3,8,15,22,30,31]. Fukayama et al. [8] developed a system\nnamed Orpheus that generates a melody for a given lyric\nin a way that the prosody of the lyric matches the dynam-\nics of the melody. Roig et al. [22] proposed a method of\ngenerating a monophonic melody by using a probabilistic\nmodel of rhythm patterns and pitch contours.\nRecent studies have applied deep learning techniques.\nInMagenta project [30], for example, recurrent neural net-\nworks (RNNs) are used for learning long-term dependency\nof music. Yang et al. [31] proposed a novel method for\ngenerating diverse monophonic melodies by combining a\ngenerative adversarial network (GAN) with a convolutional\nneural network (CNN). To generate diverse melodies, Mo-\ngren [15] proposed adversarial training of an RNN that\nworks on continuous sequential data. The method based\non a restricted Boltzmann machine (RBM) conditioned on\nRNNs that models temporal dependency has been proposed\nto generate polyphonic music [3]. In addition, Eck et al. [7]\nhave proposed an LSTM-based method for generating both\nmelodies and chords by capturing the characteristic of note-\nby-note transitions and the mutual dependency between\nmusical notes and chord symbols.146 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20183. USER INTERFACE\nThe proposed system, which is implemented as a web ser-\nvice based on HTML5, enables a user to incrementally re-\nﬁne a chord sequence and a melody on a GUI (Fig. 1). To\nuse a system, a user is asked to upload a melody of eight\nbars. The system then estimates a chord sequence that har-\nmonizes with the melody. The chord onsets are located at\nthe bar lines. Supported arrangement operations are:\n\u000fUpdating the chord symbols : The chord symbols\nand the latent tree structure behind the chord sym-\nbols are jointly optimized for the current melody.\n\u000fUpdating a chord onset : One of the chord onsets\n(boundaries) speciﬁed by a user is optimized.\n\u000fSplitting a chord : One of the chords speciﬁed by a\nuser is split into two adjacent chords.\n\u000fMerging chords : Two adjacent chords that form a\nsubtree are merged into a single chord.\n\u000fUpdating the melody : Melody notes in the region\nof a chord speciﬁed by a user are updated while keep-\ning consistency with neighboring measures.\n4. PROBABILISTIC MODELING\nThis section explains a uniﬁed probabilistic model that rep-\nresents the hierarchical generative process of a chord se-\nquence and a melody. The proposed model consists of four\nsub-models, which are trained independently.\n4.1 Mathematical Notation\nWe assume that chord and melody onsets are on the 16th-\nnote-level grid. Let Lbe the number of measures of a mu-\nsical piece ( L= 8 in this paper) and T= 16Lbe the\ntotal number of time units. A sequence of chord symbols\nand that of chord onsets are denoted by z=fzngN\nn=1\nandϕ=fϕngN\nn=1, respectively, where Nis the number\nof chords and ϕntakes an integer in [0;T). Similarly, a\nsequence of melody pitches and that of melody onsets in\nthe region of chord znis denoted by pn=fpn;igIn\ni=1and\n n=f n;igIn\ni=1, respectively, where Inis the number of\nmusical notes in that time span, pn;iis a MIDI note num-\nber from 32to93, and n;itakes an integer in [ϕn;ϕn+1).\nThe whole melody is denoted by p=fpngN\nn=1and =\nf ngN\nn=1, whereI=∑N\nn=1Inis the number of melody\nnotes.\nLettbe a latent tree that derives zaccording to a PCFG\nandtm:nbe an inside part (subtree) of tthat deriveszm:n.\nThust=t1:N. We often use tm:nto indicate the root node\nof the subtree for simplicity. Let t:m:nbe an outside part\noftthat derivesz1:m\u00001,tm:n, andzn+1:N.\n4.2 Model Formulation\nWe formulate a uniﬁed probabilistic model that represents\nthe generative process of a latent tree t, chord symbols z,\nchord onsetsϕ, melody pitches p, and melody onsets  .\n4.2.1 Probabilistic Context-Free Grammar for tandz\nA derivation tree tand chord symbols zare generated in\nthis order according to a PCFG G= (V;\u0006;R;S ), deﬁned\n!\"\" !\"\"\n!\"\"!\"\"\n!\"\" !\"\"\n!\"\"!\"\"\n/c47/c56/c4d/c4d/c5a/c0e/c44/c50/c4f/c4f/c46/c44/c55/c46/c45\n/c4d/c42/c5a/c46/c53\n/c47/c56/c4d/c4d/c5a/c0e/c44/c50/c4f/c4f/c46/c44/c55/c46/c45\n/c2d/c34/c35/c2e/c01/c4d/c42/c5a/c46/c53!\"\" !\"\"!\"\"\n!\"\"\n!\"\"!\"#$!\"#%!\"\n!\"#%!\"!\"&%'\"#$'\"#%'\"Figure 3 : Conﬁguration of the LSTM network\nby a set of non-terminal symbols Vthat are expected to\nrepresent the hierarchical structure and syntactic roles of\nchords, a set of terminal symbols (chord symbols) \u0006, a\nset of rule probabilities R, and a start symbol S(a non-\nterminal symbol located on the root of a syntax tree). There\nare three types of rule probabilities. \u0012A!BCis the prob-\nability that a non-terminal symbol A2Vbranches to\nnon-terminal symbols B2VandC2V.\u0011A!\u000bis the\nprobability that A2Vemits terminal symbol \u000b2\u0006. A\nnon-terminal symbol A2Vemits a terminal symbol with\na probability of 0< \u0015A<1and otherwise it branches.\nThese probabilities are normalized as follows:\n∑\nB;C2V\u0012A!BC= 1;∑\n\u000b2\u0006\u0011A!\u000b= 1: (1)\nWe let\u0012A=f\u0012A!BCgB;C2Vand\u0011A=f\u0011A!\u000bg\u000b2\u0006.\n4.2.2 Metrical Markov Models for ϕand \nThe metrical Markov model for chord onsets ϕon the reg-\nular 16th-note-level grid is deﬁned by\np(ϕnjϕn\u00001) =\u0019ϕn\u00001mod16;ϕn\u0000ϕn\u00001; (2)\nwhere\u0019a;bindicates the probability that a chord starting at\nthea-th position in a measure ( 0\u0014a<16) continues for\nthe duration of btime units ( 0<b\u0014T).\nA similar model for melody onsets  is deﬁned by\np( n;1j n\u00001;In\u00001) =\u001a n\u00001;In\u00001mod16; n;1\u0000 n\u00001;In\u00001;\np( n;ij n;i\u00001) =\u001a n;i\u00001mod16; n;i\u0000 n;i\u00001(1<i);(3)\nwhere\u001aa;bindicates the probability that a musical note\nstarts at the a-th position in a measure ( 0\u0014a < 16) and\ncontinues for the duration of btime units ( 0<b\u0014T).\n4.2.3 Markov Model for pConditioned on z\nThe Markov model for melody pitches pconditioned by a\nchord sequence given by zis deﬁned by\np(pn;1jpn\u00001;In\u00001;zn) =\u001czn\npn\u00001;In\u00001;pn;1; (4)\np(pn;ijpn;i\u00001;zn) =\u001czn\npn;i\u00001;pn;i(2\u0014i\u0014In); (5)\nwhere\u001cc\na;bis the transition probability from pitch ato pitch\nbunder chord symbol c.\n4.2.4 Bayesian Integration of Four Sub-models\nLettingΩ=ft;z;ϕ;p; gbe a set of the external random\nvariables and \u0002=f\u0012;\u0011;\u0015;\u0019;\u001a;\u001cgbe a set of the model\nparameters, the uniﬁed model is given by\np(Ω;\u0002) =p(t;zj\u0012;\u0011;\u0015)p(ϕj\u0019)p( j\u001c)p(pjz)p(\u0002);(6)\nwherep(\u0002) =p(\u0012)p(\u0011)p(\u0015)p(\u0019)p(\u001a)p(\u001c)is a prior dis-\ntribution over \u0002. To make Bayesian inference tractable,Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 147we use conjugate Dirichlet and beta priors as follows:\n\u0012A\u0018Dir(\u0018A);\u0011A\u0018Dir(\u0010A); \u0015A\u0018Beta(\u0013A);(7)\n\u0019a\u0018Dir(\fa);\u001aa\u0018Dir(\ra);\u001cc\na\u0018Dir(\u000ec\na); (8)\nwhere\u0018A,\u0010A,\u0013A,\fa,\ra, and\u000ec\naare hyperparameters.\n4.2.5 LSTM Network for xConditioned on c\nIn melody arrangement, we can also use an LSTM model\nthat can learn complicated long-term dynamics of melodies.\nLetx=fxtgT\nt=1be another representation of the entire\nmelody, where xttakes a MIDI note number at the t-th\nposition ( 0\u0014t < T ) if the note onset is at that position\nand otherwise takes 0. Letc=fctgT\nt=1be another rep-\nresentation of the entire chord sequence given by zand\nϕ, wherectindicates a chord symbol at the t-th position.\nGiven a sequence of musical notes x1:t=fxigt\ni=1and\nthat of chord symbols c1:t=fcigt\ni=1, the LSTM model\ndetermines the probability of the next musical note given\nbyp(xt+1jx1:t;c1:t)(Fig. 3).\n4.3 Model Training\nOur goal is to obtain the maximum a posteriori (MAP) es-\ntimates of the model parameters \u0002=f\u0012;\u0011;\u0015;\u0019;\u001a;\u001cg.\nTo estimate the parameters \u0012,\u0011, and\u0015of the PCFG from\na chord sequence z(multiple sequences are used in prac-\ntice) in an unsupervised manner, we use an inside-ﬁltering-\noutside-sampling algorithm [13,28] for generating samples\nfrom the true posterior distribution p(\u0012;\u0011;\u0015;tjz). More\nspeciﬁcally, the latent tree tand the parameters \u0012,\u0011, and\u0015\nare alternately sampled from the conditional posterior dis-\ntributionsp(tj\u0012;\u0011;\u0015;z)andp(\u0012;\u0011;\u0015jt;z), respectively.\nThe parameters \u0019,\u001cand\u001aof the Markov models are\nlearned independently. Given a sequence of chord onsets ϕ\nand a sequence of melody onsets  , the posterior distribu-\ntion of\u0019and that of\u001acan be calculated, respectively, be-\ncause of the conjugacy between the Dirichlet and categor-\nical distributions. Similarly, given a sequence of melody\npitchespassociated with a chord sequence speciﬁed by z\nandϕ, the posterior distribution of \u001ccan be calculated.\nThe LSTM network is also trained from the same data.\n5. CHORD AND MELODY ARRANGEMENT\nThis section explains how to leverage the uniﬁed model de-\nscribed in Section 4 for implementing the ﬁve operations\ndescribed in Section 3. Let Ω=ft;z;ϕ;p; gbe a set\nof random variables. To estimate a “missing” part \u001f\u001aΩ,\nwe take a principled statistical approach based on the con-\nditional posterior distribution p(\u001fjΩ:\u001f;\u0002), whereA:B\nindicates a subset of Aobtained by removing the elements\nofBfromA. Note that full automatic music composition\ncan be achieved by sampling Ωfromp(Ωj\u0002).\n5.1 Updating the Chord Symbols\nWhen the melody pitches pare ﬁxed, the chord symbols z\nand the latent tree tcan be optimized by maximizing the\nconditional posterior distribution p(t;zjp;\u0002). Since both\ntandzare latent variables in this operation, we extend the\nViterbi algorithm to infer tandzfromp. First, the inside\n!\"#\"\n!\"$\n%\"$%\"&\n'\" '\"()/c34/c51/c4d/c4a/c55/c01/c42/c01/c44/c49/c50/c53/c45!\"#\"\n%\"\n'\" '\"()!\"#\"()\n!\"#\"!\n%\"%\"()*+,#*+,\n'\"'\"()'\"(-/c2e/c46/c53/c48/c46/c01/c44/c49/c50/c53/c45/c54\n%\n'\" '\"(-!\"&!\"#\"()\n'Figure 4 : Split and merge operations.\nprobabilities are recursively calculated from the layer of\nterminal symbols zto the start symbol Saccording to\npA\nn;n=\u0015Amax\nz2\u0006\u0011A!zp(pnjz); (9)\npA\nn;n+k= (1\u0000\u0015A) max\nB;C2V\n1\u0014l\u0014k\u0012A!BCpB\nn;n+l\u00001pC\nn+l;n+k;(10)\nwherep(pnjzn)is the probability that a pitch subsequence\npnis generated conditionally on chord zn:\np(pnjzn) =In∏\ni=1p(pn;ijpn;i\u00001;zn); (11)\nwherepn;0=pn\u00001;In\u00001. The most likely tandzare ob-\ntained by recursively back-tracking the most likely paths\nfrom the start symbol S.\n5.2 Updating a Chord Onset\nWhen the melody pitches pand the melody onsets  are\ngiven and the chord symbols zare ﬁxed, a chord onset ϕn\ncan be optimized by maximizing the conditional posterior\ndistribution given by\np(ϕnjz;ϕ:n;p; ;\u0002)\n/p(pn\u00001jzn\u00001)p(pnjzn)p(ϕnjϕn\u00001)p(ϕn+1jϕn);(12)\nwhereϕnis restricted such that  n\u00001;1\u0014ϕ\u0014 n;In.\n5.3 Splitting a Chord and Merging Chords\nThe chord symbols zand the chord onsets ϕcan be locally\nreﬁned by splitting a chord into adjacent chords or merging\nadjacent chords into another chord (Fig. 4). A subtree of t\nis updated accordingly. The split operation can be applied\nto any chord znwhile the merge operation is restricted to\nadjacent chords zn:n+1forming a subtree tn:n+1.\nA chordznassociated with a non-terminal symbol tn:n\nis split at a 16th-note-level position ϕinto two new chords\nzL\nnandzR\nnassociated with two new symbols tL\nnandtR\nn\nby maximizing the conditional posterior distribution given\nbyp(tL\nn;tR\nn;zL\nn;zR\nn;ϕjt:n:n;z:n;ϕ;p; ;\u0002). This oper-\nation makes a new subtree that has tn:nas its root node,\nderivestL\nnandtR\nn, and generates zL\nnandzR\nn. To do this, we\nuse the extended Viterbi algorithm for estimating the most\nlikely subtree from pn. First, the inside probabilities are\nrecursively calculated from the layer of terminal symbols\nzL\nnandzR\nnto the root node tn:naccording to\n\u000bA\nϕ=\u0015Amax\nz2\u0006\u0011A!zp(pL\nnjz;ϕ); (13)\n\fA\nϕ=\u0015Amax\nz2\u0006\u0011A!zp(pR\nnjz;ϕ); (14)\nptn:n\nϕ= max\nB;C2V\u0012tn:n!BC\u000bB\nϕ\fC\nϕp(ϕjϕn)p(ϕn+1jϕ);(15)148 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018wherepL\nnandpR\nnare the subsequences of pitches obtained\nby splittingpnwith a boundary ϕ. The most likely zL\nn,zR\nn,\ntL\nn,tR\nn, andϕare obtained by recursively back-tracking the\nmost likely paths from tn:n.\nTwo adjacent chords znandzn+1associated with non-\nterminal symbols tn:nandtn+1:n+1are merged into a sin-\ngle chordzassociated with a non-terminal symbol tn:n+1\nby maximizing the conditional posterior distribution given\nbyp(zjt:n:n+1;z:n:n+1;ϕ:n+1;p; ;\u0002). The most likely\nzis obtained as follows:\nz= arg max\nz′2\u0006\u0011tn:n+1!z′p(pnjz′)p(pn+1jz′): (16)\n5.4 Updating the Melody\nWhen a chord symbol zn, the last pitch pn\u00001;In\u00001in the re-\ngion of the previous chord zn\u00001, and the ﬁrst pitch pn+1;1\nin the region of the next chord zn+1are given, a sequence\nof musical notes in the region of zn(betweenϕnandϕn+1)\nis obtained by maximizing the conditional posterior dis-\ntributionp(pnjzn;pn\u00001;In\u00001;pn+1;1;\u0002). To do this, we\npropose an efﬁcient algorithm based on dynamic program-\nming. Let\u000byt;dtbe the marginal likelihood that a note at\nthe pitchytis located on the score time tand the duration\nof the previous note is dton a chordzn:\n\u000byt;dt=p(yt;dtjzn) (17)\nThis probability can be calculated recursively in the score\ntimet2 fϕn;:::;ϕn+1; n+1;1g.\n\u000byt;dt=\u001at\u0000dt;t∑\nyt\u0000dt;dt\u0000dt\u000byt\u0000dt;dt\u0000dt\u001czn\nyt\u0000dt;yt\nIn each score time t,dtcan take values in f1;:::;t;t \u0000\n n\u00001;In\u00001g. By using this probability, we can recursively\nsamplepnfrom the beat score time  n+1;1to n\u00001;In\u00001.\nAnother improved way of partially updating the melody\nis to use the LSTM model. Suppose that we aim to update\nxi:jin the whole melody x. Given a chord sequence c\nand melody segments x1:i\u00001andxj+1:T, the missing part\nxi:jcan be sampled from the conditional posterior distribu-\ntionp(xi:jjc;x1:i\u00001;xj+1:T)/p(xjc). First, the pitches\nx1:i\u00001and chordsc1:i\u00001are fed to the network to update\nthe hidden states. The missing part xi:jis then sampled\nsequentially according to the probability p(xt+1jx1:t;c1:t)\nlearned by the LSTM. This enables us to evaluate p(xjc).\nAmong a sufﬁcient number of generated samples of x1:i\u00001,\na sample with the highest p(xjc)is selected.\n6. EVALUATION\nThis section reports objective and subjective evaluations on\nthe user interface and the music arrangement method.\n6.1 Experimental Conditions\nTo train the PCFG, we used 705 chord sequences of mu-\nsical sections (e.g., verse, bridge, and chorus) from 468\npieces of popular music included in the SALAMI dataset\n[25]. Only chord sequences with a length between 8 and\n32 measures were chosen. The vocabulary of chord sym-\nbols was limited to the combinations of the 12 root notes\nfC, C#, ..., B gand the 2 chord types fmajor, minor g. Thenumber of kinds of non-terminal symbols of the PCFG was\nset to 12. The values of the hyperparameter \u0013Awere all set\nto1:0and those of the other parameters were all set to 0:1.\nTo train the three Markov models, we used 9902 pairs of\nmelodies and the corresponding chord sequences from 194\npieces of popular music included in Rock Corpus [5]. To\ntrain the LSTM, we used 9265 melodies associated with\nchord sequences from pieces of popular music included in\nRock Corpus and Nottingham Database [1]. Note that all\nof the data used in our experiments were transposed to the\nC major or C minor key. The number of the hidden units\nwas 50 and the softmax-cross-entropy was used as a loss\nfunction. The parameters of the LSTM were optimized by\nusing Adam [14]. The number of samples generated by the\nLSTM (described in Section 5.4) was 50.\n6.2 Objective Evaluation of Melody Arrangement\nWe evaluated the function of updating a melody in terms\nof the note density of the generated musical notes via 10-\nfold cross validation on the Rock Corpus and Nottingham\nDatabase. For the region of each chord zn, a sequence of\nmelodypnis arranged by using the two methods based on\nthe Markov model and the LSTM described in Section 5.4.\nWe measured the mean squared error (MSE) between the\nnote density per measure of the generated musical notes\nand the mean value of the density of other regions given\nby\nMSE=1\nN\u00001N\u00001∑\nn=1{16I\u0003\nn\nϕn+1\u0000ϕn\u0000∑\nm̸=n16Im∑\nm̸=n(ϕm+1\u0000ϕm)}2\n;\nwhereI\u0003\nnandInwere the number of generated musical\nnotes and that of the original musical notes, respectively.\nThe average MSE was calculated over all melodies. The\naverage MSE obtained by the LSTM model was 5:52while\nthat obtained by the Markov model was 6:42. This indi-\ncates that the LSTM-based method is a little more effec-\ntive for updating a partial melody in consideration of the\nnote density of the whole melody because it can capture\nthe long-term dependency.\n6.3 Subjective Evaluation of the Proposed System\nWe conducted the subjective evaluation of the system1in\nterms of usability and effectiveness in interactive chord and\nmelody arrangement. Five melodies of 8 measures were\nextracted from the RWC music database [9, 10]. We asked\n11 subjects to test our system. Four subjects who had\nthe experience of playing musical instruments for more\nthan ﬁve years were regarded as people with musical back-\ngrounds. Each subject was asked to interactively make a\nmusical piece by using each of the ﬁve melodies as an ini-\ntial seed and then grade the system on a 5-point Likert scale\n(from “strongly agree (1)” to “strongly disagree (5)”) in\nterms of the following 15 criteria:\n\u000fThe chord sequences obtained were suitable for the\nmelodies (I).\n\u000fThe chord sequences obtained by the split or merge\noperation were musically natural (II, III).\n1The interface used in this experiment is available online:\nhttp://sap.ist.i.kyoto-u.ac.jp/members/tsushima/ismir2018/Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 149/c16\n/c14\n/c12/c13/c15\n/c16\n/c14\n/c12/c13/c15/c29/c50/c58/c01/c4f/c42/c55/c56/c53/c42/c4d /c29/c50/c58/c01/c4a/c4f/c55/c46/c53/c46/c54/c55/c4a/c4f/c48 /c29/c50/c58/c01/c56/c54/c46/c47/c56/c4d\n/c2a/c0f /c2a/c2a/c0f /c2a/c2a/c2a/c0f /c2a/c37/c0f /c37/c0f /c37/c2a/c0f /c37/c2a/c2a/c0f /c37/c2a/c2a/c2a/c0f /c2a/c39/c0f /c39/c0f /c39/c2a/c0f /c39/c2a/c2a/c0f /c39/c2a/c2a/c2a/c0f /c39/c2a/c37/c0f /c39/c37/c0f/c16\n/c14\n/c12/c13/c15\n/c16\n/c14\n/c12/c13/c15Figure 5 : Results for people with musical backgrounds\n(top) and those for people without musical backgrounds\n(bottom). The middle bars indicate the mean value.\n\u000fThe melodies obtained were suitable for the chord se-\nquences (IV).\n\u000fThe melodies obtained were musically natural (V).\n\u000fThe musical pieces obtained by updating chord sym-\nbols, splitting a chord, merging chords, or updating a\nmelody were interesting (VI, VII, VIII, IX).\n\u000fThe function of updating chord symbols, splitting a\nchord, merging chords, or updating a melody was use-\nful (X, XI, XII, XIII, XIV).\n\u000fThe user interface has the capability of helping users\nmake musical pieces (XV).\nWe also asked the subjects to tell us how each of them felt\nabout the system.\nThe results of this user study is shown in Fig 5. In terms\nof the naturalness and the interestingness, the two opera-\ntions, updating chord symbols and updating melodies, ob-\ntained the slightly high mean ratings of 3:67in criterion (I),\n3:69in criterion (IV), and 3:51in criterion (VI). As seen in\nthe score for the criterion (V), the subjects with musical\nbackgrounds, compared with the others, tended to feel that\nthe updated melodies were less musically natural.As seen\nin the score for the criterion (IX), the subjects with musical\nbackgrounds tended to feel that the updated melodies were\nmore interesting. In terms of the usefulness of each oper-\nation, each operation obtained the reasonably high mean\nratings (from 3:27to3:91).\nWe obtained the following opinions on the usability of\nour system:\n\u000fIt was interesting that even a user without any expe-\nriences in music composition can edit a musical piece\nby iterating several operations.\n\u000fAn operation that updates one chord symbol is neces-\nsary for more freely editing a chord sequence.\nWe also obtained the following opinions on the problems\nof some operations:\n\u000fThe chord sequences obtained were almost always ap-\npropriate for all samples of melodies but the system\ntended to generate only basic chords (e.g., C major).\n\u000fThe updated melodies were often unnatural when an\noriginal melody has some repeated sections.\nThe reason for the former problem may be that the chord\nsymbols are updating by using the Viterbi algorithm. The\nreason for the latter problem is probably that the LSTM\ncannot capture the global repetitive structure of a melody.\n/c24 /c24 /c25/c4e /c23/c06 /c24 /c22/c4e /c25/c4e\n/c36/c51/c45/c42/c55/c46/c01/c4e/c46/c4d/c50/c45/c5a\n/c34/c51/c4d/c4a/c55/c01/c24/c49/c50/c53/c45/c36/c51/c45/c42/c55/c46/c01/c4e/c46/c4d/c50/c45/c5a/c23/c06\n/c24 /c24 /c25/c4e /c23/c06 /c24 /c22/c4e /c25/c4e /c23/c06\n/c24 /c24 /c25/c4e /c23/c06 /c24 /c22/c4e /c25/c4e /c23/c06 /c27\n/c2e/c46/c53/c48/c46/c01/c24/c49/c50/c53/c45/c54\n/c24 /c24 /c25/c4e /c23/c06 /c24 /c22/c4e /c23/c06 /c27Figure 6 : Example operation for interactive generation of\nchord sequences and melodies.\n6.4 Example of Chord and Melody Arrangement\nFig. 6 shows how the proposed method generates chord se-\nquences and melodies. The score (melodies and chords) at\nthe top shows an initial state in which the chord symbols\nwere optimized for the melody in the input ﬁle (the chord\nonsets were located at the bar lines). The second score\nshows the state in which the two regions of the melody un-\nder the 3rd and 6th chords were updated in order. The third\nchord sequence shows the state in which the 4th chord, B ♭\nmajor, was split into F major and B ♭major. The fourth\nchord sequence shows the state in which the 7th chord, A\nminor, and the 8th chord, D minor, were merged into A\nminor. This indicates that the proposed method can suc-\ncessfully help a user partially update a melody while keep-\ning the consistency of the whole melody and that it can\ngenerate a chord sequence by considering the latent tree\nstructure behind the chord sequence.\n7. CONCLUSION\nThis paper presented an interactive music arrangement sys-\ntem that enables a user to incrementally reﬁne a chord se-\nquence and a melody. The experimental results showed\nthat the proposed system has a great potential to help a\nuser create his or her original musical pieces.\nThere would be much room for improving our method.\nTo improve the diversity of generated chord symbols, the\nuse of some sampling or beam-search method would be ef-\nfective. To improve the naturalness of generated melodies,\nthe use of a bidirectional LSTM [23] would be effective for\nconsidering the repetitive structures of melodies.\nFor more speciﬁc studies on the effectiveness of our sys-\ntem, we plan to measure how well test users can incremen-\ntally reﬁne a musical piece compared with the conventional\nmethods, by counting the number of necessary operations\nto make musical pieces meet their satisfaction. We also\nplan to conduct large-scale user studies of the system on\nthe Web. Collecting time-series data of users’ operations\nand created pieces, it would be possible to infer their mu-\nsical preference and improve the model by reinforcement\nlearning. Using the same data, it would be possible to re-\nveal the process of music creation by humans in terms of\nedit operations and optimization strategies.\nAcknowledgements: This study was partially supported by JST ACCEL\nNo. JPMJAC1602, JSPS KAKENHI No. 26700020 and No. 16H01744,\nand Grant-in-Aid for JSPS Research Fellow No. 16J05486.150 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20188. REFERENCES\n[1]ABC version of the Nottingham music database.\nhttp://abc.sourceforge.net/NMD/.\n[2]M. Allan and C. Williams. Harmonising chorales by\nprobabilistic inference. In NIPS , pages 25–32, 2005.\n[3]N. Boulanger-Lewandowski, Y . Bengio, and P. Vin-\ncent. Modeling temporal dependencies in high-\ndimensional sequences: Application to polyphonic\nmusic generation and transcription. In ICML , 2012.\n[4]C. H. Chuan and E. Chew. A hybrid system for auto-\nmatic generation of style-speciﬁc accompaniment. In\nIJWCC , pages 57–64, 2007.\n[5]T. D. Clercq and D. Temperley. A corpus analysis of\nrock harmony. Popular Music , 30(01):47–70, 2011.\n[6]K. Ebcio ˘glu. An expert system for harmonizing four-\npart chorales. Computer Music Journal , 12(3):43–51,\n1988.\n[7]D. Eck and J. Schmidhuber. A ﬁrst look at music com-\nposition using LSTM recurrent neural networks. ID-\nSIA, 103(07-02), 2002.\n[8]S. Fukayama et al. Orpheus: Automatic composi-\ntion system considering prosody of Japanese lyrics. In\nICMC , pages 309–310. Springer, 2009.\n[9]M. Goto. AIST annotation for the RWC music\ndatabase. In ISMIR , pages 359–360, 2006.\n[10] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Popular, classical and jazz mu-\nsic databases. In ISMIR , pages 287–288, 2002.\n[11] R. Groves. Automatic harmonization using a hidden\nsemi-Markov model. In AIIDE , pages 48–54, 2013.\n[12] G. Hadjeres and F. Pachet. DeepBach: A steerable\nmodel for Bach chorales generation. In ICML , pages\n1362–1371, 2017.\n[13] M. Johnson, T. L. Grifﬁths, and S. Goldwater. Bayesian\ninference for PCFGs via Markov chain Monte Carlo. In\nNAACL-HLT , pages 139–146, 2007.\n[14] D. P. Kingma and J. Ba. Adam: A method for stochas-\ntic optimization. In ICMR , pages 1–15, 2014.\n[15] O. Mogren. C-RNN-GAN: Continuous recurrent neu-\nral networks with adversarial training. In Constructive\nMachine Learning Workshop (NIPS 2016) , 2016.\n[16] J. F. Paiement, D. Eck, and S. Bengio. Probabilis-\ntic melodic harmonization. In CSCSI , pages 218–229,\n2006.\n[17] G. Papadopoulos and G. Wiggins. AI methods for al-\ngorithmic composition: A survey, a critical view and\nfuture prospects. In AISB Symposium on Musical Cre-\nativity , pages 110–117, 1999.[18] R. D. Prisco and R. Zaccagnino. An evolutionary mu-\nsic composer algorithm for bass harmonization. In Ap-\nplications of Evolutionary Computing , pages 567–572.\nSpringer, 2009.\n[19] R. De Prisco, A. Eletto, A. Torre, and R. Zaccagnino.\nA neural network for bass functional harmonization.\nInEuropean Conference on the Applications of Evolu-\ntionary Computation , pages 351–360. Springer, 2010.\n[20] S. A. Raczy ´nski, S. Fukayama, and E. Vincent. Melody\nharmonization with interpolated probabilistic models.\nJournal of New Music Research , 42(3):223–235, 2013.\n[21] M. Rohrmeier. Mathematical and computational ap-\nproaches to music theory, analysis, composition and\nperformance. Journal of Mathematics and Music ,\n5(1):35–53, 2011.\n[22] C. Roig, L. J. Tard ´on, T. Barbancho, and A. M. Bar-\nbancho. Automatic melody composition based on a\nprobabilistic model of music style and harmonic rules.\nKnowledge-Based Systems , 71:419–434, 2014.\n[23] M. Schuster and K. K. Paliwal. Bidirectional recurrent\nneural networks. IEEE Transactions on Signal Pro-\ncessing , 45(11):2673–2681, 1997.\n[24] I. Simon, D. Morris, and S. Basu. Mysong: auto-\nmatic accompaniment generation for vocal melodies.\nInSIGCHI Conference on Human Factors in Comput-\ning Systems , pages 725–734. ACM, 2008.\n[25] J. B. L. Smith, J. A. Burgoyne, I. Fujinaga, D. D.\nRoure, and J. S. Downie. Design and creation of a\nlarge-scale database of structural annotations. In IS-\nMIR, pages 555–560, 2011.\n[26] M. J. Steedman. A generative grammar for jazz chord\nsequence. Music Perception , 2(1):52–77, 1984.\n[27] M. Towsey, A. Brown, S. Wright, and J. Diederich. To-\nwards melodic extension using genetic algorithms. Ed-\nucational Technology & Society , 4(2):54–65, 2001.\n[28] H. Tsushima, E. Nakamura, K. Itoyama, and K. Yoshii.\nFunction- and rhythm-aware melody harmonization\nbased on tree-structured parsing and split-merge sam-\npling of chord sequences. In ISMIR , pages 502–508,\n2017.\n[29] H. Tsushima, E. Nakamura, K. Itoyama, and K. Yoshii.\nGenerative statistical models with self-emergent gram-\nmar of chord sequences. Journal of New Music Re-\nsearch , 2018. To appear.\n[30] E. Waite. Generating long-term structure in songs\nand stories. https://magenta.tensorﬂow.org/2016/07/\n15/lookback-rnn-attention-rnn.\n[31] L. C. Yang, S. Y . Chou, and Y . H. Yang. MidiNet:\nA convolutional generative adversarial network for\nsymbolic-domain music generation. In ISMIR , pages\n324–331, 2017.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 151"
    },
    {
        "title": "Deep Watershed Detector for Music Object Recognition.",
        "author": [
            "Lukas Tuggener",
            "Ismail Elezi",
            "Jürgen Schmidhuber",
            "Thilo Stadelmann"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492401",
        "url": "https://doi.org/10.5281/zenodo.1492401",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/225_Paper.pdf",
        "abstract": "Optical Music Recognition (OMR) is an important and challenging area within music information retrieval, the accurate detection of music symbols in digital images is a core functionality of any OMR pipeline. In this paper, we introduce a novel object detection method, based on synthetic energy maps and the watershed transform, called Deep Watershed Detector (DWD). Our method is specifically tailored to deal with high resolution images that contain a large number of very small objects and is therefore able to process full pages of written music. We present state-of-the-art detection results of common music symbols and show DWD's ability to work with synthetic scores equally well as with handwritten music.",
        "zenodo_id": 1492401,
        "dblp_key": "conf/ismir/TuggenerESS18",
        "keywords": [
            "Optical Music Recognition",
            "core functionality",
            "synthetic energy maps",
            "watershed transform",
            "Deep Watershed Detector",
            "high resolution images",
            "small objects",
            "full pages",
            "written music",
            "state-of-the-art detection results"
        ],
        "content": "DEEP WATERSHED DETECTOR FOR MUSIC OBJECT RECOGNITION\nLukas Tuggener1;3Ismail Elezi1;2\nJ¨urgen Schmidhuber3Thilo Stadelmann1\n1ZHAW Datalab, Zurich University of Applied Sciences, Winterthur, Switzerland\n2Dept. of Environmental Sciences, Informatics and Statistics, Ca’Foscari University of Venice, Italy\n3Faculty of Informatics, Universit `a della Svizzera Italiana, Lugano, Switzerland\ntugg@zhaw.ch, ismail.elezi@unive.it, juergen@idsia.ch, stdm@zhaw.ch\nABSTRACT\nOptical Music Recognition (OMR) is an important and\nchallenging area within music information retrieval, the\naccurate detection of music symbols in digital images is\na core functionality of any OMR pipeline. In this paper,\nwe introduce a novel object detection method, based on\nsynthetic energy maps and the watershed transform, called\nDeep Watershed Detector (DWD). Our method is speciﬁ-\ncally tailored to deal with high resolution images that con-\ntain a large number of very small objects and is therefore\nable to process full pages of written music. We present\nstate-of-the-art detection results of common music sym-\nbols and show DWD’s ability to work with synthetic scores\nequally well as with handwritten music.\n1. INTRODUCTION AND PROBLEM STATEMENT\nThe goal of Optical Music Recognition (OMR) is to trans-\nform images of printed or handwritten music scores into\nmachine readable form, thereby understanding the seman-\ntic meaning of music notation [2]. It is an important and\nactively researched area within the music information re-\ntrieval community. The two main challenges of OMR are:\nﬁrst the accurate detection and classiﬁcation of music ob-\njects in digital images; and second, the reconstruction of\nvalid music in some digital format. This work is focusing\nsolely on the ﬁrst task, meaning that we recover position\nand class (based on the shape only) of every object without\ninferring any higher level information.\nRecent progress in computer vision [9] thanks to the\nadaptation of convolutional neural networks (CNNs) [8,\n15] provide a solid foundation for the assumption that\nOMR systems can be drastically improved by using CNNs\nas well. Initial results of applying deep learning [26] to\nheavily restricted settings such as stafﬂine removal [25],\nsymbol classiﬁcation [20] or end-to-end OMR for mono-\nphonic scores [5], support such expectations.\nc\rLukas Tuggener, Ismail Elezi, J ¨urgen Schmidhuber,\nThilo Stadelmann. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: Lukas Tuggener, Is-\nmail Elezi, J ¨urgen Schmidhuber, Thilo Stadelmann. “Deep Watershed\nDetector for Music Object Recognition”, 19th International Society for\nMusic Information Retrieval Conference, Paris, France, 2018.In this paper, we introduce a novel general object detec-\ntion method called Deep Watershed Detector (DWD) mo-\ntivated by the following two hypotheses: a) deep learning\ncan be used to overcome the classical OMR approach of\nhaving hand-crafted pipelines of many preprocessing steps\n[21] by being able to operate in a fully data-driven fashion;\nb) deep learning can cope with larger, more complex inputs\nthan simple glyphs, thereby learning to recognize musical\nsymbols in their context. This will disambiguate meanings\n(e.g., between staccato and augmentation dots) and allow\nthe system to directly detect a complex alphabet.\nDWD operates on full pages of music scores in one pass\nwithout any preprocessing besides interline normalization\nand detects handwritten and digitally rendered music sym-\nbols without any restriction on the alphabet of symbols to\nbe detected. We further show that it learns meaningful rep-\nresentation of music notation and achieves state-of-the art\ndetection rates on common symbols.\nThe remaining structure of this paper is as follows: Sec.\n2 puts our approach in context with existing methods; in\nSec. 3 we derive our original end-to-end model, and give\na detailed explanation on how we use the deep watershed\ntransform for the task of object recognition; Sec. 4 reports\non experimental results of our system on the DeepScores\ndigitally rendered dataset in addition to the MUSCIMA++\nhandwritten dataset before we conclude in Sec. 5 with a\ndiscussion and give pointers for future research.\n2. RELATED WORK\nThe visual detection and recognition of objects is one of\nthe most central problems in the ﬁeld of computer vision.\nWith the recent developments of CNNs, many competing\nCNN-based approaches have been proposed to solve the\nproblem. R-CNNs [10], and in particular their succes-\nsors [23], are generally considered to be state-of-the-art\nmodels in object recognition, and many developed recog-\nnition systems are based on R-CNN. On the other hand, re-\nsearchers have also proposed models which are tailored to-\nwards computational efﬁciency instead of detection accu-\nracy. YOLO systems [22] and Single-Shot Detectors [18]\nwhile slightly compromising on accuracy, are signiﬁcantly\nfaster than R-CNN models, and can even achieve super\nreal-time performance.\nA common aspect of the above-mentioned methods is271Input: N*M*1\nRefine-Net\nOutput Featuremaps:\nN*M*256Energy map Me:\nN*M*#energy_levels\nClass map Mc:\nN*M*#classes\nBBox map Mb:\nN*M*2Base Network\n= 1x1 convolution\nFigure 1 . Illustration of the DWD network and its sub-components together with input and outputs. The outputs have been\ncropped to improve visibility\nthat they are speciﬁcally developed to work on cases where\nthe images are relatively small, and where images contain\na small number of relatively large objects [7, 17]. On the\ncontrary, musical sheets usually have high-resolution, and\ncontain a very large number of very small objects, making\nthe mentioned methods not suitable for the task.\nThe watershed transform is a well understood method\nthat has been applied to segmentation for decades [4].\nBai and Urtasun [1] were ﬁrst to propose combining the\nstrengths of deep learning with the power of this classi-\ncal method. They proposed to directly learn the energy\n(in our application the distance to an object center) for the\nwatershed transform such that all dividing ridges are at the\nsame height. As a consequence, the components can be ex-\ntracted by a cut at a single energy level without leading to\nover-segmentation. The model has been shown to achieve\nstate of the art performance on object segmentation.\nFor the most part, OMR detectors have been rule-based\nsystems working well only within a hard set of constraints\n[21]. Typically, they require domain knowledge, and work\nwell only on simple typeset music scores with a known\nmusic font, and a relatively small number of classes [24].\nWhen faced with low-quality images, complex or even\nhandwritten scores [3], the performance of these models\nquickly degrades, to some degree because errors propagate\nfrom one step to another [20]. Additionally, it is not clear\nwhat to do when the classes change, and in many cases,\nthis requires building the new model from scratch.\nIn response to the above mentioned issues some deep\nlearning based, data driven approaches have been devel-\noped. Hajic and Pecina [13] proposed an adaptation of\nFaster R-CNN with a custom region proposal mechanism\nbased on the morphological skeleton to accurately detect\nnoteheads, while Choi et al. [6] were able to detect ac-\ncidentals in dense piano scores with high accuracy, given\npreviously detected noteheads, that are being used as input-\nfeatures to the network. A big limitation of both ap-\nproaches is that the experiments have been done only on\na tiny vocabulary of the musical symbols, and therefore\ntheir scalability remains an open question.\nTo our knowledge, the best results so far has been re-\nported in the work of Pacha and Choi [19] where they ex-\nplored many models on the MUSCIMA++ [11] dataset of\nhandwritten music notation. They got the best results witha Faster R-CNN model, achieving an impressive score on\nthe standard mAP metric. A serious limitation of that work\nis that the system was not designed in an end-to-end fash-\nion and needs heavy pre- and post-processing. In particu-\nlar, they cropped the images in a context-sensitive way, by\ncutting images ﬁrst vertically and then horizontally, such\nthat each image contains exactly one staff and has a width-\nto-height-ratio of no more than 2 :1, with about 15% hor-\nizontal overlap to adjacent slices. In practice, this means\nthat all objects signiﬁcantly exceeding the size of such a\ncropped region will neither appear in the training nor test-\ning data, as only annotations that have an intersection-over-\narea of 0:8or higher between the object and the cropped\nregion are considered part of the ground truth. Further-\nmore, all the intermediate results must be combined to one\nconcise ﬁnal prediction, which is a non-trivial task.\n3. DEEP WATERSHED DETECTION\nIn this section we present the Deep Watershed Detector\n(DWD) as a novel object detection system, built on the\nidea of the deep watershed transform [1]. The watershed\ntransform [4] is a mathematically well understood method\nwith a simple core idea that can be applied to any topo-\nlogical surface. The algorithm starts ﬁlling up the surface\nfrom all the local minima, with all the resulting basins cor-\nresponding to connected regions. When applied to image\ngradients, the basins correspond to homogeneous regions\nof said image (see Fig. 2a). One key drawback of the wa-\ntershed transform is its tendency to over segment. This\nissue can be addressed by using the deep watershed trans-\nform. It combines the classical method with deep learning\nby training a deep neural network to create an energy sur-\nface based on an input image. This has the advantage that\none can design the energy surface to have certain proper-\nties. When designed in a way that all segmentation bound-\naries have energy zero, the watershed transform is reduced\nto a simple cutoff at a ﬁxed energy level (see Fig. 2b). An\nobjectness energy of this fashion has been used by Bai and\nUrtasun for instance segmentation [1]. Since we want to\ndo object detection, we further simplify the desired energy\nsurface to having small conical energy peaks of radius n\npixels at the center of each object and be zero everywhere\nelse (see Fig. 2c).272 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018a b c d ea)One-dimensional energy function of ﬁve\nclasses without any structural constraints.\na b c d e\nb)Energy function for the same ﬁve classes\nwith ﬁxed boundary energy.\na b c d e\nc)Energy function for the same ﬁve classes this time\nwith small energy markers at the class centers.\nFigure 2 . Illustration of the watershed transform applied\nto different one-dimensional functions.\nMore formally, we deﬁne our energy surface (or: energy\nmap) Meas follows:\nMe\n(i;j)=max8\n<\n:argmax\nc2C[Emax\u0001(1\u0000p\n(i\u0000ci)2+(j\u0000cj)2\nr)]\n0\n(1)\nwhere Me\n(i;j)is the value of Meat position (i; j),C\nis the set of all object centers and ci; cjare the center co-\nordinates of a given center c.Emax corresponds to the\nmaximum energy and ris the radius of the center marking.\nAt ﬁrst glance this deﬁnition might lead to the misin-\nterpretation that object centers that are closer together than\nrcannot be disambiguated using the watershed transform\nonMe. This is not the case since we can cut the energy\nmap at any given energy level between 1andEmax. How-\never, using this method it is not possible to detect multiple\nbounding boxes that share the exact same center.\n3.1 Retrieving Object Centers\nAfter computing an estimate ^Meof the energy map, we re-\ntrieve the coordinates of detected objects by the following\nsteps:\n1. Cut the energy map at a certain ﬁxed energy level\nand then binarize the result.\n2. Label the resulting connected components, using the\ntwo-pass algorithm [30]. Every component receives\na label lin1:::n, for every component olwe deﬁne\nOl\nindas the set of all tuples (i; j)for which the pixel\nwith coordinates jandiis part of ol:3. The center ^clof any component olis given by its\ncenter of gravity:\n^cl=ol\ncenter =jOl\nindj\u00001\u0001X\n(i;j)2Ol\nind(i; j) (2)\nWe use these component centers ^cas estimates for the ob-\nject centers c.\n3.2 Object Class and Bounding Box\nIn order to recover bounding boxes we do not only need\nthe object centers, but also the object classes and bounding\nbox dimensions. To achieve this we output two additional\nmaps McandMbas predictions of our network. Mcis\ndeﬁned as:\nMc\n(i;j)=(\n\u0003(i;j); ifMe\n(i;j)>0\n\u0003background ;otherwise(3)\nwhere \u0003backgroud is the class label indicating back-\nground and \u0003(i;j)is the class label associated with the cen-\ntercthat is closest to (i; j). We deﬁne our estimate for\nthe class of component olby a majority vote of all values\n^Mc\n(i;j)for all (i; j)2Ol\nind, where ^Mcis the estimate of\nMc. Finally, we deﬁne the bounding box map Mbas fol-\nlows:\nMb\n(i;j)=(\n(yl; xl);ifMe\n(i;j)>0\n(0;0); otherwise(4)\nwhere ylandxlare the width and height of the bound-\ning box for component ol. Based on this we deﬁne our\nbounding box estimation as the average of all estimations\nfor label l:\n(^yl;^xl) =jOl\nindj\u00001\u0001X\n(i;j)2Ol\nind^Mb\n(i;j) (5)\n3.3 Network Architecture and Losses\nAs mentioned above we use a deep neural network to pre-\ndict the dense output maps Me,McandMb(see Fig. 1).\nThe base neural network for this prediction can be any fully\nconvolutional network with the same input and output di-\nmensions. We use a ResNet-101 [12] (a special case of a\nHighway Net [27]) in conjunction with the elaborate Re-\nﬁneNet [16] upsampling architecture. For the estimators\ndeﬁned above it is crucial to have the highest spacial pre-\ndiction resolution possible. Our network has three output\nlayers, all of which are an 1by1convolution applied to the\nlast feature map of the ReﬁneNet.\n3.3.1 Energy prediction\nWe predict a quantized and one-hot encoded version of\nMe, called Meo, by applying a 1 by 1 convolution of depth\nEmax to the last feature map of the base network. The\nloss of the prediction ^Meo,losse, is deﬁned as the cross-\nentropy between Meoand^Meo.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 273a)Example result from DeepScores with detected bounding boxes as overlays. The tiny numbers are class labels from the\ndataset introduced with the overlay. This system is roughly one forth of the size of a typical DeepScores input we process at once.\nb)Example result from MUSCIMA++ with detected bounding boxes and class labels as overlays. This system is roughly\none half of the size of a typical processed MUSCIMA++ input. The images are random picks amongst inputs with many symbols.\nFigure 3 . Detection results for DeepScores andMUSCIMA++ examples, drawn on crops from corresponding input images.\n3.3.2 Class prediction\nWe again use the corresponding one-hot encoded version\nMcoand predict it using an 1by1convolution, with the\ndepth equal to the number of classes, on the last feature\nmap of the base network. The cross-entropy losscis calcu-\nlated between Mcoand^Mco. Since it is not the goal of this\nprediction to distinguish between foreground and back-\nground, all the loss stemming from locations with Me= 0\nwill get masked out.\n3.3.3 Bounding box prediction\nMbis predicted in its initial form using an 1by1convolu-\ntion of depth 2on the last feature map of the base network.\nThe bounding box loss lossbis the mean-squared differ-\nence between Mband ^Mb. For lossb, the components\nstemming from background locations will be masked out\nanalogous to lossc.\n3.3.4 Combined prediction\nWe want to jointly train in all tasks, therefore we deﬁne a\ntotal loss losstotas:\nlosstot=w1\u0003losse\nve+w2\u0003lossc\nvc+w3\u0003lossb\nvb(6)\nwhere the v:are running means of the corresponding losses\nand the scalars w:are hyper-parameters of the DWD net-\nwork. We purposefully use very short extraction heads of\none convolutional layer; by doing so we force the base net-\nwork to do all three tasks simultaneously. We expect this\nleads to the base network learning a meaningful represen-\ntation of music notation, from which it can extract the so-\nlutions of the three above deﬁned tasks.\n4. EXPERIMENTS AND RESULTS\n4.1 Used Datasets\nFor our experiments we use two datasets: DeepScores [29]\nandMUSCIMA++ [11].DeepScores is currently the largest publicly available\ndataset of musical sheets with ground truth for various ma-\nchine learning tasks, consisting of high-quality pages of\nwritten music, rendered at 400dots per inch. The dataset\nhas300;000full pages as images, containing tens of mil-\nlions of objects, separated in 123classes. We randomly\nsplit the set into training and testing, using 200kimages\nfor training and 50kimages each for testing and valida-\ntion. The dataset being so large allows efﬁcient training of\nlarge convolutional neural networks, in addition to being\nsuitable for transfer learning [32].\nMUSCIMA++ is a dataset of handwritten music no-\ntation for musical symbol detection. It contains 91;255\nsymbols spread into 140 pages, consisting of both nota-\ntion primitives and higher-level notation objects, such as\nkey signatures or time signatures. It features 105 object\nclasses. There are 23;352notes in the dataset, of which\n21;356have a full notehead, 1;648have an empty note-\nhead, and 348 are grace notes. We randomly split the\ndataset into training, validation, and testing, with the train-\ning set consisting of 110pages, while validation and test-\ning each consists of 15pages.\n4.2 Network Training and Experimental Setup\nWe pre-train our network in two stages in order to achieve\nreasonable results. First we train the ResNet on music\nsymbol classiﬁcation using the DeepScores classiﬁcation\ndataset [29]. Then, we train the ResNet and ReﬁneNet\njointly on semantic segmentation data also available from\nDeepScores . After this pre-training stage we are able to\nuse the network on the tasks deﬁned above in Sec. 3.3.\nSince music notation is composed of hierarchically or-\nganized sub-symbols, there does not exist a canonical way\nto deﬁne a set of atomic symbols to be detected (e.g., indi-\nvidual numbers in time signatures vs. complete time sig-\nnatures). We address this issue using a fully data-driven\napproach by detecting atomic classes as they are provided\nby the two datasets.274 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Class AP@1\n2Class AP@1\n4\nrest16th 0.8773 tuplet6 0.9252\nnoteheadBlack 0.8619 keySharp 0.9240\nkeySharp 0.8185 rest16th 0.9233\ntuplet6 0.8028 noteheadBlack 0.9200\nrestQuarter 0.7942 accidentalSharp 0.8897\nrest8th 0.7803 rest32nd 0.8658\nnoteheadHalf 0.7474 noteheadHalf 0.8593\nﬂag8thUp 0.7325 rest8th 0.8544\nﬂag8thDown 0.6634 restQuarter 0.8462\naccidentalSharp 0.6626 accidentalNatural 0.8417\naccidentalNatural 0.6559 ﬂag8thUp 0.8279\ntuplet3 0.6298 keyFlat 0.8134\nnoteheadWhole 0.6265 ﬂag8thDown 0.7917\ndynamicMF 0.5563 tuplet3 0.7601\nrest32nd 0.5420 noteheadWhole 0.7523\nﬂag16thUp 0.5320 fClef 0.7184\nrestWhole 0.5180 restWhole 0.7183\ntimeSig8 0.5180 dynamicPiano 0.7069\naccidentalFlat 0.4949 accidentalFlat 0.6759\nkeyFlat 0.4685 ﬂag16thUp 0.6621\nTable 1 . AP with overlap 0:5and overlap 0:25for the\ntwenty best detected classes of the DeepScores dataset.\nWe rescale every input image to the desired interline\nvalue. We use 10pixels for DeepScores and20pixels for\nMUSCIMA++ . Other than that we apply no preprocessing.\nWe do not deﬁne a subset of target objects for our exper-\niments, but attempt to detect all classes for which there\nis ground truth available. We always feed single images\nto the network, i.e. we only use batch size = 1. During\ntraining we crop the full page input (and the ground truth)\nto patches of 960by960pixels using randomized coordi-\nnates. This serves two purposes: it saves GPU memory and\nperforms efﬁcient data augmentation. This way the net-\nwork never sees the exact same input twice, even if we train\nfor many epochs. For all of the results described below we\ntrain individually on losse,losscandlossband then reﬁne\nthe training using losstot. It turns out that the prediction of\nMeis the most fragile to effects introduced by training on\nthe other losses, therefore we retrain on losseagain after\ntraining on the individual losses in the order deﬁned above,\nbefore moving on to losstot. All the training is done using\nthe RMSProp optimizer [28] with a learning rate of 0:001\nand a decay rate of 0:995.\nSince our design is invariant to how many objects are\npresent on the input (as long as their centers do not over-\nlap) and we want to obtain bounding boxes for full pages\nat once, we feed whole pages to the network at inference\ntime. The maximum input size is only bounded by the\nmemory of the GPU. For typical pieces of sheet music this\nis not an issue, but pieces that use very small interline val-\nues (e.g. pieces written for conductors) result in very large\ninputs due to the interline normalization. At about 10:5\nmillion pixels even a Tesla P40 with 24gigabytes runs out\nof memory.\n4.3 Results and Discussion\nTable 1 shows the average precision (AP) for the twenty\nbest detected classes with an overlap of the detectedClass AP@1\n2Class AP@1\n4\nhalf-rest 0.8981 whole-rest 0.9762\nﬂat 0.8752 ledger-line 0.9163\nnatural 0.8531 half-rest 0.8981\nwhole-rest 0.8226 ﬂat 0.8752\nnotehead-full 0.8044 natural 0.8711\nsharp 0.8033 stem 0.8377\nnotehead-empty 0.7475 staccato-dot 0.8302\nstem 0.7426 notehead-full 0.8298\nquarter-rest 0.6699 sharp 0.8121\n8th-rest 0.6432 tenuto 0.7903\nf-clef 0.6395 notehead-empty 0.7475\nnumeral-4 0.6391 duration-dot 0.7285\nletter-c 0.6313 numeral-4 0.7158\nletter-c 0.6313 8th-ﬂag 0.7055\n8th-ﬂag 0.6051 quarter-rest 0.6849\nslur 0.5699 letter-c 0.6643\nbeam 0.5188 letter-c 0.6643\ntime-signature 0.4940 8th-rest 0.6432\nstaccato-dot 0.4793 beam 0.6412\nletter-o 0.4793 f-clef 0.6395\nTable 2 . AP with overlap 0:5and overlap 0:25for the\ntwenty best detected classes from MUSCIMA++ .\nbounding box and ground truth of 50% and25%, respec-\ntively. We observe that in both cases there are common\nsymbol classes that get detected very well, but there is also\na steep fall off. The detection rate outside the top twenty\ncontinues to drop and is almost zero for most of the rare\nclasses. We further observe that there is a signiﬁcant per-\nformance gain for the lower overlap threshold, indicating\nthat the bounding-box regression is not very accurate.\nFig. 3 shows an example detection for qualitative anal-\nysis. It conﬁrms the conclusions drawn above. The rarest\nsymbol present, an arpeggio, is not detected at all, while\nthe bounding boxes are sometimes inaccurate, especially\nfor large objects (note that stems, bar-lines and beams are\nnot part of the DeepScores alphabet and hence do not con-\nstitute missed detections). On the other hand, staccato dots\nare detected very well. This is surprising since they are typ-\nically hard to detect due to their small size and the context-\ndependent interpretation of the symbol shape (compare the\ndots in dotted notes or F-clefs). We attribute this to the op-\nportunity of detecting objects in context, enabled by train-\ning on larger parts of full raw pages of sheet music in con-\ntrast to the classical processing of tiny, pre-processed im-\nage patches or glyphs.\nThe results for the experiments on MUSCIMA++ in\nTab. 2 and Fig. 3b show a very similar outcome. This is in-\ntriguing because it suggests that the difﬁculty in detecting\ndigitally rendered and handwritten scores might be smaller\nthan anticipated. We attribute this to the fully data-driven\napproach enabled by deep learning instead of hand-crafted\nrules for handling individual symbols. It is worth noting\nthat ledger-lines are detected with very high performance\n(see AP@1\n4). This explains the relatively poor detection of\nnote-heads on MUSCIMA++ , since they tend to overlap.\nFig. 4 shows an estimate for a class map with its corre-\nsponding input overlayed. Each color corresponds to one\nclass. This ﬁgure proofs that the network is learning a sen-\nsible representation of music notation: even though it isProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 275Figure 4 . Estimate of a class map ^Mcfor every input pixel\nwith the corresponding MUSCIMA++ input overlayed.\nonly trained to mark the centers of each object with the cor-\nrect colors, it learns a primitive segmentation mask. This is\nbest illustrated by the (purple) segmentation of the beams.\n5. CONCLUSIONS AND FUTURE WORK\nWe have presented a novel method for object detection that\nis speciﬁcally tailored to detect many tiny objects on large\ninputs. We have shown that it is able to detect common\nsymbols of music notation with high precision, both in\ndigitally rendered music as well as in handwritten music,\nwithout a drop in performance when moving to the ”more\ncomplicate” handwritten input. This suggests that deep\nlearning based approaches are able to deal with handwrit-\nten sheets just as well as with digitally rendered ones, addi-\ntionally to their beneﬁt of recognizing objects in their con-\ntext and with minimal preprocessing as compared to clas-\nsical OMR pipelines. Pacha et al. [19] show that higher de-\ntection rates, especially for uncommon symbols, are pos-\nsible when using R-CNN on small snippets (cp. Fig. 5).\nDespite their higher scores, it is unclear how recognition\nperformance is affected when results of overlapping and\npotentially disagreeing snippets are aggregated to full page\nresults. A big advantage of our end-to-end system is the\ncomplete avoidance of error propagation in longer recog-\nnition pipeline of independent components like classiﬁers,\naggregators, etc [14]. Moreover, our full-page end-to-end\napproach has the advantages of speed (compared to a slid-\ning window patch classiﬁer), change of domain (we use\nthe same architecture for both the digital and handwrit-\nten datasets) and is easily integrated into complete OMR\nframeworks.\nArguably the biggest problem we faced is that sym-\nbol classes in the dataset are heavily unbalanced. In the\nDeepScores dataset in particular, the class notehead con-\ntains more than half of all the symbols in the entire dataset,\nwhile the top 10classes contain more than 85% of the sym-\nbols. Considering that we did not do any class-balancing\nwhatsoever, this imbalance had its effect in training. We\nFigure 5 . Typical input snippet used by Pacha et al. [19]\nFigure 6 . Evolution of lossb(on the ordinate) of a suf-\nﬁciently trained network, when training for another 8000\niterations (on the abscissa).\nobserve that in cases where the symbol is common, we get\na very high average precision, but it quickly drops when\nsymbols become less common. Furthermore, it is inter-\nesting to observe that the neural network actually forgets\nabout the existence of these rarer symbols: Fig. 6 depicts\nthe evolution of lossbof a network that is already trained\nand gets further trained for another 8;000iterations. When\nfaced with an image containing rare symbols, the initial\nloss is larger than the loss on more common images. But\nto our surprise, later during the training process, the loss\nactually increases when the net encounters rare symbols\nagain, giving the impression that the network is actually\ntreating these symbols as outliers and ignoring them.\nFuture work will thus concentrate on dealing with the\ncatastrophic imbalance in the data to successfully train\nDWD to detect all classes. We believe that the solution\nlies in a combination of data augmentation and improved\ntraining regimes (i.e. sample pages containing rare objects\nmore often, synthesizing mock pages ﬁlled with rare ob-\njects etc.).\nAdditionally, we plan to investigate the ability of our\nmethod beyond OMR on natural images. Initially we will\napproach canonical datasets like PASCAL VOC [7] and\nMS-COCO [17] that have been at the front-line of object\nrecognition tasks. However, images in those datasets are\nnot exactly natural, and for the most part they are simplistic\n(small images, containing a few large objects). Recently,\nresearchers have been investigating the ability of state-of-\nthe-art recognition systems on more challenging natural\ndatasets, like DOTA [31], and unsurprisingly, the results\nleave much to be desired. The DOTA dataset shares a lot of\nsimilarities with musical datasets, with images being high\nresolution and containing hundreds of small objects, mak-\ning it a suitable benchmark for our DWD method to recog-\nnize tiny objects.\nAcknowledgements This work is ﬁnancially supported\nby CTI grant 17963.1 PFES-ES “DeepScore”. The authors\nare grateful for the collaboration with ScorePad AG.276 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20186. REFERENCES\n[1] Min Bai and Raquel Urtasun. Deep watershed trans-\nform for instance segmentation. In 2017 IEEE Con-\nference on Computer Vision and Pattern Recognition,\nCVPR 2017, Honolulu, HI, USA, July 21-26, 2017 ,\npages 2858–2866, 2017.\n[2] David Bainbridge and Tim Bell. The challenge of opti-\ncal music recognition. Computers and the Humanities ,\n35.2:95 – 121, 2001.\n[3] Arnau Baro, Pau Riba, and Alicia Forn ´es. Towards\nthe recognition of compound music notes in handwrit-\nten music scores. In 15th International Conference on\nFrontiers in Handwriting Recognition, ICFHR 2016,\nShenzhen, China, October 23-26, 2016 , pages 465–\n470, 2016.\n[4] Serge Beucher et al. The watershed transforma-\ntion applied to image segmentation. SCANNING\nMICROSCOPY-SUPPLEMENT- , pages 299–299,\n1992.\n[5] Jorge Calvo-Zaragoza, Jose J. Valero-Mas, and Anto-\nnio Pertusa. End-to-end optical music recognition us-\ning neural networks. In Proceedings of the 18th Inter-\nnational Society for Music Information Retrieval Con-\nference, ISMIR 2017, Suzhou, China, October 23-27,\n2017 , pages 472–477, 2017.\n[6] Kwon-Young Choi, Bertrand Co ¨uasnon, Yann Ricque-\nbourg, and Richard Zanibbi. Bootstrapping samples of\naccidentals in dense piano scores for cnn-based de-\ntection. In 12th International Workshop on Graphics\nRecognitio, 14th IAPR International Conference on\nDocument Analysis and Recognition, GREC@ICDAR\n2017, Kyoto, Japan, November 9-15, 2017 , pages 19–\n20, 2017.\n[7] Mark Everingham, Luc J. Van Gool, Christopher K. I.\nWilliams, John M. Winn, and Andrew Zisserman. The\npascal visual object classes (VOC) challenge. Inter-\nnational Journal of Computer Vision , 88(2):303–338,\n2010.\n[8] Kunihiko Fukushima and Sei Miyake. Neocognitron:\nA new algorithm for pattern recognition tolerant of de-\nformations and shifts in position. Pattern Recognition ,\n15(6):455–469, 1982.\n[9] Ross Girshick, Ilija Radosavovic, Georgia\nGkioxari, Piotr Doll ´ar, and Kaiming He. Detectron.\nhttps://github.com/facebookresearch/\ndetectron , 2018.\n[10] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and\nJitendra Malik. Rich feature hierarchies for accurate\nobject detection and semantic segmentation. In 2014\nIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2014, Columbus, OH, USA, June\n23-28, 2014 , pages 580–587, 2014.\n[11] Jan Hajic and Pavel Pecina. The MUSCIMA++ datasetfor handwritten optical music recognition. In 14th\nIAPR International Conference on Document Analysis\nand Recognition, ICDAR 2017, Kyoto, Japan, Novem-\nber 9-15, 2017 , pages 39–46, 2017.\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition. In\n2016 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2016, Las Vegas, NV , USA,\nJune 27-30, 2016 , pages 770–778, 2016.\n[13] Jan Hajic Jr. and Pavel Pecina. Detecting noteheads in\nhandwritten scores with convnets and bounding box re-\ngression. CoRR , abs/1708.01806, 2017.\n[14] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner.\nGradient-based learning applied to document recog-\nnition. Proceedings of the IEEE , 86(11):2278–2324,\n1998.\n[15] Yann LeCun, Bernhard E. Boser, John S. Denker, Don-\nnie Henderson, Richard E. Howard, Wayne E. Hub-\nbard, and Lawrence D. Jackel. Backpropagation ap-\nplied to handwritten zip code recognition. Neural Com-\nputation , 1(4):541–551, 1989.\n[16] Guosheng Lin, Anton Milan, Chunhua Shen, and\nIan D. Reid. Reﬁnenet: Multi-path reﬁnement net-\nworks for high-resolution semantic segmentation. In\n2017 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2017, Honolulu, HI, USA,\nJuly 21-26, 2017 , pages 5168–5177, 2017.\n[17] Tsung-Yi Lin, Michael Maire, Serge J. Belongie,\nJames Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C. Lawrence Zitnick. Microsoft COCO:\ncommon objects in context. In Computer Vision -\nECCV 2014 - 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings, Part\nV, pages 740–755, 2014.\n[18] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Chris-\ntian Szegedy, Scott E. Reed, Cheng-Yang Fu, and\nAlexander C. Berg. SSD: single shot multibox detec-\ntor. In Computer Vision - ECCV 2016 - 14th European\nConference, Amsterdam, The Netherlands, October 11-\n14, 2016, Proceedings, Part I , pages 21–37, 2016.\n[19] Alexander Pacha, Kwon-Young Choi, Bertrand\nCo¨uasnon, Yann Ricquebourg, and Richard Zanibbi.\nHandwritten music object detection: Open issues and\nbaseline results. In Proceedings of the 13th IAPR In-\nternational Workshop on Document Analysis Systems,\nVienna, April 2018 .\n[20] Alexander Pacha and Horst Eidenberger. Towards self-\nlearning optical music recognition. In 16th IEEE In-\nternational Conference on Machine Learning and Ap-\nplications, ICMLA 2017, Cancun, Mexico, December\n18-21, 2017 , pages 795–800, 2017.\n[21] Ana Rebelo, G. Capela, and Jaime S. Cardoso. Optical\nrecognition of music symbols - A comparative study.\nIJDAR , 13(1):19–31, 2010.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 277[22] Joseph Redmon and Ali Farhadi. YOLO9000: better,\nfaster, stronger. In 2017 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2017, Hon-\nolulu, HI, USA, July 21-26, 2017 , pages 6517–6525,\n2017.\n[23] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian\nSun. Faster R-CNN: towards real-time object detection\nwith region proposal networks. In Advances in Neural\nInformation Processing Systems 28: Annual Confer-\nence on Neural Information Processing Systems 2015,\nDecember 7-12, 2015, Montreal, Quebec, Canada ,\npages 91–99, 2015.\n[24] Florence Rossant and Isabelle Bloch. Robust and adap-\ntive OMR system including fuzzy modeling, fusion of\nmusical rules, and possible error detection. EURASIP\nJ. Adv. Sig. Proc. , 2007, 2007.\n[25] Antonio Javier Gallego S ´anchez and Jorge Calvo-\nZaragoza. Staff-line removal with selectional auto-\nencoders. Expert Syst. Appl. , 89:138–148, 2017.\n[26] J ¨urgen Schmidhuber. Deep learning in neural net-\nworks: An overview. Neural Networks , 61:85–117,\n2015.\n[27] Rupesh Kumar Srivastava, Klaus Greff, and Juer-\ngen Schmidhuber. Training very deep networks. In\nAdvances in Neural Information Processing Systems\n(NIPS) , 2015.[28] Tijmen Tieleman and Geoffrey E. Hinton. Lecture 6.5-\nrmsprop: Divide the gradient by a running average of\nits recent magnitude. COURSERA: Neural networks\nfor machine learning 4.2 , pages 26–31, 2012.\n[29] Lukas Tuggener, Ismail Elezi, Jurgen Schmidhuber,\nMarcello Pelillo, and Thilo Stadelmann. Deepscores\n- a dataset for segmentation, detection and classiﬁ-\ncation of tiny objects. International Conference on\nPattern Recognition , 2018. Preprint arXiv:1804.00525\n[cs.CV], March 2018.\n[30] Kesheng Wu, Ekow Otoo, and Kenji Suzuki. Opti-\nmizing two-pass connected-component labeling algo-\nrithms. Pattern Anal. Appl. , 12(2):117–135, February\n2009.\n[31] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu,\nSerge J. Belongie, Jiebo Luo, Mihai Datcu, Marcello\nPelillo, and Liangpei Zhang. DOTA: A large-scale\ndataset for object detection in aerial images. CoRR ,\nabs/1711.10398, 2017.\n[32] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. How transferable are features in deep neural\nnetworks? In Advances in Neural Information Pro-\ncessing Systems 27: Annual Conference on Neural In-\nformation Processing Systems 2014, December 8-13\n2014, Montreal, Quebec, Canada , pages 3320–3328,\n2014.278 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Deep Neural Networks with Voice Entry Estimation Heuristics for Voice Separation in Symbolic Music Representations.",
        "author": [
            "Reinier de Valk",
            "Tillman Weyde"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492403",
        "url": "https://doi.org/10.5281/zenodo.1492403",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/304_Paper.pdf",
        "abstract": "In this study we explore the use of deep feedforward neural networks for voice separation in symbolic music representations. We experiment with different network architectures, varying the number and size of the hidden layers, and with dropout. We integrate two voice entry estimation heuristics that estimate the entry points of the individual voices in the polyphonic fabric into the models. These heuristics serve to reduce error propagation at the beginning of a piece, which, as we have shown in previous work, can seriously hamper model performance. The models are evaluated on the 48 fugues from Johann Sebastian Bach's The Well-Tempered Clavier and his 30 inventions—a dataset that we curated and make publicly available. We find that a model with two hidden layers yields the best results. Using more layers does not lead to a significant performance improvement. Furthermore, we find that our voice entry estimation heuristics are highly effective in the reduction of error propagation, improving performance significantly. Our best-performing model outperforms our previous models, where the difference is significant, and, depending on the evaluation metric, performs close to or better than the reported state of the art.",
        "zenodo_id": 1492403,
        "dblp_key": "conf/ismir/ValkW18",
        "keywords": [
            "deep feedforward neural networks",
            "voice separation",
            "symbolic music representations",
            "different network architectures",
            "hidden layers",
            "dropout",
            "voice entry estimation heuristics",
            "polyphonic fabric",
            "previous work",
            "error propagation"
        ],
        "content": "DEEP NEURAL NETWORKS WITH VOICE ENTRY ESTIMATION\nHEURISTICS FOR VOICE SEPARATION IN SYMBOLIC MUSIC\nREPRESENTATIONS\nReinier de Valk\nJukedeck Ltd.\nreinier@jukedeck.comTillman Weyde\nDepartment of Computer Science\nCity, University of London\nt.e.weyde@city.ac.uk\nABSTRACT\nIn this study we explore the use of deep feedforward neu-\nral networks for voice separation in symbolic music rep-\nresentations. We experiment with different network archi-\ntectures, varying the number and size of the hidden layers,\nand with dropout. We integrate two voice entry estimation\nheuristics that estimate the entry points of the individual\nvoices in the polyphonic fabric into the models. These\nheuristics serve to reduce error propagation at the begin-\nning of a piece, which, as we have shown in previous work,\ncan seriously hamper model performance.\nThe models are evaluated on the 48 fugues from Johann\nSebastian Bach’s The Well-Tempered Clavier and his 30\ninventions—a dataset that we curated and make publicly\navailable. We ﬁnd that a model with two hidden layers\nyields the best results. Using more layers does not lead to\na signiﬁcant performance improvement. Furthermore, we\nﬁnd that our voice entry estimation heuristics are highly\neffective in the reduction of error propagation, improv-\ning performance signiﬁcantly. Our best-performing model\noutperforms our previous models, where the difference is\nsigniﬁcant, and, depending on the evaluation metric, per-\nforms close to or better than the reported state of the art.\n1. INTRODUCTION\nIn the domain of symbolic music representation, the term\nvoice separation denotes the identiﬁcation of individual\nlines ( voices ) in polyphonic music. More formally, it\ncan be deﬁned as “the task of separating a musical work\nconsisting of multi-note sonorities into independent con-\nstituent voices” [3]. With regard to the term voice itself,\nwhose meaning is left ambiguous in the above deﬁnition,\na distinction can be made between (i) a voice as a mono-\nphonic sequence of successive, non-overlapping notes, and\n(ii) a voice as a perceptually independent, but not nec-\nessarily monophonic, sequence of notes or multi-note si-\nmultaneities [3]. The former deﬁnition corresponds to the\nc\rReinier de Valk, Tillman Weyde. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Reinier de Valk, Tillman Weyde. “Deep neural networks\nwith voice entry estimation heuristics for voice separation in symbolic\nmusic representations”, 19th International Society for Music Information\nRetrieval Conference, Paris, France, 2018.music-theoretical notion of a voice (also part) [3,9], while\nthe latter corresponds to the music-psychological notion of\nanauditory stream [2]. For certain genres of music—e.g.,\npiano sonatas or string quartets—it is more appropriate to\nthink of the polyphonic fabric as consisting of multiple\nstreams that may or may not be (partly) monophonic.\nV oice separation, especially in monotimbral polyphonic\nmusic (e.g., harpsichord or lute music) for more than three\nconcurrent voices, has been recognised as a difﬁcult task\neven for professional musicians [15, 16, 30]. From a mu-\nsic information retrieval (MIR) perspective, voice separa-\ntion is considered a challenge that has not yet been ad-\ndressed satisfactorily. It is, however, an important task:\nan adequate identiﬁcation of the individual voices is a pre-\nrequisite for tackling several open MIR and musicological\nproblems, such as automatic transcription [1], pattern re-\ntrieval [11, 26, 29], and melodic querying [24, 36].\nOver the past decade, deep neural networks (DNNs)\nhave been successfully applied to various computer vision,\nspeech recognition, and natural language processing tasks,\nand, increasingly, to MIR tasks [5]. Consisting of multi-\nple processing layers, DNNs can learn representations of\ndata with multiple levels of abstraction [25], which makes\nthem better suited than their shallow counterparts to model\ncomplex input-output relationships. Despite their success-\nful application to a number of MIR tasks, DNNs have not\nyet been used for voice separation.\nThe main contributions of this paper are:\n\u000fthe implementation and evaluation of DNNs for\nvoice separation in symbolic music representations;\n\u000fthe implementation and evaluation of improved\nvoice entry estimation heuristics ;\n\u000fthe creation of a public benchmark dataset for voice\nseparation, which currently does not exist.\nWe show that a model that combines a DNN with the voice\nentry estimation heuristics performs close to or better than\nthe reported state of the art.\nIn what follows, in Section 2, related work is discussed.\nIn Section 3, the model and the integrating framework are\npresented, and in Section 4, the evaluation method is ex-\nplained. Section 5 is dedicated to the voice entry estima-\ntion heuristics, and Section 6 to the dataset. In Section 7,\nthe experimental results are discussed, and in Section 8,\nconclusions and directions for future work are presented.2812. RELATED WORK\nThe existing models addressing the task of voice separation\ncan be divided into two categories: rule-based models and\nmachine learning models. A characteristic that the models\nin both categories share is that they almost all lean heavily\non at least one of two perceptual principles fundamental\nin auditory stream segregation, coined the Pitch Proximity\nPrinciple and the Principle of Temporal Continuity in [16].\nThese principles dictate that the closer two notes are to one\nanother in terms of pitch or time, respectively, the more\nlikely they are perceived as belonging to the same voice.\n2.1 Rule-based models\nThe rule-based models form the largest category, contain-\ning a wide array of approaches. In [34], a preference rule\nsystem for contrapuntal analysis is presented. Preference\nrules are criteria by which a possible analysis is evalu-\nated. Dynamic programming techniques are used to limit\nthe amount of possible analyses to be evaluated. In later\nwork [35], a probabilistic model of polyphonic music anal-\nysis, incorporating a stream segregation component that\nbuilds on the earlier work, is introduced. Inspired by [34]\nis the algorithm presented in [27], which consists of a voice\nconﬁguration unit generating well-formed local solutions,\nand a note assignment unit calculating a preferred solution.\nIn [4], a contig mapping approach is presented, in which\nthe music is divided into segments where a constant num-\nber of voices is active, the contigs. Starting from the con-\ntigs where the number of voices active equals the nominal\nnumber of voices, the optimal connections to the neigh-\nbouring contigs are determined. Gradually branching out,\nthis process is repeated until all contigs are connected.\nA modiﬁed version of this approach is proposed in [17],\nwhere the connection of contigs that share a boundary at\nwhich the number of voices increases is prioritised. The\nidea is that the distinctiveness (in terms of pitch distance)\nof the new voice will prevent it from being connected in-\ncorrectly to one of the voices active in the smaller-size con-\ntig. A further improvement of the approach is described\nin [14], where, taking into account more context informa-\ntion, additional criteria that underly the contig connection\npolicy are proposed. The criteria are weighted using a ge-\nnetic algorithm with mutation and crossover operators.\nIn [33], voice separation is modelled as a clustering\nproblem. Using an agglomerative single-link clustering al-\ngorithm, in an iterative process that starts from an initial\ndistribution in which each note is a cluster, all clusters are\ncombined into larger clusters until nsimultaneous clusters,\nthe voices, remain. In [10], the music is modelled as a di-\nrected graph. The goal is to create a set of disjoint paths,\nthe voices, through the graph. To this end, the graph is di-\nvided into segments, which are analysed through constraint\nsatisfaction optimisation. Using a sequence alignment al-\ngorithm, the analyses are then connected.\nTwo models stand out as they allow for non-\nmonophonic voices. In the local optimisation approach\nproposed in [21], a piece is partitioned into slices that\nare processed iteratively, assigning the notes to voices. Astochastic local search algorithm is used to ﬁnd assign-\nments that minimise a parametric cost function assessing\nthe assignments; weighting the parameters in a certain\nway can result in non-monophonic assignments. In the\nV oice Integration/Segregation Algorithm (VISA) as pro-\nposed in [19, 20] and later reﬁned in [31], vertical inte-\ngration —concurrent notes with the same onset and dura-\ntion merging perceptually into a single sonority—is con-\nsidered to be prior to horizontal integration —successive\nnotes close in pitch and time merging perceptually into a\nsingle voice. VISA thus ﬁrst identiﬁes concurrent notes\nthat merge into single sonorities, and, using a bipartite\nmatching algorithm, then assigns the sonorities to separate\nstreams.\n2.2 Machine learning models\nIn [23], V oiSe, a system for separating voices in both im-\nplicit and explicit polyphony, is presented. The system\nconsists of two components: a same-voice predicate im-\nplemented as a learned decision tree, which determines\nwhether or not two notes belong to the same voice, and\na hard-coded algorithm that maps notes to voices.\nA probabilistic, Markov chain-like, system is proposed\nin [18]. Based on pitch information only, the system learns\nhow likely a note is to occur for a voice, as well as how\nlikely a transition between two notes is to occur. The sys-\ntem is inspired by [4] in that the music is processed in\na similar manner—starting at chords in which all voices\nare present. Another probabilistic approach is described\nin [6], where the music is represented as a sequence of\nchords, and a discrete hidden Markov model (HMM) is\nused to determine the most likely sequence of mappings\nto voices (the hidden states) for the chords (the observa-\ntions). A similar, although more sophisticated, approach\nusing an HMM is proposed in [28]. This model explicitly\nallows notes within a single voice to overlap. This not only\nmakes preprocessing (quantisation) redundant, but also en-\nables application to data generated from live performance.\nIn [6], the task of voice separation is modelled both as a\nmulti-class classiﬁcation problem (see also [7]), where the\nmusic is represented as a sequence of notes, which are as-\nsigned to voices (the classes), and as a regression problem,\nwhere the music is represented as a sequence of chords,\nfor which mappings to voices are rated. Standard single-\nhidden layer feedforward neural networks are used as the\nclassiﬁer and regressor, respectively. In [13], too, the mu-\nsic is represented as a sequence of chords, and a single-\nhidden layer feedforward neural network is used to greed-\nily assign each chord note to the voice that maximises a\ntrained assignment probability.\n3. PROBLEM FORMULATION, MODEL, AND\nFRAMEWORK\nAs in [6, 7], in this paper we formulate the task of voice\nseparation as a multi-class classiﬁcation problem, where\neach note in a piece is assigned to one of vvoices (the\nclasses). We assume that a voice is always monophonic282 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018(see Section 1), and that the number of voices in a piece is\nequal to its nominal number of voices, which we infer from\nthe size of its largest chord. (These assumptions do not al-\nways hold true, but in pure contrapuntal music one gener-\nally ﬁnds only few exceptions. We resolve such cases by\nremoving the offending notes from the dataset; this is dis-\ncussed in Section 6.) Furthermore, for practical reasons we\nset the maximum value of vto 5. This enables us to pro-\ncess pieces containing up to ﬁve voices, which currently\nsufﬁces. The maximum number of voices determines the\nnumber of classes and hence the size of the neural net-\nwork’s output layer; for the sake of efﬁciency, it should\nthus be kept as small as possible.\n3.1 Model\nWe use the open source TensorFlow machine learning li-\nbrary1(version 1.6.0) to implement a multi-layer deep\nfeedforward neural network that uses the rectiﬁed linear\nunit activation function for all L\u00001hidden layers and\nthe softmax activation function for the output layer, and\nthat has ﬁve output neurons, each representing a class.\nGiven that our dataset is relatively small, we use batch\ntraining, where we check the performance on the valida-\ntion set (comprising every ﬁfth training example) every\n10 epochs as early stopping strategy, and store the earliest\nbest-performing model. We use Xavier initialisation [12]\nfor the weights and initialisation with zeros for the bi-\nases, the Adam optimisation algorithm [22] to minimise\nthe cross-entropy loss, and dropout [32] to prevent overﬁt-\nting. We set the learning rate to 0.01 and the number of\ntraining epochs to 600, values we observed to work well.\nThree further hyperparameters are optimised using a grid\nsearch (see Section 7): the dropout keep probability , the\nnumber of hidden layers, and the size of the hidden layers.\n3.2 Framework\nWe integrate the model in our previously developed frame-\nwork for data preprocessing, feature extraction, and cross-\nvalidated training and evaluation [6], implemented in\nJava.2In this framework, the music is represented as a se-\nquence of notes, by default ordered by (i) onset time (low\nto high) and (ii) pitch (low to high). When evaluating the\nmodel, this sequence is processed in linear fashion, where\nfor each note a feature vector is calculated that is given as\ninput to the model, which then makes a class decision—\nthus assigning the note to a voice.\n3.2.1 Feature vector\nEach note is represented by a 33-dimensional feature vec-\ntor, containing properties of that note in its polyphonic\ncontext. The features are handcrafted and can be divided\ninto four categories of increasing scope: (i) note-level fea-\ntures, encoding individual properties of the note; (ii) note-\nchord features, encoding the note’s position in the chord;\n(iii) chord-level features, encoding properties shared by all\n1https://www.tensorflow.org/\n2https://www.github.com/reinierdevalk/voice_\nseparation/notes in the chord; and (iv) polyphonic embedding fea-\ntures, encoding the note’s polyphonic relation to the notes\nin the previous as well as the current chord. All feature\nvalues (except certain default values) are scaled to fall in\nthe range [0, 1]. An overview is presented in Table 1; more\ndetail is provided in [6].\nIndex Feature Description\n0 pitch pitch, as a MIDI number\n1 duration duration, in whole notes\n2 isOrnamentation true (1) if a 16th note or\nshorter, false (0) if not\n3 indexInChord index (pitch-based) in\nthe chord\n4 pitchDistBelow distance to note below\n5 pitchDistAbove distance to note above\n6 chordSize number of chord notes\n7 metricPosition metric position in the bar\n8 numNotesNext number of notes (onsets)\nin the next chord\n9-12 intervals intervals in the chord\n13-17 pitchProx for each voice v, the\npitch proximity to the\nadjacent left note in v\n18-22 interOnsetProx idem, inter-onset\n23-27 offsetOnsetProx idem, offset-onset\n28-32 voicesOccupied for each voice v, whether\nit is currently occupied\n(1) or not (0)\nTable 1 . The feature vector, containing note-level (0-2),\nnote-chord (3-5), chord-level (6-12), and polyphonic em-\nbedding features (13-32). Pitch distances and intervals are\nmeasured in semitones; proximities are inverted distances.\n4. EVALUATION\nWe evaluate the models using k-fold cross-validation. Be-\ncause it is not desirable that identical or highly similar sam-\nples extracted from one piece end up in both the training\nand the test set, we partition a dataset along its individual\npieces rather than randomly. kthus equals the number of\npieces in a dataset; each piece in it serves as test set once.\n4.1 Evaluation metrics\nWe use four metrics to assess model performance. Accu-\nracy is a per-note metric that measures the proportion of\nnotes that have been assigned to the correct voice:\nacc=jCj\njNj; (1)\nwhere Cis the set of notes assigned to the correct voice,\nandNthe set of all notes.\nSoundness andcompleteness are complementary met-\nrics that measure transitions between note pairs. We use\nthe deﬁnitions provided in [23]. If fis an assignedProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 283voice and ga correct voice, then a pair of adjacent notes\n(nt; nt+1)infis considered sound ifg(nt) =g(nt+1)\nholds. (Note that, according to this deﬁnition, fandgneed\nnot be the same voice.) Extending the deﬁnition, we take\nsoundness to be the proportion of sound pairs in allvoices:\nsnd=jSj\njPj; (2)\nwhere Sis the set of sound pairs, and Pthe set of all pairs\nin all assigned voices f. Similarly, a pair of adjacent notes\n(nt; nt+1)in correct voice gis considered complete if as-\nsigned voice f(nt) =f(nt+1)holds. We take complete-\nness to be the proportion of complete pairs in all voices:\ncmp=jCj\njPj; (3)\nwhere Cis the set of complete pairs, and Pthe set of all\npairs in all correct voices g.3\nAverage voice consistency (A VC), coined in [4], mea-\nsures, “on average, the proportion of notes from the same\nvoice that have been assigned . . . to the same voice”. The\nvoice consistency (VC) for voice vis calculated as follows:\nVC(v) =1\njS(v)jmax\nu2Vfjn2S(v) :vN(n) =ujg;(4)\nwhere S(v)is the set of notes assigned to v,Vthe set of all\nvoices, and vN(n)the correct voice for note n. The A VC,\nthen, is the average VC over all voices:\nA VC =1\njVjX\nv2VVC(v): (5)\nThe per-fold percentages for each metric mare\nweighted by the number of notes (or note pairs) in the piece\nfor the fold, so that the average values over all folds are al-\nways per-note (or per-pair):\navg(m) =Pk\ni=1(mi\u0001jNij)\nPk\ni=1jNij; (6)\nwhere kis the number of folds, and Nthe set of notes in a\npiece.\n4.2 Evaluation modes\nWe use two evaluation modes: test mode andapplication\nmode . In test mode, the feature vectors are calculated us-\ning the correct voice information for the preceding notes.\nTest mode serves a gauging function in that it reﬂects the\noptimal model performance on unseen data. In application\nmode, the feature vectors are calculated using the model-\ngenerated voice information. This mode corresponds to the\nreal-world application scenario where no correct voice in-\nformation is available, and where all voice decisions must\nbe based on previous decisions—it thus reﬂects the ex-\npected model performance on unseen data. In application\nmode, model performance can suffer from error propaga-\ntion.\n3The deﬁnitions are equal to those given for precision andrecall in\n[10], metrics used in [13, 14, 18, 27, 28]. The terms appear to be used\ninterchangeably.5. VOICE ENTRY ESTIMATION HEURISTICS\nError propagation is the phenomenon in which an incorrect\nvoice assignment inﬂuences the voice decision for the fol-\nlowing notes negatively. Given the accuracy in test (acc T)\nand application mode (acc A), the proportion of misassign-\nments due to error propagation, q, is calculated as follows:\nq=acc T\u0000acc A\n1\u0000acc A: (7)\nIn previous work [6, 7], depending on the dataset we ob-\nserved qvalues up to 0.87, indicating that model perfor-\nmance is indeed seriously hampered by error propagation.\nAlthough error propagation can occur throughout a\npiece, it tends to be particularly strong in thinly-textured\nopenings of pieces, where the model may start ‘on the\nwrong foot’. This often leads to a chain of misassignments.\nTo address this problem, we propose a preprocessing step\nthat applies two heuristics, h1 and h2 (improving on [8]).\nThey estimate which notes belong to the new voices at each\ndensity increase , that is, each point where the maximal\nnumber of simultaneous notes so far increases. h1 and,\npartly, h2 are based on the prior assumptions that (i) voices\ntend to move in small steps, and that when new voice(s)\nenter, (ii) none of the already active voices has a rest, and\n(iii) none of the voices is involved in voice crossing.\n01 function estimate(list notes) returns list x\n02 density increases d:= [ d1, ..., dm] x\n03 available voices av:= [1, ..., dm] x\n04 add avto new list fw x\n05 for ifrom mto 2: x\n06 ifh1: find lowest-cost configuration x\n07 at pos( di) x\n08 ifh2: find pattern at pos( di) x\n09 remove new voices from av x\n10 prepend avtofw x\n11 av:= copy( av) x\n12 return fw x\nFigure 1 . Algorithm outline. Underlined concepts are ex-\nplained in the main text.\nh1 and h2 share a similar overall algorithmic structure,\nas shown in Figure 1. The algorithm takes as input the\nsequence of notes representing a piece (see Section 3.2),\nand returns, for each density increase (including the open-\ning), a vector of voice assignments for the ﬁrst chord of\nthe increased density. If the voices enter successively, h2\nis called; if not, or if h2 fails, h1 is called. The voice as-\nsignments returned remain ﬁxed when the DNN is applied.\n(1) \u000f \u000f \u000f \u000f \u000f (2) \u000f \u000f \u000f \u000f \u000f (3) \u000f \u000f \u000e \u000e\n(1) \u000f \u000f \u000f \u000f \u000f (2) \u000f \u000f \u000e \u000e \u000f (3) \u000f \u000f \u000f \u000f\n(1) \u000f \u000f \u000e \u000e \u000f (2) \u000f \u000f \u000f \u000f \u000f (3) \u000f \u000f \u000f \u000f\nFigure 2 . Chord conﬁgurations ( n= 2). Columns repre-\nsent chords; rows represent layers.\nh1 is the more generic heuristic. It determines the\nnew voices by calculating, at each density increase start-\ning at the last, the lowest-cost conﬁguration . A conﬁgura-\ntion organises the last nchords (i.e., ordered sequences of284 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018pitches) of density di\u00001and the ﬁrst nchords of density di\ninto horizontal layers, as shown in Figure 2. The cost for a\nconﬁguration is calculated as follows:\nnX\nj=1di\u00001X\nk=1nX\nl=1jpj;k\u0000pl;kj; (8)\nwhere pj;kis the pitch at the kth2n-sized layer in the jth\nchord of density di\u00001, andpl;kthe pitch at the kth2n-sized\nlayer in the lth chord of density di. The positions of the\nremaining n-sized layers in the lowest-cost conﬁguration,\nthen, determine the new voices.\nh2 caters speciﬁcally to imitative pieces, and attempts\nto determine the new voices by ﬁnding, at each density\nincrease starting at the last, a match for the pattern (as de-\nﬁned by the ﬁrst nnotes of the piece, the opening motif’s\nhead) in the ﬁrst nchords of density di. The ﬁrst match-\ning criterion is rhythmic sequence; if this yields multiple\nmatches, melodic contour (up, same, down) is added as a\nsecond matching criterion. If a single match is thus found,\nthe new voice is identiﬁed; if multiple matches are still\nfound, the lowest-cost conﬁguration (as in h1) is used to\ndisambiguate. If no match is found, which can happen if\nthe prior assumptions do not hold true, the new voice is as-\nsumed to enter below the existing voice(s). If at more than\nhalf of the density increases no match is found, h2 fails.\n6. DATASET\nThe models are evaluated on the 48 fugues from Johann\nSebastian Bach’s The Well-Tempered Clavier (BWV 846-\n893), containing one two-, 26 three-, 19 four-, and two\nﬁve-voice pieces, as well as his 30 inventions (BWV 772-\n801), containing 15 two- and 15 three-voice pieces (also\nknown as sinfonias ). The dataset, in MIDI format, was\noriginally retrieved from the MuseData repository of the\nCenter for Computer Assisted Research in the Humani-\nties,4and has been slightly modiﬁed. First, all in-voice\nchords —instances where a voice is non-monophonic—\nwere reduced to single notes, and all temporarily added\nextra voices were removed. Figure 3 shows an example\nof both. Second, because of liberties in performance or\nrounding errors leading to note overlap within a voice,\noccasionally some quantisation was required. This was\nachieved by adjusting each offending left note’s offset to\nequal its adjacent right note’s onset. These ﬁrst two modi-\nﬁcations are necessary in order for the data to comply with\nthe assumptions that underly our modelling approach (a\nvoice is always monophonic, and the number of voices in\na piece is equal to its nominal number of voices—see Sec-\ntion 3). Third, to create a more equal distribution of train-\ning and test data in cross-validation, the two-voice fugue\nwas split into two parts, and the two ﬁve-voice fugues\nwere split into four (BWV 849) and two (BWV 867) parts.\nFourth, for a number of pieces starting with an anacrusis,\nsome padding with rests was required to ensure a correct\nmetrical alignment. Fifth, where necessary, time signature\nor key signature information was corrected or added.\n4http://www.musedata.org/\nFigure 3 .The Well-Tempered Clavier , Fugue 17 in A [\nmajor (BWV 886), closing bars. Temporarily added ex-\ntra voice, chromatically descending from G 3to E[3(lower\nstaff), and in-voice chord (upper staff, ﬁnal chord).\nThus, a total of 206 notes were pruned from the orig-\ninal 53230 notes in the fugues, and a total of ﬁve notes\nfrom the original 19872 notes in the inventions—yielding\na dataset containing 72891 notes. We publish this dataset\nas a curated benchmark dataset for voice separation,5that\nenables the comparison of results in a rigorous manner, and\nthat thus facilitates reproducible research [37].\n7. EXPERIMENTAL RESULTS AND DISCUSSION\nIn a ﬁrst experiment, we performed a grid search to opti-\nmise three hyperparameters: the number of hidden layers\n(HL), the size of the hidden layers (HLS), and the value of\nthe dropout keep probability (KP). We explored a small hy-\nperparameter space determined in earlier experimentation,\nconsisting of four HL values (2, 3, 4, and 5), four HLS val-\nues (25, 33, 50, and 66), and three KP values (0.75, 0.875,\nand 0.9375). The grid search was performed on the 19\nfour-voice fugues; as the deciding metric, accuracy in test\nmode was used (metrics in test mode are more stable in-\ndicators of model performance; see Section 4.2). For each\nHL value, we selected the best-performing model, which\nwe then trained and evaluated on all 48 fugues and all 30\ninventions. This was done separately on the different sub-\nsets (two-voice, three-voice, etc.); the performance on all\nfugues or inventions is the per-note (or per-pair) average\nover their subsets as calculated using Equation (6). Ta-\nble 2 shows that on the fugues, the two-layer model yields\nthe highest performance in both test and application mode.\nOn the inventions, the results are less clear—although the\ntwo more shallow models seem to perform better here too.\nOverall, however, the results are fairly similar, indicating a\nlimited effect of the number of layers.\nFocussing on the best model (HL = 2; HLS = 66, KP\n= 0.875), in a second experiment, we then investigated\nthe effect of using a deep(er) neural network, as well as\nthe effect of the integration of the voice entry estimation\nheuristics. To this end, we compared four models: the\nsingle-hidden layer neural network as described in [6, 7]\n(N), the same model with the heuristics integrated (N/h),\nthe two-layer model (D), and the two-layer model with the\nheuristics integrated (D/h). The heuristics were not used in\ntest mode, as error propagation does not occur there. Ta-\nble 3 shows that D always outperforms N, and that N/h and\nD/h always outperform N and D, respectively. A test for\n5https://www.github.com/reinierdevalk/data/Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 285HL HLS KP Test Application\nacc snd cmp A VC acc snd cmp A VC\n2 66 0.875 98.34 97.11 97.26 98.27 90.72 96.43 96.39 90.76\n3 66 0.75 98.36 97.12 97.24 98.27 89.96 96.30 96.30 90.05\n4 50 0.75 98.32 97.07 97.23 98.25 89.97 96.36 96.36 90.12\n5 50 0.75 98.27 96.98 97.13 98.20 89.96 96.38 96.35 90.23\n2 66 0.875 99.09 98.52 98.58 99.06 96.64 98.14 98.09 96.49\n3 66 0.75 99.17 98.64 98.62 99.13 96.53 98.22 98.21 96.35\n4 50 0.75 99.20 98.64 98.66 99.15 96.54 98.17 98.13 96.34\n5 50 0.75 99.00 98.40 98.39 98.96 96.44 97.95 97.93 96.28\nTable 2 . Experiment 1. Best-performing models per HL value, 48 fugues (top) and 30 inventions (bottom). Values are\naverages over the different subsets (see Section 7 and Equation (6); all values are percentages.\nModel Test Application q\nacc snd cmp A VC acc snd cmp A VC F 1\nN 97.86 96.54 96.70 97.78 86.36 95.46 95.33 86.73 95.39 0.84\nN/h 90.44 95.86 95.69 90.62 95.78 0.77\nD 98.34 97.11 97.26 98.27 87.69 96.26 96.20 87.43 96.23 0.86\nD/h 90.72 96.43 96.39 90.76 96.41 0.82\n[28] 88.23 97.00\n[17] 89.21\n[10] 92.5\nTable 3 . Experiment 2 and 3. N, N/h, D, and D/h models, 48 fugues (top); [10, 17, 28] models, 48 fugues (bottom). Values\nare averages over the different subsets (see Section 7 and Equation (6); all values except qare percentages. The F 1score is\nthe harmonic mean of soundness and completeness.\nstatistical signiﬁcance (we used the one-tailed Wilcoxon\nsigned-rank test with p < 0:05as the signiﬁcance crite-\nrion) reveals that these performance differences are always\nsigniﬁcant. We thus conclude that both using a deep(er)\nneural network and integrating the heuristics yield a signif-\nicant performance improvement. Furthermore, the qvalues\nshow that the heuristics indeed reduce error propagation—\nbut the effect is weaker in case of the D model, where error\npropagation is also slightly worse. Finally, we note that in-\ntegrating the heuristics leads to a strong improvement in\nterms of accuracy and A VC. The improvement in terms of\nsoundness and completeness—which are by deﬁnition less\naffected by error propagation—, on the other hand, is only\nsmall.\nAdditionally, we compared the performance of our\noverall best model (D/h) on the 48 fugues with the per-\nformances reported for the three voice separation models\nthat, to our knowledge, represent the current state of the\nart, and that have also been evaluated on the 48 fugues. As\nTable 3 shows, D/h outperforms the [17] and [10] models.\nIt also outperforms the [28] model in terms of A VC, but\nnot in terms of F 1score—which may be because the lat-\nter model is speciﬁcally optimised for that metric, whereas\nD/h is optimised for accuracy.\nIt should be noted, ﬁnally, that a strict comparison with\nthe state of the art is problematic due to the heterogeneity\nof datasets and metrics used. We address this by making\nour dataset publicly available as a benchmark dataset (see\nSection 6).8. CONCLUSIONS AND FUTURE WORK\nIn this paper, we present the implementation and evalua-\ntion of DNNs for voice separation in symbolic music rep-\nresentations as well as the implementation and evaluation\nof two voice entry estimation heuristics. We evaluate the\nmodels on 78 keyboard works by Johann Sebastian Bach,\nwhich we publish as a curated benchmark dataset for com-\nparing voice separation models. We observe that both the\nuse of deep(er) neural networks for the task and the inte-\ngration of the heuristics into the models improve perfor-\nmance signiﬁcantly. The best model outperforms our pre-\nvious models, and performs close to or better than the re-\nported state of the art.\nA ﬁrst analysis of the results reveals that the model\nhas difﬁculties processing musically challenging passages,\ncontaining, for example, voice crossings or reduced tex-\ntures. Furthermore, despite the success of the voice entry\nestimation heuristics, error propagation remains problem-\natic. An in-depth analysis of the results, planned for fu-\nture work, is required to gain better insight into these mat-\nters. Possible explanations are that the model is not given\nenough context information, and that it does not have any\nmemory. We therefore also plan to encode a larger poly-\nphonic window into the features as to increase the context\ninformation, and we plan to experiment with other types\nof DNNs, such as recurrent neural networks, which allow\ninformation to persist, or long short-term memory models,\nwhich are capable of learning long-time dependencies.286 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20189. REFERENCES\n[1] E. Benetos, S. Dixon, D. Giannoulis, Automatic music\ntranscription: Challenges and future directions. Jour-\nnal of Intelligent Information Systems , 41(3):407–434,\n2013.\n[2] A. S. Bregman and J. Campbell. Primary auditory\nstream segregation and perception of order in rapid se-\nquences of tones. Journal of Experimental Psychology ,\n89(2):244–249, 1971.\n[3] E. Cambouropoulos. V oice and stream: Perceptual and\ncomputational modeling of voice separation. Music\nPerception , 26(1):75–94, 2008.\n[4] E. Chew and X. Wu. Separating voices in polyphonic\nmusic: A contig mapping approach. In U. K. Wiil, ed-\nitor,Computer Music Modeling and Retrieval: Second\ninternational symposium, CMMR 2004 , pages 1–20.\nSpringer, Berlin, 2005.\n[5] K. Choi, G. Fazekas, K. Cho, and M. Sandler. A tu-\ntorial on deep learning for music information retrieval.\narXiv:1709.04396v1 [cs.CV] , 2017.\n[6] R. de Valk. Structuring lute tablature and MIDI data:\nMachine learning models for voice separation in sym-\nbolic music representations. PhD thesis, City Univer-\nsity, London, 2015.\n[7] R. de Valk and T. Weyde. Bringing ‘Musicque into the\ntableture’: Machine-learning models for polyphonic\ntranscription of 16th-century lute tablature. Early Mu-\nsic, 43(4):563–576, 2015.\n[8] R. de Valk and T. Weyde. V oice entry estimation\nheuristics to reduce error propagation in voice separa-\ntion models. In Proc. of the 18th International Society\nfor Music Information Retrieval Conference, Suzhou,\nChina, Late-Breaking/Demo session, 2017.\n[9] W. Drabkin. Part (ii). In Stanley Sadie, editor, The new\nGrove dictionary of music and musicians , volume 19,\npage 164. Macmillan, London, 2nd edition, 2001.\n[10] B. Duane and B. Pardo. Streaming from MIDI us-\ning constraint satisfaction optimization and sequence\nalignment. In Proc. of the International Computer Mu-\nsic Conference, Montreal, QC, Canada , 2009.\n[11] M. Giraud, R. Groult, and F. Lev ´e. Subject and\ncounter-subject detection for analysis of the Well-\nTempered Clavier fugues. In M. Aramaki, M. Bar-\nthet, R. Kronland-Martinet, and S. Ystad, editors,\nFrom sounds to music and emotions: 9th international\nsymposium, CMMR 2012 , pages 422–438. Springer,\nBerlin, 2013.\n[12] X. Glorot and Y . Bengio. Understanding the difﬁculty\nof training deep feedforward neural networks. In Proc.\nof the 13th International Conference on Artiﬁcial Intel-\nligence and Statistics, Sardinia, Italy , pages 249–256,\n2010.[13] P. Gray and R. Bunescu. A neural greedy model for\nvoice separation in symbolic music. In Proc. of the 17th\nInternational Society for Music Information Retrieval\nConference, New York, NY, USA , pages 782–788, 2016.\n[14] N. Guiomard-Kagan, M. Giraud, R. Groult, and\nF. Lev ´e. Improving voice separation by better connect-\ning contigs. In Proc. of the 17th International Society\nfor Music Information Retrieval Conference, New York,\nNY, USA , pages 164–170, 2016.\n[15] D. Huron. V oice denumerability in polyphonic music\nof homogeneous timbres. Music Perception , 6(4):361–\n382, 1989.\n[16] D. Huron. Tone and voice: A derivation of the rules of\nvoice-leading from perceptual principles. Music Per-\nception , 19(1):1–64, 2001.\n[17] A. Ishigaki, M. Matsubara, and H. Saito. Prioritized\ncontig combining to segregate voices in polyphonic\nmusic. In Proc. of the 8th Sound and Music Comput-\ning Conference, Padua, Italy , 2011.\n[18] A. Jordanous. V oice separation in polyphonic music:\nA data-driven approach. In Proc. of the International\nComputer Music Conference, Belfast, Ireland , 2008.\n[19] I. Karydis, A. Nanopoulos, A. Papadopoulos, E. Cam-\nbouropoulos, and Y . Manolopoulos. Horizontal and\nvertical integration/segregation in auditory streaming:\nA voice separation algorithm for symbolic musical\ndata. In Proc. of the 4th Sound and Music Computing\nConference, Lefkada, Greece , pages 299–306, 2007.\n[20] I. Karydis, A. Nanopoulos, A. N. Papadopoulos,\nand E. Cambouropoulos. VISA: The voice integra-\ntion/segregation algorithm. In Proc. of the 8th Inter-\nnational Conference on Music Information Retrieval,\nVienna, Austria , pages 445–448, 2007.\n[21] J. Kilian and H. H. Hoos. V oice separation—A lo-\ncal optimization approach. In Proc. of the 3rd Inter-\nnational Conference on Music Information Retrieval,\nParis, France , pages 39–46, 2002.\n[22] D. P. Kingma and J. L. Ba. Adam: A method for\nstochastic optimization. arXiv:1412.6980v9 [cs.LG] ,\n2017.\n[23] P. B. Kirlin and P. E. Utgoff. V oiSe: Learning to segre-\ngate voices in explicit and implicit polyphony. In Proc.\nof the 6th International Conference on Music Informa-\ntion Retrieval, London, UK , pages 552–557, 2005.\n[24] I. Knopke and F. J ¨urgensen. A system for identifying\ncommon melodic phrases in the masses of Palestrina.\nJournal of New Music Research , 38(2):171–181, 2009.\n[25] Y . LeCun, Y . Bengio, and G. Hinton. Deep learning.\nNature , 521(7553):436–444, 2015.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 287[26] D. Lewis, T. Crawford, and D. M ¨ullensiefen. Instru-\nmental idiom in the 16th century: Embellishment pat-\nterns in arrangements of vocal music. In Proc. of the\n17th International Society for Music Information Re-\ntrieval Conference, New York, NY, USA , pages 524–\n530, 2016.\n[27] S. T. Madsen and G. Widmer. Separating voices in\nMIDI. In Proc. of the 7th International Conference\non Music Information Retrieval, Victoria, BC, Canada ,\npages 57–60, 2006.\n[28] A. McLeod and M. Steedman. HMM-based voice sep-\naration of MIDI performance. Journal of New Music\nResearch , 45(1):17–26, 2016.\n[29] D. Meredith, K. Lemstr ¨om, and G. A. Wiggins. Algo-\nrithms for discovering repeated patterns in multidimen-\nsional representations of polyphonic music. Journal of\nNew Music Research , 31(4):321–345, 2002.\n[30] N. Orio. Music retrieval: A tutorial and review. Foun-\ndations and Trends in Information Retrieval , 1(1):1–\n90, 2006.\n[31] D. Rafailidis, E. Cambouropoulos, and Y . Manolopou-\nlos. Musical voice integration/segregation: VISA re-\nvisited. In Proc. of the 6th Sound and Music Computing\nConference, Porto, Portugal , pages 42–47, 2009.\n[32] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,\nand R. Salakhutdinov. Dropout: A simple way to pre-\nvent neural networks from overﬁtting. Journal of Ma-\nchine Learning Research , 15(Jun):1929–1958, 2014.\n[33] W. M. Szeto and M. H. Wong. Stream segregation\nalgorithm for pattern matching in polyphonic mu-\nsic databases. Multimedia Tools and Applications ,\n30(1):109–127, 2006.\n[34] D. Temperley. The cognition of basic musical struc-\ntures . MIT Press, Cambridge, MA, 2001.\n[35] D. Temperley. A uniﬁed probabilistic model for poly-\nphonic music analysis. Journal of New Music Re-\nsearch , 38(1):3–18, 2009.\n[36] G. Velarde, T. Weyde, and D. Meredith. An approach\nto melodic segmentation and classiﬁcation based on ﬁl-\ntering with the haar-wavelet. Journal of New Music Re-\nsearch , 42(4):325–345, 2013.\n[37] M. D. Wilkinson et al. The FAIR Guiding Principles\nfor scientiﬁc data management and stewardship. Scien-\ntiﬁc Data , 3(160018), 2016.288 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Comparison of Audio Features for Recognition of Western and Ethnic Instruments in Polyphonic Mixtures.",
        "author": [
            "Igor Vatolkin",
            "Günter Rudolph"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492475",
        "url": "https://doi.org/10.5281/zenodo.1492475",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/139_Paper.pdf",
        "abstract": "Studies on instrument recognition are almost always restricted to either Western or ethnic music. Only little work has been done to compare both musical worlds. In this paper, we analyse the performance of various audio features for recognition of Western and ethnic instruments in chords. The feature selection is done with the help of a minimum redundancy - maximum relevance strategy and a multi-objective evolutionary algorithm. We compare the features found to be the best for individual categories and propose a novel strategy based on non-dominated sorting to evaluate and select trade-off features which may contribute as best as possible to the recognition of individual and all instruments.",
        "zenodo_id": 1492475,
        "dblp_key": "conf/ismir/VatolkinR18",
        "keywords": [
            "instrument recognition",
            "Western music",
            "ethnic music",
            "audio features",
            "minimum redundancy - maximum relevance strategy",
            "multi-objective evolutionary algorithm",
            "chords",
            "individual categories",
            "novel strategy",
            "trade-off features"
        ],
        "content": "COMPARISON OF AUDIO FEATURES FOR RECOGNITION OF\nWESTERN AND ETHNIC INSTRUMENTS IN POLYPHONIC MIXTURES\nIgor Vatolkin G ¨unter Rudolph\nTU Dortmund, Department of Computer Science\nfigor.vatolkin;guenter.rudolph g@tu-dortmund.de\nABSTRACT\nStudies on instrument recognition are almost always re-\nstricted to either Western or ethnic music. Only little work\nhas been done to compare both musical worlds. In this\npaper, we analyse the performance of various audio fea-\ntures for recognition of Western and ethnic instruments in\nchords. The feature selection is done with the help of a\nminimum redundancy - maximum relevance strategy and\na multi-objective evolutionary algorithm. We compare the\nfeatures found to be the best for individual categories and\npropose a novel strategy based on non-dominated sorting\nto evaluate and select trade-off features which may con-\ntribute as best as possible to the recognition of individual\nand all instruments.\n1. INTRODUCTION\nInstrument recognition in polyphonic audio signals, when\nacoustic properties of multiple simultaneously played\nsources contribute together to the spectrum, corresponds to\na very challenging problem in music information retrieval.\nAn unknown number of sound sources, instrument bod-\nies with different characteristics, dissimilarities of over-\ntone distribution across pitches, various playing styles, ap-\nplied effects, etc. hinder the robust identiﬁcation of playing\ninstruments. Earlier works started with recognition of in-\ndividual tones [6, 15]. Several years later ﬁrst studies on\npolyphonic instrument recognition were published [5, 7].\nIn further works, many different and complex methods\nwere proposed, like source separation [14], complex fea-\nture engineering [27], or deep neural networks [12].\nHowever, most studies concentrate on the detection of\nWestern instruments in Western classical or popular mu-\nsic. Recently, more attention was paid to analyse also eth-\nnic/world music, for example for onset detection in Car-\nnatic music [22] or rhythm analysis in Indian music [23],\nbut only little work was reported on recognition of eth-\nnic instruments, in particularly in polyphonic recordings,\nor in both Western and ethnic recordings. [10] presented a\nc\rIgor Vatolkin, G ¨unter Rudolph. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). Attri-\nbution: Igor Vatolkin, G ¨unter Rudolph. “Comparison of Audio Features\nfor Recognition of Western and Ethnic Instruments in Polyphonic Mix-\ntures”, 19th International Society for Music Information Retrieval Con-\nference, Paris, France, 2018.study on recognition of 10 Indian stringed, wind, and per-\ncussive instruments (sitar, edakkai, indian ﬂute, etc.), how-\never only two instruments were mixed together at the same\ntime. Classiﬁcation of solo recordings into three families\n(string, woodwind, percussion) of 9 Pakistani instruments\n(benju, bainsuri, tabla, etc.) was done in [17]. [2] exam-\nined properties of various acoustic features for recognition\nof ﬁve Hindustani instruments. In [25], the performance\nof models trained for recognition of Western instruments\nin Western mixtures was validated when applied for poly-\nphonic mixtures with ethnic samples.\nThe goal of our study was to propose a strategy how au-\ndio features can be automatically validated for their ability\nto detect Western and/or ethnic instruments. We adopt the\ngeneral experiment setup from [25], starting with a large\nfeature set and applying feature selection for identiﬁca-\ntion of the most relevant features. However, in contrast\nto [25], we aim at the recognition of not only Western, but\nalso ethnic instruments, and incorporate datasets created\nwith both Western and ethnic samples, making recogni-\ntion tasks harder, but also allowing the classiﬁcation mod-\nels to be more robust and not restricted to data sets created\nwith more similar instruments. Furthermore, we propose a\nnovel strategy based on non-dominated sorting to identify\nfeatures which are particularly well suited to classify either\nWestern, ethnic, or both categories of instruments.\nThe remainder of this paper is organised as follows. In\nSection 2, we introduce the backgrounds of multi-objective\nfeature selection. Section 3 describes the study setup. In\nSection 4, we discuss the results, compare the features, and\npresent our strategy how the most relevant features for the\nrecognition of Western, ethnic, and both groups of instru-\nments can be identiﬁed. We conclude with Section 5.\n2. MULTI-OBJECTIVE FEATURE SELECTION\nThe goal of feature selection (FS) is to identify relevant\nfeatures and to remove irrelevant and redundant ones. Rel-\nevant features contribute to the “best” classiﬁcation mod-\nels, so that their removal would decrease the classiﬁcation\nquality. Irrelevant features do not capture any important\nproperties of classiﬁcation categories; if too many of such\nfeatures are contained in the data set, some of them may\nbe identiﬁed by chance as relevant, leading to decreased\ngeneralisation ability of models. Redundant features can\nbe removed from the feature set without decrease of clas-\nsiﬁcation quality for models trained with this set, because554other features already describe the same properties. For a\ngood introduction into feature selection, we refer to [11].\nTo evaluate feature sets, some criterion is needed, like\nclassiﬁcation accuracy, or correlation with the target. In\nthe multi-objective feature selection (MO-FS), several of\nsuch criteria are optimised simultaneously:\nq\u0003= arg min\nqfmi(y;^y;\b(F;q)) :i= 1; :::; Kg;(1)\nwhereFis the complete feature set, \b(F;q)is the se-\nlected feature set, qis the binary vector which indicates\nfeatures to be selected (zero entry at position imeans that\ni-th feature is not selected), yare correct labels (categories\nto predict), ^yare predicted labels, and m1; :::; m KareK\nevaluation or objective functions, which may measure clas-\nsiﬁcation performance (accuracy, precision, recall, etc.)\nbut also other relevant criteria (number of selected features,\ndegree of internal redundancies across features, etc.)\nIn [25], it is proposed to minimise two criteria: the num-\nber of selected features and the balanced classiﬁcation er-\nror, which is deﬁned as follows:\ne=1\n2\u0012FN\nTP+FN+FP\nTN+FP\u0013\n; (2)\nwhere TPis the number of true positives, TNtrue neg-\natives, FPfalse positives, and FN false negatives.\nWhen MO-FS is applied, some feature sets cannot be\ncompared: consider a smaller feature set, which leads to a\nhigher classiﬁcation error, and a larger feature set, which\nleads to a lower error; none of these sets can be described\nas superior to another one. However, some sets may be\nworse with regard to both criteria, and can be identiﬁed\nwith the help of non-dominance relation: feature set q1\ndominates feature set q2(q1\u001eq2), if and only if\n8i2f1; :::; Kg:mi(q1)\u0014mi(q2)and\n9j2f1; :::; Kg:mj(q1)< m j(q2):(3)\nIn other words, q1dominates q2, when it is not worse\nthanq2with regard to all criteria, and is better with regard\nto at least one criterium. Here, we restrict us to minimi-\nsation of all criteria; criteria to be maximised can be sim-\nply redeﬁned for minimisation (e.g., multiplying them with\n-1). The goal of MO-FS is to ﬁnd a non-dominated front\nof incomparable feature sets, which are not dominated by\nany other feature set.\n3. EXPERIMENTAL SETUP\nIn the experimental setup, we mostly follow [25]. Table\n1 lists all instruments used in this study. The instruments\nwhich were recognised in classiﬁcation experiments, are\nwritten in normal font. Further instruments, which were\npresent in audio mixtures, but were not considered as\nclasses to be identiﬁed, are written in italic font. The\nWestern instrument samples are taken from MUMS [4],RWC [9], and University of Iowa1databases, and ethnic\nfrom Ethno World 5 Professional & V oices2.\nThe chords are randomly mixed from individual sam-\nples as described in [25], however, in contrast to previous\nwork, we have created heterogeneous data sets, so that in\neach mixture of three to four tones at least one Western and\nat least one ethnic sample is contained. The experiment set\nconsists of 3000 chords. During feature selection, it is di-\nvided into the training and optimisation set by means of\n5-fold cross-validation. Training set is used to train clas-\nsiﬁcation models, and optimisation set to estimate the bal-\nanced classiﬁcation error e. Other independently mixed\n3000 chords are used as holdout set to measure the effect\nof overﬁtting towards the experiment set.\nThe audio features comprise acoustic characteristics\navailable for extraction with open-source AMUSE frame-\nwork [26], including mel frequency cepstral coefﬁcients\n(MFCCs) [21], root mean square (RMS) of the time sig-\nnal, various spectral characteristics (centroid, bandwidth,\nkurtosis, skewness, ﬂux, etc.), chroma [8] and chroma en-\nergy normalized statistics (CENS) [20], but also other less\nfrequently used features like characteristics of ERB bands\nand Bark scale domain [19] or phase domain [18]. Before\nthe extraction, the audio was downsampled to 22,1 kHz\nmono signal, and the most short-framed features were ex-\ntracted from 512 samples without overlap; for exact details\nsee [24]. For each feature, three dimensions are stored sep-\narately: a value from the middle of the attack interval, from\nthe onset frame (the end of the attack interval equal to the\nbeginning of the release interval), and from the middle of\nthe release interval, where attack and release intervals were\npreviously extracted with MIR Toolbox [16], leading to the\ncomplete set of 795 feature dimensions. Classiﬁcation is\ndone with random forest classiﬁer [13].\nThe ﬁrst FS strategy was minimum redundancy-\nmaximum relevance (MRMR) [3], which aims at the min-\nimisation of redundancy between selected features and\nmaximisation of relevance to the target category. The sec-\nond FS strategy was evolutionary multi-objective feature\nselection (EMO-FS) with S-metric selection evolutionary\nmulti-objective algorithm (SMS-EMOA) [1], for further\ndetails please see [25].\n4. DISCUSSION OF RESULTS\n4.1 Performance Analysis\nTable 2 provides the summary of results after feature se-\nlection. eH(\b)denotes the baseline classiﬁcation error us-\ning the complete feature set. jb\bOjdenotes the cardinal-\nity of the feature set with the smallest optimisation error\neO(b\bO).eH(b\bO)denotes the holdout error for that feature\nset, and eH(b\bH)the best holdout error among all output\nfeature sets after feature selection.\nBoth MRMR and EMO-FS signiﬁcantly outperform the\nbaseline method which trains models with all features.\nThis means, that FS explicitly makes sense. As it can be\n1http://theremin.music.uiowa.edu/MIS.html\n2http://www.bestservice.deProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 555Category Instruments\nWESTERN\nBowed Cello, viola, violin\nKey Piano\nStringed Acoustic guitar, electric guitar\nWoodwind/brass Flute, trumpet\nETHNIC\nBowed Dilruba, egyptian ﬁddle, erhu, jinghu opera violin ,morin khuur violin\nKey Hohner melodica, scale changer harmonium\nStringed Balalaika, bandura, banjolin, banjo framus, bouzouki ,ceylon guitar ,c¨umb¨us,domra ,kantele ,oud,sitar,tampura ,tanbur ,saz,ukulele\nWoodwind/brass Bawu, dung dkar trumpet, fujara, pan ﬂute ,pinkillo ,pivana ,shakuhachi\nTable 1 : Instruments used in this study.\nNo FS MRMR EMO-FS\nTask eH(\b)jb\bOjeO(b\bO)eH(b\bO)eH(b\bH)jb\bOjeO(b\bO)eH(b\bO)eH(b\bH)\nWESTERN\nAcoustic guitar 0.4395 10 0.3962 0.3906 0.3906 46 0.3809 0.3885 0.3751\nCello 0.4696 12 0.4295 0.4382 0.4382 37 0.4358 0.4574 0.4404\nElectric guitar 0.1704 309 0.1532 0.1424 0.1369 44 0.1238 0.1158 0.1084\nFlute 0.4907 3 0.4485 0.4651 0.4516 45 0.4513 0.4675 0.4547\nPiano 0.2531 7 0.2148 0.2411 0.2260 54 0.2112 0.2320 0.2237\nTrumpet 0.3119 20 0.2516 0.2538 0.2506 64 0.2706 0.2604 0.2488\nViola 0.4968 13 0.4735 0.4857 0.4687 36 0.4417 0.4561 0.4406\nViolin 0.4791 8 0.4518 0.4639 0.4632 55 0.4538 0.4605 0.4504\nETHNIC\nBalalaika 0.3976 34 0.3070 0.2987 0.2821 48 0.3113 0.3226 0.2931\nBandura 0.5000 2 0.4653 0.4893 0.4587 50 0.4713 0.4809 0.4689\nBanjo framus 0.4909 11 0.3915 0.4252 0.4151 39 0.4184 0.4139 0.3994\nBanjolin 0.4827 18 0.3504 0.3895 0.3895 32 0.3661 0.4012 0.3937\nBawu 0.3776 12 0.1814 0.2150 0.1836 66 0.2028 0.2138 0.1963\nDilruba 0.4492 32 0.3769 0.3987 0.3974 59 0.3297 0.3761 0.3503\nDung dkar 0.4213 47 0.3971 0.3487 0.3487 49 0.3478 0.3373 0.2983\nEgyptian ﬁddle 0.3533 79 0.2387 0.2750 0.2420 57 0.1763 0.1984 0.1692\nErhu 0.4507 12 0.3719 0.3766 0.3672 42 0.3609 0.3489 0.3293\nFujara 0.3061 28 0.1945 0.1887 0.1840 51 0.1994 0.2236 0.1959\nMelodica 0.3889 33 0.2721 0.3041 0.2926 48 0.2638 0.2898 0.2633\nScale changer harmonium 0.3192 22 0.2342 0.2590 0.2445 43 0.2263 0.2578 0.2149\nTable 2 : Results after feature selection, details are explained in Section 4.1.\nexpected, eH(b\bO)is often higher than eO(b\bO). However,\nthis difference is signiﬁcant only for ethnic instruments and\nEMO-FS. The null hypothesis that both errors come from\nthe same distribution is rejected by means of Wilcoxon\nsigned rank test for paired observations (MATLAB func-\ntion SIGNRANK ) only for ethnic instruments/EMO-FS\nwith p-value of 0.0210. For combination Western/EMO-\nFS, p = 0.1484, for Western/MRMR p = 0.1094 and eth-\nnic/MRMR p = 0.0922. Both MRMR and EMO-FS are\ncomparable: each method leads to a smaller eO(b\bO)for\nexactly a half of Western and a half of ethnic categories,\nand there is no signiﬁcant difference between these errors\nafter the application of Wilcoxon rank sum test for un-\npaired observations (MATLAB function RANKSUM ).\n4.2 Best Features for Individual Categories\nTo identify the most relevant features for each category, we\nmay estimate feature ranks as follows. Let Ncbe the num-\nber of solutions (feature sets) in the non-dominated front\nafter the multi-objective optimisation for category c(recall\nthat the non-dominated front contains the best incompara-\nble solutions, cf. Section 2). Let qk;ibe 1 when feature\nkin non-dominated solution iis selected, and 0 when this\nfeature is not selected. We may count the number of occur-rences of feature kin the front and normalise this number\nby the size of the front:\nr(c; k) =1\nNc\u0001NcX\ni=1qk;i: (4)\nTable 3 lists two most relevant features for individual\nclassiﬁcation tasks using either MRMR (columns 2-5) or\nEMO-FS (columns 6-9). As MRMR starts with the most\nrelevant feature and only adds further features during the\niteration process, r= 1for all 1st best features in that case.\nBecause of the differences in operating methods (EMO-\nFS explores a signiﬁcantly larger number of feature sets,\nbut is slower), the two most relevant features are usually\nnot the same. However, for cello, ﬂute, and erhu the best\nfeature is exactly the same for MRMR and EMO-FS. For\nscale changer harmonium and bawu the 1st best feature for\nMRMR is the same as the 2nd best feature for EMO-FS.\nOne observation is that MFCCs seem to play a more\nimportant role for the recognition of Western instruments:\nalthough all processed MFCC dimensions together account\nfor appr. 17% of the complete feature set, they correspond\nto 56.25% of all 16 entries for 1st best features (8 entries\nfor each MRMR and EMO-FS) and 18.75% for 2nd best\nfeatures for Western instruments, but only to 20.83% for556 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018MRMR EMO-FS\nTask 1st best feature r2nd best feature r 1st best feature r 2nd best feature r\nWESTERN\nAc. guitar A(MFCC 2) 1O(Bark scale magn. 6) 0.83 A(Phase domain angles) 0.80 A(Chroma 11) 0.60\nCello O(MFCC 4) 1A(Spectral slope) 0.89 O(MFCC 4) 0.50 A(RMS) 0.42\nEl. guitar A(1st period. ampl. peak) 1A(RMS ERB band 2) 0.95 R(Bark scale magn. 19) 0.58 R(Delta MFCC 3) 0.42\nFlute A(MFCC 3) 1R(MFCC 6) 0.67 A(MFCC 3) 0.55 O(LPC 2) 0.36\nPiano R(MFCC 1) 1O(RMS ERB band 2) 0.83 O(RMS ERB band 1) 0.67 A(Sum corr. components) 0.56\nTrumpet R(Ampl. 4th spectr. peak) 1R(Inharmonicity) 0.92 R(Ampl. 5th spectr. peak) 0.73 R(MFCC 2) 0.64\nViola O(Low energy) 1A(CENS chroma 11) 0.80 O(MFCC 9) 0.82 R(RMS ERB band 1) 0.55\nViolin A(MFCC 1) 1A(Var. aver. dist. betw. ZC) 0.80 O(MFCC 5) 0.60 A(RMS ERB band 2) 0.50\nETHNIC\nBalalaika O(Bark scale magn. 21) 1A(LPC 3) 0.89 O(Bark scale magn. 21) 0.73 O(LPC 3) 0.53\nBandura A(Sub-band energy rat. 4) 1R (MFCC 1) 0.50 A(MFCC 3) 0.86 A(Spectral kurtosis) 0.71\nBanjo framus O(Spectral ﬂux) 1A(LPC 4) 0.83 A(LPC 4) 0.89 A(ZC rate ERB band 6) 0.67\nBanjolin O(Bark scale magn. 23) 1O(MFCC 8) 0.92 A(LPC 2) 0.80 O(Sum corr. components) 0.80\nBawu A(RMS peak num. above\nmean ampl.)1O(Bark scale magn. 6) 0.83 O(RMS peak number) 0.56 A(RMS peak num. above\nmean ampl.)0.56\nDilruba O(MFCC 3) 1O(MFCC 1) 0.88 A(Spectral extent) 0.88 O(RMS ERB band 2) 0.75\nDung dkar O(Spectral kurtosis) 1R(ZC rate ERB band 2) 0.89 R(Ampl. 1st spectr. peak) 0.88 R(LPC 1) 0.63\nEgypt. ﬁddle A(Phase domain angles) 1A(MFCC 7) 0.94 A(Spectral ﬂux)) 0.69 O(Bark scale magn. 23) 0.62\nErhu R(MFCC 4) 1A(RMS peak number) 0.80 R(MFCC 4) 0.70 R(RMS) 0.60\nFujara A(Max. ampl. chroma) 1O(Ampl. 4th spectr. peak) 0.91 R(RMS) 0.90 O(Spectral ﬂatness 4) 0.70\nMelodica R(RMS ERB band 8) 1A(Spectral bandwidth) 0.92 R(MFCC 2) 0.56 A(ZC rate ERB band 6) 0.44\nScale changer\nharmoniumR(LPC 5) 1A(MFCC 1) 0.91 R(Phase domain angles) 0.60 R(LPC 5) 0.50\nTable 3 : Ranks of features for categorisation of individual instruments. A( \u0001): features from middles of attack intervals;\nO(\u0001): from onset frames; R( \u0001): from middles of release intervals. LPC: linear prediction coefﬁcient; ZC: zero-crossings.\n1st and 20.83% for 2nd best ethnic features. Among eth-\nnic instruments, the half of all MFCC occurrences belongs\nto bowed instruments (dilruba, egyptian ﬁddle, erhu), and\nother two belong to key instruments (melodica and scale\nchanger harmonium). This leads to a careful suggestion\nthat the mel spectrum is probably not the best feature do-\nmain for ethnic stringed and brass instruments, which de-\nserves further investigations. Among two most relevant\nfeatures for ethnic instruments, particularly LPCs occur\nrather frequently (8 times / 16.7% of all entries against 1\ntime / 3.13% for Western instruments).\nWith regard to attack/onset/release-envelope, we may\nobserve, that all three extraction frame categories appear\nfrequently in Table 3. However, the attack phase seems to\nbe generally more relevant for all instruments (for Western\ninstruments, 43.75% of all entries belong to features stored\nfrom middles of attack intervals, for ethnic, 39.58%). On-\nset features correspond to 28.13% of Western and 33.33%\nof ethnic entries, and release features to 28.13% of Western\nand 27.08% of ethnic entries.\n4.3 Best Features for Western and Ethnic Instruments\nTo compare the signiﬁcance of features for all Western\ninstruments against all ethnic instruments, we may esti-\nmate mean feature ranks across all categories of the same\n“world”. Let MWbe the number of Western instruments\n(MW= 8) andMEof ethnic instruments ( ME= 12 ). Let\nqc;k;ibe 1, when feature kis selected in i-th non-dominated\nsolution of the category c, and 0, when it is not selected.\nThen, the accumulated rank of feature kfor all Western\ninstruments is calculated as:r(W; k) =1\nMW\u0001MWX\nc=1 \n1\nNc\u0001NcX\ni=1qc;k;i!\n; (5)\nand, similarly, the accumulated rank of feature kfor all\nethnic instruments as:\nr(E; k) =1\nME\u0001MEX\nc=1 \n1\nNc\u0001NcX\ni=1qc;k;i!\n; (6)\nThe accumulated rank corresponds to the relative share\nof selections of a feature kamong all non-dominated solu-\ntions for all categories of the same world.\nTable 4 lists top 10 features for Western and ethnic cat-\negories. Additionally to non-dominated fronts from the\noptimisation set (columns 1,2,5,6), we analyse the impor-\ntance of features for the independent holdout set (columns\n3,4,7,8). As we can observe, the best features for the op-\ntimisation set are often the same as for the holdout set,\nwhich supports the suggestion, that those features are well\nsuitable for different data sets. Again, we see, that with re-\ngard to accumulated ranks, MFCCs appear rather often for\nWestern categories (8 entries) than for ethnic categories (6\nentries), and LPCs rather often for ethnic categories (6 en-\ntries vs. no entry). EMO-FS selected often RMS for ERB\nbands among top 10 Western features (9 of 20 correspond-\ning entries vs. no entry for ethnic categories).\nTo measure the statistical difference between impor-\ntances of features for both worlds, we validate the fol-\nlowing statistical hypothesis H0: given the accumulated\nranks of top 20 features of one world, we assume that the\nranks of the same features but for another world belong to\nthe same distribution. As Western and ethnic categorisa-\ntion tasks are independent, we validate this hypothesis by\nmeans of Wilcoxon rank sum test ( RANKSUM function inProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 557MRMR EMO-FS\nOptimisation set r Holdout set r Optimisation set r Holdout set r\nTOP10FEATURES FOR WESTERN CATEGORIES\nA(1st period. ampl. peak) 0.228 R(1st period. ampl. peak) 0.259 R(RMS ERB band 1) 0.182 A(RMS ERB band 2) 0.258\nA(MFCC 1) 0.211 A(1st period. ampl. peak) 0.251 A(RMS ERB band 2) 0.177 R(RMS ERB band 1) 0.221\nO(Low energy) 0.178 A(MFCC 1) 0.227 A(Max. ampl. chroma) 0.172 A(Max. ampl. chroma) 0.185\nR(1st period. ampl. peak) 0.177 O(Low energy) 0.195 A(Phase domain angles) 0.169 R(Var. aver. dist. betw. ZC) 0.176\nR(ZC rate ERB band 1) 0.169 A(MFCC 3) 0.191 O(Bark scale magnitude 1) 0.156 R(RMS ERB band 2) 0.173\nA(MFCC 3) 0.164 A(Spectral slope) 0.178 A(MFCC 3) 0.152 A(Ampl. 1st spectral peak) 0.167\nA(Spectral slope) 0.151 R(MFCC 3) 0.159 A(Sum corr. components) 0.147 O(RMS ERB band 1) 0.157\nR(Inharmonicity) 0.135 R(Spectr. centroid ERB 8) 0.150 O(RMS ERB band 1) 0.148 A(Phase domain angles) 0.144\nA(ZC rate ERB band 10) 0.135 R(Low energy) 0.144 A(Ampl. 1st spectr. peak) 0.143 O(RMS ERB band 2) 0.143\nR(MFCC 1) 0.132 R(MFCC 1) 0.141 R(RMS ERB band 2) 0.141 O(CENS chroma 6) 0.142\nETHNIC\nA(Low energy) 0.203 A(Low energy) 0.155 R(Ampl. 1st spectral peak) 0.199 R(Ampl. 1st spectral peak) 0.203\nR(LPC 6) 0.158 A(Sub-band energy ratio 4) 0.148 O(Bark scale magnitude 21) 0.195 O(Bark scale magnitude 21) 0.178\nA(Sub-band energy ratio 4) 0.146 R(LPC 6) 0.137 O(RMS peak number) 0.170 O(Spectral extent) 0.161\nO(LPC 7) 0.130 A(Phase domain angles) 0.137 R(RMS) 0.170 O(LPC 4) 0.157\nA(Phase domain angles) 0.129 O(MFCC 3) 0.135 A(Spectral bandwidth) 0.159 R(Bark scale magnitude 3) 0.154\nO(Bark scale magnitudes 6) 0.125 O(Bark scale magnitude 6) 0.128 O(Spectral ﬂux)) 0.143 R(RMS) 0.153\nO(MFCC 3) 0.121 R(ZC rate for ERB band 2) 0.119 R(RMS peak num. above\nmean ampl.)0.141 A(Strength of 6.major key) 0.145\nO(Bark scale magnitude 21) 0.104 O(Bark scale magnitude 21) 0.111 R(MFCC 2) 0.137 A(Inharmonicity) 0.135\nA(LPC 3) 0.097 O(Spectr. centroid ERB 10) 0.109 R(MFCC 4) 0.137 A(MFCC 6) 0.130\nO(Spectr. centroid ERB 10) 0.095 A(LPC 4) 0.108 A(Bark scale magnitude 17) 0.136 O(MFCC 1) 0.130\nTable 4 : Accumulated ranks of features for categorisation of Western and ethnic instruments. A( \u0001): features from middles\nof attack intervals; O( \u0001): from onset frames; R( \u0001): from middles of release intervals. LPC: linear prediction coefﬁcient; ZC:\nzero-crossings.\nMATLAB). H0 is rejected in all cases for both feature se-\nlection strategies and both sets (optimisation/holdout). Ta-\nble 5 contains p-values. This means that top 20 features\nwhich are particularly good for recognition of Western in-\nstruments are not similarly good for the recognition of eth-\nnic instruments, and vice versa. However, please note that\nH0 is rejected only for a limited set of 8 Western and 12\nethnic instruments, even if they were carefully chosen to\nrepresent different instrument categories. Further studies\nwith a signiﬁcantly larger number of instruments may sup-\nport or weaken this statement.\n4.4 Best Features for All Categories\nTo provide generic recommendations on features which are\nparticularly useful for the recognition of both Western and\nethnic instruments, Figure 1 plots the accumulated ranks\nr(W; k)andr(E; k)of all features. Upper subﬁgures con-\ntain results for MRMR, bottom subﬁgures for EMO-FS,\nleft subﬁgures correspond to optimisation set, and right\nsubﬁgures to holdout set. Dashed lines divide the rank\nspace in three regions. For features in the bottom right\nregion, r(W; k)is at least twice as large as r(E; k). For\nfeatures in the top left region, r(E; k)is at least twice\nas large as r(W; k). The ranks of features in the mid-\ndle region are comparable for Western and ethnic instru-\nments. As we are interested to identify features which are\nbest suited for for the classiﬁcation of all instruments, we\nmarked the ﬁrst non-dominated front with large ﬁlled cir-\ncles and the second non-dominated front with small ﬁlled\ncircles, supported with feature IDs. The mapping of IDs\nto feature names is provided in Table 6. For MRMR, the\nfeatures belonging to ﬁrst non-dominated fronts are low\nenergy, MFCC 1, and 1st periodicity amplitude peak. ForEMO-FS, these features are RMS, Bark scale magnitude\n3, RMS for ERB bands 1 and 2, maximal amplitude in the\nchromagram, and amplitude of the 1st spectral peak.\nIt is worth to mention that even if our feature vector\ncontains almost 800 dimensions, the features can be ex-\ntracted from various frame lengths or with varying param-\neters, and further signal descriptors can be added. Fur-\nther work is necessary to identify better features for in-\nstrument recognition, and our framework provides an au-\ntomatic strategy to evaluate the suitability of features or\ntheir extraction parameters to classify instruments of dif-\nferent categories by means of non-dominance relation.\n5. CONCLUSIONS\nIn this paper, we have applied two feature selection meth-\nods for recognition of Western and ethnic instruments in\npolyphonic audio mixtures. Both methods lead to a sig-\nniﬁcant reduction of the classiﬁcation error compared to\nmodels trained with all features. To measure the rele-\nvance of features for individual categories as well as for\na set of 8 Western and 12 ethnic categories, we proposed a\nsimple rank measure based on feature occurrence in non-\ndominated fronts, with the aim to simultaneously minimise\nthe number of features and the classiﬁcation error. Even if\nlarger feature sets with a smaller error are usually prefer-\nable for classiﬁcation scenarios, also small feature sets\nwith higher errors give valueful insights into relevance of\nindividual features. The statistical comparison of features\nbest suited for recognition of Western instruments against\nfeatures best suited for recognition of ethnic instruments\nshowed that their performance is signiﬁcantly different.\nThis empirically supports the suggestion, that many acous-\ntic descriptors developed and optimised for music instru-558 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018MRMR EMO-FS\nH0 Optimisation set Holdout set Optimisation set Holdout set\nTop 20 Western features are similarly good for ethnic categories 5.69e-08 5.69e-08 2.21e-07 6.70e-08\nTop 20 ethnic features are similarly good for Western categories 1.99e-04 1.11e-04 6.67e-06 2.66e-06\nTable 5 : p-values for comparison of top 20 Western and top 20 ethnic features represented by their accumulated ranks.\nFigure 1 : Best (large circles) and 2nd best (small circles) non-dominated features for both Western and ethnic categories.\nThe fronts were estimated for the maximisation of accumulated ranks.\nNo. Name No. Name No. Name\n29 R(LPC 6) 132 A(MFCC 3) 422 A(RMS for ERB band 2)\n45 R(Var. of aver. dist. between ZC) 178 A(Phase domain angles) 431 O(RMS for ERB band 1)\n48 R(RMS) 186 A(MFCC 3) 432 O(RMS for ERB band 2)\n49 A(Low energy) 207 O(MFCC 4) 441 R(RMS for ERB band 1)\n50 O(Low energy) 226 R(MFCC 3) 480 R(Spectral centroid ERB band 10)\n51 R(Low energy) 227 R(MFCC 4) 568 A(Max. ampl. chroma)\n62 O(RMS peak number) 311 O(Bark scale magnitude 6) 682 A(Ampl. 1st spectral peak)\n66 R(RMS peak number above mean ampl.) 326 O(Bark scale magnitude 21) 688 R(Ampl. 1st spectral peak)\n107 O(Spectral extent) 331 R(Bark scale magnitude 3) 784 A(1st periodicity ampl. peak)\n121 A(Sub-band energy ratio 4) 411 R(ZC rate for ERB band 1) 786 R(1st periodicity ampl. peak)\n130 A(MFCC 1)\nTable 6 : Names of features from two best fronts of Figure 1. A( \u0001): features from middles of attack intervals; O( \u0001): from\nonset frames; R(\u0001): from middles of release intervals. LPC: linear prediction coefﬁcient; ZC: zero-crossings.\nment recognition in Western music are not best suited for\nthe recognition of ethnic instruments.\nAnother focus of our investigation was to identify those\nfeatures which are particularly well suited for the recog-\nnition of both Western and ethnic instruments. This can\nbe done by means of non-dominated sorting in the two-\ndimensional rank space. Even if the goal of identifying the\nbest “compromise” features is somewhat contrary to the\nidentiﬁcation of the best speciﬁc features for Western and\nethnic instruments, both approaches make sense. Keep-ing a nearly unlimited number of possible combinations\nof many world instruments with different effects and play-\ning styles in mind, a good strategy is to start with a sufﬁ-\nciently large set of audio descriptors. In the second time-\nconsuming optimisation step, more efforts can be spent for\nreﬁning the extraction parameters of these features and de-\nvelopment of further ones, which are particularly relevant\nfor a concrete instrument class. With the help of our frame-\nwork, both tasks can be executed and analysed automati-\ncally.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 5596. REFERENCES\n[1] N. Beume, B. Naujoks, and M. Emmerich. Sms-emoa: Mul-\ntiobjective selection based on dominated hypervolume. Eu-\nropean Journal of Operational Research , 181(3):1653–1669,\n2007.\n[2] A. K. Datta, S. S. Solanki, R. Sengupta, S. Chakraborty,\nK. Mahto, and A. Patranabis. Automatic Musical Instrument\nRecognition , pages 167–232. Springer Singapore, Singapore,\n2017.\n[3] C. H. Q. Ding and H. Peng. Minimum redundancy feature\nselection from microarray gene expression data. Journal on\nBioinformatics and Computational Biology , 3(2):185–206,\n2005.\n[4] T. Eerola and R. Ferrer. Instrument library (MUMS) revised.\nMusic Perception , 25(3):253–255, 2008.\n[5] J. Eggink and G. J. Brown. A missing feature approach to in-\nstrument identiﬁcation in polyphonic music. In Proc. of 2003\nIEEE Int’l Conf. on Acoustics, Speech, and Signal Processing\n(ICASSP) , pages 553–556. IEEE, 2003.\n[6] A. J. Eronen and A. Klapuri. Musical instrument recognition\nusing cepstral coefﬁcients and temporal features. In Proc. of\nIEEE Int’l Conf. on Acoustics, Speech, and Signal Processing\n(ICASSP) , pages 753–756. IEEE, 2000.\n[7] S. Essid, G. Richard, and B. David. Instrument recognition\nin polyphonic music. In Proc. of 2005 IEEE Int’l Conf. on\nAcoustics, Speech, and Signal Processing (ICASSP) , pages\n245–248. IEEE, 2005.\n[8] T. Fujishima. Realtime chord recognition of musical sound:\na system using common lisp music. In Proc. of the Interna-\ntional Computer Music Conference (ICMC) , pages 464–467,\n1999.\n[9] M. Goto, H. Hashiguchi, T.i Nishimura, and R. Oka. RWC\nmusic database: Music genre database and musical instru-\nment sound database. In Proc. of the 4th Int’l Conf. on Music\nInformation Retrieval (ISMIR) , pages 229–230, 2003.\n[10] S. Gunasekaran and K. Revathy. Fractal dimension analysis\nof audio signals for indian musical instrument recognition.\nInProc. of the Int’l Conf. on Audio, Language and Image\nProcessing (ICALIP) , pages 257–261, 2008.\n[11] I. Guyon, M. Nikravesh, S. Gunn, and L. A. Zadeh, editors.\nFeature Extraction. Foundations and Applications , volume\n207 of Studies in Fuzziness and Soft Computing . Springer,\nBerlin Heidelberg, 2006.\n[12] Y . Han, J.-H. Kim, and K. Lee. Deep convolutional neu-\nral networks for predominant instrument recognition in poly-\nphonic music. IEEE/ACM Transactions on Audio, Speech &\nLanguage Processing , 25(1):208–221, 2017.\n[13] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of\nStatistical Learning . Springer, New York, 2009.\n[14] T. Heittola, A. Klapuri, and T. Virtanen. Musical instrument\nrecognition in polyphonic audio using source-ﬁlter model for\nsound separation. In Proc. of the 10th Int’l Society for Music\nInformation Retrieval Conference (ISMIR) , pages 327–332,\n2009.\n[15] I. Kaminsky and A. Materka. Automatic source identiﬁcation\nof monophonic musical instrument sounds. In Proc. of IEEE\nInt’l Conf. on Neural Networks , volume 1, pages 189–194\nvol.1, 1995.[16] O. Lartillot and P. Toiviainen. MIR in Matlab (II): A toolbox\nfor musical feature extraction from audio. In Proc. 8th Int’l\nConf. on Music Information Retrieval (ISMIR) , pages 127–\n130, 2007.\n[17] S. A. Lashari, R. Ibrahim, and N. Senan. Soft set theory for\nautomatic classiﬁcation of traditional pakistani musical in-\nstruments sounds. In Proc. of Int’l Conf. on Computer Infor-\nmation Science (ICCIS) , volume 1, pages 94–99, 2012.\n[18] I. Mierswa and K. Morik. Automatic feature extraction for\nclassifying audio data. Machine Learning Journal , 58(2-\n3):127–149, 2005.\n[19] B. C. J. Moore and B. R. Glasberg. A revision of\nZwicker’s loudness model. Acta Acustica united with Acus-\ntica, 82(2):335–345, 1996.\n[20] M. M ¨uller, F. Kurth, and M. Clausen. Audio matching via\nchroma-based statistical features. In Proc. of the 6th Int’l\nConf: on Music Information Retrieval (ISMIR) , pages 288–\n295, 2005.\n[21] L. Rabiner and B.-H. Juang. Fundamentals of Speech Recog-\nnition . Prentice Hall, Upper Saddle River, 1993.\n[22] J. Sebastian and H. A. Murthy. Onset detection in composi-\ntion items of carnatic music. In Proc. of the 18th Int’l Society\nfor Music Information Retrieval Conf. , pages 560–567, 2017.\n[23] A. Srinivasamurthy. A Data-driven Bayesian Approach to Au-\ntomatic Rhythm Analysis of Indian Art Music . PhD thesis,\nUniversitat Pompeu Fabra, Barcelona, 2016.\n[24] I. Vatolkin. Improving Supervised Music Classication by\nMeans of Multi-Objective Evolutionary Feature Selection .\nPhD thesis, Dep. of Computer Science, TU Dortmund, 2013.\n[25] I. Vatolkin. Generalisation performance of western instru-\nment recognition models in polyphonic mixtures with ethnic\nsamples. In Proc. of the 6th Int’l Conf. on Computational In-\ntelligence in Music, Sound, Art and Design (EvoMUSART) ,\nvolume 10198 of Lecture Notes in Computer Science , pages\n304–320, 2017.\n[26] I. Vatolkin, W. Theimer, and M. Botteck. AMUSE (Advanced\nMUSic Explorer) - a multitool framework for music data\nanalysis. In J. S. Downie and R. C. Veltkamp, editors, Proc. of\nthe 11th Int’l Society on Music Information Retrieval Conf.\n(ISMIR) , pages 33–38, 2010.\n[27] A. Zlatintsi and P. Maragos. Multiscale fractal analysis of\nmusical instrument signals with application to recognition.\nIEEE Transactions on Audio, Speech & Language Process-\ning, 21(4):737–748, 2013.560 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Precision of Sung Notes in Carnatic Music.",
        "author": [
            "Venkata Subramanian Viraraghavan",
            "Rangarajan Aravind",
            "Hema A. Murthy"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492461",
        "url": "https://doi.org/10.5281/zenodo.1492461",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/120_Paper.pdf",
        "abstract": "Carnatic music is replete with continuous pitch movement called gamakas and can be viewed as consisting of constant-pitch notes (CPNs) and transients. The stationary points (STAs) of transients – points where the pitch curve changes direction – also carry melody information. In this paper, the precision of sung notes in Carnatic music is studied in detail by treating CPNs and STAs separately. There is variation among the nineteen musicians considered, but on average, the precision of CPNs increases exponentially with duration and settles at about 10 cents for CPNs longer than 0.5 seconds. For analyzing STAs, in contrast to Western music, r¯aga (melody) information is found to be necessary, and errors in STAs show a significantly larger standard deviation of about 60 cents. To corroborate these observations, the music was automatically transcribed and re-synthesized using CPN and STA information using two interpolation techniques. The results of perceptual tests clearly indicate that the grammar is highly flexible. We also show that the precision errors are not due to poor pitch tracking, singer deficiencies or delay in auditory feedback.",
        "zenodo_id": 1492461,
        "dblp_key": "conf/ismir/ViraraghavanAM18",
        "keywords": [
            "Carnatic music",
            "continuous pitch movement",
            "gamakas",
            "constant-pitch notes",
            "transients",
            "stationary points",
            "pitch curve",
            "melody information",
            "perceptual tests",
            "grammar flexibility"
        ],
        "content": "PRECISION OF SUNG NOTES IN CARNATIC MUSIC\nVenkata Subramanian Viraraghavan1;2Hema A Murthy3R Aravind2\n1TCS Research and Innovation, Embedded Systems and Robotics, Bangalore, India\n2Department of Electrical Engineering, Indian Institute of Technology, Madras\n3Department of Computer Science and Engineering, Indian Institute of Technology, Madras\nvenkatasubramanian.v@tcs.com, hema@cse.iitm.ac.in, aravind@ee.iitm.ac.in\nABSTRACT\nCarnatic music is replete with continuous pitch move-\nment called gamaka s and can be viewed as consisting of\nconstant-pitch notes (CPNs) and transients. The stationary\npoints (STAs) of transients – points where the pitch curve\nchanges direction – also carry melody information. In this\npaper, the precision of sung notes in Carnatic music is stud-\nied in detail by treating CPNs and STAs separately. There\nis variation among the nineteen musicians considered, but\non average, the precision of CPNs increases exponentially\nwith duration and settles at about 10 cents for CPNs longer\nthan 0.5 seconds. For analyzing STAs, in contrast to West-\nern music, r¯aga (melody) information is found to be nec-\nessary, and errors in STAs show a signiﬁcantly larger stan-\ndard deviation of about 60 cents.\nTo corroborate these observations, the music was au-\ntomatically transcribed and re-synthesized using CPN and\nSTA information using two interpolation techniques. The\nresults of perceptual tests clearly indicate that the grammar\nis highly ﬂexible. We also show that the precision errors\nare not due to poor pitch tracking, singer deﬁciencies or\ndelay in auditory feedback.\n1. INTRODUCTION\nThe precision of sung notes in Western classical music\nhas been well studied [3, 13, 19]. However, as far as we\nknow, they have not been published for Indian classical\nmusic. Previous controlled precision studies were typically\nconcerned with long constant-pitch notes (e.g. [3]), or vi-\nbratos [18]. This approach is not suitable for Carnatic Mu-\nsic (CM), where gamaka s are characterized by expansive\npitch movements. Previous work on Indian music, such\nas [11], studied gamaka s by analyzing svara s. However,\nthe term ‘ svara ’ denotes both the note in the musical scale\nand the gamaka s that embellish it. Thus, separating the\nsteady parts of the pitch from the continuous movement is\nbeneﬁcial [5] [17].\nIn this paper, we quantify the precision of constant-pitch\nnotes (CPNs) and stationary points (STA) separately (see\nc\rV S Viraraghavan, H A Murthy, R Aravind. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: V S Viraraghavan, H A Murthy, R Aravind. “Pre-\ncision of Sung Notes in Carnatic Music”, 19th International Society for\nMusic Information Retrieval Conference, Paris, France, 2018.\nFigure 1 . (From [21]) Blue lines are CPNs and red circles,\nSTAs. Some STAs’ neighbors and duration are shown.\nName Sa Ri Ga Ma Pa Da Ni\nCarnatic SR1 R2 G2 G3 M1 M2 PD1 D2 N2 N3\nWestern CC# DD# E FF# GG# AA# B\nTable 1 .Svara -names and positions of the 12 semi-\ntones/octave for Carnatic & Western music. C is the tonic;\nthe correspondence is well-deﬁned only for CPNs.\nFigure 1 for examples). We adapt below their deﬁnitions\nfrom [21] and use the svara names given in Table 1.\n1.Silence -segments (SIL) are identiﬁed by the pitch-\ntracking algorithm [15].\n2. A constant-pitch note (CPN) is one whose pitch does\nnot vary from its mean pitch by more than 0:3semi-\ntones and lasts for at least 80ms. Non-SIL and non-\nCPN regions are called transients .Anchor note(s)\nare CPN(s) that ﬂank transient(s).\n3.Stationary points (STAs) [4, 20], are pitch positions\nwhere a continuous pitch curve changes direction. In\n[4] STAs also occur in CPNs, but they are restricted\nto the transients in this paper. STAs carry melody\ninformation [12] and are useful analytically [4, 14].\n4. The duration of CPNs and SILs is fairly straightfor-\nward. The duration of a STA, typically 100ms, is\ndeﬁned in [21]. See Figure 1 for an illustration.\nThe rest of the paper is organized as follows. Section 2\ndescribes a method to statistically analyze precision in\nCM. Section 2.2 then focuses on precision-errors in CPNs499(CPN-errors). In Section 2.3, we discuss the ambiguity\ninherent in measuring the precision-errors in STAs (STA-\nerrors) and propose the use of r¯aga-speciﬁc information\nto overcome it. In Section 3, we observe that STAs have\nabout half the precision of short CPNs, suggesting a ﬂex-\nible grammar. Section 4 describes two re-synthesis tech-\nniques with different interpolation schemes, which are\nused in a listening experiment that conﬁrms this ﬂexibil-\nity. Section 5 discusses the nature of this ﬂexibility in the\ngrammar.\n2. PRECISION-ERROR MEASUREMENTS\n2.1 Database, CPNs and STAs\nFor this work, the database comprised the same 84con-\ncert pieces used in [21], which is a subset of [8]. These\nare in the r¯agast¯od.¯i,bhairav ¯i,kharaharapriy ¯a,k¯ambh ¯oji,\n´sankar ¯abharan .am,var¯al.¯i, and kaly¯an.¯i. The database\nhas the dominant pitch (strictly ‘fundamental frequency’)\ntracked according to [15] and the tonic identiﬁed by the al-\ngorithm speciﬁed in [7]. Silence segments identiﬁed by\nthe pitch tracker are ignored. For non-SIL segments in\neach piece in the database, we convert the fundamental\nfrequency values to semitones (or equivalent cents) with\nreference to its tonic. Henceforth, the term ‘pitch’ in this\npaper implies measurement in semitones or cents.\nAlgorithm 1 of [21] is run hierarchically for the\nduration-threshold of CPNs set to 1000 ms,300ms, and\n80ms to get an initial set of CPNs (CPN-set-f). It is then\nrun backwards (in time) to get CPN-set-b. Only CPNs in\nthe intersection of these two sets are retained. Algorithm 2\nof the same work is used to identify STAs. STAs adjacent\nto two CPNs of unequal pitch on either side, and having an\nintermediate pitch value ( j¯arus, see Figure 1) are ignored.\nNineteen professional singers, whose renditions had sufﬁ-\ncient data for analysis are chosen.\n2.2 Precision-errors of CP-notes\nWe measure the statistics of the error of the mean value of\na CPN compared to a target. Instead of assuming a mu-\nsical scale, the target pitch-values of CPNs are obtained\nstatistically as the mean-values of a pitch class [13, 19].\nThat is, the locations of the signiﬁcant peaks ( >0:01\u0002\nmax value) in the histogram of CPN pitch values folded\nto one octave are chosen as the target pitch values. This\nstep is repeated for each piece independently and only\nCPNs longer than 150 ms are considered in ﬁnding tar-\nget CPN pitch-values. Two examples are shown in Fig-\nures 2(a) and 2(b). In Figure 2(a), which corresponds to a\npiece of length just under 49 minutes, the important notes\nof the r¯aga ´sankar ¯abharan .am, S, G3 and P are evident.\nThree other notes (M1, R2 and D2) that seldom occur in\nther¯aga as CPNs or anchor notes, have very small peaks.\nThere is no peak at N3, which reﬂects its rarity as a CPN in\nther¯aga. In Figure 2(b), corresponding to a piece of length\n47 minutes, the peak at R2 is not in the deﬁned scale of\nr¯aga t ¯od.¯i, but Carnatic musicians are aware of its use as\nan anchor note.A CPN-error is deﬁned as the difference of a CPN’s\nmean in semitones from the closest target CPN pitch-value.\nQualitatively, it is expected that longer CPNs have bet-\nter precision. To study this behavior, CPNs were grouped\nby duration according to Equation 1, where the bin-width,\nBw= 40 ms. An additional bin was used for any duration\nover440ms.\nBini= [iBw;(i+ 1)Bw);i2f2;3;:::; 10g (1)\nFigure 3(a) shows the histogram of CPN-errors for three\nduration bins. Figures 3(b) and 3(c) show the quantile\nplots [22] for two duration ranges. Note that the num-\nber of samples for the longer CPNs are smaller than for\nshorter ones and thus show more outliers. We also ran the\nShapiro-Wilk parametric hypothesis test of composite nor-\nmality1, with the default conﬁdence level, \u000b= 0:05. The\ntest showed that CPN-errors are, in general, notnormally\ndistributed. In fact, less than a dozen duration-bins out of\nover 200across all singers showed a normal distribution.\nNevertheless, we focus on the ﬁrst two orders of statistics\n– mean and standard deviation – of the CPN-errors and\nSTA-errors. Further, we treat the standard deviation of the\nerrors as a quantitative measure of the precision.\nThe means and standard deviations of CPN-errors for\nthe 19 singers are shown in Figure 4 as a function of dura-\ntion. The means are \u00063cents for all duration-bins. While\nthere is variation among singers, there is a trend of the stan-\ndard deviation of CPN-errors decreasing with duration.\n2.3 Statistics of Stationary Points\n2.3.1 Ambiguity in deﬁning STA targets\nThe peaks identiﬁed in Figure 2(a) correspond well with\nr¯aga-characterics even though explicit r¯aga-information is\nnot used in identifying them. This result is encouraging,\nand the natural step is to adopt the same procedure for\nidentifying STA target pitch-values. Figure 5(a) shows the\nhistogram of STAs, with signiﬁcant peaks identiﬁed ex-\nactly as for CPNs for the same piece that corresponds to\nFigure 2(a). Clearly, they do notcluster around scale notes.\nFurther, where the peaks are visible, they are wider than in\nthe case of CPNs. This suggests a larger tolerance for STA\npitch errors and is worth verifying. Figure 6 shows a manu-\nally annotated spectrogram (using the method of reassign-\nment [1, 10]) of an excerpt from a piece by a very famous\nsinger, known for her exceptional tonal purity. Manual an-\nnotation removes the possibility of errors in fundamental\nfrequency tracking. Sixteen of 37 STAs are at semitone\nvalues that are not expected in the r¯aga, but on listen-\ning to this sample, there is no hint of pitch errors. With\nSTA-errors being of the order of a semitone, the simple\nhistogram-based technique used for CPNs will not sufﬁce.\nThus, we propose the use of domain knowledge from CM\nto deﬁne target pitch-values for STAs.\n1https://in.mathworks.com/\nmatlabcentral/fileexchange/\n13964-shapiro-wilk-and-shapiro-francia-normality-tests?\nfocused=3823443&tab=function500 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018(a)´Sankar ¯abharan .am(Singer 17) S, R2, G3, M1, P, D2, N3. (b) T¯od.¯i(Singer 18) S, R1, G2, M1, P, D1, N2.\nFigure 2 . Histogram of CPNs of two sample r¯agas. The CPNs cluster around centers close to the notes of the just-tempered\nscale. Indian music note names and their values in semitones are marked per peak. Each r¯aga’s scale-sequence is also given.\n(a) Normalized histogram (b) Duration-bin 80ms to 120ms (c) 440ms to 480ms\nFigure 3 . CPN-errors for Singer 04: Histogram and quantile-plots two duration-bins. Unmarked axes are in semitones.\nFigure 4 . CPN-error statistics for 19singers. Mean-values\nare\u00063cents, except for two singers. The standard devia-\ntions are in a wider band, but decrease with duration.\n2.3.2 Restricting Measurements to Speciﬁc STAs\nAs explained above, when a sequence of adjacent STAs\nis encountered, their target pitch-values are not easy to\ndeﬁne. To reduce ambiguity, we propose restricting the\nmeasurements to a speciﬁc type of STA: one that is adja-\ncent to at least one CPN. This effectively pegs one side of\nthe continuous pitch movement, thus providing a practi-\ncally usable reference. We can then deﬁne the precision-\nerror of such a STA with respect to its adjacent CPN. Let\nsuch a CPN have a mean pitch pcin semitones. Then,\nthe target scale-note of this STA is from one of S=\nf[pc\u00061];[pc\u00062];[pc\u00063]g, where [\u0001]denotes rounding\nthe pitch to the nearest integer semitone. In the rare cases\nthat a STA is adjacent to a CPN on both sides, Sis a union\nof the sets formed by each adjacent CPN. Note that the el-\nements of Sare integer semitones. The mean errors will\nbe affected by a few cents, but as we shall see later, the\nstandard deviation of STA-errors is much larger than the\ndifferences between corresponding notes of different mu-Gamaka s Elements from S InS0\nTofR2, M1, Pgfpc\u00002;pc+ 1;pc+ 3g Yes\nTofR1, G2, M2gfpc\u00003;pc\u00001;pc+ 2g No\nTable 2 . Oscillatory gamaka s at G3 in ´sankar ¯abharan .am\nsical scales. Thus, the equal-tempered scale, or any other\nsimilar scale, can be used to deﬁne target pitch-values for\nSTAs, with only a marginal effect on the measured pre-\ncision. For consistency with Section 2.2, only CPNs and\nSTAs that have a duration \u0015150ms are included in the\nmeasurement.\nFor each CPN in a r¯aga, the valid STA pitch-targets are\na subset S0ofS. For example, with the context being an\nanchor note, say pc=G3 in the r¯aga ´sankar ¯abharan .am,\nthe choices shown in Table 2 can be made. Such rules are\nnot fully documented and are known more by practice. A\n(proprietary) synthesis algorithm that uses these rules was\nused to check and correct them in an iterative manner. Fi-\nnally, overshoots and undershoots of STAs have been re-\nported in the literature [17]. To account for them, STAs\nin ascending movements of pitch, i.e. where a STA is a\nlocal maximum, and in descending movements, where a\nSTA is a local minimum, were measured separately. These\nsubsets of STAs show histograms with sharp peaks in Fig-\nures 5(b) and 5(c). Corresponding to Table 2, the upward\ngamaka s from G3 (4 semitones) to M1 (5 semitones) and P\n(7 semitones) are visible in Figure 5(b) and the downward\ngamaka to R2 (2 semitones), in Figure 5(c). Further, the\nupward gamaka from D2 (9 semitones) to N2 (10 semi-\ntones) in Figure 5(b) matches CM practice, although N2 is\nnot in the r¯aga’s scale.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 501(a) (b) (c)\nFigure 5 . (a) Histogram of STAs for the same piece as in Figure 2(a). Peaks identiﬁed automatically are not clustered\naround notes of any musical scale. However, visually, STAs adjacent to anchor notes show sharp peaks in both (b) upward\nand (c) downward movements. To avoid clutter, note names and exact peak-locations are not given.\nFigure 6 . CPNs (blue lines) and STAs (circles and diamonds) in the spectrogram of an excerpt in the\nr¯aga ´sankar ¯abharan .am. The lower dark black curve is the pitch emphasized by reassignment. The tonic (Sa) is 188\nHz. Horizontal lines mark semitones from D1 .to˙S. Yellow diamonds mark STAs at expected pitch values and red circles,\nat unexpected ones. This musician is famous for tonal purity and singer-errors can be discounted.\n(a) Normalized histogram (b) Quantile plot for all duration-bins\nFigure 7 . STA-errors for Singer 04: Histogram and quantile-plot. Unmarked axes are in semitones.\n(a) (b)\nFigure 8 . (a) Means and standard deviations (SDs) of precision errors in STAs for 19singers. The SDs are mostly in a\nband of\u000610cents and is more or less constant, unlike that of CPNs (also shown for comparison). (b) SDs of CPN-errors\nand STA-errors for Singer 04, and for smaller CPNs ‘split’ from larger ones. The empirical ﬁt for this singer is also shown.502 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018The precision error, \u000f, of any STA chosen thus, with\npitch valuepsemitones, is measured as:\nI= arg min\nijp\u0000pij (2)\n\u000f=p\u0000pI (3)\nwhereiindexes S0peranchor note per r¯aga. Table 2\ndoes not consider gamaka s traversing more than 3 semi-\ntones, which are rare in CM. Even so, STAs for which\nj\u000fj>2semitones are not included in the measurement.\nWith this more reliable deﬁnition of the precision error of\nthese STAs, their statistics were collected per singer. The\nhistogram and quantile plots of STA-errors (Singer 04) are\npresented in Figures 7(a) and 7(b), which have two virtu-\nally indistinguishable plots each: one for STAs in ascend-\ning movements, and another for downward movements.\nThe STA-errors also do not follow a normal distribution.\n3. ANALYSIS OF OBSERVATIONS\nFigure 8(a) shows the means and standard deviations of\nthe STA-errors for the 19 singers. Unlike that of CPN-\nerrors, the standard deviation of STA-errors does notvary\nmuch with duration. The standard deviation of STA-errors\nis also about twice as large as that of CPN-errors for CPNs\nof duration around 100ms. The slight negative bias of the\nmeans of STA-errors is not yet understood.\nModeling the observed precision can be useful in ap-\nplications such as transcription. We propose a singer-\ndependent, composite empirical model to predict the stan-\ndard deviations of both CPN-errors and STA-errors. In\nthis model with two components, the ﬁrst exponentially\ndecreases with time, and can be expressed as:\n\u001bx(t) =\u001bse\u0000t=T(4)\nAs most singers do not ever reach zero precision-error\nfor even very long CPNs, we need to introduce a constant\nterm\u001br. Thus, the overall standard deviation, as a function\nof timet, can be written as:\n\u001b(t) =p\n\u001b2x(t) +\u001b2r (5)\nThe forms of Equations 4 and 5 serve to emphasize the ﬁrst\ncomponent for low values of t(STAs and short CPNs) and\nthe second, for larger t(long CPNs). For each singer, \u001br\nwas set as the average of the CPN standard deviations for\nthe last three duration-bins. The values of twere chosen\nas the mid-points of the duration-bins of Equation 1, i.e.\n(i+ 0:5)Bw. We also propose that STAs can be viewed\nas ‘point-CPNs’. For example, the short CPN around 1.4\nseconds in Figure 1 can be shrunk to a point, which would\nmake it a STA. Practically, a STA lasts at least as long as\nthe shift in windowing algorithms. For the data presented\nin this paper, this shift is 4:44ms. Consequently, we set the\nvalue of\u001b(0)as the average value of the standard deviation\nof STA-errors.\nThe best values of Tand\u001bswere found by minimiz-\ning the mean squared error of the standard deviation pre-\ndicted by Equations 4 and 5 over the following ranges ofSinger ID \u001brT\u001bsRMSE\n01 11 120 45 2.6\n03 9 115 60 2.7\n04 13 170 65 2.5\n05 10 155 55 0.8\n06 9 200 45 2.6\n07 10 165 55 2.0\n08 13 225 65 3.7\n09 8 155 70 3.7\n10 10 165 55 1.8\n13 15 145 50 2.4\n17 9 165 60 3.1\n18 11 180 85 6.0\n19 8 115 60 2.0\n20 19 140 50 7.0\n21 11 210 40 2.8\n27 9 110 50 1.8\n28 11 120 65 1.8\n30 10 75 65 1.8\n31 14 145 55 3.9\nTable 3 . Prediction parameters for the nineteen musicians\naliased by Singer ID. The parameter Tis in ms and \u001br; \u001bs\nand the root mean squared error (RMSE) are in cents.\nvalues:T2 f20;21;:::; 300gms and\u001bs2 fi\u0012; i =\n1;2;:::; 20g,\u0012= 0:05semitones. An example for Singer\n04 is given in Figure 8(b), which shows a good ﬁt of the\nmodel with the observations. The quantitative measure of\nthe ﬁt (RMSE) and the values of T,\u001bsand\u001brfor the 19\nsingers are given in Table 3. The typical value of \u001brbeing\nin the range 0:08to0:15semitones (one outlier at 0:19) is\nin good agreement with the precision range of 0:1to0:15\nsemitones reported for choir singers [19]. It remains to be\nseen if STAs of types other than in Section 2.3 also follow\nthe same statistics.\nThe cause(s) of the precision-error trend is (are) not\nfully clear, but we eliminate one possibility here. Two\ntypes of auditory feedback have been reported in the lit-\nerature. The ﬁrst is involuntary, and takes about 100ms\nto take effect, and another, voluntary taking about 300\nms [9]. For the smaller duration bins, the voluntary mech-\nanism does not have time to effect corrections. Even the\ninvoluntary mechanism does not seem to explain all of\nthe variance. Speciﬁcally, the precision error of CPNs\nis not mirrored in successively longer initial segments of\nCPNs. That is, for each CPNs of duration t\u0015300ms,\nand preceded by SIL, several CPNs of duration iTsplit;i2\nf1;2;:::;bt\nTsplitcg, whereTsplit= 20 ms, were split from\nit and their precision for the duration-bins (Equation 1)\nwere calculated. This result for Singer 04 is also shown\nin Figure 8(b). It is clear that, for durations around 100\nms, the standard deviation of precision error for such ‘split\nnotes’ is far lower than that for CPNs found from the def-\ninition. Thus, it cannot explain the trend seen in CPN-\nerrors. Further, this was seen to be true for all the singers.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 503Pair-set U, cosine R, cosine R, linear\n1 B T NA\n2 NA B T\nTable 4 . Pair-sets per r ¯aga for the quantization algorithms\n(U or R) and interpolation schemes (linear or cosine) of the\nsynthesized samples in the pair-wise comparison test.\n4. EXPERIMENTAL CONFIRMATION\nA musicological view (e.g [12]) is that STAs should be\nprecise, which is not consistent with the observations pre-\nsented hitherto. In a previous experiment2, the STAs at\nR1 and D1 in the r¯aga pantuvar ¯al.¯iwere shifted to R2 and\nD2 respectively. The shift is not perceivable in the audio\nsynthesized from manual notation [16]. While this exper-\niment conﬁrms the relatively large precision-error of the\nSTAs, perceptual tests were not conducted. Independently,\nwe designed an experiment3to conﬁrm the large variabil-\nity in the pitch-values of STAs. We concatenated one ex-\ncerpt at slow speed, and another relatively fast one, both\nchosen from ¯al¯apana s in four important, gamaka -heavy\nr¯agas. These pieces of approximately one-minute duration\nwere transcribed in two ways. In uniform quantization (U),\nthe pitch in semitones of each CPN or STA, p, was set to\np0= [p], where [\u0001]denotes rounding towards the nearest\ninteger semitone. In this method, 13% to19% of STAs\nwere quantized to pitch values not in the r¯aga-speciﬁc list\nR, i.e. Table 2 extended to all anchor notes and octaves. In\nthe second method (R), with iindexing R, andei2R, a\nCPN/STA pitch ( p) was set top0=eIwhere:\nI= arg min(jp\u0000eij) (6)\nThe STAs and CPNs were then synthesized by con-\nstructing a pitch curve that was constant at CPN-locations.\nSTAs and CPNs (and SILs) were connected to each other\nby using linear or cosine-interpolation. For the latter, the\nphase was set to 0(\u0019)at a starting higher (lower) STA/CPN\nand to\u0019(0)at the ending lower (higher) STA/CPN. These\npitch curves, sampled at 1 kHz, and the short-term en-\nergy of the original excerpts resampled to 1 kHz, were\nfed to a good-quality, 5-harmonics synthesis algorithm [6].\nWe asked listeners to rate pair-wise, the synthesis sam-\nples on the basis of adherence to the r¯aga. The pair-sets\nperr¯aga are given in Table 4. Twenty four participants\n(twelve experts) heard all r¯agas of pair-set 1 in the order\nk¯ambh ¯oji,´sankar ¯abharan .am,t¯od.¯i, and bhairav ¯i. Within\na pair, the order was random. This was then repeated for\npair-set 2.\nTable 5 shows the results of the listening test. For each\npair, the preference-percentages, the average rating across\nparticipants and r¯agas, the average difference between rat-\nings, and the average absolute-difference between the rat-\nings are given. All measures indicate that there is no clear\n2http://carnatic2000.tripod.com/maya.zip\n3https://www.iitm.ac.in/donlab/pctestmusic/\nindex.html?owner=venkat1&testid=test1&testcount=\n8U vs. R Cosine vs. Linear\nMeasure U R Cosine Linear\nPreference 34 (33) 34 (35) 25 (33) 26 (21)\npercentage Equal: 31 (31) Equal: 49 (46)\nAvg. rating 3.5 (3.3) 3.4 (3.3) 3.6 (3.4) 3.6 (3.4)\nAvg. diff. 0.0 (0.1); 0.7 (0.7) 0.0 (0.1); 0.7 (0.7)\nTable 5 . Results of the pair-wise comparison test (expert-\nratings in brackets). In the last row, average differences and\naverage absolute-differences are separated by semicolons.\npreference among the possibilities. This result can also\nexplain why many interpolation schemes for gamaka s –\nBezier curves [2], Hermite polynomials [6, 14], Gaayaka\nsoftware [16], sine curves [17] etc. – all seem to work.\n5. CONCLUSIONS\nWe presented the statistics of precision errors of CPNs and\nSTAs, and measured their means and standard deviations\nas a function of duration. While the analysis was done sep-\narately, the precision-errors for both CPNs and STAs were\nempirically ﬁtted in a single model. We also presented the\nresults of a listening experiment using the outputs of two\nsynthesis algorithms, both of which also treat CPNs and\nSTAs separately. The key conclusions that can be drawn\nfrom this work are:\n1. The standard deviation of the precision error in\nCPNs decreases with duration. A nominal value of\n20cents may be used for a duration of 200ms, and\n10cents for long CPNs.\n2. The standard deviation of the precision error in STAs\nis independent of duration ( 45to85cents across\nsingers). A nominal value of 60cents may be used.\n3. Even experts could not tell apart samples that had\nSTAs quantized to notes within a r¯aga’s grammar\nand those that did not. Also, samples that used linear\nand cosine interpolation were not distinguishable.\nThus, it appears that there is a large tolerance for both\nthe precision of STAs and the way they are connected,\nwhich implies a highly ﬂexible grammar for CM. Point\n3 suggests that a rich transcription for a CM piece need\nnot be unique. It may also indicate that re-synthesis qual-\nity cannot be used to rate algorithms for rich transcription.\nHowever, given that Figures 5(b) and 5(c) show peaks only\nat expected locations, the existence of unique rich tran-\nscription with a large tolerance for STAs is likely.\nFinally, it should be noted that the ﬂexibility in its gram-\nmar does notmean that ‘CM is imprecise’ or that the preci-\nsion of STAs is unimportant. Instead, this ﬂexibility should\nbe seen as natural in a form of music that employs contin-\nuous pitch movement in profusion.504 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20186. ACKNOWLEDGMENTS\nThis research was partly funded by the European Research\nCouncil under the European Unions Seventh Framework\nProgram, as part of the CompMusic project (ERC grant\nagreement 267583). We thank all participants of the listen-\ning test and Ms. Anju Leela Thomas and Mr. Krishnaraj\nSekhar for helping set it up.\n7. REFERENCES\n[1] Audacity 2.1.2 [Audio editor] (2016).\nLatest version freely available from:\nhttp://www.audacityteam.org/download/. Accessed:\n2017-04-16.\n[2] Bret Battey. Bezier spline modeling of pitch-\ncontinuous melodic expression and ornamentation.\nComputer Music Journal , 28(4):25–39, 2004.\n[3] Simone Dalla Bella, Jean-Franc ¸ois Gigu `ere, and Is-\nabelle Peretz. Singing proﬁciency in the general pop-\nulation. The journal of the Acoustical Society of Amer-\nica, 121(2):1182–1189, 2007.\n[4] Shrey Dutta and Hema A Murthy. A modiﬁed rough\nlongest common subsequence algorithm for motif spot-\nting in an alapana of carnatic music. In Communica-\ntions (NCC), 2014 Twentieth National Conference on ,\npages 1–6. IEEE, 2014.\n[5] Kaustuv Kanti Ganguli, Ashwin Lele, Saurabh Pinjani,\nPreeti Rao, Ajay Srinivasamurthy, and Sankalp Gulati.\nMelodic shape stylization for robust and efﬁcient mo-\ntif detection in hindustani vocal music. In Proc. of the\nNational Conference on Communication (NCC) , 2017.\n[6] Kaustuv Kanti Ganguli and Preeti Rao. Discrimination\nof melodic patterns in indian classical music. In Com-\nmunications (NCC), 2015 Twenty First National Con-\nference on , pages 1–6. IEEE, 2015.\n[7] Sankalp Gulati, Ashwin Bellur, Justin Salamon, Vig-\nnesh Ishwar, Hema A Murthy, and Xavier Serra. Au-\ntomatic tonic identiﬁcation in indian art music: ap-\nproaches and evaluation. Journal of New Music Re-\nsearch , 43(1):53–71, 2014.\n[8] Sankalp Gulati, Joan Serr `a, Kaustuv Kanti Gan-\nguli, Sertan Sent ¨urk, and Xavier Serra. Time-delayed\nmelody surfaces for r ¯aga recognition. In ISMIR , pages\n751–757, 2016.\n[9] Timothy C Hain, Theresa A Burnett, Swathi Ki-\nran, Charles R Larson, Shajila Singh, and Mary K\nKenney. Instructing subjects to make a voluntary re-\nsponse reveals the presence of two components to\nthe audio-vocal reﬂex. Experimental Brain Research ,\n130(2):133–141, 2000.\n[10] Kunihiko Kodera, Roger Gendrin, and C de Villedary.\nAnalysis of time-varying signals with small bt values.\nIEEE Transactions on Acoustics, Speech, and Signal\nProcessing , 26(1):64–76, 1978.[11] Gopala Krishna Koduri, Joan Serr `a Juli `a, and Xavier\nSerra. Characterization of intonation in carnatic music\nby parametrizing pitch histograms. In Proceedings of\nthe 13th International Society for Music Information\nRetrieval Conference; 2012 Oct 8-12; , 2012.\n[12] TM Krishna and Vignesh Ishwar. Carnatic music:\nSvara, gamaka, motif and raga identity. In Serra X,\nRao P , Murthy H, Bozkurt B, editors. Proceedings of\nthe 2nd CompMusic Workshop; 2012 Jul 12-13; Istan-\nbul, Turkey. Universitat Pompeu Fabra, 2012.\n[13] Peter Q Pfordresher, Steven Brown, Kimberly M\nMeier, Michel Belyk, and Mario Liotti. Imprecise\nsinging is widespread. The Journal of the Acoustical\nSociety of America , 128(4):2182–2190, 2010.\n[14] HG Ranjani, Deepak Paramashivan, and Thippur V\nSreenivas. Quantized melodic contours in indian art\nmusic perception: Application to transcription. In Pro-\nceedings of the 18th International Society for Music In-\nformation Retrieval Conference , pages 174–180, 2017.\n[15] Justin Salamon and Emilia G ´omez. Melody Extraction\nfrom Polyphonic Music Signals using Pitch Contour\nCharacteristics. IEEE Transactions on Audio, Speech\nand Language Processing , 20:1759–1770, 2012.\n[16] M Subramanian. Carnatic music-automatic computer\nsynthesis of gamakams. Sangeet Natak , 43(3):28–36,\n2009.\n[17] Srikumar K Subramanian, Lonce Wyse, and Kevin\nMcGee. A two-component representation for modeling\ngamakas of carnatic music. In Proc. of 2nd CompMusic\nWorkshop , 2012.\n[18] Johan Sundberg. Acoustic and psychoacoustic aspects\nof vocal vibrato.\n[19] Sten Ternstr ¨om and Johan Sundberg. Intonation pre-\ncision of choir singers. The Journal of the Acoustical\nSociety of America , 84(1):59–69, 1988.\n[20] Venkatasubramanian V , K R Ramakrishnan, and H V\nSahasrabuddhe. Music information retrieval using con-\ntinuity. In Proceedings of the Symposium on the Fron-\ntiers of Research on Speech and Music (FRSM 2004) ,\nnumber 228, pages 74–83. Annamalai University, Chi-\ndambaram, 2004.\n[21] Venkata S Viraraghavan, R Aravind, and Hema\nMurthy. A statistical analysis of gamakas in carnatic\nmusic. In Proceedings of the 18th International Soci-\nety for Music Information Retrieval Conference , pages\n243–249, 2017.\n[22] Martin B Wilk and Ram Gnanadesikan. Probability\nplotting methods for the analysis for the analysis of\ndata. Biometrika , 55(1):1–17, 1968.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 505"
    },
    {
        "title": "Driftin&apos; Down the Scale: Dynamic Time Warping in the Presence of Pitch Drift and Transpositions.",
        "author": [
            "Simon Waloschek",
            "Aristotelis Hadjakos"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492495",
        "url": "https://doi.org/10.5281/zenodo.1492495",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/97_Paper.pdf",
        "abstract": "Recordings of a cappella music often exhibit significant pitch drift. This drift may accumulate over time to a total transposition of several semitones, which renders the canonical 2-dimensional Dynamic Time Warping (DTW) useless. We propose Transposition-Aware Dynamic Time Warping (TA-DTW), an approach that introduces a 3rd dimension to DTW. Steps in this dimension represent changes in transposition. Paired with suitable input features, TA-DTW computes an optimal alignment path between a symbolic score and a corresponding audio recording in the presence of pitch drift or arbitrary transpositions.",
        "zenodo_id": 1492495,
        "dblp_key": "conf/ismir/WaloschekH18",
        "keywords": [
            "cappella music",
            "pitch drift",
            "Dynamic Time Warping",
            "transposition",
            "Dynamic Time Warping",
            "optimal alignment path",
            "symbolic score",
            "audio recording",
            "transposition-aware",
            "input features"
        ],
        "content": "DRIFTIN’ DOWN THE SCALE: DYNAMIC TIME WARPING IN THE\nPRESENCE OF PITCH DRIFT AND TRANSPOSITIONS\nSimon Waloschek, Aristotelis Hadjakos\nCenter of Music and Film Informatics, Detmold University of Music, Germany\nfs.waloschek, a.hadjakos g@cemfi.de\nABSTRACT\nRecordings of a cappella music often exhibit signiﬁcant\npitch drift. This drift may accumulate over time to a to-\ntal transposition of several semitones, which renders the\ncanonical 2-dimensional Dynamic Time Warping (DTW)\nuseless. We propose Transposition-Aware Dynamic Time\nWarping (TA-DTW), an approach that introduces a 3rd\ndimension to DTW. Steps in this dimension represent\nchanges in transposition. Paired with suitable input fea-\ntures, TA-DTW computes an optimal alignment path be-\ntween a symbolic score and a corresponding audio record-\ning in the presence of pitch drift or arbitrary transpositions.\n1. INTRODUCTION\nExisting audio-to-score alignment systems based on DTW\nare not yet able to handle performances appropriately if\nthey exhibit signiﬁcant pitch drift. However, pitch drift\nis rather common in a cappella singing and choir per-\nformances due to accumulation of intonation inaccuracies\nover time.\nAbsolute pitch, which is the ability to recognize and\nproduce a given pitch without an external reference, is a\nrare ability of about 0.01% of the population [23]. There-\nfore, most singers have to rely on a combination of ref-\nerencing with previous and simultaneous pitches together\nwith muscle memory to intonate appropriately. In solo\nsinging, the accuracy of a good singer has been reported\nto range from about 13 cents (standard deviation) [24] to\nabout 22 cents [27] for very short melodies and intervals.\nExpert listeners judge a deviation of 20-25 cents to be\nstill in tune [27]. The accuracy of note production can\nhowever be inﬂuenced adversely by various factors, e.g.\nby an unbalanced ratio of the sound pressure level of the\nreference sound in relation to the feedback sound of the\nsinger’s own voice [25]. Also the presence of vibrato, the\nabsence of common partials between the voices and the\nabsence of high partials make it more difﬁcult to intonate\ncorrectly [24].\nDue to the lack of an absolute reference, these minor de-\nc\rSimon Waloschek, Aristotelis Hadjakos. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Simon Waloschek, Aristotelis Hadjakos. “Driftin’\ndown the scale: Dynamic time warping in the presence of pitch drift\nand transpositions”, 19th International Society for Music Information Re-\ntrieval Conference, Paris, France, 2018.viations can lead to relatively large pitch deviations in the\nlong run. It is widely reported that choirs have signiﬁcant\npitch drift, see [1]. Seaton et al. [20] surveyed amateur and\nprofessional choir singers and conductors regarding their\nexperiences with pitch drift. Nearly half of the participants\nreport that pitch drift occurs regularly while only 14% re-\nport that drift happens rarely or not at all. Nearly 80% of\nthe participants say that the direction of the drift is usually\ndownward while almost all other participants say that drift\noccurs in either direction similarly often.\nHowever, pitch drift is not just an addition of small in-\naccuracies: Howard argues that pitch drift is almost in-\nevitable when singing unaccompanied music that modu-\nlates from one key to another [12]. This arises mathemat-\nically from the observation that singers use non-tempered\nintonation based on the ratios of small integer numbers.\nHoward’s measurements provide evidence that singers in\nfact use non-tempered intonation and that they consequen-\ntially shift their intonation as hypothesized. He even argues\n“that conductors who have a desire to correct overall into-\nnation drift for its own sake in an a cappella performance\n[...] may be misguided” [12] if the piece contains consid-\nerable modulation.\nThis paper contributes a novel method called\nTransposition-Aware Dynamic Time Warping (TA-DTW)\naiming at making an alignment between a symbolic score\nand a corresponding audio recording. TA-DTW is able to\nhandle pitch drift (in contrast to a constant transposition),\nwhich makes it particularly useful to synchronize choir\nand singing recordings. Furthermore, it may be used as a\ndrop-in replacement for existing solutions that can handle\n“only” ﬁxed transpositions, which are commonly encoun-\ntered in transcriptions of a piece for another instrument\nand historically informed performance practice.\nThe structure of this paper as follows: Section 2 de-\nscribes the feature design, derived from Harmonic Pitch\nClass Proﬁles. Section 3 introduces TA-DTW as a 3-\ndimensional DTW based on the aforementioned features,\nfollowed by an evaluation, conclusions and future work in\nSections 4 and 5. Related work is discussed in comparison\nto our approach within the individual sections.\n2. ROBUST PITCH CLASS FEATURES IN THE\nPRESENCE OF PITCH DRIFT\nAudio-to-Score Alignment algorithms that are based on\nDTW generally use pitch features such as chroma fea-\ntures [13,15] as an intermediate data representation format630Figure 1 . Chromagrams showing logarithmic sine sweeps\nfrom C4 to B4. Left: Canonical CQT chromas. Right:\nHPCPs with frame-wise tuning frequency estimation as\nused in this paper.\nbetween the symbolic score and corresponding audio data.\nChroma features are 12-dimensional vectors that describe,\nto a certain extent, tonality of a speciﬁc and usually very\nshort sequence of music. They are obtained by measur-\ning the relative intensity of each of the 12 pitch classes (C,\nC], D, ..., B) of the equal-tempered scale within an anal-\nysis frame. While this undoubtedly reduces the informa-\ntional content in relation to tonal characteristics, this very\nreduction makes chroma features robust to changes in in-\nstrumentation and timbre. They still capture melodic and\nharmonic characteristics of music and thus provide a useful\nabstraction for various tasks within the music information\nretrieval research area.\nFor symbolic scores chroma features can be computed\ndirectly by mapping the pitch of the individual notes to\ntheir corresponding element in the chroma vector [13]. In\ncase of audio data they are mostly computed using the Fast\nFourier Transform (FFT) or the specialized Constant Q\nTransform (CQT) [3]. The latter is especially useful for\nwestern music since it allows for 1-to-1 mapping of ﬁlter\nbins to MIDI pitches. CQT can be expressed as a ﬁlter\nbank with ﬁxed center frequencies for all ﬁlters, deﬁned\nby a given reference pitch. In the presence of pitch drift,\nhowever, this reference pitch and thus the ﬁlter center fre-\nquencies have to be dynamically adapted to get high selec-\ntivity between adjacent semitones. Figure 1 (left) shows\nthe effect of ﬁxed center frequencies for a continuous sine\nsweep: The resulting chromagram appears to be “fuzzy”\nwith leakage between semitones. For music that is more\ncomplex inaccurate center frequencies can lead to practi-\ncally unusable chroma features.\n2.1 Tuning Frequency Estimation\nOne way of dealing with these inaccuracies is the usage\nof multiple ﬁlter banks with slightly diverging reference\npitch [18] and picking the best ﬁt for each frame. A more\ngeneral solution, however, is the use of tuning frequency\nestimation. Gnann et al. [8] proposed a real-time estima-\ntion algorithm, speciﬁcally addressing pitch drift in choir\nmusic. While their method of reducing the quadratic tun-\ning deviation serves the purpose of having an active “pitch\ndrift warning system” for rehearsals quite well, it does not\nallow for a time resolution down to a single analysis frame.The same problem arises in an approach by Dressler [7]\nbased on circular statistics: These methods calculate the\ntuning frequency iteratively resulting in an initial delay.\nSuch behavior is unfavorable for DTW algorithms that rely\non greatest feature accuracy possible in each frame. Both\ntuning estimation approaches were evaluated by Degani et\nal. [6] together with a third option that utilizes Harmonic\nPitch Class Proﬁles (HPCP) [9, 29] and allows for the cal-\nculation of the deviation from reference pitch within a sin-\ngle analysis frame [11]. As all three tuning estimation\nmethods demonstrated similar performance, we will focus\non HPCPs and their superior time resolution.\nHPCPs are closely related to chroma features but differ\nin one important aspect: They are tuning independent by\ndeﬁnition, so that the reference frequency is not explicitly\ndeﬁned. The result of HPCP computation is an octave-\nindependent histogram with 12, 24, 36, or even more bins,\ndepending on the needed frequency resolution as shown in\nFigure 2. For a constant quality spectrum CwithNbins\nin total and 36 per octave, the value of the k-th bin of a\n36-bin HPCP His calculated by\nHk=N=36X\nn=0jCk+36nj 8k2[1 : 36]: (1)\nIn order to estimate the tuning deviation, each HPCP\nframe is processed with a peak detection algorithm. Mul-\ntiple peaks might be found in such a frame and the global\ndeviation from an assumed reference pitch can be averaged\nover the individual deviations of the peaks.\nIn this paper we decided to use quadratic interpolation\nas described by Smith in [22] with 36-bin HPCPs. How-\never, we do not look for the peaks explicitly but rather ac-\ncumulate the magnitudes of each semitone’s 3 correspond-\ning bins:\nmk=11X\nn=0H3n+k8k2f1;2;3g: (2)\nWe assume that m2is the highest of these values, otherwise\nwe would have shifted the values of mcyclically by a value\ns2f\u0000 1;0;1g. To be consistent with [22] and increase\nreadability, we deﬁne \u000b=m1,\f=m2, and\r=m3.\nNext, we ﬁt a parabola to these magnitudes, i.e. through\n0102030\nAcculuatedb isrotcvMooMcetg doMtgrseubd\nFigure 2 . Harmonic Pitch Class Proﬁle with 36 bins per\noctave for a single analysis frame.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 631-1 0 1py(p)\nonsmemtnkatedpeau kagsntude\nαβ\nγFigure 3 . Parabola that is ﬁtted to the bins’ magnitudes \u000b,\n\f, and\r.\n(\u00001;\u000b);(0;\f);and(1;\r)as shown in Figure 3. (The bins\nhave been arbitrarily renumbered about the estimated peak\nthat is represented by the parabola’s vertex.)\nLooking at the general formula for a parabola\ny(x) =a(x\u0000p)2+b (3)\nwe can directly tell the location of the vertex: The center\npointpgives us the error offset of our actual pitch in bins,\nwhile the amplitude y(p) =bequals the peak amplitude.\nAll three magnitudes can be calculated as follows:\n\u000b=ap2+ 2ap+a+b;\n\f=ap2+b;\n\r=ap2\u00002ap+a+b(4)\nThe peak location in (fractional) bins is given by\np=1\n2\u000b\u0000\r\n\u000b\u00002\f+\r2\u0014\n\u00001\n2;1\n2\u0015\n(5)\nand the estimated peak magnitude gets calculated by:\ny(p) =\f\u00001\n4(\u000b\u0000\r)p: (6)\n2.2 Feature Computation\nKnowing the global tuning deviation p, we can calculate\nthe estimated peak magnitude for every pitch class (from\nits 3 bins) of our HPCP Hvia Equation 6 with sbeing the\npotential cyclic shifting done after Equation 2 and\n\u000b=H3k\u00002\u0000s\n\f=H3k\u00001\u0000s\n\r=H3k\u0000s8k2[1 : 12]: (7)\nThis step effectively reduces the 36 bins per octave to 12\nbins per octave, which makes the resulting HPCPs compa-\nrable to standard chroma features. To decrease differences\nin dynamics between the features, each HPCP vector is\nnormalized to have length 1. We obtain a chromagram as\nexemplary shown in Figure 1 (right) by repeating this for\nthe entire audio recording.\nIf the estimated tuning is off by approximately \u00000:5or\n+0:5bins and the actual tuning of the recording ﬂuctuates,\nthe resulting features can be off by 1 semitone in either di-\nrection. This can be considered a local “unintended trans-\nposition” and will be handled in the next section.3. TRANSPOSITION-AWARE DYNAMIC TIME\nWARPING\nSupport for changing transpositions over time is very lim-\nited in current alignment systems. In this context the\nterm transposition covers intended alterations in pitch, e.g.\n“baroque pitch” or simply singing in a different key, as\nwells as unintended pitch drift that exceeds the scope of a\nsemitone. Most alignment systems, such as Antescofo [4]\npass the problem of unknown transpositions on to the user\nand force them to adapt their symbolic score accordingly\nby themselves. Niedermayer [19] solves this step by com-\nputing all possible transpositions and picking the best ﬁt.\nThis is similar to a work by M ¨uller [16] that determines\nthe best transpositions for each individual pitch feature,\nthough the results are not used for audio-to-score align-\nment. All these systems assume that the global tuning does\nnot change over time and has to be estimated only once at\nthe beginning, except Arzt [2]: He uses ﬁngerprinting to\ndetermine a musical piece, the current position within that\npiece, and its transposition before the actual alignment.\nHence, the system is theoretically able to “recover” from\nunforeseen pitch changes after some time but is (for now)\nrestricted to piano music and does not allow for continuous\nalignment in such cases.\nWe propose an extended version of DTW called\nTransposition-Aware Dynamic Time Warping (TA-DTW)\nthat allows for continuous changes in transposition during\nalignment. It shares conceptual ideas with the multidimen-\nsional DTW presented in [28] but focuses on special prop-\nerties of chroma features and the nature of musical trans-\npositions. The remainder of this section presupposes basic\nknowledge of the original DTW algorithm. A comprehen-\nsive overview can be found in [15].\n3.1 Distance Calculation & Transpositions\nIn order to compute the alignment we need the distances\nbetween the vectors from the score and the audio HPCP\nvectors as computed in Section 2. Various metrics have\nbeen used to calculate these distances such as the Eu-\nclidean distance (2-norm distance) [13, 15] and Manhat-\ntan distance (1-norm distance) [2, 15]. For computational\ncomplexity reasons, however, we opted for the Cosine Dis-\ntance1which is deﬁned for two nonzero vectors aandb\nas\nc(a;b) = 1\u0000cos(\u0012) = 1\u0000a\u0001b\nkak2\u0001kbk2(8)\nand represents the angular distance ranging from 0 (equal\norientation) to 2 (diametrically opposed). Taking into ac-\ncount that our HPCP features already have the length 1, the\ndenominator can be reduced to 1and leaves us with\nc(a;b) = 1\u0000a\u0001b= 1\u0000nX\ni=1ai\u0001bi: (9)\nThis operation can be extended to two matrices with the\nsame number of rows and unit length columns. It results in\n1As the cosine distance does not obey the triangle inequality it is\nstrictly speaking not a proper distance metric, see [21]. Nevertheless,\nit can be used as such in this particular context.632 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018a matrix that contains distances between all combinations\nof column vectors of these matrices ( 1denotes theN\u0002M\nmatrix of ones):\nc(A;B) =1\u0000ATB (10)\nTo make use of this in our context we will represent our\nsequence of HPCP vectors a1;a2;:::;aNfrom the score\nas a matrix2:\nA=2\n6664a1;1a1;2::: a 1;N\na2;1a2;2::: a 2;N\n............\na12;1a12;2::: a 12;N3\n7775(11)\nThe same applies to the sequence of HPCP vectors B=\n[b1;b2;:::;bM]from the audio recording.\nCyclically shifting the elements of a pitch class vector\nby a valuetequals transposing by tsemitones as pointed\nout by Goto [10]. We make use of this property and com-\npute all 11 possible transpositions t2[1 : 11] for the\nscore HPCP matrix A. Shifting the rows of Acan be done\nby multiplying the cyclic permutation matrix P2R12\u000212\nwithA:\nAt= (Pt)A;P=2\n6666640 0::: 0 1\n1 0::: 0 0\n0 1::: 0 0\n...............\n0 0::: 1 03\n777775(12)\nFinally, we calculate all distances for all transpositions by\nc(A;B;t) =1\u0000(At)TB: (13)\n3.2 Accumulated Multidimensional Cost Matrix\nThroughout this section we will express the results of\nEquation 13 as the cost matrix C2RN\u0002M\u000212.\nWithCwe can compute the accumulated cost matrix\nD2RN\u0002M\u000212by means of dynamic programming. Ad-\nditionally to steps in the n\u0002mplane, as performed in\nthe canonical DTW, steps in the transposition dimension\ntneed to be considered. Moving a semitone cyclically\ndownwards and upwards along the t2[0 : 11] axis will\nbe deﬁned as\nt\u0000= (t\u00001) mod 12\nt+= (t+ 1) mod 12(14)\nwhich allows arbitrary transposition changes that may even\nexceed one octave. We will restrict the possible transpo-\nsition changes between two adjacent analysis windows to\none semitone in order to keep the underlying math concise\nin this paper. While this seems reasonable in practice, too,\nit is not an inherent restriction of the algorithm. Figure 4\nvisualizes the resulting valid steps inside the accumulated\ncost matrix Dfor a possible alignment path.\n2Music analysis frameworks such as librosa ormadmom already use\nsuch a representation anyway.\ntn\nmFigure 4 . Valid steps inside the accumulated multidimen-\nsional cost matrix D.\nThe accumulated multidimensional cost matrix will be\ncalculated as follows:\nDn;m;t =8\n>>>>>><\n>>>>>>:Pm\nk=1C1;k;t ifn= 1\nPn\nk=1Ck;1;t ifm= 1\nmin(steps )+\nw(\u0001t)Cn;m;totherwise(15)\nThe (recursive) steps for computing Dare deﬁned as\nsteps =8\n>>>>>>>>>>>>>>>><\n>>>>>>>>>>>>>>>>:Dn; m \u00001; t\nDn\u00001; m; t\nDn\u00001; m \u00001; t\nDn; m \u00001; t\u0000\nDn\u00001; m; t \u0000\nDn\u00001; m \u00001; t\u0000\nDn; m \u00001; t+\nDn\u00001; m; t +\nDn\u00001; m \u00001; t+9\n>>>>>>>>>>>>>>>>=\n>>>>>>>>>>>>>>>>;: (16)\nSteps along the t-axis alone are not allowed since it is im-\npossible to calculate Dunder these conditions. The factor\nw(\u0001t)is a weighting factor for penalizing relative move-\nments along the taxis. An increased weight has shown\nto stabilize the algorithm by reducing accidental transposi-\ntion changes for special cases, e.g. monophonic passages,\nwhere regular pitch changes might look equivalent to trans-\npositions.\n3.3 Backtracking\nIn order to compute the warping path pwe use a backtrack-\ning algorithm. The starting point pLfor the recursive com-\nputation is the point along the (N;M;t )-axis in Dwith the\nleast costs:\nT= arg mint(DN;M;t )\npL= (N;M;T ):(17)Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 633Feature Algorithm DriftPercentage of events with absolute misalignment error\n\u00140.15s\u00140.20s\u00140.25s\u00140.30s\u00140.40s\u00140.50s\u00141.00s\nChroma DTW 83.46% 87.75% 89.72% 90.87% 92.04% 92.67% 93.99%\nHPCP DTW 86.28% 91.09% 93.23% 94.39% 95.75% 96.36% 97.64%\nHPCP TA-DTW 86.52% 91.69% 93.96% 95.18% 96.40% 96.92% 97.89%\nChroma DTW X 20.51% 23.00% 24.61% 25.98% 28.35% 30.23% 36.53%\nHPCP TA-DTW X 79.89% 88.35% 92.09% 93.97% 95.56% 96.28% 97.31%\nTable 1 . Results of the audio-to-score alignment evaluation. Best results are emphasized.\nNow we move recursively through the accumulated cost\nmatrix:\np`\u00001=8\n>>>>>>>>>>>><\n>>>>>>>>>>>>:arg min0\nB@D1;m\u00001;t\nD1;m\u00001;t\u0000\nD1;m\u00001;t+1\nCA ifn= 1andm\u00152\narg min0\nB@Dn\u00001;1;t\nDn\u00001;1;t\u0000\nDn\u00001;1;t+1\nCA ifm= 1andn\u00152\narg min(steps ) ifn;m\u00152\n(18)\nThe resulting 3-dimensional warping path can be orthogo-\nnally projected onto different planes as shown in Figure 5:\nn\u0002mcorresponds to the ﬁnal alignment between the in-\nput feature matrices AandB.\nm\u0002tgives information about the location of transposi-\ntion changes in the audio data in relation to the score.\nThe “local unintended transpositions” as outlined in\nSection 2.2 are clearly visible.\nn\u0002tshows accordingly these transposition change posi-\ntions in the score.\nThe transposition changes in the planes n\u0002tandm\u0002t\ncan be reﬁned by adding the center frequency offsets pas\ncalculated in Section 2 for each audio HPCP vector. This\nallows for computing continuous pitch drift data.\nnn\ntt\nmm\nFigure 5 . Orthogonal projections of the 3-dimensional\nwarping path onto the n\u0002t,m\u0002t, andn\u0002mplane.4. EVALUATION\nWe evaluated the alignment accuracy of our TA-DTW with\nHPCPs in comparison with the canonical DTW and plain\nCQT-based chroma features. Due to the lack of datasets\nwith choral music and corresponding beat-level annota-\ntions, we generated the evaluation data from the complete\nMusicNet dataset [26]. It consists of almost 1.3 million\nnote events (that were manually veriﬁed by expert anno-\ntators) for approximately 34 hours of chamber music per-\nformances with various instrumentations. All material is\navailable in 44.1 kHz sampling rate. Although a cappella\nmusic is not part of the dataset, we considered it meaning-\nful for evaluation due to its substantial scope. Since the\npieces of the dataset present no signiﬁcant pitch drift, we\nextracted all recordings to raw PCM ﬁles and introduced\ncontinuously changing random artiﬁcial pitch drift.\nThis was done by loading each of the 330 pieces\ninto a Digital Audio Workstation (DAW) and generating\n100 equidistantly distributed random pitch change mark-\ners along the time axis for each piece. The amount of in-\ntroduced pitch drift was kept within the range of \u00064 semi-\ntones and followed brownian motion to introduce corre-\nlation with previous markers. Between the markers, the\npitch deviation was linearly interpolated. A randomization\nas such is a reasonably realistic simulation according to\nthe pitch drift model for a cappella music of Mauch et al.\nin [14].\nBased on the evaluation methodology of Cont et al. in\n[5] for audio-to-score alignment, the absolute alignment\nerrors in seconds for note onsets were calculated. 1024\nsamples per window and no overlap for the audio data were\nused. This equals a feature rate of \u001843vectors per second\nor a window length of approximately 23ms. The results\nare shown in Table 1. We found w(\u0001t) = 6:5to be a\nsuitable penalty factor for changes along the t-axis in D,\nsee Equation 15.\nIn the absence of any pitch drift, we found that using\nHPCPs showed superior performance in contrast to plain\nchroma features, regardless of whether the calculation of\nthe alignment was done using DTW or our proposed TA-\nDTW. This can be explained by slight deviations in tun-\ning frequency of the recordings that are compensated in\nthe computation of HPCPs. Using them as calculated in\nthis paper introduces occasional errors in the form of “lo-\ncal transpositions” as explained in Section 2.2. Such errors\ncan be minimized by switching from DTW to TA-DTW,\nwhich further improves the alignment.634 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018For music with drifting pitch our proposed method\nshows comparable results while the “classic” approach\nfailed for the majority of note onsets. We assumed that the\nremaining\u001830% are based on the limited maximum pitch\ndrift in our test data, resulting in this amount of data effec-\ntively being not or only marginally pitched. This hypoth-\nesis was validated by exemplarily computing the align-\nment with plain DTW using audio ﬁles that exhibit con-\nstant pitch drift >1 semitone. These cases showed results\n<1% for all shown time intervals.\nThe drawbacks of this approach are the increased mem-\nory and computation requirements. TA-DTW requires a\ncost matrix of dimension N\u0002M\u000212, which is 12 times\nlarger compared to the 2-dimensional cost matrix for DTW.\nComputing the warping path with TA-DTW involves com-\nputingN\u0002M\u000212\u00029path scores. This is 36 times greater\nin contrast to M\u0002N\u00023in DTW. We have empirically\nveriﬁed these numbers. For music recordings with a length\nthat exceeds several minutes, the algorithm demands well-\nequipped hardware in terms of memory.\n5. CONCLUSIONS & FUTURE WORK\nIn this paper we presented a DTW-based method to com-\npute audio-to-score alignment for audio data that suffers\nfrom drift in global pitch throughout the recording. To this\nend we explained the computation of suitable pitch fea-\ntures that allow for “sharper” distinction between adjacent\nnotes on the pitch scale. We used these features in con-\njunction with a DTW algorithm that we extended to sup-\nport static and dynamically changing transpositions. A ﬁ-\nnal evaluation proved the robustness and effectiveness of\nour approach.\nApart from normalizing our pitch features to have\nlength 1, we did not process them any further. This en-\nsures that they can be used as basis for additional feature\nenhancements [17]. Similarly, this applies to our DTW ex-\ntension: Since the Transposition-Aware DTW is conceptu-\nally very close to the original DTW algorithm, many vari-\nations and improvements such as varying step size con-\nditions, local weights, or global constraints [15] can be\nadapted easily.\nPotential on-line variants of TA-DTW could greatly re-\nduce the computational complexity by only calculating\ncost values for t\u00061for the current audio window.\n6. REFERENCES\n[1] Per-Gunnar Alldahl. Choral intonation . Gehrmans\nMusikf ¨orlag, 2008.\n[2] Andreas Arzt. Flexible and Robust Music Tracking .\nPhD thesis, Johannes Kepler University, Linz, Austria,\n2016.\n[3] Judith C Brown. Calculation of a constant Q spectral\ntransform. The Journal of the Acoustical Society of\nAmerica , 89(1):425–434, 1991.[4] Arshia Cont. Antescofo: Anticipatory synchronization\nand control of interactive parameters in computer mu-\nsic. In Proceedings of the International Computer Mu-\nsic Conference (ICMC) , pages 33–40, 2008.\n[5] Arshia Cont, Diemo Schwarz, Norbert Schnell, and\nChristopher Raphael. Evaluation of real-time audio-\nto-score alignment. In Proceedings of the 8th Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR) , 2007.\n[6] Alessio Degani, Marco Dalai, Riccardo Leonardi, and\nPierangelo Migliorati. Comparison of tuning frequency\nestimation methods. Multimedia Tools and Applica-\ntions , 74(15):5917–5934, 2015.\n[7] Karin Dressler and Sebastian Streich. Tuning fre-\nquency estimation using circular statistics. In Proceed-\nings of the 8th International Conference on Music In-\nformation Retrieval (ISMIR) , 2007.\n[8] V olker Gnann, Markus Kitza, Julian Becker, and Mar-\ntin Spiertz. Least-squares local tuning frequency esti-\nmation for choir music. In Audio Engineering Society\nConvention 131 . Audio Engineering Society, 2011.\n[9] Emilia G ´omez. Tonal description of music au-\ndio signals . PhD thesis, Universitat Pompeu Fabra,\nBarcelona, Spain, 2006.\n[10] Masataka Goto. A chorus section detection method for\nmusical audio signals and its application to a music lis-\ntening station. IEEE Transactions on Audio, Speech,\nand Language Processing , 14(5):1783–1794, 2006.\n[11] Christopher Harte and Mark Sandler. Automatic chord\nrecognition using quantised chroma and harmonic\nchange segmentation. Centre for Digital Music, Queen\nMary University of London , 2009.\n[12] David M Howard. Intonation drift in a capella soprano,\nalto, tenor, bass quartet singing with key modulation.\nJournal of Voice , 21(3):300–315, 2007.\n[13] Ning Hu, Roger B Dannenberg, and George Tzane-\ntakis. Polyphonic audio matching and alignment for\nmusic retrieval. In IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics , pages 185–\n188, 2003.\n[14] Matthias Mauch, Klaus Frieler, and Simon Dixon. In-\ntonation in unaccompanied singing: Accuracy, drift,\nand a model of reference pitch memory. The Journal\nof the Acoustical Society of America , 136(1):401–411,\n2014.\n[15] Meinard M ¨uller. Information retrieval for music and\nmotion , volume 2. Springer, 2007.\n[16] Meinard M ¨uller and Michael Clausen. Transposition-\ninvariant self-similarity matrices. In Proceedings of the\n8th International Conference on Music Information\nRetrieval (ISMIR) , 2007.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 635[17] Meinard M ¨uller and Sebastian Ewert. Chroma tool-\nbox: Matlab implementations for extracting variants\nof chroma-based audio features. In Proceedings of the\n12th International Society for Music Information Re-\ntrieval Conference (ISMIR) , 2011.\n[18] Meinard M ¨uller, Peter Grosche, and Frans Wiering.\nAutomated analysis of performance variations in folk\nsong recordings. In Proceedings of the 11th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , 2010.\n[19] Bernhard Niedermayer. Accurate audio-to-score align-\nment: data acquisition in the context of computational\nmusicology . PhD thesis, Johannes Kepler University,\nLinz, Austria, 2012.\n[20] Richard Seaton, Dennis Pim, and David Sharp. Pitch\ndrift in a cappella choral singing – work in progress.\nInProceedings of the Institute for Acoustics Annual\nSpring Conference , volume 35, 2013.\n[21] John R Smith. Integrated spatial and feature image sys-\ntems: Retrieval, analysis and compression . PhD thesis,\nColumbia University, New York, USA, 1997.\n[22] Julius O. Smith. Spectral audio signal processing .\nW3K publishing, 2011.\n[23] Annie H Takeuchi and Stewart H Hulse. Absolute\npitch. Psychological bulletin , 113(2):345–361, 1993.\n[24] Sten Ternstr ¨om and Johan Sundberg. Acoustical fac-\ntors related to pitch precision in choir singing. Speech\nTransmission Laboratory Quarterly Progress and Sta-\ntus Report (STL-QPSR , 2(3):1982, 1982.\n[25] Sten Ternstr ¨om and Johan Sundberg. Intonation pre-\ncision of choir singers. The Journal of the Acoustical\nSociety of America , 84(1):59–69, 1988.\n[26] John Thickstun, Zaid Harchaoui, and Sham Kakade.\nLearning features of music from scratch. In Proceed-\nings of the International Conference on Learning Rep-\nresentations (ICLR) , 2017.\n[27] Allan Vurma and Jaan Ross. Production and perception\nof musical intervals. Music Perception: An Interdisci-\nplinary Journal , 23(4):331–344, 2006.\n[28] Martin W ¨ollmer, Marc Al-Hames, Florian Eyben,\nBj¨orn Schuller, and Gerhard Rigoll. A multidimen-\nsional dynamic time warping algorithm for efﬁcient\nmultimodal fusion of asynchronous data streams. Neu-\nrocomputing , 73(1-3):366–380, 2009.\n[29] Yongwei Zhu, Mohan S Kankanhalli, and Sheng Gao.\nMusic key detection for musical audio. In Proceedings\nof the 11th International Multimedia Modelling Con-\nference (MMM) , 2005.636 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Computational Corpus Analysis: A Case Study on Jazz Solos.",
        "author": [
            "Christof Weiss",
            "Stefan Balke",
            "Jakob Abeßer",
            "Meinard Müller"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492439",
        "url": "https://doi.org/10.5281/zenodo.1492439",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/23_Paper.pdf",
        "abstract": "For musicological studies on large corpora, the compilation of suitable data constitutes a time-consuming step. In particular, this is true for high-quality symbolic representations that are generated manually in a tedious process. A recent study on Western classical music has shown that musical phenomena such as the evolution of tonal complexity over history can also be analyzed on the basis of audio recordings. As our first contribution, we transfer this corpus analysis method to jazz music using the Weimar Jazz Database, which contains high-level symbolic transcriptions of jazz solos along with the audio recordings. Second, we investigate the influence of the input representation type on the corpus-level observations. In our experiments, all representation types led to qualitatively similar results. We conclude that audio recordings can build a reasonable basis for conducting such type of corpus analysis.",
        "zenodo_id": 1492439,
        "dblp_key": "conf/ismir/WeissBAM18",
        "keywords": [
            "corpus analysis",
            "symbolic representations",
            "audio recordings",
            "tonal complexity",
            "corpus-level observations",
            "Weimar Jazz Database",
            "corpus analysis method",
            "qualitatively similar results",
            "input representation type",
            "corpus analysis"
        ],
        "content": "COMPUTATIONAL CORPUS ANALYSIS: A CASE STUDY ON JAZZ\nSOLOS\nChristof Weiß1Stefan Balke1Jakob Abeßer2Meinard Müller1\n1International Audio Laboratories Erlangen, Germany\n2Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany\nchristof.weiss@audiolabs-erlangen.de\nABSTRACT\nFor musicological studies on large corpora, the compila-\ntion of suitable data constitutes a time-consuming step. In\nparticular, this is true for high-quality symbolic represen-\ntations that are generated manually in a tedious process.\nA recent study on Western classical music has shown that\nmusical phenomena such as the evolution of tonal com-\nplexity over history can also be analyzed on the basis of\naudio recordings. As our ﬁrst contribution, we transfer this\ncorpus analysis method to jazz music using the Weimar\nJazz Database, which contains high-level symbolic tran-\nscriptions of jazz solos along with the audio recordings.\nSecond, we investigate the inﬂuence of the input represen-\ntation type on the corpus-level observations. In our exper-\niments, all representation types led to qualitatively similar\nresults. We conclude that audio recordings can build a rea-\nsonable basis for conducting such type of corpus analysis.\n1. INTRODUCTION\nCharacterized by keywords such as systematic musicology\norcomputational music analysis , quantitative and data-\ndriven methods have recently gained importance within\nmusicology. As one central beneﬁt, computational meth-\nods enable corpus-based studies on a large scale. Several\nstudies have been conducted recently for different music\ngenres including pop music [13], jazz [1, 6, 9], and West-\nern classical music [2, 17, 21, 24], and also in the ﬁeld of\nethnomusicology [14, 16, 19]. For conducting such corpus\nstudies, a number of different aspects are important. Be-\nsides methodological questions such as the musical char-\nacteristics under investigation (e. g., melodic, harmonic, or\nrhythmic aspects), also the way these characteristics are\nmeasured, evaluated, and presented matters. Moreover, the\ncorpus itself plays a crucial role. Beyond its size and com-\nposition, the representation of the music data constitutes an\nimportant aspect. For example, the data can be given as a\nsymbolic transcription [9,16], as a graphical score [17], or\nas an audio recording [6, 13, 18].\nc\rChristof Weiß, Stefan Balke, Jakob Abeßer, Meinard\nMüller. Licensed under a Creative Commons Attribution 4.0 Interna-\ntional License (CC BY 4.0). Attribution: Christof Weiß, Stefan Balke,\nJakob Abeßer, Meinard Müller. “Computational Corpus Analysis: A\nCase Study on Jazz Solos”, 19th International Society for Music Infor-\nmation Retrieval Conference, Paris, France, 2018.\nYearComplexity\nS1\n1920S2\n1950S3\n1992Figure 1 . Procedure for mapping feature values from indi-\nvidual solos onto the timeline using the recording years.\nIn this paper, we investigate the inﬂuence of the mu-\nsic representation type on the corpus analysis results. For\nthis purpose, we present a case study for jazz music using\nthe solos contained in the Weimar Jazz Database [15]. As\nan example for a corpus analysis, we investigate the tonal\ncomplexity of the jazz solos using a measure introduced\nin [22]. Inspired by recent work on pop [13] and classi-\ncal music [21], we apply a visualization technique where\nquantitative descriptors for individual pieces are mapped\nonto a timeline as shown in Figure 1. The resulting evolu-\ntion curves [21] allow for studying the evolution of musical\nphenomena (here: tonal complexity) over history.\nAs input data for this study, we compare different repre-\nsentations of the jazz solos including a high-quality sym-\nbolic transcription of the solo melody as well as the full\nmix audio recording of the solo section. Furthermore, we\ninvestigate intermediate representations, which rely on sig-\nnal processing techniques [3,4,7,8] for enhancing the pres-\nence of the solo instrument and for suppressing accom-\npanying instruments and audio-speciﬁc artifacts. Specif-\nically, we consider the approaches proposed in [4, 7].\nThough the music representations—as well as the derived\nfeatures—exhibit a different behavior on the piece level ,\nour experiments show that on the corpus level , results\nare qualitatively similar for audio-based procedures and\nfor analyses based on high-quality symbolic transcriptions.\nOur ﬁndings encourage to perform corpus studies on the\nbasis of audio recordings. This opens up new ways for\nmusicological research since audio recordings are avail-\nable easily without an extensive transcription or annotation\nprocess that often needs to be done manually.\nThe remainder of this paper is structured as follows.\nFirst, we describe our music scenario and sketch some mu-\nsicological hypotheses (Section 2). Second, we detail on416Figure 2 . Complexity measure \u0000based on the circle of ﬁfths. Values for a sparse chroma vector (left), a ﬂat chroma vector\n(middle), and a more realistic chroma vector (right) are shown. The red arrows denote the resultant vectors.\nour tonal complexity measure and explain its musical im-\nplications (Section 3). We then describe the different repre-\nsentations and signal processing techniques we use in this\nstudy (Section 4). Next, we describe our corpus analysis\nstrategy and present the experimental results (Section 5).\nFinally, we discuss the implications of our ﬁndings.\n2. JAZZ SCENARIO\nWithin the scope of jazz music, the Weimar Jazz Database\n(WJD) with its 456manually generated transcriptions of\nfamous jazz solos constitutes a unique dataset [15]. A ma-\njor beneﬁt of the WJD lies in its clean annotations of the\nsolo melody (fundamental frequency, F0), which create a\ncontrolled environment for systematic experiments. The\ndata served as basis for a number of musicological studies,\nwhich mainly focus on performance analysis [1, 6, 9].\nBesides the rhythmical aspects of the solos [6] and\nthe melodic phrasing [9], also the played pitch material\n(scales) can be of musicological interest. In our experi-\nments, we consider this dimension by measuring the tonal\ncomplexity of the pitches played by the soloist. We expect\nto ﬁnd a lower tonal complexity for solos from the Chicago\nJazz era (1920s), compared to, for instance, Bebop solos\nfrom the 1950s. However, there might be some outliers\nin each period. For example, Chet Baker’s intimate solos\nwill probably obtain lower complexity values than Clifford\nBrown’s solos—although both perform in the same period.\n3. MEASURING TONAL COMPLEXITY\nThe analysis of music complexity has been an important\ntask within MIR research in the past years. Streich [20]\ntackled multiple dimensions of this notion denoted as\nacoustic, timbral, rhythmic, and tonal complexity. Con-\ncerning tonality, many studies [5, 12, 20] focus on sequen-\ntial complexity aspects such as the complexity of chord\nprogressions [5]. As opposed to this, chroma-based com-\nplexity measures were introduced in [22], which locally\ndescribe the pitch class distribution without explicitly cap-\nturing transitional characteristics. Despite their simplicity,\nthese features have shown a high correspondence to an in-\ntuitive understanding of music complexity over the course\nof an individual piece [22]. Beyond that, they have turned\nout to be useful for classifying music recordings accord-\ning to style categories [23]. Averaging such complexity\nfeatures over many pieces provides meaningful and stableresults, which has been shown in a large-scale study of mu-\nsical evolution in classical music [21]. As one contribu-\ntion, we transfer this concept to jazz music and show that\ncomplexity features also yield meaningful results for this\nscenario. In contrast to [21], the WJD scenario provides\ndata in different representations (see Section 4), whose in-\nﬂuence we want to investigate. Moreover, we have detailed\nmetadata such as the recording year of each solo.\nThe complexity measures introduced in [22, 23] de-\nscribe statistical properties of an underlying normalized\nchroma distribution. Flat distributions result in high com-\nplexity values while sharp distributions result in low ones.\nIn [23], several different measures are introduced for this\npurpose such as entropy-, sparsity-, and ﬂatness-based\nquantities. Here, we restrict ourselves to one feature\nthat additionally accounts for the tonal relationship of the\nprominent pitch classes. Following [23], we now summa-\nrize the deﬁnition of this measure \u0000 :R12![0;1]. Let\nc= (c0; c1; : : : ; c 11)T2R12denote a chroma vector with\npositive entries ( cn\u00150) normalized with respect to the `1-\nnorm\u0010P11\nn=0cn= 1\u0011\n. The entries cnwithn2[0 : 11]\nindicate the salience of the twelve pitch classes C,C],: : :,\nB, respectively. Because of octave invariance, the features\nshow a cyclic behavior so that a transposition in pitch leads\nto a circular shift.\nFor computing the complexity \u0000(c)2[0;1]of a chroma\nvector c2R12, we ﬁrst re-sort the chroma values to an\nordering of perfect ﬁfth intervals ( 7semitones) resulting in\nthe vector c\ffthdeﬁned by:\nc\ffth\nn=c(n\u00017) mod 12 : (1)\nBased on the reordered vector c\ffth, we deﬁne the resultant\nvector r(c)with a length of\nr(c) =\f\f1\nNPN\u00001\nn=0c\ffth\nnexp\u00002\u0019in\n12\u0001\f\f: (2)\nThen, the complexity \u0000(c)is deﬁned as:\n\u0000(c) =p\n1\u0000r(c): (3)\nThis measure corresponds to the angular deviation and de-\nscribes the spread of the pitch classes around the circle of\nﬁfths. Figure 2 shows the complexity feature and the re-\nsultant vector r(c)(in red) for three input chroma vectors\nc. For a sparse vector (left), the complexity is minimal\n(\u0000(c) = 0) . For a ﬂat vector (middle), we obtain maximal\ncomplexity (\u0000(c) = 1) .Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 417(a) (b)\n(c)\n (d)\nTempo (BPM) Tempo (BPM)Mean Complexity\nMean ComplexityChroma 200 ms\nChroma 400 ms\nChroma 1 s\nChroma GlobalChroma 200 ms\nChroma 400 ms\nChroma 1 s\nChroma Global1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n40 100 180 40 100 1801\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4Figure 3 . Complexity values for musical scales in several tempi, computed with different window lengths. (a) Diatonic\nscale. (b) Chromatic scale. (c) Complexity values for the diatonic scale. (d) Complexity values for the chromatic scale.\nIn this paper, we compute complexity features for jazz\nsolos. Relying on chroma features of the full audio record-\nings, the features describe the complexity of the overall\ntonal content—comprising the sounding pitches of the solo\ninstrument as well as the accompanying instruments (e. g.,\npiano, double bass, drums). Since noise-like sounds such\nas drum hits contribute in an approximately equal fash-\nion to each of the twelve chroma values, this results in\nan overall increase of complexity. As opposed to the full\nmix recording, a symbolic transcription of the solo only\ncaptures the pitches played by the solo instrument. Since\nwe deal with monophonic solo instruments (mainly sax-\nophone, trumpet, trombone), there is only one non-zero\npitch class at a time. Using a small window length (ﬁne-\ngrained resolution) for the chroma features, this results in\nlow complexity values. As soon as we use a larger window\nlength—e. g., by smoothing over several chroma frames—\nthe complexity features are computed from local pitch\nclass histograms and, thus, show mostly non-zero values\nin case that different pitch classes are played within the\nanalysis interval. Hereby, the feature values depend on the\nnumber of pitch classes played but also, on their tonal rela-\ntionship. Playing many ﬁfth-related pitch classes—such as\na diatonic scale—yields a distribution pointing towards a\nspeciﬁc direction in the circle of ﬁfths and, thus, results in\na rather low complexity value (see Figure 3a and c). For a\nchromatic scale, in contrast, pitch classes all over the circle\nof ﬁfths contribute equally resulting in a high complexity\nvalue (Figure 3b and d).\nBeyond the pitch classes and their relationship, the du-\nration of the notes has a crucial effect on the complexity\nfeatures. To illustrate this effect, we show in Figure 3 com-\nplexity values for scales played in different tempi. For this\nexperiment, we synthesized a diatonic scale and a chro-\nmatic scale from music notation software using a saxo-\nphone sound. From the generated audio, we computed\nchroma features in different temporal resolutions. On the\nbasis of these chroma features, we calculated complex-\nity values and averaged these over the full segment. Fig-\nures 3c and d show the resulting complexity features for\ndifferent resolutions and playing tempi. In a higher tempo,\nmore pitch classes are sounding within a window lead-ing to higher complexity. The absolute complexity val-\nues also depend on the analysis window length. The four\ncurves in Figures 3c and d refer to different chroma win-\ndow lengths of 200 ms, 400 ms, 1 s, and a global chroma\nhistogram, respectively. With larger smoothing windows,\nwe obtain higher complexity values. Using global chroma\nstatistics, the complexity is practically independent of the\ntempo since it always relies on the same pitch class dis-\ntribution. For a monophonic input signal, our feature cap-\ntures the tonal complexity of the melody pitches rather than\ndescribing a “melodic complexity,” which usually accounts\nfor further properties such as direction, jumps, melodic in-\ntervals. etc. Despite these simpliﬁcations, our complexity\nfeature mostly behaves in a musically meaningful way.\n4. INPUT DATA AND PRE-PROCESSING\nThe complexity feature \u0000(c)can be computed from differ-\nent pitch class representations. This enables us to compare\nthe feature values for different representation types. Be-\nsides symbolic representations with explicit pitch informa-\ntion, we can also use audio-based chromagrams.1In our\nexperiments (Section 5), we investigate how the choice of\nthe input representation inﬂuences the complexity features\n(see Figure 4).\nBeyond the symbolic transcription (Figure 4a) cre-\nated in the Jazzomat project (manual F0 annotation of\nthe solo melody), we consider the full mix audio sig-\nnal (d), as well as two modiﬁed audio versions (b, c).\nFor this, we use signal processing methods to suppress\ncomponents that might affect our harmony analysis. One\nsuch method is harmonic–percussive–residual separation\n(HPRS) [7], which is an extension of the technique pre-\nsented by Fitzgerald [8]. HPRS aims to decompose a given\naudio recording into a harmonic component, a percussive\ncomponent, and a residual component. The residual com-\nponent captures portions of the audio recording which are\nneither of harmonic, nor percussive nature, e. g., noise-like\nsignals such as applause or the breathy component of the\nsaxophone sound. For enhancing the tonal parts of the jazz\n1In contrast to our complexity measure, high-level measures as pre-\nsented in [5, 20] often require pre-processing steps that involve challeng-\ning tasks such as automatic transcription.418 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018(a)\n (b) (c) (d)\nFigure 4 . Log-frequency representations of Dexter Gordon’s solo from “Society Red” (excerpt of 14 seconds). (a) Sym-\nbolic transcription. (b) Source-separated melody (score-informed). (c) Harmonic–Percussive–Residual separation, har-\nmonic part. (d) Full audio mix.\nrecordings, we use HPRS and throw away both the residual\nand the percussive components (see Figure 4c).\nBeyond this straight-forward separation, we also use a\nmore sophisticated decomposition. Hereby, we try to ex-\ntract the solo signal from the full mix via source separa-\ntion. Similar to previous approaches [10, 11], we make\nuse of score information (F0 trajectories) for the separa-\ntion into solo instrument and backing track [4]. The funda-\nmental frequency trajectory of the solo instrument is used\nto construct time-variant masks that follow in principle a\ncomb ﬁlter structure covering a certain number of the in-\nstrument’s partials. Several post-processing steps ensure\nthat the bandwidth of the single comb spikes covers the\nrange of the individual partials and that interference from\ntransient sound events is attenuated. Due to the score in-\nformation, the resulting solo track is almost free of back-\nground instruments (see Figure 4b). Only signals that over-\nlap the solo instrument’s partials (such as broad-band per-\ncussive components) are sometimes perceivable.\nFrom the four representations, we compute pitch class\nfeatures by summing up energies from different octaves. A\ncomparison of the representation types is interesting since\nthey fundamentally differ from each other in several re-\nspects. First, the representations capture different musical\nparts. Symbolic transcription (a) and source-separated sig-\nnal (b) only contain the solo instrument, whereas in the\nother representations, accompaniment is also present. Sec-\nond, the transcription (a) only contains the fundamental\nfrequency while all other representations also capture over-\ntones. Third, transcription (a) and HPRS-enhancement\n(c) only capture harmonic information while the separated\nsolo (b) and the full mix (d) also contain residual and per-\ncussive components. We will now study how these proper-\nties inﬂuence a large-scale analysis on the corpus level.\n5. CORPUS ANALYSIS\nBased on the different types of music representations dis-\ncussed above, we conduct studies on the tonal complexity\nof the WJD solos. Inspired by [21], we compute evolu-\ntion curves mapping solo-wise complexity features onto\na historical timeline. For this purpose, we use the anno-\ntated recording year of each solo. To smooth the curve, we\nuse a soft mapping employing a Gaussian window of size\n11years. Thus, a solo contributes not only to its concrete\nrecording year but also, to a smaller degree, to each 5yearsbefore and after.2With this technique, the jazz solos dis-\ntribute over the timeline as shown in Figure 5a. At about\n1955, more than 15 solos contribute on average. Around\n1932 (beginning of our timeline) and 2002 (end), there are\nhardly any solos. This means that a solo contributing to\nthese years has a higher inﬂuence on the evolution curve.\nTo investigate the complexity of the jazz solos, we ﬁrst\nanalyze each solo individually by computing complexity\nfeatures in one resolution using the global chroma his-\ntogram. In Figure 5b and c, we show these complexity\nvalues of individual solos as gray crosses. Figure 5b relies\non the symbolic transcription and Figure 5c on the HPRS-\nenhanced audio (harmonic part). We ﬁnd a broad range of\nvalues for most years. Except for the ﬁrst 15 years, which\ndo not show very high complexity values, there are solos of\ndiverse complexity at all times. Thus, it is hard to ﬁnd gen-\neral structures and trends for individual solos. The overall\ndistribution, however, is similar in both ﬁgures.\nTo analyze this in more detail, we now compute evo-\nlution curves. We project the feature value of every piece\nonto the timeline using the procedure described above. The\ncomplexity curves are normalized regarding the number of\nsolos contributing to each year.3Figure 5 shows the re-\nsulting curves as blue lines. As an additional cue, we com-\npute for the most frequent soloists the complexity value av-\neraged over all their solos, respectively. For each soloist,\nwe plot the average value as horizontal bar from the ﬁrst to\nthe last solo’s recording year. Overall, we observe a slight\nincrease of complexity over the years. The ﬁrst major in-\ncrease develops towards the year 1948, where soloists such\nas Don Byas and Charlie Parker start to contribute. Around\nthe 1960s, we ﬁnd soloists such as Chet Baker with lower\ncomplexity as well as Clifford Brown or Joe Henderson\nwith higher complexity. During the 1970s, there is a ma-\njor drop, before the complexity again increases towards the\nearly 2000s (David Liebman or Michael Brecker).\nComparing the two curves in Figures 5b and c, we ob-\nserve that their shape is similar—only the overall scale of\nthe complexity values differs slightly. Most of the promi-\nnent changes in complexity can be observed on the basis of\nboth representations—such as the increase around 1945,\nthe drop in the 1970s, and even smaller changes such as\nthe local minimum around 1950. The peak and drop after\n2The window is normalized so that the total weight of a solo summed\nup over all 11years is one.\n3We sum up the weighted complexity values for all pieces and divide\nby the number of solos per year as shown in Figure 5a.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 419(a)\n(b)\n(c)1920 1940 1960 1980 2000\n1920 1940 1960 1980 2000\n1920 1940 1960 1980 20000.80.850.90.9510.80.850.90.951\n0.75051015\nYearYearComplexity ComplexityNo. ofSolosFigure 5 . (a) Average number of solos per year contained in the dataset. Evolution curve and artist means based on (b)\nsymbolic transcriptions and (c) harmonic component of audio recordings.\n2000 behave very similar. However, we have to take these\nresults with care since only a few solos contribute here.\nWe also ﬁnd differences between the two plots. For the\nﬁrst years, there are higher values in the symbolic-based\nplot (b). Here, we could identify several solos with longer\nsilence between the phrases such as Kid Ory’s solo in “Gut\nBucket Blues.” In the symbolic representation, these silent\nframes are all zero which results in a ﬂat chroma vector\n(high complexity). This leads to a higher overall com-\nplexity of these solos.4In the audio-based chromagrams,\nthere are accompanying instruments playing between the\nphrases, which leads to a lower complexity here. At the\nyear 1972, the drop in Figure 5b is more extreme than in\nFigure 5c. Looking at the individual solos, we can iden-\ntify four points of low complexity here. These are solos\nby Sonny Rollins, two of them played within the piece\n“Playin’ in the Yard” and two within “The Everywhere Ca-\nlypso” (red ellipses in Figure 5). Indeed, these solos are\nconstructed of only a few pitch classes with clear tonal re-\nlationships. For “Playin’ in the Yard”, Rollins only uses a\npentatonic scale for both solos whereas the solos in “The\n4Removing silent frames before computing features suppresses this\neffect to some degree but, at the same time, produces artiﬁcial pitch com-\nbinations within local windows (phrases squeezed together).Everywhere Calypso” mainly consist of major scales and\nbroken major triads (arpeggios). In the symbolic represen-\ntation, these structures lead to a low complexity since there\nis no accompaniment. In the audio, the background instru-\nments dampen this drop. Overall, we can observe several\ninteresting structures that might be relevant for jazz his-\ntory. These phenomena could be observed in a similar way\non the basis of both symbolic and audio representations.\nTo test these observations in more detail, we now con-\nsider four different feature resolutions (see Section 4). Be-\nyond the inﬂuence of the representation type, we want to\ntest how signal processing technologies for suppressing\nbackground instruments affect the evolution curves. Fig-\nure 6 summarizes this experiment’s results. In addition\nto the global complexity, we use chroma window lengths\nof 20 s, 1 s, and 400 ms. Looking at the vertical axes, we\nobserve different absolute ranges. For the symbolic tran-\nscription (Figure 6a), the values of \u0000for the global com-\nplexity (blue curve) lie in the interval [0:84;0:95]. In con-\ntrast, the audio-based complexity curve (d) lies in the range\n[0:93;0:98]. The enhanced audio versions are located be-\ntween these extremes. HPRS-enhancement (c) leads to a\ncurve with values in [0:85;0:97]. Score-informed source\nseparation (b) produces a global complexity curve ranging420 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Chroma Global\nChroma 20 sChroma 1 s\nChroma 400 ms\n1920 1940 1960 1980 2000Complexity\n(b) Audio melody (source separation).\n(c) Audio mix (harmonic part).\n(d) Original, full audio mix.0.80.91\n0.7\nYear1920 1940 1960 1980 2000Complexity\n0.80.91\n1920 1940 1960 1980 2000Complexity\n0.80.91\n1920 1940 1960 1980 2000Complexity\n0.90.951(a) Symbolic transcription.Figure 6 . Evolution curve based on (a) symbolic tran-\nscription, (b) source-separated melody (score-informed),\n(c) harmonic part of audio (HPRS), (d) full audio mix.\nin[0:9;0:96]. Interestingly, these values are higher than\nin the HPRS-enhanced case (c). It seems that the percus-\nsive components or other artifacts remaining in the sepa-\nrated signal affect the complexity more than the harmonic\nparts of the background instruments do. For other window\nlengths, the behavior is similar. Only for the symbolic tran-\nscription (a), the smaller window lengths of 1 s and 400 ms\n(outside the plotting range) behave differently. Since the\ntranscription of a monophonic solo exhibits only one non-\nzero pitch class at a time, this is no surprise—our complex-\nity feature drops to zero then. With larger window lengths,\nwe capture several pitch classes simultaneously leading to\nhigher complexity.Apart from the different ranges, we ﬁnd only minor\ndifferences between the curves. As in Figure 5, the ﬁrst\nyears show higher complexity for the symbolic transcrip-\ntion (a) but also for the source-separated audio (b). As\nmentioned above, this is due to the long silence gaps be-\ntween solo phrases. Considering the background instru-\nments leads to a lower complexity and thus, stabilizes the\nanalysis in some way. We also discover a special behav-\nior at the year 1972. The symbolic-based curve (a) shows\na sharp drop here stemming from Rollins’ solos discussed\nabove. This drop is weakened when using source sepa-\nration (b) or the full mix (d) but it can still be observed\nin the HPRS-enhanced analysis (c). We conclude that not\nthe background instrument but the percussive and residual\ncomponents of the melody instrument (and possible over-\nlap signals) eliminate this drop.\nBeyond these rather subtle differences, the overall be-\nhavior is similar for all curves. In all settings, we observe a\nmajor increase around 1940 followed by a slightly increas-\ning plateau between 1945 and 1967. Then, all curves drop,\nagain reach a peak around 1983, and ﬁnally rise towards\nthe 2000s. Even detailed structures are preserved through-\nout all representations such as the small drops around 1950\nand 1965, or the curvature during the 1990s. Even for years\nwith a low number of contributing solos where we have\nto take the results with care, the behavior is stable across\nrepresentations. These observations show that corpus-level\ncharacteristics of the WJD appear in a widely coherent way\nover all of our experimental settings.\n6. DISCUSSION\nFrom our experiments, we conclude that meaningful cor-\npus analyses can be performed on the basis of different\nmusic representations. Though our evolution curves for the\nWJD vary in their absolute range, general trends can be ob-\nserved for all representations. Some audio-related artifacts\nin the analysis could be suppressed with standard signal\nprocessing tools such as harmonic–percussive separation.\nIn contrast, using a high-quality score-informed technol-\nogy for melody separation did not necessarily improve the\nresults regarding audio-speciﬁc artifacts. It seems that tim-\nbral characteristics have a greater effect on the curves than\nthe presence of background instruments. Quite the con-\ntrary, the presence of background instruments could even\nstabilize the analysis since it helps to suppress extreme\ncomplexity values when the solo instrument is silent. The\nhigh similarity between symbolic- and audio-based analy-\nses lets us conclude that in a typical jazz scenario, the solo\ninstrument is prominent enough in the full mix for analyz-\ning some interesting solo characteristics directly from au-\ndio. This is an encouraging ﬁnding since audio-based stud-\nies can be scaled up to a large number of solos easily—in\ncontrast to the time-consuming procedure needed for cre-\nating the WJD melody annotations. Since the deviations\nbetween our curves occurred in regions with low solo cov-\nerage, we suppose that in a large-scale corpus study, indi-\nvidual outliers are suppressed even better leading to more\nreliable results.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 421Acknowledgments: This work has been supported\nby the German Research Foundation (MU 2686/10-1,\nMU 2686/11-1, AB 675/2-1). The International Au-\ndio Laboratories Erlangen are a joint institution of\nthe Friedrich-Alexander-Universität Erlangen-Nürnberg\n(FAU) and Fraunhofer Institut für Integrierte Schaltungen\nIIS. The authors want to thank the members of the Jaz-\nzomat research project led by Martin Pﬂeiderer for creating\nthe WJD.\n7. REFERENCES\n[1] Jakob Abeßer, Klaus Frieler, Estefanía Cano, Martin\nPﬂeiderer, and Wolf-Georg Zaddach. Score-informed\nanalysis of tuning, intonation, pitch modulation, and\ndynamics in jazz solos. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing , 25(1):168–\n177, 2017.\n[2] Héctor G. Bellmann. Categorization of Tonal Music\nStyle: A Quantitative Investigation . PhD thesis, Grif-\nﬁth University, Brisbane, Australia, 2012.\n[3] Juan J. Bosch, Rachel M. Bittner, Justin Salamon, and\nEmilia Gómez. A comparison of melody extraction\nmethods based on source-ﬁlter modelling. In Proceed-\nings of the International Society for Music Information\nRetrieval Conference (ISMIR) , pages 571–577, New\nYork City, USA, 2016.\n[4] Estefanía Cano, Gerald Schuller, and Christian\nDittmar. Pitch-informed solo and accompaniment sep-\naration towards its use in music education applications.\nEURASIP Journal on Advances in Signal Processing ,\n2014(23), 2014.\n[5] Bruno Di Giorgi, Simon Dixon, Massimiliano Zanoni,\nand Augusto Sarti. A data-driven model of tonal chord\nsequence complexity. IEEE/ACM Transactions on Au-\ndio, Speech, and Language Processing , 25(11):2237–\n2250, 2017.\n[6] Christian Dittmar, Martin Pﬂeiderer, Stefan Balke,\nand Meinard Müller. A swingogram representation\nfor tracking micro-rhythmic variation in jazz perfor-\nmances. Journal of New Music Research , 47(2):97–\n113, 2018.\n[7] Jonathan Driedger, Meinard Müller, and Sascha Disch.\nExtending harmonic–percussive separation of audio\nsignals. In Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 611–616, Taipei, Taiwan, 2014.\n[8] Derry FitzGerald. Harmonic/percussive separation us-\ning median ﬁltering. In Proceedings of the Interna-\ntional Conference on Digital Audio Effects (DAFx) ,\npages 246–253, Graz, Austria, 2010.\n[9] Klaus Frieler, Martin Pﬂeiderer, Wolf-Georg Zaddach,\nand Jakob Abeßer. Midlevel analysis of monophonic\njazz solos: A new approach to the study of improvisa-\ntion. Musicae Scientiae , 20(2):143–162, 2016.[10] Romain Hennequin, Bertrand David, and Roland\nBadeau. Score informed audio source separation using\na parametric model of non-negative spectrogram. In\nProceedings of the IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\npages 45–48, Prague, Czech Republic, 2011.\n[11] Antoine Liutkus, Jean-Louis Durrieu, Laurent Daudet,\nand Gaël Richard. An overview of informed audio\nsource separation. In Proceedings of the International\nWorkshop on Image and Audio Analysis for Multime-\ndia Interactive Services (WIAMIS) , pages 93–96, Paris,\nFrance, 2013.\n[12] Matthias Mauch and Mark Levy. Structural change on\nmultiple time scales as a correlate of musical complex-\nity. In Proceedings of the International Society for Mu-\nsic Information Retrieval Conference (ISMIR) , pages\n489–494, Miami, Florida, USA, 2011.\n[13] Matthias Mauch, Robert M. MacCallum, Mark Levy,\nand Armand M. Leroi. The evolution of popular music:\nUSA 1960–2010. Royal Society Open Science , 2(5),\n2015.\n[14] Maria Panteli, Emmanouil Benetos, and Simon Dixon.\nA review of manual and computational approaches for\nthe study of world music corpora. Journal of New Mu-\nsic Research , 47(2):176–189, 2018.\n[15] Martin Pﬂeiderer, Klaus Frieler, Jakob Abeßer, Wolf-\nGeorg Zaddach, and Benjamin Burkhart. Inside the\nJazzomat . Schott Campus, Mainz, Germany, 2017.\n[16] Marcelo Rodríguez-López and Anja V olk. Symbolic\nsegmentation: A corpus-based analysis of melodic\nphrases. In Sound, Music, and Motion , pages 548–557,\nCham, 2014. Springer.\n[17] Pablo H. Rodriguez Zivic, Favio Shifres, and\nGuillermo A. Cecchi. Perceptual basis of evolving\nWestern musical styles. Proceedings of the National\nAcademy of Sciences , 110(24):10034–10038, 2013.\n[18] Xavier Serra. Creating research corpora for the com-\nputational study of music: The case of the CompMusic\nproject. In Proceedings of the AES International Con-\nference on Semantic Audio , London, UK, 2014.\n[19] Ajay Srinivasamurthy, Andre Holzapfel, Kaus-\ntuv Kanti Ganguli, and Xavier Serra. Aspects of tempo\nand rhythmic elaboration in hindustani music: A\ncorpus study. Frontiers in Digital Humanities , 4(20),\n2017.\n[20] Sebastian Streich. Music Complexity a Multi-Faceted\nDescription of Audio Content . PhD thesis, University\nPompeu Fabra, Barcelona, Spain, 2007.\n[21] Christof Weiß, Matthias Mauch, Simon Dixon, and\nMeinard Müller. Investigating style evolution of West-\nern classical music: A computational approach. Musi-\ncae Scientiae , 2018.422 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018[22] Christof Weiß and Meinard Müller. Quantifying and vi-\nsualizing tonal complexity. In Proceedings of the Con-\nference on Interdisciplinary Musicology (CIM) , pages\n184–187, Berlin, Germany, 2014.\n[23] Christof Weiß and Meinard Müller. Tonal complexity\nfeatures for style classiﬁcation of classical music. In\nProceedings of the IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\npages 688–692, Brisbane, Australia, 2015.\n[24] Christopher Wm. White. Some Statistical Properties of\nTonality, 1650-1900 . PhD thesis, Yale University, New\nHaven, Connecticut, USA, 2013.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 423"
    },
    {
        "title": "VocalSet: A Singing Voice Dataset.",
        "author": [
            "Julia Wilkins",
            "Prem Seetharaman",
            "Alison Wahl",
            "Bryan Pardo"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.10265401",
        "url": "https://doi.org/10.5281/zenodo.10265401",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/114_Paper.pdf",
        "abstract": "There has been a persistent lack of publicly accessible data in singing voice research, particularly concerning the diversity of languages and performance styles. In this paper, we introduce SingStyle111, a large studio-quality singing dataset with multiple languages and different singing styles, and present singing style transfer examples. The dataset features 111 songs performed by eight professional singers, spanning 12.8 hours and covering English, Chinese, and Italian. SingStyle111 incorporates different singing styles, such as bel canto opera, Chinese folk singing, pop, jazz, and children. Specifically, 80 songs include at least two distinct singing styles performed by the same singer. All recordings were conducted in professional studios, yielding clean, dry vocal tracks in mono format with a 44.1 kHz sample rate. We have segmented the singing voices into phrases, providing lyrics, performance MIDI, and scores with phoneme-level alignment. We also extracted acoustic features such as Mel-Spectrogram, F0 contour, and loudness curves. This dataset applies to various MIR tasks such as Singing Voice Synthesis, Singing Voice Conversion, Singing Transcription, Score Following, and Lyrics Detection. It is also designed for Singing Style Transfer, including both performance and voice timbre style. We make the dataset freely available for research purposes. Examples and download information can be found at https://shuqid.net/singstyle111.",
        "zenodo_id": 10265401,
        "dblp_key": "conf/ismir/WilkinsSWP18",
        "keywords": [
            "publicly accessible",
            "diversity of languages",
            "performance styles",
            "large studio-quality",
            "singing dataset",
            "multiple languages",
            "different singing styles",
            "professional singers",
            "111 songs",
            "acoustic features"
        ],
        "content": "VOCALSET: A SINGING VOICE DATASET\nJulia Wilkins1;2Prem Seetharaman1Alison Wahl2;3Bryan Pardo1\n1Computer Science, Northwestern University, Evanston, IL\n2School of Music, Northwestern University, Evanston, IL\n3School of Music, Ithaca College, Ithaca, NY\njuliawilkins2018@u.northwestern.edu\nABSTRACT\nWe present V ocalSet, a singing voice dataset of a capella\nsinging. Existing singing voice datasets either do not\ncapture a large range of vocal techniques, have very few\nsingers, or are single-pitch and devoid of musical context.\nV ocalSet captures not only a range of vowels, but also a\ndiverse set of voices on many different vocal techniques,\nsung in contexts of scales, arpeggios, long tones, and ex-\ncerpts. V ocalSet has recordings of 10.1 hours of 20 pro-\nfessional singers (11 male, 9 female) performing 17 differ-\nent different vocal techniques. This data will facilitate the\ndevelopment of new machine learning models for singer\nidentiﬁcation, vocal technique identiﬁcation, singing gen-\neration and other related applications. To illustrate this, we\nestablish baseline results on vocal technique classiﬁcation\nand singer identiﬁcation by training convolutional network\nclassiﬁers on V ocalSet to perform these tasks.\n1. INTRODUCTION\nV ocalSet is a singing voice dataset containing 10.1 hours\nof recordings of professional singers demonstrating both\nstandard and extended vocal techniques in a variety of mu-\nsical contexts. Existing singing voice datasets aim to cap-\nture a focused subset of singing voice characteristics, and\ngenerally consist of fewer than ﬁve singers. V ocalSet con-\ntains recordings from 20 different singers (11 male, 9 fe-\nmale) performing a variety of vocal techniques on 5 vow-\nels. The breakdown of singer demographics is shown in\nFigure 1 and Figure 3, and the ontology of the dataset is\nshown in Figure 4. V ocalSet improves the state of exist-\ning singing voice datasets and singing voice research by\ncapturing not only a range of vowels, but also a diverse\nset of voices on many different vocal techniques, sung in\ncontexts of scales, arpeggios, long tones, and excerpts.\nRecent generative audio models based on machine\nlearning [11, 25] have mostly focused on speech applica-\ntions, using multi-speaker speech datasets [6, 13]. Gen-\neration of musical instruments has also recently been ex-\nc\rJulia Wilkins, Prem Seetharaman, Alison Wahl, Bryan\nPardo. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Julia Wilkins, Prem Seetharaman,\nAlison Wahl, Bryan Pardo. “V ocalSet: A Singing V oice Dataset”, 19th\nInternational Society for Music Information Retrieval Conference, Paris,\nFrance, 2018.\n01234567891011\nF M\nGenderCountVoice Type\nBaritone\nBass\nBass−Baritone\nCountertenor\nMezzo−Soprano\nSoprano\nTenorGender and Voice Type DistributionFigure 1 . Distribution of singer gender and voice type.\nV ocalSet data comes from 20 professional male and female\nsingers ranging in voice type.\nplored [2,5]. V ocalSet can be used in a similar way, but for\nsinging voice generation. Our dataset can also be used to\ntrain systems for vocal technique transfer (e.g. transform-\ning a sung tone without vibrato into one with vibrato) and\nsinger style transfer (e.g. transforming a female singing\nvoice to a male singing voice). Deep learning models for\nmulti-speaker source separation have shown great success\nfor speech [7, 21]. They work less well on singing voice.\nThis is likely because they were never trained on a vari-\nety of singers and singing techniques. This dataset could\nbe used to train machine learning models to separate mix-\ntures of multiple singing voices. The dataset also con-\ntains recordings of the same musical material with different\nmodulation patterns (vibrato, straight, trill, etc), making it\nuseful for training models or testing algorithms that per-\nform unison source separation using modulation pattern as\na cue [17, 22]. Other obvious uses for such data are train-\ning models to identify singing technique, style [9, 19], or a\nunique singer’s voice [1, 10, 12, 14].\nThe structure of this article is as follows: we ﬁrst com-\npare V ocalSet to existing singing voice datasets and cover\nexisting work in singing voice analysis and applications.\nWe then describe the collection and recording process for\nV ocalSet and detail the structure of the dataset. Finally, we\nillustrate the utility of V ocalSet by implementing baseline\nclassiﬁcation systems for identifying vocal technique and4680 0.6 1.2 1.8 2.4 3 3.6 4.2 4.8\nTime512102420484096HzVibrato\n0 0.6 1.2 1.8 2.4 3 3.6 4.2 4.8\nTime512102420484096HzStraight\n0 0.6 1.2 1.8 2.4 3 3.6 4.2 4.8\nTime512102420484096HzBreathy\n0 0.6 1.2 1.8 2.4 3 3.6 4.2 4.8\nTime512102420484096HzVocal Fry\n0 0.6 1.2 1.8 2.4 3 3.6 4.2 4.8\nTime512102420484096HzLip Trill\n0 0.6 1.2 1.8 2.4 3 3.6 4.2 4.8\nTime512102420484096HzTrill\n0 0.6 1.2 1.8 2.4 3 3.6 4.2 4.8\nTime512102420484096HzTrillo\n0 0.6 1.2 1.8 2.4 3 3.6 4.2 4.8\nTime512102420484096HzInhaled\n0 0.5 1 1.5 2 2.5 3 3.5 4\nTime512102420484096HzBelt\n0 0.6 1.2 1.8 2.4 3 3.6 4.2 4.8\nTime512102420484096HzSpokenFigure 2 . Mel spectrograms of 5-second samples of the 10 techniques used in our vocal technique classiﬁcation model. All\nsamples are from Female 2, singing scales, except “Trill”, “Trillo”, and “Inhaled” which are found only in the Long Tones\nsection of the dataset, and “Spoken” which is only in the Excerpts section.\nsinger identiﬁcation, trained on V ocalSet.\n2. RELATED WORK\nA few singing voice datasets already exist. The Phona-\ntion Modes Dataset [18] captures a range of vocal sounds,\nbut limits the included techniques to ’breathy’, ’pressed’,\n’ﬂow’, and ’neutral’. The dataset consists of a large num-\nber of sustained, sung vowels on a wide range of pitches\nfrom four singers. While this dataset does contain a sub-\nstantial range of pitches, the pitches are isolated, lacking\nany musical context (e.g. a scale, or an arpeggio). This\nmakes it difﬁcult to model changes between pitches. V o-\ncalSet consists of recordings within musical contexts, al-\nlowing for this modeling. The techniques listed above that\nare observed in the Phonation Modes Dataset are based\non the different formations of the throat when singing\nand not necessarily on musical applications of these tech-\nniques. Our dataset focuses on a broader range of tech-\nniques in singing, such as vibrato, trill, vocal fry, and in-\nhaled singing. See Table 2 for the full set of techniques in\nour dataset.\nThe V ocobox dataset1focuses on single vowel and\nconsonant vocal samples. While they feature a broad range\nof pitches, they only capture data from one singer. Our data\ncontains 20 singers, with a wide range of voice types and\nsinging styles over a larger range of pitches.\nThe Singing V oice Dataset [3] contains over 70 vocal\nrecordings of 28 professional, semi-professional, and am-\nateur singers performing predominantly Chinese Opera.\nThis dataset does capture a large range of voices, like V o-\ncalSet. However, it does not focus on the distinction be-\ntween vocal techniques but rather on providing extended\nexcerpts of one genre of music. V ocalSet provides a wide\n1https://github.com/vocobox/human-voice-datasetrange of vocal techniques that one would not necessarily\nclassify within a single genre so that models trained on\nV ocalSet could generalize well to many different singing\nvoice tasks.\nWe illustrate the utility of V ocalSet by implementing\nbaseline systems trained on V ocalSet for identifying vo-\ncal technique and singer identiﬁcation. Prior work on vo-\ncal technique identiﬁcation includes work that explored\nthe salient features of singing voice recordings in order to\nbetter understand what distinguishes one person’s singing\nvoice from another as well as differences in sung vow-\nels [4, 12], and work using source separation and F0 es-\ntimation to allow a user to edit the vocal technique used in\na recorded sample [8].\nAutomated singer identiﬁcation has been a topic of in-\nterest since at least 2001 [1,10,12,14]. Typical approaches\nuse shallow classiﬁers and hand-crafted features (e.g. mel\nceptral coefﬁcients) [16, 24]. Kako et al. [9] identiﬁes four\nsinging styles music style using the phase plane. Their\nwork was not applied to speciﬁc vocal technique classi-\nﬁcation, likely due to the lack of a suitable dataset. We hy-\npothesize that deep models have not been proposed in this\narea due to the scarcity of high-quality training data with\nmultiple singers. The V ocalSet data addresses these issues.\nWe illustrate this point by training deep models for singer\nidentiﬁcation and vocal technique classiﬁcation using this\ndata.\nFor singing voice generation, [20] synthesized singing\nvoice by replicating distinct and natural acoustic features\nof sung voice. In this work, we focus on classiﬁcation tasks\nrather than generation tasks. However, V ocalSet could be\napplied to generation tasks as well, and we hope our mak-\ning this dataset available will facilitate that research.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 4690123456\n2025303540455055\nAgeCountGender\nF\nMAge and Gender DistributionFigure 3 . Distribution of singer age and gender. Singer\nage\u0016= 30:9;\u001b= 8:7. We observe that the majority\nof singers lie in the range of 20to32, with a few older\noutlying singers.\n3. VOCALSET\n3.1 Singer Recruitment\n9 female and 11 male professional singers were recruited\nto participate in the data collection. A professional singer\nwas considered to be someone who has had vocal training\nleading to a bachelors or graduate degree in vocal perfor-\nmance and also earns a portion of their salary from vo-\ncal performance. The singers are of a wide age range and\nperformance specializations. V oice types present in the\ndataset include soprano, mezzo, countertenor, tenor, bari-\ntone, and bass. See Figure 1 for a detailed breakdown of\nsinger gender and voice type and Figure 3 for the distri-\nbution of singer age vs. gender. We chose to include a\nrelatively even balance of genders and voice types in the\ndataset in order to capture a wide variety of timbre and\nspectral range.\n3.2 Recording setup\nParticipants were recorded in a studio-quality recording\nbooth with an Audio-Technica AT2020 condenser micro-\nphone, with a cardioid pickup pattern. Singers were placed\nclose to the microphone in a standing position. Reference\npitches were given to singers to ensure pitch accuracy. A\nmetronome was played for the singers immediately prior\nto recording for techniques that required a speciﬁc tempo.\nTechniques marked ’fast’ in Table 2 were targeted at 330\nBPM, while techniques marked ’slow’ were targeted at 60\nBPM. Otherwise, the tempo is varied.\n3.3 Dataset Organization\nThe dataset consists of 3,560 WA V ﬁles, totalling 10.1\nhours of recorded, edited audio. The audio ﬁles vary in\nlength, from less than 1 second (quick arpeggios) to 1\nminute. Participants were asked to sing short vocalises\nof arpeggios, scales, long tones, and excerpts during thedata collection. The arpeggios and scales were sung us-\ning 10 different techniques. The long tones were sung on\n7 techniques, some of which also appear in arpeggios and\nscales (see Figure 4). Each singer was also asked to sing\nRow, Row, Row Your Boat ,Caro Mio Ben , and Dona Nobis\nPacem each in vibrato and straight tone, as well as an ex-\ncerpt of their choice. The techniques included range from\nstandard techniques such as ’fast, articulated forte’ to dif-\nﬁcult extended techniques such as ’inhaled singing’. For\narpeggios, scales, and long tones, every vocalise was sung\non vowels ’a’, ’e’, ’i’, ’o’, and ’u’. A portion of the arpeg-\ngios and scales are in both C major and F major (underlined\nin 4, while the harsher extended techniques and long tones\nare exclusively in C major. For example, singers were in-\nstructed to ’belt’ a C major arpeggio on each vowel, to-\ntalling to 5 audio clips (one per vowel). This is shown in\nFigure 4. Table 2 shows the data broken down quantita-\ntively by technique.\nThe data is sorted in nested folders specifying the\nsinger, type of sample, and vocal technique used. This\nfolder hierarchy is displayed in Figure 4.\nEach sample is uniquely labelled based on this nested\nfolder structure that it lies within. For example, Female 2\nsinging a slow, forte arpeggio in the key of F and on the\nvowel ’e’ is labelled as ’f2 arpeggios fslow forte e.wav’.\nThe dataset is publicly available2and samples from\nthe dataset used in training the classiﬁcation models are\nalso available on a demo website3.\n4. EXPERIMENTS\nAs an illustrative example of the utility of this data, we per-\nform two classiﬁcation tasks using a deep learning model\non the V ocalSet data. In the ﬁrst task, we classify vocal\ntechniques from raw time series audio using convolutional\nneural networks. In the second task, we identify singers\nfrom raw audio using a similar architecture. The network\narchitectures are shown in Table 1. Note, architectures are\nidentical except for the ﬁnal output layer.\n4.1 Training data and data preprocessing\nWe removed silence from the beginning, middle, and end\nof the recordings and then partitioned them into 3 second,\nnon-overlapping chunks at a sample rate of 44.1k. The\nchunks were then normalized using their mean and stan-\ndard deviation so that the network didn’t use amplitude as\na feature for classiﬁcation. Additionally, by limiting the\nchunk to 3 seconds of audio, our models can’t use musical\ncontext as a cue for learning the vocal technique. These\nvocal techniques can be deployed in a variety of contexts,\nso being context-invariant is important for generalization.\nFor each task, we partitioned the dataset into a training\nand a test set. For the vocal technique classiﬁcation, we\nplace all samples from 15 singers in the training set and\nall samples from the remaining 5 singers in the test set.\nFor the singer identiﬁcation, we needed to ensure that all\n2https://doi.org/10.5281/zenodo.1203819\n3https://interactiveaudiolab.github.io/demos/vocalset470 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Straight\nVocal \nFry\nSlow \nPiano\nVibrato\nLip \nTrill\nTrillo\nTrill\nInhaled\nMessa \ndi \nvoce\nUnique \nsingerID \n(i.e. \n'f2')\nArpeggios\nLong \nTones\nScales\nExcerpts\nSpoken\nStraight\nVibrato\nBreathy\nFast \npiano\nSlow \nForte\nFast \nForte\nBelt\nVibrato\nStraight\nForte\nPianissimo\na\ne\nu\ni\no\nApplies \nto \nevery \ntechnique\nVowels\nStraight\nVocal \nFry\nSlow \nPiano\nVibrato\nLip \nTrill\nBreathy\nFast \npiano\nSlow \nForte\nFast \nForte\nBeltFigure 4 . Breakdown of the techniques used in the V ocalSet dataset. Each singer performs in four different contexts:\narpeggios, long tones, scales, and excerpts. The techniques used in each context are shown. Each technique is sung on 5\nvowels, and underlined techniques indicate that the technique was sung in F major andC major.\nLayer Name Input Conv1 BatchNorm1 MaxPool1 Conv2 BatchNorm2 MaxPool2 Conv3 BatchNorm3 MaxPool3 Dense1 Dense2\n# of Units/Filters 3*44100 16 16 - 8 8 - 32 32 - 32 10/20\nFilter Size, Stride - (1, 128), (1, 1) - (1, 64), (1, 8) (1, 64), (1, 1) - (1, 64), (1, 8) (1, 256), (1, 1) - (1, 64), (1, 8) - -\nActivation function - ReLU - - ReLU - - ReLU - - ReLU softmax\nTable 1 . Network architecture. The input to the network is 3 seconds of time series audio samples from V ocalSet. The out-\nput is a 10-way classiﬁcation for vocal technique classiﬁcation and a 20-way classiﬁcation for Singer ID. The architecture\nfor both classiﬁers is identical except for the output size of the ﬁnal dense layer. For the dense layers, L2 regularization was\nset to:001.\nsingers were present in both the training and the test sets in\norder to both train and test the model using the full range\nof singer ID possibilities. We randomly sampled the entire\ndataset to create training and test sets with a ratio of 0:8\n(train): 0:2(test), while ensuring all singers were both in\ntraining and testing data. The recordings were disjoint be-\ntween the training and test sets, meaning that parts of the\nsame recording were not put in both training and testing\ndata.\nOur vocal technique classiﬁer model was trained and\ntested on the following ten vocal techniques: vibrato,\nstraight tone, belt, breathy, lip trill, spoken, inhaled\nsinging, trill, trillo, and vocal fry (bold in Table 2). Mel\nspectrograms of each technique are shown in 2, illustrating\nsome of the differences between these vocal techniques.\nThe remaining categories, such as fast/articulated forte\nandmessa di voce were not included in training for vo-\ncal technique classiﬁcation. These techniques are heav-\nily dependent on the amplitude of the recorded sample,\nand the inevitable human variation in the interpretation\nof dynamic instructions makes these samples highly vari-\nable in amplitude. Additionally, singers were not directed\nto sing a particular technique when making amplitude-\noriented technique. As a result, singers often paired\nthese amplitude-based techniques with other techniques at\nthe same time, making the categories non-exclusive (e.g.\nsinging fast/articulated forte with a lot of vibrato, or pos-\nsibly with straight tone). Additionally, messa di voce was\nexcluded because this technique requires singers to slowly\ncrescendo and then decrescendo which, in full, was gen-\nerally much longer than 3 seconds (the length of training\nsamples).\nWe train our models with a convolution neural network\nusing RMSProp [23], a learning rate of 1e-4, ReLU activa-\ntion functions, an L2 regularization of 1e-3, and a dropoutof 0.4 for the second to last dense layer. We use cross en-\ntropy as the loss function and a a batch size of 64. We train\nboth the singer identiﬁcation and vocal technique classiﬁ-\ncation models for 200,000 iterations each, where the only\ndifference between the two model architectures is the out-\nput size of the ﬁnal dense layer (10 for vocal technique,\n20 for singer ID). Both models were implemented in Py-\nTorch. [15].\n4.1.1 Data augmentation\nWe can also augment our data using standard data augmen-\ntation techniques for audio such as pitch shifting. We do\nthis to our training set for vocal technique classiﬁcation,\nbut not for singer identiﬁcation. Every excerpt is pitch\nshifted up and down 0:5and 0:25half steps. We report\nthe effect of data augmentation on our models in Table 3.\nAs shown in the table, we did observe some but not a sig-\nniﬁcant accuracy boost when using the augmented model.\n4.2 Vocal technique classiﬁcation\n4.2.1 Results\nEvaluation metrics for our best 10-way vocal technique\nclassiﬁcation model are shown in Table 3. We were able\nto achieve these results using the model architecture in Ta-\nble 1. This model performs well on unseen test data as we\ncan see from table metrics. When examining sources of\nconfusion for the model, we observed that the model most\nfrequently incorrectly labels test samples as “straight” and\n“vibrato”. We attribute this in part to the class imbalance in\nthe training data in which there are many more “vibrato”\nand “straight” samples than other techniques. Addition-\nally, for techniques such as “belt”, many singers exhib-\nited a great deal of vibrato when producing those samples\nwhich could place such techniques under the umbrella ofProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 471Figure 5 . Confusion matrix for the technique classiﬁcation\nmodel showing the quantity of predicted labels vs. true la-\nbels for each vocal technique. This model was trained on\n10 vocal techniques. A class imbalance can be observed, as\nthe number of vibrato and straight samples is much larger\nthan the remaining techniques. The model performs rela-\ntively well for a majority of the techniques, however we see\nthat nearly half of the vocal technique test samples were in-\ncorrectly classiﬁed as straight tone.\nFigure 6 . Confusion matrix for the singer identiﬁcation\nmodel displaying the predicted singer identiﬁcation vs. the\ntrue singer identiﬁcation. We can observe that female\nvoices are much more commonly classiﬁed incorrectly ver-\nsus male voices, likely due to the broader range of male\nvoices present in the training data.Vocal Techniques Examples (#) Time (min.)\nFast/articulated forte 394 22:57\nFast/articulated piano 386 23:03\nSlow/legato forte 395 65:28\nSlow/legato piano 397 69:75\nLip trill 202 24:40\nVibrato 255 57:79\nBreathy 200 28:00\nBelt 205 26:24\nVocal fry 198 34:10\nFull voice forte 100 16:29\nFull voice pianissimo 100 16:58\nTrill (upper semitone) 95 18:45\nTrillo (goat tone) 100 14:54\nMessa di voce 99 23:47\nStraight tone 361 71:65\nInhaled singing 100 9:95\nSpoken excerpt 20 4:06\nStraight tone excerpt 60 24:19\nMolto vibrato excerpt 59 24:55\nExcerpt of choice 20 20:50\nTable 2 . The content of V ocalSet, totalling to 10.1 hours of\naudio. Each vocal technique is performed by all 20singers\n(11male, 9female). Some vocal techniques are performed\nin more musical contexts (e.g. scales) than others. Bold\ntechniques were used for our classiﬁcation task.\n“vibrato”. We also observed a little bit of expected confu-\nsion between “trill” and “vibrato”, as these techniques may\nhave some overlap depending on the singer performing the\ntechnique. As seen in Figure 2, the spectrogram represen-\ntation of these two techniques looks very similar. To ad-\ndress the issue of class imbalance, we tried using data aug-\nmentation with pitch shifting to both balance the classes\nand create more data, but as previously stated and shown\nin Table 3, there was little improvement over the original\nmodel when using training data augmentation.\n4.3 Singer identiﬁcation (ID)\n4.3.1 Results\nEvaluation metrics for our best 20-way singer identiﬁca-\ntion model are shown in Table 3. The model architecture is\nidentical to that of the vocal technique classiﬁcation model\n(see 1), with the exception of the number of output nodes in\nthe ﬁnal dense layer (20 in the singer identiﬁcation model\nvs. 10 in the technique model). The singer identiﬁcation\nmodel did not perform as well as the vocal technique clas-\nsiﬁcation model. As shown in Table 3, classifying male\nvoices correctly was much easier for the model than clas-\nsifying female voices. This is expected due to the high\nsimilarity between the female voices in the training data.\nFigure 1 shows that the female data only contains 2 voice\ntypes, while the male data contains 5 voice types.\nBecause voice type is largely dependent on the vocal\nrange of the singer, having 5 different voice types within\nthe male singers makes it much easier to distinguish be-472 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Classiﬁcation Task Prior Precision Recall Top-2 Accuracy Top-3 Accuracy Male Accuracy Female Accuracy\nV ocal Technique 0:242 0:676 0:619 0:801 0:867 - -\nV ocal Technique (trained on augmented data) 0:242 0:677 0:628 0:815 0:891 - -\nSinger ID - 0:473 0:516 0:638 0:700 0:684 0:351\nTable 3 . Evaluation metrics for our vocal technique and Singer ID classiﬁcation models performing on unseen test data.\n“Prior” indicates the accuracy if we were to simply choose the most popular class (“straight”) to predict test data. We\nobserve a very slight increase in accuracy in the augmented vocal technique model. Our singer ID model has lower\nperformance, likely due to the similarity between different, primarily female, singers.\ntween male singers than female singers. The accuracy (re-\ncall) for classifying unseen male singers was nearly twice\nas good as that of unseen female singers.\n5. FUTURE WORK\nIn the future, we plan to experiment with more network\narchitectures and training techniques (e.g. Siamese train-\ning) to improve the performance of our classiﬁers. We also\nexpect researchers to use the V ocalSet dataset to train a vo-\ncal style transformation model that can transform a voice\nrecording into one using one of the techniques that we have\nrecorded in V ocalSet. For example, an untrained singer\ncould sing a simple melody on a straight tone, and our sys-\ntem could remodel their voice using the vibrato or articula-\ntion of a professional singer. We envision this as a tool for\nboth musicians and non-musicians alike, and hope to cre-\nate a web application or even a physical sound installation\nthat users could transform their voices in. We would also\nlike to use V ocalSet to train autoregressive models (e.g.\nWavenet [25]) that can generate singing voice of speciﬁc\ntechniques.\n6. CONCLUSION\nV ocalSet is a large dataset of high-quality audio record-\nings of 20 professional singers demonstrating a variety of\nvocal techniques on different vowels. Existing singing\nvoice datasets either do not capture a large range of vo-\ncal techniques, have very few singers, or are single-pitch\nand lacking musical context. V ocalSet was collected to ﬁll\nthis gap. We have shown illustrative examples of how V o-\ncalSet can be used to develop systems for diverse tasks.\nThe V ocalSet data will facilitate the development of a\nnumber of applications, including vocal technique iden-\ntiﬁcation, vocal style transformation, pitch detection, and\nvowel identiﬁcation. V ocalSet is available for download at\nhttps://doi.org/10.5281/zenodo.1203819.\n7. ACKNOWLEDGMENTS\nThis work was supported by NSF Award #1420971 and\nby a Northwestern University Center for Interdisciplinary\nResearch in the Arts grant.\n8. REFERENCES\n[1] Mark A Bartsch and Gregory H Wakeﬁeld. Singing\nvoice identiﬁcation using spectral envelope estimation.IEEE Transactions on speech and audio processing ,\n12(2):100–109, 2004.\n[2] Merlijn Blaauw and Jordi Bonada. A neural paramet-\nric singing synthesizer modeling timbre and expres-\nsion from natural songs. Applied Sciences , 7(12):1313,\n2017.\n[3] Dawn A. Black, Ma Li, and Mi Tian. Automatic iden-\ntiﬁcation of emotional cues in chinese opera singing.\n2014.\n[4] Thomas F. Cleveland. Acoustic properties of voice\ntimbre types and their inﬂuence on voice classiﬁca-\ntion. The Journal of the Acoustical Society of America ,\n61(6):1622–1629, 1977.\n[5] Jesse Engel, Cinjon Resnick, Adam Roberts, Sander\nDieleman, Douglas Eck, Karen Simonyan, and Mo-\nhammad Norouzi. Neural audio synthesis of musi-\ncal notes with wavenet autoencoders. arXiv preprint\narXiv:1704.01279 , 2017.\n[6] John S Garofolo, Lori F Lamel, William M Fisher,\nJonathan G Fiscus, and David S Pallett. Darpa timit\nacoustic-phonetic continous speech corpus cd-rom.\nnist speech disc 1-1.1. NASA STI/Recon technical re-\nport n , 93, 1993.\n[7] John R Hershey, Zhuo Chen, Jonathan Le Roux,\nand Shinji Watanabe. Deep clustering: Discrimina-\ntive embeddings for segmentation and separation. In\nAcoustics, Speech and Signal Processing (ICASSP),\n2016 IEEE International Conference on , pages 31–35.\nIEEE, 2016.\n[8] Yukara Ikemiya, Katsutoshi Itoyama, and Kazuyoshi\nYoshii. Singing voice separation and vocal f0 estima-\ntion based on mutual combination of robust princi-\npal component analysis and subharmonic summation.\n24(11), Nov. 2016.\n[9] Tatsuya Kako, Yasunori Ohishi, Hirokazu Kameoka,\nKunio Kashino, and Kazuya Takeda. Automatic iden-\ntiﬁcation for singing style based on sung melodic con-\ntour characterized in phase plane. In ISMIR , pages\n393–398. Citeseer, 2009.\n[10] Youngmoo E Kim and Brian Whitman. Singer identiﬁ-\ncation in popular music recordings using voice coding\nfeatures. In Proceedings of the 3rd International Con-\nference on Music Information Retrieval , volume 13,\npage 17, 2002.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 473[11] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani,\nRithesh Kumar, Shubham Jain, Jose Sotelo, Aaron\nCourville, and Yoshua Bengio. Samplernn: An un-\nconditional end-to-end neural audio generation model.\narXiv preprint arXiv:1612.07837 , 2016.\n[12] Maureen et al. Mellody. Modal distribution analysis,\nsynthesis, and perception of a soprano’s sung vowels.\npages 469–482, 2001.\n[13] Gautham J Mysore. Can we automatically transform\nspeech recorded on common consumer devices in real-\nworld environments into professional production qual-\nity speech? a dataset, insights, and challenges. IEEE\nSignal Processing Letters , 22(8):1006–1010, 2015.\n[14] Tin Lay Nwe and Haizhou Li. Exploring vibrato-\nmotivated acoustic features for singer identiﬁcation.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 15(2):519–530, 2007.\n[15] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin,\nAlban Desmaison, Luca Antiga, and Adam Lerer. Au-\ntomatic differentiation in pytorch. In NIPS-W , 2017.\n[16] Hemant A Patil, Purushotam G Radadia, and TK Basu.\nCombining evidences from mel cepstral features and\ncepstral mean subtracted features for singer identiﬁ-\ncation. In Asian Language Processing (IALP), 2012\nInternational Conference on , pages 145–148. IEEE,\n2012.\n[17] Fatemeh Pishdadian, Bryan Pardo, and Antoine Li-\nutkus. A multi-resolution approach to common fate-\nbased audio separation. In Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2017 IEEE International\nConference on , pages 566–570. IEEE, 2017.\n[18] Polina Prooutskova, Christopher Rhodes, and Tim\nCrawford. Breathy, resonant, pressed - automatic de-\ntection of phonation mode from audio recordings of\nsinging. 2013.\n[19] Keijiro Saino, Makoto Tachibana, and Hideki Ken-\nmochi. A singing style modeling system for singing\nvoice synthesizers. In Eleventh Annual Conference of\nthe International Speech Communication Association ,\n2010.\n[20] T. Saitou, M. Goto, M. Unoki, and M. Akagi. Speech-\nto-singing synthesis: Converting speaking voices to\nsinging voices by controlling acoustic features unique\nto singing voices. pages 215–218, Oct 2007.\n[21] Paris Smaragdis, Gautham Mysore, and Nasser Mo-\nhammadiha. Dynamic non-negative models for audio\nsource separation. In Audio Source Separation , pages\n49–71. Springer, 2018.\n[22] Fabian-Robert St ¨oter, Antoine Liutkus, Roland\nBadeau, Bernd Edler, and Paul Magron. Common\nfate model for unison source separation. In Acoustics,Speech and Signal Processing (ICASSP), 2016 IEEE\nInternational Conference on , pages 126–130. IEEE,\n2016.\n[23] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-\nrmsprop: Divide the gradient by a running average\nof its recent magnitude. COURSERA: Neural networks\nfor machine learning , 4(2):26–31, 2012.\n[24] Tsung-Han Tsai, Yu-Siang Huang, Pei-Yun Liu, and\nDe-Ming Chen. Content-based singer classiﬁcation on\ncompressed domain audio data. Multimedia Tools and\nApplications , 74(4):1489–1509, 2015.\n[25] Aaron Van Den Oord, Sander Dieleman, Heiga\nZen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray\nKavukcuoglu. Wavenet: A generative model for raw\naudio. arXiv preprint arXiv:1609.03499 , 2016.474 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "From Labeled to Unlabeled Data - On the Data Challenge in Automatic Drum Transcription.",
        "author": [
            "Chih-Wei Wu",
            "Alexander Lerch 0001"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492447",
        "url": "https://doi.org/10.5281/zenodo.1492447",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/185_Paper.pdf",
        "abstract": "Automatic Drum Transcription (ADT), like many other music information retrieval tasks, has made progress in the past years through the integration of machine learning and audio signal processing techniques. However, with the increasing popularity of data-hungry approaches such as deep learning, the insufficient amount of data becomes more and more a challenge that concerns the generality of the resulting models and the validity of the evaluation. To address this challenge in ADT, this paper first examines the existing labeled datasets and how representative they are of the research problem. Next, possibilities of using unlabeled data to improve general ADT systems are explored. Specifically, two paradigms that harness information from unlabeled data, namely feature learning and student-teacher learning, are applied to two major types of ADT systems. All systems are evaluated on four different drum datasets. The results highlight the necessity of more and larger annotated datasets and indicate the feasibility of exploiting unlabeled data for improving ADT systems.",
        "zenodo_id": 1492447,
        "dblp_key": "conf/ismir/WuL18",
        "keywords": [
            "Automatic Drum Transcription",
            "Machine learning",
            "Audio signal processing",
            "Data-hungry approaches",
            "Deep learning",
            "Labeled datasets",
            "Unlabeled data",
            "Feature learning",
            "Student-teacher learning",
            "Drum datasets"
        ],
        "content": "FROM LABELED TO UNLABELED DATA – ON THE DATA CHALLENGE\nIN AUTOMATIC DRUM TRANSCRIPTION\nChih-Wei Wu, Alexander Lerch\nGeorgia Institute of Technology, Center for Music Technology\nfcwu307, alexander.lerch g@gatech.edu\nABSTRACT\nAutomatic Drum Transcription (ADT), like many other mu-\nsic information retrieval tasks, has made progress in the past\nyears through the integration of machine learning and audio\nsignal processing techniques. However, with the increasing\npopularity of data-hungry approaches such as deep learning,\nthe insufﬁcient amount of data becomes more and more\na challenge that concerns the generality of the resulting\nmodels and the validity of the evaluation. To address this\nchallenge in ADT, this paper ﬁrst examines the existing\nlabeled datasets and how representative they are of the re-\nsearch problem. Next, possibilities of using unlabeled data\nto improve general ADT systems are explored. Speciﬁcally,\ntwo paradigms that harness information from unlabeled\ndata, namely feature learning and student-teacher learning,\nare applied to two major types of ADT systems. All sys-\ntems are evaluated on four different drum datasets. The\nresults highlight the necessity of more and larger annotated\ndatasets and indicate the feasibility of exploiting unlabeled\ndata for improving ADT systems.\n1. INTRODUCTION\nAutomatic drum transcription (ADT), a sub-task of Auto-\nmatic Music Transcription (AMT) [2] that concerns the\nextraction of drum events from music signals, witnesses a\ngrowth in data-driven approaches such as deep learning in\nrecent years [24, 25, 31 –33]. The majority of these ADT\nstudies use the popular ENST-Drums dataset [11] for de-\nvelopment by splitting the dataset into different subsets for\ntraining, validation, and testing purposes. Nevertheless,\nthe limited amount of labeled data and its potential impact\non ADT systems are rarely discussed. The heavy reliance\non one dataset raises two major concerns: (i) the model\ncould easily overﬁt the data, which questions its generality,\nand (ii) the evaluation results could be overly optimistic\ndue to the small sample size of the split. To avoid these\npitfalls, larger datasets and cross-dataset evaluation are nec-\nessary. This need has been identiﬁed by researchers and\nhas been addressed with newly released annotated datasets\nsuch as MDB-Drums [26] and RBMA [33]. These new\nc\rChih-Wei Wu, Alexander Lerch. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). Attri-\nbution: Chih-Wei Wu, Alexander Lerch. “From labeled to unlabeled data\n– on the data challenge in automatic drum transcription”, 19th International\nSociety for Music Information Retrieval Conference, Paris, France, 2018.data enable us to revisit ENST-Drums and re-examine the\nrepresentativeness of this widely-used dataset through a\nuniﬁed comparison.\nMotivated by the above mentioned issues concerning the\ndata in ADT, this paper aims to address the challenge from\ntwo different angles, (i) examining the effectiveness of the\nexisting datasets and (ii) investigating additional resources\n(e.g., unlabeled data) and techniques for supporting the\ndevelopment of general ADT systems. The contributions\nof this work include: ﬁrst, the examination of four differ-\nent datasets, highlighting the importance of data diversity.\nSecond, the evaluation of two paradigms for integrating\nunlabeled data to two major types of ADT systems. Third,\nthe demonstration of potential improvements of both types\nof ADT systems on different drum instruments using unla-\nbeled data.\n2. RELATED WORK\n2.1 Automatic Drum Transcription\nThe task of automatic drum transcription can be described\nas converting drum related audio events into music notation.\nMost of the early ADT systems, as summarized by FitzGer-\nald and Paulus [9], detect onsets of HiHat (HH), Bass Drum\n(BD), and Snare Drum (SD) in drum only recordings. Re-\ncently, this focus has shifted towards transcribing drums\nin polyphonic mixtures comprised of both percussive and\nmelodic instruments. Following these conventions, this pa-\nper deﬁnes the ADT task as detecting HH, BD, and SD in\npolyphonic mixtures.\nGenerally speaking, the existing ADT systems can be cat-\negorized into four types according to the literature [12, 19].\nThese are (i) Segment and Classify : following the standard\npattern recognition pipeline, these approaches extract audio\nfeatures from detected onset locations and classiﬁes them\nwith pre-trained models; this is a popular approach with\nmany proposed systems using different combinations of\nclassiﬁers and features [10, 12, 27, 28], (ii) Separate and\nDetect : deriving activation functions from recordings to rep-\nresent the activities of each drum, these systems subsequen-\ntially perform onset detection on these activation functions\nto locate drum hits; approaches include matrix factoriza-\ntion methods such as Non-negative Matrix Factorization\n(NMF) [6,23,35] and deep-learning-based methods such as\nRecurrent Neural Networks (RNNs) [24,31,32] and Convo-\nlutional Neural Networks (CNNs) [25, 33], (iii) Match and\nAdapt : identifying drum events by comparing with a set of445Unlabeled dataSegment and ClassifyADTsystemSeparate and DetectADTsystemFeatureLearningS-TLearningSoft TargetsFeatureExtractorFigure 1 . The overview of the evaluated paradigms for\nintegrating unlabeled data to two major ADT approaches\npre-deﬁned templates, these systems often iteratively up-\ndate the templates [38], and (iv) HMM-based Recognition :\nmodeling the temporal connections between drum events\nusing probabilistic models such as Hidden Markov Models\n(HMMs), these models try to identify the underlying drum\nsequence by using the Viterbi algorithm [7, 20].\nTo date, the majority of the existing ADT systems fall\ninto the categories of Segment and Classify andSeparate\nand Detect . Both these types of systems, despite having\nfundamental differences, use data-driven methods and face\nthe challenge described in Sect. 1. Therefore, in this paper,\nwe considered both types of systems in our experiments.\n2.2 Learning from Unlabeled Data\nTo address the data challenge in MIR tasks, techniques that\nbuild upon the existing labeled data have been proposed.\nFor example, in transfer learning [4], a deep neural network\ntrained on a task that has sufﬁcient data can be used to derive\nfeatures for another task with limited data. This method alle-\nviates the data insufﬁciency by re-using the effective models\nin the similar domains. Data augmentation , a technique\nto increase diversity of training data through music-related\ndeformations (e.g., time-stretching, pitch shifting, or distor-\ntion) and synthesis, has been successfully applied to MIR\ntasks [18] and in ADT speciﬁcally [32, 36]. However, these\ntechniques still require a reasonably sized correctly anno-\ntated dataset as a starting point, which remains a challenge\nin certain scenarios.\nAnother direction for addressing the data scarcity is to\nuse unlabeled data. Intuitively, a large collection of un-\nlabeled data can be helpful in deriving more generalized\nfeatures. This is the main concept of unsupervised feature\nlearning , and it can be implemented with algorithms such\nas Sparse Coding [22], Deep Belief Networks [13], and\nAuto-encoders [17]. More recently, the student-teacher\nlearning paradigm has also emerged as an interesting con-\ncept to incorporate unlabeled data. Referred by Hinton et\nal. as “knowledge distillation” [14], this paradigm transfers\nthe knowledge of a teacher model to a student model us-\ning the soft-targets generated by the teacher. As opposed\nto learning from the hard targets (i.e., ground truth), the\nstudent learns from the “dark knowledge” residing in the\nsoft-targets, which can be created using either labeled or\nunlabeled data [15]. A successful student model can reduce\nthe complexity of the original teacher model without signif-\nicant performance loss. Several studies also report superior\nperformance of the student models [5, 34, 37]. Overall,\nmethods that work directly with unlabeled data obviously\nhave less dependency on existing labeled data and have\nSVMPredictFeatureLearningFeatureExtractorTest DataOnset DetectionTranscriptionTrainingTestingFeature ExtractionUnlabeled dataTraining DataFeature ExtractionSVMTrainOnset DetectionModelFigure 2 . The ﬂowchart of the feature learning paradigm\nfor ADT\nhigher potential to be applicable to more tasks.\n3. METHOD\n3.1 Overview\nTo connect general ADT systems to the abundant resources\nof unlabeled data, this paper investigates the application of\nfeature learning andstudent-teacher learning toSegment\nand Classify -based and Separate and Detect -based ADT\nsystems, respectively. Figure 1 shows the two paradigms for\nintegrating unlabeled data to ADT systems as investigated\nin this paper. The feature learning paradigm is designed for\nSegment and Classify -based ADT systems. In this paradigm,\nthe unlabeled data is used to derive a feature extractor using\nan unsupervised feature learning algorithm. The resulting\nfeature extractor is then integrated into a generic Segment\nand Classify ADT framework. The student-teacher learning\nparadigm is suitable for Separate and Detect -based ADT\nsystems. This paradigm uses teacher models and unlabeled\ndata to generate soft-targets; these soft-targets play the\nimportant role of connecting any Separate and Detect -based\nsystem with unlabeled data and enable the training of the\nstudent model. In the following sections, more details of\nboth paradigms are presented.\n3.2 Feature Learning\nThe ﬂowchart in Fig. 2 shows the feature learning paradigm\nfor ADT, including both training and testing. The training\nphase starts with the training of a feature extractor using the\nunlabeled data. Speciﬁcally, we use a Convolutional Auto-\nencoder (CAE) as the feature extractor. A generic Segment\nand Classify -based ADT system is then constructed with\nthe following steps: ﬁrst, the features are extracted from the\naudio signals using the pre-trained feature extractor. Sec-\nond, the onset locations are determined by using the ground\ntruth annotations while training. Finally, the feature vectors\naround the onset locations are collected and used to train\nthree binary classiﬁers for HH, BD, and SD, respectively.\nThe classiﬁers used in this paper are Support Vector Ma-\nchines (SVMs). In the testing phase, the same pipeline is\nfollowed except for the onset detection step, which uses\nan onset detector instead of the ground truth locations. Fi-\nnally, the presence of each drum can be predicted using the\npre-trained SVMs.\nThe architecture of the CAE is shown in Fig. 3. The\ninput Xof the CAE is a Mel-spectrogram, and the output446 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 201832EncoderDecoder\n16844BlocksMel frequencyInput XOutput X’\nFeature MatrixBlocks64Figure 3 . The architecture of the proposed CAE for un-\nsupervised feature learning. The input Xis a 128\u0002N\nMel-spectrogram.\nX0is the reconstruction of X. The encoder consists of four\nconvolutional layers with f32, 16, 8, 4gchannels of 3\u00023\nkernels, accordingly. Each convolutional layer is used by a\nbatch normalization layer and a max-pooling layer of (2, 1).\nThis design maintains the temporal resolution, allowing the\nextraction of block-wise features. The bottleneck layer is\nalso a convolutional layer with 4 channels of 3\u00023kernels.\nAll non-linear units are Rectiﬁed Linear Units (ReLUs).\nThe structure of the decoder is symmetric to the encoder\nwith the max-pooling layers replaced by the up-sampling\nlayers. The CAE is trained to minimize the Mean Squared\nError (MSE) between XandX0using a gradient-descent-\nbased optimization process, and the number of training\nepochs is 30.\nThe feature extraction process, as shown in Fig. 3, is\ninspired by the method proposed by Choi et al. [4]: ﬁrst, the\nintermediate activation maps from all the layers in the en-\ncoder (including the bottleneck layer) are computed. Next,\naverage pooling is performed on these maps across the Mel-\nfrequency axis. Finally, these outputs are stacked into a\n64\u0002Nfeature matrix, where Nis the number of blocks.\nTo derive the ﬁnal feature vector at each block, the feature\nvectors from the current block and the following two blocks\nare spliced together to capture the temporal variations of\nthe event. This leads to a ﬁnal feature vector with a dimen-\nsionality d= 3\u0002F, in which Fis the number of features\n(i.e., 64).\nIn addition to the learned features, a set of baseline fea-\ntures consisting of 20 Mel Frequency Cepstral Coefﬁcients\n(MFCCs) and their ﬁrst and second derivatives is also in-\ncluded in this paradigm. As a result, the baseline feature\nvector has a dimensionality d= 3\u000260 = 180 after the\nfeature splicing.\n3.3 Student-Teacher Learning\nFigure 4 shows the ﬂowchart of the student-teacher learning\nparadigm for ADT. In the training phase, the teacher models\nare used to analyze the unlabeled data and generate the soft-\ntargets. These soft-targets, used as pseudo ground truth to\nTeacher Model(s) PredictSoft TargetsStudent ModelPredictTest DataModelPeak PickingTranscriptionTrainingTestingStudent ModelTrainUnlabeled dataFigure 4 . The ﬂowchart of the student-teacher learning\nparadigm for ADT\ntrain a student model, contain the activation functions for\nthe different drums. When multiple teachers are present,\nthe student model can be trained by iteratively passing the\nunlabeled data and its corresponding soft-targets from each\nteacher. The student model is trained by minimizing the\nMSE between the soft-targets and the model outputs. In the\ntesting phase, the trained student model processes the test\ndata and generates the corresponding activation functions.\nThe estimated locations of drum hits are identiﬁed with a\nsimple peak picking process.\nThe model architecture, conﬁguration, and parametriza-\ntion of this evaluated paradigm generally follows the setup\ndescribed in [37]. This includes two teacher models based\non Partially-Fixed NMF (PFNMF) [35] and one student\nmodel using a fully-connected, feed-forward Deep Neural\nNetwork (DNN). The soft-targets are scaled to a numerical\nrange between 0 and 1 using min-max scaling across the\ntraining data for each instrument in order to ensure their\ncompatibility with the outputs from the student DNN.\n3.4 Implementation\nThe main input representations for both paradigms are de-\nrived from the magnitude spectrogram of the Short Time\nFourier Transform (STFT), which is computed using a block\nsize of 2048 and a hop size of 512 samples with Hann win-\ndow. All of the audio signals are normalized to a range\nbetween 1 and -1, down-mixed to mono, and resampled to\n44.1 kHz prior to the computation of STFT.\nFor the feature learning paradigm, both the Mel-\nspectrogram in dB scale with 128 bins and the MFCCs\nare computed using librosa,1a Python library for audio sig-\nnal processing. The onset detection is implemented using\ntheCNNOnsetProcessor from Madmom.2Additionally,\nthe implementation of Linear SVMs from scikit-learn,3a\nPython library for machine learning, is used. A grid search\non the penalty parameter Cwithinf0.1, 1, 10, 100, 1000 g\nis performed to optimize the performance of the SVMs.\nFor student-teacher learning paradigm, the teacher mod-\nels are implemented using the PFNMF function from Nmf-\nDrumToolbox.4The peak-picking parameters are set to\nthe same as in the original paper [37].\n1https://librosa.github.io, last access 2018/03/27\n2https://madmom.readthedocs.io/en/latest/, last access 2018/03/27\n3http://scikit-learn.org/stable/, last access 2018/03/27\n4https://github.com/cwu307/NmfDrumToolbox, last access 2018/03/27Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 447The neural networks in both paradigms are implemented\nusing Keras5and the Tensorﬂow [1] backend. The weights\nare randomly initialized with normal distributions, and the\nparameters of the ADAM optimizer are set to default. The\nsource code used in this paper is available on Github.6\n4. EXPERIMENT\n4.1 Unlabeled Data\nThe unlabeled dataset in this paper is built using the source\ncode provided in [37]; this tool allows the compilation of\na list of songs from the Billboard Chart7and the retrieval\nof these songs from Youtube. This dataset consists of six\nmusical genres, including R&B =HipHop, Pop, Rock, Latin,\nAlternative, and Dance =Electronic. Each genre has 1900\nsongs, which leads to a collection of 11400 songs. All the\nsongs are cross-checked for duplicates and converted to\nmp3 format with a sampling rate of 44.1 kHz. In our exper-\niments, this dataset is further split into training, validation,\nand testing set with a percentage of 70%, 15%, and 15%,\nrespectively. To speed up the process while maintaining the\ndiversity, only a 30 s segment is extracted from each song\nfor training. The segment starts in the middle of the song to\navoid potential inactivity at the beginning. As a result, the\nentire training set has a total duration of 66.5 hrs, which is\nsigniﬁcantly larger than any existing ADT dataset. The list\nof songs and links are available on Github.8\n4.2 Labeled Data\nIn this paper, four different labeled datasets featuring poly-\nphonic mixtures are used: (i) the popular ENST-Drums (re-\nferred to as ENST) [11], (ii) the MIREX 2005 (referred to\nas m2005),(iii) the MDB-Drums (referred to as MDB) [26],\nand (iv) the RBMA dataset [33]. The latter three public sets\nhave been used in the 2017 Music Information Retrieval\nEvaluation eXchange (MIREX)9drum transcription task.\nENST minus one subset consists of 64 recordings per-\nformed by three different drummers on their own drum\nkits. The average duration of the recordings is 55 s. These\nrecordings feature different musical genres and playing\nstyles, and the multi-track ﬁles are available for remixing.\nIn this paper, the accompaniments are mixed with their cor-\nresponding drum tracks using a scaling factor of 1/3 and 2/3,\nrespectively. This setup is consistent with several previous\nstudies [24, 31, 35].\nm2005 was originally collected for the ﬁrst MIREX\ndrum transcription task in 2005 and recently made avail-\nable for MIREX 2017 drum transcription task participants.\nThe public set includes 23 recordings contributed from all\nthe participants of MIREX 2005. While covering a variety\nof musical genres, J-pop has the highest presence in this\n5https://keras.io, last access 2018/03/27\n6https://github.com/cwu307/ADT with unlabeledData, last access\n2018/06/14\n7https://www.billboard.com/charts, last access 2018/03/27\n8https://github.com/cwu307/unlabeledDrumDataset, last access\n2018/06/14\n9http://www.music-ir.org/mirex/wiki/2017, last access 2018/03/27dataset with 10 recordings. The average duration of this\ndataset is 125 s.\nMDB consists of 23 recordings of the MusicDelta sub-\nset from the MEDLEYDB dataset [3]. These recordings\ninclude a variety of musical genres such as Rock, Coun-\ntry, Disco, Reggae, and Jazz. The average duration of the\nrecordings is 54 s. Similar to ENST , this dataset contains\nmulti-track ﬁles as well as the full mixtures. In this paper,\nwe use the full-mixtures directly without any adjustment of\nthe mixing levels.\nRBMA was released as part of the public set for the\nMIREX 2017 drum transcription task. This public set in-\ncludes 27 recordings featuring mostly Electronic Dance\nMusic (EDM). The average duration of the tracks is 230 s.\nSince this dataset focuses on electronic music, it contains\nelectronic drum sounds that can be distinctively different\nfrom the other three datasets.\nIn total, there are 137 ﬁles with annotations available for\nevaluation. All ﬁles have a sampling rate of 44.1 kHz.\n4.3 Metrics\nThe evaluation metrics in this paper are Precision (P), Re-\ncall (R), and F-measure (F). Only the averaged F-measure\nis reported due to the limited space. These metrics are\nimplemented using mireval, a Python library of common\nMIR metrics [21]. To determine whether an onset is a\nmatch with the ground truth, a tolerance window of 50 ms\non both sides is used. This setting is consistent with the\nliterature [12, 24, 35], although some authors use smaller\ntolerance windows such as 30 ms [20] and 20 ms [32].\n4.4 Experiment Setup\nThis paper evaluates 9 ADT systems, comprising 4 systems\nfor the feature learning paradigm and 5 systems for the\nstudent-teacher learning paradigm. The conﬁgurations of\nthese systems are described as follows:\nFor the feature learning paradigm, the 4 systems are\ndifferentiated by their features. These features are:\n(i)MFCC: this set of features has shown its effectiveness\nin previous ADT studies [20, 27, 29]. Therefore, it is\nincluded as a baseline.\n(ii)CONV-RANDOM: this set of features is extracted\nusing the proposed CAE architecture with all the\nweights randomly initialized without further training.\nThis is another baseline inspired by [4] to serve as a\nsanity check for the effectiveness of the unsupervised\ntraining process.\n(iii) CONV-AE: this is the set of features extracted from\nthe proposed CAE after training. During the training\nprocedure, the original input is used as the target for\noptimization. In other words, the CAE is trained to\nreconstruct the input.\n(iv) CONV-DAE: this set of features is similar to CONV-\nAE except for the optimization target. In this case, a\nprocessed input is used as the target. Speciﬁcally, the\npercussive component from the Harmonic Percussive\nSource Separation (HPSS) [8] algorithm is used, and\nthe CAE is trained to approximate the percussive448 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20180.700.480.710.620.450.520.600.460.500.480.280.490.200.300.400.500.600.700.80\nKDSDHHAvg. F-measureENSTm2005MDBRBMAFigure 5 . The evaluation results of all labeled datasets with\naveraged F-measure across all systems.\ncomponent. This conﬁguration is inspired by the\nconcept of the Denoising Autoencoder (DAE) [30]\nand is designed to encourage the extraction of\ndrum-related features.\nThe teacher models for student-teacher learning\nparadigm are described in [37]. The 3 student models can\nbe differentiated by their training data. The systems are:\n(i)PFNMF (SMT): a teacher PFNMF initialized with the\ndrum templates extracted from the IDMT-SMT-Drum\ndataset [6].\n(ii)PFNMF (200D): a teacher PFNMF initialized with the\ndrum templates extracted from the 200 Drum Machine\ndataset.10\n(iii) FC-200: a fully-connected student DNN trained with\na subset of the unlabeled dataset, which consists of\n200 randomly selected songs from each genre.\n(iv) FC-ALL: a fully-connected student DNN trained with\nall the songs from all genres.\n(v)FC-ALL (ALT): a fully connected student DNN\ntrained with all the songs from only the “Alternative”\ngenre. This particular genre is selected for its superior\nperformance in preliminary tests.\nBased on these 9 systems, the following experiments are\nconducted:\nE1: Experiment 1 aims to examine the variance of the\nlabeled datasets. For each dataset, the averaged\nF-measures across all 9 systems are reported.\nE2: Experiment 2 aims to evaluate the usefulness of un-\nlabeled data for Segment and Classify -based ADT\nsystems using the feature learning paradigm. For\neach system, the averaged F-measures across all the\ndatasets are reported.\nE3: Experiment 3 aims to evaluate the usefulness of un-\nlabeled data for Separate and Detect -based ADT sys-\ntems using the student-teacher learning paradigm.\nFor each system, the averaged F-measures across all\nthe datasets are reported.\nNote that for the feature learning paradigm, a cross-\ndataset validation process is performed (e.g., train on three\ndatasets and test on the remaining one) in order to train\nthe binary classiﬁers (see Sect. 3.2). For student-teacher\n10http://www.hexawe.net/mess/200.Drum.Machines/, last access\n2018/03/27Experiments Averaged F-measure\nRole System HH BD SD\nBaseline MFCC 0.61 0.62 0.40\nBaseline CONV-RANDOM 0.61 0.54 0.39\nEvaluated CONV-AE 0.61 0.62 0.42\nEvaluated CONV-DAE 0.61 0.61 0.42\nTable 1 . Evaluation results of the feature-learning-\nparadigm-based systems.\nlearning paradigm, since the student model does not need\nadditional labeled data for training so that a cross-dataset\nvalidation is unnecessary.\n4.5 Results\nFigure 5 shows the evaluation result of E1. On average, all\nsystems tend to perform the best on ENST and the worst on\nRBMA . For some instruments, this gap can be as large as\n22% in F-measure. There are two possible reasons for the\ngood performance on ENST . First, as many ADT systems,\nincluding Segment and Classify -based and Separate and\nDetect -based, have been developed and evaluated on ENST ,\nthere could be potential bias towards this dataset. Second,\ntheENST dataset might be relatively simple compared to\nthe others. A closer examination of the dataset shows a lack\nof singing voices and the dominance of MIDI synthesized\naccompaniments, which could potentially over-simplify the\nADT problem. The relative poor performance on the RBMA\ndataset might be related to its focus on EDM; the electronic\ndrum sounds with strong audio effects could possibly in-\ncrease the difﬁculty for ADT. This seems to be especially\ntrue in case of the SD. Overall, the results show that the\nevaluated systems leave much room for optimization; since\nmany of the parameters in these systems are not extensively\ntuned, this result is to be expected. However, this also re-\nﬂects the challenge of building an ADT system that is easily\ngeneralizable.\nThe results of E2are shown in Table 1. The following\ntrends can be observed: ﬁrst, the unlabeled data seems to be\nhelpful in Segment and Classify -based ADT systems. A di-\nrect comparison between CONV-AE and MFCC shows that\nthe features learned from unlabeled data seem to slightly im-\nprove for SD while achieving equal performance on HH and\nBD. Second, the unsupervised training process is useful for\nderiving better features. Compared to CONV-RANDOM,\nboth CONV-AE and CONV-DAE show improvements on\nnearly all instruments, indicating the advantage of the train-\ning process. Third, the DAE-inspired training process does\nnot lead to improvements for ADT. This is shown by the\nalmost equivalent results from CONV-AE and CONV-DAE.\nSince HPSS also introduces artifacts, it might not be the\nmost ideal method for this task; experimentation with other\nsource separation algorithms might provide more insights.\nTable 3 shows the results of E3. The general trends\ncan be summarized as follows: ﬁrst, the student-teacher\nlearning seems to be useful for Separate and Detect -based\nADT systems as all students show a noticeable improve-\nment on HH over the teacher models. This observationProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 449Compared SystemsInst. ParadigmImprovement Deterioration\nTest Ref # Files F-measure Gain # Files F-measure Loss\nCONV-AE MFCC SD Feature Learning 70/137 6.5% 40/137 -4.6%\nFC-200 PFNMF (SMT) HH S-T Learning 78/137 13.8% 44/137 -7.6%\nTable 2 . Signiﬁcance check of the most improved pair from each paradigm.\nExperiments Averaged F-measure\nRole System HH BD SD\nTeacher PFNMF (SMT) 0.47 0.61 0.45\nTeacher PFNMF (200D) 0.47 0.67 0.40\nStudent FC-200 0.56 0.57 0.44\nStudent FC-ALL 0.53 0.59 0.42\nStudent FC-ALL (ALT) 0.55 0.58 0.44\nTable 3 . Evaluation results of the student-teacher-paradigm-\nbased systems. The performance of the teacher models are\nthe baseline.\nconsolidates the preliminary ﬁnding reported in [37]. Sec-\nond, more unlabeled data do not necessarily lead to better\nresults. For example, FC-200 and FC-ALL (ALT) both out-\nperform FC-ALL on HH and SD. Since the student model is\na simple feed-forward DNN, the lack of model capacity and\ntemporal information could limit its potential for further\nimprovement as the data size grows. Experiments using\nother student models (e.g., CNNs and RNNs) are neces-\nsary for conﬁrmation. Third, the student models seem to\nstruggle on BD. A detailed examination on the individual\nresults from each dataset shows that teachers and students\nare mostly comparable on BD except for RBMA . This is\npossibly due to the challenging nature of RBMA as dis-\ncussed in E1. However, further investigation is needed\nbefore drawing conclusions.\nThe results of E2andE3show that feature learning and\nstudent-teacher learning paradigms are able to improve the\nperformance on SD and HH, respectively. In light of these\nresults, an interesting question is: “Are these improvements\nsigniﬁcant?” In an attempt to answer this question, two\npairs of systems are selected for further analysis. Each\npair consists of the best baseline and the best evaluated\nsystem of each paradigm. A t-test is performed on each\npair by comparing their results on all 137 ﬁles. Both pairs\nhave shown signiﬁcant improvements with p\u001c0:0014\nfor both t-tests. Furthermore, the number of improved\nand deteriorated ﬁles is calculated. The results, shown in\nTable 2, show a positive trend: the number of improved ﬁles\nis, in both cases, greater than the number of deteriorated\nﬁles. Moreover, the averaged F-measure gains are also\nhigher than the averaged F-measure loss for both pairs.\nFrom Table 2, it can be observed that the improvements\non HH from the student-teacher learning paradigm seems\nto be more substantial. To further investigate the cause\nof this improvement, one example from the ENST dataset,\nwhich has the largest F-measure gain among all ﬁles, is\nselected. The HH activation functions generated from both\nteacher and student model are shown in Fig. 6. Compared\nto the teacher’s activation function, the student’s activation\n0 200 400 600 800 10000.00.10.20.30.40.50.6Normalized ActivityHH Activation Function of (Top) Teacher (Bottom) Student\n0 200 400 600 800 1000\nBlock Index0.00.10.20.30.40.50.6Normalized ActivityFigure 6 . Example of the (top) teacher’s and (bottom)\nstudent’s HH activation function in comparison.\nfunction is sharper and less noisy, demonstrating the beneﬁt\nof this paradigm.\n5. CONCLUSION\nWe discussed the data challenge in ADT and investigated\ntwo approaches to address this challenge by considering\nboth labeled and unlabeled data. First, we compared sys-\ntem performance on multiple existing labeled datasets in an\nuniﬁed setting. The results indicate a potential bias of rely-\ning on one dataset and highlight the necessity of including\nmore datasets in the future ADT evaluation. Furthermore,\nwe evaluated the usefulness of unlabeled data for two major\ntypes of ADT systems via two different learning paradigms,\nfeature learning and the student-teacher learning approach.\nFor both paradigms, we got encouraging (and statistically\nsigniﬁcant) results demonstrating the potential of achieving\nbetter performance than the baseline systems on different\ndrum instruments.\nThese results, while suggesting the need for additional\nlabeled data in the ﬁeld of ADT, also encourage the ex-\nploration of incorporating unlabeled data in the training.\nPossible future directions include (i) the evaluation of var-\nious methods for unsupervised feature learning such as\nSparse Coding [22] and Deep Belief Networks [13], (ii) the\nevaluation of different combinations of teacher and student\nmodels, for example, the combination of different types of\nDNN either as teachers or students; the identiﬁcation of suit-\nable architectures for these roles could also be an interesting\ndirection, and (iii) the application of outlier detection [16]\napproaches to ﬁlter out noisy unlabeled data.450 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20186. REFERENCES\n[1]Mart ´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng\nChen, Andy Davis, Jeffrey Dean, Matthieu Devin, San-\njay Ghemawat, Geoffrey Irving, Michael Isard, Man-\njunath Kudlur, Josh Levenberg, Rajat Monga, Sherry\nMoore, Derek G Murray, Benoit Steiner, Paul Tucker,\nVijay Vasudevan, Pete Warden, Martin Wicke, Yuan\nYu, Xiaoqiang Zheng, and Google Brain. TensorFlow:\nA System for Large-Scale Machine Learning Tensor-\nFlow: A system for large-scale machine learning. In\nProc. of USENIX Symp. on Operating Systems Design\nand Implementation (OSDI) , pages 265–284, 2016.\n[2]Emmanouil Benetos, Simon Dixon, Dimitrios Gian-\nnoulis, Holger Kirchhoff, and Anssi Klapuri. Automatic\nmusic transcription: challenges and future directions.\nJournal of Intelligent Information Systems , jul 2013.\n[3]Rachel Bittner, Justin Salamon, Mike Tierney, Matthias\nMauch, Chris Cannam, and Juan Bello. MedleyDB:\na multitrack dataset for annotation-intensive MIR re-\nsearch. In Proc. of the International Society for Music\nInformation Retrieval Conference (ISMIR) , 2014.\n[4]Keunwoo Choi, Gy ¨orgy Fazekas, Mark Sandler, and\nKyunghyun Cho. Transfer learning for music classiﬁca-\ntion and regression tasks. In Proc. of the International\nSociety for Music Information Retrieval Conference (IS-\nMIR) , 2017.\n[5]Jia Cui, Brian Kingsbury, Bhuvana Ramabhadran,\nGeorge Saon, Tom Sercu, Kartik Audhkhasi, Abhinav\nSethy, Markus Nussbaum-Thom, and Andrew Rosen-\nberg. Knowledge Distillation Across Ensembles of Mul-\ntiplingual Models for Low-resource Languages. In Proc.\nof the International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , pages 4825–4829,\n2017.\n[6]Christian Dittmar and Daniel G ¨artner. Real-time tran-\nscription and separation of drum recordings based on\nNMF decomposition. In Proc. of the International Con-\nference on Digital Audio Effects (DAFx) , pages 187–\n194, Erlangen, Germany, September 2014.\n[7]Georgi Dzhambazov. Towards a drum transcription sys-\ntem aware of bar position. In Proc. Audio Engineering\nSociety Conference on Semantic Audio (AES) , London,\nUK, Jan 2014.\n[8]Derry Fitzgerald. Harmonic / Percussive Separation\nUsing Median Filtering. In Proc. of International Con-\nference on Digital Audio Effects (DAFx) , 2010.\n[9]Derry FitzGerald and Jouni Paulus. Unpitched percus-\nsion transcription. In Signal Processing Methods for\nMusic Transcription , pages 131–162. Springer, 2006.\n[10] Nicolai Gajhede, Oliver Beck, and Hendrik Purwins.\nConvolutional Neural Networks with Batch Normaliza-\ntion for Classifying Hi-hat, Snare, and Bass PercussionSound Samples. In Proc. of the Audio Mostly , pages\n111–115, 2016.\n[11] Olivier Gillet and Ga ¨el Richard. Enst-drums: an exten-\nsive audio-visual database for drum signals processing.\nInProc. of the International Society for Music Informa-\ntion Retrieval Conference (ISMIR) , 2006.\n[12] Olivier Gillet and Ga ¨el Richard. Transcription and sep-\naration of drum signals from polyphonic music. IEEE\nTransactions on Audio, Speech and Language Process-\ning, 16(3):529–540, 2008.\n[13] Philippe Hamel and Douglas Eck. Learning Features\nfrom Music Audio with Deep Belief Networks. In Proc.\nof International Society for Music Information Retrieval\nConference (ISMIR) , pages 339–344, 2010.\n[14] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\nDistilling the Knowledge in a Neural Network.\narXiv:1503.02531 , pages 1–9, 2015.\n[15] Jinyu Li, Rui Zhao, Jui Ting Huang, and Yifan Gong.\nLearning small-size DNN with output-distribution-\nbased criteria. In Proc. of the Conference of the Inter-\nnational Speech Communication Association (INTER-\nSPEECH) , pages 1910–1914, 2014.\n[16] Yen-Cheng Lu, Chih-Wei Wu, Chang-Tien Lu, and\nAlexander Lerch. Automatic outlier detection in mu-\nsic genre datasets. In Proc. of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 101–107, 2016.\n[17] Jonathan Masci, Ueli Meier, Dan Cirean, and J ¨urgen\nSchmidhuber. Stacked convolutional auto-encoders for\nhierarchical feature extraction. In Proc. of International\nConference on Artiﬁcial Neural Networks (ICANN) ,\n2011.\n[18] Brian Mcfee, Eric J Humphrey, and Juan P Bello. A\nsoftware framework for musical data augmentation. In\nProc. of the International Society for Music Information\nRetrieval Conference (ISMIR) , pages 248–254, 2015.\n[19] Jouni Paulus. Signal Processing Methods for Drum\nTranscription and Music Structure Analysis . PhD thesis,\nTampere University of Technology, Tampere, Finland,\n2009.\n[20] Jouni Paulus and Anssi Klapuri. Drum sound detec-\ntion in polyphonic music with hidden markov models.\nEURASIP Journal on Audio, Speech, and Music Pro-\ncessing , (14), 2009.\n[21] Colin Raffel, Brian Mcfee, Eric J. Humphrey, Justin\nSalamon, Oriol Nieto, Dawen Liang, and Daniel P. W.\nEllis. mir eval: A Transparent Implementation of Com-\nmon MIR Metrics. Proc. of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 367–372, 2014.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 451[22] Rajat Raina, Alexis Battle, Honglak Lee, Benjamin\nPacker, and Andrew Y Ng. Self-taught learning: trans-\nfer learning from unlabeled data. In Proc. of the Interna-\ntional Conference on Machine Learning (ICML) , pages\n759–766, 2007.\n[23] Axel Roebel, Jordi Pons, Marco Liuni, and Mathieu\nLagrange. On Automatic Drum Transcription Using\nNon-Negative Matrix Deconvolution and Itakura Saito\nDivergence. In Proc. of the International Conference\non Acoustics Speech and Signal Processing (ICASSP) ,\n2015.\n[24] Carl Southall, Ryan Stables, and Jason Hockman. Au-\ntomatic Drum Transcription Using Bi-Directional Re-\ncurrent Neural Networks. In Proc. of the International\nSociety for Music Information Retrieval Conference (IS-\nMIR) , 2016.\n[25] Carl Southall, Ryan Stables, and Jason Hockman. Au-\ntomatic drum transcription for polyphonic recordings\nusing soft attention mechanisms and convolutional neu-\nral networks. In Proc. of the International Society for\nMusic Information Retrieval Conference (ISMIR) , pages\n606–612, 2017.\n[26] Carl Southall, Chih-Wei Wu, Alexander Lerch, and\nJason Hockman. MDB DRUMS - an annotated sub-\nset of medleydb for automatic drum transcription. In\nProc. of the International Society for Music Information\nRetrieval Conference (ISMIR)(Late-breaking Demo) ,\n2017.\n[27] Vin´ıcius M. A. Souza, Gustavo E. A. P. A. Batista, and\nNilson E. Souza-Filho. Automatic classiﬁcation of drum\nsounds with indeﬁnite pitch. In Proc. of the Interna-\ntional Joint Conference on Neural Networks (IJCNN) ,\npages 1–8, Killarney, Ireland, Jul 2015.\n[28] Dirk Van Steelant, Koen Tanghe, Sven Degroeve,\nBernard De Baets, Marc Leman, and Jean-Pierre\nMartens. Support vector machines for bass and snare\ndrum recognition. In Classiﬁcation – the Ubiquitous\nChallenge , pages 616–623. Springer, 2005.\n[29] Lucas Thompson, Matthias Mauch, and Simon Dixon.\nDrum Transcription via Classiﬁcation of Bar-Level\nRhythmic Patterns. In Proc. of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2014.\n[30] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. Extracting and composingrobust features with denoising autoencoders. In Proc.\nof the International Conference on Machine Learning\n(ICML) , 2008.\n[31] Richard. V ogl, Matthias Dorfer, and Peter Knees. Re-\ncurrent Neural Networks for Drum Transcription. In\nProc. of the International Society for Music Information\nRetrieval Conference (ISMIR) , pages 730–736, 2016.\n[32] Richard V ogl, Matthias Dorfer, and Peter Knees. Drum\nTranscription From Polyphonic Music With Recurrent\nNeural Networks. In Proc. of the International Con-\nference on Acoustics Speech and Signal Processing\n(ICASSP) , pages 201–205, 2017.\n[33] Richard V ogl, Matthias Dorfer, Gerhard Widmer, and\nPeter Knees. Drum Transcription Via Joint Beat and\nDrum Modeling Using Convolutional Recurrent Neural\nNetworks. In Proc. of the International Society for Mu-\nsic Information Retrieval Conference (ISMIR) , pages\n150–157, 2017.\n[34] Shinji Watanabe, Takaaki Hori, Jonathan L. Roux, and\nJohn R. Hershey. Student-Teacher Network Learning\nwith Enhanced Features. In Proc. of the International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP) , pages 5275–5279, 2017.\n[35] Chih-Wei Wu and Alexander Lerch. Drum transcription\nusing partially ﬁxed non-negative matrix factorization\nwith template adaptation. In Proc. of the International\nSociety for Music Information Retrieval Conference\n(ISMIR) , pages 257–263, 2015.\n[36] Chih-Wei Wu and Alexander Lerch. On drum playing\ntechnique detection in polyphonic mixtures. In Proc. of\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 218–224, 2016.\n[37] Chih-Wei Wu and Alexander Lerch. Automatic\ndrum transcription using the student-teacher learning\nparadigm with unlabeled music data. In Proc. of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 613–620, 2017.\n[38] Kazuyoshi Yoshii, Masataka Goto, and Hiroshi G.\nOkuno. Drum sound recognition for polyphonic audio\nsignals by adaptation and matching of spectrogram tem-\nplates with harmonic structure suppression. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n15(1):333–345, 2007.452 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "GuitarSet: A Dataset for Guitar Transcription.",
        "author": [
            "Qingyang Xi",
            "Rachel M. Bittner",
            "Johan Pauwels",
            "Xuzhou Ye",
            "Juan Pablo Bello"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.12674910",
        "url": "https://doi.org/10.5281/zenodo.12674910",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/188_Paper.pdf",
        "abstract": "This dataset is used for additional experiments for our work: Distortion Recovery: A Two-Stage Method for Guitar Effect Removal, published at DAFx 2024.We randomly rendered dry (direct input) guitar tracks from the EGDB datasetusing BIAS FX2 and fine-tuned both our model and Demucs V3 on thisrendered dataset.\nThe EGDB dataset consists of 240 tracks (~1.5 hours) and was randomly split by tracks into training, validation, and test sets, following an 8/1/1 split. Our model was then compared with Demucs V3. During both the training and testing phases, each track was randomly segmented into 4-second clips.\nFor reproducibility, the segmented clips for testing, including wet/dry signal, results of our model and Demucs V3, are available here. The test code for evaluating the following metrics is available at thisGitHub repository.",
        "zenodo_id": 12674910,
        "dblp_key": "conf/ismir/XiBPYB18",
        "keywords": [
            "EGDB dataset",
            "240 tracks",
            "8/1/1 split",
            "4-second clips",
            "Demucs V3 comparison",
            "Reproducibility",
            "GitHub repository",
            "Metrics evaluation",
            "Random rendering",
            "Fine-tuning"
        ],
        "content": "GUITARSET: A DATASET FOR GUITAR TRANSCRIPTION\nQingyang Xi1Rachel M. Bittner1Johan Pauwels2\nXuzhou Ye1Juan P. Bello1\n1Music and Audio Research Lab, New York University, USA\n2Center for Digital Music, Queen Mary University of London, UK\ntom.xi@nyu.edu\nABSTRACT\nThe guitar is a popular instrument for a variety of reasons,\nincluding its ability to produce polyphonic sound and its\nmusical versatility. The resulting variability of sounds,\nhowever, poses signiﬁcant challenges to automated meth-\nods for analyzing guitar recordings. As data driven meth-\nods become increasingly popular for difﬁcult problems\nlike guitar transcription, sets of labeled audio data are\nhighly valuable resources. In this paper we present Gui-\ntarSet, a dataset that provides high quality guitar record-\nings alongside rich annotations and metadata. In partic-\nular, by recording guitars using a hexaphonic pickup, we\nare able to not only provide recordings of the individual\nstrings but also to largely automate the expensive annota-\ntion process. The dataset contains recordings of a vari-\nety of musical excerpts played on an acoustic guitar, along\nwith time-aligned annotations of string and fret positions,\nchords, beats, downbeats, and playing style. We conclude\nwith an analysis of new challenges presented by this data,\nand see that it is interesting for a wide variety of tasks\nin addition to guitar transcription, including performance\nanalysis, beat/downbeat tracking, and chord estimation.\n1. INTRODUCTION\nWell-annotated audio ﬁles are key to MIR research. They\nare necessary both for evaluating algorithm performance\nand for developing models. For time-varying musical in-\nformation such as notes in a polyphonic context, the pro-\ncess of creating accurate annotations can be an especially\ndifﬁcult and slow process. For monophonic audio, there\nare software tools, such as Tony [12], built to facilitate the\nmanual annotation process by ﬁrst providing an estimate\nand allowing the user to manually correct the mistakes.\nHowever, there is no equivalent tool for polyphonic au-\ndio, and the accuracy of pitch estimation methods on poly-\nphonic audio is signiﬁcantly worse than for monophonic\naudio.\nc\rQingyang Xi, Rachel Bittner, Johan Pauwels, Xuzhou\nYe, Juan Bello. Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: Qingyang Xi, Rachel\nBittner, Johan Pauwels, Xuzhou Ye, Juan Bello. “GuitarSet: A Dataset\nfor Guitar Transcription”, 19th International Society for Music Informa-\ntion Retrieval Conference, Paris, France, 2018.Recently, several methods have been developed to ad-\ndress the problem of creating pitch annotations. Most no-\ntably, Su and Yang’s work [22] provides an efﬁcient way\nof generating note-level annotations for recordings of poly-\nphonic music by utilizing the midi-keyboard as an annota-\ntion interface. An alternative approach was proposed that\nuses an analysis-synthesis framework to generate annota-\ntions by re-synthesizing estimates [18]. However, these\nmethods are insufﬁcient when applied to guitar record-\nings. In the midi-keyboard approach, it would be very dif-\nﬁcult for a keyboard player to replicate note-by-note what\na guitarist is playing. The analysis-synthesis approach re-\nquires the analysis (i.e. estimate of the correct notes) to\nbe reasonably close to the ground truth in order to gen-\nerate realistic sounding audio; unfortunately, the existing\ntranscription algorithms perform woefully badly on poly-\nphonic solo guitar recordings. Unsurprisingly, there is no\nsizable database of guitar recordings with note-level anno-\ntations and realistic guitar playing.\nIn this paper, we present GuitarSet: a sizable dataset of\nrichly annotated, realistic guitar recordings. We describe\nour data collection and annotation process in detail and in-\ntroduce our solution for efﬁciently creating note-level an-\nnotations. Our solution relies on the use of an acoustic\nguitar with a hexaphonic pickup , which outputs one chan-\nnel of audio signal per guitar string; as well as custom\nannotation tools. This effectively turns polyphonic tran-\nscription into monophonic transcription. We conclude with\nan analysis of new challenges presented by this data, and\nsee that it is interesting for a wide variety of tasks in ad-\ndition to guitar transcription, including performance anal-\nysis, beat/downbeat tracking, and chord estimation. The\ndataset (audio and annotations) and the code used to gen-\nerate the annotations are made freely available online.1\n2. RELATED WORK\nA handful of datasets exist for polyphonic instrument tran-\nscription. The MAPS dataset [7] contains a large collec-\ntion of transcribed piano notes, chords, and pieces (us-\ning a Disklavier), recorded in different acoustic conditions.\nSimilarly, the UMA-Piano [2] dataset contains all possible\ncombinations of notes at varying dynamics. These datasets\nhave been critical to the development of automated pi-\nano transcription methods; Sigtia’s deep-learning powered\n1https://github.com/marl/GuitarSet453piano transcription algorithm [20] and Ewert’s algorithm\nbased on non-negative matrix deconvolution [8] are just\ntwo of many data driven algorithms that rely on the MAPS\ndataset. More recently, efforts devoted to historic preser-\nvation of player piano rolls also provide new ways of ex-\ntending transcription datasets for piano music [19].\nFor guitar, the Guitar Playing Techniques dataset [23]\ncontains 6580 clips of single notes along with playing\ntechnique annotations. The IDMT-SMT-Audio-Effects\ndataset [21] contains \u001920hours of single guitar notes\nand chords with varying audio effects. Finally, the IDMT-\nSMT-Guitar dataset [11] contains several types of guitar\ndata, including single notes, playing techniques, note clus-\nters, and note and chord-level annotations for short ex-\ncerpts. While each of these datasets are useful, none of\nthem provide note-level annotations of realistic polyphonic\nguitar pieces, which is a limiting factor in exploring many\ninteresting new research directions.\nThe absence of a sizable dataset for realistic polyphonic\nguitar playing is largely due to the difﬁculty of annotating\ncomplex guitar recordings directly. In order to help facili-\ntate analysis of guitar recordings, hexaphonic guitar pick-\nups have become a useful research tool. The idea of us-\ning hexaphonic pickups to generate transcriptions was ﬁrst\nproposed by O’Grady and Rickard in 2009 [16]. In their\nmethod, signals from individual strings are analyzed using\nsupervised non-negative matrix factorization. Hexaphonic\npickups have also been used for analysis and resynthesis of\nmonophonic single-note guitar recordings [15], as well as\nfor visualizing guitar performances [1].\nWe posit that, despite piano and guitar having compara-\nble popularity, research has focused much more heavily on\nanalysis of piano recordings simply because of the avail-\nability of data. Online communities that provide guitar tab-\nlature such as Ultimate Guitar2are very popular, and accu-\nrate methods for guitar tablature transcription would have\nthe potential to attract a vibrant community. By creating\nGuitarSet, and therefore demonstrating an efﬁcient process\nof creating detailed note level annotations for guitar, we\nhope to provide the community with better resources for\nstudying guitar transcription and more.\nThe collection and analysis methods for GuitarSet was\ndesigned with the principles described by Su and Yang [22]\nin mind: (1) Generality : We chose well-known progres-\nsions in popular styles as the basis of GuitarSet’s mate-\nrial, and collect realistic, complex and polyphonic musi-\ncal phrases. (2) Efﬁciency : The method of creating an-\nnotations for GuitarSet is mostly automated, with human\nexperts focusing on correcting onsets, which requires con-\ntext and expertise. GuitarSet can be easily extended for\nthis reason. (3) Cost : The key equipment, the hexaphonic\npickup, is very affordable. and (4) Quality : In order to\npreserve nuances in the performance, including intra-note\npitch deviations and inter-string onset-time patterns, we\ncraft special tools and provide multiple annotation formats\nto ensure high quality annotations.\n2http://www.ultimate-guitar.com/3. DATA COLLECTION PROCESS\nHexaphonic pickups are magnetic pickups that have in-\ndividual outputs for each magnet. We ordered a clip-on\nhexaphonic pickup from ubertar.com , which has 6 in-\ndividual single coil magnets, and is manually attached to\nan acoustic guitar. For better pickup signal-to-noise ratio\n(SNR), nickel wound steel strings are used for the acoustic\nguitar.\nThe audio was recorded in a small, soundproof record-\ning studio with minimal reverberation. In addition to the\nsix channels from the hexaphonic pickup, we also record\nthe guitar using a Neumann U87 condenser microphone,\nplaced\u001930 cm in front of the 18th fret of the guitar. This\nresults in seven channels of audio overall.\nSix experienced guitarists were recruited to record for\nthis database. All six players have more than 10 years of\nguitar playing experience, and were recruited by the au-\nthors. The guitarists were asked to play 30 twelve to six-\nteen bar excerpts from lead-sheets in a variety of keys, tem-\npos, and musical genres, described in Section 4. During\nrecording, guitarists were provided with a backing track\nthat consisted of a click track, drum set, and bass line,\nheard through monitoring headphones. For each excerpt,\nplayers were asked to comp (play chords), and then to solo\nover their own comping. The guitarists were allowed to\nreplay excerpts until they were aesthetically satisﬁed with\ntheir performance.\n4. DATASET OVERVIEW\nWe use the JAMS ﬁle format [10] to store the rich collec-\ntion of annotations for this dataset. For each recording,\nthe JAMS ﬁle contains annotations for tempo, key, and\nstyle (metadata); beats and downbeats (inferred from the\nclick track); instructed chords (from the lead-sheets); per-\nformed chords (via automatic estimation); note-level tran-\nscription, including string and fret position (via automatic\nestimation), onsets (via annotation), offsets (via automatic\nestimation) and pitch contour for each note (via validated\nautomatic estimation). Descriptions of each of these an-\nnotation types are detailed in Section 5. Figure 1 gives a\nvisualization of some of the annotations provided for an\nexcerpt of the dataset.\nIn total, each player provided 30.47 minutes of musical\nmaterial, resulting in just over 3 hours of content in to-\ntal. Each player was asked to play 30 excerpts, organized\nas follows: 3 different chord progressions are paired with\neach of the 5 different genres, all recorded at two differ-\nent tempi: slow and fast. The three progressions were the\n12 bar blues, Autumn Leaves, and Pachelbel’s Canon. The\nﬁve different genres were Rock, Jazz, Funk, Bossa Nova\n(BN), and Singer-Songwriter (SS). In order to broaden the\nchord gamut in GuitarSet, key signatures were indepen-\ndently assigned to each of the 30 excerpts.\n5. ANNOTATION METHODS\nThe hexaphonic recordings are analyzed to generate anno-\ntations for each string individually, and a complete tran-454 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Figure 1 . A 5 second excerpt of Jazz comping. Downbeats\nand beats are indicated with solid and dashed vertical lines\nrespectively. (Top) Played chords and pitch contours, col-\nored by string. (Bottom) Instructed chords (lead sheet) and\nstring/fret positions.\nscription of the excerpt is generated by aggregation. For\neach string, onset/offset time pairs along with continuous\npitch tracks are annotated semi-automatically, with manual\nvalidation. The validated transcriptions are then used to au-\ntomatically create derivative annotations, including chords,\nstring and fret number, and more.\nWe ﬁrst pre-process the hexaphonic recordings using\nthe KAMIR bleed removal algorithm [17] to reduce noise\npicked up by the single coil magnets from adjacent strings.\nWe then generate a rough note-level transcription by run-\nning pYIN-note [13] over the recording of each string; this\nrough transcription is used as the starting point for manual\nvalidation.\n5.1 Note-Level Annotations\nWe focus our manual annotation efforts on creating note-\nlevel annotations, such as the one shown in Figure 1 (Top).\nAccurately creating or correcting annotations of individual\nstring recordings requires contextual information and mu-\nsical expertise. For example, an intentionally muted string\nin a full chord still produces a clear pitch and onset when\nexamining the single muted string in isolation. However,\nwhen mixed together with the other more resonant strings,\nthe muted note is completely masked. Because the muted\nnote is neither intended by performer nor heard by listen-\ners, we chose not to annotate it.\nIn order to address this issue efﬁciently and maximize\nautomation, we simplify the problem by taking a compo-\nnent approach, and determine the onsets, offsets and pitch\ntracks sequentially. We ﬁrst focus on generating high qual-\nity onset annotations.By manually validating the onsets,\nmuted notes that shouldn’t be included in the annotationand other non-note events are left out of the annotation.\nOffsets are then automatically estimated, and the result-\ning note regions are used to facilitate highly accurate pitch\ntrack estimations.\n5.1.1 Onsets\nGiven automatically estimated onsets, removing false posi-\ntive onsets can efﬁciently be done manually, but accurately\nadding missed onsets efﬁciently requires machine assis-\ntance. In order to allow annotators to easily add missed on-\nsets, we automatically adjust human-estimated onset times\nby searching for the most likely spectral ﬂux peak in a lo-\ncal neighborhood. Concretely, for a human estimated onset\ntime ~a, the true onset time ais determined by ﬁnding the\nposition for which the windowed onset strength function\nGa(t)is maximized.\nLetE(t)be the root-mean-squared (RMS) energy cal-\nculated at time t, andNa(t)be the spectral ﬂux novelty\nfunction at time t[3]:\nNa(t) =n=2X\nk=1H(jX(t;k)j\u0000jX(t\u0000l;k)j) (1)\nwhereH(x) =x+jxj\n2is the half-wave rectiﬁcation func-\ntion andl= 5:8ms is a constant lag in time, nis the num-\nber of analysis bins, and kis the bin index.\nThe windowed onset strength function Ga(t)is con-\nstructed as follows,\nGa(t) =E(t)\u0003Na(t)\u0003N(~a; \u001b2) (2)\nand the onset time is computed as\na= arg max\nt(Ga(t)); (3)\nwheret2[max(aprev+\u001ca;a\u00003\u001b);a+ 3\u001b],\u001ca= 50 ms\nand\u001b= 30 ms. The lower limit on tensures there are at\nleast\u001caseconds between consecutive onsets. The Gaus-\nsian component in Ga(t)ensures the locality of the on-\nset search, favoring proximity with the human estimate.\nFigure 2 shows an instance of such an adjustment.\n5.1.2 Offsets\nFor all onsets a, the corresponding offset bis estimated\nautomatically, using the following criteria. First the offset\nnoveltyNb(t)is modiﬁed slightly from Equation 1:\nNb(t) =\u0000n=2X\nk=1H0(jX(t;k)j\u0000jX(t\u0000l;k)j)(4)\nwhereH0(x) =x\u0000jxj\n2is the negative half wave rectiﬁca-\ntion function and l= 5:8ms.\nUsing the generated offset novelty function, an offset\nstrength function Gb(t)is generated.\nGb(t) =Nb(t)\u0003(logE(t\u0000l)\u0000logE(t))\nE(t)(5)\nwheret2[a+\u001cb;anext)and\u001cb= 30 ms.l= 5:8ms is\nthe hop length in time of the analysis window.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 455Figure 2 . (Top) Waveform of a single note. (Upper Mid-\ndle) Human estimated onsets, adjusted by examining the\nonset strength function. (Lower Middle) Offset novelty\nfunctions and the detected offset in black. (Bottom) Gb(t)\nand detected peaks.\nThe intuition behind the offset strength function Gb(t)\nis straightforward: log-RMS difference and spectral ﬂux\nboth give peaks for potential offsets, the RMS in the de-\nnominator penalizes peaks of Gb(t)that still have signif-\nicant energy. The peaks of the offset strength function\nGb(t)are then thresholded to generate the offset candi-\ndates as shown in Figure 2. Offset candidates within 30\nms of the onset aare discarded, and the ﬁrst offset candi-\ndate in time is then chosen from the remaining offsets as\nb.\n5.1.3 Pitch\nAfter onsets and offsets are determined, the pitch tracks of\nvoiced regions are then estimated using pYin. The result-\ning estimation is then cleaned by the ﬁrst author in Tony,\nmostly correcting octave mistakes.\nWhile we annotated the continuous pitch trajectories of\neach note, the overall center pitches still needs to be in-\nferred. We choose a simple heuristic that averages the pitch\ntrack frequencies. For a note with onset at time aand as-\nsociated pitch track f(t);t2[a;b); the center pitch of the\nnotepis estimated by taking the average pitch track over\na subset of the note region t2[a0;b0), wherea0andb0are\n25% and 50% of the note duration respectively:\np=1\nb0\u0000a0b0X\nt=a0f(t) (6)\nWe only consider the subset t2[a0;b0)to ensure a percep-\ntually relevant average pitch, since the pitch near the onset\nand offset of a guitar note can sometimes be unstable (e.g.\nsee Figure 1).5.2 Derivative Annotations\nGiven note-level annotations, the lead sheets and the click\ntrack, we automatically generate a series of derivative an-\nnotations.\n5.2.1 String and Fret Position\nSince the tuning of the guitar is known at the time of data\ncollection, fret positions can be determined simply by ﬁnd-\ning the difference in semitones between the annotated pitch\nand the pitch of the open string. A visualization of these\nannotations is shown in Figure 1 (Bottom).\n5.2.2 Chords\nTwo different types of chord annotations accompany each\nof the 180 excerpts. The ﬁrst type of chord annotation\nis the chord written in the lead sheet that is provided to\nthe guitar players at the time of data collection. How-\never, in order to better ﬁt the given genre, the players of-\nten modiﬁed the given chords, hereafter called instructed\nchords . Therefore the performed chords are not necessar-\nily the same as the instructed chords. Because the backing\ntrack contains a bass line that is aligned to the root and\nthe timing of the instructed chords, the instructed and per-\nformed chords vary mostly in chord type, not root. The\ninstructed chords have only four types (major, minor, dom-\ninant seventh, half-diminished seventh); speciﬁc voicings,\nextensions and alterations could be freely determined by\nthe players without suggestion bias.\nWe infer the performed chords by combining informa-\ntion from the lead sheet and the annotated notes. In order to\nmake the comparison between the chords as instructed by\nthe lead sheet and the actual performed chords straightfor-\nward, the chord segmentation is determined from the lead\nsheets. A drawback of this approach is that anticipated or\nlagging chords changes lead to a slight mismatch between\nthe audio signal and the annotations, which may disturb\ndata-driven methods using this data as a training set. How-\never, we argue that such quantization leads to annotations\nthat are more ﬁt for displaying as sheet music and more\nconsistent than human segmentation, which is subjective\nin this regard3. Furthermore, these cases are expected to\nbe rare because of the aforementioned backing track.\nFor each chord segment, we ﬁrst determine if a string is\nactive by verifying whether the total duration of all notes\nplayed on that string exceeds 5% of the segment duration.\nThis activity thresholding ensures that notes in adjacent\nchord segments do not accidentally cause otherwise silent\nstrings to appear active simply because of an offset in chord\nchanges between the lead sheet and recording. Next, the\npredominant note is determined for all active strings per\nsegment. This is done by taking the MIDI note value with\nthe longest total duration per-string (summed over all note\nrepetitions in the chord segment), resulting in a set of up to\nsix notes per chord segment from which we subsequently\nderive a chord label.\n3Informal experiments with symbolic chord recognition software re-\nsulted in a far worse segmentation.456 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018The root of the chord is also taken from the lead sheet\nand the inversion naturally arises from the lowest note in\nthe set. Finally, the chord type is determined from the\nchroma of the set of notes per string through a decision\ntree that is part of the open-source MusOO library4. See\nFigure 1 for examples of instructed and played chords.\nIt is notable that our approach of determining the played\nchord has several biases; namely, the boundary of each\nchord and the root of each chord is predetermined. More\nin depth investigation is needed to determine chord bound-\naries and roots purely automatically, but this is left for fu-\nture work.\n5.2.3 Beats and Downbeats\nSince the data is recorded against a click track, the tempo,\nbeats, downbeats and meter of all the excerpts are known.\nThese annotations are generated for each excerpt automat-\nically given this known metadata.\n5.3 Inferred Stroke Information\nAnother pattern that can be recognized from the annotated\ndata is the inter-string onsets. With the help of the on-\nset adjustment step, the annotation captures minute tim-\ning differences across onsets on different strings; and by\nlooking at these onset patterns, one can gain a much better\nunderstanding of the picking activity that would be other-\nwise complex to analyze. Figure 3 shows the onsets per\nstring for a short excerpt. Four different strokes can be\nclearly identiﬁed within this 650 ms excerpt. By examin-\ning the relative order of strings in each of the strokes, we\ncan clearly observe that the ﬁrst and last stroke are down-\nstrokes, and the second and third are up-strokes. Evident\nfrom Figure 3, the inter-string onsets are only milliseconds\napart during fast strokes, and would be very difﬁcult for\nhumans to manually annotate precisely. This nuanced de-\ntail would have been lost if the onset adjustment step were\nnot applied.\nFigure 3 . Onsets for each string are shown in different\ncolors.\n6. BASELINE EXPERIMENTS\nIn order to better understand the new challenges posed by\nthis dataset, we evaluate the performance of strong base-\nline algorithms against our ground truth notes, chords, and\n4https://github.com/jpauwels/libMusOObeats/downbeats. These experiments are performed with-\nout the algorithms seeing any of GuitarSet’s data. Detailed\nresults can be found in the GuitarSet repository.5All box\nplots used in this section have box edges showing the ﬁrst\nand third quartile, and the whiskers showing 1.5 interquar-\ntile range (IQR) away from the box edges.\n6.1 Notes\nWe evaluate the performance of the Deep Salience\nmultiple-f0estimation algorithm [4] on GuitarSet’s poly-\nphonic rhythmic recordings. Figure 4 shows the results\nacross different splits of the data.\nOverall, the model has an accuracy of \u001946%, and the\nmost common type of error is missed, rather than incor-\nrect, notes. Looking at Figure 4 (Top Left), the results are\nsplit by genre, and we see that Jazz is overall the most dif-\nﬁcult genre to transcribe (likely due to the more complex\nchord combinations), while Funk has the highest recall and\nlowest precision (due to short notes and more unvoiced re-\ngions). In Figure 4 (Bottom Left), we see that the audio\nfrom the pickup is easier to transcribe than the audio from\nthe microphone, likely because the pickup signal is cleaner.\nFrom Figure 4 (Top Right), we see that the performance\nvaries by player, both in terms of average accuracy and\nin terms of the variance across all the player’s recordings.\nThis suggests that each player’s technique or playing style\nis different enough that algorithm performance differs sig-\nniﬁcantly. Finally, in Figure 4 (Bottom Right), we see the\nclear trend that the faster the tempo, the more difﬁcult the\nexcerpt is to transcribe.\nFigure 4 . Baseline algorithm multiple- f0scores on differ-\nent splits of GuitarSet. The metrics are A(Accuracy), CA\n(Chroma Accuracy), P(Precision), and R(Recall). (Top\nLeft) Scores split by recording mode. (Top Right) Scores\nsplit by excerpt tempo. (Bottom Left) Scores split by\ngenre. (Bottom Right) Scores split by player.\nIf only the microphone split of the data is considered,\nwe see that the Deep Salience model performs worse on\n5https://github.com/marl/GuitarSetProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 457GuitarSet than it does on Bach10 and MedleyDB [4]. With\nthe accuracy at only \u001943%, there are still signiﬁcant pos-\nsible performance gains to be had.\n6.2 Chords\nNext, we evaluate the performance of a state-of-the-art\nchord recognition baseline [14] against the GuitarSet chord\nlabels. The results, stratiﬁed by genre, are show in\nFigure 5. First, we see that again, some genre’s chord la-\nbels are easier to estimate than others; in particular, the\nRock and Singer Songwriter genres are much easier due\nto the generally simpler chord types used in those gen-\nres compared with the others. Next, we see that there is\na large variance in the scores and that there are many out-\nliers. Upon investigating the reason for these outliers, we\ndiscovered that some popular guitar textures are not repre-\nsented in the estimation algorithm’s output space. Power\nchords and octaves, for example, are common guitar tex-\ntures that are not within the range of typical chord esti-\nmation output. While the lead sheet that guides the data\ncollection contains 42 unique chords, the actual detailed\nchord annotations had a total of 478 unique chord labels\n(counting all inversions and variations as unique), most of\nwhich were small variations of the 42 due to players adding\nor removing notes.\nAs shown in Table 1, the overall performance of the\nbaseline chord recognition algorithm on GuitarSet is com-\nparable with the dataset evaluated by Humphrey and\nBello [9]. However, as mentioned above, some strata of\nthe dataset are considerably more difﬁcult than the rest.\nFigure 5 . Chord recognition baseline algorithm results on\nGuitarSet, stratiﬁed by genre.\n6.3 Beats and Downbeats\nThe performance of a state-of-the-art beat and downbeat\ndetection algorithm [6] is evaluated on GuitarSet, and the\nresults, stratiﬁed by player, are shown in Figure 6. More so\nthan for the previous two tasks, there is a substantial dif-\nference between the beat tracker’s performance for differ-\nent players. This suggests that the guitarists have differentDataset Root 3rds Triads 7ths Tetrads\nGuitarSet\n— Instructed 0.903 0.862 0.838 0.669 0.619\n— Played 0.903 0.866 0.708 0.810 0.544\nH. & B. [9] 0.861 0.836 0.812 0.729 0.671\nTable 1 . Median weighted recall scores for the baseline\nalgorithm [14] performed on different datasets\ncharacteristics in how they play that affect beat detection,\nsuch as their choice of strumming patterns or the strength\nof their attacks. For example, player 00 has a fast strum-\nming style, and plays chords with embedded melodies,\nwhich proves difﬁcult for the algorithm.\nFigure 6 . Evaluation of baseline beat/downbeat detection\nalgorithm on GuitarSet, split by player. The metrics are\nF(F-measure), AML-t (Any Metric Level-Total), and IG\n(Information gain).\nWhile the median beat and downbeat tracking F-\nmeasure is in the 90% range for several players (which is\ntypical for state-of-the art-beat tracking [5]), several sub-\nstratas of GuitarSet are challenging for beat and downbeat\nestimation. This is especially true because the tempo and\nmeter do not change over time for each excerpt, yet the data\nis still challenging for a state-of-the-art beat and downbeat\nestimation algorithm.\n7. CONCLUSIONS\nIn this paper, we presented a large and carefully anno-\ntated dataset of guitar recordings which is available as an\nopen source resource to the research community. We gave\na detailed overview of the data collection process and a\ndescription of the data itself. Finally, we described our\nnovel process for efﬁciently and accurately creating note,\nchord, and beat annotations, and reported the performance\nof state-of-the-art algorithms on these annotations.\nWe hope GuitarSet will be useful beyond providing\ntraining and evaluation data for transcription models by\nproviding a gateway to investigate interesting problems\nsuch as stroke analysis or harmony segmentation. We are\npleased to release GuitarSet to the research community and\nhope that it will foster new, guitar-focused research.458 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20188. ACKNOWLEDGEMENTS\nJohan Pauwels has been partly funded by the UK Engi-\nneering and Physical Sciences Research Council (EPSRC)\ngrant EP/L019981/1 and by the European Unions Hori-\nzon 2020 research and innovation programme under grant\nagreement N\u000e688382.\n9. REFERENCES\n[1] I ˜nigo Angulo, Sergio Giraldo, and Rafael Ramirez.\nHexaphonic guitar transcription and visualisation. In\nProceedings of the Second International Conference\non Technologies for Music Notation and Representa-\ntion (TENOR) , 2016.\n[2] Ana M Barbancho, Isabel Barbancho, Lorenzo J\nTard ´on, and Emilio Molina. Database of Piano\nChords: An Engineering View of Harmony . Springer,\n2013.\n[3] Juan Pablo Bello, Laurent Daudet, Samer Abdallah,\nChris Duxbury, Mike Davies, and Mark B Sandler.\nA tutorial on onset detection in music signals. IEEE\nTransactions on speech and audio processing , 2005.\n[4] R.M. Bittner, B. McFee, J. Salamon, P. Li, and J.P.\nBello. Deep salience representations for f0estima-\ntion in polyphonic music. In 18th International Society\nfor Music Information Retrieval Conference , ISMIR,\n2017.\n[5] Sebastian B ¨ock, Florian Krebs, and Gerhard Widmer.\nJoint beat and downbeat tracking with recurrent neural\nnetworks. In ISMIR , 2016.\n[6] S. Durand, J. P. Bello, B. David, and G. Richard. Ro-\nbust downbeat tracking using an ensemble of convolu-\ntional networks. IEEE Transactions on Audio, Speech,\nand Language Processing , 25(1):76–89, 2017.\n[7] Valentin Emiya, Roland Badeau, and Bertrand David.\nMultipitch estimation of piano sounds using a new\nprobabilistic spectral smoothness principle. IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , 18(6):1643–1654, 2010.\n[8] Sebastian Ewert and Mark B Sandler. An augmented\nlagrangian method for piano transcription using equal\nloudness thresholding and lstm-based decoding. arXiv\npreprint arXiv:1707.00160 , 2017.\n[9] Eric J Humphrey and Juan Pablo Bello. Four timely in-\nsights on automatic chord estimation. In ISMIR , pages\n673–679, 2015.\n[10] Eric J. Humphrey, Justin Salamon, Oriol Nieto, Jon\nForsyth, Rachel M. Bittner, and Juan P. Bello. JAMS:\nA JSON annotated music speciﬁcation for reproducible\nMIR research. In International Society of Music Infor-\nmation Retrieval (ISMIR) , October 2014.[11] Christian Kehling, Jakob Abeßer, Christian Dittmar,\nand Gerald Schuller. Automatic tablature transcription\nof electric guitar recordings by estimation of score-and\ninstrument-related parameters. In DAFx , pages 219–\n226, 2014.\n[12] Matthias Mauch, Chris Cannam, Rachel Bittner,\nGeorge Fazekas, Justin Salamon, Jiajie Dai, Juan\nBello, and Simon Dixon. Computer-aided melody note\ntranscription using the tony software: Accuracy and\nefﬁciency. In 2015 International Conference on Tech-\nnologies for Music Notation and Representation , May\n2015.\n[13] Matthias Mauch and Simon Dixon. pyin: A fundamen-\ntal frequency estimator using probabilistic threshold\ndistributions. In Acoustics, Speech and Signal Process-\ning (ICASSP), 2014 IEEE International Conference on ,\npages 659–663. IEEE, 2014.\n[14] B. McFee and J.P. Bello. Structured training for large-\nvocabulary chord recognition. In 18th International\nSociety for Music Information Retrieval Conference ,\nISMIR, 2017.\n[15] Raymond Vincent Migneco. Analysis and synthesis\nof expressive guitar performance . PhD dissertation,\nDrexel University, 2012.\n[16] Paul D. O’Grady and Scott T. Rickard. Automatic\nhexaphonic and guitar transcription and using and non-\nnegative constraints. In IET Irish Signals and Systems\nConference (ISSC 2009) . IET, 2009.\n[17] Thomas Pr ¨atzlich, Rachel M. Bittner, Antoine Liutkus,\nand Meinard Muller. Kernel additive modeling for in-\nterference reduction in multi-channel music recording.\nInAcoustics, Speech and Signal Processing (ICASSP),\n2015 IEEE International Conference on , May 2015.\n[18] Justin Salamon, Rachel M Bittner, Jordi Bonada,\nJuan Jos ´e Bosch Vicente, Emilia G ´omez Guti ´errez, and\nJuan P Bello. An analysis/synthesis framework for au-\ntomatic f0 annotation of multitrack datasets. In 18th In-\nternational Society of Music Information Retrieval (IS-\nMIR) Conference , October 2017.\n[19] Zhengshan Shi, Kumaran Arul, and Julius O Smith.\nModeling and digitizing reproducing piano rolls. In\n18th International Society for Music Information Re-\ntrieval Conference , ISMIR, 2017.\n[20] Siddharth Sigtia, Emmanouil Benetos, and Simon\nDixon. An end-to-end neural network for polyphonic\npiano music transcription. IEEE/ACM Transactions on\nAudio, Speech and Language Processing (TASLP) ,\n24(5):927–939, 2016.\n[21] Michael Stein, Jakob Abeßer, Christian Dittmar, and\nGerald Schuller. Automatic detection of audio effects\nin guitar and bass recordings. In Audio Engineering\nSociety Convention 128 . Audio Engineering Society,\n2010.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 459[22] Li Su and Yi-Hsuan Yang. Escaping from the abyss\nof manual annotation: New methodology of building\npolyphonic datasets for automatic music transcription.\nInInternational Symposium on Computer Music Multi-\ndisciplinary Research , pages 309–321. Springer, 2015.\n[23] Li Su, Li-Fan Yu, and Yi-Hsuan Yang. Sparse cepstral,\nphase codes for guitar playing technique classiﬁcation.\nInISMIR , pages 9–14, 2014.460 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Part-invariant Model for Music Generation and Harmonization.",
        "author": [
            "Yujia Yan",
            "Ethan Lustig",
            "Joseph VanderStel",
            "Zhiyao Duan"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492383",
        "url": "https://doi.org/10.5281/zenodo.1492383",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/293_Paper.pdf",
        "abstract": "Automatic music generation has been gaining more attention in recent years. Existing approaches, however, are mostly ad hoc to specific rhythmic structures or instrumentation layouts, and lack music-theoretic rigor in their evaluations. In this paper, we present a neural language (music) model that tries to model symbolic multi-part music. Our model is part-invariant, i.e., it can process/generate any part (voice) of a music score consisting of an arbitrary number of parts, using a single trained model. For better incorporating structural information of pitch spaces, we use a structured embedding matrix to encode multiple aspects of a pitch into a vector representation. The generation is performed by Gibbs Sampling. Meanwhile, our model directly generates note spellings to make outputs human-readable. We performed objective (grading) and subjective (listening) evaluations by recruiting music theorists to compare the outputs of our algorithm with those of music students on the task of bassline harmonization (a traditional pedagogical task). Our experiment shows that errors of our algorithm and students are differently distributed, and the range of ratings for generated pieces overlaps with students' to varying extents for our three provided basslines. This experiment suggests some future research directions.",
        "zenodo_id": 1492383,
        "dblp_key": "conf/ismir/YanLVD18",
        "keywords": [
            "Automatic music generation",
            "symbolic multi-part music",
            "neural language model",
            "part-invariant model",
            "structured embedding matrix",
            "Gibbs Sampling",
            "objective evaluations",
            "subjective evaluations",
            "music theorists",
            "students"
        ],
        "content": "PART-INVARIANT MODEL FOR MUSIC GENERATION AND\nHARMONIZATION\nYujia Yan[, Ethan Lustig\\, Joseph VanderStel\\, Zhiyao Duan[\nElectrical and Computer Engineering[and Eastman School of Music\\, University of Rochester\nfyujia.yan, j.vanderstel, zhiyao.duan g@rochester.edu\nethan.s.lustig@gmail.com\nABSTRACT\nAutomatic music generation has been gaining more at-\ntention in recent years. Existing approaches, however, are\nmostly ad hoc to speciﬁc rhythmic structures or instrumen-\ntation layouts, and lack music-theoretic rigor in their eval-\nuations. In this paper, we present a neural language (mu-\nsic) model that tries to model symbolic multi-part music.\nOur model is part-invariant, i.e., it can process/generate\nany part (voice) of a music score consisting of an arbi-\ntrary number of parts, using a single trained model. For\nbetter incorporating structural information of pitch spaces,\nwe use a structured embedding matrix to encode multiple\naspects of a pitch into a vector representation. The gener-\nation is performed by Gibbs Sampling. Meanwhile, our\nmodel directly generates note spellings to make outputs\nhuman-readable. We performed objective (grading) and\nsubjective (listening) evaluations by recruiting music the-\norists to compare the outputs of our algorithm with those\nof music students on the task of bassline harmonization\n(a traditional pedagogical task). Our experiment shows\nthat errors of our algorithm and students are differently\ndistributed, and the range of ratings for generated pieces\noverlaps with students’ to varying extents for our three pro-\nvided basslines. This experiment suggests some future re-\nsearch directions.\n1. INTRODUCTION\nIn recent years, there has been a growing interest in auto-\nmatic music composition. Automatic music composition\nis a challenging problem, and it remains an open research\ntopic regardless of many overblown statements in the press\nsince the early days of artiﬁcial intelligence.\nApart from purely rule-based models that are difﬁcult\nto craft, log-linear models, e.g., Hidden Markov Models\n(HMM), Conditional Random Fields (CRF), and Proba-\nbilistic Context-Free Grammars (PCFG) form a set of tra-\nditional methods for sequence modeling involving discrete\nc\rYujia Yan[, Ethan Lustig\\, Joseph VanderStel\\, Zhiyao\nDuan[. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Yujia Yan[, Ethan Lustig\\, Joseph\nVanderStel\\, Zhiyao Duan[. “Part-invariant Model for Music Generation\nand Harmonization”, 19th International Society for Music Information\nRetrieval Conference, Paris, France, 2018.variables (e.g., [13] [19] [18] [20]). When used for model-\ning music, they typically model each aspect of music (e.g.,\nmelody, harmony, durations) separately, or condition one\nvariable on a small set of other variables (e.g, [1]). This is\nbecause, in music, when multiple aspects join together, the\nnumber of resulting combinations is prohibitively large,\nand the dataset is too small for learning every combina-\ntion. Moreover, over-sized probability tables make infer-\nence extremely slow. Neural network based approaches\nsolve this problem by expressing functions with a general\nhigh capacity approximator at the cost of higher computa-\ntional requirements (relative to the small factorized model,\nbut not always), less interpretability and fewer theoretic\nguarantees.\nIn [9] and [14], multi-layered LSTMs are used to model\nBach’s four-part chorales. For generation, the former uses\nGibbs sampling and the later uses greedy search. In [10],\na neural autoregressive distribution estimator is used to\nmodel the same Bach Chorales dataset, and for generation,\nauthors compare Gibbs sampling, block Gibbs sampling,\nand ancestral sampling. In [22] and [6], Generative Ad-\nversarial Networks (GAN) are used to model and generate\nmusic pieces in their MIDI piano-roll form, and for genera-\ntion, GAN based models sample the result directly without\nthe need for an iterative sampling procedure.\nHowever, most existing models, during training, adapt\nto speciﬁc music structures of the corpus being modeled.\nAs our ﬁrst attempt to extend the expressiveness of a music\nlanguage model, we wonder if there is some invariance that\ncan be exploited to obtain better generality. It is commonly\nbelieved that Bach wrote his chorale harmonizations by\nﬁrstly writing out basslines for given melodies and then\nﬁlling in inner voices (Alto, Tenor) [15]. Also, rules for\neach part (voice) share much in common, for example, a\nsingle part tends to move in the reverse direction after a\nleap. This motivated our idea of treating parts as the basic\nentity to model.\nIn this paper, we propose a part-invariant model for\nmulti-part music1. Our generation framework follows the\nMarkov Blanket formalism used in DeepBach [9]. Our\nmodel is a part-based model. As a basic consideration of\ncounterpoint, each part should be in a good shape by it-\nself, and when multiple parts are put together, the resulting\n1Supplementary materials and some generation examples can be\nfound at http://www.ece.rochester.edu/projects/air/\nprojects/model0.html204aggregated sonority should be good. By part-invariance ,\nwe mean that the structure of our model explicitly cap-\ntures the relationship among notes within every single part,\nand we share this structure with all parts of the score. A\nseparate structure aggregates the information of how dif-\nferent parts would look like when joined together. As a\nresult, our model is capable of expressing/processing mu-\nsic scores with any number of parts using a single trained\nmodel.\n2. MULTI-PART MUSIC\nIn this work, we focus on music containing multiple mono-\nphonic parts (voices). For example, most of Bach chorales\nwere written in the SATB format (Soprano, Alto, Tenor,\nand Bass), with each part containing a monophonic stream\nof notes. It is a traditional pedagogical practice to teach\nfundamental concepts of music theory by having students\nanalyze and compose (i.e. “part write”) this kind of music.\nWhen analyzing or composing music, we often separate\na musical score into streams of notes [2], consciously or\nunconsciously. This part-separated form of music scores\nis easier to analyze and manipulate algorithmically, and\nmany symbolic music analysis tasks use this separation\nas one of their preprocessing steps [7]. There are some\nexisting approaches to perform part (voice) segmentation;\nsee [7, 8] for more details. Therefore, our proposed tech-\nnique focuses on encoding a part-segmented representation\nassuming the segmentation is known.\n2.1 Representation\nIn traditional western music notation, durations of notes\nare derived by uniformly dividing a duration of a unit\nlength recursively. Notes start and end on a subdivided\nposition. It is thus reasonable to represent a music score\nas events on a grid, with each grid point representing a\ntime frame. This process is commonly known as quan-\ntization . This practice can be seen in many works, e.g.,\n[1, 9, 14, 22]. In this work, we keep the quantization step\nsize ﬁxed throughout the piece.\nWe encode two aspects of a music score: pitch andmet-\nrical structure . We make the following requirements for\nthis representation:\n1. This representation is able to encode a minimal set\nof musical notational elements, from which the re-\nconstructed music score is human-readable.\n2. Values at the same beat position under different\nquantization step sizes are the same.\nExisting works make use of MIDI pitch numbers for en-\ncoding pitch. However, MIDI pitch numbers discard one\nelement that is important for context determination: note\nspelling. In the proposed representation, pitch is repre-\nsented by a tuple (diatonic note number;accidental ),\nwhere diatonic note number is the index of a note name\nwith accidental removed (imagine the indices for white\nkeys on a piano keyboard), and accidental has a range of\n[\u00002;2], that is, up to 2 ﬂats and 2 sharps. For representinga whole note event, similar to [9,17], we use a special con-\ntinuation symbol, which is \u00001in the diatonic note number\nﬁeld. For positions of rest notes, we artiﬁcially set their\ndiatonic note number to 0. Accidentals are undeﬁned in\nthese two cases, therefore zeros can be ﬁlled in.\nWe encode the metrical structure into three simultane-\nous sequences sharing the same time resolution as the pitch\nframes: 1) Bar Line is a binary sequence encoding mea-\nsure boundaries. A value of 1is assigned to the frame at\nthe ﬁrst beat of a measure, and 0is assigned elsewhere. 2)\nBeat Level encodes a frame’s beat (sub-)division level in\nthe metric hierarchy within a measure. Frames at the high-\nest beat division level are assigned a value of 0; frames\nat the next level are assigned \u00001, etc. 3) Accent Level\nencodes the relative strength of beat positions of frames\nwithin a measure, with 0representing the highest strength\nand\u00001representing the second highest strength, etc. For\nexample, for a classical 4/4 time signature, the frame at the\nﬁrst beat of a measure is assigned 0, the frame at the third\nbeat is assigned\u00001, etc.\nThe ﬁrst two encoding sequences work together to make\nit possible to reconstruct bar lines and the time signa-\nture. The third sequence further encodes metrical ac-\ncents that are indicative of different music styles, and reg-\nular/irregular metrical structures.\nBar Line 1 0 0 0 0 0 0 0\nBeat Level 0 -1 0 -1 0 -1 0 -1\nAccent Level 0 -3 -2 -3 -1 -3 -2 -3\n3. THE PART-INVARIANT MODEL\n3.1 Model Architecture\nFollowing the general practice of language models, our\nmodel predicts one symbol at a position given its (musi-\ncal) context, that is,\nP(xt;kjcontext t;k);\nwheretis the time frame index, kis the part index, and xt;k\nis the pitch representation at position (t;k). We further as-\nsumecontext t;kto be able to separate xt;kfrom inﬂuences\nof all other variables ( Markov Blanket assumption). This\nMarkov Blanket formalism is also used in [9].\nFor obtaining a vector summarizing the context for part\nkand framet, after masking the symbol at the posi-\ntion(t;k)as a special UNK symbol, we ﬁrst use a part-\nwise summarizer, which is a single-layered bidirectional\nRNN2, to produce a part-wise context vector for each part.\nThen all part-wise context vectors are aggregated by one of\nreduction operations, e.g., max ,min,sum , along the axis\nof part indexes, to produce an aggregated context vector .\nWe also summarize the metrical structure (bar line, beat\nlevel, and accent level) with another single-layered bidirec-\ntional RNN to produce a metrical context vector . Finally,\nfor time frame t, the part-wise context vector for part k, the\n2Bidirectional here means that the output is a concatenation of outputs\nfor the same time step from two RNNs with opposite directions.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 205aggregated context vector, and the metrical context vec-\ntor are concatenated and fed into a feed-forward network\nwith asoftmax output layer to obtain the ﬁnal prediction\nP(xt;kjcontext t;k).\nOur model is illustrated in Figure 1. Inputs to the part-\nwise context summarizer are vector embeddings described\nin Section 3.1.1, Inputs to the metrical context summarizer\nare raw metrical sequences, and the RNN structure used in\nour experiment is described in Section 3.1.2.\nC3E4\nG3D4\nC3C4\nC3E4\nG3D4\nC3C4\n(a)Sequence of Sets vs.Bag of Parts . Our model is\nbuilt upon the idea of bag of parts .\n context vector for\npos (t,k)metrical context\nvector for time tNote PredictorP( | ) xt,kcontextt,k\naggregated Context\nVector for time t\n(b) Predicting a note given its context.\nFrame tPart k+1 · · · · · ·Part k · · · · · ·Part k-1 · · · · · ·Part-wise Context SummarizerContext V ector  \nfor Part k, Frame t\n(c) Part-wise context vector: each part is summarized\nby a bidirectional RNN.\nprojected context vector\nfor Part 1  Reduce\nalong\nthis axisProjectionThis can be max, sum,\nmin,mean, etc.\nprojected context vector\nfor Part 2  \nprojected context vector\nfor Part 3  aggregated context vector\n(d) Aggregated context vector: taking a reduction op-\neration along the axis of projected part-wise context\nvectors and then projecting to a desired dimension.\nFigure 1 : Model Architecture.\n3.1.1 Structured Pitch Vector Embedding\nAn embedding layer, which is usually the ﬁrst layer of neu-\nral networks for modeling discrete symbols, learns a vector\nrepresentation for each symbol. For embedding pitches, if\neach pitch is treated as a separate symbol, some general\nrelationships that are already known (e.g., octave equiva-\nlence, intervals) will be lost. Therefore, we propose to use\na factorized vector embedding representation (i.e., multi-\nple terms in Eq. (1)) for each pitch for better generality.\nFor readers not familiar with embedding layers, one can\ntreatVk’s below as lookup tables, each of which createsone entry (vector) for every possible value it takes.\nThe ﬁnal vector embedding V(p)is the sum of a series\nof embedding vectors, with each encoding a different “as-\npect” of a pitch.\nV(p) =V1(diatonicPitchClass (d)) +V2(d)\n+V3(p) +V4(MIDI (p))\n+V5(chromaticPitchClass (MIDI (p)));(1)\nwherep= (d;acc )is the pitch tuple deﬁned in Sec-\ntion 2, with dbeing the diatonic note number and\nacc being the accidental; MIDI (\u0001)is the MIDI pitch\nnumber; diatonicPitchClass (\u0001)and chromaticPitchClass (\u0001)\nwrap numbers according to octave equivalence; Vis the\nﬁnal vector embedding; V1;V2;V3;V4;V5are vector em-\nbeddings for different aspects. These vector embeddings\nare jointly learned during training.\n3.1.2 Stack Augmented Multiplicative Gated Recurrent\nUnit\nThe temporal dependency can be long for a representation\nusing ﬁne quantized time frames. In this work, instead of\nusing standard LSTMs, we use a stack augmented mul-\ntiplicative Gated Recurrent Unit as the RNN block. The\nGRU part implements the short-term memory. We choose\nthe stack mechanism [11] for the long-term memory be-\ncause of its resemblance to the pushdown automata, which\nhas more expressive power than a ﬁnite state machine and\nis able to recognize context-free languages, which are of-\nten used to model some elements in music.\nWe make the following convention for our notation: un-\nbolded lowercase letters, e.g, a, denote scalars; bolded\nlowercase letters, e.g, x,h, denote vectors; bolded upper-\ncase letters, e.g, W,S, denote matrices.\nThe original GRU, as introduced in [4], transforms the\ninput sequence ofhxtiinto a sequence of hhti, wheretis\nthe time step index:\nrt=\u001b(Wr[xt;ht\u00001] +br);\nut=\u001b(Wu[xt;ht\u00001] +bu);\nct= tanh( Wc[xt;rt\fht\u00001] +bc);\nht=ut\fht\u00001+ (1\u0000ut)\fct;(2)\nwhere rtis the reset gate, utis the update gate, ctis the\nupdate candidate, \u001b(x) =1\n1+e\u0000xis the Sigmoid func-\ntion. W’s and b’s are all trainable parameters, repre-\nsenting weights and biases respectively, \fhere represents\nelement-wise multiplication, [xt;ht\u00001]concatenates vec-\ntors into a longer column vector.\nMultiplicative integration [21] adds quadratic terms into\nRNN update equations in order to improve the expressive\npower. In our implementation, we replace the equation for\nthe update candidate with\nct;x=Wcxxt;\nct;r1=Wcr1(rt\fht\u00001) +bcr1;\nct;r2=Wcr2(rt\fht\u00001) +bcr2;\nct= tanh( ct;x\f(ct;r1+ 1) + ct;r2):(3)206 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018A stack-based external memory for RNN is introduced\nin [11], which is reported to be able to learn some se-\nquences that are not learnable by a traditional RNN, e.g,\nLSTM.\nWe denote the stack as a matrix St, with dimensions of\nN\u0002M, whereNis the length of one entry in the stack,\nandMis the capacity of the stack. In our implementation,\na stack augmented memory performs the following proce-\ndure at each time step:\n1. Fetch vtfrom the stack by positional attention,\nwhich is a linear combination of columns in Stwith\nweights kt:\nkt=softmax (Wread[xt;ht\u00001] +bread);\nvt=Stkt;(4)\nwhere softmax (x) =exp(x)\n1Texp(x);\n2. Augment the input with the fetched value:\n~ xt= [xt;vt]; (5)\nand then run one step RNN with input ~ xt, which\nproducesht;\n3. Generate the input to the stack:\nzt= tanh( Wz[~ xt;ht] +bz); (6)\n4. Make decisions on how to update the stack:\n2\n4at;no-op\nat;push\nat;pop3\n5=softmax (Wa[~ xt;ht] +ba);(7)\nwherea’s are probabilities that sum up to 1repre-\nsenting the probability of stack operations (no oper-\nation, push, pop);\n5. Update the stack by expectation of operations:\nSt;pushed = [ zt;ﬁrstk(St\u00001)];\nSt;popped = [ lastk(St\u00001);0];\nSt=at;no-opSt\u00001\n+at;pushSt;pushed\n+at;popSt;popped;(8)\nwhere ﬁrst k(\u0001)extracts the ﬁrst kcolumns, and\nlastk(\u0001)extracts the last kcolumns. Here k=M\u00001.\nOperator [;]concatenates vectors/matrices horizon-\ntally.\n3.1.3 Context Aggregation: Obtaining Part-Invariance\nAs mentioned above, the aggregated context vector is ob-\ntained by reduction operations on projected part-wise con-\ntext vectors, and is then projected to the desired dimension.\nCaggregated\nt =Wproj2(KM\nk=1Wproj1Cpart\nt;k); (9)\nwhere Caggregated\nt andCpart\nt;kare aggregated context vector\nand partwise context vector respectively,LK\nk=1denotes areduction operator over kfrom 1toK, whereKis the\nnumber of parts, Wproj1andWproj2are projection ma-\ntrices for transforming the context vector into a higher di-\nmension and back in order to improve the expressiveness\nof this reduction operation. In our experiment, we use max\nreduction. The proof of the universal approximation prop-\nerty for approximating a continuous set function when max\nreduction is used can be found in [3].\nThe reduction operation applied here produces a contin-\nuous bag of parts (bagmeans (multi-)set). This terminol-\nogy draws similarity to continuous bag of words (CBOW,\n[16]), which averages all vector embeddings for all words\n(mean reduction) within a window to obtain the vector\nrepresentation for this context. For comparison, existing\nworks conceptually make use of sequence-of-set paradigm\nfor context modeling (see Figure 1a), therefore the con-\ntext model is conﬁned to learning sequential relationships\nbetween sets. Our conceptual paradigm is on a different\ndirection. We built a model for processing monophonic\nparts and a model for putting them into a bag. One impor-\ntant feature for doing this is that it allows learning shared\nproperties of parts. Also, the ordering of parts, which is\nredundant for a context encoder, is discarded and only the\ncontent information of all parts is aggregated. As a result,\nit reduces the model complexity required.\n3.2 Sampling and Generation\nAfter training the Markov blanket model for approximat-\ning the probability of a note conditioned on its context,\nP(xt;kjcontext t;k), the process of generation is performed\nby Gibbs sampling with an annealing schedule. This pro-\ncedure is almost the same as the one used in [9].\nFirstly, we initialize notes xt;kof all positions in the\nempty parts randomly. Then we iterate:\n1. Randomly or deterministically select the next posi-\ntion(t;k)that is not ﬁxed3to sample;\n2. Sample new xt;k, according to\n~P(\u0001jcontext t;k)/(P(\u0001jcontext t;k) )1=T;(10)\ni.e, the annealed distribution with temperature T >\n0;\nFor vanilla Gibbs sampling, T\u00111. However, as\npointed out in [9], conditional distributions outputted are\nlikely to be incompatible and there is no guarantee that the\nGibbs sampler will converge to the desired joint distribu-\ntion.\nIn Gibbs sampling with an annealing schedule, the tem-\nperature starts from a high value and gradually decreases to\na low value. By incorporating this annealing scheme, the\nalgorithm can escape from initial bad values much easier at\nthe beginning, and the average likelihood for the selected\nnew samples increases as the temperature decreases. For\nillustration, in the limiting case that T!0, the algorithm\n3Fixed positions are used as conditions. For example, if the task of\nmelody harmonization, the melody part is ﬁxed.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 207greedily selects new samples that maximizes the local like-\nlihood.\nSince in our model parts are orderless, the generated re-\nsult does not ensure all parts in their usual notated staffs.\nAlso, the imperfection of Gibbs sampler often makes the\nconﬁguration get stuck in a region where voice crossing\noccurs even if parts in the training set rarely cross. In\nthe experiment, we enforce one constraint during sampling\nas a workaround: in each time frame, the pitch of each\npart cannot go above/below the part that is immediately\nabove/below it, i.e., no voice crossing is allowed. This\nconstraint is achieved by limiting the range of candidates\nto sample. How to design a better sampling procedure is\nleft for future investigation.\n4. EXPERIMENT\n4.1 Training\n4.1.1 Dataset\nWe trained our model on the Bach Chorale dataset in-\ncluded in Music21 [5]. We chose this dataset to perform\nour experiment for the following reasons: ﬁrstly, it is pub-\nlicly available; secondly, it matches the objective evalua-\ntion methods we designed4; thirdly, there is no need to\nperform voice separation in this dataset. Different parts\nare separately encoded in the ﬁle format.\nWe performed data augmentation by transposition with\na range such that the transposed piece is within the lowest\npitch minus 5 semitones to a highest pitch plus 5 semitones\nfor the whole dataset. Enharmonic spellings are resolved\nby selecting the one that creates the minimum number of\naccidentals for the entire transposed piece.\n4.1.2 Model Speciﬁcation\nIn our experiment, we use a quantization step size of a\nsixteenth note. Embedding layers have a dimension of\n200. We use single-layered RNNs as the partwise con-\ntext summarizer and metrical sequence summarizer. All\nRNNs have a hidden state size of 200, stack vector length\n200, stack size 24. The intermediate dimension for the\npart aggregating layer is 1000. The ﬁnal predictor is a\nfeed-forward neural network with 3layers, each of which\ncontains 400 hidden units. The ﬁnal softmax layer has a\ndimension of 400, each corresponding to a speciﬁc pitch\ntuple (diatonic note number, accidental). We use cross en-\ntropy as the loss function. Curriculum learning is used in\nour training: we started from a small half-window width of\n8and gradually doubled the half-window width to a max-\nimum of 128. During training, pitches within the context\nwindow are randomly set to a rest with probability 0:1. All\nlayers except for the RNN layers use a dropout rate of 0:1.\nIn our evaluation, we use an half-window width of 64\nfor generation. We use a simple linear cooling schedule\nto decrease the temperature from an initial value of 1:2to\n4The objective evaluation follows rules used in textbook part writing,\nwhich are greatly inﬂuenced by Bach Chorales, however, these rules are\nnot strictly followed by Bach himself.0:25. The total number of iterations is selected such that\nevery position is sampled 40times.\nMusic scores are reconstructed by directly using acci-\ndentals, diatonic note numbers and the original encoded\nmetrical sequences. Key signatures and clefs are automat-\nically determined by Music21’s [5] built-in functions.\n4.2 Evaluation\n    44 \n(a) bassline1\n  34\n(b) bassline2\n     44\n(c) bassline3\nFigure 2 : Basslines used in our evaluation.\nTo perform evaluation, we compared our algorithm’s\nharmonizations of basslines with harmonizations of those\nsame basslines completed by music students. We used\nthree basslines which vary in difﬁculty, ranging from di-\natonic (bassline 1) to moderately chromatic (bassline 2) to\nhighly chromatic (bassline 3).5For each bassline, our al-\ngorithm generated 30outputs, for a total of 30*3 outputs.\nAs a side note, 4 bars is the usual length for a harmo-\nnization exercise. This length is different from lengths of\npieces in the training set.\nWe recruited 33 second-semester sophomore music\nmajors, offering them extra credit for harmonizing each\nbassline. We gave each student a .xml ﬁle containing\nthe three basslines, with three blank upper staves . We\ninstructed students to harmonize each of the basslines in\nfour-part, SATB chorale style, following the usual rules of\nvoice-leading and harmony. We used valid responses from\n27students (those not empty and returned timely) in the\nfollowing evaluation tasks.\nWe recruited two teams for evaluation: graders and lis-\nteners. The graders were three music theory PhD students.\nThey were given the 57valid outputs ( 57\u00033in total) in\n.pdf format; we created a grading rubric6. A deduction\nless than 0was computed by each grader for each output.\nThe lower the value, the greater the number of errors. One\ngraded example can be found in Figure 3.\nFigure 3 : Example annotation from one of our graders.\n5Basslines 1 and 2 were taken from Exercises 10.3C and 21.2, respec-\ntively, from [12]. In Bassline 2, the B was originally a Bb in [12], but we\nchanged it to increase chromaticism. Bassline 3 was created by us, and\nintended to represent highly modulatory chromatic harmony.\n6The rubric is typical of traditional music theory textbooks and\nclasses. For the detailed rubric, see the supplementary website.208 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018While the grading method was fairly objective (corre-\nlations between error values from the three graders were\n.85, .88, and .92), we also wanted subjective ratings. In ad-\ndition, we recruited a listening team of another three mu-\nsic theory PhD students. We gave them the same 57 * 3\noutputs in .mp3 format, synthesized with a software piano\nsynthesizer and tempo 93, and these instructions:\nFor each output, answer the following four questions:\n1. As you listen, how much are you enjoying this solu-\ntion (on a scale of 1 to 4, where 1 = not enjoying at\nall and 4 = greatly enjoying)?\n2. As you listen, how conﬁdent are you that this solu-\ntion is by a computer vs. a sophomore (on a scale\nof 1 to 4, where 1 = probably a computer and 4 =\nprobably a sophomore)?\n3. As you listen, to what extent does this solution\nconform to textbook/common-practice voice-leading\nand harmony (on a scale of 1 to 4, where 1 = not very\nidiomatic and 4 = quite idiomatic)?\n4. Please share any other comments or thoughts (for\nexample, why does it sound like it’s a computer vs. a\nsophomore?)\nTo summarize, for each output we had 3 gradings (1\nvalue * 3 graders) and 9 subjective ratings (3 ratings*3 lis-\nteners) plus additional open-ended comments.\nTo minimize bias, graders only received .pdf outputs;\nlisteners only received .mp3 outputs. The outputs were\npresented to the graders and listeners in random order.\nBoth teams were blind to the output source (computer or\nstudent), and were allowed to take as much time as they\nneeded to make their assessments.\nOur experiment result is summarized in Figure 4. Our\nexperiment shows that gradings and listening ratings for\nour algorithm and students overlap to different extents (our\nalgorithm performs best on the second bassline). For the\nlistening test, our algorithm consistently performs a bit\nworse than average second-year second-semester music\nmajors.\nThe comments from the listener who contributed most\nof the open-ended comments (question 4) suggest that the\npresence of tonality was one of the main factors in their\nTuring judgements. This listener attributed harmoniza-\ntions that feature small stylistic errors (e.g., oddly repeated\nnotes, parallel voice leading, etc.) to both human and com-\nputer, but those harmonizations that sounded resolutely\ntonal were only attributed to humans. Another listener\nseemed to ground their judgments on a different feature:\n“A lot of the ones I think are computer-generated do ca-\ndences super well.” Indeed, for the most part the computer\ndid generate well-formed cadences.\nBy examining the detailed responses from our graders,\nwe have the following rudimentary observations:\n1. Errors of our algorithm and students are differently\ndistributed.\n2. Parallel octave/ﬁfth (Error 3) is one frequent error\nproduced by our algorithm, more often than stu-\ndents. This type of error is also observed in gen-\neration examples shown in [9].\nbassline1 bassline2 bassline325\n20\n15\n10\n5\n0deductions/pts\nComputer\nStudents(a) Results for the objective grading test.\nEnjoymentTuring\nTextbookEnjoymentTuring\nTextbookEnjoymentTuring\nTextbook1.01.52.02.53.03.54.0score\nComputer\nStudentsbassline1 bassline2 bassline3\n(b) Results for the subjective listening test.\nFigure 4 : Objective and subjective comparisons between\nour algorithm’s and music students’ harmonization on the\nthree basslines.\n3. Our algorithm produces more non-stylistic progres-\nsions (Error 6 and Error 7). In our algorithm, it is\nobserved that, smooth/melodic voice leading may\nsometimes suppress the requirement of the vertical\nsonority.\n4. Students are much more likely to exceed the octave\nrange limit between nearby upper voices (Error 9)\nFrom our experiment result, it is revealed that the pro-\nposed algorithm cannot learn what is bad/incorrect just by\nwatching correct examples. Therefore, there is a need to\ntrain with negative examples. Our experiment provides\nuseful data for future development.\n5. CONCLUSION\nIn this work, we proposed a part-invariant model for music\ngeneration and harmonization that operates on multi-part\nmusic scores, which are scores containing multiple mono-\nphonic parts. We trained our model on Bach Chorales\ndataset. We performed objective and subjective evalua-\ntions by comparing the outputs of our algorithm against the\ntextbook-style part writings of undergraduate music ma-\njors. Our experiment result provides insights and data that\nwill be useful for future development.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 2096. REFERENCES\n[1] Moray Allan and Christopher Williams. Harmonising\nchorales by probabilistic inference. In Advances in\nneural information processing systems , pages 25–32,\n2005.\n[2] Albert S. Bregman. Auditory scene analysis . MIT\nPress, 1996.\n[3] R. Qi Charles, Hao Su, Mo Kaichun, and Leonidas J.\nGuibas. Pointnet: Deep learning on point sets for 3d\nclassiﬁcation and segmentation. In 2017 IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR) , pages 77–85, 2017.\n[4] Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. Learning phrase rep-\nresentations using rnn encoder–decoder for statistical\nmachine translation. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1724–1734, 2014.\n[5] Michael Scott Cuthbert and Christopher Ariza. mu-\nsic21: A toolkit for computer-aided musicology and\nsymbolic music data.\n[6] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-\nHsuan Yang. Musegan: Multi-track sequential gener-\native adversarial networks for symbolic music genera-\ntion and accompaniment. In AAAI-18 AAAI Conference\non Artiﬁcial Intelligence , 2018.\n[7] Patrick Gray and Razvan C Bunescu. A neural greedy\nmodel for voice separation in symbolic music. In Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR 2016) , pages 782–788, 2016.\n[8] Nicolas Guiomard-Kagan, Mathieu Giraud, Richard\nGroult, and Florence Lev ´e. Comparing voice and\nstream segmentation algorithms. In International So-\nciety for Music Information Retrieval Conference (IS-\nMIR 2015) , pages 493–499, 2015.\n[9] Ga ¨etan Hadjeres, Franc ¸ois Pachet, and Frank Nielsen.\nDeepbach: a steerable model for bach chorales gener-\nation. In International Conference on Machine Learn-\ning, pages 1362–1371, 2017.\n[10] Cheng-Zhi Anna Huang, Tim Cooijmans, Adam\nRoberts, Aaron C. Courville, and Douglas Eck. Coun-\nterpoint by convolution. In ISMIR , pages 211–218,\n2017.\n[11] Armand Joulin and Tomas Mikolov. Inferring algorith-\nmic patterns with stack-augmented recurrent nets. In\nAdvances in neural information processing systems ,\npages 190–198, 2015.\n[12] Steven G. Laitz. Writing and analysis workbook to ac-\ncompany The complete musician: an integrated ap-\nproach to tonal theory, analysis, and listening, 3rd edi-\ntion. Oxford University Press, 2012.[13] Victor Lavrenko and Jeremy Pickens. Polyphonic mu-\nsic modeling with random ﬁelds. In Proceedings of the\neleventh ACM international conference on Multimedia ,\npages 120–129. ACM, 2003.\n[14] Feynman T. Liang, Mark Gotham, Matthew Johnson,\nand Jamie Shotton. Automatic stylistic composition of\nbach chorales with deep lstm. In ISMIR , pages 449–\n456, 2017.\n[15] Robert L Marshall. How JS Bach composed four-\npart chorales. The Musical Quarterly , 56(2):198–220,\n1970.\n[16] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. Efﬁcient estimation of word representations in\nvector space. arXiv preprint arXiv:1301.3781 , 2013.\n[17] Franois Pachet, Alexandre Papadopoulos, and Pierre\nRoy. Sampling variations of sequences for structured\nmusic generation. In ISMIR , pages 167–173, 2017.\n[18] Martin Rohrmeier. A generative grammar approach to\ndiatonic harmonic structure. In Proceedings of the 4th\nsound and music computing conference , pages 97–100,\n2007.\n[19] Ian Simon, Dan Morris, and Sumit Basu. Mysong: au-\ntomatic accompaniment generation for vocal melodies.\nInProceedings of the SIGCHI Conference on Human\nFactors in Computing Systems , pages 725–734. ACM,\n2008.\n[20] Andries Van Der Merwe and Walter Schulze. Music\ngeneration with markov models. IEEE MultiMedia ,\n18(3):78–85, 2011.\n[21] Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua\nBengio, and Ruslan R Salakhutdinov. On multiplica-\ntive integration with recurrent neural networks. In\nAdvances in Neural Information Processing Systems ,\npages 2856–2864, 2016.\n[22] Li-Chia Yang, Szu-Yu Chou, and Yi-Hsuan Yang.\nMidinet: A convolutional generative adversarial net-\nwork for symbolic-domain music generation. In Pro-\nceedings of the 18th International Society for Music In-\nformation Retrieval Conference (ISMIR2017), Suzhou,\nChina , 2017.210 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "The Many Faces of Users: Modeling Musical Preference.",
        "author": [
            "Eva Zangerle",
            "Martin Pichl"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1492515",
        "url": "https://doi.org/10.5281/zenodo.1492515",
        "ee": "https://ismir2018.ircam.fr/doc/pdfs/128_Paper.pdf",
        "abstract": "User models that capture the musical preferences of users are central for many tasks in music information retrieval and music recommendation, yet, it has not been fully explored and exploited. To this end, the musical preferences of users in the context of music recommender systems have mostly been captured in collaborative filtering-based approaches. Alternatively, users can be characterized by their average listening behavior and hence, by the mean values of a set of content descriptors of tracks the users listened to. However, a user may listen to highly different tracks and genres. Thus, computing the average of all tracks does not capture the user's listening behavior well. We argue that each user may have many different preferences that depend on contextual aspects (e.g., listening to classical music when working and hard rock when doing sports) and that user models should account for these different sets of preferences. In this paper, we provide a detailed analysis and evaluation of different user models that describe a user's musical preferences based on acoustic features of tracks the user has listened to.",
        "zenodo_id": 1492515,
        "dblp_key": "conf/ismir/ZangerleP18",
        "keywords": [
            "collaborative filtering",
            "average listening behavior",
            "content descriptors",
            "mean values",
            "acoustic features",
            "user preferences",
            "contextual aspects",
            "music recommender systems",
            "music information retrieval",
            "task in music"
        ],
        "content": "CONTENT-BASED USER MODELS:\nMODELING THE MANY FACES OF MUSICAL PREFERENCE\nEva Zangerle\nUniversit ¨at Innsbruck\nDepartment of Computer Science\neva.zangerle@uibk.ac.atMartin Pichl\nUniversit ¨at Innsbruck\nDepartment of Computer Science\nmartin.pichl@uibk.ac.at\nABSTRACT\nUser models that capture the musical preferences of users\nare central for many tasks in music information retrieval\nand music recommendation, yet, it has not been fully ex-\nplored and exploited. To this end, the musical preferences\nof users in the context of music recommender systems have\nmostly been captured in collaborative ﬁltering-based ap-\nproaches. Alternatively, users can be characterized by their\naverage listening behavior and hence, by the mean values\nof a set of content descriptors of tracks the users listened\nto. However, a user may listen to highly different tracks\nand genres. Thus, computing the average of all tracks does\nnot capture the user’s listening behavior well. We argue\nthat each user may have many different preferences that\ndepend on contextual aspects (e.g., listening to classical\nmusic when working and hard rock when doing sports) and\nthat user models should account for these different sets of\npreferences. In this paper, we provide a detailed analy-\nsis and evaluation of different user models that describe\na user’s musical preferences based on acoustic features of\ntracks the user has listened to.\n1. INTRODUCTION\nIn the last decade, the amount of tracks available on\nstreaming platforms has literally exploded. Users are sup-\nported in exploring and wading through these music col-\nlections by means of personalization—mostly by recom-\nmender systems that provide users with a list of tracks they\nmight like to listen to. Such personalization is central for\nthe success of streaming platforms as it eases the task of\ndiscovering new and enjoyable music for users.\nFor music information retrieval (MIR) and particularly,\nfor personalization tasks in this context, modeling the mu-\nsical preferences of users is naturally a central aspect. Yet,\nuser modeling for MIR and music recommender systems\n(MRS) has hardly been investigated [4,32,33]. To this end,\nmusic recommender systems have mostly been realized by\nmeans of collaborative ﬁltering (CF) methods [16] or more\nc\rEva Zangerle, Martin Pichl. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: Eva Zangerle, Martin Pichl. “Content-based User Models:\nModeling the many faces of musical preference”, 19th International So-\nciety for Music Information Retrieval Conference, Paris, France, 2018.advanced factorization approaches [17], where recommen-\ndations are based on interactions between users and items.\nSuch systems are agnostic to content features as recom-\nmendations are computed based on the similarity of users\n(or items) based on their co-occurrence in the listening his-\ntories of all users. On the other hand, (the less adopted)\ncontent-based recommender systems [22] compute recom-\nmendations based on the similarity of content descriptors\nof tracks. Also, hybrid recommender systems combining\nCF- and content-based approaches have been proposed [7].\nIn the ﬁeld of MIR, tracks are traditionally character-\nized by content descriptors—these range from detailed fea-\ntures such as MFCCs [21] to high-level content descrip-\ntors such as acousticness, tempo or danceability (e.g., pro-\nvided by the Spotify platform1). While these features\nare widely used to characterize single tracks, for a user\nmodel that captures the user’s preferences well, these fea-\ntures have to be aggregated across all tracks the user has\nlistened to. To this end, Pichl et al. [30] utilized con-\ntent descriptors of tracks for representing a user’s musi-\ncal preference by computing the average acoustic features\nacross all tracks the user has listened to. They also ﬁnd\nthat users create different playlists that feature different\nacoustic characteristics—implying that these playlists cor-\nrespond to different sets of preferences of a user (which\nmay naturally be context-related) and stress the need for\nmore comprehensive user models to describe users’ musi-\ncal preferences [30]. Similarly, Wang et al. [36] state that\npeople prefer different music for different daily activities.\nAlong these lines, we argue that users may exhibit differ-\nent preferences depending on the context and e.g., listen to\nmore energetic music when doing sports or calming mu-\nsic when being at home [36]. These different preferences\ncannot be sufﬁciently reﬂected in a model that averages the\ncharacteristics of all the tracks a user listened to. In a prob-\nabilistic user model, Bogdanov et al. [4] characterize a user\nin a semantic feature space derived from low-level content\nfeatures by utilizing Gaussian Mixture Models.\nIn this paper, we build upon and extend these previous\nworks by proposing different user models to describe the\nmusical preferences of users based on content descriptors\nof tracks. We perform a large-scale evaluation of these\nmodels in a track recommendation task based on 8 million\nlistening events of 13,000 users. Our experiments show\n1https://developer.spotify.com/web-api/get-several-audio-features/709that utilizing a user model based on a user’s speciﬁc pref-\nerences regarding different types of music (modeled prob-\nabilistically by GMMs) complemented with a user’s gen-\neral musical preference achieves the best results. Our re-\nsults show that in terms of recommendation quality, the\nproposed models contribute to substantially improved rec-\nommendation performance. We believe that our ﬁndings\ncan contribute to improved user models for music recom-\nmender systems and generally, MIR tasks.\nThe remainder of this paper is organized as follows.\nSection 2 discusses related work and Section 3 presents\nthe features utilized and the dataset underlying our experi-\nments. Section 4 presents the user models proposed. Sec-\ntion 5 details the experimental setup and Section 6 presents\nthe results of our study, which are discussed in Section 7.\nSection 8 concludes the paper and discusses future work.\n2. RELATED WORK\nGenerally, Schedl et al. [32, 33] note that the user and\nhis/her preferences are often not considered when it comes\nto MIR and MRS tasks. Particularly, the authors lay out\nthat user modeling for such tasks has hardly been explored\nand evaluated yet.\nTo this end, content descriptors have widely been used\nin MIR and MRS. For similarity search, often a content-\nbased similarity measure is used for matching queries and\na music database [9, 20, 35, 39]. In the context of mu-\nsic recommender systems, Yoshii et al. [38] propose a hy-\nbrid recommender system that combines collaborative ﬁl-\ntering via user ratings and content-based features modeled\nvia Gaussian Mixture Models over MFCCs by utilizing\na Bayesian network. Also, Liu [20] investigates differ-\nent distance metrics for content-based recommender sys-\ntems. Recently, also deep learning-based hybrid MRS have\nalso been proposed [37]. In regards to user modeling for\nMRS, Bogdanov et al. compute a user’s musical prefer-\nences by a set of exemplary tracks that the user enjoyed.\nThey model the user’s preference in a latent semantic space\nbased on a set of diverse content features and propose a\nset of similarity-based recommender systems. One sys-\ntem models a user by a Gaussian Mixture Model based on\nthe proposed semantic audio feature space. The authors\nevaluated these recommender systems in a user experiment\nwith twelve users. As for musical preferences of users,\nPichl et al. found in a large-scale study of Spotify users\nthat music streaming users listen to different types of mu-\nsic. Those types can be observed via k-means clustering\nof content descriptors of tracks. They also found that users\norganize their music in playlists based on these types and\nstress the importance of more comprehensive user models\nto describe users’ musical preferences [30]. Along these\nlines, we speciﬁcally investigate user models that are solely\nbased on content descriptors. We propose six user models\nand compare these in a large-scale ofﬂine study based on a\nrecommendation task comprising 13,000 users and 8 mio.\nlistening events.3. DATASET AND FEATURES\nThe main data source used in our experiments is the pub-\nlicly available LFM-1b dataset [31], which provides the\nfull listening histories of 120,322 Last.fm users. For each\nlistening event (i.e., a certain user listening to a certain\ntrack), information about the track, artist, album and user\nis available. Besides the information contained within the\nLFM-1b dataset, we also require content features to de-\nscribe tracks. Following the lines of, e.g., [1, 25, 30], we\npropose to rely on the Spotify API2to gather the follow-\ning content descriptors for each track:\n1.Danceability describes how suitable a track is for\ndancing and is based “on a combination of musi-\ncal elements including tempo, rhythm stability, beat\nstrength, and overall regularity.”\n2.Energy measures the perceived intensity and activ-\nity of a track. This feature is based on the dynamic\nrange, perceived loudness, timbre, onset rate and\ngeneral entropy of a track.\n3.Speechiness detects presence of spoken words. High\nspeechiness values indicate a high degree of spo-\nken words (talk shows, audio book, etc.), whereas\nmedium to high values indicate e.g., rap music.\n4.Acousticness measures the probability that the given\ntrack is acoustic.\n5.Instrumentalness measures the probability that a\ntrack is not vocal (i.e., instrumental).\n6.Tempo quantiﬁes the pace of a track in beats per\nminute.\n7.Valence measures the “musical positiveness” con-\nveyed by a track (i.e., cheerful and euphoric tracks\nreach high valence values).\n8.Liveness captures the probability that the track was\nperformed live (i.e., whether an audience is present\nin the recording).\nThese features are high-level descriptors of the acous-\ntic content of tracks. We argue that they are neverthe-\nless representative and hence, the obtained results should\ngive a good impression on the differences of the user mod-\nels. We expect our ﬁndings to also hold for more com-\nplex and lower-level content descriptors such as e.g., Mel-\nFrequency Cepstral Coefﬁcients (MFCC) [21].\nTo obtain these features for all tracks of the dataset, we\napply the following steps: we perform a conjunctive query\nfor the <track, artist, album >-triples extracted from the\nLFM-1b dataset using the Spotify search API3to gather\nthe Spotify URI of each track. This URI is subsequently\nused to query the acoustic features API4. Finally, we add\ntracks for which can obtain all required features to the\ndataset5\nSince the set of tracks a user listened to may also con-\ntain outlier tracks that may distort the user proﬁle, we\npropose to remove outlier tracks from this set by apply-\n2A detailed description of these features and the API can be found at\nhttps://developer.spotify.com/web-api/get-several-audio-features/.\n3https://developer.spotify.com/web-api/search-item/\n4https://developer.spotify.com/web-api/get-several-audio-features/\n5Except for tempo, all of these features are given in the range of [0;1]\nand for tempo, we apply a linear min-max scaling.710 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018Item Value\nListening Events (LE) 8,457,205\nUsers 12,995\nTracks distinct 965,293\nMin. LE per User 1\nQ1LE per User 252\nMedian LE per User 478\nQ3LE per User 826\nMax. LE per User 21,660\nAvg. LE per User 650.80 ( \u0006713.99)\nTable 1 . Dataset statistics.\ning the median absolute deviation (MAD) outlier detection\nmethod [19]. We consider a feature value an outlier if it is\nnot within M\u0006a\u0001MAD , where Mis the median of this\nparticular feature across all tracks of a user and MAD is\nthe median absolute deviation of these values. We consider\na value an outlier if it is not within within three MAD s\naround the median, setting a rather conservative threshold\na= 3 as proposed by [19]. Lastly, a track is considered\nas an outlier in the list of tracks of a particular user if one\nof its features is considered an outlier and consequently re-\nmoved from the user listening history.\nApplying this procedure results in a dataset of 55,149\nusers, 394,944,868 listening events and 3,478,399 distinct\ntracks. We randomly sample users from this dataset for\nour experiments, where we require each user to have more\nthan 100 listening events to ensure that our user models\nare representative. We present basic statistics about the\nresulting dataset in Table 1. As can be seen, on average,\neach user has listened to 651 tracks.\n4. USER MODELS\nIn the following, we present the proposed user models to\ncapture user’s listening preferences. We speciﬁcally focus\non modeling users solely by acoustic features of tracks they\nlistened to and deliberately neglect other information that\ncould contribute to a user model (e.g., demographic user\naspects, cultural information or further contextual features\nthat might improve MRS and MIR performance).\n4.1 Feature Space\nBased on the users, tracks and their acoustic features\nwithin the dataset, we perform the following steps prior to\nthe computation of the user models. Most of the proposed\nmodels require clustering tracks based on their acoustic\nfeatures to ﬁnd groups of tracks that exhibit similar fea-\ntures. Given that we aim to perform a large-scale anal-\nysis of the proposed user models (we perform the analy-\nsis on 8 million tracks and 13,000 users), these clustering\ncomputations are computationally intensive. Hence, we\nﬁrstly perform a proximity-preserving dimension reduc-\ntion on the input data by applying UMAP (Uniform Mani-\nfold Approximation and Projection) [23]. Also, the use of\nlatent representations of elements in the musical ecosystem\n(users, tracks, etc.) has been to be effective in MIR andMRS tasks [18, 26, 27]. In our experiments, we compute\na 2-d latent representation of tracks for the computation of\nuser models. This allows us to inspect the resulting clus-\nters visually during the development of the user models\nand, more importantly, reduces cluster computation time\nsubstantially, which naturally permits better scalability for\nlarger datasets.\n4.2 User Models\nFor modeling user preferences for musical tracks and their\ncharacteristics, we naturally require models for both tracks\nand users as we utilize a user’s model and compare it with\ntrack models to ﬁnd suitable similar tracks that may be rec-\nommended to the user.\nAs for modeling tracks and their characteristics, we rely\non their acoustic features (AF; e.g., danceability or tempo).\nHowever, for users we require more sophisticated user\nmodels, as these have to represent a possibly extensive and\ndiverse set of tracks and their characteristics to eventually\nrepresent a user’s musical preferences. We propose user\nmodels that are based on clusters of similar tracks and uti-\nlize a user’s membership in these clusters (i.e., the fact that\nuser has listened to tracks that belong to a given cluster)\nto get a ﬁne-grained representation of the many faces of\nthe listening preferences of a given user. For determining\nsuch clusters and computing the membership of tracks in\nthese clusters, we experiment with two approaches: (i) we\nutilize k-means clustering to ﬁnd tracks that exhibit similar\nacoustic features and use the characteristics of these clus-\nters to characterize users; and (ii) we apply Gaussian Mix-\nture Models (GMM) [24] as these allow to model a track\nby the computed probability density function regarding the\nGMM’s components. Based on a track’s density functions,\nwe derive a set of GMM-based user models. Generally,\nthe idea is that based on these clusters or components, we\naim to model a user based on the characteristics of one or\nmultiple of these track clusters.\nIn the following, we describe the proposed user models\nto capture the musical preferences of users. An overview\nof the user models and the features used to characterize\nusers and tracks is shown in Table 2.\nContent avg: In a baseline model, we utilize the eight\nacoustic features of all tracks a user has listened to and\ncompute the average across all tracks of a user for each of\nthe features presented in Section 3. This allows us to de-\nscribe a user with his/her average listening behavior, break-\ning a user’s preferences down into eight acoustic features.\nPlease note that in the remainder of this paper, we refer to\nmodels as Content -models if the representation of the user\nor a track relies on acoustic features.\nContent avg, sd: This model is built upon the Content\navg model, which we extend by adding the standard de-\nviation of each of the acoustic features across all tracks\nof a user. We expect the added SD to mitigate the ef-\nfects of averaging a large number of features that poten-\ntially differ substantially as users may listen to music with\nhighly diverse acoustic characteristics. We again consider\nthis model a baseline that additionally quantiﬁes to whichProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 711Model User Features Track Feat.\nContent avg user AF avg AF\nContent avg, sd user AF avg and SD AF\nContent binary k-means avg. AF of single cluster AF\nContent weighted k-means weighted avg. AF of clusters AF\nGMM avg. densities of user’s tracks GMM densities\nContent binary GMM avg. AF of single GMM comp. AF\nContent weighted GMM weighted avg. AF of GMM comp. AF\nGMM + Content avg, sd GMM and user AF avg and SD GMM, AF\nTable 2 . Overview of evaluated models (AF stands for\nacoustic features, GMM for Gaussian Mixture Model and\nSD for the standard deviation).\nextent the user’s musical preferences vary regarding the\nacoustic features of his/her listening history.\nContent binary k-means: In this model, we rely on\nthe clusters computed by a k-means clustering of all tracks\nwithin the dataset in the computed 2-d latent space. In a\nnext step, we attribute each of the tracks a user has listened\nto a cluster and do a majority vote on the clusters to obtain\nthe cluster that holds most of the user’s tracks. We subse-\nquently model a user using the characteristics of the cluster\nthat contains the majority of the user’s track. To represent\nthis cluster, we compute the average of the eight acous-\ntic features of all tracks contained in the cluster and add\nthe according standard deviations. Single tracks are repre-\nsented by its acoustic features. We consider this a rather\nsimple model as we assign the user to a single cluster and\nhence, limit the model to a single preference scope.\nContent weighted k-means: The previous model is\nlimited as it is restricted to a single preference scope. To\ntackle this problem, we propose the Content weighted k-\nmeans model in which we now aim to address multiple sets\nof preferences of a user. Therefore, we again rely on the\nk-means clusters, however, we compute a weight for each\ncluster based on the number of tracks a user has listened\nto in each cluster. Based on the user’s weights for each\ncluster, we compute a weighted average for each acoustic\nfeature to represent the user, where each cluster is again\ncharacterized by its average acoustic features and its stan-\ndard deviation. Again, in this model each track is repre-\nsented by its acoustic features.\nGMM: In this model, we utilize a Gaussian Mixture\nModel [24] for representing both the track and the user.\nTherefore, we compute Gaussian components and repre-\nsent a track by its probability densities regarding the GMM\ncomponents. For users, we compute the average proba-\nbilities for each component across all of the user’s tracks\nto model a user’s musical preferences by using the GMM\ncomponents. We consider this model a proxy, as it does not\ndirectly utilize acoustic features to represent a track, but\nthe probabilistic assignments of a track to a set of groups\nof tracks (components).\nContent binary GMM: In contrast to the pure GMM\nmodel, this model relies on content features instead of\nprobability densities to represent a user. Analogously to\nthe Content binary k-means model, we rely on GMM to\nassign the user’s tracks to components. In particular, we\nassign the tracks found in the user’s listening history toGMM components. In a next step, we select the compo-\nnent with the highest number of user tracks assigned to,\nwhere we assign a track to the component with the highest\nprobability density for the track. The user is then modeled\nby the characteristics of the selected component (again us-\ning the average and standard deviation across all acoustic\nfeatures of the tracks assigned to the component), whereas\neach track is again represented by its acoustic features.\nContent weighted GMM: This model is again analo-\ngous to the content weighted k-means model. However, we\nrely on a GMM to assign a user’s tracks to certain a com-\nponent as described in the previous model. Based on these\nassignments, we analogously compute the weighted mean\nand standard deviation for each acoustic feature for each\nGMM cluster to represent a user and the characteristics of\ntracks are captured by their acoustic features.\nGMM + content avg, sd: In this model, we com-\nbine the GMM components baseline model with the con-\ntent avg, sd baseline model and hence, represent a user by\nhis/her component weights regarding the Gaussian Mix-\nture Model and further add the average and standard devi-\nation across all acoustic features of the user’s tracks. Sim-\nilarly, a track is represented by its GMM densities and its\nacoustic features.\nWe also performed experiments on representing users\nand tracks with cluster or component assignments only and\ndid an analysis of further combinations of the proposed\nmodels. However, the results were below the evaluated\nbaselines and hence, we do not list these models here.\n5. EXPERIMENTAL SETUP\nWe model the evaluation of the proposed user models as a\nrecommendation task, where we aim to obtain a ranked list\nof tracks that are of interest to the user. For this task, we\nrely on Gradient Boosting Decision Trees. Particularly, we\nutilize the popular XGBoost system [8], a scalable end-\nto-end tree boosting approach that has been shown to be\nhighly suited for recommendation tasks [2, 28]. For the\ntraining phase of the tree, we set the training objective to\nbe the binary classiﬁcation error rate (i.e., the number of\nwrongly classiﬁed tracks in relation to all tracks classi-\nﬁed, where tracks with a predicted probability of relevance\nlarger than 0:5are classiﬁed as relevant for the given user,\nand all other tracks are considered irrelevant for the user).\nPlease note that we deliberately chose a classiﬁcation-\nbased recommendation approach and refrained from uti-\nlizing more elaborate recommender approaches such as\ncontext-aware matrix factorization [3] or tensor-based fac-\ntorization approaches [15] as we aim to focus on user mod-\neling aspects in this paper.\nFor the recommendation task carried out, we require\na rating for each track in the dataset to deﬁne whether a\ngiven track was listened to and thus, considered relevant\nfor a given user. Hence, we add a binary factor rating\nto the processed dataset: for each unique <user; track> -\ncombination, the rating r i;jis 1 if the user uihas listened\nto track tj. Due to a lack of publicly available data, our\ndataset does not contain any implicit feedback of users712 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018(i.e., skipping behavior, session durations or dwell times\nduring browsing the catalog). This is why we cannot esti-\nmate any preference towards a track a user not listened to\nas proposed by [14]. Thus, we assume tracks the user has\nnot listened to as negative examples [14] and hence, assign\na rating of 0 to these tracks. Even though there is a certain\nbias towards negative values as some missing values might\nbe positive, Pan et al. [29] found that this method for rating\nestimation works well. To perform the proposed recom-\nmendation task via classiﬁcation, we require the dataset to\nalso include negative examples. Therefore, for each user,\nwe add random tracks the user did not interact with (i.e.,\ntracks tjwithri;j= 0for the given user ui) to the dataset\nuntil both the training and test sets are ﬁlled with 50% rele-\nvant and 50% non-relevant items. We chose to oversample\nthe positive class to avoid class imbalance and hence, a\nbias towards the negative class.\nUsing the resulting data set, we train a XGBoost model\nthat performs a binary classiﬁcation on the relevance of\ntracks for a given users. We extract the probabilities un-\nderlying the classiﬁcation decision to rank tracks by their\nprobability of relevance in the recommendation task.\nTo evaluate the performance of the proposed user mod-\nels in regards to recommendation quality, we perform a\nper-user evaluation. Therefore, we use each user’s lis-\ntening history and perform a leave-k-out evaluation (also\nknown as hold-out evaluation) [6, 10] per user. Based on\nthe dataset that now contains both positive and negative\nsamples for each user, we compute a hold-out set of size k:\nalong the lines of previous research [12, 13], we randomly\nselect 10 positive samples (tracks that the user has listened\nto) and 100 negative samples (tracks the user has not lis-\ntened to). These 110 tracks form the test set for each user,\nwhereas the recommender system is trained on the remain-\nder of the dataset. We compute the predicted ratings for\nthe tracks in the test set and rank the track recommenda-\ntion candidates w.r.t. the probability that the current track\nbelongs to the positive class in descending order. For our\nexperiments, we consider all predicted probabilities >0:5\nas a predicted interaction and thus, we consider these items\nas relevant, all others as irrelevant and hence, not added to\nthe list of recommendations.6\nBased on the predicted ratings, we compute precision ,\nrecall , and the F1-measure to assess the top-10 accu-\nracy [11]. We evaluate the 10 top ranked tracks as too\nmany track recommendations might provoke choice over-\nload and hence, is not feasible. The problem of choice\noverload has been addressed by Bollen et al. [5] who state\nthat user satisfaction is highest when presenting the user\nwith Top-5 to Top-20 items—naturally assuming that the\nrecommendation list contains a sufﬁcient number of rele-\nvant items for the user. For assessing the overall precision ,\nrecall , andF1-measure of the evaluated recommender sys-\ntems, we compute the measures for each individual user\nand compute the average among all users. For computing\ntherecall measure, all relevant items in the test set are con-\n6This distinction between the two classes is also utilized by XGBoost\nfor binary classiﬁcation tasks based on logistic regression.sidered, independent of the number of recommendations.\nThus, there is a natural cap for recall , namely the num-\nber of recommendations divided by the number of relevant\nitems in the test set.\nFor the tuning of XGBoost parameters, we did a pre-\nliminary cross-evaluation aiming to optimize precision val-\nues for the proposed models and hence, set the number\nof maximum trees to learn the models to 2,000. For all\nother parameters, we relied on the default settings. For\nthe training and tuning of k-means and GMM for the cre-\nation of the user models, we performed the following steps.\nFor k-means, estimated the number of clusters by utiliz-\ning the elbow method based on the within-cluster sum of\nsquares. For the given dataset, we estimated the number of\nclusters to be 5. For the GMM, we performed a training\nphase based on expectation maximization and determined\nthe number of components using the Bayesian Information\nCriterion (BIC), which resulted in a total of 9 components\nfor the GMM.\n6. RESULTS\nWe present the results of our evaluation for a recommen-\ndation list of size ten in Table 3 and in a precision-recall\nplot depicted in Figure 1.\nThe best results are obtained by the GMM + Content\navg, sd model, reaching a precision@10 of 0.771 and a re-\ncall@10 of 0.427 and hence, achieving substantially higher\nprecision and recall scores than any other model. Compar-\ning the results of this model to the GMM model (relying on\nsolely the assignments to GMM components) and the Con-\ntent avg, sd baseline model shows that those two models\nindividually perform substantially worse than when com-\nbined. When inspecting the results of the GMM model,\nwe ﬁnd that solely relying on the GMM density functions\ndoes not sufﬁce to represent a user’s musical taste. Partic-\nularly, all content-based GMM or k-means models achieve\nhigher performance when applied in isolation. However,\ncombining a simple content-based approach that provides\nacoustic features regarding the user’s general preferences,\nwith GMM, provides us with a representative user model.\nThis suggests that the GMM model captures a user’s di-\nverse preferences regarding the detected components and\nhence, his/her distribution in preference towards speciﬁc\ntypes of music, while his/her general preferences are cap-\ntured by the average acoustic features and the according\nstandard deviation.\nModel Prec Rec F 1\nGMM + Content avg, sd 0.771 0.427 0.632\nContent k-means weighted 0.606 0.316 0.400\nContent k-means binary 0.573 0.300 0.383\nContent binary GMM 0.569 0.298 0.381\nContent weighted GMM 0.569 0.298 0.381\nGMM 0.231 0.122 0.226\nContent avg, sd 0.161 0.089 0.241\nContent avg 0.159 0.087 0.241\nTable 3 . Precision, Recall and F 1@10, ordered by F 1.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 713Our results also show that the user models based on\nk-means clusters slightly outperform the methods based\non GMM components (1.8% in recall, 3.7% in precision).\nPlease note that for k-means we determined the number of\nclusters to be ﬁve, whereas we created nine GMM compo-\nnents (as described in Section 5). Our ﬁndings regarding\nthe number of clusters are also in line with previous analy-\nses on playlists [30], where the authors found that cluster-\ning the tracks within playlists into ﬁve clusters allows for\ncohesive and homogeneous clusters.\nThe weighted k-means approach achieves better results\nthan the binary k-means approach. This seems natural as\nthe former incorporates the user’s membership in all clus-\nters, whereas the latter does a majority vote and utilizes the\nresulting (single) cluster to characterize the user. However,\nthis does not hold for the GMM-based approaches. While\nthe differences between the weighted and binary k-means\napproaches are marginal, for GMM there is no difference\nbetween weighted and binary Content GMM.\nThe proposed baseline model Content avg achieves the\nlowest values regarding recall, precision and F 1. Adding\nthe standard deviation to this model hardly impacts the\nresults. We initially suspected that adding the SD to the\nmodel may allow mitigating the effects of aggregating pos-\nsibly highly different tracks as we aggregate across all\ntracks of a user (regarding their acoustic features), how-\never, this is not conﬁrmed by our experiments. In pre-\nliminary experiments, we also used different representa-\ntions of clusters: while we now utilize the mean acous-\ntic features and the according SDs, we also used only the\nmean features. We found that the SD contributes only\nmarginally as the dispersion of tracks in regards to acous-\ntic features is already captured by the individual clus-\nters/components and hence, the tracks contained in a sin-\ngle cluster/component are more homogeneous. We also\nexperimented with models that utilize user-cluster assign-\nments for k-means, however, those models achieved in-\nferior results. In contrast, representing those clusters by\nthe average acoustic features across all contained tracks\nseems to be representative. Combining k-means cluster\nassignments with content-based models also lead to infe-\nrior results, which we lead back to the fact that the GMM\nprobability densities provide more information than sheer\ncluster-assignments.\nGenerally, we conclude that content features strongly\ncontribute to user models and that grouping tracks into\nclusters (k-means) or components (GMM) and solely re-\nlying on the assignment to those clusters or components\nis not sufﬁcient for a representative user model. Finding\ngroups of similar tracks to represent users by user-group\nassignments via the tracks a user listened to is not expres-\nsive enough. Naturally, utilizing content features allows\nto compute higher-dimensional similarities between users\nand their tracks (in our experiments, 8 dimensions) and\nhence, a more ﬁne-grained notion of similarity.\n0.20.40.60.8\n0.0 0.2 0.4 0.6 0.8\nRecallPrecision GMM + Content avg, sd\nContent weighted k−means\nContent binary k−means\nContent binary GMM\nContent weighted GMM\nContent avg\nContent avg, sd\nGMMFigure 1 . Precision-Recall curves for all models.\n7. DISCUSSION\nWe ﬁnd that a GMM that captures the speciﬁc preferences\nof a user towards a set of nine types of music (captured\nby nine GMM components) complemented by the general\nmusical preference of a user (captured by the avg. acoustic\nfeatures of his/her tracks) provides the best results.\nRegarding the limitations of this study, we note that the\ncontent descriptors utilized are aggregated high-level fea-\ntures. This allowed us to keep the feature space smaller and\nto speciﬁcally focus on the user modeling aspects. Further-\nmore, this evaluation is solely based on aspects related to\nthe content of tracks and no further user-related aspects as\ne.g., proposed by Schedl et al. [34]. Lastly, while the pro-\nposed models characterize users based on their interest in\ndifferent clusters/components and hence, are able to build\nmore speciﬁc user models, we still represent each clus-\nter/component by the mean acoustic features of the tracks\ncontained, which naturally limits the user model’s speci-\nﬁcity. However, we believe that our ﬁndings are a valuable\ncontribution to advance user modeling for MIR and MRS\nand to foster further research in this direction.\n8. CONCLUSION AND FUTURE WORK\nWe proposed and evaluated a set of user models for de-\nscribing the musical preference of users by leveraging con-\ntent descriptors of tracks the user has listened to. We ﬁnd\nthat a GMM complemented by the user’s general musi-\ncal preferences describes a user’s different musical pref-\nerences best. We believe that our ﬁndings can contribute\nto improved user models for music recommender systems\nand generally, MIR tasks. In future work, we aim to in-\nvestigate methods to combine the models evaluated by\ne.g., ensemble methods. Furthermore, we aim to tackle\nthe problem that our current model still computes average\nacoustic features across a large number of tracks.714 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20189. REFERENCES\n[1] Jesper Steen Andersen. Using the echo nest’s automat-\nically extracted music features for a musicological pur-\npose. In 2014 4th International Workshop on Cognitive\nInformation Processing (CIP) , pages 1–6, 2014.\n[2] Takashi Ayaki, Hidekazu Yanagimoto, and Michi-\nfumi Yoshioka. Recommendation from access logs\nwith ensemble learning. Artiﬁcial Life and Robotics ,\n22(2):163–167, 2017.\n[3] Linas Baltrunas, Bernd Ludwig, and Francesco Ricci.\nMatrix factorization techniques for context aware rec-\nommendation. In Proceedings of the Fifth ACM Con-\nference on Recommender Systems , pages 301–304.\nACM, 2011.\n[4] Dmitry Bogdanov, Mart ´ın Haro, Ferdinand Fuhrmann,\nEmilia G ´omez, and Perfecto Herrera. Content-based\nmusic recommendation based on user preference ex-\namples. In 4th ACM Conference on Recommender Sys-\ntems. Workshop on Music Recommendation and Dis-\ncovery (Womrad 2010) , page 33, 2010.\n[5] Dirk Bollen, Bart P. Knijnenburg, Martijn C. Willem-\nsen, and Mark Graus. Understanding choice overload\nin recommender systems. In Proceedings of the Fourth\nACM Conference on Recommender Systems , pages 63–\n70, New York, NY , USA, 2010. ACM.\n[6] John S. Breese, David Heckerman, and Carl Kadie.\nEmpirical analysis of predictive algorithms for col-\nlaborative ﬁltering. In Proceedings of the Fourteenth\nConference on Uncertainty in Artiﬁcial Intelligence ,\nUAI’98, pages 43–52. Morgan Kaufmann Publishers\nInc., 1998.\n[7] Robin Burke. Hybrid recommender systems: Survey\nand experiments. User modeling and user-adapted in-\nteraction , 12(4):331–370, 2002.\n[8] Tianqi Chen and Carlos Guestrin. XGBoost: A Scal-\nable Tree Boosting System. In Proceedings of the 22nd\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining , pages 785–794.\nACM, 2016.\n[9] Parag Chordia, Mark Godfrey, and Alex Rae. Extend-\ning content-based recommendation: The case of indian\nclassical music. In Eight International Society for Mu-\nsic Information Retrieval Conference , pages 571–576,\n2008.\n[10] P. Cremonesi, R. Turrin, E. Lentini, and M. Mat-\nteucci. An evaluation methodology for collaborative\nrecommender systems. In 2008 International Confer-\nence on Automated Solutions for Cross Media Content\nand Multi-Channel Distribution , pages 224–231, 2008.\n[11] Paolo Cremonesi, Yehuda Koren, and Roberto Turrin.\nPerformance of recommender algorithms on top-n rec-\nommendation tasks. In Proceedings of the Fourth ACMConference on Recommender Systems , pages 39–46,\nNew York, NY , USA, 2010. ACM.\n[12] Ali Mamdouh Elkahky, Yang Song, and Xiaodong He.\nA multi-view deep learning approach for cross do-\nmain user modeling in recommendation systems. In\nProceedings of the 24th International Conference on\nWorld Wide Web , pages 278–288, 2015.\n[13] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie,\nXia Hu, and Tat-Seng Chua. Neural collaborative ﬁl-\ntering. In Proceedings of the 26th International Con-\nference on World Wide Web , pages 173–182, 2017.\n[14] Yifan Hu, Yehuda Koren, and Chris V olinsky. Collab-\norative ﬁltering for implicit feedback datasets. In 2008\nEighth IEEE International Conference on Data Min-\ning, pages 263–272. IEEE, 2008.\n[15] Alexandros Karatzoglou, Xavier Amatriain, Linas Bal-\ntrunas, and Nuria Oliver. Multiverse recommendation:\nn-dimensional tensor factorization for context-aware\ncollaborative ﬁltering. In Proceedings of the Fourth\nACM Conference on Recommender Systems , pages 79–\n86. ACM, 2010.\n[16] Yehuda Koren and Robert Bell. Advances in collab-\norative ﬁltering. In Recommender Systems Handbook ,\npages 145–186. Springer US, Boston, MA, 2011.\n[17] Yehuda Koren, Robert Bell, and Chris V olinsky. Ma-\ntrix factorization techniques for recommender systems.\nComputer , 42(8):30–37, August 2009.\n[18] Mark Levy and Mark Sandler. Learning latent seman-\ntic models for music from social tags. Journal of New\nMusic Research , 37(2):137–150, 2008.\n[19] Christophe Leys, Christophe Ley, Olivier Klein,\nPhilippe Bernard, and Laurent Licata. Detecting out-\nliers: Do not use standard deviation around the mean,\nuse absolute deviation around the median. Journal\nof Experimental Social Psychology , 49(4):764 – 766,\n2013.\n[20] Ning-Han Liu. Comparison of content-based music\nrecommendation using different distance estimation\nmethods. Applied Intelligence , 38(2):160–174, Mar\n2013.\n[21] Beth Logan et al. Mel frequency cepstral coefﬁcients\nfor music modeling. In Proceedings of the 1st Inter-\nnational Symposium on Music Information Retrieval ,\nvolume 270, pages 1–11, 2000.\n[22] Pasquale Lops, Marco de Gemmis, and Giovanni Se-\nmeraro. Content-based recommender systems: State\nof the art and trends. In Recommender Systems Hand-\nbook , pages 73–105. Springer US, Boston, MA, 2011.\n[23] L. McInnes and J. Healy. UMAP: Uniform Manifold\nApproximation and Projection for Dimension Reduc-\ntion. ArXiv e-prints , 2018.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 715[24] Geoffrey McLachlan and David Peel. Finite mixture\nmodels . Wiley Series in Probability and Statistics,\n2004.\n[25] Matt McVicar, Tim Freeman, and Tijl De Bie. Min-\ning the correlation between lyrical and audio features\nand the emergence of mood. In Proceedings of the 12th\nInternational Society for Music Information Retrieval\nConference , pages 783–788, 2011.\n[26] Joshua L Moore, Shuo Chen, Thorsten Joachims, and\nDouglas Turnbull. Learning to embed songs and tags\nfor playlist prediction. In Twelth International Soci-\nety on Music Information Retrieval Conference , pages\n349–354, 2012.\n[27] Joshua L Moore, Thorsten Joachims, and Douglas\nTurnbull. Taste space versus the world: an embedding\nanalysis of listening habits and geography. In Four-\nteenth International Society for Music Information Re-\ntrieval Conference , pages 439–444, 2014.\n[28] Andrzej Pacuk, Piotr Sankowski, Karol Wegrzycki,\nAdam Witkowski, and Piotr Wygocki. Job recommen-\ndations based on preselection of offers and gradient\nboosting. In Proceedings of the Recommender Systems\nChallenge , pages 10:1–10:4. ACM, 2016.\n[29] Rong Pan, Yunhong Zhou, Bin Cao, Nathan N. Liu,\nRajan Lukose, Martin Scholz, and Qiang Yang. One-\nclass collaborative ﬁltering. In Proceedings of the 2008\nEighth IEEE International Conference on Data Min-\ning, pages 502–511, 2008.\n[30] Martin Pichl, Eva Zangerle, and Gnther Specht. Un-\nderstanding playlist creation on music streaming plat-\nforms. In IEEE International Symposium on Multime-\ndia, pages 475–480. IEEE Computer Society, 2016.\n[31] Markus Schedl. The LFM-1B Dataset for Music Re-\ntrieval and Recommendation. In Proceedings of the\n2016 ACM on International Conference on Multimedia\nRetrieval , pages 103–110, New York, NY , USA, 2016.\nACM.\n[32] Markus Schedl and Arthur Flexer. Putting the user in\nthe center of music information retrieval. In Twelth\nInternational Society for Music Information Retrieval\nConference , pages 385–390. Citeseer, 2012.\n[33] Markus Schedl, Arthur Flexer, and Juli ´an Urbano. The\nneglected user in music information retrieval research.\nJournal of Intelligent Information Systems , 41(3):523–\n539, 2013.\n[34] Markus Schedl, Peter Knees, and Fabien Gouyon. New\npaths in music recommender systems research. In Pro-\nceedings of the Eleventh ACM Conference on Recom-\nmender Systems , pages 392–393. ACM, 2017.\n[35] B. Shao, D. Wang, T. Li, and M. Ogihara. Mu-\nsic recommendation based on acoustic features and\nuser access patterns. IEEE Transactions on Audio,Speech, and Language Processing , 17(8):1602–1611,\nNov 2009.\n[36] Xinxi Wang, David Rosenblum, and Ye Wang.\nContext-aware mobile music recommendation for\ndaily activities. In Proceedings of the 20th ACM Inter-\nnational Conference on Multimedia , MM ’12, pages\n99–108, New York, NY , USA, 2012. ACM.\n[37] Xinxi Wang and Ye Wang. Improving content-based\nand hybrid music recommendation using deep learn-\ning. In Proceedings of the 22nd ACM International\nConference on Multimedia , pages 627–636, New York,\nNY , USA, 2014. ACM.\n[38] Kazuyoshi Yoshii, Masataka Goto, Kazunori Ko-\nmatani, Tetsuya Ogata, and Hiroshi G Okuno. Hybrid\ncollaborative and content-based music recommenda-\ntion using probabilistic model with latent user prefer-\nences. In Seventh International Society for Music In-\nformation Retrieval Conference , 2006.\n[39] Bingjun Zhang, Jialie Shen, Qiaoliang Xiang, and\nYe Wang. Compositemap: A novel framework for mu-\nsic similarity measure. In Proceedings of the 32nd In-\nternational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval , pages 403–410,\nNew York, NY , USA, 2009. ACM.716 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018, Paris, France, September 23-27, 2018",
        "author": [
            "Emilia Gómez",
            "Xiao Hu 0001",
            "Eric Humphrey",
            "Emmanouil Benetos"
        ],
        "year": "2018",
        "doi": "10.5281/zenodo.1432913",
        "url": "https://doi.org/10.5281/zenodo.1432913",
        "ee": null,
        "abstract": "The OpenMIC-2018 dataset is made available througha collaboration between Spotifyand MARL@NYU. Additionally, the cost of annotation was sponsored by Spotify, whose contributions to open-source research can be found online at the developer site, engineering blog, and public GitHub.\n\nIf you use this dataset, please cite the following work:\n\n\nHumphrey, Eric J., Durand, Simon, and McFee, Brian. OpenMIC-2018: An Open Dataset for Multiple Instrument Recognition. in Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR), 2018. [pdf]\n\n\nThe dataset is made available by Spotify ABunder a Creative Commons Attribution 4.0 International (CC BY 4.0)license. The full terms of this license are included alongside this dataset.\n\nThis dataset contains the following:\n\n\n\t10 second snippets of audio, in a directory format like &#39;audio/{0:3}/{0}.ogg&#39;.format(sample_key)\n\tVGGish features as JSON objects,in a directory format like &#39;vggish/{0:3}/{0}.json&#39;.format(sample_key)\n\tMD5 checksums for each OGG and JSON file\n\tAnonymized individual responses, in &#39;openmic-2018-individual-responses.csv&#39;\n\tAggregated labels, in &#39;openmic-2018-aggregated-labels.csv&#39;\n\tTrack metadata, with licenses for each audio recording, in &#39;openmic-2018-metadata.csv&#39;\n\tA Python-friendly NPZ file of features and labels, &#39;openmic-2018.npz&#39;\n\tSample partitions for train and test, in &#39;partitions/*.txt&#39;\n",
        "zenodo_id": 1432913,
        "dblp_key": "conf/ismir/2018"
    }
]