[
    {
        "title": "Theory and Evaluation of a Bayesian Music Structure Extractor.",
        "author": [
            "Samer A. Abdallah",
            "Katy C. Noland",
            "Mark B. Sandler",
            "Michael A. Casey",
            "Christophe Rhodes"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416018",
        "url": "https://doi.org/10.5281/zenodo.1416018",
        "ee": "https://zenodo.org/records/1416018/files/AbdallahNSCR05.pdf",
        "abstract": "We introduce a new model for extracting classified structural segments, such as intro, verse, chorus, break and so forth, from recorded music. Our approach is to classify signal frames on the basis of their audio properties and then to agglomerate contiguous runs of similarly classified frames into texturally homogenous (or \u2018self-similar\u2019) segments which inherit the classificaton of their consituent frames. Our work extends previous work on automatic structure extraction by addressing the classification problem using using an unsupervised Bayesian clustering model, the parameters of which are estimated using a variant of the expectation maximisation (EM) algorithm which includes deterministic annealing to help avoid local optima. The model identifies and classifies all the segments in a song, not just the chorus or longest segment. We discuss the theory, implementation, and evaluation of the model, and test its performance against a ground truth of human judgements. Using an analogue of a precisionrecall graph for segment boundaries, our results indicate an optimal trade-off point at approximately 80% precision for 80% recall. Keywords: structure, segmentation, boundary, audio 1",
        "zenodo_id": 1416018,
        "dblp_key": "conf/ismir/AbdallahNSCR05"
    },
    {
        "title": "Iterative Deepening for Melody Alignment and Retrieval.",
        "author": [
            "Norman H. Adams",
            "Daniela Marquez",
            "Gregory H. Wakefield"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415712",
        "url": "https://doi.org/10.5281/zenodo.1415712",
        "ee": "https://zenodo.org/records/1415712/files/AdamsMW05.pdf",
        "abstract": "For melodic theme retrieval there is a fundamental tradeoff between retrieval performance and retrieval speed. Melodic representations of large dimension yield the best retrieval performance, but at high computational cost, and vice versa. In the present work we explore the use of iterative deepening to achieve robust retrieval performance, but without the accompanying computational burden. In particular, we propose the use of a smooth pitch contour that facilitates query and target representations of variable length. We implement an iterative query-by-humming system that yields a dramatic increase in speed, without degrading performance compared to contemporary retrieval systems. Furthermore, we expand the conventional iterative framework to retain the alignment paths found in each iteration. These alignment paths are used to adapt the alignment window of subsequent iterations, further expediting retrieval without degrading performance. Keywords: Iterative deepening, DTW, melody retrieval. 1",
        "zenodo_id": 1415712,
        "dblp_key": "conf/ismir/AdamsMW05"
    },
    {
        "title": "The CLAM Annotator: A Cross-Platform Audio Descriptors Editing Tool.",
        "author": [
            "Xavier Amatriain",
            "Jordi Massaguer",
            "David Garc\u00eda",
            "Ismael Mosquera"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416908",
        "url": "https://doi.org/10.5281/zenodo.1416908",
        "ee": "https://zenodo.org/records/1416908/files/AmatriainMGM05.pdf",
        "abstract": "This paper presents the CLAM Annotator tool. This application has been developed in the context of the CLAM framework and can be used to manually edit any previously computed audio descriptors. The application offers a convenient GUI that allows to edit low-level frame descriptors, global descriptors of any kind and segmentation marks. It is designed in such a way that the interface adapts itself to a user-defined schema, offering possibilities to a large range of applications. Keywords: Audio Descriptors, XML, Annotating Tool 1",
        "zenodo_id": 1416908,
        "dblp_key": "conf/ismir/AmatriainMGM05"
    },
    {
        "title": "Ringomatic: A Real-Time Interactive Drummer Using Constraint-Satisfaction and Drum Sound Descriptors.",
        "author": [
            "Jean-Julien Aucouturier",
            "Fran\u00e7ois Pachet"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416532",
        "url": "https://doi.org/10.5281/zenodo.1416532",
        "ee": "https://zenodo.org/records/1416532/files/AucouturierP05.pdf",
        "abstract": "We describe a real-time musical agent that generates an audio drum-track by concatenating audio segments automatically extracted from pre-existing musical files. The drum-track can be controlled in real-time by specifying high-level properties (or constraints) holding on metadata automatically extracted from the audio segments. A constraint-satisfaction mechanism, based on local search, selects audio segments that best match those constraints at any time. We report on several drum track audio descriptors designed for the system. We also describe a basic mecanism for controlling the tradeoff between the agent\u2019s autonomy and reactivity, which we illustrate with experiments made in the context of a virtual duet between the system and a human pianist. Keywords: interaction, drumtrack, metadata, constraint satisfaction, concatenative synthesis 1",
        "zenodo_id": 1416532,
        "dblp_key": "conf/ismir/AucouturierP05"
    },
    {
        "title": "Design of a Digital Music Stand.",
        "author": [
            "Tim Bell 0001",
            "David Blizzard",
            "Richard D. Green",
            "David Bainbridge 0001"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416290",
        "url": "https://doi.org/10.5281/zenodo.1416290",
        "ee": "https://zenodo.org/records/1416290/files/BellBGB05.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1416290,
        "dblp_key": "conf/ismir/BellBGB05"
    },
    {
        "title": "A Robust Mid-Level Representation for Harmonic Content in Music Signals.",
        "author": [
            "Juan Pablo Bello",
            "Jeremy Pickens"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417431",
        "url": "https://doi.org/10.5281/zenodo.1417431",
        "ee": "https://zenodo.org/records/1417431/files/BelloP05.pdf",
        "abstract": "When considering the problem of audio-to-audio matching, determining musical similarity using low-level features such as Fourier transforms and MFCCs is an extremely difficult task, as there is little semantic information available. Full semantic transcription of audio is an unreliable and imperfect task in the best case, an unsolved problem in the worst. To this end we propose a robust mid-level representation that incorporates both harmonic and rhythmic information, without attempting full transcription. We describe a process for creating this representation automatically, directly from multi-timbral and polyphonic music signals, with an emphasis on popular music. We also offer various evaluations of our techniques. Moreso than most approaches working from raw audio, we incorporate musical knowledge into our assumptions, our models, and our processes. Our hope is that by utilizing this notion of a musically-motivated mid-level representation we may help bridge the gap between symbolic and audio research. Keywords: Harmonic description, segmentation, music similarity 1",
        "zenodo_id": 1417431,
        "dblp_key": "conf/ismir/BelloP05"
    },
    {
        "title": "Scalable Metadata and Quick Retrieval of Audio Signals.",
        "author": [
            "Nancy Bertin",
            "Alain de Cheveign\u00e9"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417079",
        "url": "https://doi.org/10.5281/zenodo.1417079",
        "ee": "https://zenodo.org/records/1417079/files/BertinC05.pdf",
        "abstract": "Audio search algorithms have reached a degree of speed and accuracy that allows them to search efficiently within large databases of audio. For speed, algorithms generally depend on precalculated indexing metadata. Unfortunately, the size of the metadata follows the same exponential trend as the audio data itself, and this may lead to an exponential increase in storage cost and search time. The concept of scalable metadata has been introduced to allow metadata to adjust to such trends and alleviate the effects of forseeable increases of data and metadata size. Here, we argue that scalability fits the needs of the hierarchical structures that allow fast search, and illustrate this by adapting a state-of-the-art search algorithm to a scalable indexing structure. Scalability allows search algorithms to adapt to the increase of database size without loss of performance. Keywords: Search, indexing, scalability, audio retrieval, scalable metadata. 1",
        "zenodo_id": 1417079,
        "dblp_key": "conf/ismir/BertinC05"
    },
    {
        "title": "Distributed Audio Feature Extraction for Music.",
        "author": [
            "Stuart Bray",
            "George Tzanetakis"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417563",
        "url": "https://doi.org/10.5281/zenodo.1417563",
        "ee": "https://zenodo.org/records/1417563/files/BrayT05.pdf",
        "abstract": "One of the important challenges facing music information retrieval (MIR) of audio signals is scaling analysis algorithms to large collections. Typically, analysis of audio signals utilizes sophisticated signal processing and machine learning techniques that require significant computational resources. Therefore, audio MIR is an area were computational resources are a significant bottleneck. For example, the number of pieces utilized in the majority of existing work in audio MIR is at most a few thousand files. Computing audio features over thousands files can sometimes take days of processing. In this paper, we describe how Marsyas-0.2, a free software framework for audio analysis and synthesis can be used to rapidly implement efficient distributed audio analysis algorithms. The framework is based on a dataflow architecture which facilitates partitioning of audio computations over multiple computers. Experimental results demonstrating the effectiveness of the proposed approach are presented. Keywords: distributed processing, dataflow networks, large-scale music information retrieval 1",
        "zenodo_id": 1417563,
        "dblp_key": "conf/ismir/BrayT05"
    },
    {
        "title": "Learning Harmonic Relationships in Digital Audio with Dirichlet-Based Hidden Markov Models.",
        "author": [
            "John Ashley Burgoyne",
            "Lawrence K. Saul"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414870",
        "url": "https://doi.org/10.5281/zenodo.1414870",
        "ee": "https://zenodo.org/records/1414870/files/BurgoyneS05.pdf",
        "abstract": "Harmonic analysis is a standard musicological tool for understanding many pieces of Western classical music and making comparisons among them. Traditionally, this analysis is done on paper scores, and most past research in machine-assisted analysis has begun with digital representations of them. Human music students are also taught to hear their musical analyses, however, in both musical recordings and performances. Our approach attempts to teach machines to do the same, beginning with a corpus of recorded Mozart symphonies. The audio files are first transformed into an ordered series of normalized pitch class profile (PCP) vectors. Simplified rules of tonal harmony are encoded in a transition matrix. Classical music tends to change key more frequently than popular music, and so these rules account not only for chords, as most previous work has done, but also for the keys in which they function. A hidden Markov model (HMM) is used with this transition matrix to train Dirichlet distributions for major and minor keys on the PCP vectors. The system tracks chords and keys successfully and shows promise for a real-time implementation. Keywords: Dirichlet, harmony, PCP, HMM, Mozart 1",
        "zenodo_id": 1414870,
        "dblp_key": "conf/ismir/BurgoyneS05"
    },
    {
        "title": "Automatic X Traditional Descriptor Extraction: the Case of Chord Recognition.",
        "author": [
            "Giordano Cabral 0001",
            "Fran\u00e7ois Pachet",
            "Jean-Pierre Briot"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415702",
        "url": "https://doi.org/10.5281/zenodo.1415702",
        "ee": "https://zenodo.org/records/1415702/files/CabralPB05.pdf",
        "abstract": "Audio descriptor extraction is the activity of finding mathematical models which describe properties of the sound, requiring signal processing skills. The scientific literature presents a vast collection of descriptors (e.g. energy, tempo, tonality) each one representing a significant effort of research in finding an appropriate descriptor for a particular application. The Extractor Discovery System (EDS)  [1] is a recent approach for the discovery of such descriptors, which aim is to extract them automatically. This system can be useful for both non experts \u2013 who can let the system work fully automatically \u2013 and experts \u2013 who can start the system with an initial solution expecting it to enhance their results. Nevertheless, EDS still needs to be massively tested. We consider that its comparison with the results of problems already studied would be very useful to validate it as an effective tool. This work intends to perform the first part of this validation, comparing the results from classic approaches with EDS results when operated by a completely na\u00efve user building a guitar chord recognizer.",
        "zenodo_id": 1415702,
        "dblp_key": "conf/ismir/CabralPB05"
    },
    {
        "title": "Melodic Similarity Algorithms -Using Similarity Ratings for Development and Early Evaluation.",
        "author": [
            "Margaret Cahill",
            "Donncha \u00d3 Maid\u00edn"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415642",
        "url": "https://doi.org/10.5281/zenodo.1415642",
        "ee": "https://zenodo.org/records/1415642/files/CahillM05.pdf",
        "abstract": "This paper focuses on gathering similarity ratings for use in the construction, optimization and evaluation of melodic similarity algorithms. The approach involves conducting listening experiments to gather these ratings for a piece in Theme and Variation form.",
        "zenodo_id": 1415642,
        "dblp_key": "conf/ismir/CahillM05"
    },
    {
        "title": "A Pattern Extraction Algorithm for Abstract Melodic Representations that Allow Partial Overlapping of Intervallic Categories.",
        "author": [
            "Emilios Cambouropoulos",
            "Maxime Crochemore",
            "Costas S. Iliopoulos",
            "Manal Mohamed 0001",
            "Marie-France Sagot"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415008",
        "url": "https://doi.org/10.5281/zenodo.1415008",
        "ee": "https://zenodo.org/records/1415008/files/CambouropoulosCIMS05.pdf",
        "abstract": "REPRESENTATIONS THAT ALLOW PARTIAL OVERLAPPING OF INTERVALLIC CATEGORIES Emilios Cambouropoulos1, Maxime Crochemore2,3, Costas Iliopoulos3, Manal Mohamed3, Marie-France Sagot4 1 Department of Music Studies, University of Thessaloniki, 540006, Thessaloniki, Greece emilios@mus.auth.gr 2 Institut Gaspard-Monge, University of Marne-la-Vall\u00b4ee, 77454 Marne-la-Vall\u00b4ee CEDEX 2, France maxime.crochemore@univ-mlv.fr 3 Department of Computer Science, King\u2019s College London, London WC2R 2LS, England {mac,csi,manal}@dcs.kcl.ac.uk 4 INRIA Rh\u02c6one-Alpes, Universit\u00b4e Claude Bernard, 43 Bd du 11 novembre 1918, 69622 Villeurbanne cedex, France Marie-France.Sagot@inria.fr This paper proposes an efficient pattern extraction algorithm that can be applied on melodic sequences that the melodic representation introduces special \u201cdon\u2019t care\u201d symbols for intervals that may belong to two partially overlapping intervallic categories. As a special case the well established \u201cstep-leap\u201d representation is examined. In the step-leap representation, each melodic diatonic interval is classified as a step (\u00b1s), a leap (\u00b1l) or a unison (u). Binary don\u2019t care symbols are introduced to reprecategories e.g. \u2217= s, \u2217= l and # = \u2212s, # = \u2212l. For such a sequence, we are interested in finding maximal repeating pairs and repetitions with a hole (two matching subsequences separated with an intervening non-matching symbol). We propose an O(n + d(n \u2212d) + z)-time algorithm for computing all such repetitions in a given sequence x = x[1..n] with d binary don\u2019t care symbols, where z is the output size. Keywords: string, don\u2019t care, repetitions, suffix tree, lowest common ancestor. 1",
        "zenodo_id": 1415008,
        "dblp_key": "conf/ismir/CambouropoulosCIMS05"
    },
    {
        "title": "On Tuning the (\\delta, \\alpha)-Sequential-Sampling Algorithm for \\delta-Approximate Matching with Alpha-Bounded Gaps in Musical Sequences.",
        "author": [
            "Domenico Cantone",
            "Salvatore Cristofaro",
            "Simone Faro"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416892",
        "url": "https://doi.org/10.5281/zenodo.1416892",
        "ee": "https://zenodo.org/records/1416892/files/CantoneCF05a.pdf",
        "abstract": "The \u03b4-approximate matching problem arises in many questions concerning musical information retrieval and musical analysis. In the case in which gaps are not allowed between consecutive pitches of the melody, transposition invariance is automatically taken care of, provided that the musical melodies are encoded using the pitch interval encoding. However, in the case in which nonnull gaps are allowed between consecutive pitches of the melodies, transposition invariance is not dealt with properly by the algorithms present in literature. In this paper, we propose two slightly different variants of the approximate matching problem under transposition invariance and for each of them provide an algorithm, obtained by adapting an efficient algorithm for the \u03b4-approximate matching problem with \u03b1-bounded gaps. Keywords: approximate string matching, musical information retrieval, experimental algorithms. 1",
        "zenodo_id": 1416892,
        "dblp_key": "conf/ismir/CantoneCF05"
    },
    {
        "title": "Solving the (\\delta, \\alpha)-Approximate Matching Problem Under Transposition Invariance in Musical Sequences.",
        "author": [
            "Domenico Cantone",
            "Salvatore Cristofaro",
            "Simone Faro"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.11202953",
        "url": "https://doi.org/10.5281/zenodo.11202953",
        "ee": "http://ismir2005.ismir.net/proceedings/2070.pdf",
        "abstract": "The academic librarian's role shifting from administrative staff providing access to scientific information to training staff in the management of that content is increasingly identified as resources digital dissemination has been established, especially under Open Access Dissemination arrangement. The role of the information professional as an educator is expanding to that of a producer of learning material for the management of scientific information, a teacher trainer and an educational partner, making a substantially higher quality contribution to achieving the objectives set by the Institutions. Information literacy skills are placed at the heart of academic skills development models, as the educational and research process and performance is qualitatively determined by the management of information needs. The educational work of academic librarians and their related collaborations has been explored for many years. Positive results of these collaborations have been documented, but still the need for teaching competence of information professionals is highlighted with particular emphasis. This article discusses the role of the academic librarian as an information content producer and as a teaching partner. At the same time, special reference is made to challenges that sometimes act as an impediment to the success of collaborative activities between libraries and educational staff. The Library of the University of Ioannina responded to an invitation for cooperation from the Center for Teaching and Learning Support (KEDIMA) to develop learning content in the context of the goals and needs of KEDIMA. With the main objective of mutual contribution, a memorandum of understanding was signed in which a model of cooperation between an academic library and a learning and teaching unit was defined. This is a recent collaborative action, which is evolving dynamically and reinforces the optimistic scenario for consolidating the role of the Library as a producer of learning content and a teaching partner.",
        "zenodo_id": 11202953,
        "dblp_key": "conf/ismir/CantoneCF05a"
    },
    {
        "title": "Frame-Level Audio Feature Extraction Using AdaBoost.",
        "author": [
            "Norman Casagrande",
            "Douglas Eck",
            "Bal\u00e1zs K\u00e9gl"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414718",
        "url": "https://doi.org/10.5281/zenodo.1414718",
        "ee": "https://zenodo.org/records/1414718/files/CasagrandeEK05.pdf",
        "abstract": "In this paper we adapt an AdaBoost-based image processing algorithm to the task of predicting whether an audio signal contains speech or music. We derive a frame-level discriminator that is both fast and accurate. Using a simple FFT and no built-in prior knowledge of signal structure we obtain an accuracy of 88% on frames sampled at 20ms intervals. When we smooth the output of the classifier with the output of the previous 40 frames our forecast rate rises to 93% on the Scheirer-Slaney (Scheirer and Slaney, 1997) database. To demonstrate the efficiency and effectiveness of the model, we have implemented it as a graphical real-time plugin to the popular Winamp audio player. 1",
        "zenodo_id": 1414718,
        "dblp_key": "conf/ismir/CasagrandeEK05"
    },
    {
        "title": "Foafing the Music: A Music Recommendation System based on RSS Feeds and User Preferences.",
        "author": [
            "\u00d2scar Celma",
            "Miquel Ram\u00edrez",
            "Perfecto Herrera"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414800",
        "url": "https://doi.org/10.5281/zenodo.1414800",
        "ee": "https://zenodo.org/records/1414800/files/CelmaRH05.pdf",
        "abstract": "In this paper we give an overview of the Foafing the Music system. The system uses the Friend of a Friend (FOAF) and Rich Site Summary (RSS) vocabularies for recommending music to a user, depending on her musical tastes. Music information (new album releases, related artists\u2019 news and available audio) is gathered from thousands of RSS feeds \u2014an XML format for syndicating Web content. On the other hand, FOAF documents are used to define user preferences. The presented system provides music discovery by means of: user profiling \u2014defined in the user\u2019s FOAF description\u2014, context-based information \u2014extracted from music related RSS feeds\u2014 and content-based descriptions \u2014extracted from the audio itself. 1",
        "zenodo_id": 1414800,
        "dblp_key": "conf/ismir/CelmaRH05"
    },
    {
        "title": "Detection of Key Change in Classical Piano Music.",
        "author": [
            "Wei Chai",
            "Barry Vercoe"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415538",
        "url": "https://doi.org/10.5281/zenodo.1415538",
        "ee": "https://zenodo.org/records/1415538/files/ChaiV05.pdf",
        "abstract": "Tonality is an important aspect of musical structure. Detecting key of music is one of the major tasks in tonal analysis and will benefit semantic segmentation of music for indexing and searching. This paper presents an HMM-based approach for segmenting musical signals based on key change and identifying the key of each segment. Classical piano music was used in the experiment. The performance, evaluated by three proposed measures (recall, precision and label accuracy), demonstrates the promise of the method.",
        "zenodo_id": 1415538,
        "dblp_key": "conf/ismir/ChaiV05"
    },
    {
        "title": "Segmentation and Recognition of Tabla Strokes.",
        "author": [
            "Parag Chordia"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416000",
        "url": "https://doi.org/10.5281/zenodo.1416000",
        "ee": "https://zenodo.org/records/1416000/files/Chordia05.pdf",
        "abstract": "A system that segments and labels tabla strokes from real performances is described. Performance is evaluated on a large database taken from three performers under different recording conditions, containing a total of 16,834 strokes. The current work extends previous work by Gillet and Richard (2003) on categorizing tabla strokes, by using a larger, more diverse database that includes their data as a benchmark, and by testing neural networks and treebased classification methods. First, the time-domain signal was segmented using complex-domain thresholding that looked for sudden changes in amplitude and phase discontinuities. At the optimal point on the ROC curve, false positives were less than 1% and false negatives were less than 2%. Then, classification was performed using a multivariate Gaussian model (mv gauss) as well as non-parametric techniques such as probabilistic neural networks (pnn), feed-forward neural networks (ffnn), and tree-based classifiers. Two evaluation protocols were used. The first used 10-fold cross validation. The recognition rate averaged over several experiments that contained 10-15 classes was 92% for the mv gauss, 94% for the ffnn and pnn, and 84% for the tree based classifier. To test generalization, a more difficult independent evaluation was undertaken in which no test strokes came from the same recording as the training strokes. The average recognition rate over a wide variety of test conditions was 76% for the mv gauss, 83% for the ffnn, 76% for the pnn, and 66% for the tree classifier. Keywords: instrument recognition, tabla, automatic transcription, timbre. 1",
        "zenodo_id": 1416000,
        "dblp_key": "conf/ismir/Chordia05"
    },
    {
        "title": "Fuzzy Analysis in Pitch-Class Determination for Polyphonic Audio Key Finding.",
        "author": [
            "Ching-Hua Chuan",
            "Elaine Chew"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417297",
        "url": "https://doi.org/10.5281/zenodo.1417297",
        "ee": "https://zenodo.org/records/1417297/files/ChuanC05.pdf",
        "abstract": "This paper presents a fuzzy analysis technique for pitch class determination that improves the accuracy of key finding from audio information. Errors in audio key finding, typically incorrect assignments of closely related keys, commonly result from imprecise pitch class determination and biases introduced by the quality of the sound. Our technique is motivated by hypotheses on the sources of audio key finding errors, and uses fuzzy analysis to reduce the errors caused by noisy detection of lower pitches, and to refine the biased raw frequency data, in order to extract more correct pitch classes. We compare the proposed system to two others, an earlier one employing only peak detection from FFT results, and another providing direct key finding from MIDI. All three used the same key finding algorithm (Chew\u2019s Spiral Array CEG algorithm) and the same 410 classical music pieces (ranging from Baroque to Contemporary). Considering only the first 15 seconds of music in each piece, the proposed fuzzy analysis technique outperforms the peak detection method by 12.18% on average, matches the performance of direct key finding from MIDI 41.73% of the time, and achieves an overall maximum correct rate of 75.25% (compared to 80.34% for MIDI key finding).",
        "zenodo_id": 1417297,
        "dblp_key": "conf/ismir/ChuanC05"
    },
    {
        "title": "Using a Pitch Detector for Onset Detection.",
        "author": [
            "Nick Collins"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417309",
        "url": "https://doi.org/10.5281/zenodo.1417309",
        "ee": "https://zenodo.org/records/1417309/files/Collins05.pdf",
        "abstract": "A segmentation strategy is explored for monophonic instrumental pitched non-percussive material (PNP) which proceeds from the assertion that human-like event analysis can be founded on a notion of stable pitch percept. A constant-Q pitch detector following the work of Brown and Puckette provides pitch tracks which are post processed in such a way as to identify likely transitions between notes. A core part of this preparation of the pitch detector signal is an algorithm for vibrato suppression. An evaluation task is undertaken on slow attack and high vibrato PNP source files with human annotated onsets, exemplars of a difficult case in monophonic source segmentation. The pitch track onset detection algorithm shows an improvement over the previous best performing algorithm from a recent comparison study of onset detectors. Whilst further timbral cues must play a part in a general solution, the method shows promise as a component of a note event analysis system. Keywords: onset detection, pitch detection, segmentation 1",
        "zenodo_id": 1417309,
        "dblp_key": "conf/ismir/Collins05"
    },
    {
        "title": "&quot;The Pain, the Pain&quot;: Modelling Music Information Behavior and the Songs We Hate.",
        "author": [
            "Sally Jo Cunningham",
            "J. Stephen Downie",
            "David Bainbridge 0001"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417209",
        "url": "https://doi.org/10.5281/zenodo.1417209",
        "ee": "https://zenodo.org/records/1417209/files/CunninghamDB05.pdf",
        "abstract": "The paper presents a grounded theory analysis of 395 user responses to the survey question, \u201cWhat is the worst song ever?\u201d  Important factors uncovered include: lyric quality, the \u201cearworm\u201d effect, voice quality, the influence of associated music videos, over-exposure, perceptions of pretentiousness, and associations with unpleasant personal experiences.",
        "zenodo_id": 1417209,
        "dblp_key": "conf/ismir/CunninghamDB05"
    },
    {
        "title": "Using the Gamera Framework for Building a Lute Tablature Recognition System.",
        "author": [
            "Christophe Dalitz",
            "Thomas Karsten"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416378",
        "url": "https://doi.org/10.5281/zenodo.1416378",
        "ee": "https://zenodo.org/records/1416378/files/DalitzK05.pdf",
        "abstract": "In this article we describe an optical recognition system for historic lute tablature prints that we have built with the aid of the Gamera toolkit for document analysis and recognition. We give recognition rates for various historic sources and show that our system works quite well on printed tablature sources using movable types. For engraved and manuscript sources, we discuss some principal current limitations of our system and Gamera. Keywords: Optical Music Recognition, Lute Tablature. 1 LUTE TABLATURE From the 16th and early 17th century a large body of lute tablature sources is extant. As a major part of this music is derived from vocal models, it can be an ideal investigation object for music information retrieval questions. Consequently there are efforts like the ECOLM project [1] to build a data base of machine readable tablature encodings of lute music sources. Usual optical music recognition (OMR) systems designed for common music notation (CMN) cannot be used for this purpose because lute music is written in tablature rather than CMN. Figure 1 shows the characteristics of lute tablature notation: rather than specifying the sound of the music, it specifies when and in which frets the strings of the instrument are stopped. The symbols used for fret and rhythm had not been standardized, but almost every historic source used its own unique set of symbols. This is an important difference to CMN, which consists of a limited set of symbols which are consistent across different music scores. Consequently a system for optical tablature recognition (OTR) must not be designed to work with a single set of a priori known symbols, but to be adaptable to differing tablature symbols. We shall see below that the conception of training in Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. c\u20dd2005 Queen Mary, University of London d a d d a cd a c d a c d a 8 course 4, fret 0 course 3, fret 3 string (course) 6 5 4 3 2 1 Figure 1: Lute tablature sample and music transcription statistical pattern recognition provides an adequate framework for this. Besides the above problem, the complexity of recognizing a particular source depends on two factors: how much the individual symbols vary within the source and whether symbols overlap and intersect. While the former can hinder the identification of individual symbols, the latter poses problems on their proper segmentation. In our present work we only consider sources printed with movable types because in these the symbol variance is limited to random defects and symbols are not overlapping. 2 THE GAMERA FRAMEWORK The Gamera framework [2] has been developed by M. Droettboom et al. as a flexible toolkit for building recognition systems. Gamera essentially is a library for the Python programming language and has a number of distinctive features: \u2022 Gamera already provides functions for image segmentation (projections, connected component analysis) and classification (kNN) and it has a classifier training interface \u2022 all methods can be combined flexibly because they are provided as python modules. Own C++ functions for image processing can be added as plugins \u2022 platform independence, which has let us develop parallel on both Linux and MacOS X without noticeable differences 478 \u2022 Gamera\u2019s source code is freely available under the GNU general public license The last point is particularly important for research projects, because it allows for full access to the underlying techniques and enables other researchers to participate in the development of Gamera. Consequently we have also made the full source code of our system freely available as a Gamera toolkit from our project homepage [3]. 3 OUR RECOGNITION SYSTEM The recognition process in our system consists of the four steps preprocessing, segmentation, classification and postprocessing. Table 1 provides an overview over the individual operations and shows what we could use from Gamera and what we had to implement ourselves. The following sections describe the individual steps in more detail. Table 1: Operations used from Gamera and implemented by ourselves Preprocessing: rotation correction own implementation (now in Gamera) smoothing Gamera (convolution) staff line removal own implementation Segmentation: symbol isolation Gamera (CC analysis) Classification: heuristic own implementation statistic Gamera (kNN, grouping) Postprocessing: semantic interpretation own implementation tablature coding own implementation (abc) (midi with abc2midi, postscript with abctab2ps)",
        "zenodo_id": 1416378,
        "dblp_key": "conf/ismir/DalitzK05"
    },
    {
        "title": "Toward Automated Holistic Beat Tracking, Music Analysis and Understanding.",
        "author": [
            "Roger B. Dannenberg"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415246",
        "url": "https://doi.org/10.5281/zenodo.1415246",
        "ee": "https://zenodo.org/records/1415246/files/Dannenberg05.pdf",
        "abstract": "Most music processing attempts to focus on one particular feature or structural element such as pitch, beat location, tempo, or genre. This hierarchical approach, in which music is separated into elements that are analyzed independently, is convenient for the scientific researcher, but is at odds with intuition about music perception. Music is interconnected at many levels, and the interplay of melody, harmony, and rhythm are important in perception. As a first step toward more holistic music analysis, music structure is used to constrain a beat tracking program. With structural information, the simple beat tracker, working with audio input, shows a large improvement. The implications of this work for other music analysis problems are discussed. Keywords: Beat tracking, tempo, analysis, music structure 1 INTRODUCTION Music is full of multi-faceted and inter-related information. Notes of a melody fall into a rhythmic grid, rhythm is hierarchical with beats, measures, and phrases, and harmony generally changes in coordination with both meter and melody. Although some music can be successfully decomposed into separate dimensions of rhythm, harmony, melody, texture, and other features, this kind of decomposition generally loses information, making each dimension harder to understand. In fact, it seems that musicians deliberately complicate individual dimensions to make them more interesting, knowing that listeners will use other information to fill in the gaps. Syncopation can be exaggerated when the tempo is very steady, but we hear less syncopation when tempo is more variable. Confusing rhythms are often clarified by an unmistakeable chord change on the first beat of a measure. Repetition in music often occurs in some power-of-two number of measures, providing clear metrical landmarks even where beats and tempo might be ambiguous. It is easy to notice these interrelationships in music, but difficult to take advantage of them for automatic music analysis. If everything depends on everything else, where does one start? If perception is guided by expectations, will we fail to perceive the \u201ctruth\u201d when it is unexpected? Music analysis produces all kinds of data and representations. How can the analysis of one dimension of music inform the analysis of another, given the inevitable errors that will occur? These are all difficult questions and certainly will form the topic of much future research. This paper describes a small step in this general direction. I will show how information about music structure can be used to inform a beat tracker. In all previous beat trackers known to the author, an algorithm to identify beats is applied uniformly, typically from the beginning to the end of a work. Often times, beat trackers have a tendency to be distracted by syncopation and other musical complexities, and the tracker will drift to some faster or slower tempo, perhaps beating 4 against 3 or 3 against 4. In contrast, when musical structure is taken into account, the beat tracker can be constrained such that when a beat is predicted in one section of music, a beat is also predicted at the corresponding place in all repetitions of that section of music. In practice, these are not absolute constraints but probabilistic tendencies that must be balanced against two other goals: to align beats with sonic events and to maintain a fairly steady tempo. It might seem that if a beat tracker can handle one section of music, it can handle any repetition of that section. If this were the case, the additional constraint of music structure would not help with the beat-tracking problem. Tests with real data, however, show a dramatic improvement when music structure is utilized. How can this be? A simple answer is that the input data is audio, and the detection of likely beat events is error prone. Music structure helps the beat tracker to consolidate information from different sections of music and ultimately do a better job. This will be explained in greater detail in the discussion section. The next section describes related work. Then, in Section 3, I explain the basic beat tracker used for Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or com-mercial advantage and that copies bear this notice and the full citation on the first page. \u00a9 2005 Queen Mary, University of London 366",
        "zenodo_id": 1415246,
        "dblp_key": "conf/ismir/Dannenberg05"
    },
    {
        "title": "A Simulated Annealing Optimization of Audio Features for Drum Classification.",
        "author": [
            "Sven Degroeve",
            "Koen Tanghe",
            "Bernard De Baets",
            "Marc Leman",
            "Jean-Pierre Martens"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417311",
        "url": "https://doi.org/10.5281/zenodo.1417311",
        "ee": "https://zenodo.org/records/1417311/files/DegroeveTBLM05.pdf",
        "abstract": "Current methods for the accurate recognition of instruments within music are based on discriminative data descriptors. These are features of the music fragment that capture the characteristics of the audio and suppress details that are redundant for the problem at hand. The extraction of such features from an audio signal requires the user to set certain parameters. We propose a method for optimizing the parameters for a particular task on the basis of the Simulated Annealing algorithm and Support Vector Machine classification. We show that using an optimized set of audio features improves the recognition accuracy of drum sounds in music fragments. Keywords: drum classification, Mel Frequency Cepstral Coefficients, Support Vector Machine, Simulated Annealing 1",
        "zenodo_id": 1417311,
        "dblp_key": "conf/ismir/DegroeveTBLM05"
    },
    {
        "title": "Automatic Prediction of Hit Songs.",
        "author": [
            "Ruth Dhanaraj",
            "Beth Logan"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417571",
        "url": "https://doi.org/10.5281/zenodo.1417571",
        "ee": "https://zenodo.org/records/1417571/files/DhanarajL05.pdf",
        "abstract": "We explore the automatic analysis of music to identify likely hit songs. We extract both acoustic and lyric information from each song and separate hits from non-hits using standard classifiers, specifically Support Vector Machines and boosting classifiers. Our features are based on global sounds learnt in an unsupervised fashion from acoustic data or global topics learnt from a lyrics database. Experiments on a corpus of 1700 songs demonstrate performance that is much better than random. The lyricbased features are slightly more useful than the acoustic features in correctly identifying hit songs. Concatenating the two features does not produce significant improvements. Analysis of the lyric-based features shows that the absence of certain semantic information indicates that a song is more likely to be a hit. Keywords: hit song detection, music classification. 1",
        "zenodo_id": 1417571,
        "dblp_key": "conf/ismir/DhanarajL05"
    },
    {
        "title": "MATCH: A Music Alignment Tool Chest.",
        "author": [
            "Simon Dixon",
            "Gerhard Widmer"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416952",
        "url": "https://doi.org/10.5281/zenodo.1416952",
        "ee": "https://zenodo.org/records/1416952/files/DixonW05.pdf",
        "abstract": "We present MATCH, a toolkit for aligning audio recordings of different renditions of the same piece of music, based on an efficient implementation of a dynamic time warping algorithm. A forward path estimation algorithm constrains the alignment path so that dynamic time warping can be performed with time and space costs that are linear in the size of the audio files. Frames of audio are represented by a positive spectral difference vector, which emphasises note onsets in the alignment process. In tests with Classical and Romantic piano music, the average alignment error was 41ms (median 20ms), with only 2 out of 683 test cases failing to align. The software is useful for content-based indexing of audio files and for the study of performance interpretation; it can also be used in real-time for tracking live performances. The toolkit also provides functions for displaying the cost matrix, the forward and backward paths, and any metadata associated with the recordings, which can be shown in real time as the alignment is computed. Keywords: audio alignment, content-based indexing, dynamic time warping, music performance analysis 1",
        "zenodo_id": 1416952,
        "dblp_key": "conf/ismir/DixonW05"
    },
    {
        "title": "Extracting Quality Parameters for Compressed Audio from Fingerprints.",
        "author": [
            "Peter Jan O. Doets",
            "Reginald L. Lagendijk"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416080",
        "url": "https://doi.org/10.5281/zenodo.1416080",
        "ee": "https://zenodo.org/records/1416080/files/DoetsL05.pdf",
        "abstract": "An audio fingerprint is a compact yet very robust representation of the perceptually relevant parts of audio content. It can be used to identify audio, even when of severely distorted. Audio compression causes small changes in the fingerprint. We aim to exploit these small fingerprint differences due to compression to assess the perceptual quality of the compressed audio file. Analysis shows that for uncorrelated signals the Bit Error Rate (BER) is approximately inversely proportional to the square root of the Signal-to-Noise Ratio (SNR) of the signal. Experiments using real music confirm this relation. Further experiments show how the various local spectral characteristics cause a large variation in the behavior of the fingerprint difference as a function of SNR or the bitrate set for compression. 1",
        "zenodo_id": 1416080,
        "dblp_key": "conf/ismir/DoetsL05"
    },
    {
        "title": "The 2005 Music Information retrieval Evaluation Exchange (MIREX 2005): Preliminary Overview.",
        "author": [
            "J. Stephen Downie",
            "Kris West",
            "Andreas F. Ehmann",
            "Emmanuel Vincent 0001"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416044",
        "url": "https://doi.org/10.5281/zenodo.1416044",
        "ee": "https://zenodo.org/records/1416044/files/DownieWEV05.pdf",
        "abstract": "preliminary overview of the 2005 Music Information Retrieval Evaluation eXchange (MIREX 2005). The MIREX organizational framework and infrastructure are outlined. Summary data concerning the 10 evaluation contests is provided. Key issues affecting future MIR evaluations are identified and discussed. The paper concludes with a listing of targets items to be undertaken before MIREX 2006 to ensure the ongoing success of the MIREX framework.",
        "zenodo_id": 1416044,
        "dblp_key": "conf/ismir/DownieWEV05"
    },
    {
        "title": "Finding Meter in Music Using An Autocorrelation Phase Matrix and Shannon Entropy.",
        "author": [
            "Douglas Eck",
            "Norman Casagrande"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415650",
        "url": "https://doi.org/10.5281/zenodo.1415650",
        "ee": "https://zenodo.org/records/1415650/files/EckC05.pdf",
        "abstract": "This paper introduces a novel way to detect metrical structure in music. We introduce a way to compute autocorrelation such that the distribution of energy in phase space is preserved in a matrix. The resulting autocorrelation phase matrix is useful for several tasks involving metrical structure. First we can use the matrix to enhance standard autocorrelation by calculating the Shannon entropy at each lag. This approach yields improved results for autocorrelationbased tempo induction. Second, we can efficiently search the matrix for combinations of lags that suggest particular metrical hierarchies. This approach yields a good model for predicting the meter of a piece of music. Finally we can use the phase information in the matrix to align a candidate meter with music, making it possible to perform beat induction with an autocorrelation-based model. We present results for several meter prediction and tempo induction datasets, demonstrating that the approach is competitive with models designed specifically for these tasks. We also present preliminary beat induction results on a small set of artificial patterns. Keywords: Meter prediction, tempo induction, beat induction, autocorrelation, entropy 1",
        "zenodo_id": 1415650,
        "dblp_key": "conf/ismir/EckC05"
    },
    {
        "title": "Inferring Efficient Hierarchical Taxonomies for MIR Tasks: Application to Musical Instruments.",
        "author": [
            "Slim Essid",
            "Ga\u00ebl Richard",
            "Bertrand David 0002"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416268",
        "url": "https://doi.org/10.5281/zenodo.1416268",
        "ee": "https://zenodo.org/records/1416268/files/EssidRD05.pdf",
        "abstract": "A number of approaches for automatic audio classification are based on hierarchical taxonomies since it is acknowledged that improved performance can be thereby obtained. In this paper, we propose a new strategy to automatically acquire hierarchical taxonomies, using machine learning methods, which are expected to maximize the performance of subsequent classification. It is shown that the optimal hierarchical taxonomy of musical instruments (in the sense of inter-class distances) does not follow the traditional and more intuitive instrument classification into instrument families. Keywords: Hierarchical taxonomy, musical instrument, clustering, probabilistic distance. 1",
        "zenodo_id": 1416268,
        "dblp_key": "conf/ismir/EssidRD05"
    },
    {
        "title": "Combining D2K and JGAP for Efficient Feature Weighting for Classification Tasks in Music Information Retrieval.",
        "author": [
            "Rebecca Fiebrink",
            "Cory McKay",
            "Ichiro Fujinaga"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415754",
        "url": "https://doi.org/10.5281/zenodo.1415754",
        "ee": "https://zenodo.org/records/1415754/files/FiebrinkMF05.pdf",
        "abstract": "Music classification continues to be an important component of music information retrieval research. An underutilized tool for improving the performance of classifiers is feature weighting. A major reason for its unpopularity, despite its benefits, is the potentially infinite calculation time it requires to achieve optimal results. Genetic algorithms offer potentially sub-optimal but reasonable solutions at much reduced calculation time, yet they are still quite costly. We investigate the advantages of implementing genetic algorithms in a parallel computing environment to make feature weighting an affordable instrument for researchers in MIR.",
        "zenodo_id": 1415754,
        "dblp_key": "conf/ismir/FiebrinkMF05"
    },
    {
        "title": "Novelty Detection Based on Spectral Similarity of Songs.",
        "author": [
            "Arthur Flexer",
            "Elias Pampalk",
            "Gerhard Widmer"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416504",
        "url": "https://doi.org/10.5281/zenodo.1416504",
        "ee": "https://zenodo.org/records/1416504/files/FlexerPW05.pdf",
        "abstract": "We are introducing novelty detection, i.e. the automatic identification of new or unknown data not covered by the training data, to the field of music information retrieval. Two methods for novelty detection one based solely on the similarity information and one also utilizing genre label information are evaluated within the context of genre classification based on spectral similarity. Both are shown to perform equally well. Keywords: novelty detection, spectral similarity, genre classification 1",
        "zenodo_id": 1416504,
        "dblp_key": "conf/ismir/FlexerPW05"
    },
    {
        "title": "Singer Identification Based on Accompaniment Sound Reduction and Reliable Frame Selection.",
        "author": [
            "Hiromasa Fujihara",
            "Tetsuro Kitahara",
            "Masataka Goto",
            "Kazunori Komatani",
            "Tetsuya Ogata",
            "Hiroshi G. Okuno"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418285",
        "url": "https://doi.org/10.5281/zenodo.1418285",
        "ee": "https://zenodo.org/records/1418285/files/FujiharaKGKOO05.pdf",
        "abstract": "This paper describes a method for automatic singer identification from polyphonic musical audio signals including sounds of various instruments. Because singing voices play an important role in musical pieces with a vocal part, the identification of singer names is useful for music information retrieval systems. The main problem in automatically identifying singers is the negative influences caused by accompaniment sounds. To solve this problem, we developed two methods, accompaniment sound reduction and reliable frame selection. The former method makes it possible to identify the singer of a singing voice after reducing accompaniment sounds. It first extracts harmonic components of the predominant melody from sound mixtures and then resynthesizes the melody by using a sinusoidal model driven by those components. The latter method then judges whether each frame of the obtained melody is reliable (i.e. little influenced by accompaniment sound) or not by using two Gaussian mixture models for vocal and non-vocal frames. It enables the singer identification using only reliable vocal portions of musical pieces. Experimental results with forty popular-music songs by ten singers showed that our method was able to reduce the influences of accompaniment sounds and achieved an accuracy of 95%, while the accuracy for a conventional method was 53%. Keywords: Singer identification, artist identification, melody extraction, singing detection, similarity-based MIR 1",
        "zenodo_id": 1418285,
        "dblp_key": "conf/ismir/FujiharaKGKOO05"
    },
    {
        "title": "Pitch Track Target Deviation in Natural Singing.",
        "author": [
            "David Gerhard"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418115",
        "url": "https://doi.org/10.5281/zenodo.1418115",
        "ee": "https://zenodo.org/records/1418115/files/Gerhard05.pdf",
        "abstract": "Unlike fixed-pitch instruments such as the piano, human singing can stray from a target pitch by as much as a semitone while still being perceived as a single fixed note. This paper presents a study of the difference between target pitch and actualized pitch in natural singing. A set of 50 subjects singing the same melody and lyric is used to compare utterance styles. An algorithm for alignment of idealized template pitch tracks to measured frequency tracks is presented. Specific examples are discussed, and generalizations are made with respect to the types of deviations typical in human singing. Demographics, including the skill of the singer, are presented and discussed in the context of the pitch track deviation from the ideal. Keywords: Singing, melody alignment, ornamentation, pitch track, vibrato. 1",
        "zenodo_id": 1418115,
        "dblp_key": "conf/ismir/Gerhard05"
    },
    {
        "title": "Drum Track Transcription of Polyphonic Music Using Noise Subspace Projection.",
        "author": [
            "Olivier Gillet",
            "Ga\u00ebl Richard"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415606",
        "url": "https://doi.org/10.5281/zenodo.1415606",
        "ee": "https://zenodo.org/records/1415606/files/GilletR05.pdf",
        "abstract": "This paper presents a novel drum transcription system for polyphonic music. The use of a band-wise harmonic/noise decomposition allows the suppression of the deterministic part of the signal, which is mainly contributed by nonrhythmic instruments. The transcription is then performed on the residual noise signal, which contains most of the rhythmic information. This signal is segmented, and the events associated to each onset are classified by support vector machines (SVM) with probabilistic outputs. The features used for classification are directly extracted from the sub-band signals. An additional pre-processing stage in which the instances are reclassified using a localized model was also tested. This transcription method is evaluated on ten test sequences, each of them being performed by two drummers and being available with different mixing settings. The whole system achieves precision and recall rates of 84% for the bass drum and snare drum detection tasks. Keywords: Drum transcription, Rhythm analysis, highresolution methods 1",
        "zenodo_id": 1415606,
        "dblp_key": "conf/ismir/GilletR05"
    },
    {
        "title": "Musicream: New Music Playback Interface for Streaming, Sticking, Sorting, and Recalling Musical Pieces.",
        "author": [
            "Masataka Goto",
            "Takayuki Goto"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415842",
        "url": "https://doi.org/10.5281/zenodo.1415842",
        "ee": "https://zenodo.org/records/1415842/files/GotoG05.pdf",
        "abstract": "This paper describes a novel music playback interface, called Musicream, which lets a user unexpectedly come across various musical pieces similar to those liked by the user. With most previous \u201cquery-by-example\u201d interfaces used for similarity-based searching, for the same query and music collection a user will always receive the same list of musical pieces ranked by their similarity and opportunities to encounter unfamiliar musical pieces in the collection are limited. Musicream facilitates active, flexible, and unexpected encounters with musical pieces by providing four functions: the music-disc streaming function which creates a flow of many musical-piece entities (discs) from a (huge) music collection, the similaritybased sticking function which allows a user to easily pick out and listen to similar pieces from the flow, the metaplaylist function which can generate a playlist of playlists (ordered lists of pieces) while editing them with a high degree of freedom, and the time-machine function which automatically records all Musicream activities and allows a user to visit and retrieve a past state as if using a time machine. In our experiments, these functions were used seamlessly to achieve active and creative querying and browsing of music collections, confirming the effectiveness of Musicream. Keywords: Music interface, Music player, Musiccollection browser, Query-by-example, Playlist. 1",
        "zenodo_id": 1415842,
        "dblp_key": "conf/ismir/GotoG05"
    },
    {
        "title": "Music Information Retrieval, Memory and Culture: Some Philosohpical Remarks.",
        "author": [
            "Cynthia M. Grund"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415626",
        "url": "https://doi.org/10.5281/zenodo.1415626",
        "ee": "https://zenodo.org/records/1415626/files/Grund05.pdf",
        "abstract": "The burgeoning field of Music Information Retrieval (MIR) raises issues which are of interest within traditional areas of discussion in philosophy of music and of philosophy of culture in general. The purpose of this paper is twofold: the first goal is to highlight and briefly discuss a selection of these issues, while the second is to make a case for increased mutual awareness of each other on the parts of MIR and of humanistic research. Many traditional debates within the latter receive infusions of new perspectives from MIR, while research within MIR could be fruitfully pointed in directions suggested by questions of interest within traditional research in the humanities, e.g. the relationship of individual memory to cultural memory, issues regarding crosscultural understanding and the importance of authenticity in artistic contexts. Keywords: Philosophy of music, culture, memory, ethics, authenticity 1 INTRODUCTION",
        "zenodo_id": 1415626,
        "dblp_key": "conf/ismir/Grund05"
    },
    {
        "title": "Visual Playlist Generation on the Artist Map.",
        "author": [
            "Rob van Gulik",
            "Fabio Vignoli"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415206",
        "url": "https://doi.org/10.5281/zenodo.1415206",
        "ee": "https://zenodo.org/records/1415206/files/GulikV05.pdf",
        "abstract": "This paper describes a visual playlist creation method based on a previously designed visualization technique for large music collections. The method gives users high-level control over the contents of a playlist as well as the progression of songs in it, while minimizing the interaction requirements. An interesting feature of the technique is that it creates playlists that are independent of the underlying music collection, making them highly portable. Future work includes an extensive user evaluation to compare the described method with alternative techniques and to measure its qualities, such as the perceived ease of use and perceived usefulness.",
        "zenodo_id": 1415206,
        "dblp_key": "conf/ismir/GulikV05"
    },
    {
        "title": "ATTA: Automatic Time-Span Tree Analyzer Based on Extended GTTM.",
        "author": [
            "Masatoshi Hamanaka",
            "Keiji Hirata 0001",
            "Satoshi Tojo"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415572",
        "url": "https://doi.org/10.5281/zenodo.1415572",
        "ee": "https://zenodo.org/records/1415572/files/HamanakaHT05.pdf",
        "abstract": "This paper describes a music analyzing system called the automatic time-span tree analyzer (ATTA), which we have developed. The ATTA derives a time-span tree that assigns a hierarchy of 'structural importance' to the notes of a piece of music based on the generative theory of tonal music (GTTM). Although the time-span tree has been applied with music summarization and collaborative music creation systems, these systems use time-span trees manually analyzed by experts in musicology. Previous systems based on GTTM cannot acquire a timespan tree without manual application of most of the rules, because GTTM does not resolve much of the ambiguity that exists with the application of the rules. To solve this problem, we propose a novel computational model of the GTTM that re-formalizes the rules with computer implementation. The main advantage of our approach is that we can introduce adjustable parameters, which enables us to assign priority to the rules. Our analyzer automatically acquires time-span trees by configuring the parameters that cover 26 rules out of 36 GTTM rules for constructing a time-span tree. Experimental results showed that after these parameters were tuned, our method outperformed a baseline performance. We hope to distribute the time-span tree as the content for various musical tasks, such as searching and arranging music.",
        "zenodo_id": 1415572,
        "dblp_key": "conf/ismir/HamanakaHT05"
    },
    {
        "title": "Symbolic Representation of Musical Chords: A Proposed Syntax for Text Annotations.",
        "author": [
            "Christopher Harte",
            "Mark B. Sandler",
            "Samer A. Abdallah",
            "Emilia G\u00f3mez"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415114",
        "url": "https://doi.org/10.5281/zenodo.1415114",
        "ee": "https://zenodo.org/records/1415114/files/HarteSAG05.pdf",
        "abstract": "In this paper we propose a text represention for musical chord symbols that is simple and intuitive for musically trained individuals to write and understand, yet highly structured and unambiguous to parse with computer programs. When designing feature extraction algorithms, it is important to have a hand annotated test set providing a ground truth to compare results against. Hand labelling of chords in music files is a long and arduous task and there is no standard annotation methodology, which causes difficulties sharing with existing annotations. In this paper we address this problem by defining a rigid, contextindependent syntax for representing chord symbols in text, supported with a new database of annotations using this system. Keywords: Music, Chords, Harmony, Notation, Annotation 1",
        "zenodo_id": 1415114,
        "dblp_key": "conf/ismir/HarteSAG05"
    },
    {
        "title": "MUCOSA: A Music Content Semantic Annotator.",
        "author": [
            "Perfecto Herrera",
            "\u00d2scar Celma",
            "Jordi Massaguer",
            "Pedro Cano",
            "Emilia G\u00f3mez",
            "Fabien Gouyon",
            "Markus Koppenberger"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415980",
        "url": "https://doi.org/10.5281/zenodo.1415980",
        "ee": "https://zenodo.org/records/1415980/files/HerreraCMCGGK05.pdf",
        "abstract": "MUCOSA (Music Content Semantic Annotator) is an environment for the annotation and generation of music metadata at different levels of abstraction. It is composed of three tiers: an annotation client that deals with microannotations (i.e. within-file annotations), a collection tagger, which deals with macro-annotations (i.e. acrossfiles annotations), and a collaborative annotation subsystem, which manages large-scale annotation tasks that can be shared among different research centres. The annotation client is an enhanced version of WaveSurfer, a speech annotation tool. The collection tagger includes tools for automatic generation of unary descriptors, invention of new descriptors, and propagation of descriptors across sub-collections or playlists. Finally, the collaborative annotation subsystem, based on Plone, makes possible to share the annotation chores and results between several research institutions. A collection of annotated songs is available, as a \u201cstarter pack\u201d to all the individuals or institutions that are eager to join this initiative.",
        "zenodo_id": 1415980,
        "dblp_key": "conf/ismir/HerreraCMCGGK05"
    },
    {
        "title": "The Persian Music and the Santur Instrument.",
        "author": [
            "Peyman Heydarian",
            "Joshua D. Reiss"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415048",
        "url": "https://doi.org/10.5281/zenodo.1415048",
        "ee": "https://zenodo.org/records/1415048/files/HeydarianR05.pdf",
        "abstract": "Persian music has had a profound effect on various Eastern musical cultures, and also influenced Southern European and Northern African music. The Santur, a hammered dulcimer, is one of the most important instruments in Persia. In this paper, Persian music and the Santur instrument are explained and analysed. Techniques for fundamental frequency detection are applied to data acquired from the Santur and results are reported.",
        "zenodo_id": 1415048,
        "dblp_key": "conf/ismir/HeydarianR05"
    },
    {
        "title": "A Benchmark Dataset for Audio Classification and Clustering.",
        "author": [
            "Helge Homburg",
            "Ingo Mierswa",
            "B\u00fclent M\u00f6ller",
            "Katharina Morik",
            "Michael Wurst"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417065",
        "url": "https://doi.org/10.5281/zenodo.1417065",
        "ee": "https://zenodo.org/records/1417065/files/HomburgMMMW05.pdf",
        "abstract": "We present a freely available benchmark dataset for audio classification and clustering. This dataset consists of 10 seconds samples of 1886 songs obtained from the Garageband site. Beside the audio clips themselves, textual meta data is provided for the individual songs. The songs are classified into 9 genres. In addition to the genre information, our dataset also consists of 24 hierarchical cluster models created manually by a group of users. This enables a user centric evaluation of audio classification and clustering algorithms and gives researchers the opportunity to test the performance of their methods on heterogeneous data. We first give a motivation for assembling our benchmark dataset. Then we describe the dataset and its elements in more detail. Finally, we present some initial results using a set of audio features generated by a feature construction approach. Keywords: Benchmark Dataset, Audio Classification, Audio Clustering, Meta Learning 1 CHALLENGES IN LEARNING ON AUDIO DATA Information retrieval has started several efforts to automatic indexing [1] and retrieval (e.g., querying by humming [2]). Machine Learning has shown its benefits for text classification and ranked document retrieval with respect to user preferences [3]. It is straightforward to expect a similar benefit for the classification and personalized retrieval of music records. However, this area is still very challenging for several reasons. Unlike many other types of data used with Machine Learning, audio data consists of time series which are usually quite large. Given a sampling rate of 44100 Hz, a three minute music record has a length of about 8 \u00b7 106 values. Moreover, current approaches to time series indexing and similarity Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. c\u20dd2005 Queen Mary, University of London measures rely on a more or less fixed time scale [4]. The key problem for automatic audio processing based on Machine Learning is to obtain a fixed set of features from the wave forms [5, 6, 7, 8, 9]. Beside the problems connected with these characteristics of audio data, current applications lead to additional challenging problems. Firstly, different classification tasks ask for different feature sets. It is not very likely that a feature set delivering excellent performance on the separation of classical and popular music works well also for the separation of techno and hip hop music. Machine Learning methods should be able to adapt to different areas of the input space. This is usually referred to as local learning [10]. Secondly, for many kinds of audio data important additional information exist as title or artist. This information is often called meta data. Other useful information about songs could be the lyrics, ratings or comments provided by listeners. To integrate all this information for Machine Learning is a very challenging task usually referred to as Multi View Learning [11]. Finally, audio processing based on Machine Learning is often applied in user oriented applications, such as personal media organizers (e. g. iTunes). Such organizers typically help users to manage a collection of songs by automatically classifying songs, clusterings songs, searching for similar songs, etc. However, music is a highly personal issue, users often arrange their songs using very different viewpoints [12, 13, 14]. This leads to problems similar to the ones mentioned above: classification with respect to different viewpoints may ask for different representations. Think for example of a first user, who arranges songs according to mood and a second user arranging songs according to whether the singer is male or female (as shown in Figure 1). The second task may require a set of features that is completely different from the first one, even if the songs themselves are from the same genre. Furthermore the possible viewpoints are usually neither restricted nor anticipated when the system is designed. The Machine Learning methods must be flexible enough to handle any possible viewpoint and thus classification. Still another problem in end user applications is that datasets are of varying size. While some users only arrange very few items, others have large collections of songs. Methods have to provide both, a high accuracy for small datasets and efficiency for large datasets. 528 Figure 1: Two examples of user defined classification schemes. The currently most popular freely available dataset is the RWC Music Database [15]. It provides audio samples together with extensive meta data and is well suited for the evaluation of many kinds of audio processing tasks. Unfortunately, the size of this data set is relatively small and hence does not meet the requirements of many Machine Learning methods. The dataset does also not contain different user viewpoints. We consider these heterogeneous viewpoints a major challenge for many real-world retrieval tasks. The idea of our benchmark dataset is to provide a possibility to compare how different approaches and algorithms handle the described challenges. It reflects all of the above problems. It contains 1886 songs given as 10 s samples from 9 genres. Beside the audio data itself, meta data (band name, genre, etc.), user comments and partially even lyrics are available for each song. Also we provide 24 classification schemes created by our students using arbritrary personal viewpoints. This allows to test methods on very heterogeneous learning tasks, as could be expected in many real life user oriented scenarios. As the user classification schemes only cover parts of the songs, they also provide a way to test how well a given method can adapt to such local problems. Given audio and textual data, the dataset is especially well suited for Multi View Machine Learning. In the next section, the dataset is described in more detail. 2 THE DATASET The dataset1 consists of 1886 songs from the Garageband site. Garageband is a website that allows artists to upload their music and offer it for free download. Visitors of the site might download the audio clips, rate them or write comments. A group of students downloaded the songs to1www-ai.cs.uni-dortmund.de/audio.html Genre Number Blues 120 Electronic 113 Jazz 319 Pop 116 Rap/HipHop 300 Rock 504 Folk/Country 222 Alternative 145 Funk/Soul 47 total 1886 Table 1: Number of songs per genre. gether with some meta information. Then they created personal classification schemes on these songs described in section 2.4. The songs were taken from nine different genres: Pop, Rock, Folk/Country, Alternative, Jazz, Electronic, Blues, Rap/HipHop, Funk/Soul. The number of songs in each genre varies, Table 1 gives an overview.",
        "zenodo_id": 1417065,
        "dblp_key": "conf/ismir/HomburgMMMW05"
    },
    {
        "title": "Lyrics Recognition from a Singing Voice Based on Finite State Automaton for Music Information Retrieval.",
        "author": [
            "Toru Hosoya",
            "Motoyuki Suzuki",
            "Akinori Ito",
            "Shozo Makino"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417855",
        "url": "https://doi.org/10.5281/zenodo.1417855",
        "ee": "https://zenodo.org/records/1417855/files/HosoyaSIM05.pdf",
        "abstract": "Recently, several music information retrieval (MIR) systems have been developed which retrieve musical pieces by the user\u2019s singing voice. All of these systems use only the melody information for retrieval. Although the lyrics information is useful for retrieval, there have been few attempts to exploit lyrics in the user\u2019s input. In order to develop a MIR system that uses lyrics and melody information, lyrics recognition is needed. Lyrics recognition from a singing voice is achieved by similar technology to that of speech recognition. The difference between lyrics recognition and general speech recognition is that the input lyrics are a part of the lyrics of songs in a database. To exploit linguistic constraints maximally, we described the recognition grammar using a finite state automaton (FSA) that accepts only lyrics in the database. In addition, we carried out a \u201csinging voice adaptation\u201d using a speaker adaptation technique. In our experimental results, about 86% retrieval accuracy was obtained. Keywords: MIR, lyrics recognition, FSA 1",
        "zenodo_id": 1417855,
        "dblp_key": "conf/ismir/HosoyaSIM05"
    },
    {
        "title": "A Bootstrap Method for Training an Accurate Audio Segmenter.",
        "author": [
            "Ning Hu",
            "Roger B. Dannenberg"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416200",
        "url": "https://doi.org/10.5281/zenodo.1416200",
        "ee": "https://zenodo.org/records/1416200/files/HuD05.pdf",
        "abstract": "Supervised learning can be used to create good systems for note segmentation in audio data. However, this requires a large set of labeled training examples, and handlabeling is quite difficult and time consuming. A bootstrap approach is introduced in which audio alignment techniques are first used to find the correspondence between a symbolic music representation (such as MIDI data) and an acoustic recording. This alignment provides an initial estimate of note boundaries which can be used to train a segmenter. Once trained, the segmenter can be used to refine the initial set of note boundaries and training can be repeated. This iterative training process eliminates the need for hand-segmented audio. Tests show that this training method can improve a segmenter initially trained on synthetic data. Keywords: Bootstrap, music audio segmentation, note onset detection, audio-to-score alignment. 1",
        "zenodo_id": 1416200,
        "dblp_key": "conf/ismir/HuD05"
    },
    {
        "title": "Mining Music Reviews: Promising Preliminary Results.",
        "author": [
            "Xiao Hu 0001",
            "J. Stephen Downie",
            "Kris West",
            "Andreas F. Ehmann"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417067",
        "url": "https://doi.org/10.5281/zenodo.1417067",
        "ee": "https://zenodo.org/records/1417067/files/HuDWE05.pdf",
        "abstract": "In this paper we present a system for the automatic mining of information from music reviews. We demonstrate a system which has the ability to automatically classify reviews according to the genre of the music reviewed and to predict the simple one-to-five star rating assigned to the music by the reviewer. This experiment is the first step in the development of a system to automatically mine arbitrary bodies of text, such as weblogs (blogs) for musically relevant information.",
        "zenodo_id": 1417067,
        "dblp_key": "conf/ismir/HuDWE05"
    },
    {
        "title": "What You See Is What You Get: on Visualizing Music.",
        "author": [
            "Eric J. Isaacson"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415992",
        "url": "https://doi.org/10.5281/zenodo.1415992",
        "ee": "https://zenodo.org/records/1415992/files/Isaacson05.pdf",
        "abstract": "Though music is fundamentally an aural phenomenon, we often communicate about music through visual means. The paper examines a number of visualization techniques developed for music, focusing especially on those developed for music analysis by specialists in the field, but also looking at some less successful approaches. It is hoped that, by presenting them in this way, those in the MIR community will develop a greater awareness of the kinds of musical problems music scholars are concerned with, and might lend a hand toward addressing them Keywords: visualization, analysis, harmony 1",
        "zenodo_id": 1415992,
        "dblp_key": "conf/ismir/Isaacson05"
    },
    {
        "title": "Tonal Similarity from Audio Using a Template Based Attractor Model.",
        "author": [
            "\u00d6zg\u00fcr Izmirli"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416688",
        "url": "https://doi.org/10.5281/zenodo.1416688",
        "ee": "https://zenodo.org/records/1416688/files/Izmirli05.pdf",
        "abstract": "A model that calculates similarity of tonal evolution among pieces in an audio database is presented. The model employs a template based key finding algorithm. This algorithm is used in a sliding window fashion to obtain a sequence of tonal center estimates that delineate the trajectory of tonal evolution in tonal space. A chroma based representation is used to capture tonality information. Templates are formed from instrument sounds weighted according to pitch distribution profiles. For each window in the input audio, the chroma based representation is interpreted with respect to the precalculated templates that serve as attractor points in tonal space. This leads to a discretization in both time and tonal space making the output representation compact. Local and global variations in tempo are accounted for using dynamic time warping that employs a special type of music theoretical distance measure. Evaluation is given in two stages. The first is evaluation of the key finding model to assess its performance in key finding for raw audio input. The second is based on cross validation testing for pieces that have multiple performances in the database to determine the success of recall by distance.",
        "zenodo_id": 1416688,
        "dblp_key": "conf/ismir/Izmirli05"
    },
    {
        "title": "Continuous HMM and Its Enhancement for Singing/Humming Query Retrieval.",
        "author": [
            "Jyh-Shing Roger Jang",
            "Chao-Ling Hsu",
            "Hong-Ru Lee"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414842",
        "url": "https://doi.org/10.5281/zenodo.1414842",
        "ee": "https://zenodo.org/records/1414842/files/JangHL05.pdf",
        "abstract": "The use of HMM (Hidden Markov Models) for speech recognition has been successful for various applications in the past decades. However, the use of continuous HMM (CHMM) for melody recognition via acoustic input (MRAI for short), or the so-called query by singing/humming, has seldom been reported, partly due to the difference in acoustic characteristics between speech and singing/humming inputs. This paper will derive the formula of CHMM training for frame-based MRAI. In particular, we shall propose enhancement to CHMM and demonstrate that with the enhancement scheme, CHMM can compare favourably with DTW in both efficiency and effectiveness.",
        "zenodo_id": 1414842,
        "dblp_key": "conf/ismir/JangHL05"
    },
    {
        "title": "Rhythm-Based Segmentation of Popular Chinese Music.",
        "author": [
            "Kristoffer Jensen",
            "Jieping Xu",
            "Martin Zachariasen"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418117",
        "url": "https://doi.org/10.5281/zenodo.1418117",
        "ee": "https://zenodo.org/records/1418117/files/JensenXZ05.pdf",
        "abstract": "We present a new method to segment popular music based on rhythm. By computing a shortest path based on the self-similarity matrix calculated from a model of rhythm, segmenting boundaries are found along the diagonal of the matrix. The cost of a new segment is optimized by matching manual and automatic segment boundaries. We compile a small song database of 21 randomly selected popular Chinese songs which come from Chinese Mainland, Taiwan and Hong Kong. The segmenting results on the small corpus show that 78% manual segmentation points are detected and 74% automatic segmentation points are correct. Automatic segmentation achieved 100% correct detection for 5 songs. The results are very encouraging.",
        "zenodo_id": 1418117,
        "dblp_key": "conf/ismir/JensenXZ05"
    },
    {
        "title": "Harmonic-Temporal Clustering via Deterministic Annealing EM Algorithm for Audio Feature Extraction.",
        "author": [
            "Hirokazu Kameoka",
            "Takuya Nishimoto",
            "Shigeki Sagayama"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417629",
        "url": "https://doi.org/10.5281/zenodo.1417629",
        "ee": "https://zenodo.org/records/1417629/files/KameokaNS05.pdf",
        "abstract": "This paper proposes \u201charmonic-temporal structured clustering (HTC) method\u201d, that allows simultaneous estimation of pitch, intensity, onset, duration, etc., of each underlying source in multi-stream audio signal, which we expect to be an effective feature extraction for MIR systems. STC decomposes the energy patterns diffused in timefrequency space, i.e., a time series of power spectrum, into distinct clusters such that each of them is originated from a single sound stream. It becomes clear that the problem is equivalent to geometrically approximating the observed time series of power spectrum by superimposed harmonictemporal structured models (HTMs), whose parameters are directly associated with the specific acoustic characteristics. The update equations in DA(Deterministic Annealing)EM algorithm for the optimal parameter convergence are derived by formulating the model with Gaussian kernel representation. The experiment showed promising results, and verified the potential of the proposed method. Keywords: audio feature extraction, multi-pitch estimation, harmonic-temporal structured clustering. 1",
        "zenodo_id": 1417629,
        "dblp_key": "conf/ismir/KameokaNS05"
    },
    {
        "title": "New Music Interfaces for Rhythm-Based Retrieval.",
        "author": [
            "Ajay Kapur",
            "Richard I. McWalter",
            "George Tzanetakis"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418313",
        "url": "https://doi.org/10.5281/zenodo.1418313",
        "ee": "https://zenodo.org/records/1418313/files/KapurMT05.pdf",
        "abstract": "In the majority of existing work in music information retrieval (MIR) the user interacts with the system using standard desktop components such as the keyboard, mouse or sometimes microphone input. It is our belief that moving away from the desktop to more physically tangible ways of interacting can lead to novel ways of thinking about MIR. In this paper, we report on our work in utilizing new non-standard interfaces for MIR purposes. One of the most important but frequently neglected ways of characterizing and retrieving music is through rhythmic information. We concentrate on rhythmic information both as user input and as means for retrieval. Algorithms and experiments for rhythm-based information retrieval of music, drum loops and indian tabla thekas are described. This work targets expert users such as DJs and musicians which tend to be more curious about new technologies and therefore can serve as catalysts for accelerating the adoption of MIR techniques. In addition, we describe how the proposed rhythm-based interfaces can assist in the annotation and preservation of perfomance practice. Keywords: user interfaces, rhythm analysis, controllers, live performance 1",
        "zenodo_id": 1418313,
        "dblp_key": "conf/ismir/KapurMT05"
    },
    {
        "title": "Content-Based Music Information Retrieval in Wireless Ad-Hoc Networks.",
        "author": [
            "Ioannis Karydis",
            "Alexandros Nanopoulos",
            "Apostolos N. Papadopoulos",
            "Dimitrios Katsaros 0001",
            "Yannis Manolopoulos"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417665",
        "url": "https://doi.org/10.5281/zenodo.1417665",
        "ee": "https://zenodo.org/records/1417665/files/KarydisNPKM05.pdf",
        "abstract": "This paper, introduces the application of Content-Based Music Information Retrieval (CBMIR) in wireless ad-hoc networks. We investigate for the first time the challenges posed by the wireless medium and recognise the factors that require optimisation. We propose novel techniques, which attain a significant reduction in both response times and traffic, compared to naive approaches. Extensive experimental results illustrate the appropriateness and efficiency of the proposed method in this bandwidth-starving and volatile, due to mobility, environment. Keywords: music information retrieval, content-based similarity, wireless ad-hoc, P2P. 1",
        "zenodo_id": 1417665,
        "dblp_key": "conf/ismir/KarydisNPKM05"
    },
    {
        "title": "VOISE: Learning to Segregate Voices in Explicit and Implicit Polyphony.",
        "author": [
            "Phillip B. Kirlin",
            "Paul E. Utgoff"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417225",
        "url": "https://doi.org/10.5281/zenodo.1417225",
        "ee": "https://zenodo.org/records/1417225/files/KirlinU05.pdf",
        "abstract": "Finding multiple occurrences of themes and patterns in music can be hampered due to polyphonic textures. This is caused by the complexity of music that weaves multiple independent lines of music together. We present and demonstrate a system, VoiSe, that is capable of isolating individual voices in both explicit and implicit polyphonic music. VoiSe is designed to work on a symbolic representation of a music score, and consists of two components: a same-voice predicate implemented as a learned decision tree, and a hard-coded voice numbering algorithm. Keywords: voice segregation, explicit polyphony, implicit polyphony, machine learning, theme finding. 1",
        "zenodo_id": 1417225,
        "dblp_key": "conf/ismir/KirlinU05"
    },
    {
        "title": "Instrument Identification in Polyphonic Music: Feature Weighting with Mixed Sounds, Pitch-Dependent Timbre Modeling, and Use of Musical Context.",
        "author": [
            "Tetsuro Kitahara",
            "Masataka Goto",
            "Kazunori Komatani",
            "Tetsuya Ogata",
            "Hiroshi G. Okuno"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415724",
        "url": "https://doi.org/10.5281/zenodo.1415724",
        "ee": "https://zenodo.org/records/1415724/files/KitaharaGKOO05.pdf",
        "abstract": "This paper addresses the problem of identifying musical instruments in polyphonic music. Musical instrument identification (MII) is an improtant task in music information retrieval because MII results make it possible to automatically retrieving certain types of music (e.g., piano sonata, string quartet). Only a few studies, however, have dealt with MII in polyphonic music. In MII in polyphonic music, there are three issues: feature variations caused by sound mixtures, the pitch dependency of timbres, and the use of musical context. For the first issue, templates of feature vectors representing timbres are extracted from not only isolated sounds but also sound mixtures. Because some features are not robust in the mixtures, features are weighted according to their robustness by using linear discriminant analysis. For the second issue, we use an F0-dependent multivariate normal distribution, which approximates the pitch dependency as a function of fundamental frequency. For the third issue, when the instrument of each note is identified, the a priori probablity of the note is calculated from the a posteriori probabilities of temporally neighboring notes. Experimental results showed that recognition rates were improved from 60.8% to 85.8% for trio music and from 65.5% to 91.1% for duo music. Keywords: Musical instrument identification, mixedsound template, F0-dependent multivariate normal distribution, musical context, MPEG-7 1",
        "zenodo_id": 1415724,
        "dblp_key": "conf/ismir/KitaharaGKOO05"
    },
    {
        "title": "Multiple Lyrics Alignment: Automatic Retrieval of Song Lyrics.",
        "author": [
            "Peter Knees",
            "Markus Schedl",
            "Gerhard Widmer"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415140",
        "url": "https://doi.org/10.5281/zenodo.1415140",
        "ee": "https://zenodo.org/records/1415140/files/KneesSW05.pdf",
        "abstract": "We present an approach to automatically retrieve and extract lyrics of arbitrary songs from the Internet. It is intended to provide easy and convenient access to lyrics for users, as well as a basis for further research based on lyrics, e.g. semantic analysis. Due to the fact that many lyrics found on the web suffer from individual errors like typos, we make use of multiple versions from different sources to eliminate mistakes. This is accomplished by Multiple Sequence Alignment. The different sites are aligned and examined for matching sequences of words, finding those parts on the pages that are likely to contain the lyrics. This provides a means to find the most probable version of lyrics, i.e. a version with highest consensus among different sources. Keywords: Lyrics, Web Mining, Multiple Sequence Alignment. 1",
        "zenodo_id": 1415140,
        "dblp_key": "conf/ismir/KneesSW05"
    },
    {
        "title": "Geospatial Location of Music and Sound Files for Music Information Retrieval.",
        "author": [
            "Ian Knopke"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417765",
        "url": "https://doi.org/10.5281/zenodo.1417765",
        "ee": "https://zenodo.org/records/1417765/files/Knopke05.pdf",
        "abstract": "A relatively new avenue of Web-based information retrieval research, intended to semantically improve information extraction, is the idea of using geographical information to accurately locate resources. This paper introduces a technique for locating sound and music files geographically. It uses information extracted from the Web relating to audio resources and combines it with geospatial location data to provide new information about audio usage in various countries. The results presented here illustrate the enormous potential for MIR to use the vast amount of audio materials on the Web within a physical and geographical context. Statistics of audio usage around the world are provided, as well as examples of other applications of these techniques. Keywords: ISMIR, AROOOGA, geospatial, mapping, World Wide Web, Web crawler, GIS, semantic, CIA, McGill, music 1",
        "zenodo_id": 1417765,
        "dblp_key": "conf/ismir/Knopke05"
    },
    {
        "title": "Syncplayer An Advanced System for Multimodal Music Access.",
        "author": [
            "Frank Kurth",
            "Meinard M\u00fcller",
            "David Damm",
            "Christian Fremerey",
            "Andreas Ribbrock",
            "Michael Clausen"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416496",
        "url": "https://doi.org/10.5281/zenodo.1416496",
        "ee": "https://zenodo.org/records/1416496/files/KurthMDFRC05.pdf",
        "abstract": "In this paper, we present the SyncPlayer system for multimodal presentation of high quality audio and associated music-related data. Using the SyncPlayer client interface, a user may play back an audio recording that is locally available on his computer. The recording is then identified by the SyncPlayer server, a process which is performed entirely content-based. Subsequently, the server delivers music-related data like scores or lyrics to the client, which are then displayed synchronously with audio playback using a multimodal visualization plug-in. In addition to visualization, the system provides functionality for contentbased music retrieval and semi-manual content annotation. To the best of our knowledge, our system is moreover the first to systematically exploit automatically generated synchronization data for content-based symbolic browsing in high quality audio recordings. SyncPlayer has already proved to be a valuable tool for evaluating algorithms in MIR research on a larger scale. In this paper, we describe the technical background of the SyncPlayer framework in detail. We also give an overview of the underlying MIR techniques of audio matching, music synchronization, and text-based retrieval that are incorporated in the current version of the system. Keywords: MIR systems and infrastructure, multimodal interfaces and music access, synchronization 1",
        "zenodo_id": 1416496,
        "dblp_key": "conf/ismir/KurthMDFRC05"
    },
    {
        "title": "Annotating Musical Scores in ENP.",
        "author": [
            "Mika Kuuskankare",
            "Mikael Laurson"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417805",
        "url": "https://doi.org/10.5281/zenodo.1417805",
        "ee": "https://zenodo.org/records/1417805/files/KuuskankareL05.pdf",
        "abstract": "The focus of this paper is on ENP-expressions that can be used for annotating ENP scores with user definable information. ENP is a music notation program written in Lisp and CLOS with a special focus on compositional and music analytical applications. We present number of built-in expressions suitable for visualizing, for example, music analytical information as a part of music notation. A Lisp and CLOS based system for creating user-definable annotation information is also presented along with some sample algorithms. Finally, our system for automatically analyzing and annotating an ENP score is illustrated through several examples including some dealing with music information retrieval. Keywords: Music representation, annotating, symbolic notation. 1 OVERVIEW Expressive Notation Package (ENP, [1, 2]) is a music notation program that is designed for displaying scores using the common Western music notation. ENP has been used in several research projects ranging from computer aided composition to controlling virtual instruments. A special focus has been given to compositional and computerassisted music analysis applications. ENP has a graphical user interface that allows musical objects to be edited directly with the mouse. It supports two fundamental notational styles, i.e., mensural and non-mensural notation, and a number of special notational styles such as time notation, frame notation, etc. Representation of musical units must offer ways of making annotations, giving names, making comments, adding images or diagrams, providing links to informative resources on the web, etc [3]. ENP provides a collection of standard and non-standard notational attributes Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. c\u20dd2005 Queen Mary, University of London (e.g, articulations) called ENP-expressions. Furthermore, it offers a set of attributes that can be used to represent analytical information or other user-defined annotations as a part of a musical texture. In addition to their traditional use, ENP-expressions can be used in wide range of applications: (1) display music theoretical analysis information, e.g, annotate motives, harmonic progressions, etc; (2) visualize specialized analytical information, such as Schenker graphs, or pitch-class set theoretical information; (3) attach arbitrary textual annotations, names, or comments to objects; (4) dynamically inspect and visualize data contained by notational objects, i.e., duration, velocity, start-time, etc.; (5) add instructions to tutorials, documentation, or presentations, etc. All ENP-Expressions can access the data contained by the notational objects they are associated with. This allows to design dynamic expressions that can automatically display relevant information about themselves and their musical context. It is also possible to use a scripting language called ENP Script [4] as an algorithmic complement to the manual approach where the user inserts ENPexpressions by hand. This is useful when building, for example, computer-assisted music analysis applications or automatically annotating a musical score. In the following, we present and discuss the annotation possibilities of ENP in more detail. We start with a brief introduction of ENP-Expressions and the notational output of ENP. Section 3 is the main part of this study. Here we present a wide range of annotation devices along with some example scores. Section 4 gives a brief look at the possibilities of automatic music information retrieval and annotation. Section 5 presents some conclusions and ideas for future work. 2 ENP-EXPRESSIONS IN BRIEF Every expression is attached to some musical object or to a group of objects. Currently these objects can be either notes or chords (see [5] for a description of the object hierarchy in ENP). All the expressions are aware of their musical context and can automatically adjust their position and graphical appearance accordingly. New expressions can be created through a textual interface using Lisp and CLOS or by using a set of specialized editors inside ENP. Figure 1 gives a concise overview of the notational possibilities of ENP in a modern context. The example is 72 written in non-mensural notation (time notation) and contains various expressions, special note heads and playing styles. Figure 1: An example of the notational output of ENP. 3 ANNOTATIONS OVERVIEW",
        "zenodo_id": 1417805,
        "dblp_key": "conf/ismir/KuuskankareL05"
    },
    {
        "title": "Preservation Digitization of David Edelberg&apos;s Handel LP Collection: A Pilot Project.",
        "author": [
            "Catherine Lai",
            "Beinan Li",
            "Ichiro Fujinaga"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416616",
        "url": "https://doi.org/10.5281/zenodo.1416616",
        "ee": "https://zenodo.org/records/1416616/files/LaiLF05.pdf",
        "abstract": "This paper describes the digitization process for building an online collection of LPs and the procedure for creating the ground-truth data essential for developing an automated metadata and content capturing system.",
        "zenodo_id": 1416616,
        "dblp_key": "conf/ismir/LaiLF05"
    },
    {
        "title": "Musical Genre Classification Enhanced by Improved Source Separation Technique.",
        "author": [
            "Aristomenis S. Lampropoulos",
            "Paraskevi S. Lampropoulou",
            "George A. Tsihrintzis"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416292",
        "url": "https://doi.org/10.5281/zenodo.1416292",
        "ee": "https://zenodo.org/records/1416292/files/LampropoulosLT05.pdf",
        "abstract": "We present a system for musical genre classification based on audio features extracted from signals which correspond to distinct musical instrument sources. For the separation of the musical sources, we propose an innovative technique in which the convolutive sparse coding algorithm is applied to several portions of the audio signal. The system is evaluated and its performance is assessed. Keywords: Musical Genre Classification, Source Separation, Convolutive Sparse Coding. 1",
        "zenodo_id": 1416292,
        "dblp_key": "conf/ismir/LampropoulosLT05"
    },
    {
        "title": "Efficient Extraction of Closed Motivic Patterns in Multi-Dimensional Symbolic Representations of Music.",
        "author": [
            "Olivier Lartillot"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418129",
        "url": "https://doi.org/10.5281/zenodo.1418129",
        "ee": "https://zenodo.org/records/1418129/files/Lartillot05.pdf",
        "abstract": "An efficient model for discovering repeated patterns in symbolic representations of music is presented. Combinatorial redundancy inherent in the pattern discovery paradigm is usually filtered using global selective mechanisms, based on pattern frequency and length. The proposed approach is founded instead on the concept of closed pattern, and insures lossless compression through an adaptive selection of most specific descriptions in the multi-dimensional parametric space. A notion of cyclic pattern is introduced, enabling the filtering of another form of combinatorial redundancy provoked by successive repetitions of patterns. The use of cyclic patterns implies a necessary chronological scanning of the piece, and the addition of mechanisms formalising particular Gestalt principles. This study shows therefore that automated analysis of music cannot rely on simple mathematical or statistical approaches, but requires instead a complex and detailed modelling of the cognitive system ruling the listening processes. The resulting algorithm is able to offer for the first time compact and relevant motivic analyses of monodies, and may therefore be applied to automated indexing of symbolic music databases. Numerous additional mechanisms need to be added in order to consider all aspects of music expression, including polyphony and complex motivic transformations. Keywords: Closed pattern discovery, Gallois connection, Formal Concept Analysis, cyclic pattern, cognitive modelling. 1",
        "zenodo_id": 1418129,
        "dblp_key": "conf/ismir/Lartillot05"
    },
    {
        "title": "Challenges in Cross-Cultural/Multilingual Music Information Seeking.",
        "author": [
            "Jin Ha Lee 0001",
            "J. Stephen Downie",
            "Sally Jo Cunningham"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416706",
        "url": "https://doi.org/10.5281/zenodo.1416706",
        "ee": "https://zenodo.org/records/1416706/files/LeeDC05.pdf",
        "abstract": "Understanding and meeting the needs of a broad range of music users across different cultures and languages are central in designing a global music digital library. This exploratory study examines cross-cultural/multilingual music information seeking behaviors and reveals some important characteristics of these behaviors by analyzing 107 authentic music information queries from a Korean knowledge search portal Naver \uc9c0\uc2dd (knowledge) iN and 150 queries from Google Answers website. We conclude that new sets of access points must be developed to accommodate music queries that cross cultural or language boundaries.",
        "zenodo_id": 1416706,
        "dblp_key": "conf/ismir/LeeDC05"
    },
    {
        "title": "Genre Classification via an LZ78-Based String Kernel.",
        "author": [
            "Ming Li",
            "Ronan Sleep"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415162",
        "url": "https://doi.org/10.5281/zenodo.1415162",
        "ee": "https://zenodo.org/records/1415162/files/LiS05.pdf",
        "abstract": "We develop the notion of normalized information distance (NID) [7] into a kernel distance suitable for use with a Support Vector Machine classifier, and demonstrate its use for an audio genre classification task. Our classification scheme involves a relatively small number of low-level audio features, is efficient to compute, yet generates an accuracy which compares well with recent works.",
        "zenodo_id": 1415162,
        "dblp_key": "conf/ismir/LiS05"
    },
    {
        "title": "A Hierarchical Approach for Audio Stream Segmentation and Classification.",
        "author": [
            "Wei Liang 0009",
            "Shuwu Zhang",
            "Bo Xu 0002"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415166",
        "url": "https://doi.org/10.5281/zenodo.1415166",
        "ee": "https://zenodo.org/records/1415166/files/LiangZX05.pdf",
        "abstract": "This paper describes a hierarchical approach for fast audio stream segmentation and classification. With this approach, the audio stream is firstly segmented into audio clips by MBCR (Multiple sub-Bands spectrum Centroid relative Ratio) based histogram modeling. Then a MGM (Modified Gaussian modeling) based hierarchical classifier is adopted to put the segmented audio clips into six pre-defined categories in terms of discriminative",
        "zenodo_id": 1415166,
        "dblp_key": "conf/ismir/LiangZX05"
    },
    {
        "title": "A Histogram Algorithm for Fast Audio Retrieval.",
        "author": [
            "Wei Liang 0009",
            "Shuwu Zhang",
            "Bo Xu 0002"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415812",
        "url": "https://doi.org/10.5281/zenodo.1415812",
        "ee": "https://zenodo.org/records/1415812/files/LiangZX05a.pdf",
        "abstract": "This paper describes a fast audio detection method for specific audio retrieval in the AV stream. The method is a histogram matching algorithm based on structural and perceptual features. This algorithm extracts audio features based on human perception on the sound scene and locates the special audio clip by fast histogram matching. Experimental results based on the advertisement detection in TV program showed that the algorithm can achieve a very high overall precision and  recall rate both about 97% with very fast search time about 1/40 on real time.",
        "zenodo_id": 1415812,
        "dblp_key": "conf/ismir/LiangZX05a"
    },
    {
        "title": "Evaluation of Feature Extractors and Psycho-Acoustic Transformations for Music Genre Classification.",
        "author": [
            "Thomas Lidy",
            "Andreas Rauber"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416856",
        "url": "https://doi.org/10.5281/zenodo.1416856",
        "ee": "https://zenodo.org/records/1416856/files/LidyR05.pdf",
        "abstract": "We present a study on the importance of psycho-acoustic transformations for effective audio feature calculation. From the results, both crucial and problematic parts of the algorithm for Rhythm Patterns feature extraction are identified. We furthermore introduce two new feature representations in this context: Statistical Spectrum Descriptors and Rhythm Histogram features. Evaluation on both the individual and combined feature sets is accomplished through a music genre classification task, involving 3 reference audio collections. Results are compared to published measures on the same data sets. Experiments confirmed that in all settings the inclusion of psycho-acoustic transformations provides significant improvement of classification accuracy. Keywords: content-based retrieval, psycho-acoustic, audio feature extraction, music genre classification 1",
        "zenodo_id": 1416856,
        "dblp_key": "conf/ismir/LidyR05"
    },
    {
        "title": "Fast Capture of Sheet Music for an Agile Digital Music Library.",
        "author": [
            "Richard Lobb",
            "Tim Bell 0001",
            "David Bainbridge 0001"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417989",
        "url": "https://doi.org/10.5281/zenodo.1417989",
        "ee": "https://zenodo.org/records/1417989/files/LobbBB05.pdf",
        "abstract": "A personal digital music library needs to be \u201cagile\u201d, that is, it needs to make it easy to capture and index material on the fly. A digital camera is a particularly effective way of achieving this, but there are several issues with the quality of the captured image, including distortions in the shape of the image due to the camera not being aligned properly with the page, non-planarity of the page, lens distortion from close-up shots, and inconsistent lighting across the page. In this paper we explore ways to improve the quality of music images captured by a digital camera or an inexpensive scanner, where the user is not expected to pay a lot of attention to the process. Such pre-processing will significantly aid Music Information Retrieval indexing through Optical Music Recognition, for example. The research presented here is primarily based around using a Fast Fourier Transform (FFT) to determine the orientation of the page. We find that a windowed FFT is effective at correcting rotational errors, and we make significant progress towards removing perspective distortion introduced by the camera not being parallel with the music. Keywords: image capture, FFT, digital camera 1",
        "zenodo_id": 1417989,
        "dblp_key": "conf/ismir/LobbBB05"
    },
    {
        "title": "SoniXplorer: Combining Visualization and Auralization for Content-Based Exploration of Music Collections.",
        "author": [
            "Dominik L\u00fcbbers"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418021",
        "url": "https://doi.org/10.5281/zenodo.1418021",
        "ee": "https://zenodo.org/records/1418021/files/Lubbers05.pdf",
        "abstract": "Music can be described best by music. However, current research in the design of user interfaces for the exploration of music collections has mainly focused on visualization aspects ignoring possible benefits from spatialized music playback. We describe our first development steps towards two novel user-interface designs: The Sonic Radar arranges a fixed number of prototypes resulting from a content-based clustering process in a circle around the user\u2019s standpoint. To derive an auralization of the scene, we introduce the concept of an aural focus of perception that adapts well-known principles from the visual domain. The Sonic SOM is based on Kohonen\u2019s SelfOrganizing Map. It helps the user in understanding the structure of his music collection by positioning titles on a two-dimensional grid according to their high-dimensional similarity. We show how our auralization concept can be adapted to extend this visualization technique and thereby support multimodal navigation. Keywords: Content-based Music Retrieval, Exploration, Visualization, Auralization, User Interface 1",
        "zenodo_id": 1418021,
        "dblp_key": "conf/ismir/Lubbers05"
    },
    {
        "title": "Song-Level Features and Support Vector Machines for Music Classification.",
        "author": [
            "Michael I. Mandel",
            "Dan Ellis"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415024",
        "url": "https://doi.org/10.5281/zenodo.1415024",
        "ee": "https://zenodo.org/records/1415024/files/MandelE05.pdf",
        "abstract": "Searching and organizing growing digital music collections requires automatic classification of music. This paper describes a new system, tested on the task of artist identification, that uses support vector machines to classify songs based on features calculated over their entire lengths. Since support vector machines are exemplarbased classifiers, training on and classifying entire songs instead of short-time features makes intuitive sense. On a dataset of 1200 pop songs performed by 18 artists, we show that this classifier outperforms similar classifiers that use only SVMs or song-level features. We also show that the KL divergence between single Gaussians and Mahalanobis distance between MFCC statistics vectors perform comparably when classifiers are trained and tested on separate albums, but KL divergence outperforms Mahalanobis distance when trained and tested on songs from the same albums. Keywords: Support vector machines, song classification, artist identification, kernel spaces 1",
        "zenodo_id": 1415024,
        "dblp_key": "conf/ismir/MandelE05"
    },
    {
        "title": "jAudio: An Feature Extraction Library.",
        "author": [
            "Daniel McEnnis",
            "Cory McKay",
            "Ichiro Fujinaga",
            "Philippe Depalle"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416648",
        "url": "https://doi.org/10.5281/zenodo.1416648",
        "ee": "https://zenodo.org/records/1416648/files/McEnnisMFD05.pdf",
        "abstract": "jAudio is a new framework for feature extraction designed to eliminate the duplication of effort in calculating features from an audio signal. This system meets the needs of MIR researchers by providing a library of analysis algorithms that are suitable for a wide array of MIR tasks. In order to provide these features with a minimal learning curve, the system implements a GUI that makes the process of selecting desired features straight forward. A command-line interface is also provided to manipulate jAudio via scripting. Furthermore, jAudio provides a unique method of handling multidimensional features and a new mechanism for dependency handling to prevent duplicate calculations. The system takes a sequence of audio files as input. In the GUI, users select the features that they wish to have extracted\u2014letting jAudio take care of all dependency problems\u2014and either execute directly from the GUI or save the settings for batch processing. The output is either an ACE XML file or an ARFF file depending on the user\u2019s preference. Keywords: Java Audio Environment, Audio Feature Extraction, Music Information Retrieval. 1",
        "zenodo_id": 1416648,
        "dblp_key": "conf/ismir/McEnnisMFD05"
    },
    {
        "title": "ACE: A Framework for Optimizing Music Classification.",
        "author": [
            "Cory McKay",
            "Rebecca Fiebrink",
            "Daniel McEnnis",
            "Beinan Li",
            "Ichiro Fujinaga"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415720",
        "url": "https://doi.org/10.5281/zenodo.1415720",
        "ee": "https://zenodo.org/records/1415720/files/McKayFMLF05.pdf",
        "abstract": "This paper presents ACE (Autonomous Classification Engine), a framework for using and optimizing classifiers. Given a set of feature vectors, ACE experiments with a variety of classifiers, classifier parameters, classifier ensembles and dimensionality reduction techniques in order to arrive at a good configuration for the problem at hand. In addition to evaluating classification methodologies in terms of success rates, functionality is also being incorporated into ACE allowing users to specify constraints on training and classification times as well as on the amount of time that ACE has to arrive at a solution. ACE is designed to facilitate classification for those new to pattern recognition as well as provide flexibility for those with more experience. ACE is packaged with audio and MIDI feature extraction software, although it can certainly be used with existing feature extractors. This paper includes a discussion of ways in which existing general-purpose classification software can be adapted to meet the needs of music researchers and shows how these ideas have been implemented in ACE. A standardized XML format for communicating features and other information to classifiers is proposed. A special emphasis is placed on the potential of classifier ensembles, which have remained largely untapped by the MIR community to date. A brief theoretical discussion of ensemble classification is presented in order to promote this powerful approach.",
        "zenodo_id": 1415720,
        "dblp_key": "conf/ismir/McKayFMLF05"
    },
    {
        "title": "An Investigation of Feature Models for Music Genre Classification Using the Support Vector Classifier.",
        "author": [
            "Anders Meng",
            "John Shawe-Taylor"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416052",
        "url": "https://doi.org/10.5281/zenodo.1416052",
        "ee": "https://zenodo.org/records/1416052/files/MengS05.pdf",
        "abstract": "In music genre classification the decision time is typically of the order of several seconds, however, most automatic music genre classification systems focus on short time features derived from 10 \u221250ms. This work investigates two models, the multivariate Gaussian model and the multivariate autoregressive model for modelling short time features. Furthermore, it was investigated how these models can be integrated over a segment of short time features into a kernel such that a support vector machine can be applied. Two kernels with this property were considered, the convolution kernel and product probability kernel. In order to examine the different methods an 11 genre music setup was utilized. In this setup the Mel Frequency Cepstral Coefficients were used as short time features. The accuracy of the best performing model on this data set was \u223c44% compared to a human performance of \u223c52% on the same data set. Keywords: Feature Integration, Product Probability Kernel, Convolution Kernel, Support Vector Machine, Music Genre 1",
        "zenodo_id": 1416052,
        "dblp_key": "conf/ismir/MengS05"
    },
    {
        "title": "Comparing Pitch Spelling Algorithms.",
        "author": [
            "David Meredith 0001",
            "Geraint A. Wiggins"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416366",
        "url": "https://doi.org/10.5281/zenodo.1416366",
        "ee": "https://zenodo.org/records/1416366/files/MeredithW05.pdf",
        "abstract": "A pitch spelling algorithm predicts the pitch names of the notes in a musical passage when given the onset-time, MIDI note number and possibly the duration and voice of each note. Various versions of the algorithms of LonguetHiggins, Cambouropoulos, Temperley and Sleator, Chew and Chen, and Meredith were run on a corpus containing 195972 notes, equally divided between eight classical and baroque composers. The standard deviation of the accuracies achieved by each algorithm over the eight composers was used as a measure of its style dependence (SD). Meredith\u2019s ps1303 was the most accurate algorithm, spelling 99.43% of the notes correctly (SD = 0.54). The best version of Chew and Chen\u2019s algorithm was the least dependent on style (SD = 0.35) and spelt 99.15% of the notes correctly. A new version of Cambouropoulos\u2019s algorithm, combining features of all three versions described by Cambouropoulos himself, also spelt 99.15% of the notes correctly (SD = 0.47). The best version of Temperley and Sleator\u2019s algorithm spelt 97.79% of the notes correctly, but nearly 70% of its errors were due to a single sudden enharmonic change. LonguetHiggins\u2019s algorithm spelt 98.21% of the notes correctly (SD = 1.79) but only when it processed the music a voice at a time. Keywords: pitch spelling, transcription, algorithms, evaluation. 1",
        "zenodo_id": 1416366,
        "dblp_key": "conf/ismir/MeredithW05"
    },
    {
        "title": "The Mel-Frequency Cepstral Coefficients in the Context of Singer Identification.",
        "author": [
            "Annamaria Mesaros",
            "Jaakko Astola"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417539",
        "url": "https://doi.org/10.5281/zenodo.1417539",
        "ee": "https://zenodo.org/records/1417539/files/MesarosA05.pdf",
        "abstract": "The singing voice is the oldest and most complex musical instrument. A familiar singer\u2019s voice is easily recognizable for humans, even when hearing a song for the first time. On the other hand, for automatic identification this is a difficult task among sound source identification applications. The signal processing techniques aim to extract features that are related to identity characteristics. The research presented in this paper considers 32 Mel-Frequency Cepstral Coefficients in two subsets: the low order MFCCs characterizing the vocal tract resonances and the high order MFCCs related to the glottal wave shape. We explore possibilities to identify and discriminate singers using the two sets. Based on the results we can affirm that both subsets have their contribution in defining the identity of the voice, but the high order subset is more robust to changes in singing style. Keywords: sound source identification, singing voice, MFCC 1",
        "zenodo_id": 1417539,
        "dblp_key": "conf/ismir/MesarosA05"
    },
    {
        "title": "Databionic Visualization of Music Collections According to Perceptual Distance.",
        "author": [
            "Fabian M\u00f6rchen",
            "Alfred Ultsch",
            "Mario N\u00f6cker",
            "Christian Stamm"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417967",
        "url": "https://doi.org/10.5281/zenodo.1417967",
        "ee": "https://zenodo.org/records/1417967/files/MorchenUNS05.pdf",
        "abstract": "We describe the MusicMiner system for organizing large collections of music with databionic mining techniques. Low level audio features are extracted from the raw audio data on short time windows during which the sound is assumed to be stationary. Static and temporal statistics were consistently and systematically used for aggregation of low level features to form high level features. A supervised feature selection targeted to model perceptual distance between different sounding music lead to a small set of non-redundant sound features. Clustering and visualization based on these feature vectors can discover emergent structures in collections of music. Visualization based on Emergent Self-Organizing Maps in particular enables the unsupervised discovery of timbrally consistent clusters that may or may not correspond to musical genres and artists. We demonstrate the visualizations capabilities of the U-Map, displaying local sound differences based on the new audio features. An intuitive browsing of large music collections is offered based on the paradigm of topographic maps. The user can navigate the sound space and interact with the maps to play music or show the context of a song. Keywords: audio features, music similarity, perception, clustering, visualization 1",
        "zenodo_id": 1417967,
        "dblp_key": "conf/ismir/MorchenUNS05"
    },
    {
        "title": "Audio Matching via Chroma-Based Statistical Features.",
        "author": [
            "Meinard M\u00fcller",
            "Frank Kurth",
            "Michael Clausen"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416800",
        "url": "https://doi.org/10.5281/zenodo.1416800",
        "ee": "https://zenodo.org/records/1416800/files/MuellerKC05.pdf",
        "abstract": "In this paper, we describe an efficient method for audio matching which performs effectively for a wide range of classical music. The basic goal of audio matching can be described as follows: consider an audio database containing several CD recordings for one and the same piece of music interpreted by various musicians. Then, given a short query audio clip of one interpretation, the goal is to automatically retrieve the corresponding excerpts from the other interpretations. To solve this problem, we introduce a new type of chroma-based audio feature that strongly correlates to the harmonic progression of the audio signal. Our feature shows a high degree of robustness to variations in parameters such as dynamics, timbre, articulation, and local tempo deviations. As another contribution, we describe a robust matching procedure, which allows to handle global tempo variations. Finally, we give a detailed account on our experiments, which have been carried out on a database of more than 110 hours of audio comprising a wide range of classical music. Keywords: audio matching, chroma feature, music identification 1",
        "zenodo_id": 1416800,
        "dblp_key": "conf/ismir/MuellerKC05"
    },
    {
        "title": "Speech/Music Discrimination Using a Single Warped LPC-Based Feature.",
        "author": [
            "J. Enrique Mu\u00f1oz Exp\u00f3sito",
            "Sebasti\u00e1n Garc\u00eda Gal\u00e1n",
            "Nicol\u00e1s Ruiz-Reyes",
            "Pedro Vera-Candeas",
            "F. Rivas-Pe\u00f1a"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417711",
        "url": "https://doi.org/10.5281/zenodo.1417711",
        "ee": "https://zenodo.org/records/1417711/files/Munoz-ExpositoGRVR05.pdf",
        "abstract": "Automatic discrimination of speech and music is an important tool in many multimedia applications. The paper presents a low complexity but effective approach for speech/music discrimination, which exploits only one simple feature, called Warped LPC-based Spectral Centroid (WLPC-SC). A three-component Gaussian Mixture Model (GMM) classifier is used because it showed a slightly better performance than other Statistical Pattern Recognition (SPR) classifiers. Comparison between WLPC-SC and the timbral features proposed in Tzanetakis and Cook (2002) is performed, aiming to assess the good discriminatory power of the proposed feature. Experimental results reveal that our speech/music discriminator is robust and fast, making it suitable for real-time multimedia applications. Keywords: speech/music discrimination, LPC, spectral centroid, GMM. 1",
        "zenodo_id": 1417711,
        "dblp_key": "conf/ismir/Munoz-ExpositoGRVR05"
    },
    {
        "title": "PlaySOM and PocketSOMPlayer, Alternative Interfaces to Large Music Collections.",
        "author": [
            "Robert Neumayer",
            "Michael Dittenbach",
            "Andreas Rauber"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414818",
        "url": "https://doi.org/10.5281/zenodo.1414818",
        "ee": "https://zenodo.org/records/1414818/files/NeumayerDR05.pdf",
        "abstract": "With the rising popularity of digital music archives the need for new access methods such as interactive exploration or similarity-based search become significant. In this paper we present the PlaySOM, as well as the PocketSOMPlayer, two novel interfaces that enable one to browse a music collection by navigating a map of clustered music tracks and to select regions of interest containing similar tracks for playing. The PlaySOM system is primarily designed to allow interaction via a large-screen device, whereas the PocketSOMPlayer is implemented for mobile devices, supporting both local as well as streamed audio replay. This approach offers content-based organization of music as an alternative to conventional navigation of audio archives, i.e. flat or hierarchical listings of music tracks that are sorted and filtered by meta information. Keywords: User Interaction, Music Collections, Information Discovery and Retrieval, Audio Clustering, Audio Interfaces, Mobile Devices. 1",
        "zenodo_id": 1414818,
        "dblp_key": "conf/ismir/NeumayerDR05"
    },
    {
        "title": "Experiments on Segmentation Techniques for Music Documents Indexing.",
        "author": [
            "Giovanna Neve",
            "Nicola Orio"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416996",
        "url": "https://doi.org/10.5281/zenodo.1416996",
        "ee": "https://zenodo.org/records/1416996/files/NeveO05.pdf",
        "abstract": "This paper presents an overview of different approaches to melody segmentation aimed at extracting music lexical units, which can be used as content descriptors of music documents. Four approaches have been implemented and compared on a test collection of real documents and queries, showing their impact on index term size and on retrieval effectiveness. From the results, simple but extensive approaches seem to give better performances than more sophisticated segmentation algorithms. Keywords: indexing, melodic segmentation. 1",
        "zenodo_id": 1416996,
        "dblp_key": "conf/ismir/NeveO05"
    },
    {
        "title": "Factors Affecting Automatic Genre Classification: An Investigation Incorporating Non-Western Musical Forms.",
        "author": [
            "Noris Mohd. Norowi",
            "Shyamala Doraisamy",
            "Rahmita Wirza O. K. Rahmat"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418067",
        "url": "https://doi.org/10.5281/zenodo.1418067",
        "ee": "https://zenodo.org/records/1418067/files/NorowiDR05.pdf",
        "abstract": "The number of studies investigating automated genre classification is growing following the increasing amounts of digital audio data available. The underlying techniques to perform automated genre classification in general include feature extraction and classification. In this study, MARSYAS was used to extract audio features and the suite of tools available in WEKA was used for the classification. This study investigates the factors affecting automated genre classification. As for the dataset, most studies in this area work with western genres and traditional Malay music is incorporated in this study. Eight genres were introduced; Dikir Barat, Etnik Sabah, Inang, Joget, Keroncong, Tumbuk Kalang, Wayang Kulit, and Zapin.  A total of 417 tracks from various Audio Compact Discs were collected and used as the dataset. Results show that various factors such as the musical features extracted, classifiers employed, the size of the dataset, excerpt length, excerpt location and test set parameters improve classification results.",
        "zenodo_id": 1418067,
        "dblp_key": "conf/ismir/NorowiDR05"
    },
    {
        "title": "A Probabilistic Model for Chord Progressions.",
        "author": [
            "Jean-Fran\u00e7ois Paiement",
            "Douglas Eck",
            "Samy Bengio"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416922",
        "url": "https://doi.org/10.5281/zenodo.1416922",
        "ee": "https://zenodo.org/records/1416922/files/PaiementEB05.pdf",
        "abstract": "Chord progressions are the building blocks from which tonal music is constructed. Inferring chord progressions is thus an essential step towards modeling long term dependencies in music. In this paper, a distributed representation for chords is designed such that Euclidean distances roughly correspond to psychoacoustic dissimilarities. Estimated probabilities of chord substitutions are derived from this representation and are used to introduce smoothing in graphical models observing chord progressions. Parameters in the graphical models are learnt with the EM algorithm and the classical Junction Tree algorithm is used for inference. Various model architectures are compared in terms of conditional out-of-sample likelihood. Both perceptual and statistical evidence show that binary trees related to meter are well suited to capture chord dependencies. 1",
        "zenodo_id": 1416922,
        "dblp_key": "conf/ismir/PaiementEB05"
    },
    {
        "title": "On the Detection of Melody Notes in Polyphonic Audio.",
        "author": [
            "Rui Pedro Paiva"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417681",
        "url": "https://doi.org/10.5281/zenodo.1417681",
        "ee": "https://zenodo.org/records/1417681/files/Paiva05.pdf",
        "abstract": "This paper describes a method for melody detection in polyphonic musical signals. Our approach starts by obtaining a set of pitch candidates for each time frame, with recourse to an auditory model. Trajectories of the most salient pitches are then constructed. Next, note candidates are obtained by trajectory segmentation (in terms of frequency and pitch salience variations). Too short, lowsalience and harmonically related notes are then eliminated. Finally, the notes comprising the melody are extracted. This is the main topic of this paper. We select the melody notes by making use of note saliences and melodic smoothness. First, we select the notes with highest pitch salience at each moment. Then, by the melodic smoothness principle, we exploit the fact that tonal melodies are usually smooth. Thus, long music intervals indicate the presence of possibly erroneous notes, which are substituted by notes that smooth out the melodic contour. Finally, false positives in the extracted melody should be eliminated. To this end, we remove spurious notes that correspond to abrupt drops in note saliences or durations. Additionally, note clustering is conducted to further discriminate between true melody notes and false positives.",
        "zenodo_id": 1417681,
        "dblp_key": "conf/ismir/Paiva05"
    },
    {
        "title": "Improvements of Audio-Based Music Similarity and Genre Classificaton.",
        "author": [
            "Elias Pampalk",
            "Arthur Flexer",
            "Gerhard Widmer"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418083",
        "url": "https://doi.org/10.5281/zenodo.1418083",
        "ee": "https://zenodo.org/records/1418083/files/PampalkFW05.pdf",
        "abstract": "Audio-based music similarity measures can be applied to automatically generate playlists or recommendations. In this paper spectral similarity is combined with complementary information from fluctuation patterns including two new descriptors derived thereof. The performance is evaluated in a series of experiments on four music collections. The evaluations are based on genre classification, assuming that very similar tracks belong to the same genre. The main findings are that, (1) although the improvements are substantial on two of the four collections our extensive experiments confirm earlier findings that we are approaching the limit of how far we can get using simple audio statistics. (2) We have found that evaluating similarity through genre classification is biased by the music collection (and genre taxonomy) used. Furthermore, (3) in a cross validation no pieces from the same artist should be in both training and test set. 1",
        "zenodo_id": 1418083,
        "dblp_key": "conf/ismir/PampalkFW05"
    },
    {
        "title": "Dynamic Playlist Generation Based on Skipping Behavior.",
        "author": [
            "Elias Pampalk",
            "Tim Pohle",
            "Gerhard Widmer"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414932",
        "url": "https://doi.org/10.5281/zenodo.1414932",
        "ee": "https://zenodo.org/records/1414932/files/PampalkPW05.pdf",
        "abstract": "Common approaches to creating playlists are to randomly shuffle a collection (e.g. iPod shuffle) or manually select songs. In this paper we present and evaluate heuristics to adapt playlists automatically given a song to start with (seed song) and immediate user feedback. Instead of rich metadata we use audio-based similarity. The user gives feedback by pressing a skip button if the user dislikes the current song. Songs similar to skipped songs are removed, while songs similar to accepted ones are added to the playlist. We evaluate the heuristics with hypothetical use cases. For each use case we assume a specific user behavior (e.g. the user always skips songs by a particular artist). Our results show that using audio similarity and simple heuristics it is possible to drastically reduce the number of necessary skips. 1",
        "zenodo_id": 1414932,
        "dblp_key": "conf/ismir/PampalkPW05"
    },
    {
        "title": "Polyphonic Musical Sequence Alignment for Database Search.",
        "author": [
            "Bryan Pardo",
            "Manan Sanghi"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417909",
        "url": "https://doi.org/10.5281/zenodo.1417909",
        "ee": "https://zenodo.org/records/1417909/files/PardoS05.pdf",
        "abstract": "Finding the best matching database target to a melodic query has been of great interest in the music IR world. The string alignment paradigm works well for this task when comparing a monophonic query to a database of monophonic pieces. However, most tonal music is polyphonic, with multiple concurrent musical lines. Such pieces are not adequately represented as strings. Moreover, users often represent polyphonic pieces in their queries by skipping from one part (the soprano) to another (the bass). Current string matching approaches are not designed to handle this situation. This paper outlines approaches to extending string alignment that allow measuring similarity between a monophonic query and a polyphonic piece. These approaches are compared using synthetic queries on a database of Bach pieces. Results indicate that when a monophonic query is drawn from multiple parts in the target, a method which explicitly takes the multi-part structure of a piece into account significantly outperforms the one that does not.",
        "zenodo_id": 1417909,
        "dblp_key": "conf/ismir/PardoS05"
    },
    {
        "title": "Applications of Binary Classification and Adaptive Boosting to the Query-By-Humming Problem.",
        "author": [
            "Charles L. Parker"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416482",
        "url": "https://doi.org/10.5281/zenodo.1416482",
        "ee": "https://zenodo.org/records/1416482/files/Parker05.pdf",
        "abstract": "In the \u0093 query-by-humming\u0094 problem, we attempt to retrieve a speci\u0002 c song from a target set based on a sung query. Recent evaluations of query-by-humming systems show that the state-of-the-art algorithm is a simple dynamic programming-based interval matching technique. Other techniques based on hidden Markov models are far more expensive computationally and do not appear to offer signi\u0002 cant increases in performance. Here, we borrow techniques from arti\u0002 cial intelligence to create an algorithm able to outperform the current state-of-the-art with only a negligible increase in running time. Keywords: melodic retrieval, sequence alignment, arti\u0002 cial intelligence 1",
        "zenodo_id": 1416482,
        "dblp_key": "conf/ismir/Parker05"
    },
    {
        "title": "User Evaluation of a New Interactive Playlist Generation Concept.",
        "author": [
            "Steffen Pauws",
            "Sander van de Wijdeven"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415180",
        "url": "https://doi.org/10.5281/zenodo.1415180",
        "ee": "https://zenodo.org/records/1415180/files/PauwsW05.pdf",
        "abstract": "Selecting the \u2018right\u2019 songs and putting them in the \u2018right\u2019 order are key to a great music listening or dance experience. \u2018SatisFly\u2019 is an interactive playlist generation system in which the user can tell what kind of songs should be contained in what order in the playlist, while she navigates through the music collection. The system uses constraint satisfaction to generate a playlist that meets all user wishes. In a user evaluation, it was found that users created high-quality playlists in a swift way and with little effort using the system, while still having complete control on their music choices. The novel interactive way of creating a playlist, while browsing through the music collection, was highly appreciated. Ease of navigation through a music collection is still an issue that needs further attention. Keywords: playlist generation, user evaluation, constraint satisfaction. 1",
        "zenodo_id": 1415180,
        "dblp_key": "conf/ismir/PauwsW05"
    },
    {
        "title": "Rhythm Classification Using Spectral Rhythm Patterns.",
        "author": [
            "Geoffroy Peeters"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417495",
        "url": "https://doi.org/10.5281/zenodo.1417495",
        "ee": "https://zenodo.org/records/1417495/files/Peeters05.pdf",
        "abstract": "In this paper, we study the use of spectral patterns to represent the characteristics of the rhythm of an audio signal. A function representing the position of onsets over time is first extracted from the audio signal. From this function we compute at each time a vector which represents the characteristics of the local rhythm. Three feature sets are studied for this vector. They are derived from the amplitude of the Discrete Fourier Transform, the AutoCorrelation Function and the product of the DFT and of a Frequency-Mapped ACF. The vectors are then sampled at some specific frequencies, which represents various ratios of the local tempo. The ability of the three feature sets to represent the rhythm characteristics of an audio item is evaluated through a classification task. We show that using such simple spectral representations allows obtaining results comparable to the state of the art. Keywords: rhythm representation, classification 1",
        "zenodo_id": 1417495,
        "dblp_key": "conf/ismir/Peeters05"
    },
    {
        "title": "Classifier Combination for Capturing Musical Variation.",
        "author": [
            "Jeremy Pickens"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418219",
        "url": "https://doi.org/10.5281/zenodo.1418219",
        "ee": "https://zenodo.org/records/1418219/files/Pickens05.pdf",
        "abstract": "At its heart, music information retrieval is characterized by the need to find the similarity between pieces of music. However, \u201csimilar\u201d does not mean \u201cthe same\u201d. Therefore, techniques for approximate matching are crucial to the development of good music information retrieval systems. Yet as one increases the level of approximation, one finds not only additional similar, relevant music, but also a larger number of not-as-similar, non-relevant music. The purpose of this work is to show that if two different retrieval systems do approximate matching in different manners, and both give decent results, they can be combined to give results better than either system individually. One need not sacrifice accuracy for the sake of flexibility. Keywords: Classifier Combination, Approximate Matching 1",
        "zenodo_id": 1418219,
        "dblp_key": "conf/ismir/Pickens05"
    },
    {
        "title": "Markov Random Fields and Maximum Entropy Modeling for Music Information Retrieval.",
        "author": [
            "Jeremy Pickens",
            "Costas S. Iliopoulos"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414716",
        "url": "https://doi.org/10.5281/zenodo.1414716",
        "ee": "https://zenodo.org/records/1414716/files/PickensI05.pdf",
        "abstract": "Music information retrieval is characterized by a number of various user information needs. Systems are being developed that allow searchers to find melodies, rhythms, genres, and singers or artists, to name but a few. At the heart of all these systems is the need to find models or measures that answer the question \u201chow similar are two given pieces of music\u201d. However, similarity has a variety of meanings depending on the nature of the system being developed. More importantly, the features extracted from a music source are often either single-dimensional (i.e.: only pitch, or only rhythm, or only timbre) or else assumed to be orthogonal. In this paper we present a framework for developing systems which combine a wide variety of non-independent features without having to make the independence assumption. As evidence of effectiveness, we evaluate the system on the polyphonic theme similarity task over symbolic data. Nevertheless, we emphasize that the framework is general, and can handle a range of music information retrieval tasks. Keywords: Random Fields, Music Modeling 1",
        "zenodo_id": 1414716,
        "dblp_key": "conf/ismir/PickensI05"
    },
    {
        "title": "A Novel HMM Approach to Melody Spotting in Raw Audio Recordings.",
        "author": [
            "Aggelos Pikrakis",
            "Sergios Theodoridis"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414886",
        "url": "https://doi.org/10.5281/zenodo.1414886",
        "ee": "https://zenodo.org/records/1414886/files/PikrakisT05.pdf",
        "abstract": "This paper presents a melody spotting system based on Variable Duration Hidden Markov Models (VDHMM\u2019s), capable of locating monophonic melodies in a database of raw audio recordings. The audio recordings may either contain a single instrument performing in solo mode, or an ensemble of instruments where one of the instruments has a leading role. The melody to be spotted is presented to the system as a sequence of note durations and music intervals. In the sequel, this sequence is treated as a pattern prototype and based on it, a VDHMM is constructed. The probabilities of the associated VDHMM are determined according to a set of rules that account (a) for the allowable note duration flexibility and (b) with possible structural deviations from the prototype pattern. In addition, for each raw audio recording in the database, a sequence of note durations and music intervals is extracted by means of a multi pitch tracking algorithm. These sequences are subsequently fed as input to the constructed VDHMM that models the pattern to be located. The VDHMM employs an enhanced Viterbi algorithm, previously introduced by the authors, in order to account for pitch tracking errors and performance improvisations of the instrument players. For each audio recording in the database, the best-state sequence generated by the enhanced Viterbi algorithm is further post-processed in order to locate occurrences of the melody which is searched. Our method has been successfully tested with a variety of cello recordings in the context of Western Classical music, as well as with Greek traditional multi-instrument recordings, in which clarinet has a leading role. Keywords: Melody Spotting, Variable Duration Hidden Markov Models. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. c\u20dd2005 Queen Mary, University of London 1",
        "zenodo_id": 1414886,
        "dblp_key": "conf/ismir/PikrakisT05"
    },
    {
        "title": "A Classification Approach to Melody Transcription.",
        "author": [
            "Graham E. Poliner",
            "Daniel P. W. Ellis"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414796",
        "url": "https://doi.org/10.5281/zenodo.1414796",
        "ee": "https://zenodo.org/records/1414796/files/PolinerE05.pdf",
        "abstract": "Melodies provide an important conceptual summarization of polyphonic audio. The extraction of melodic content has practical applications ranging from content-based audio retrieval to the analysis of musical structure. In contrast to previous transcription systems based on a model of the harmonic (or periodic) structure of musical pitches, we present a classification-based system for performing automatic melody transcription that makes no assumptions beyond what is learned from its training data. We evaluate the success of our algorithm by predicting the melody of the ISMIR 2004 Melody Competition evaluation set and on newly-generated test data. We show that a Support Vector Machine melodic classifier produces results comparable to state of the art model-based transcription systems. Keywords: Melody Transcription, Classification 1",
        "zenodo_id": 1414796,
        "dblp_key": "conf/ismir/PolinerE05"
    },
    {
        "title": "A Graphical Model for Recognizing Sung Melodies.",
        "author": [
            "Christopher Raphael"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417052",
        "url": "https://doi.org/10.5281/zenodo.1417052",
        "ee": "https://zenodo.org/records/1417052/files/Raphael05.pdf",
        "abstract": "A method is presented for automatic transcription of sung melodic fragments to score-like representation, including metric values and pitch. A joint model for pitch, rhythm, segmentation, and tempo is defined for a sung fragment. We then discuss the identification of the globally optimal musical transcription, given the observed audio data. A post process estimates the location of the tonic, so the transcription can be presented into they key of C. Experimental results are presented for a small test collection. Keywords: monophonic music recognition, graphical models 1",
        "zenodo_id": 1417052,
        "dblp_key": "conf/ismir/Raphael05"
    },
    {
        "title": "Exploiting Musical Connections: A Proposal for Support of Work Relationships in a Digital Music Library.",
        "author": [
            "Jenn Riley"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415660",
        "url": "https://doi.org/10.5281/zenodo.1415660",
        "ee": "https://zenodo.org/records/1415660/files/Riley05.pdf",
        "abstract": "Musical works in the Western art music tradition exist in a complex, inter-related web. Works that are derivative or part of another work are common; however, most music information retrieval systems, including traditional library catalogs, don\u2019t use these essential relationships to improve search results or provide information about them to end-users. As part of the NSF-funded Variations2 Digital Music Library project at Indiana University, we have developed a set of functional requirements defining how derivative and whole/part relationships between musical works should be acted upon in search results, and how these results should be displayed. This paper describes recent research into these relationships, provides examples why they are important in Western art music, outlines how Variations2 or any other music information retrieval system could use these relationships in matching user queries, and describes optimal displays of these relationships to end-users.",
        "zenodo_id": 1415660,
        "dblp_key": "conf/ismir/Riley05"
    },
    {
        "title": "Exploiting the Tradeoff Between Precision and Cpu-Time to Speed Up Nearest Neighbor Search.",
        "author": [
            "Pierre Roy",
            "Jean-Julien Aucouturier",
            "Fran\u00e7ois Pachet",
            "Anthony Beuriv\u00e9"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417453",
        "url": "https://doi.org/10.5281/zenodo.1417453",
        "ee": "https://zenodo.org/records/1417453/files/RoyAPB05.pdf",
        "abstract": "We describe an incremental filtering algorithm to quickly compute the N nearest neighbors according to a similarity measure in a metric space. The algorithm exploits an intrinsic property of a large class of similarity measures for which some parameter p has a positive influence both on the precision and the cpu cost (precision-cputime tradeoff). The algorithm uses successive approximations of the measure to compute first cheap distances on the whole set of possible items, then more and more expensive measures on smaller and smaller sets. We illustrate the algorithm on the case of a timbre similarity algorithm, which compares gaussian mixture models using a Monte Carlo approximation of the Kullback-Leibler distance, where p is the number of points drawn from the distributions. We describe several Monte Carlo algorithmic variants, which improve the convergence speed of the approximation. On this problem, the algorithm performs more than 30 times faster than the naive approach. Keywords: Nearest Neighbor, Similarity Measure, Timbre, Large Databases. 1",
        "zenodo_id": 1417453,
        "dblp_key": "conf/ismir/RoyAPB05"
    },
    {
        "title": "Specmurt Analysis of Multi-Pitch Music Signals with Adaptive Estimation of Common Harmonic Structure .",
        "author": [
            "Shoichiro Saito",
            "Hirokazu Kameoka",
            "Takuya Nishimoto",
            "Shigeki Sagayama"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417707",
        "url": "https://doi.org/10.5281/zenodo.1417707",
        "ee": "https://zenodo.org/records/1417707/files/SaitoKNS05.pdf",
        "abstract": "This paper describes a multi-pitch analysis method using specmurt analysis with iterative estimation of the quasioptimal common harmonic structure function. Specmurt analysis (Sagayama et al., 2004) is based upon the idea that superimposed harmonic structure pattern can be expressed as a convolution of two components, a fundamental frequency distribution and a \u2018common harmonic structure\u2019 function if each underlying tone component has similar harmonic structure pattern. As proved in our previous work (Sagayama et al., 2004) inappropriate common structure function leads to inaccurate analysis results. The iterative algorithm proposed in this paper automatically chooses a proper structure, which results in finding concurrent multiple fundamental frequencies and reduces the dependency on heuristically chosen initial common harmonic structure. The experimental evaluation showed promising results. Keywords: audio feature extraction, specmurt analysis, visualization of the fundamental frequency. 1",
        "zenodo_id": 1417707,
        "dblp_key": "conf/ismir/SaitoKNS05"
    },
    {
        "title": "Online Database of Scores in the Humdrum File Format.",
        "author": [
            "Craig Stuart Sapp"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417281",
        "url": "https://doi.org/10.5281/zenodo.1417281",
        "ee": "https://zenodo.org/records/1417281/files/Sapp05.pdf",
        "abstract": "KernScores, an online library of musical data currently consisting of over 5 million notes, has been created to assist projects dealing with the computational analysis of musical scores.  The online scores are in a format suitable for processing with the Humdrum Toolkit for Music Research, but the website also provides automatic translations into several other popular data formats for digital musical scores.",
        "zenodo_id": 1417281,
        "dblp_key": "conf/ismir/Sapp05"
    },
    {
        "title": "On the Modeling of Time Information for Automatic Genre Recognition Systems in Audio Signals.",
        "author": [
            "Nicolas Scaringella",
            "Giorgio Zoia"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416064",
        "url": "https://doi.org/10.5281/zenodo.1416064",
        "ee": "https://zenodo.org/records/1416064/files/ScaringellaZ05.pdf",
        "abstract": "The creation of huge databases coming from both restoration of existing analogue archives and new content is demanding fast and more and more reliable tools for content analysis and description, to be used for searches, content queries and interactive access. In that context, musical genres are crucial descriptors since they have been widely used for years to organize music catalogues, libraries and shops. Despite their use musical genres remain poorly defined concepts which make of the automatic classification problem a non-trivial task. Most automatic genre classification models rely on the same pattern recognition architecture: extracting features from chunks of audio signal and classifying features independently. In this paper, we focus instead on the low-level temporal relationships between chunks when classifying audio signals in terms of genre; in other words, we investigate means to model short-term time structures from context information in music segments to consolidate classification consistency by reducing ambiguities. A detailed comparative analysis of five different time modelling schemes is provided and classification results are reported for a database of 1400 songs evenly distributed over 7 genres.",
        "zenodo_id": 1416064,
        "dblp_key": "conf/ismir/ScaringellaZ05"
    },
    {
        "title": "Discovering and Visualizing Prototypical Artists by Web-Based Co-Occurrence Analysis.",
        "author": [
            "Markus Schedl",
            "Peter Knees",
            "Gerhard Widmer"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418315",
        "url": "https://doi.org/10.5281/zenodo.1418315",
        "ee": "https://zenodo.org/records/1418315/files/SchedlKW05.pdf",
        "abstract": "Detecting artists that can be considered as prototypes for particular genres or styles of music is an interesting task. In this paper, we present an approach that ranks artists according to their prototypicality. To calculate such a ranking, we use asymmetric similarity matrices obtained via co-occurrence analysis of artist names on web pages. We demonstrate our approach on a data set containing 224 artists from 14 genres and evaluate the results using the rank correlation between the prototypicality ranking and a ranking obtained by page counts of search queries to Google that contain artist and genre. High positive rank correlations are achieved for nearly all genres of the data set. Furthermore, we elaborate a visualization method that illustrates similarities between artists using the prototypes of all genres as reference points. On the whole, we show how to create a prototypicality ranking and use it, together with a similarity matrix, to visualize a music repository. Keywords: prototypical artist detection, visualization, asymmetric artist similarity, web mining, co-occurrence analysis 1",
        "zenodo_id": 1418315,
        "dblp_key": "conf/ismir/SchedlKW05"
    },
    {
        "title": "Beatbox Classification Using ACE.",
        "author": [
            "Elliot Sinyor",
            "Cory McKay",
            "Rebecca Fiebrink",
            "Daniel McEnnis",
            "Ichiro Fujinaga"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414920",
        "url": "https://doi.org/10.5281/zenodo.1414920",
        "ee": "https://zenodo.org/records/1414920/files/SinyorMFMF05.pdf",
        "abstract": "This paper describes the use of the Autonomous Classification Engine (ACE) to classify beatboxing (vocal percussion) sounds. A set of unvoiced percussion sounds belonging to five classes (bass drum, open hihat, closed hihat and two types of snare drum) were recorded and manually segmented. ACE was used to compare various classification techniques, both with and without feature selection. The best result was 95.55% accuracy using AdaBoost with C4.5 decision tress. Keywords: ACE, beatboxing, classification, feature selection. 1 INTRODUCTION Besides tapping one\u2019s fingers, vocalizing percussion is perhaps the most intuitive way for musicians and nonmusicians alike to express a rhythm. The range of sounds that can be made by one\u2019s mouth, however, is far greater than that of fingers alone. The act of vocalizing percussive sounds is as old as music itself, and almost every culture has its own approach. A notable example is Indian Tabla players\u2019 use of bols, a set of vocal sounds used to express rhythmic phrases. In North American culture, two examples immediately come to mind: 50\u2019s doo-wop, and more recently, beatboxing. Both originated in African-American music. The term \u201cbeatboxing\u201d originally referred to the mimicking of early 80\u2019s drum-machines, also known as beatboxes. While beatboxing was first used as a backing rhythm for rap performance, it has been developed into an art form in and of itself by performers like Biz Markie and Rahzel. In MIR research, the main interest in beatboxing has been in using it as a means of querying stored drum data, but other possibilities exist. Reliable recognition of different drum sounds could serve as a starting point for developing an intuitive rhythm-performance interface. Similarly, it could be used as an input to a metrical analysis system. The methods described here can also be used to develop a more general mouth-based control channel, as the sounds require very little effort to produce and do not necessarily need to be mapped to drum sounds, or even sounds for that matter. This project centres around an attempt to reliably classify five different drum sounds: bass or kick drum, closed hihat, open hihat and two types of snare drum. We began by collecting a set of vocal percussion samples, from both expert beatboxers as well as nonbeatboxers. The recording of beatboxers was carried out primarily as a study on common ways to vocally express drum sounds. For classification, we ran our collected data through the Autonomous Classification Engine (ACE) described in [1]. ACE combines several approaches to classification and can be used to determine which classifiers and features are effective at classifying a given data set. Furthermore, we used a k-nearest neighbour classifier coupled with genetic algorithm (GA) based feature selection (described in [2]) as a baseline to compare with ACE\u2019s performance. 2 RELATED WORK There are numerous publications dealing with classification of instrument sounds and several dealing specifically with drum sounds (e.g., [3]). This problem also closely resembles speech-recognition problems. Since the audio signals in question are for the most part unvoiced and extremely short (20\u2013100ms), pitch-based analysis tools are generally not successful. This rules out many phoneme-based techniques. Approaches based on plosives and fricatives [4\u20136], however, are relevant. Generally, previous attempts have used a number of timbral features and statistical classifiers. In [7], the authors describe a system to retrieve a MIDI drum loop from a bank of recorded drum loops by means of a user beatboxing into a microphone. This system attempts to classify the drum sounds into one of three categories, namely bass drum, snare, and hihat. A 97.3% accuracy is reported using zero-crossing rate as the sole feature. While this result is impressive, simply using zero-crossing rate is unlikely to yield similarly high results for more classes or for a larger number of subjects. In [8], the author describes a variety of spectral and temporal features to classify beatboxing samples into four classes: bass drum, snare drum, closed hihat and open hihat. A success rate of 86% for the training set is Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or com-mercial advantage and that copies bear this notice and the full citation on the first page. \u00a9 2005 Queen Mary, University of London 672 achieved using 24 features and a C4.5 classifier with boosting. Interestingly, the author reports 90% using the same classifier for a previously unseen test set. Somewhat different, but worth mentioning, is the system described in [9], which uses complete syllables (e.g., \u201cdon\u201d, \u201cta\u201d, \u201czur\u201d) to represent drum sounds. Each syllable is subdivided into consonants, vowels, and nasal sounds. 3 DATA COLLECTION",
        "zenodo_id": 1414920,
        "dblp_key": "conf/ismir/SinyorMFMF05"
    },
    {
        "title": "Improving Content-Based Similarity Measures by Training a Collaborative Model.",
        "author": [
            "Richard Stenzel",
            "Thomas Kamps"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416090",
        "url": "https://doi.org/10.5281/zenodo.1416090",
        "ee": "https://zenodo.org/records/1416090/files/StenzelK05.pdf",
        "abstract": "We observed that for multimedia data \u2013 especially music collaborative similarity measures perform much better than similarity measures derived from content-based sound features. Our observation is based on a large scale evaluation with >250,000,000 collaborative data points crawled from the web and >190,000 songs annotated with content-based sound feature sets. A song mentioned in a playlist is regarded as one collaborative data point. In this paper we present a novel approach to bridging the performance gap between collaborative and contentbased similarity measures. In the initial training phase a model vector for each song is computed, based on collaborative data. Each vector consists of 200 overlapping unlabelled 'genres' or song clusters. Instead of using explicit numerical voting, we use implicit user profile data as collaborative data source, which is, for example, available as purchase histories in many large scale ecommerce applications. After the training phase, we used support vector machines based on content-based sound features to predict the collaborative model vectors. These predicted model vectors are finally used to compute the similarity between songs. We show that combining collaborative and content-based similarity measures can help to overcome the new item problem in e-commerce applications that offer a collaborative similarity recommender as service to their customers.",
        "zenodo_id": 1416090,
        "dblp_key": "conf/ismir/StenzelK05"
    },
    {
        "title": "Collecting Ground Truth Annotations for Drum Detection in Polyphonic Music.",
        "author": [
            "Koen Tanghe",
            "Micheline Lesaffre",
            "Sven Degroeve",
            "Marc Leman",
            "Bernard De Baets",
            "Jean-Pierre Martens"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417715",
        "url": "https://doi.org/10.5281/zenodo.1417715",
        "ee": "https://zenodo.org/records/1417715/files/TangheLDLBM05.pdf",
        "abstract": "In order to train and test algorithms that can automatically detect drum events in polyphonic music, ground truth data is needed. This paper describes a setup used for gathering manual annotations for 49 real-world music fragments containing different drum event types. Apart from the drum events, the beat was also annotated. The annotators were experienced drummers or percussionists. This paper is primarily aimed towards other drum detection researchers, but might also be of interest to others dealing with automatic music analysis, manual annotation and data gathering. Its purpose is threefold: providing annotation data for algorithm training and evaluation, describing a practical way of setting up a drum annotation task, and reporting issues that came up during the annotation sessions while at the same time providing some thoughts on important points that could be taken into account when setting up similar tasks in the future.",
        "zenodo_id": 1417715,
        "dblp_key": "conf/ismir/TangheLDLBM05"
    },
    {
        "title": "Classification of Musical Metre with Autocorrelation and Discriminant Functions.",
        "author": [
            "Petri Toiviainen",
            "Tuomas Eerola"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416040",
        "url": "https://doi.org/10.5281/zenodo.1416040",
        "ee": "https://zenodo.org/records/1416040/files/ToiviainenE05.pdf",
        "abstract": "The performance of autocorrelation-based metre induction was tested with two large collections of folk melodies, consisting of approximately 13,000 melodies in MIDI file format, for which the correct metres were available. The analysis included a number of melodic accents assumed to contribute to metric structure. The performance was measured by the proportion of melodies whose metre was correctly classified by Multiple Discriminant Analysis. Overall, the method predicted notated metre with an accuracy of 75 % for classification into nine categories of metre. The most frequent confusions were made within the groups of duple and triple/compound metres, whereas confusions across these groups where significantly less frequent. In addition to note onset locations and note durations, Thomassen's melodic accent was found to be an important predictor of notated metre. Keywords: Metre, classification, autocorrelation 1 INTRODUCTION Most music is organized to contain temporal periodicities that evoke a percept of regularly occurring pulses, or beats. The period of the most salient pulse is typically within the range of 400 to 900 ms [1-3]. The perceived pulses are often hierarchically organized and consist of at least two simultaneous levels, whose periods have an integer ratio. This gives rise to a percept of regularly alternating strong and weak beats, a phenomenon referred to as metre [4,5]. In Western music, the ratio of the pulse lengths is usually limited to 1:2 (duple metre) and 1:3 (triple metre).  Metre in which each beat has three subdivisions, such as 6/8 or 9/8, is referred to as compound metre. A number of computational models have been developed for the extraction of the basic pulse from music. Modelling of metre perception has, however, received less attention. Large and Kolen [6] presented a model of metre perception based on resonating oscillators. Toiviainen [7] presented a model of competing subharmonic oscillators for determining the metre (duple vs. triple) from an acoustical representation of music. Brown [8] proposed a method for determining the metre of musical scores by applying autocorrelation to a temporal function consisting of impulses at each tone onset whose heights are weighted by the respective tone durations. A shortcoming of Brown's study [8] is that it fails to provide any explicit criteria for the determination of metre from the autocorrelation function. Frieler [9] presents a model based on autocorrelation of gaussified onsets for the determination of metre from performed MIDI files. Pikrakis, Antonopoulos, and Theodoridis [10] present a method for the extraction of music metre and tempo from raw polyphonic audio recordings based on selfsimilarity analysis of mel-frequency cepstral coefficients. When tested with a corpus of 300 recordings, the method achieved a 95 % correct classification rate. Temperley and Sleator [11] present a preference-rule model of metre-finding. An overview of models of metrical structure is provided in [12]. Although there is evidence that the pitch information present in music may affect the perception of pulse and metre [13-15], most models of pulse and metre finding developed to date rely only on note onset times and durations. Dixon and Cambouropoulos [16], however, proposed a multi-agent model for beat tracking that makes use of pitch and amplitude information. They found that including this information when determining the salience of notes significantly improved the performance of their model. Vos, van Dijk, and Schomaker [17] applied autocorrelation to the determination of metre in predominantly isochronous music. They utilized a method similar to that proposed in [8], except for using the melodic intervals between subsequent notes to represent the accent of each note. In a previous study [18], we applied discriminant function analysis to autocorrelation functions calculated from Brown's [8] impulse functions for classification of folk melodies into duple vs. triple/compound metre. Using two large folk song collections with a total of 12,368 melodies, we obtained a correct classification rate of 92 %. Furthermore, we examined whether the inclusion of different melodic accent types would improve the classification performance. By determining the components of the autocorrelation functions that were significant in the classification, we found that periodicity in note onset locations above the measure level was the most important cue for the determination of metre. Of the melodic accents included, Thomassen's [14] melodic accent provided the most reliable cues for Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. \u00a9 2005 Queen Mary, University of London 351",
        "zenodo_id": 1416040,
        "dblp_key": "conf/ismir/ToiviainenE05"
    },
    {
        "title": "Query-By-Example Technique for Retrieving Cover Versions of Popular Songs with Similar Melodies.",
        "author": [
            "Wei-Ho Tsai",
            "Hung-Ming Yu",
            "Hsin-Min Wang"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415200",
        "url": "https://doi.org/10.5281/zenodo.1415200",
        "ee": "https://zenodo.org/records/1415200/files/TsaiYW05.pdf",
        "abstract": "Retrieving audio material based on audio queries is an important and challenging issue in the research field of content-based access to popular music. As part of this research field, we present a preliminary investigation into retrieving cover versions of songs specified by users. The technique enables users to listen to songs with an identical tune, but performed by different singers, in different languages, genres, and so on. The proposed system is built on a query-by-example framework, which takes a fragment of the song submitted by the user as input, and returns songs similar to the query in terms of the main melody as output. To handle the likely discrepancies, e.g., tempos, transpositions, and accompaniments between cover versions and the original song, methods are presented to remove the non-vocal portions of the song, extract the sung notes from the accompanied vocals, and compare the similarities between the sung note sequences.",
        "zenodo_id": 1415200,
        "dblp_key": "conf/ismir/TsaiYW05"
    },
    {
        "title": "A Survey of Music Information Retrieval Systems.",
        "author": [
            "Rainer Typke",
            "Frans Wiering",
            "Remco C. Veltkamp"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417383",
        "url": "https://doi.org/10.5281/zenodo.1417383",
        "ee": "https://zenodo.org/records/1417383/files/TypkeWV05.pdf",
        "abstract": "This survey paper provides an overview of content-based music information retrieval systems, both for audio and for symbolic music notation. Matching algorithms and indexing methods are briefly presented. The need for a TREC-like comparison of matching algorithms such as MIREX at ISMIR becomes clear from the high number of quite different methods which so far only have been used on different data collections. We placed the systems on a map showing the tasks and users for which they are suitable, and we find that existing content-based retrieval systems fail to cover a gap between the very general and the very specific retrieval tasks. Keywords: MIR, matching, indexing. 1",
        "zenodo_id": 1417383,
        "dblp_key": "conf/ismir/TypkeWV05"
    },
    {
        "title": "Separation of Vocals from Polyphonic Audio Recordings .",
        "author": [
            "Shankar Vembu",
            "Stephan Baumann 0001"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414852",
        "url": "https://doi.org/10.5281/zenodo.1414852",
        "ee": "https://zenodo.org/records/1414852/files/VembuB05.pdf",
        "abstract": "Source separation techniques like independent component analysis and the more recent non-negative matrix factorization are gaining widespread use for the monaural separation of individual tracks present in a music sample. The underlying principle behind these approaches characterises only stationary signals and fails to separate nonstationary sources like speech or vocals. In this paper, we make an attempt to solve this problem and propose solutions to the extraction of vocal tracks from polyphonic audio recordings. We also present techniques to identify vocal sections in a music sample and design a classifier to perform a vocal\u2013nonvocal segmentation task. Finally, we describe an application wherein we try to extract the melody from the separated vocal track using existing monophonic transcription techniques. The experimental work leads us to the conclusion that the quality of vocal source separation, albeit satisfactory, is not sufficient enough for further F0 analysis to extract the melody line from the vocal track. We identify areas that need further investigation to improve the quality of vocal source separation. Keywords: Blind source separation, independent component analysis, non-negative matrix factorization, vocal\u2013 nonvocal discrimination, melody extraction. 1",
        "zenodo_id": 1414852,
        "dblp_key": "conf/ismir/VembuB05"
    },
    {
        "title": "A Music Retrieval System Based on User Driven Similarity and Its Evaluation.",
        "author": [
            "Fabio Vignoli",
            "Steffen Pauws"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418359",
        "url": "https://doi.org/10.5281/zenodo.1418359",
        "ee": "https://zenodo.org/records/1418359/files/VignoliP05.pdf",
        "abstract": "Large music collections require new ways to let users interact with their music. The concept of finding \u2018similar\u2019 songs, albums, or artists provides handles to users for easy navigation and instant retrieval. This paper presents the realization and user evaluation of a music retrieval music that sorts songs on the basis of similarity to a given seed song. Similarity is based on a userweighted combination of timbre, genre, tempo, year, and mood. A conclusive user evaluation assessed the usability of the system in comparison to two control systems in which the user control of defining the similarity measure was diminished.",
        "zenodo_id": 1418359,
        "dblp_key": "conf/ismir/VignoliP05"
    },
    {
        "title": "Herding Folksongs.",
        "author": [
            "Robert Young Walser"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415640",
        "url": "https://doi.org/10.5281/zenodo.1415640",
        "ee": "https://zenodo.org/records/1415640/files/Walser05.pdf",
        "abstract": "Cataloging a large, multi-media collection of traditional song and drama in preparation for online presentation highlights issues of song identity and access in the context of contemporary digitized archives. In the James Madison Carpenter collection a particular folksong sung by a particular individual may exist in multiple manifestations: typed song text, sound recording(s), and/or manuscript music notation. While controlled vocabulary",
        "zenodo_id": 1415640,
        "dblp_key": "conf/ismir/Walser05"
    },
    {
        "title": "Finding An Optimal Segmentation for Audio Genre Classification.",
        "author": [
            "Kris West",
            "Stephen Cox"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416746",
        "url": "https://doi.org/10.5281/zenodo.1416746",
        "ee": "https://zenodo.org/records/1416746/files/WestC05.pdf",
        "abstract": "In the automatic classification of music many different segmentations of the audio signal have been used to calculate features. These include individual short frames (23 ms), longer frames (200 ms), short sliding textural windows (1 sec) of a stream of 23 ms frames, large fixed windows (10 sec) and whole files. In this work we present an evaluation of these different segmentations, showing that they are sub-optimal for genre classification and introduce the use of an onset detection based segmentation, which appears to outperform all of the fixed and sliding windows segmentation schemes in terms of classification accuracy and model size. Keywords: genre, classification, segmentation, onset, detection 1",
        "zenodo_id": 1416746,
        "dblp_key": "conf/ismir/WestC05"
    },
    {
        "title": "Efficient Melody Retrieval with Motif Contour Classes.",
        "author": [
            "Tillman Weyde",
            "Christian Datzko"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414738",
        "url": "https://doi.org/10.5281/zenodo.1414738",
        "ee": "https://zenodo.org/records/1414738/files/WeydeD05.pdf",
        "abstract": "This paper describes the use of motif contour classes for efficient retrieval of melodies from music collections. Instead of extracting incipits or themes, complete monophonic pieces are indexed for their motifs, using classes of motif contours. Similarity relations between these classes can be used for a very efficient search. This can serve as a first level search, which can be refined by using more computationally intensive comparisons on its results. The model introduced has been implemented and tested using the MUSITECH framework. We present empirical and analytical results on the retrieval quality, the complexity, and quality/efficiency trade-off. Keywords: melody retrieval, motivic analysis, melodic similarity, retrieval efficiency 1",
        "zenodo_id": 1414738,
        "dblp_key": "conf/ismir/WeydeD05"
    },
    {
        "title": "On Techniques for Content-Based Visual Annotation to Aid Intra-Track Music Navigation.",
        "author": [
            "Gavin Wood",
            "Simon O&apos;Keefe"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417401",
        "url": "https://doi.org/10.5281/zenodo.1417401",
        "ee": "https://zenodo.org/records/1417401/files/WoodO05.pdf",
        "abstract": "Despite the fact that people are increasingly listening to music electronically, the core interface of the common tools for playing the music have had very little improvement. In particular the tools for intra-track navigation have remained basically static, not taking advantage of recent studies into the field of audio jisting, summarising and segmentation. We introduce a novel mechanism for musical audio linear summarisation and modify a widely used open source media player to utilise several music information retrieval techniques directly in the graphical user interface. With a broad range of music, we provide a qualitative discussion on several techniques used for contentbased music information retrieval and perform quantitative investigation to their usefulness. 1",
        "zenodo_id": 1417401,
        "dblp_key": "conf/ismir/WoodO05"
    },
    {
        "title": "A Partial Searching Algorithm and Its Application for Polyphonic Music Transcription.",
        "author": [
            "Wen Xue",
            "Mark Sandler 0001"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415740",
        "url": "https://doi.org/10.5281/zenodo.1415740",
        "ee": "https://zenodo.org/records/1415740/files/XueS05.pdf",
        "abstract": "This paper proposes an algorithm for studying spectral contents of pitched sounds in real-world recordings. We assume that the 2nd-order difference, w.r.t. partial index, of a pitched sound is bounded by some small positive value, rather than equal to 0 in a perfect harmonic case. Given a spectrum and a fundamental frequency f0, the algorithm searches the spectrum for partials that can be associated with f0 by dynamic programming. In section 3 a background-foreground model is plugged into the algorithm to make it work with reverberant background, such as in a piano recording. In section 4 we illustrate an application of the algorithm in which a multipitch scoring machine, which involves special processing for close or shared partials, is coupled with a tree searching method for polyphonic transcription task. Results are evaluated on the traditional note level, as well as on a partial-based sub-note level.",
        "zenodo_id": 1415740,
        "dblp_key": "conf/ismir/XueS05"
    },
    {
        "title": "Towards a Fast and Efficient Match Algorithm for Content-Based Music Retrieval on Acoustic Data.",
        "author": [
            "Yi Yu 0001",
            "Chiemi Watanabe",
            "Kazuki Joe"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415874",
        "url": "https://doi.org/10.5281/zenodo.1415874",
        "ee": "https://zenodo.org/records/1415874/files/YuWJ05.pdf",
        "abstract": "In this paper we present a fast and efficient match algorithm, which consists of two key techniques: Spectral Correlation Based Feature Merge(SCBFM) and Two-Step Retrieval(TSR). SCBFM can remove the redundant information. In consequence, the resulting feature sequence has a smaller size, requiring less storage and computation. In addition, most of the tempo variation is removed; thus a much simpler sequence match method can be adopted. Also, TSR relies on the characteristics of Mel-Frequency Cepstral Coefficient(MFCC), where the precise match in the second step depends on the first step to filter out most of the dissimilar references with only the low order MFCC feature. As a result, the whole retrieval speed can be further improved. The experimental evaluation verifies that SCBFM-TSR yields more meaningful results in comparatively short time. The experiment results are analyzed with a theoretical approach that seeks to find the relation between Spectral Correlation(SC) threshold and storage, computation. Keywords: Content based music retrieval, spectral correlation, dynamic programming, feature merge, prefiltering. 1",
        "zenodo_id": 1415874,
        "dblp_key": "conf/ismir/YuWJ05"
    },
    {
        "title": "ISMIR 2005, 6th International Conference on Music Information Retrieval, London, UK, 11-15 September 2005, Proceedings",
        "author": [],
        "year": "2005",
        "doi": "10.5281/zenodo.1284501",
        "url": "https://doi.org/10.5281/zenodo.1284501",
        "ee": null,
        "abstract": "This release contains the annotations and the scores to test the audio-score alignment methodology explained in:\n\n\n\u015eentrk, S., Gulati, S., and Serra, X. (2014). Towards alignment of score and audio recordings of Ottoman-Turkish makam music. In Proceedings of 4th International Workshop on Folk Music Analysis, pages 5760, Istanbul, Turkey.\n\n\nThe dataset in this release is derived from the transcription test dataset used in the paper:\n\n\nBenetos, E.  Holzapfel, A. (2013). Automatic transcription of Turkish makam music. In Proceedings of 14th International Society for Music Information Retrieval Conference, 4 8 Nov 2013, Curitiba, PR, Brazil.\n\n\nThe scores for each composition are obtained from the SymbTr collection explained in:\n\n\nKaraosmano\u011flu, K. (2012). A Turkish makam music symbolic database for music information retrieval: SymbTr. In Proceedings of 13th International Society for Music Information Retrieval Conference (ISMIR), pages 223228.\n\n\nFrom the annotated score onsets for some of the above recordings only the main singing voice segments have been selected. Further separately only a subset of vocal onsets crresponding to phoneme transitions rules have been explicitly annotated as annotationOnsets.txt\n\n\nDzhambazov, G., Srinivasamurthy A., \u015eentrk S.,  Serra X. (2016).On the Use of Note Onsets for Improved Lyrics-to-audio Alignment in Turkish Makam Music. 17th International Society for Music Information Retrieval Conference (ISMIR 2016\n\n\nUsing this dataset\n\nPlease cite the above publications if you use this dataset in a publication.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\n\n\nhttp://compmusic.upf.edu/node/233",
        "zenodo_id": 1284501,
        "dblp_key": "conf/ismir/2005"
    }
]