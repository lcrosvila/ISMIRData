[
    {
        "title": "Theory and Evaluation of a Bayesian Music Structure Extractor.",
        "author": [
            "Samer A. Abdallah",
            "Katy C. Noland",
            "Mark B. Sandler",
            "Michael A. Casey",
            "Christophe Rhodes"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416018",
        "url": "https://doi.org/10.5281/zenodo.1416018",
        "ee": "https://zenodo.org/records/1416018/files/AbdallahNSCR05.pdf",
        "abstract": "We introduce a new model for extracting classified structural segments, such as intro, verse, chorus, break and so forth, from recorded music. Our approach is to classify signal frames on the basis of their audio properties and then to agglomerate contiguous runs of similarly classified frames into texturally homogenous (or ‘self-similar’) segments which inherit the classificaton of their consituent frames. Our work extends previous work on automatic structure extraction by addressing the classification problem using using an unsupervised Bayesian clustering model, the parameters of which are estimated using a variant of the expectation maximisation (EM) algorithm which includes deterministic annealing to help avoid local optima. The model identifies and classifies all the segments in a song, not just the chorus or longest segment. We discuss the theory, implementation, and evaluation of the model, and test its performance against a ground truth of human judgements. Using an analogue of a precisionrecall graph for segment boundaries, our results indicate an optimal trade-off point at approximately 80% precision for 80% recall. Keywords: structure, segmentation, boundary, audio 1",
        "zenodo_id": 1416018,
        "dblp_key": "conf/ismir/AbdallahNSCR05",
        "keywords": [
            "classification",
            "audio properties",
            "signal frames",
            "agglomerate",
            "contiguous runs",
            "self-similar segments",
            "Bayesian clustering",
            "unsupervised",
            "EM algorithm",
            "deterministic annealing"
        ],
        "content": "THEORY AND EVALUATION OF A BAYESIAN MUSIC STRUCTURE\nEXTRACTOR\nSamer Abdallah, Katy Noland, Mark Sandler\nCentre for Digital Music\nQueen Mary, University of London\nMile End Road, London E1, UK\nsamer.abdallah@elec.qmul.ac.uk\nkaty.noland@elec.qmul.ac.uk\nmark.sandler@elec.qmul.ac.ukMichael Casey, Christophe Rhodes\nCentre for Cognition, Computation and Culture\nGoldsmiths College, University of London\nNew Cross Gate, London SE14 6NW, UK\nm.casey@gold.ac.uk\nc.rhodes@gold.ac.uk\nABSTRACT\nWe introduce a new model for extracting classiﬁed struc-\ntural segments, such as intro,verse,chorus,breakand so\nforth, from recorded music. Our approach is to classify\nsignal frames on the basis of their audio properties and\nthen to agglomerate contiguous runs of similarly classi-\nﬁed frames into texturally homogenous (or ‘self-similar’)\nsegments which inherit the classiﬁcaton of their con-\nsituent frames. Our work extends previous work on au-\ntomatic structure extraction by addressing the classiﬁca-\ntion problem using using an unsupervised Bayesian clus-\nteringmodel,theparametersofwhichareestimatedusing\navariantoftheexpectationmaximisation(EM)algorithm\nwhich includes deterministic annealing to help avoid lo-\ncaloptima. Themodelidentiﬁesandclassiﬁesalltheseg-\nments in a song, not just the chorus or longest segment.\nWe discuss the theory, implementation, and evaluation of\nthe model, and test its performance against a ground truth\nof human judgements. Using an analogue of a precision-\nrecall graph for segment boundaries, our results indicate\nanoptimaltrade-offpointatapproximately80%precision\nfor 80% recall.\nKeywords: structure, segmentation, boundary, audio\n1 INTRODUCTION\nMethods for automatically segmenting music recordings\ninto structural segments, such as verseandchorus, have\nimmediate applications in music summarization, song\nidentiﬁcation, feature segmentation, feature compressio n\nand content-based music query systems. In order to eval-\nuate an automatically-generated segmentation, however,\nwe must develop an understanding of both the act of seg-\nmentationandtheusetowhichasegmentationwillbeput.\nThenotionof‘asegment’isintimatelyboundupwith\nthe notion of ‘a boundary’. It would be difﬁcult to dis-\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrstpage.\nc/circlecopyrt2005 Queen Mary, University of Londonagree with the proposition that, if a segment is (or is at\nleast associated with) a temporal interval deﬁned by its\nend-points, then these end-points must be ‘boundaries’\n(in a sense which we intentionally leave undeﬁned at this\nstage). Conversely,onemightwishtoarguethattheinter-\nvalbetweenanytwoconsecutiveboundariesisasegment.\nDoesthisprecludethepossibilitythattheintervalbetwee n\ntwo non-consecutive boundaries is also a segment, per-\nhapsonalargerscale? Furthermore,onecouldarguethat,\neven if every boundary must be the start or end of some\nsegment,theintervalsbetweencertainpairsofboundaries ,\nsuchasthegapbetweentwotracksonaCD,neednothave\nthe same ontological status as more substantive events,\nsuch as a verse or a drum solo. (To give a visual analogy,\nthe space between objects is not necessarily an object.)\nThus, we may conclude (a) that an enumeration of\nsegments necessarily ﬁxes all the boundaries, but (b) that\nthe boundaries do not necessarily determine the segments\nwithout further information. In fact, the models we dis-\ncuss in this paper are so constructed that the segments are\nindeed uniquely determined by the boundaries.\nOnce we have come to a logically consistent posi-\ntionontherelationshipbetweensegmentsandboundaries,\nthereremainsthequestionofwhatcriteriawearegoingto\nuse to deﬁne and detect them. One approach, as exempli-\nﬁedbymostthemethodssummarizedbelowaswellasour\nown contribution, is to consider some local properties of\nthe signal (a sort of generalised ‘texture’) and assert that\nthe segments are ‘texturally’ homogenous regions over\nwhich those properties are relatively constant. A corol-\nlary of this is that the boundaries can only appear where\nthere is a change in the local texture. Whilst this has been\nthe most common approach to segmentation from audio,\nitwillfailincertaincircumstances: considerasongwhich\ncontainstwoseparatedversesintheﬁrsthalfbuttwocon-\nsecutiveversesinthesecond. Ifwesuccessfullyidentifya\nlocal property which corresponds to ‘versiness’, that is, i t\nis true whenever a verse is in progress , we will detect the\nﬁrsttwoversesbutothertwowillbemergedintoonelong\nverse, even if there are other features marking the bound-\narybetweenthetwo. Thisapproachisthereforeincapable\nof detecting what one might call ‘unitary’ or ‘gestalt’ or\n‘countable’ events; only that a certain type of event or\nprocess is occurring. Such distinctions are examined at\ngreat length in the literature on temporal logics and event\ncalculi (eg,. Allen, 1984; Galton, 1987).\n420Assuming an approach based on textural similarity, a\ncommonlyusedtacticiswhatonemightcall‘atomisation-\nclustering-agglomeration,’whichinvolvesthreesteps: ( 1)\ndivide the signal into a number of equal length fragments\nat thetemporal resolution required for the boundaries and\ncompute the value of the textural property (or feature tu-\nple) for each fragment; (2) cluster the collection of prop-\nertyvaluesignoring the temporal relationships between\nthe fragments to which they belong and thereby assign\na class label to each fragment; (3) agglomerate runs of\nequally classiﬁed fragments into segments. In addition,\nthe segments themselves can inherit the classiﬁcation of\ntheir constituent fragments. This algorithm is liable to\nproduce excessively fragmented segments if the clusters\nidentiﬁed at stage (2) overlap, since fragments are classi-\nﬁed without regard to the classiﬁcations of their temporal\nneighbours. This behaviour can be traced to a failure to\nencode our prior expectations about the durations of the\nsegments we wish to detect. Indeed, this is an impor-\ntant factor in the segmentation process since there may\nbe many valid segmentations of a piece of music, distin-\nguished by their different time scales.\nIn the following sections, we discuss previous work\non audio segmentation, and present an atomisation-\nclustering-agglomeration algorithm built around a proba-\nbilisticclusteringmodel,whichclassiﬁesallthesegment s\nfound not just the ‘key’ segment or chorus. We evaluate\nour model against a ground truth of structural segmenta-\ntions for a set of 14 popular song recordings, and discuss\nplanned extensions to our system.\n1.1 Segmentation by timbre\nIf broad spectral features are used to assess textural\nsimilarity, then we obtain what is essentially a timbre\nbased segmentation resulting in timbrally homogenous\nsegments. This is the approach taken by Aucouturier\net al. (2005), who use mel frequency cepstral coefﬁcients\n(MFCCs), which are selectivity of wide-band modulation\nin the source power spectrum whilst remaining relatively\ninvariant to ﬁne spectral structure.\nFoote (1999) proposed the dissimilarity matrix or S-\nmatrix, containing a measure of dissimilarity for all pairs\noffeaturetuples,formusicstructureanalysisusingMFCC\nfeatures. With the initial analysis at 100 fragments per\nsecond, this means that a 3-minute song produces an\n18000 ×18000 S-matrix. This extremely large, dense\ndata object is the basis for the proposed methods, which\nare related to the recurrence plots discussed in Eckmann\net al. (1987); for instance, Foote proposed that the chorus\nshould be labelled as the longest ‘self-similar’ segment\nusing a cosine distance measure and MFCC features.\nLogan and Chu (2000) proposed a method for sum-\nmarization, also using MFCCs, employing both Hidden-\nMarkov Models (HMMs) and threshold-based cluster-\ning methods, grouping features into key song segments.\nPeeters et al. (2002) propose a multi-pass clustering ap-\nproachthatusesboth k-meansandHMM-basedclustering\nusing multi-scale MFCC features. However, these studies\nprovide no measure of performance for all segments in a\nsong.1.2 Segmentation by harmony\nSome recent studies addressed the structure extraction\nproblemintermsofharmonicratherthantimbralfeatures.\nForexampleWakeﬁeld(1999)proposedchromagramfea-\nturesthatrepresentthedistributionofpowerspectrumen-\nergies among the twelve equal-temperament pitch classes\nbased on A440, providing invariance to timbral changes\nin repeated segments.\nOne desirable property of harmonic features is the\npossibilityofimplementingexplicittranspositionalinv ari-\nance. Goto (2003) describes a system called RefraiDthat\nlocatesrepeatedstructuralsegmentsindependentoftrans -\nposition. The RefraiDsystem is able to track a chorus,\nfor example, even if it modulates up a sequence of semi-\ntone key changes. The problem of chorus extraction was\ndivided into four stages: computation of acoustic features\nand similarity measures; repetition judgement criterion;\nestimating end-points of repeated sections; and detecting\nmodulated repetitions. This was the ﬁrst work to explore\nthe extraction of multiple structural segment types, i.e.\nverseandintroas well as chorus. The results for chorus\ndetection were reported as accurate for 80 of 100 songs.\nHowever, the quality of the segmentation for non-chorus\nsegments was not evaluated inthat study.\nDannenbergandHu(2002)alsodescribeasystemthat\nusedagglomerativeclusteringwithchroma-basedfeatures\nformusicstructureanalysisofasmallsetofJazzandClas-\nsicalpieces. Theydonotreportanevaluationofthemeth-\nods over a corpus.\n1.3 Segmentation by rhythm and pitch\nSymbolicapproachestostructureanalysisattempttoiden-\ntify the repeated thematic material in string-based mu-\nsic representations. Whilst these methods show much\npromise in identifying structure from score information,\ntheyarenotwelladaptedforuseinstructureanalysisfrom\naudio,largelyduetotheadditionofsigniﬁcantuncertaint y\nin audio representations.\nThere has recently been some work on combined au-\ndio and symbolic representations, attempting to unify the\ndifferent views of similarity. Maddage et al. (2004) de-\nscribe a system in which a partial transcription is used to\nmake decisions about structure, integrating beat tracking ,\nrhythm extraction, chord detection and melodic similar-\nity in a heuristic framework for detecting all segments in\na song. They also propose using octave-scale rather than\nmel-frequencyscalecepstralcoefﬁcientsaspitch-orient ed\nrepresentation. The authors report 100% accuracy for de-\ntecting instrumental sections in songs, and report results\nfor detection and labelling of verse,chorus,bridge,in-\ntroandoutrosections. Similarly, Lu et al. (2004) de-\nscribeanHMM-basedapproachtosegmentationthatused\na1\n12th-octaveconstant- Qﬁlterbankforpitchselectivityin\naddition to MFCC features. They report improved per-\nformance in segmentation for the constant- Qtransform\nwhen used with MFCCs over use of MFCCs alone. Both\nof these methods used an S-matrix approach with an ex-\nhaustive search to ﬁnd the best ﬁt segment boundaries to\na given objective function.\n4212 SEGMENTATION METHODS\nOur segmentation algorithm follows the atomisation-\nclustering-agglomeration approach described earlier, bu t\nseveral steps are required to compute the feature tuples,\nwhich are actually short-term histograms over state occu-\npancy in a hidden Markov model (see section 2.1). These\nhistograms are subsequently clustered using one of two\nmethods described in sections 2.2 and 2.3.\n2.1 Feature extraction\nThe processing chain begins with mono audio in WAVE\nformat(IBM,1991)andbreaksitintoasequenceofshort\noverlappingfragments. Thisisthenreducedtoasequence\nof discrete valued HMM states, going via a constant- Q\nlog-power spectrum, normalisation to provide invariance\nto gross dynamics, and dimensionality reduction using\nPCA. The resulting 20-dimensional feature tuples repre-\nsent the short-term power spectrum in a way compara-\nble to the ﬁrst 20 MFCCs, but using PCA results in the\nbest (in a least-squares sense) low-dimensional approxi-\nmation to the normalised log-power spectra. A Gaussian-\nobservation HMM is then ﬁtted to the sequence of PCA\ncoefﬁcients and the most probable state path inferred us-\ning the Viterbi algorithm1. Finally, a sequence of short-\nterm state occupancy histograms are formed using a slid-\ning window. For example, if the HMM has 20 states and\nthe histogram window covers 15 states, then each his-\ntogramhasatotalbincountof15distributedover20bins.\n2.2 Pairwise clustering\nThe histograms resulting from the above processing steps\ninhabit a space which is not self-evidently Euclidean;\nclustering methods based on Euclidean feature values are\ntherefore not trivially applicable. One way to proceed is\nto deﬁne an empirical dissimilarity measure between ob-\nserved windowed state histograms with reasonable prop-\nerties: histograms with the same distribution should be\nmaximally similar, while those with no overlap should be\nmaximally dissimilar.\nOne such measure is the cosine dissimilarity measure\nas used by Foote (1999): using the vectors xandx′to\ndenote two l2-normalized histograms, this is deﬁned as\ndc(x,x′) = cos−1(x·x′).\nAs an alternative, we propose a symmetrization of\nthe Kullback-Leibler divergence based on the interpre-\ntation of the histograms as summaries of data drawn\nfrom a multinomial probability distribution. With l1-\nnormalized histograms x,x′, we set dkl(x,x′) =/summationtextM\ni=1[xilog (xi/qi) +x′\nilog (x′\ni/qi)]where qi=1\n2(xi+\nx′\ni)andMis the number of bins in the histograms. This\ncan be interpreted as the sum of the KL divergences from\neither histogram to their mutual average q.\nThese pairwise distances are then used in assigning\nframestoclustersusinganalgorithmduetoHofmannand\n1These preprocessing stages correspond closely to descrip-\ntorsAudioSpectrumEnvelopeD ,AudioSpectrum-\nProjectionD ,SoundModelDS andSoundModel-\nStatePathD , deﬁned in the MPEG-7 standard (Casey, 2001;\nISO, 2002).Buhmann(1997)whichusesaformofmean-ﬁeldanneal-\ning to minimise a cost function while avoiding local min-\nima.\n2.3 Histogram clustering\nSincethedatawewishtoclusterarehistogramsrepresent-\ning a distribution over a discrete feature space (the HMM\nstates), we may, following Puzicha et al. (1999), consider\neach underlying class to determine a probability distribu-\ntion over the feature space. The observed histograms are\nthen modelled as the result of drawing samples from one\nofthesedistributions. Thisleadsquitenaturallytoaprob -\nabilistic latent variable model with an optimisable likeli -\nhood function.\nAssuming the existence of Kunderlying classes, the\ndiscretedistributionsareparameterisedbyan M×Kma-\ntrixA, such that Ajkis the probability of observing the\njthHMMstateinwhileintheregimemodelledbythe kth\nclass. If C∈(1..K)Lis the sequence of class assigments\nfor a given sequence of histograms X∈NM×L, then the\noverall log-likelihood of the model reduces to\nHh=L/summationdisplay\ni=1M/summationdisplay\nj=1K/summationdisplay\nk=1δ(k,Ci)XjilogXji\nAjk(1)\nwhere Lis the total number of histograms being consid-\nered, each of which relates to a certain fragment of the\noriginal signal. This cost function is optimised using a\nform of deterministic annealing as described by Puzicha\net al. (1999), which is equivalent to expectation maximi-\nsation(Dempsteretal.,1977)witha‘temperature’param-\neterwhichgraduallyfallstozero. Theendresultisamax-\nimuma posteriori estimate for the class assignments C\nand the class-conditional distributions A.\n3 EXPERIMENTS\nWe performed segmentations using the above-described\nmethods on 14 popular music songs from Sony’s cata-\nlogue,whichhadbeendown-sampledto11kHzmonobe-\nforebeingdistributedtotheMPEG-7workinggroup. The\nconstant- Qspectrograms were computed every 200ms\nover 600ms frames and at a resolution of1\n8-octave. The\nnormalised log-power spectra were then encoded using\ntheir ﬁrst 20 principal components. HMMs were trained\nwith10,20,40and80states,andthestateoccupancyhis-\ntograms were computed over windows of 15 states with\na hop size of 1. Both clustering algorithms were applied\nwithbetween2and10classes,resultinginsegmentations\nwith between 2 and 10 segment types. A sample segmen-\ntation,alongwithsomeoftheintermediateresults,ispre-\nsented in ﬁgure 1.\n4 EVALUATION\nIn order to evaluate the segmentations, they were com-\npared against a ground truth consisting of annotations\nmade by an expert listener, giving, for each ground truth\nsegment, a start time in seconds, an end time in seconds\nand a label.\n422Nirvana:Smells Like Teen Spirit\n80 state HMM histograms\npairwise(kl) : regions(0.2299,0.03494,0.8676), info(0.6786,2.069,0.6019)\nhistclust(mf) : regions(0.2369,0.06965,0.8467), info(0.5502,2.148,0.523)\n0 50 100 150 200 250annotation\ntime/s\nFigure1: Asegmentationofasamplefromthetestset,compar ingtheresultsofdyadicclustering(usingthesymmetrized\nKullback-Leibler distance) and the histogram clustering a lgorithm, both with 7 clusters. The constant- Qspectrogram\nis displayed in the top panel. The ‘ground truth’ annotation s are displayed as different shades of grey for the different\nsegmentlabels. Notehowtheﬁfthsegmentanditsrepeatshav ebeensplitovertwoclasses: inallcases,thesameinternal\nstructure is visible. This effect was seen consistently in m any of the songs in the test set.\nTo make the comparison it is necessary to map the\nboundaries between segments back to the original contin-\nuous timeline on which the ground truth annotations are\ndeﬁned. Bearing in mind that the sequence of short-term\nhistograms is deﬁned on a discrete timeline which is it-\nself derived via two framing operations from the original\ndiscrete time signal, this is not a trivial operation. De-\npending on how the fragment classiﬁcation is interpreted,\ntheboundarybetweentwosegments(essentiallythe‘gap’\nbetween two discrete time moments) could be mapped\nback to one of several points or intervals on the contin-\nuous timeline. We shall, for the time being, map the gap\nbetween two discrete moments back to the middle of the\noverlap between their respective continuous time inter-\nvals, which, at 15 HMM states, are 3.4s long and overlap\nby 3.2s.\nHaving found times for the detected segment bound-\naries, we adapted the segmentation evaluation measure of\nHuang and Dom (1995). Considering the measurement\nMasasequenceofsegments Si\nM,andthegroundtruth G\nlikewise as segments Sj\nG, we compute a directional Ham-\nming distance dGMby ﬁnding for each Si\nMthe segment\nSj\nGwiththemaximumoverlap,andthensummingthedif-\nference, dGM =/summationtext\nSi\nM/summationtext\nSk\nG/negationslash=Sj\nG|Si\nM∩Sk\nG|where |·|de-\nnotesthedurationofasegment. Wenormalise dGMbythe\ntracklength Ltogiveameasureofthemissedboundariesm=dGM/L. Similarly, we compute dMG, the inverse\ndirectional Hamming distance, and a similar normalised\nmeasure f=dMG/Lofthesegmentfragmentation. Note\nthatthesemeasuresconsideronlythetimeintervalsoccu-\npied by each segment, not the classiﬁcations of the seg-\nments. Plots of fandmagainst the number of clusters\nfor our corpus are presented in ﬁgures 2 and 3.\nAnalternativeinformation-theoreticmeasurewasalso\ninvestigated in order to assess the how well the classiﬁca-\ntion reﬂected the original segment labels. This involves\n‘rendering’ the ground-truth segmentation into a discrete\ntime sequence of numeric labels C0, using the same dis-\ncrete timebase as the sequence to be assessed, C1, and\nthen treating the the joint distribution over labels as a\nprobabilitydistribution. Thetwosequences arecompared\nby computing the conditional ‘entropies’ H(C1|C0)and\nH(C0|C1).H(C0|C1)measures the amount of ground-\ntruth information ‘missing’ from the class assignments,\nwhile H(C1|C0)measures the amount of ‘spurious’ in-\nformation in the classﬁcation, e.g. when several classes\nrepresent one segment type. The ‘mutual information’\nI(C0,C1)measures the information in the class assign-\nments about the ground truth segment label, and is max-\nimal when each segment type maps to one and only one\nclass. Inthiscaseboth H(C1|C0)andH(C0|C1)willbe\nzero. Weplotthemutualinformationforoursegmentation\nmethods in ﬁgure 4.\n4232 4 6 8 1000.20.40.6cosine distance\n2 4 6 8 1000.20.40.6KL distancefalse alarm rate\n2 4 6 8 1000.20.40.6histogram clustering\nnumber of clusters\nFigure 2: Rate of false detection ffor all segmentation\nmethods aggregated over our corpus. The four curves\nare for HMMs with 10, 20, 40 and 80 states; there is no\nstrongly statisticallysigniﬁcant difference between the m.\n2 4 6 8 1000.20.4cosine distance\n2 4 6 8 1000.20.40.6KL distancemissing rate\n2 4 6 8 1000.20.40.6histogram clustering\nnumber of clusters\nFigure3: Rateoftruenegativefailure mforallsegmenta-\ntion methods aggregated over our corpus. As in ﬁgure 2,\nthe four curves display the data for HMMs with different\nnumbers of states.2 4 6 8 10012cosine distance\n2 4 6 8 10012KL distancemutual information\n2 4 6 8 10012histogram clustering\nnumber of clusters\nFigure 4: Mutual Information (in bits) between ground\ntruth and machine segmentation for our segmentation\nmethods.\n5 CONCLUSIONS\nFirstly, it is clear from the individual results that the ap-\nproach we have taken in this paper, to a large extent inde-\npendently of the details of the particular segmentation al-\ngorithm, has met with a degree of success. While no seg-\nmentation produced by our algorithm was perfect, some\n(represented in the top right corner of ﬁgure 5) are close\nto the ideal of the expert’s segmentation.\nWe should note that the expert’s segmentation should\nnot be taken as the Platonic truth: equally valid segmen-\ntations, depending on the application, can be formed at\ngreatly different timescales; in addition, in real music\nthere is often a degree of ambiguity, not reﬂected in the\nannotations,astotheexactpointoftransitionbetweenone\nsegment and the next.\nA number of tendencies are visible in the results.\nFirstly, both the number of successfully detected bound-\naries and the number of false detections increase with the\nnumber of classes requested. This is unsurprising since,\nas the number of classes increases, each class becomes\nmore selective, which tends to break up the segments and\nintroduce more boundaries. Even if these were placed at\nrandom, this would increase both true and false positives.\nHowever,theincreaseinthemutualinformationmeasures\nshows that the extra classes are being put to good use as\nfar as reﬂecting the annotated labels.\nSecondly,acloseinspectionoftheindividualsegmen-\ntations shows that, in many cases, over-segmentation re-\nveals the internal structure of the annotated segments in a\nconsistent way; for example, in ﬁg. 1, each repetition of\n42400.20.40.60.8 100.20.40.60.81\n1−m1−f\nFigure5: Valuesof 1−f,correspondinglooselytopreci-\nsion, plotted against values of 1−m, analogous to recall,\nover all songs and segmentation methods presented. The\noptimal average tradeoff point is approximately (0.8,0.8) .\nthe ﬁfth segment produces recognisably the same pattern\nof internal sub-segments. This effect is more pronounced\nwhen more classes are requested, resulting in distinctive\npattern of several sub-segments on each repetition of the\nannotated segment type. Hence, the classiﬁed segmenta-\ntion can be thought of as a sort of ‘abstract score’.\nFragmentation also results if the clusters for two\nclasses overlap in the histogram feature space. In this\ncase, even a single frame in the middle of one segment\nwhichhappenstolookmoreanothersegmenttypewillbe\nmisclassiﬁed. Intuitively, thisoccurs because wehave not\nencoded any expectations of temporal coherence. In sub-\nsequent work, we have found that including an explicit\nprior on segment durations, to discourage very short seg-\nments, largely solves the fragmentation problem.\nFinally, in a bid to keep the parameter space tractable\nfor this investigation, we have not discussed variations in\nthe early stages in audio processing chain. In addition to\ntheobviousparameterswhichcouldbevaried,suchashop\nsizes or constant- Qresolution, the effects considering an-\notherrepresentation,suchasachromagram,inplaceofor\ninadditiontoourconstant- Qspectrum,warrantinvestiga-\ntion.\nACKNOWLEDGEMENTS\nThis research was supported by EPSRC grant\nGR/S84750/01 (Hierarchical Segmentation and Se-\nmantic Markup of Musical Signals).\nREFERENCES\nJ. Allen. Towards a general theory of action and time.\nArtiﬁcial Intelligence , 23:123–154, 1984.\nJ.-J. Aucouturier, F. Pachet, and M. Sandler. The way it\nsounds : Timbre models for analysis and retrieval of\npolyphonicmusicsignals. IEEETransactionsofMulti-\nmedia, 2005.M.Casey. MPEG-7sound-recognitiontools. IEEETrans.\nCircuits Syst. Video Techn. , 11(6):737–747, 2001.\nR. Dannenberg and N. Hu. Discovering musical structure\ninaudiorecordings. In MusicandArtiﬁcalIntelligence:\nSecond International Conference , Edinburgh, 2002.\nA.P.Dempster,N.M.Laird,andD.B.Rubin. Maximum\nlikelihoodfromincompletedataviathe EMalgorithm.\nJournaloftheRoyalStatisticalSociety ,39:1–38,1977.\nJ.-P. Eckmann, S. O. Kamphorst, and D. Ruelle. Recur-\nrence plots of dynamical systems. Europhysics Letters ,\n5:973–977, 1987.\nJ. Foote. Visualizing music and audio using self-\nsimilarity. In ACMMultimedia(1) ,pages77–80,1999.\nA.Galton,editor. TemporalLogicsandtheirApplications .\nAcademic Press, London, 1987.\nM. Goto. A chorus-section detecting method for musical\naudio signals. In Proc. ICASSP , volume V, pages 437–\n440, 2003.\nT.HofmannandJ.M.Buhmann. Pairwisedataclustering\nby deterministic annealing. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence , 19(1), 1997.\nQ. Huang and B. Dom. Quantitative methods of evaluat-\ning image segmentation. In Proc. IEEE Intl. Conf. on\nImage Processing (ICIP’95) , 1995.\nMultimedia Programming Interface and Data Speciﬁca-\ntions1.0. IBMCorporationandMicrosoftCorporation,\nAugust 1991.\nInformation Technology – Multimedia Content Descrip-\ntion Interface – Part 4: Audio . ISO, 2002. 15938-4.\nB. Logan and S. Chu. Music summarization using key\nphrases. In International Conference on Acoustics,\nSpeech and Signal Processing , 2000.\nL.Lu,M.Wang,andH.Zhang. Repeatingpatterndiscov-\nery and structure analysis from acoustic music data. In\n6th ACM SIGMM International Workshop on Multime-\ndia Information Retrieval , October 2004.\nN. Maddage, X. Changsheng, M. Kankanhalli, and\nX. Shao. Content-based music structure analysis with\napplications to music semantics understanding. In 6th\nACM SIGMM International Workshop on Multimedia\nInformation Retrieval , October 2004.\nG.Peeters,A.L.Burthe,andX.Rodet. Towardautomatic\nmusic audio summary generation from signal analysis.\nInInternational Symposium on Music Information Re-\ntrieval, 2002.\nJ. Puzicha, T. Hofmann, and J. M. Buhmann. Histogram\nclustering for unsupervised image segmentation. Pro-\nceedings of CVPR ’99 , 1999.\nL. Rabiner and B. H. Juang. Fundamentals of Speech\nRecognition . Signal Processing Series. Prentice Hall,\nEnglewood Cliffs,NJ, 1993.\nG. H. Wakeﬁeld. Mathematical representation of joi-\nint time-chroma distributions. In Advanced Signal\nProcessing Algorithms, Architectures, and Implemen-\ntations, volume 3807, IX, pages 637–645. SPIE, 1999.\n425"
    },
    {
        "title": "Iterative Deepening for Melody Alignment and Retrieval.",
        "author": [
            "Norman H. Adams",
            "Daniela Marquez",
            "Gregory H. Wakefield"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415712",
        "url": "https://doi.org/10.5281/zenodo.1415712",
        "ee": "https://zenodo.org/records/1415712/files/AdamsMW05.pdf",
        "abstract": "For melodic theme retrieval there is a fundamental tradeoff between retrieval performance and retrieval speed. Melodic representations of large dimension yield the best retrieval performance, but at high computational cost, and vice versa. In the present work we explore the use of iterative deepening to achieve robust retrieval performance, but without the accompanying computational burden. In particular, we propose the use of a smooth pitch contour that facilitates query and target representations of variable length. We implement an iterative query-by-humming system that yields a dramatic increase in speed, without degrading performance compared to contemporary retrieval systems. Furthermore, we expand the conventional iterative framework to retain the alignment paths found in each iteration. These alignment paths are used to adapt the alignment window of subsequent iterations, further expediting retrieval without degrading performance. Keywords: Iterative deepening, DTW, melody retrieval. 1",
        "zenodo_id": 1415712,
        "dblp_key": "conf/ismir/AdamsMW05",
        "keywords": [
            "melodic representations",
            "iterative deepening",
            "query-by-humming",
            "smooth pitch contour",
            "alignment paths",
            "DTW",
            "performance",
            "computational cost",
            "alignment window",
            "retrofitting"
        ],
        "content": "ITERA TIVE DEEPENING FOR MELOD YALIGNMENT AND\nRETRIEV AL\nNorman Adams, Daniela Marquez, Gregory Wake\u0002eld\nElectrical Engineering andComputer Science\nUniversityofMichigan\n1101BealAve.,AnnArbor,48109,USA\nnhadams@umich.edu\nABSTRA CT\nFormelodicthemeretrievalthereisafundamental trade-\noffbetween retrievalperformance andretrievalspeed.\nMelodicrepresentations oflargedimension yieldthebest\nretrievalperformance, butathighcomputational cost,and\nviceversa.Inthepresentworkweexploretheuseofit-\nerativedeepening toachieverobustretrievalperformance,\nbutwithouttheaccompan yingcomputational burden.In\nparticular ,weproposetheuseofasmoothpitchcontour\nthatfacilitates queryandtargetrepresentations ofvariable\nlength.Weimplement aniterativequery-by-humming\nsystemthatyieldsadramatic increaseinspeed,with-\noutdegradingperformance compared tocontemporary re-\ntrievalsystems.Furthermore, weexpandtheconventional\niterativeframeworktoretainthealignment pathsfoundin\neachiteration. Thesealignment pathsareusedtoadaptthe\nalignment windowofsubsequent iterations, furtherexpe-\nditingretrievalwithoutdegradingperformance.\nKeywords:Iterativedeepening, DTW,melodyretrieval.\n1INTR ODUCTION\nMelodies, similartoothertime-series representations,\npresentaparticular challenge fordatabase retrieval;the\ntemporal elasticity ofmusicalperformance preventsthe\nuseofadirectsimilarity measure, suchasEuclidean\ndistance[1].Thisproblem isacuteforcasuallysung\nmelodies, whichareoftenthefocusofMIRapplica-\ntions[2,3,4].Toaddresstheproblem, awidevariety\nofdynamicalignment methodshavebeendevelopedthat\naccountforatimewarpingbetweentokens,thusprovid-\ningamorerobustmeasureofsimilarity inthefaceofhu-\nmanperformance factors[5,6,7,8,9].Unfortunately ,\nallsuchalignment methodspresentacomputational bur-\ndenincomparison todirectsimilarity measures. Dy-\nnamicalignment methodshavecomputational complexity\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonO(N2),where Nisthelengthofthetokens,ratherthan\nO(N)fordirectmeasures. Accordingly ,itisdesirable to\nkeepthelength,ordimension, oftokensassmallaspos-\nsible.However,thiscomesattheexpenseofdegraded\nretrievalperformance.\nInthepresentworkweproposetheuseofitera-\ntivedeepening (ID)coupledwithdynamic timewarping\n(DTW)toexpeditemelodyretrievalwithoutdegrading\nperformance. Iterativedeepening usesascalablerepre-\nsentation toretrievetargettokenswithsuccessivelymore\naccuratesimilarity measures [10].Onlyafractionofthe\ntargetsareretainedaftereachiteration, hencetheproba-\nbilityoffalsedismissals increases. However,byallowing\nsmallincreases infalsedismissals, iterativedeepening can\ngreatlyspeedretrieval.Wedescribeandevaluateiterative\ndeepening inthecontextofaquery-by-humming (QBH)\nsystem,andpresentresultsshowingadramatic speedin-\ncreasewithanegligibleincreaseinfalsedismissals. Fur-\nthermore, weproposeanovelmethodtofurtherexpedite\nperformance byretaining alignment information forfu-\ntureiterations andusinganadaptivealignment window.\nInsodoing,weachieveanevengreaterspeedincrease\noverconventionalmethodswithonlyminimaldetriment\ntoretrievalperformance.\nTheremainder ofthissectionprovidesbackground in-\nformation aboutmelodicrepresentations forQBHandthe\ntradeoffbetweenretrievalperformance andspeed.Sec-\ntion2presentsinturndynamicalignment, iterativedeep-\nening,andadaptiveiterativedeepening. Section3de-\nscribesourmethodsforevaluation,alongwithresultsand\ndiscussion. Section4concludes thepaper.\n1.1 Backgr ound\nTheMIRcommunity hasexploredmanymelodicrepre-\nsentations formusicretrieval;sequences ofnotes[2,7,11]\nandvariousquantizations thereof[9,12,13,14],smooth\npitchcontours1[5,6,15],sequences ofhistograms [5,8],\nandhiddenMarkovmodels(HMM)[4,16].Suchresearch\nhasfocusedprimarily ontheretrievalperformance agiven\nrepresentation affords,withoutconsidering theretrieval\nspeedingreatdetail.Formodestly sizedtargetdatabases,\n1Inthepresent work, thepitch contour isde\u0002ned asthe\noutput ofapitch tracking algorithm. SeeFig.1forasample pitch\ncontour .Other authors usepitch contour torefer toacoarsely\nquantized sequence ofpitch differences ofanote sequence [13].\n199asareoftenusedforQBHexperiments, retrievalspeedis\nnotaproblem. Butasthetargetdatabase becomes mas-\nsiveexpedientretrievalbecomes amoredif\u0002cultandde-\nsirablegoal.Whilenote-based melodicrepresentations\nhavetendedtodominate researchinquery-by-humming\nsystems[2,9,11,12],thereisgrowinginterestinsmooth\npitchcontourrepresentations [5,6,15,17,18].Thisis\nduelargelytothedif\u0002cultyoftranscribing casuallysung\nmelodies, wheredetecting noteonsetsisespecially error-\nprone[4,11,19].Unfortunately ,whilecontourrepresen-\ntationsyieldpromising results,theypresentalargecom-\nputational burden[5,6,17,15].\nPreviousworkbytheauthorscomparing different\nmelodicrepresentations foundthatrepresentations that\nyieldthemostrobustretrievalperformance alsoyield\ntheslowestretrievaltime[5].Inparticular ,wefound\nasmoothpitchcontouryieldsthebestperformance on\nlargedatabases, butonlyforrelativelylongcontours, with\nlength(i.e.dimension) N\u0018100.Incontrast, asimple\nnoterepresentation hasdimension N\u001810,whichfacil-\nitatesrelativelyrapidretrievalbutgivesmediocre perfor-\nmanceonlargedatabases. Furthermore, wefoundthatus-\ningapitchcontourwithsmalldimension, N\u001810,yields\nperformance similartoconventionalnoterepresentations.\nThetradeoffbetween robustandrapidretrievalis\ncommon indatabase problems. Assuch,thedatabase\ncommunity hasbegunexploring methods tospeeddy-\nnamicalignment andretrievaloflongtimeseries.Tech-\nniquesincludelowerbounds[20],alignment andcontinu-\nityconstraints [6],indexing[16,21],anditerativemeth-\nods[10,22].Inthepresentwork,weadaptthelatterto\ntheuniquechallenges ofMIRbyusingamelodicrepre-\nsentation thatisreadilydecimated toarbitrarylength.\n2METHODS\n2.1 Repr esentation\nWerestrictourattentiontosmoothpitchcontourrepresen-\ntationsofmelody.Forasungorhummed query,thepitch\ncontourisestimated usingatime-domain method[11,23].\nThisalgorithm computes theautocorrelation foroverlap-\npingframesofrecordeddatawithconstantstep-size10ms.\nAsetofcandidate peaksisselectedforeveryframeand\ntheViterbialgorithm isusedtoconstruct asmoothcon-\ntour.Thevaluesoffundamental frequencyarethencon-\nvertedtoMIDIpitch2.Theestimated pitchcontourcon-\ntainsgapswherenopitchisestimated. Thesegapsare\n`\u0002lledin'byextending theendofeachpitchedregion\ntothestartofthenext.Theautocorrelation valuesare\nusedinextendingtheregionssoastopreventincongruous\npitch\u0003uctuations [5].Directuseoftheestimated contour\nyieldsarepresentation withprohibitivelylargedimension,\nN>1000.Hencethecontoursaredecimated toyielda\ndimension 10<N<200[1,15].Inthepresentwork,\nqueriesareindividuallyresampled suchthatthedimen-\nsionNisconstantforallqueries.\nFig.1showsasamplepitchcontourforthemain\nthemefromMyBonnieliesovertheoceanwithN=\n2Thereal-v alued MIDI pitch number pisrelated toasignal' s\nfundamental frequenc yinHz,f0,asp=12log2(f0=261)+60.Figure1:Samplealignment forthemelodysungtothe\nlyricsMyBonnieliesovertheocean.MyBonnielies\noverthesea..Thetopcontouristhetarget,andthebot-\ntomcontouristhequery.\n80.IncludedinFig.1isthepiecewiseconstantidealcon-\ntourforthesamethemestoredinthetargetdatabase. Note\nthatthetargetcontourhasbeentime-scaled tohaveequal\ndurationasthequery.Thenextsectiondetailsthesimilar-\nitymeasureusedforcomparing queryandtargetcontours.\n2.2 TimeSeries Alignment\nQuery-by-humming systemstypicallyoperatebycomput-\ningameasureofsimilarity betweenasungqueryandev-\nerythemeinadatabaseoftargets.Thetargetthemesare\nthenrankorderedbysimilarity .Inthepresentworkwe\nuseDTWtoalignthequerytoeachtargetinthedatabase,\ninterpreting thealignment costasameasureofdistance\nbetween thequeryandtarget[5].Foragivenquery\ncontourwithlength N,everytargetthemeis\u0002rsttime-\nscaledtoyieldapiece-wise constantcontouroflength\nN0\u0019N.Themeanpitchdifferencebetweenthequery\nandtargetisthensubtracted, yieldingquerycontour Q=\n(q1;q2;\u0001\u0001\u0001qN)andtargetcontour T=(t1;t2;\u0001\u0001\u0001tN0).\nToaccountforatime-warpingofthequeryrelativetothe\ntarget,theelements ofthequerycontourmustbealigned\ntotheappropriate elements ofthetarget.Asamplealign-\nmentisshowninFig.1foraquery/targetpairofthemain\nthemefromthetraditional tuneMyBonnieliesoverthe\nocean..\nForagivencontinuity andcostscheme,abrute-force\nsearchofallpossiblealignments iscomputationally pro-\nhibitive,hencedynamic programming isemployedsoas\ntocompute thecostofonlytheoptimalalignment [24].\nLet\u0000=[\rn;k]beanN\u0002N0matrixofminimum pre\u0002x\nalignment costs; \rn;kistheminimum alignment costfor\n(q1\u0001\u0001\u0001qn)and(t1\u0001\u0001\u0001tk).Letawarpingpathbegivenby\nw=(w1;w2;\u0001\u0001\u0001wT),where wt=(n;k)indicates that\nqnisalignedwithtk.Thewarpingpathmustadhereto\nseveralconstraints tobephysicallymeaningful. Thepath\nmustbeginwiththe\u0002rstelements ofthequeryandtarget\ncontour,andendwiththelastelements; w1=(1;1)and\nwT=(N;N0).Thepathmustbemonotonic nondecreas-\ningandadheretoalocalcontinuity constraint. Starting\ninthelowerleftcornerof\u0000,everyelementof\u0000isfound\nrecursivelyby\n\rn;k=min0\n@\rn\u00001;k+\u0010\n\rn;k\u00001+\u0010\n\rn\u00001;k\u000011\nA+jqn\u0000tkj(1)\nwhere \u0010\u00150isanadditivecostpenaltyappliedtofavor\n200moredirectalignments. Tospeedcomputation andpre-\nventextremewarpings,thealignment pathisrestricted to\nbenearthemaindiagonal n=k.Inparticular ,8n;k:\njn\u0000kj>N=5!\rn;k=1.The\u0002nalalignment cost\nforQandTisgivenby\rN;N0.Thecomputational com-\nplexityofthisprocedure isO(N2).Notethatthe\u0002nal\nalignment costisnotnormalized byT,whichwouldren-\nderthealignment suboptimal forthecostschemein(1).\nHowever,theeffectofsuboptimal alignment onretrieval\nperformance remainsanunanswered question. Indeed,we\nhaveinformally exploredalignment procedures, notpre-\nsentedhere,thatyieldsuboptimal alignments butslightly\nimprovedretrievalperformance.\n2.3 Iterati veDeepening\nConsider thefollowingsimpleQBHexperiment, which\nhighlights thetradeoffbetweenretrievalperformance and\nN.ForanygivenN,weareinterested insearching over\nafractionofthetargetdatabase. Letusde\u0002nea`suc-\ncessthreshold' asthetoppercentage oftheorderedtar-\ngetdatabase thatthecorrectthememustbeinforthe\nretrievaltobeconsidered asuccess3.Wethencom-\nputethefractionofqueriesforwhichtheretrievalissuc-\ncessfulasafunctionofthe`successthreshold', andla-\nbelthisfractionthe`success rate'[14].Fig.2gives\ntheretrievalperformance foraDTWQBHsystemfor\nN=f12;14;20;30;50;100;150g.Thequeryandtar-\ngetdatabases usedtogeneratethis\u0002gurearethesameas\nthosedetailedinSection3.Theabscissarepresents the\nsuccessthreshold, andtheordinategivesthefractionof\nqueriesforwhichtheretrievalissuccessful. Forexample,\nfora10%successthreshold, theN=20representation\nyieldsasuccessrateof0:86.\nForthisexperiment thetargetdatabasecontained 1000\nthemes.Hencetheleftedgeofeachcurve,whichgives\nthesuccessratefora0:1%threshold, isequivalenttothe\nclassi\u0002cation accuracyofthesystem. Thepointcorre-\nsponding toa1%threshold givesthefractionofqueries\nforwhichthecorrectthemewasinthetoptenthemesre-\nturnedbytheretrievalsystem.Itisevidentfromthe\u0002gure\nthatthebestclassi\u0002cation accuracy,85%,isachievedonly\nforN=150.Extending the85%`success' rateacrossthe\n\u0002gurehighlights the`85%performance capacity' versus\ndimension. Thatis,N=14mayyieldmediocre clas-\nsi\u0002cation accuracy,butifallthatisdesiredisthatthe\ncorrectthemeisinthetop20%ofthereturnedtargets,\nthenN=14wouldappeartoperformreasonably well.\nWhilethismaynotbeagoodstopping pointforourre-\ntrievalsystem,wehavemadetheproblemsomewhatsim-\nplerbydetermining that800ofthe1000targetscanbe\ndiscarded. Thismotivatesaretrievalsystemthatmakes\nmultiplepassesthroughthetargetdatabaseandremoves,\naftereachiteration, asmanytargetsaspossiblesubjecttoa\nboundonfalsedismissals. Numerous iterativetechniques\nhavebeenproposed, whichoftenmakestrictlytwopasses\nthroughthedatabase [14,16,20].Inthepresentwork\n3Forexample, consider atargetdatabase with 100themes.\nIftherank ofthecorrect theme foragivenquery is7,then the\nretrie valisconsidered asuccess ifthesuccess threshold is10% ,\nbutafailure ifthesuccess threshold is5%.0.1 1 4 10 20 30 500.50.60.70.80.9\nPercentage of Target DatabaseSuccess Rate150\n100\n 50\n 30\n 20\n 14\n 12\nFigure2:Retrievalperformance forvaryingdimension,\nN.\nweexplorearetrievalarchitecture amenable toavariable\nnumberofiterations [10].\nConsider adirectDTWsystem,whichmustaligna\nqueryoflength NMwiththefullsetoftargets.Letthe\ncomputational costforthedirectmethodbecF=C\u0001N2\nM\nforsomeconstant C.Ratherthanperforming therather\ncostlylength NMalignment withalltargets,theiter-\nativemethodusesasequence ofincreasing representa-\ntionlengths, N=(N1;N2;\u0001\u0001\u0001NM).Thequeryis\u0002rst\nalignedwithallthetargetsusinglength N1.Aftereach\niteration, thealignment costsaresortedandonlythetar-\ngetsyieldingthesmallestalignment costsareretainedfor\nsubsequent iterations. Letribethefractionofthetarget\ndatabaseretainedfortheithiteration, r=(1;r2;\u0001\u0001\u0001rM).\nClearly,thespeci\u0002cchoiceof(r2;\u0001\u0001\u0001rM)hasconsid-\nerableimpactontheultimateretrievalperformance and\nspeed.Thecomputational costfortheiterativemethodis4\ncID=C\u0001(N2\n1+r2N2\n2+\u0001\u0001\u0001+rMN2\nM):(2)\nHencethecomputational costfortheiterativemethodrel-\nativetothedirectmethodis\n\u001a=cID\ncF=\u0012N1\nNM\u00132\n+r2\u0012N2\nNM\u00132\n+\u0001\u0001\u0001+rM:(3)\n2.4 Adapti veAlignment Constraints\nWhileitisclearthatiterativedeepening cangreatlyex-\npeditemanydatabaseretrievaltasks,themethodiswaste-\nfulinthesensethatthealignments themselvesaredis-\ncardedaftereachiteration. Computing theoptimalalign-\nmentforthesamequery/targetpairformultiplerepresen-\ntationlengthsissomewhatredundant. AsNincreases,\ntheoverallshapeoftheoptimalalignment isunlikelyto\nchange,butratherthe\u0002nedetailoftheoptimalalignment\nwillberesolved.Thisobservationimpliesthattheiter-\nativedeepening frameworkcanyieldevenfasterperfor-\nmanceifinformation aboutprioralignments isincorpo-\nratedintoeachiteration. InSection2.2wefoundthatan\n4Weareneglecting thecost duetotheadditional sortopera-\ntions theiterati vemethod requires, which issmall compared to\nthealignment cost.\n201TargetQuery\n5 10 15 205101520\nFigure3:Samplealignment forN=20.\nTargetQuery\n20 40 60 80 100 12020406080100120\nFigure4:Samplealignment forN=120.\nalignment windowof20%yieldsgoodretrievalperfor-\nmance.Byde\u0002ningthealignment windowrelativetothe\npreviousalignment ratherthanthemaindiagonal, wecan\nreducethewidthofthewindowconsiderably withoutsac-\nri\u0002cingalignment performance, thusquickeningretrieval.\nAsimilarideawasrecentlyproposed in[22].\nAdaptivealignment windowsareperhapsbestde-\nscribedwithanexample. Consider thealignment of\nN=20contours forthesamequeryandtargetshown\ninFig.1.Fig.3showsthe20%alignment windowin\ngrey,andthe\u0002nalalignment pathinblack.Wethenre-\nalignthequeryandtargetforN=120.Wedothisintwo\nways,withandwithoutprioralignment. Withoutusing\ntheprioralignment wecannotsafelyreducethealignment\nwindowwidthbelow20%.ThiscaseisshowninFig.4.\nNotethatthebasicshapeofthealignment inFigs.3and\n4isthesame.Insteadofusinga20%windowcentered\naroundthemaindiagonal, wecanuseanarrow10%win-\ndowcenteredaroundtheshapeoftheN=20alignment.\nThiscaseisshowninFig.5.Thealignment windowin\nFig.5iscomputed byscalingtheN=20alignment tothe\nN=120matrixandverticallystretching thethewindow.\nNotethatthe\u0002nalalignments showninFigs.4and5are\nidentical. Thisimpliesthatwecanusetheprioralignment\nTargetQuery\n20 40 60 80 100 12020406080100120\nFigure5:Samplealignment forN=120usingacon-\nstrainedalignment windowfromN=20.\ntohalvethesizeofthecurrentalignment window,without\ndegradingthequalityofthe\u0002nalalignment. Byde\u0002nition,\ntheadaptivealignment constraint cannotexpeditethe\u0002rst\niterationofretrieval,butthespeedofallsubsequent iter-\nationswillbedoubled. Hencethecomputational costof\ntheadaptiveiterativedeepening (AID)relativetothedi-\nrectmethodis\n\u001a0=cAID\ncF=\u0012N1\nNM\u00132\n+1\n2 \nr2\u0012N2\nNM\u00132\n+\u0001\u0001\u0001+rM!\n:\n(4)\n3EVALU ATION\nWeevaluatetheperformance oftheiterativedeepening\ninthecontextofaquery-by-humming system. Previ-\nousworkbytheauthorsfoundthatretrievalperformance\nplateausforN>150,andfallstochanceforN<\n10.Henceaniterativeretrievalsystemthatbeginswith\nN\u001810andendswithN\u0018150wouldbeajudicious\nchoice.Weimplement anID-QBH systemusingM=3\niterations, withN1=14,N2=32andN3=144.\nThevaluesofmathbf Nweredetermined experimentally .\nN1isthemostcriticalofthethree,forsmallervalues\nyieldsubstantially worseperformance andlargervalues\nyieldsubstantially slowerretrieval.Theprecisevaluesof\nN2andN3arenotcritical,although wefoundthatN2\nshouldgenerally beclosertoN1thanN3.Wefoundus-\ningM>3didnotimproveperformance, whereasper-\nformance forM=3doesimproveperformance some-\nwhatoverM=2.Forcomparison wepresentresults\nforconventionalDTWQBHsystemsusingN=14and\nN=144.Thenexttwosubsections describethequery\nandtargetdatabases usedforevaluation. Section3.3de-\ntailsthechoiceofr2andr3,thefractionofthetarget\ndatabase retainedforthesecondandthirditerations, re-\nspectively.Resultsarethenpresented insection3.5.\n3.1 Query Database\nForperformance evaluation, weemployaquerytest\nsetcontaining manysamplequeriesoffourteenpopular\n202tunes,fromtheBeatles'Hey,JudetoRichardRodgers'\nSoundofMusic.Atotalof480querieswerecollected\nfrom\u0002fteenparticipants inourstudy.Eachparticipant\nwasaskedtosingafamiliarportionofasubsetofthefour-\nteentunesfourtimeseach.Theparticipants hadavariety\nofmusicalbackgrounds; somehadconsiderable musical\norvocaltrainingwhilemosthadnoneatall.Participants\nwereinstructed tosingeachqueryasnaturally aspossi-\nbleusingthelyricsofthetune5.Thequeriesaremono-\nphonic,16bitrecordings sampledat44.1kHzandresam-\npledto8kHztoreduceprocessing time.Thequeriesare\nbetween4and20secondsinlength.Alldatawerecol-\nlectedinaquietclassroom settingandparticipants were\nfreetoprogressattheirownpace.\nNotethatforallfourteenmelodies, everyparticipant\nsangthesamesetofmeasures. Forareal-worldQBH\nsystem,thisisanunreasonable assumption. Thisproblem\ncanbeameliorated byallowingforzero-cost insertionand\ndeletionstepsatthebeginningandendofthealignment6,\nsimilarto[4].Anothercommonpracticeistoincludemul-\ntiplethemesforeachtuneinthetargetdatabase[17],in-\ncreasingthesizeofthetargetdatabase.\n3.2 TargetDatabase\nMeasuring theretrievalperformance ofourQBHmethods\nonatargetdatabaseofonlythefourteenthemesforwhich\nwehavesamplequerieswouldnotindicatehowwellthe\nmethods wouldperforminabroadercontext.Accord-\ningly,weaugment thetargetdatabase withextrathemes\nnotincludedinthequerytestset.Weensurethattheaddi-\ntionaltargetsaresuf\u0002cientlysimilartothe14`authentic'\ntargetsbybuildingaMarkovmodelthatgenerates `syn-\nthetic'targetswithsimilar\u0002rst-order statistics astheau-\nthenticthemes[5,25].Inparticular ,wegenerate986syn-\ntheticthemes,yieldingatargetdatabaseof1000themes.\nWecanverifythatthesynthetic themesaresimilartothe\nauthentic themesbyconsidering theratioofauthentic to\nsynthetic targetsthatareretainedaftereachiterationof\nsystemretrieval.We\u0002ndthatfortheithiteration, the\nnumberofincorrect authentic themesincluded intheit-\nerationisaboutri\u000113\n999.Henceeachauthentic themeisas\nsimilar,inthesenseofDTWalignment cost,totheother\n13authentic themesasitistothe986synthetic themes.\nWhilemanyofthesynthetic themesaremusically unsat-\nisfying,theyclearlydemonstrate enoughsimilarity tothe\nauthentic themessoastochallenge theretrievalsystem.\nWenotefromtheoutsetthatusingsynthetic targetslim-\nitshowtheresultscanbeinterpreted. However,weare\nconcerned withrelativetrendsratherthanabsoluteperfor-\nmance.\n3.3 Performance-Speed Tradeoff\nHaving\u0002xedthenumberofiterations, M=3,anddi-\nmensions, N=(14;32;144),wemustchoose r2andr3\n5This contrasts substantially from thecommon practice of\nhaving participants sing isolated pitches onaneutral vowel, re-\nquiring participants toperform note segmentation [11].\n6Although, forthisapproach tobeeffective,thealignment\nwindo wmust bemade considerably largernear thebeginning\nandendofthealignmenttocompletely specifythesystem.Clearly,asr2andr3\nincrease, boththeretrievalperformance andtheretrieval\ntimeincrease. Weseektominimize r2andr3withoutde-\ngradingretrievalperformance. Thiscanbedonebyesti-\nmatingthedistributionofapproximation errorateachrep-\nresentation length,andusingthistoestimatetheprobabil-\nityoffalsedismissal foreachri[10].Inthepresentwork\nwesimplyexaminethenetretrievalperformance versus\nr2andr3,andchoosethesmallestvaluesthatdonotsub-\nstantially degraderetrieval.Figures6and7highlight the\ntradeoffbetweenretrievalperformance andspeed.\nFig.6givesthemeanreciprocal rank(MRR)versus\nr2andr3,andFig.7gives\u001aversusr2andr3.Inboth\nplotstheminimum valueofr2andr3is0:1%,andthe\nmaximum valueofr2is50%andthemaximum valueof\nr3is10%.Notethatforr2=0:1%,theMRRequalsthe\nclassi\u0002cation accuracyofadirect N=14system,and\nforr3=0:1%theMRRequalstheclassi\u0002cation accuracy\nofadirect N=32system.From(3)weknowthatas\nr2andr3approach zero,\u001aconvergesto0.0095,which\nisreadilyapparentinFig.7.WhileMRRdecreases with\nr2andr3,adistinctthreshold isevident;solongasr2is\ngreaterthan10\u000020%andr3isgreaterthan1\u00002%the\nMRRisapproximately constantat0:85.Accordingly ,we\nsetr2=20%andr3=2%.By(3)and(4),thisyields\n\u001a=0:039and\u001a0=0:024.Hencetheiterativesystem\nwillbeabout25timesfasterthanconventionalDTWwith\nN=144,andtheadaptiveiterativemethodwillbeabout\n41timesfaster.\n3.4 Results\nPerformance resultsaresummarized inTables1and2.\nFromthetwotablesitisclearthattheiterativeQBHsys-\ntemsyieldmuchfasterretrievalthanadirect N=144\nDTWQBHsystem,withoutdegradingperformance sub-\nstantially.Theiterativesystemssuccessfully circumvent\nthetradeoffbetweenretrievalperformance andspeedthat\n`one-pass' retrievalsystemsaresubjectto,yieldingasys-\ntemwiththeperformance oftheN=144systemandthe\nspeedoftheN=14system.\nTable1reportsthreeretrievalperformance metricsfor\nfourQBHsystems. Thetoptwosystemslistedaredi-\nrectimplementations oftheDTWsystemdescribed insec-\ntion2.2forN=14andN=144.Thethirdsystemlisted\nintheiterativemethoddescribed insection2.3,andthe\nfourthsystemlistedistheadaptiveiterativemethodde-\nscribedinsection2.4.The\u0002rstperformance metriclisted\nisclassi\u0002cation accuracy(CA),thefractionofqueriesfor\nwhichthecorrectthemeisrankoneintheretrievedlistof\nthemes.Thesecondperformance metriclistedisthetop\n10fraction(Top10),thefractionofqueriesforwhichthe\ncorrectthemeisamongstthetoptenthemesreturned. The\nthirdperformance metriclistedisthemeanreciprocal rank\n(MRR),theaverageinverserankofthecorrecttheme.\nFromTable1,itisclearDTWforN=144outper-\nformsthecasewhen N=14inallthreeperformance\nmeasures. IntermsofCAandMRR,thehigherdimen-\nsionyieldsconsiderably betterperformance. Intermsof\nthetop10fractionhowever,thecaseofN=14doesnot\nperformaspoorly,yieldingatop10fractionof0:80com-\n2030.542050\n0.313100.70.750.80.85\n r2   (%)  r3   (%)MRR\nFigure6:Meanreciprocal rank.\nTable1:Retrievalperformance offourQBHsystems\nSystem CA Top10 MRR\nDTW,N=140.663 0.803 0.723\nDTW,N=1440.840 0.869 0.851\nIterativeDTW 0.837 0.862 0.847\nAdt.It.DTW 0.835 0.860 0.845\nparedto0:86forthecaseofN=144.Whileasmallrep-\nresentation maynotyieldgoodCAorMRR,apparently it\ncandetermine whichsetof10targetsarelikelytocontain\nthecorrecttheme.Comparing theretrievalperformance\nofthetwoiterativesystemstothatofthedirectsystems,\nweseethatthetwoiterativesystemsyieldretrievalperfor-\nmancevirtuallyidenticaltothatofthedirectsystem.All\nthreemetricsfallmodestly fromthedirect N=144sys-\ntemtotheiterativesystem,andfurtherstillfortheadap-\ntiveiterativesystem.Theperformance dropisnegligible\ncompared totheperformance differencebetweenthedi-\nrectN=14andN=144systems,however.\nWhilethetwoiterativesystemsyieldretrievalperfor-\nmancevirtuallyidentical tothedirect N=144system,\ntheretrievalspeedoftheiterativesystemsisclosertothat\nofthedirect N=14system.Table2givesboththeav-\neragetotalnumberofalignment cells,\rn;k,thatmustbe\ncomputed foreachquery,andtheaveragetotalretrieval\ntimeperquery7.Thedirect N=144yieldsanaver-\nageperqueryretrievaltimeof106seconds. Thissys-\ntemisfartooslowtodeploydirectlyonamassivetarget\ndatabase. Theotherthreesystemsallyieldaretrievaltime\noflessthan5seconds. Thedirect N=14systemyields\nthefastestretrieval,butattheexpenseofgoodretrieval\nperformance. Theiterativesystemyieldsanaveragere-\ntrievaltimeof4.9seconds,21timesfasterthanthedirect\nN=144system.Theadaptiveiterativesystemyieldsan\naverageretrievaltimeof3.5seconds,30timesfasterthan\nthedirect N=144system.\n7The retrie valtime listed istheactual retrie valtime we\nrecorded forourMATLAB implementation running onalaptop\nPC,with a1.8GHz CPU0.542050\n0.3131000.040.080.12\n r2   (%)  r3   (%)r\nFigure7:Computation gain.\nTable2:Computation costoffourQBHsystems\nSystem Cells (1000' s) Time(s)\nDTW,N=14 93 1.61\nDTW,N=144 7720 106\nIterativeDTW 329 4.87\nAdt.It.DTW 208 3.49\n3.5 Discussion\nForourchoiceofM,N,andr,(3)predictsthattheiter-\nativemethodis25timesfasterthanthedirectimplemen-\ntation,and(4)predictsthattheadaptiveiterativemethod\nis41timesfaster.Thedisparity betweenthepredicted\nandobservedcomputational costfortheiterativemethod\nisduetotheextrasortoperations theiterativemethodre-\nquires.Thedisparity betweenthepredicted andobserved\ncomputational costfortheadaptiveiterativemethodis\nlarger.Inthiscasethedisparity isalsoduetothelarge\nmemory requirements oftheadaptiveiterativemethod.\nAftereachiteration, theoptimalalignment pathsmustbe\nstoredforuseinsubsequent iterations. Thealignment\npathsconsume aconsiderable amountofmemory,andul-\ntimatelydelayretrieval.Whenmeasured intermsofthe\naveragetotalnumberofcellvisitsperquery,thespeed\nincreases fortheiterativemethodandadaptiveiterative\nmethodare23and37,respectively.\nFromFig.6itisevidentthatforthistargetdatabase,\nifallthatisdesiredisthatthecorrectthemebeinthe\ntop10ofreturnedthemes,thenthe\u0002nalN=144iter-\nationislargelyunnecessary .Thatis,theMRRplateaus\nforr3>1%.Henceretrievalperformance canbefurther\nexpedited inthiscasebysimplyreturning theresultsof\ntheN=32iteration. Indeed,fortheiterativemethods\nthe\u0002nalN=144iterationsimplydetermines theorder\nofthe20themesreturnedbytheN=32iteration. How-\never,wehavefoundthatasthetargetdatabasegrows,the\n\u0002nalN=144iterationbecomescritical[5].Asthetarget\nspacebecomes moredenselypopulated withthemes,in-\ncreasingthedimension ofthetargetspaceplaysacrucial\nroleinretrieval.Henceiterativemethodscangreatlyfacil-\nitateQBHsystemswithmassivetargetdatabases, where\nhavingalargemelodicrepresentation iscriticalforrobust\n204retrieval,buttooslowfordirectimplementation. Theiter-\nativemethodsallowfortheretrievaltimetogrowlessthan\nlinearlyasthetargetdatabasesizeincreases. Furthermore,\nconstraining thealignment windowsizeasmuchaspos-\nsibleiscriticalfordeployingQBHsystemsonmassive\ntargetdatabases, forasthealignment windowshrinksthe\ncomputational complexityofthealignment convergesto\nO(N)8[20].\nA30foldspeedincreaseoverdirectmethods, while\nsubstantial, maynotbesuf\u0002cientforscalingDTWmeth-\nodstotrulymassivedatabases, forthecomputational com-\nplexityisstilllinearwiththetargetdatabase size.How-\never,wehaveconducted informal experiments thatin-\ndicatethatrcanbereasonably reducedasthetarget\ndatabasegrows.Hencethetotalcomputational complex-\nityofthismethodmayeffectivelybelessthanlinearwith\ntargetdatabasesize.\nFinally,wenotethatretrievalperformance forallsys-\ntemscanbeincreased considerably byremovingthesam-\nplequeriesofthreesubjectsfromourtestdatabase. For\nexample,theMRRofthedirect N=144systemin-\ncreasesto0:93ifthesequeriesareremoved.Thesequeries\nareveryinaccurate andsomearevirtuallymonotone; the\nlyricsaretheironlyrecognizable feature. Itisunclear\nthatanyQBHsystemshouldbedesigned toaccommo-\ndatesuchquerieswithoutexplicitlymodeling singerpro-\nductionandtranscription error[4,7].\n4CONCLUSION\nPreviousworkbytheauthorsexploredthetradeoffbe-\ntweenretrievalperformance andretrievalspeed.Inthe\npresentworkweproposed iterativedeepening asatech-\nniquetocircumventthistradeoff,inparticular forlarge\ntargetdatabases. Weemployedasmoothpitchcontour\nthatisamenable tovariablerepresentation length.Anit-\nerativequery-by-humming systemwasimplemented that\nmakesthreepassesthroughthetargetdatabase, remov-\ningasmanytargetsaspossiblewitheachiterationwith-\noutintroducing unduefalsedismissals. Wealsoexpanded\ntheconventionaliterativeframeworktoretainthealign-\nmentsfoundineachiteration. Thesealignments areused\ntoadaptthealignment windowofsubsequent iterations,\nfurtherexpediting retrieval.We\u0002ndthatiterativedeepen-\ningcanspeedretrievalbyafactorof25,andtheadaptive\niterativemethodcanspeedretrievalbyafactorof40,both\nwithoutsigni\u0002cantly degradingretrievalperformance.\nACKNO WLEDGEMENTS\nThisresearchwassupported byaCARAT/Rackham Inter-\ndisciplinary FellowshipandagrantfromtheITRproject\noftheNSF.WewouldalsoliketothankJoeHeremans for\nhisassistance withqueryandtargetcollection.\n8Other authors de\u0002ne thewindo wsizeasaseparate constant\nrather than asafraction ofN[22]. Although thishasnoeffect\nontheexact computational cost, itdoes yield acomple xitythat\nis,strictly speaking, linear inN.REFERENCES\n[1]E.Keogh,K.Chakrabarti, M.Pazzani,and\nS.Mehrotra. Dimensionality Reduction forFast\nSimilarity SearchinLargeTimeSeriesDatabases.\nKnowledg eandInformation Systems 3(3),2000.\n[2]R.J.McNab,L.A.Smith,andI.H.Wittenetal.\nTowardstheDigitalMusicLibrary:TuneRetrieval\nfromAcoustic Input. Proc.ACMDigital Libraries\nConfer ence,1996.Bethesda, MD.\n[3]J.S.DownieandM.Nelson.EvaluationofaSimple\nandEffectiveMusicInformation RetrievalMethod.\nInProc.ACMSIGIR,pages7380,2000.\n[4]C.MeekandW.Birmingham. Johnnycan'tsing:A\ncomprehensi veerrormodelforsungmusicqueries.\nProc.ISMIR,October2002.\n[5]N.H.Adams,M.A.Bartsch,J.Shiffrin,andG.H.\nWake\u0002eld.TimeSeriesAlignment forMusicInfor-\nmationRetrieval.Proc.ISMIR,2004.Barcelona,\nSpain.\n[6]N.HuandR.Dannenber g.AComparison of\nMelodicRetrievalTechniques UsingSungQueries.\nJointConf .onDigital Libraries,2002.\n[7]A.Pikrakis, S.Theodoridis, andD.Kamarotos.\nRecognition ofIsolatedMusicalPatternsUsingCon-\ntextDependent Dynamic TimeWarping. IEEE\nTrans. Speec hand Audio Processing,11(3):175\n183,May2003.\n[8]J.Song,S.Y.Bae,andK.Yoon.Mid-LevelMu-\nsicMelodyRepresentation ofPolyphonic Audiofor\nQuery-by-Humming System. Proc.ISMIR,2002.\nParis,France.\n[9]M.Grachten, J.-L.Arcos,andR.L´opezdeM´antaras.\nMelodicSimilarity: LookingforaGoodAbstraction\nLevel.Proc.ISMIR,2004.Barcelona, Spain.\n[10]S.Chu,E.Keogh,D.Hart,andM.Pazzani.Iter-\nativeDeepening Dynamic TimeWarpingforTime\nSeries. Second SIAM International Confer ence on\nData Mining,2002.\n[11]N.H.Adams,M.A.Bartsch, andG.H.Wake\u0002eld.\nNoteSegmentation andQuantization forMusicIn-\nformation Retreival.toappear IEEE Trans. Speec h\nandAudio Processing,January2006.\n[12]A.Ghias,J.Logan,D.Chamberlin, andB.C.Smith.\nQuerybyhumming: Musicalinformation retrieval\ninanaudiodatabase. InACM Multimedia ,pages\n231236, 1995.\n[13]Y.E.Kim,W.Chai,R.Garcia,andB.Vercoe.Anal-\nysisofaContour-basedRepresentation forMelody.\nInProc.ISMIR,October2000.\n[14]R.B.Dannenber gandN.Hu.Understanding Search\nPerformance inQuery-by-Humming Systems. Proc.\nISMIR,2004.Barcelona, Spain.\n205[15]D.Mazzoni andR.B.Dannenber g.MelodyMatch-\ningDirectly fromAudio. Proc.ISMIR,2001.\nBloomington, IN.\n[16]H.JinandH.V.Jagadish.IndexingHiddenMarkov\nModelsforMusicRetrieval. Proc.ISMIR,2002.\nParis,France.\n[17]R.B.Dannenber g,W.P.Birmingham, andG.Tzane-\ntakiset.al.TheMUSARTTestbedforQuery-By-\nHumming Evaluation. InProc.ISMIR,2003.\n[18]Y.ZhuandD.Shasha.WarpingIndexeswithEnve-\nlopeTransforms forQuerybyHumming. InProc.\nInternational Confer ence ofMana gement ofData\n(SIGMOD) ,SanDiego,CA,2003.\n[19]R.J.McNabandL.A.Smith.Evaluationofa\nMelodyTranscription System. IEEE Int.Conf .on\nMultimedia andExpo, 2000,2:819822, 2000.\n[20]C.A.Ratanamahatana andE.Keogh.Everything\nyouknowaboutDynamic TimeWarpingisWrong.\nInProc.3rdWorkshop ofMining Tempor alandSe-\nquential Data,2004.\n[21]S.W.Kim,S.Park,andW.W.Chu.Ef\u0002cientpro-\ncessingofsimilarity searchundertimewarpingin\nsequence databases: anindex-basedapproach. In-\nformation Systems,29(5):405420, 2004.\n[22]S.SalvadorandP.Chan.FastDTW:TowardAc-\ncurateDynamic TimeWarpinginLinearTimeand\nSpace.InProc.KDD Workshop onMining Tempo-\nralandSequential Data,2004.\n[23]P.Boersma. Accurate Short-TermAnalysis of\ntheFundamental FrequencyandtheHarmonics-to-\nNoiseRatioofaSampled Sound.InProc.Institute\nofPhonetic Sciences oftheUniver sityofAmster dam,\nvolume17,pages97110.1993.\n[24]L.R.RabinerandB.H.Juang. Fundamentals of\nSpeec hReco gnition.PrenticeHall,UpperSaddle\nRiver,NJ,1993.\n[25]A.Ito,S.-P.Heo,M.Suzuki,andS.Makino.\nComparison ofFeatures forDP-Matching Based\nQuery-by-Humming System. Proc.ISMIR,2004.\nBarcelona, Spain.\n206"
    },
    {
        "title": "The CLAM Annotator: A Cross-Platform Audio Descriptors Editing Tool.",
        "author": [
            "Xavier Amatriain",
            "Jordi Massaguer",
            "David García",
            "Ismael Mosquera"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416908",
        "url": "https://doi.org/10.5281/zenodo.1416908",
        "ee": "https://zenodo.org/records/1416908/files/AmatriainMGM05.pdf",
        "abstract": "This paper presents the CLAM Annotator tool. This application has been developed in the context of the CLAM framework and can be used to manually edit any previously computed audio descriptors. The application offers a convenient GUI that allows to edit low-level frame descriptors, global descriptors of any kind and segmentation marks. It is designed in such a way that the interface adapts itself to a user-defined schema, offering possibilities to a large range of applications. Keywords: Audio Descriptors, XML, Annotating Tool 1",
        "zenodo_id": 1416908,
        "dblp_key": "conf/ismir/AmatriainMGM05",
        "keywords": [
            "Audio Descriptors",
            "XML",
            "Annotating Tool",
            "CLAM Annotator tool",
            "CLAM framework",
            "GUI",
            "low-level frame descriptors",
            "global descriptors",
            "segmentation marks",
            "user-defined schema"
        ],
        "content": "The CLAM Annotator: A Cross-platform Audio DescriptorsEditing Tool\nXavier Amatriain\nCREATE\nUniversity of California\nSanta Barbara CA 93106 USA\nxavier@create.ucsb.eduJordi Massaguer\nUniversitat Pompeu Fabra\nPsg. Circumvalacio, 18\nBarcelona, Spain\njmassaguer@iua.upf.esDavid Garcia\nUniversitat Pompeu Fabra\nPsg. Circumvalacio, 18\nBarcelona, Spain\ndgarcia@iua.upf.esIsmael Mosquera\nUniversitat Pompeu Fabra\nPsg. Circumvalacio, 18\nBarcelona, Spain\nimosquera@iua.upf.es\nABSTRACT\nThis paper presents the CLAM Annotator tool. This ap-\nplication has been developed in the context of the CLAM\nframework and can be used to manually edit any previ-\nously computed audio descriptors. The application offers\na convenient GUI that allows to edit low-level frame de-\nscriptors,globaldescriptorsofanykindandsegmentation\nmarks. It is designed in such a way that the interface\nadapts itself to a user-deﬁned schema, offering possibil-\nities to a large range of applications.\nKeywords: Audio Descriptors, XML, Annotating Tool\n1 INTRODUCTION\nDescriptor extraction from an audio source is one of the\nmostimportantpracticesrelatedtotheMusicInformation\nRetrieval ﬁeld. Many different research teams are focus-\ningonﬁndingmoreandbetteralgorithmstoautomatically\nextract relevant features from the original signal.\nNevertheless, none of these algorithms can guarantee\na 100% reliability. In many cases automatically extracted\ndescriptors must be ﬁne-tuned by hand and appropriate\ntools are therefore needed. These manual tools become\nevenmoreimportantforresearchteamswhentestingnew\nalgorithms.\nThe goal of the CLAM Annotator is to provide such\na tool, offering a ﬂexible, reliable, extensible and efﬁ-\ncient alternative to other existing applications such as th e\nWaveSurfer[1].Also,thefactthattheCLAMAnnota-\ntorisprovidedinthecontextoftheCLAMframework[2]\nallowstoextenditinunlimitedways,byembeddingauto-\nmatic extraction algorithms in it, for instance.\nAnother important feature of the CLAM Annotator\nis that it uses standard XML language to represent data.\nThus it works with readable and easily understandable\ndata ﬁles that should be also easily connected to external\napplications or databases.\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrstpage.\nc/circlecopyrt2005 Queen Mary, University of London2 ON FILES AND FORMATS\nTheCLAMAnnotatormakesuseofdifferentXMLﬁlesin\nordertorelatewiththeoutsideworld. Allpreviouslygen-\nerated information is input to the program through XML\nﬁles and the result of the editing process is also dumped\ninto those ﬁles.\nTheProject File contains a pointer to a Schema File\nand another one to a Song List File . The Song List File\ncontains a list of Audio Files andDescriptors Pool Files .\nIn the following sections we will detail the content of\neach of those ﬁles.\n2.1 The Project File\nThe Project ﬁle is an XML ﬁle with the “.pro” extension.\nIt simply contains the path to the Song List ﬁle and the\npath to the Schema ﬁle. See the following example:\n<Project>\n<Songs>Songs.sl</Songs>\n<Schema>Schema.sc</Schema>\n</Project>\nListing 1: Sample Annotator Project ﬁle\n2.2 The Song List File\nThe Song List ﬁle is also an XML ﬁle with the “.sl” ex-\ntension. It contains a list of Sound ﬁle + Descriptors ﬁle\ntuples.\nA Sound ﬁle is simply the path to an existing sound\nﬁle. Thissoundﬁlecanbeinvirtuallyanyformat,includ-\ning PCM encoded ﬁles such as WAVs or AIFFs or com-\npressedformatssuchasMP3orOGG.Ontheotherhand,\ntheDescriptorsﬁlehasapointertothedescriptorsrelated\nto that particular sound ﬁle. If omitted, the program will\nsimply add the “.pool” extension to the sound ﬁle name.\nSee the following example:\n<SongFiles>\n<FileNames>\n<Name>\n<SoundFile>07.mp3</SoundFile>\n</Name>\n<Name>\n<SoundFile>08.mp3</SoundFile>\n<DescriptorsFile>08.mp3.pool</\nDescriptorsFile>\n</Name>\n...\n</FileNames>\n</SongFiles>\nListing 2: Sample SongList ﬁle\n4262.3TheSchemaFile\nTheSchemaﬁlecontainsthelistofallthedifferentde-\nscriptorsthatwilllaterwillbeloadedfromaDescriptors\nﬁle(see2.4).Insomecasesitalsogivestheirtypeand\nrangeofexpectedvalues.Althoughthisﬁleisaregular\nXMLﬁlewiththe“.sc”extension,inmanysensesitmim-\nics the purpose and syntax of an XML Schema format [3].\nTheSchemaﬁleisactuallydividedintotwodifferent\nsections.TheﬁrstonedeﬁnestheSchemaforhigh-level\ndescriptorswhilethesecondonedeﬁnestheschemafor\nlow-leveldescriptors(seeexampleinlisting3).\nAlthoughthedifferencebetweenlowandhigh-level\ndescriptorsisamatterofcontroversyandmuchhasbeen\nwrittenaboutit(see[4])inthisapplicationwehavetaken\napragmaticalapproach. Ahigh-leveldescriptorisconsid-\nered to be any descriptor that has a whole song scope and\nisuniquewithinthisscope. Thiskindofdescriptorcanbe\nof any type. On the other hand a low-level descriptor has\na Frame scope and can only take ﬂoating point values.\n<Schema>\n<HLDSchema><HLDs>\n<HLD>\n<Name>Title</Name>\n<Type>String</Type>\n</HLD>\n<HLD>\n<Name>Danceability</Name>\n<Type>Float</Type>\n<fRange><Max>10</Max><Min>0</Min></\nfRange>\n</HLD>\n<HLD>\n<Name>Key</Name>\n<Type>Enum</Type>\n<Values>A A# B C...</Values>\n</HLD>\n<HLD>\n<Name>BPM</Name>\n<Type>Int</Type>\n<iRange><Max>240</Max><Min>0</Min></\niRange>\n</HLD>\n...\n</HLDs></HLDSchema>\n<LLDSchema>\n<LLDNames>Pitch SpectralCentroid\nSpectralSpread...</LLDNames>\n</LLDSchema>\n</Schema>\nListing3: Sample Annotator Schema ﬁle\nWe will now see how the schema is deﬁned both for\nhigh-level and low-level descriptors.\n2.3.1 High-level descriptors\nAsalreadymentionedahigh-leveldescriptorhasaunique\nvalue for a whole song or sound source. It can be of any\nof the following types: ﬂoating point number (“Float”);\ninteger number (“Int”); string (“String”); or value set re-\nstricted strings (“Enum”).\nA high-level descriptor is therefore deﬁned by giving\nits “Name” and its “Type”. In case the type is a number,\nan optional range of valid values may be given (“iRange”\nin case of integer values and “fRange” in case of ﬂoating\npoint values). See the HLD section in listing3.2.3.2 Low-level Descriptors\nA low-level descriptor is in any case a vector of ﬂoating\npoint values where each value refers a particular frame.\nIn this case we only need to deﬁne the name of the de-\nscriptors. Therefore the low-level descriptors section of\nthe schema is simply a list of low-level descriptors names\n(see again listing 3).\n2.4 Descriptors Pool File\nThis is an XML ﬁle with the “.pool” extension that con-\ntains all the values, both for the high-level and low-levels\ndescriptors. The content must observe the restrictions\ngivenintherelatedSchemaorelseitwillnotbe validated.\nEvery song on the project has its own descriptors ﬁle.\nDescriptionsmaybegeneratedbyanythird-partyapplica-\ntionbyproviding aproper schema, though itismucheas-\nier to generate it from within the CLAM framework. In\nthis case, the Descriptors ﬁle is directly the XML repre-\nsentation of a CLAM Descriptors Data Pool. Any extrac-\ntion algorithm using them may dump its results in such\nformat without having to worry about formatting issues.\nAs in the Schema, a Descriptors ﬁle is divided into\ntwo sections: one for the high-level descriptors and an-\notheroneforthelow-leveldescriptors(seelisting4). Not e\nthatintheDescriptorsﬁlethisdifferenceisexplicitbyth e\nexistence of two different “Scopes”, one with the name\n“Song” and size=1 (there is only one song for each song)\nand the other one with the name “Frame” and size=8917\n(inthis case there are 89671 frames in the song).\n<DescriptorsPool>\n<ScopePool name=\"Song\" size=\"1\">\n...\n</ScopePool>\n<ScopePool name=\"Frame\" size=\"8961\">\n...\n</ScopePool>\n</DescriptorsPool>\nListing4: Sample Descriptors ﬁle\nWe will now explain how high and low-level descrip-\ntors are stored.\n2.4.1 High-level Descriptors\nIn the Song scope we basically see a list of AttributePool\nelements. In any case each of those elements has an at-\ntribute with the name of the particular descriptor and its\ncontent is the content of the descriptor. Note that the type\nof the descriptor is implicitly resolved from the schema\nandmustthereforenotbegivenintheDescriptorsﬁle(see\nHLD description in listing5).\nFinally, segmentation information is also included in\nthe high-level description. This descriptor must not be\ngiven in the schema as it is always supposed to be avail-\nable. When including segmentation marks in the descrip-\ntionyoumustgivetheirsize(i.e. howmanysegmentation\nmarksareavailable) and thelistofpositions innumber of\nsamples.\n<ScopePool name=\"Song\" size=\"1\">\n<AttributePool name=\"Title\">\nPension_Triana</AttributePool>\n<AttributePool name=\"Danceability\">7.2</\nAttributePool>\n427<AttributePoolname=\"Key\">C</\nAttributePool>\n<AttributePoolname=\"BPM\">100</\nAttributePool>\n...\n<AttributePoolname=\"Segments\"size=\"43\">\n202334497049...</AttributePool>\n</ScopePool>\nListing5:SampleHigh-levelDescription\n2.4.2Low-Leveldescriptors\nThelow-leveldescriptorssectionoftheDescriptorsﬁleis\nalsoalistofAttributePoolelementswhereforeachele-\nmentwemustdeﬁneitsnameanalistofvalues.Note\nthatinthiscasewemustnotgivethesizeofeachattribute\nbecausethisisalreadydeﬁnedbythesizeofthe“Frame”\nscope.Thereforethesevectorsmustallhaveasmanyele-\nmentsasdeﬁnedinthescope(8961intheexamplegiven\ninlisting6).\n<ScopePoolname=\"Frame\"size=\"8961\">\n<AttributePoolname=\"Pitch\">636860...<\n/AttributePool>\n<AttributePoolname=\"SpectralDeviation\">\n505048...</AttributePool>\n...\n</ScopePool>\nListing6:SampleLow-levelDescription\n3THEAPPLICATION\nTheapplicationhasbeendevelopedwithintheCLAM\nframework,usingqt[5]forthegraphicaluserinterface.\nFigure 1 is a capture of the whole interface running on an\nDebian GNU/Linux box with the KDE desktop environ-\nment, although its look is virtually the same in any of the\nmajor platforms and graphical environments (Windows,\nMac OSX, GNOME...).\nIntheAnnotatorGUI,fourdifferentpartscanbeiden-\ntiﬁed: on the upper-left the list of songs is shown, on the\nupper-right the waveform and segmentation information\nof the selected song is available, on the lower-left there is\nthe information for the high-level descriptors, and ﬁnally\non the lower-right there is the low-level descriptors infor -\nmation.\n3.1 Loading a Project\nOnce the program is started, the ﬁrst thing that the user\nmustdoistoloadaprojectﬁle. Thisprojectﬁlewillhave\na pointer to the Song List and the Schema ﬁles. Once\nloaded, the GUI is reconﬁgured and the list of songs and\nrelated descriptions is available.\nThe user can also start a project from scratch. In this\ncase the Scheme and Song List ﬁles must be loaded by\nhand from the Project menu.\n3.1.1 The Schema and the dynamic GUI\nOne of the most important features in the CLAM An-\nnotator is its ability to dynamically adapt the GUI. The\nGUI shows the descriptors according to the Schema that\nis loaded with the Project.In the case of low-level descriptors, the amount and\nlabel of each of the tabs corresponds to the schema. And\nin the case of high-level descriptors, the schema deﬁnes\nthelabelandalsothekindofeditingwidgetthatisshown.\nThescreenshotshowninﬁgure3correspondstothesam-\nple Schema introduced in section 2.3.\n3.2 Viewing Song Properties\nOnce a song is selected from the Song List on the up-\nper left, the audio ﬁle and the descriptors are loaded. Af-\nter this loading process ﬁnishes the waveform including\nsegmentation marks is available on the upper-right, the\nlow-level descriptors are shown on the lower-right, and\nthe high-level descriptors are on the lower-left. The user\ncan listento the sound ﬁle and startthe edition process.\nLowleveldescriptorsviewandsegmentationvieware\nsynchronized in respect zoom, horizontal scroll and cur-\nsor position. That feature makes easy to take segmenta-\ntion editing decissions taking into acount low level fea-\ntures values.\n3.3 Editing Low-level Descriptors\nLow-level descriptors are represented by equidistant con-\nnected points that you can drag to change its Y value.\nEach point represents the value for the descriptor in a\ngiven frame. Because point to point edition may be hard,\nsome convenient edition modes such trimordraware\nprovided.\nFigure1:Thelow-leveldescriptors\n3.4 Editing High-level Descriptors\nFigure2:Thehigh-leveldescriptors\nThe edition of a high-level description adapts on the\n“type”ofthedescriptorasdeﬁnedintheproject’sschema.\nFigure 4 shows how integer and ﬂoat descriptors may be\nedited by a slider that uses the range given in the schema,\nwhileenumerated value descriptors can be selected with\na drop-down list widget with the allowed values. Regu-\nlar strings use a simple text box widget were the user can\nenter free text.\n428Figure3:TheCLAMAnnotatorGUI\nFigure 4: Editing integer and enum high-level descriptors\n3.5 Editing Segmentation Marks\nSegmentation marks may be viewed/edited in the wave\nformviewintheupper-right. Theycanbemoved,deleted\n(with the Delete key pressed) and inserted (holding down\nthe Insert key).\n3.6 Auralizing annotations\nSometimes visual representation is not as meaningfull as\nan aural feedback could be. The CLAM Annotator pro-\nvidesauralizationofdescriptors. Thatmeansmappingan-\nnotation data to synthesizer controls so that we can listen\nthe synthesized sound synchronized to the actual wave-\nform. Currently we support segmentation marks auraliza-\ntion and ﬂoating point low level descriptors auralization.\nSegmentation marks auralization consists on listening\na percussive sound at every segment start point. Floating\npoint low level descriptors may be auralized by modulat-\ning the pitch or the volume of a sound using descriptor\nvalues. Although low level descriptors auralization was\nintended for pitch related features, we found this kind of\nauralfeedbackbeingalsomeaninfullforotherdescriptors .\n4 CONCLUSIONS AND FUTURE WORK\nIn the paper we have presented the ﬁrst version of the\nCLAMAnnotator,atoolthatcanbeusedinordertorevise\nandﬁne-tunetheresultofautomaticdescriptionextractio nalgorithms. This tool will be made available as Free Soft-\nware in the next release of the CLAM framework.\nAlthough we believe that in its current state the appli-\ncation is already useful and valuable, future plans include\nthe possibility of dynamically loading automatic extrac-\ntion algorithms. With this addition the CLAM Annotator\nwill become a complete and ﬂexible framework for audio\ndescription and annotation.\nACKNOWLEDGEMENTS\nThe authors wish to thank the rest of the CLAM devel-\nopmentteamfortheircontributionsandallthemembers\nof the UPF Music Technology Group for their valuable\nfeedback.\nPart of this work has been done in the context of the\nSIMAC IST-507142 project.\nReferences\n[1]  K.Sjölander and J. Beskow. Wavesurfer – an open\nsource speech tool. In Proceedings of the Eighth Inter-\nnational Conference on Spoken Language Processing\n(ICSLP00), Beijing, China, 2000.\n[2]  X. Amatriain, P. Arumí and M.Ramírez. CLAM, Yet\nAnother Library for Audio and Music Processing? In\nProceedings of the 2002 Conference on Object Ori-\nented Programming, Systems and Application (OOP-\nSLA2002), Seattle, USA, 2002. ACM.\n[3]\u0000 www-XMLSchema.\u0000   World\u0000   Wide\u0000   Web\u0000   Con-\nsortium    (W3C)’s    XML-Schema    home    page,\ngrammingwithQT3 .PearsonEducation,2004.\n[4]  X. Amatriain and P. Herrera. Transmitting Audio Con-\ntent as Sound Objects. In Proceedings of the AES 22nd\nConference on Virtual, Synthetic, and Entertainment\nAudio, Helsinki, 2001. Audio Engineering Society.\n[5 ]  J. Blanchette and M. Summerfield.  C++ GUI Pro-\ngramming with QT3 . Pearson Education, 2004.\n429"
    },
    {
        "title": "Ringomatic: A Real-Time Interactive Drummer Using Constraint-Satisfaction and Drum Sound Descriptors.",
        "author": [
            "Jean-Julien Aucouturier",
            "François Pachet"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416532",
        "url": "https://doi.org/10.5281/zenodo.1416532",
        "ee": "https://zenodo.org/records/1416532/files/AucouturierP05.pdf",
        "abstract": "We describe a real-time musical agent that generates an audio drum-track by concatenating audio segments automatically extracted from pre-existing musical files. The drum-track can be controlled in real-time by specifying high-level properties (or constraints) holding on metadata automatically extracted from the audio segments. A constraint-satisfaction mechanism, based on local search, selects audio segments that best match those constraints at any time. We report on several drum track audio descriptors designed for the system. We also describe a basic mecanism for controlling the tradeoff between the agent’s autonomy and reactivity, which we illustrate with experiments made in the context of a virtual duet between the system and a human pianist. Keywords: interaction, drumtrack, metadata, constraint satisfaction, concatenative synthesis 1",
        "zenodo_id": 1416532,
        "dblp_key": "conf/ismir/AucouturierP05",
        "keywords": [
            "real-time",
            "musical agent",
            "audio drum-track",
            "concatenating audio segments",
            "pre-existing musical files",
            "high-level properties",
            "metadata",
            "constraint satisfaction mechanism",
            "tradeoff between autonomy and reactivity",
            "virtual duet"
        ],
        "content": "Ringomatic: AReal-Time InteractiveDrummer\nUsing Constraint-SatisfactionandDrum Sound Descriptors\nJean-JulienAucouturier\nSONYCSL Paris\n6,rueAmyot\n75005Paris,France\njj@csl.sony.frFranc ¸oisPachet\nSONY CSL Paris\n6,rueAmyot\n75005Paris,France\npachet@csl.sony.fr\nABSTRACT\nWe describe a real-time musical agent that generates an\naudio drum-track by concatenating audio segments auto-\nmatically extracted from pre-existing musical ﬁles. The\ndrum-track can be controlled in real-time by specifying\nhigh-level properties (or constraints) holding on meta-\ndata automatically extracted from the audio segments. A\nconstraint-satisfaction mechanism, based on local search ,\nselectsaudiosegmentsthatbestmatchthoseconstraintsat\nany time. We report on several drum track audio descrip-\ntors designed for the system. We also describe a basic\nmecanismforcontrollingthetradeoffbetweentheagent’s\nautonomy and reactivity, which we illustrate with exper-\niments made in the context of a virtual duet between the\nsystem anda humanpianist.\nKeywords: interaction,drumtrack,metadata,constraint\nsatisfaction,concatenativesynthesis\n1 INTRODUCTION\nState-of-the-art sample-based drum machines (or virtual\ndrumkits)suchasFxpansion’sBFD(FXpansion,2003)or\nToontrack’s Drumkit From Hell (Toontrack, 2003) offer\ndrum programmersalmost total control over the sampled\nsoundsthatareplayed,themicrophonesused,thedrumkit\nmanufacturer,andeventheindividualdrumsandcymbals\nbeing used. Like other sampled instruments, they bene-\nﬁt fromtheimprovementofdigitalstorage,oftenoffering\ntensofthousandsofsoundsfromtensofdifferentdrumk-\nits, recorded by tens of different drummers, each using\nseveral velocities for each stroke. They also ship with\nlarge libraries of Midi-like drum patterns (or presets, or\n“grooves”), which can be associated with one of the very\nmanysetsofsoundstogiveaninstant,realisticdrumtrack.\nWhiletheexpressivepowerofsuchmachinesfordrum\nprogrammersis unprecedented, they offer very little pos-\nPermission to make digital or hard copies of all or part of thi s\nwork for personal or classroom use is granted without fee pro -\nvided that copies are not made or distributed for proﬁt or com -\nmercial advantage and that copies bear this notice and the fu ll\ncitationon the ﬁrstpage.\nc/circlecopyrt2005 Queen Mary, University ofLondon\n \nDatabasethigh energy\ntom-tomslow energy\nno tom-tomslow energy\nsome cymbalshigh energy\ntom-toms\n and cymbals\nFigure 1: The drumtrack is produced by concatenating\ndrumbars selected in a database according to their meta-\ndata.\nsibilities for interactive music systems, as proposed e.g.\nin Rowe (1993). On the one hand, the sounds and pat-\nterns are mostly undescribed, only using editorial, arbi-\ntrary metadata (e.g. what are the perceptual qualities of\n“retrobreaksﬁll A” ? Is it energetic ? Syncopated ? How\ndoes“kick-Leedy”soundscomparedto “kick-PearlB”?).\nThismakeshigh-levelmappingsbetween a real-timemu-\nsical inputandthevirtualdrummerdifﬁcult. Onthe other\nhand, it is difﬁcult to add new sounds into the system, in\norderto adapt to speciﬁc musical contexts. This typically\ninvolvesbuyingexpensive,pre-builtextensionpacks.\nIn this work, we propose a sampled-based drum ma-\nchinewhoseoutputcanbecontrolledinreal-timebyhigh-\nlevelpropertiessuchasenergy,density,saliencyofdrums\norcymbals,etc. Weuseaudioanalysistechniquesinspired\nby MIRto bothgatherthe sampledmaterial, which is au-\ntomatically extracted from pre-existingmusical ﬁles (e.g .\ndrumsolopartsinajazzmp3)andindexeachsamplewith\nacoustic metadata, automatically extracted from the sig-\nnal. Thetypical sampleusedin the system isa few beats’\naudioextractfromadrumpart,whichcorrespondtoamu-\nsical bar, and can therefore be looped while preserving a\nfeelingofsteadybeatandmetric. AsseeninFigure1,the\ndrumtrackproducedbythe drummeris a continuouscon-\ncatenation of such bars of drumming, which we call here\n412drumbars . We proposeaconstraint-satisfactionalgorithm\nto control the drumtrack’s high-level properties, such as\nits energy or its continuity, and a real-time mechanism to\nallow constraintstobe modiﬁedatanytime.\nOursystembuildsonseveralpriorworks. Rhythmisa\nwell-coveredsubject of study in computermusic. Bilmes\n(1993) works on transcriptionsto study velocity and tim-\ning deviations in human performance, aiming at building\nmorerealisticdrummachines. Therehavebeennumerous\nattempts at generating interesting rhythms, notably with\nGenetic Algorithms (Pachet, 2000; Tokui and Iba, 2001).\nOn the audio side, drum sounds have received recent at-\ntention in the MIR community,to either transcribe drum-\ntracks from polyphonic music (Zils et al., 2002), mea-\nsure similarity between drum patterns (Paulus and Kla-\npuri,2002)orclassifydrumsounds,notablybetweenbass\nand snare drum (Herreraet al., 2002; Yoshii et al., 2004).\nHowever,most of the workso far hasfocusedonthe tim-\nbre of individual drum strokes, rather than on perceptive\nqualitiesoffull drumloopslike inthispaper.\nConcatenative synthesis (Lazier and Cook (2003);\nSchwarz (2003); Zils and Pachet (2001)) is also gaining\nmore and more attention in the ﬁeld of music. Concate-\nnative synthesis uses a database of samples, or units, and\naunit selection algorithmthat ﬁnds the sequence of units\nthatmatchbestatargetsoundorphrase. Itmainlyfocuses\nontheprecisereconstructionofthetarget,e.g. forrealis tic\ninstrument synthesis. While our work uses the same ba-\nsic schemeofconcatenatingsoundsamplesselectedfrom\na database, we target an interaction context where there\nis no pre-speciﬁed target sequence. On this respect, it is\nmore on the side of the automatic sampler described in\nAucouturieret al. (2004).\nConstraint satisfaction programming (CSP) ﬁnally is\na paradigm for solving difﬁcult combinatorial problems,\nparticularly in the ﬁnite domain. In this paradigm, prob-\nlems are represented by variables having a ﬁnite set of\npossible values, and constraints represent properties tha t\nthe values of variables should have in solutions. CSP is\na powerful paradigm because it lets the user state prob-\nlems declarativelyby describinga priori the propertiesof\nits solutions and use general-purpose algorithms to ﬁnd\nthem. There have been numerous applications of CSP to\nmusic, e.g. for automatic generationof playlists of music\ntitles (Aucouturier and Pachet, 2002), automatic harmo-\nnization (Pachet and Roy, 2001) and spatialization (Pa-\nchet and Delerue, 2000). In Zils and Pachet (2001), we\nintroducedtheconceptofmusicalmosaics(“Musaicing”),\nand the idea of using CSP to generateaudio sequencesof\nsoundsamples,with high-levelconstraintsholdingonthe\nmetadata of the samples. The work presented on this pa-\nperisa real-time,interactiveextensionofMusaicing.\n2 AUTOMATIC GATHERINGAND\nINDEXINGOFAUDIOMATERIAL\nThis section describes both how drumbars are gathered\nautomatically from existing music titles, and how each\ndrumbar is described with automatically computed meta-\ndata. Both steps rely on EDS (Zils and Pachet, 2004), an\naudioclassiﬁcationsystemdevelopedat SonyCSL.\nFigure 2: Screenshot of the EDS system showing the re-\nsultsoffeaturediscoveryforthedetectionofdrumsolo\n2.1 MetadataExtractionwith EDS\nEDS (Extractor Discovery System) (Zils and Pachet,\n2004) is a generic scheme for extracting arbitrary high-\nlevel audio descriptors from audio signals. It is able to\nautomatically produce a fully-ﬂedged audio extractor (an\nexecutable) from a database of labeled audio examples.\nIt uses a supervised learning approach. Its main charac-\nteristics is that it ﬁnds automatically optimal audio fea-\ntures adapted to the problem at work. Descriptors are\ntraditionally designed by combining Low-Level Descrip-\ntors (LLDs) using machine-learning algorithms (see e.g.\nScheirer and Slaney (1997)). The key idea of EDS is to\nsubstitute the basic LLDs with arbitrarycomplex compo-\nsitionsofsignalprocessingoperators: EDS composesau-\ntomatically operators to build features as signal process-\ning functions that are optimal for a given descriptor ex-\ntraction task. The search for speciﬁc features is based on\ngeneticprogramming,awell-knowntechniqueforexplor-\ning search spaces of functioncompositions(Koza, 1992).\nResulting features are then fed to a learning model such\nas a Gaussian Mixture Models (GMM) or Support Vec-\ntor Machine (SVM) to produce a fully-ﬂedged extractor\nprogram.\n2.2 DrumSoloDetectionwithEDS\nIn order to extract drumbars from music recordings, we\nﬁrstneedtheabilitytodetectdrumsoloparts,i.e. section s\ninmusicwhereonlyadrumkitisplaying. Thisistypically\na drum solo in the middle of a jazz piece or shorter drum\nbreaksinarockorfunksong. Wemodeltheproblemasa\n2classclassiﬁcationproblem. Webuildalabeleddatabase\nof 100 5-second music extracts, the ﬁrst 50 being pure\ndrum solo, and the other 50 various extracts of popular\nmusic, encompassing many different genres (jazz, rock,\nheavy metal, classical, folk, electronic), with or without\ndrums. Figure 2 shows a screenshot of the EDS appli-\ncation after a few generations of 50 features and Table\n413Table1: EDS featuresforDrumdetection\nFeature Fitness\narcsin(power(rms(power(abs(hpFilter(x,7585.0)),-1)) ,-4)) 3.65\nmin(spectralKurtosis(hann(split (bartlett(bpFilter(t riangle(square(normalize(x))),223,2456)),134)))) 3.58\niqr(sqrt(hamming(normalize(bpFilter(triangle(square (normalize(x))),423,603))))) 3.15\nlog10(abs(sum(pitch(blackman(split (bpFilter(normali ze(x),1298,2558),16131)))))) 0.91\n... ...\nmean(zcr(split(x,2048)) 0.72\nmean(spectralFlatness(split (x,2048)) 0.71\nmean(spectralCentroid(split (x,2048)) 0.68\n1 shows the top 4 features found by EDS as well as the\nresults achieved with some common mp7 features (zero-\ncrossing rate, spectral ﬂatness and SpectralCentroid), fo r\ncomparison. The ﬁtness of the features is computed with\nthe Fisher criteria. Here are some details about the op-\nerators selected by EDS in Table 1. See Zils and Pachet\n(2004)formoredetails.\n•split (X, n)does a n-point windowing of the signal.\n•rmscomputes the square root of the mean of the squared\nvalues of a vector.\n•hpFilter is a 2-order high pass Butterworthﬁlter.\n•Blackman andBartlett are standard Blackman and\nBartlettwindows.\n•iqristheinterquantilerangeofthedata,i.e. thedifference\nbetween the 25% percentile value and the 75% percetntile\nvalue.\n•pitchis a speech-dedicated F0 analysis method, based\non autocorrelation, using the Praat toolbox (Boersma and\nWeenink, 2005).\n•Spectral Flatness ,Spectral Kurtosis ,Spectral\nCentroid andzcrare the standard MP7operators.\nThe 4 best features are then fed to a number of ma-\nchine learning algorithms, which are individually opti-\nmized over their parameter space (e.g. number of near-\nest neighbors for a knn classiﬁer). We measure the pre-\ncision by using 10-fold cross validation on the training\ndatabase. As shown in Table 2, the classiﬁcation results\nare near perfect(at best only onemis-classiﬁed instance).\nThebestmodelisak-nnclassiﬁerusing2inverse-distance\nweightednearestneighbors.\nWe apply the drum detector on sliding 3-second win-\ndows on full songs to segment drum solo parts. For ro-\nbustness, we only look for segments corresponding to at\nleast 3 successivewindowsclassiﬁed asdrums.\nTable2: PrecisionoflearningalgorithmsforDrumdetec-\ntion\nAlgorithm Precision\nk-NearestNeighbor 0.99\nSupportVectorMachine 0.98\nNeuralNetwork 0.98\nJ48pruneddecisiontree 0.97\nGaussianMixtureModel 0.932.3 Segmentation\nOnce large sections of music which only includes drum\nsolo are identiﬁed, we segment them in 4-beat drumbars\nusinga stripped-downversionof the methoddescribedin\nScheirer(1998). Sincebeattrackingondrumtracksisusu-\nallyaloteasierthanonarbitrarilycomplexpolyphonicau-\ndio, we only consider one frequencyband [0-400 Hz]. A\nﬁrstpassisdonetocomputethebpmon3-secondbuffers,\nandasecondpassisdonewithabeat-trackertunedonthe\nmost represented tempo found during the ﬁrst pass in or-\nder to localize the beats. Then, a 4-beat-long drumbar is\nextractedeverybeat. Sucha methodhasa numberof dis-\nadvantageswithrespecttothemetricofthemusicalpiece.\nFirst, this makes the assumption that the signature of the\npiece is 4/4, which is true for a vast majority of popular\nmusicpieces,butmaynotalwaysbethecase. Second,the\n1-beatoverlapofthedrumbarsextractionbreakstheorigi-\nnalpositionofstrong/weakbeats. Furtheranalysisispos-\nsible to segment more meaningful drum units, as shown\ne.g. in Zaanenet al.(2003).\n2.4 MetadataExtraction\nOnce a database of drumbars has been gathered, we in-\ndex each sample with perceptually-meaningfulmetadata.\nAgain, we use the EDS system to ﬁnd good speciﬁc sig-\nnal processing features, and to optimize machine learn-\ning algorithms that use these features. We describe here\n4 descriptorsrelevantfordrumbarsthat we analyzedwith\nEDS. For each, we give the best feature found by EDS,\nand the classiﬁcation results using 10-fold cross valida-\ntion. Each descriptor was trained on the same hand-\nlabeled database of 75 drumbars extracted from a swing\ndrumtrackgeneratedbyBFD(FXpansion,2003). Inorder\nto minimize the labeling effort, we model each problem\nas a 3-class classiﬁcation problem (low/medium/high).\nHowever, these descriptors are intrinsically continuous,\nnumerical values. Hence, we force the training to use a\n2-nnmodelforclassiﬁcation1,andre-usethesame model\nas a regression model2in order to compute the values on\ntheﬁnaldatabase.\n•Energy: the perceptiveenergyofthe drumbar,inde-\npendentoftheRMS volume(alldrumbarsareRMS-\n1i.e. a new instance is assigned tothe most represented class\namong itsneighbors\n2i.e. the value of a new instance is the weighted mean of its\nneighbors values\n414normalized). ThebestfeaturefoundbyEDS\nMean (Log(V ar(Split (Deriv (Square (X)),1s))))\n(1)\nis consistent with previous studies on a popular mu-\nsic database (Zils and Pachet, 2003). This yields a\nprecisionof0.89.\n•Onset Density : the sensation of stroke density in\nthedrumbar. Drumrollstypicallyincludeverymany\nstrokes,whilesomeﬁllsmayincludejustafewkicks\nandcrashes. ThebestfeaturefoundbyEDS\nlength (peaks (rms(hamming (split (X,4096)))))\n(2)\ncan be interpreted as a rough count of peaks of en-\nergy. Theprecisionoftheassociatedknnclassiﬁeris\n0.92\n•Presence of drums : the importance of tom and bass\ndrumstrokesasopposedtocymbalsandsnaredrums.\nJazz drummers typically use toms to give a ethnic\ngroovetoasong,ratherthancymbalsandridewhich\nare typically used for swing. The best feature found\nbyEDS\nSpectralDecrease (Deriv (Square (Norm (X))))\n(3)\ngivesa classiﬁcationprecisionof0.84\n•Presence of cymbals : the importance of high-\nfrequency sounds like cymbals and ride. The best\nfeaturefoundwith EDS\ndivision (rms(lpfilter (X,500,44100)) , rms (X))\n(4)\nis simply the ratio ofhigh frequencyenergyoverthe\ntotal energy of the signal. This achieves 0.82 preci-\nsion.\n3 Constraint-Based Concatenative\nSynthesis\n3.1 IncrementalReal-TimeConstraintSatisfaction\nWe deﬁne the interactive generation of drumtracks as a\nreal-time constraint-satisfaction problem (CSP). At any\ntime,thenextdrumbartobeselectedaswellasthe Mlat-\nest past drumbars( Marbitrary, can be as large as n−1)\nconstitute a sequence of variables Vn−M,Vn−M+1, ...,\nVn−1,Vn, whosevaluescan be taken from a ﬁnite data-\nbase of Ndrumbars, called their domain. Each variable\nVirepresentsthe ithdrumbarinthesequence. Wecallthe\nvariable Vncorresponding to the next drumbar to be se-\nlectedthe currentvariable ,andthe Vn−ifori= 1..Mthe\npast variables .\nThe problem is to successively assign values to each\nvariable so that the resulting sequence satisﬁes a set of\nconstraints deﬁned by the user. The constraints may\nchange at any time, in an asynchronous manner. Obvi-\nously, at any time, the problem can only set the value of\nthe current variable : once a variable is played, its value\ncannot be changed (“one can’t modify the past”). How-\never, the choice of the next drumbar is inﬂuenced by the\nFigure3: AnincrementalCSPwith3variablesand4con-\nstraints. V3isthe currentvariable,and V1,V2are thepast\nvariables.\nFigure 4: The same incremental CSP than in Figure 3,\naftertheincrementoperation.\npastchoices,asconstraintsholdingonthecurrentvariabl e\nmay also holdon the past variables. The constraintstypi-\ncally hold on metadata of the assigned drumbars, such as\ntheonedescribedinsection2.4.\nTo model the passing of time, we introduce the no-\ntionofincrement operation. Eachtimeavalueisassigned\nto the current variable, the problem is incremented, i.e.\na new variable is added to the problem. The former cur-\nrentvariablebecomesapastvariable,andthenewvariable\nrepresentthenextcurrentvariable.\nFigures 3 and 4 explicit the structure of the problem,\nand illustrate the increment operation. In Figure 3, the\nCSP at a given iteration iincludes M= 3variables, one\ncurrent and 2 past, with some constraints (the GandL\ncircles) holding on them. A value is selected for V3, and\nthecorrespondingaudioisscheduledtobe played. At the\nnext iteration i+ 1, the CSP is incremented, i.e. a new\nvariable V4isadded,whichbecomesthenewcurrentvari-\nable. Note that the scope of the constraints is automati-\ncally modiﬁed to also hold on the newly added variable.\nWe will explicit two strategies for such a mechanism in\nSubsection3.3.2below.\n3.2 IncrementalAdaptiveSearch\nThe technique we propose is based on an adaptation of\nlocal search techniques to constraint satisfaction, calle d\nadaptive search (Codognet and Diaz, 2001). In our con-\ntext,sinceonlyonevariablecanbemodiﬁedatatime(the\ncurrent variable), there is no combinatorial explosion of\nthe search space. A complete enumerationof all possible\nvaluesforthecurrentvariableisonlythesizeofthedrum-\nbar database. Adaptive search is mainly targeted at off-\nline problemswhere all values must be assigned simulta-\nneously, and a complete NMenumeration is intractable,\ne.g. inplaylistgeneration(AucouturierandPachet,2002) .\nHowever, adaptive search’s formulation of constraints as\nsimple cost functions is still well suited for our problem\nwhichisclearlyover-constrained: itislikelythatthecon -\nstraints cannot all be satisﬁed at the same time. The cost\n415of a constraint represents”how bad” the constraint is sat-\nisﬁed, fora givenassignmentofvariables.\nMoreprecisely,we deﬁne:\n•the cost F(Vi, C)of a given variable Viwith value\nXi, with respect to a given constraint C, which rep-\nresents”howbadly” Xisatisﬁes C\n•the cost F(Vi)of a given variable Viwith value Xi,\nwhichistheweightedsumofitscosts F(Vi, C)with\nrespect to each constraint holding on Vi. Each con-\nstraint has a weight, which enables to balance the\nimportance of some constraints over some others.\nSection 4.2 illustrates the importance of constraint\nweighting.\n•the global problem cost F(CSP ), which is the sum\nofthe F(Vi)forall Viinthe problem.\nAssigninganewvaluetothecurrentvariablemodiﬁesthe\ncostsF(vn, Ci)of all the constraints Ciholding on vn,\nandinturnmodiﬁesallthecosts F(Vi)ofallthevariables\nwithin the scope of one of several constraints Ci, and ﬁ-\nnallytheglobalproblemcost F(CSP ).\nThealgorithmworksasfollows:\n•Startwitha n= 1problem,i.e. onecurrentvariable,\nandnopast.\n•Repeat :\n–Find the best possible value for Vnby trying\nsuccessively all the values in the domain, and\nselectthe valuethatminimizes F(CSP ).\n–Assignthisvalueto Vn.\n–IncrementtheCSP. (notably n=n+ 1);\nThe ﬁrst step of the repeat loop above may take a lot of\ntime, depending on the size of the current variable’s do-\nmain. However,itcanbeinterruptedatanytime,toreturn\nthe bestsolution so far.\n3.3 Localand GlobalConstraints\n3.3.1 ConstraintsasCost functions\nThe main interest of this algorithm is that constraints are\nsimply seen as cost functions, and hence are very easy to\ndeﬁne. For instance, the ”all different” constraint statin g\nthat all variables should have different values is deﬁned\nasfollows:\nAllDifferentCt.cost ()\nReturn 1 - the number of different values\nin the problem divided by the size of the\nsequence.\nMore complex constraints can be deﬁned as easily.\nForinstance,\n•distanceconstraint: forceseachvariables Viinscope\ntohavevalues Xiforwhichagivennumericalmeta-\ndatap(Xi)is as close as possible as a target value\npt(e.g. ”all thesevariablesshouldhaveanenergyof\n0.1”). The correspondingcost function is deﬁned as\nfollows:DistanceCt.cost()\nReturn the mean distance between the\np(Xi)andpt, i.e.1\nM/summationtextM−1\ni=0|p(Xi)−pt|2\n•continuity constraint : holds on a set of variable\nVi, i= 1..s. It forces each duplet of successive\nvariables {Vi, Vi+1}to have values {Xi, Xi+1}for\nwhich a given numerical metadata phas similar val-\nues{p(Xi), p(Xi+1)}. Thecorrespondingcostfunc-\ntioncanbedeﬁnedasfollows:\nContinuityCt.cost()\nReturn the mean distance between\nallp(Xi)andp(Xi+1), i.e.\n1\nM/summationtextM−2\ni=0|p(Xi)−p(Xi+1)|2\nIn practice, the cost functions are implemented more\nefﬁciently, by passing as argument the lastly modiﬁed\nvariable to the cost functions (which in our context, is\nalways the current variable Vn). This informationis used\ntocomputeonlythedifferentialcost,insteadofthewhole\ncost.\nFor instance, the cost function of the distance con-\nstraintcanbedeﬁnedin suchadifferentialway:\nDistanceCt.cost(Variable v)\nReturns1\nM(oldcost ∗(M−1) +|p(Xn)−pt|2)\nThissaves M−1databaseaccessestocomputethe p(Xi),\nandM−1substractions and multiplications. Such opti-\nmizationsare important,as the cost functionof each con-\nstraint is called Ntimes at each iteration, where Nis the\nsize ofthecurrentvariable’sdomain.\n3.3.2 LocalandGlobalConstraints\nIn the context of incremental CSP, and for the clarity of\nfurther discussions in section 4.2, we distinguish 2 types\nofconstraints:\n•Local Constraints only hold on the current variable.\nThey inﬂuence the selection of the next drumbar by\nonly looking at its intrinsic properties, without tak-\ning past values into account. Typically, a distance\nconstraintisa localconstraint.\n•Global Constraints hold on the current variable plus\nsome or all of the past variables. They inﬂuence the\nselection of the best drumbar by also accountingfor\nthevaluesofthe pastvariables. Typically,acontinu-\nity constraint is a global constraint, trying to select\nnewvaluesso that theyarecontinuouswith the past,\nalreadyselectedvalues.\nUpon increment of the CSP, local and global con-\nstraintshaveadifferentbehavior. Alllocalconstraintsu p-\ndatetheirscopebyremovingthepreviouscurrentvariable\n(nowvn−1), and adding the new current variable vn. All\nglobalconstraintssimplyaddthe newlyaddedvariableto\ntheirscope. ThismechanismisillustratedinFigures3and\n4: beforetheincrement,theglobalconstraints G1andG2\nhold on {V1, V2, V3}and the local constraints L1andL2\nhold only on V3, which is the current variable. After in-\ncrement, G1andG2modifytheirscopetoalsoincludethe\nnew current variable V4, while L1andL2now only hold\nonV4.\n416Figure5: Thereal-timeimplementationofthesystem,us-\ning4concurrentthreads. See maintextforexplanationof\nsteps1 to5.\n3.4 Real-timeImplementation\nThereal-timeimplementationofthesystem(doneinJava)\nuses several concurrent threads. A solverthread is given\na CSP, and solvesit using the algorithm describedin sec-\ntion 3.2. A audiothread is in charge of the continuous\nplaybackofthedrumtrack,byconcatenatingthevaluesof\nthe successive current variables. A scheduler thread iter-\natively queries the solver for the best solution so far and\nschedulesthecorrespondingaudioforplaybackbytheau-\ndio thread. Finally, a controlthread can modify at any\ntime the constraints holding on the CSP which the solver\nis currentlyworkingon.\nFigure 5 explicits the interactions between the 4\nthreads. At any time, the audio corresponding to the lat-\nest selected drumbar is playing, and a new value must be\nscheduled to immediatelystart after it ﬁnishes at endtime\nti.\n1. Attime ti−∆,theschedulerwakesup,andasksthe\nsolver for the best value it has found so far for the\ncurrentvariable,givenall thecurrentconstraintsand\nthe values of the past variables. The solver replies\nand increments immediately to start looking for the\nnextdrumbar.\n2. The scheduler retrieves the audio corresponding to\nthedrumbarfoundatstep1. Inthecurrentimplemen-\ntation,thisincludesreadinganddecodinga.mp3ﬁle\nbetweenastartandenddatethroughalocalnetwork,\nwhichmaytakea variabletime δi.\n3. At time ti−∆ +δi, the scheduler thread schedules\nthe decoded audio for the current drumbar for play-\nbackattheexactendingtime tiofthelatestdrumbar,\nwhich is currently being played. The choice of ∆is\nmade a priori, to ensure that ti−∆ +δi< ti, i.e.\n∆> δi,∀i. Inourcurrentimplementation,wechose\n∆ = 500 ms.\n4. The scheduler sleeps until ti+1−∆, having ti+1=\nti+di, where diis the duration of the audio just\nscheduled. This mainly gives the priority back tothesolver,whichkeepsscouringthedatabaseforthe\nnextvaluetobescheduledat ti+1.\n5. At any time, the control threadmay modifythe con-\nstraints holding on the CSP. This in turn modiﬁes\nthe values found by the solver, which enables the\nreal-timehigh-levelcontrolofthedrummer’soutput.\nSuch changes can be done manually by a user via\na GUI, or result of the analysis of an interaction, as\nproposedbelowinSection4.1.\n4 Experiments\n4.1 Midi interaction\nWe describe here preliminary experiments in which we\nusethedrummeragentininteractionwithahumanplayer\nplaying a midi keyboard. We automatically build a data-\nbase of 150 drumbarsextractedfroma set of demo songs\nrecordedwithBFD(FXpansion,2003),andautomatically\ncompute the associated metadata (see Section 2.4). The\ndrumtrack is constrained to use only drum samples with\na bpm of 120, so that the resulting concatenation has a\nsteadybeat.\nThemidiperformanceofthehumanplayerisanalysed\ninreal-timetoextractto followinginformation:\n•energy : the mean velocity of the note-on Midi\nmessages, computed over 500 ms windows (value\n∈[0,127]).\n•onset density : the ratio number of note-on Midi\nmessages received over 500 ms windows, to a prac-\ntical maximumof10notes3(value ∈[0,1]).\n•pitch : the mean midi pitch of the note-on Midi\nmessages, computedover 500 ms windows (value ∈\n[0,1]).\nThe three streams of midi metadata are converted us-\ning a transfer function, and sent to the drummer which\nmodiﬁesitslocalconstraintsetaccordingly. Thedrummer\nthus generates a drumtrack by satisfying constraints cre-\nated by analysing the Midi performance. For instance, a\nnewMidienergyvaluemodiﬁesalocaldistanceconstraint\nholding on the drumbars’ energy metadata, i.e. which\nforces the energy of the newly selected drumbars to be\nas close as possible to the input midi energy. Similarly,\nlow midi pitchcan be inverseconverted,and mappedto a\nlocal constraint holding on the presence of cymbal meta-\ndata, so that melodies played on the lower octaves of the\nmidi instrument trigger drum track that use a lot of high\npitched sounds, and conversely, high pitched melodies\ntrigger a lot of bass drum and tom sounds. Mappingsbe-\ntween midi performance data and audio drumtrack meta-\ndata/constraints can be arbitrary complex. In the current\nsystem, the mappings are hardcoded, but they could be\nmodiﬁed in real-time. This issue of mapping between\n3corresponding to one note every 50 ms. Pachet (2002)\nanalysed phrases played by John McLaughlin said to be one\nof the fastest jazz guitarists and found a minimum inter onse t\ntimeof about 60milliseconds, sothisworks asa practical lo wer\nbound.\n417Midi performanceand Machinegeneratedmusic parame-\ntersisdiscussedat lengthin Rowe(1993).\nFigure 6 shows both the energy of the midi perfor-\nmance and the energy of the drumtrack generated by the\ndrummerovertime. We observethat the drummeris able\nto follow the energy of the performance very ﬁnely, with\nverysmalllatency(typicallythedurationofonedrumbar),\nandaprecisionwhichdependsontheavailableenergyval-\nuesinthedatabase.\nFigure 6: Energy of the Midi performance (solid line)\nandenergyofthedrumbarsreturnedbythesolver(dashed\nline)overtime.\n4.2 Autonomy/ReactivityTrade-off\nWhile technically satisfying, such fully reactive behav-\niour is often not suitable in a music interaction context,\ninwhichonewantstheinteractingagenttohavebothmu-\nsical realism andautonomy. Forinstance, it may be unre-\nalistic to instantly switch from very low to very high en-\nergy. In oursystem, we use globalconstraintsto counter-\nbalancetheimmediatereactivitycreatedbythelocalcon-\nstraints. Consider 2 constraints holding on the drumbars’\nenergy:\n•a local DistanceConstraint which forces the drum-\ntrack’s energy to match the midi performance’s en-\nergy(asabove)\n•a globalContinuityConstraint,whichforcesthecon-\nsecutiveselecteddrumbarsto havesimilar energy\nThese 2 constraints are contradictory: if the performance\nenergysuddenlyincreases,highlyenergeticdrumbarswill\nhave a lowcost accordingto the localdistance constraint,\nbut a high cost according to the global continuity con-\nstraint. On can manipulate the total cost of a drumbar xi\nbyputtingweightsonthelocalandglobalconstraints:\ncost(xi) =α.cost local(xi) +β.cost global (xi)(5)\nand a variety of behaviours can be achieved ranging be-\ntween complete reactivity (0-weight on the global con-\nstraint) and complete autonomy (0-weight on the local\nFigure7: Drummerbehaviourwith differentratio roflo-\ncal to global weight. (A) r=∞: the drummer reacts\nimmediately. (B) r= 2. (C)r= 1. (D)r=1\n2. (E)r= 0:\ncompleteautonomyofthedrummer.\nconstraint). Figure7showsthebehaviourofthedrummer\nsubjected to a typical Midi energy input, using different\nratio between local weight and global weight. With in-\ntermediatesettings,thedrummerfollowstheinputenergy\nwhilestillpreservingcontinuity,thusyieldingamoremu-\nsical output.\n5 ConclusionandFuture Work\nWe described a real-time drummer inspired by Music In-\nformation Retrieval techniques. It generates an audio\ndrum-track by concatenating drum segments automati-\ncallyextractedfrompre-existingmusicalﬁles. Thedrum-\ntrack can be controlled in real-time by local and global\nconstraints holding on the metadata of the drumbars, us-\ning a custom constraint satisfaction algorithm based on\nlocal search. We designed several drum track audio de-\nscriptors: drum detection, energy,onset density, presenc e\nof drums and presence of cymbals. We also reported on\nsimple experiments made in the context of a virtual duet\nbetween the system and a human pianist, and show that\nthe weights between local and global constraints can be\nmanipulatedtocontroltheautonomy/reactivityofthesys-\ntem.\nThispaperdescribesa basicmecanismofcompetitive\nlocal/globalconstraintsatisfaction,whichcanbeextend ed\nto support additional metadata, more reﬁned constraints,\nandmorecomplexinteractionmappings. Audiodrumbars\ncan be automatically indexed with more advanced rhyth-\nmic descriptors, such as syncopation, strong/weak beats,\nor style (e.g. for jazz drumming : swing, bebop, latin,\nethnic, free, etc.). More complex autonomy behaviours\ncanbeachievedwithadditionalglobalconstraints,suchas\ndistributionconstraints(e.g. “60%ofthedrumbarsshould\nuse a latin pattern”), or sequence constraints (e.g. “a ﬁll\nwith a lot of toms every 4 drumbars”). Additional infor-\nmation can be extracted from the Midi performance (no-\ntably metric analysis to match the audio). Finally, con-\n418straints and mappings can also be changed in real-time\nbasedonthemidianalysis.\n6 Acknowledgement\nThis work uses JSyn, a Java synthesis library (Burk,\n1998), and MidiShare, a real-time multi-task MIDI oper-\natingsystemdevelopedbyGRAME(OrlareyandLequay,\n1989). It has beenpartially foundedby The SemanticHiﬁ\nEuropeanIST project.\nReferences\nJean-Julien Aucouturier and Franc ¸ois Pachet. Scaling up\nmusic playlist generation systems. In Proceedings of\nTheIEEEInternationalConferenceonMultimediaand\nExpo,Lausanne(Switzerland) ,August2002.\nJean-Julien Aucouturier, Franc ¸ois Pachet, and Peter\nHanappe. From sound sampling to song sampling. In\nProceedings of the International Conference on Mu-\nsic Information Retrieval (ISMIR) , Barcelona, Spain.,\n2004.\nJeff A. Bilmes. Techniques to foster drum machine ex-\npressivity. In Proceedings of the International Com-\nputerMusic Conference ,1993.\nPaulBoersmaandDavidWeenink. Praat: doingphonetics\nbycomputer. Available: http://www.praat.org/,2005.\nPhil Burk. Jsyn, a real-time synthesis api for java.\nInProceedings of the International Computer Mu-\nsic Conference (ICMC) . ICMA, 1998. available:\nhttp://www.softsynth.com/jsyn/.\nPhilippe Codognet and Daniel Diaz. Yet another local\nsearch method for constraint solving. In Proceedings\noftheAAAIFall2001Symposium,CapeCod,MA ,No-\nvember2001.\nFXpansion. Bfd, premiumacoustic drum librarymodule,\n2003. website: http://www.fxpansion.com.\nPerfecto Herrera, Alexandre Yeterian, and Fabien\nGouyon. Automatic classiﬁcation of drum sounds: a\ncomparisonoffeatureselectionmethodsandclassiﬁca-\ntiontechniques.In ProceedingsofSecondInternational\nConference on Music and Artiﬁcial Intelligence , Edin-\nburgh,Scotland,2002.\nJohnKoza. GeneticProgramming . MITPress., 1992.\nAriLazierandPerryCook. Mosievius: Feature-drivenin-\nteractiveaudiomosaicing. In ProceedingsoftheCOST-\nG6ConferenceonDigitalAudio ,London,UK,Septem-\nber2003.\nYann Orlarey and Herve Lequay. Midishare: a real time\nmultitasks software module for midi applications. In\nProceedingsofthe1989InternationalComputerMusic\nConference, San Francisco . Computer Music Associa-\ntion,1989.\nFranc ¸ois Pachet. Rhythm as emerging structure. In Pro-\nceedings of the International Computer Music Confer-\nence,Berlin,Germany,2000.Franc ¸oisPachet. Musicinteractionwithstyle. In Proceed-\nings of the International Computer Music Conference ,\n2002.\nFranc ¸ois Pachet and Olivier Delerue. On-the-ﬂy multi-\ntrackmixing. In ProceedingsofAES109thConvention ,\nLosAngeles,USA.,2000.\nFranc ¸ois Pachet and Pierre Roy. Musical harmonization\nwith constraints: A survey. Constraints , 6(1):7–19,\n2001.\nJouni Paulus and Anssi. Klapuri. Measuring the similar-\nity of rhythmic patterns. In Proceedings, 3rd Inter-\nnational Conference on Music Information Retrieval ,\nParis,France,October2002.\nRobert Rowe. Interactive Music Systems . MIT Press,\nCambridge,Massachusetts,1993.\nEric Scheirer. Tempo and beat analysis of acoustic musi-\ncalsignals. JournaloftheAcousticSocietyofAmerica ,\n103(1):588–601,January1998.\nEricScheirerandMalcolmSlaney.Constructionandeval-\nuation of a robust multifeature speech/music discrimi-\nnator. InProceedingsof ICASSP ,1997.\nDiemo Schwarz. The caterpillar system for data-driven\nconcatenative sound synthesis. In Proceedings of the\nCOST-G6ConferenceonDigitalAudioEffects(DAFx) ,\nLondon,UK.,September2003.\nNaoTokuiandHitoshi Iba. Musiccompositionbymeans\nof interactive ga and gp. In Proceedings of IEEE Sys-\ntems, ManandCybernetics ,2001.\nToontrack. Dfh, drumkit from hell, 2003. website:\nhttp://www.toontrack.com.\nKazuyoshiYoshii,MasatakaGoto,andHiroshiG.Okuno.\nAutomaticdrumsounddescriptionforreal-worldmusic\nusing template adaptation and matching methods. In\nProceedings of the International Conference on Music\nInformationRetrieval(ISMIR) ,Barcelona,Spain,2004.\nMenno Zaanen, Rens Van Bod, and Henkjan Honing. A\nmemory-based approach to meter induction. In Pro-\nceedingsofthe ESCOM ,pages250–253.,2003.\nAymeric Zils and Franc ¸ois Pachet. Musical mosaicing.\nInProceedingsof the COST-G6 Conference on Digital\nAudio,Limerick,Ireland,December2001.\nAymeric Zils and Franc ¸ois Pachet. Extracting automati-\ncallytheperceivedintensityofmusictitles. In Proceed-\nings of the 6th COST-G6 Conference on Digital Audio\nEffects(DAFX03) ,London,UK,September2003.\nAymeric Zils and Franc ¸ois Pachet. Automatic extraction\nofmusicdescriptorsfromacousticsignalsusingeds. In\nProceedingsofthe116thAESConvention,Berlin ,May\n2004.\nAymeric Zils, Franc ¸ois Pachet, Olivier Delerue, and Fa-\nbienGouyon.Automaticextractionofdrumtracksfrom\npolyphonic music signals. In Proceedings of Web De-\nlivery ofMusic (WEDELMUSIC) ,December2002.\n419"
    },
    {
        "title": "Design of a Digital Music Stand.",
        "author": [
            "Tim Bell 0001",
            "David Blizzard",
            "Richard D. Green",
            "David Bainbridge 0001"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416290",
        "url": "https://doi.org/10.5281/zenodo.1416290",
        "ee": "https://zenodo.org/records/1416290/files/BellBGB05.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1416290,
        "dblp_key": "conf/ismir/BellBGB05"
    },
    {
        "title": "A Robust Mid-Level Representation for Harmonic Content in Music Signals.",
        "author": [
            "Juan Pablo Bello",
            "Jeremy Pickens"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417431",
        "url": "https://doi.org/10.5281/zenodo.1417431",
        "ee": "https://zenodo.org/records/1417431/files/BelloP05.pdf",
        "abstract": "When considering the problem of audio-to-audio matching, determining musical similarity using low-level features such as Fourier transforms and MFCCs is an extremely difficult task, as there is little semantic information available. Full semantic transcription of audio is an unreliable and imperfect task in the best case, an unsolved problem in the worst. To this end we propose a robust mid-level representation that incorporates both harmonic and rhythmic information, without attempting full transcription. We describe a process for creating this representation automatically, directly from multi-timbral and polyphonic music signals, with an emphasis on popular music. We also offer various evaluations of our techniques. Moreso than most approaches working from raw audio, we incorporate musical knowledge into our assumptions, our models, and our processes. Our hope is that by utilizing this notion of a musically-motivated mid-level representation we may help bridge the gap between symbolic and audio research. Keywords: Harmonic description, segmentation, music similarity 1",
        "zenodo_id": 1417431,
        "dblp_key": "conf/ismir/BelloP05",
        "keywords": [
            "Harmonic description",
            "segmentation",
            "music similarity",
            "mid-level representation",
            "harmonic and rhythmic information",
            "full semantic transcription",
            "automatic creation",
            "multi-timbral and polyphonic music signals",
            "pop music",
            "musical knowledge"
        ],
        "content": "A Robust Mid-level Representation for Harmonic Content in Music Sign als\nJuan P. Bello and Jeremy Pickens\nCentre for Digital Music\nQueen Mary, University of London\nLondon E1 4NS, UK\njuan.bello-correa@elec.qmul.ac.uk\nABSTRACT\nWhen considering the problem of audio-to-audio match-\ning, determining musical similarity using low-level fea-\ntures such as Fourier transforms and MFCCs is an ex-\ntremely difﬁcult task, as there is little semantic informa-\ntion available. Full semantic transcription of audio is an\nunreliable and imperfect task in the best case, an unsolved\nproblem in the worst. To this end we propose a robust\nmid-level representation that incorporates both harmonic\nand rhythmic information, without attempting full tran-\nscription. We describe a process for creating this represen -\ntation automatically, directly from multi-timbral and pol y-\nphonic music signals, with an emphasis on popular mu-\nsic. We also offer various evaluations of our techniques.\nMoreso than most approaches working from raw audio,\nwe incorporate musical knowledge into our assumptions,\nour models, and our processes. Our hope is that by utiliz-\ning this notion of a musically-motivated mid-level repre-\nsentation we may help bridge the gap between symbolic\nand audio research.\nKeywords: Harmonic description, segmentation, music\nsimilarity\n1 Introduction\nMid-level representations of music are measures that can\nbe computed directly from audio signals using a combi-\nnation of signal processing, machine learning and musical\nknowledge. They seek to emphasize the musical attributes\nof audio signals (e.g. chords, rhythm, instrumentation),\nattaining higher levels of semantic complexity than low-\nlevel features (e.g. spectral coefﬁcients, MFCC, etc), but\nwithout being bounded by the constraints imposed by the\nrules of music notation. Their appeal resides in their abil-\nity to provide a musically-meaningful description of audio\nsignals that can be used for music similarity applications,\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonsuch as retrieval, segmentation, classiﬁcation and brows-\ning in musical collections.\nPrevious attempts to model music from complex au-\ndio signals concentrate mostly on the attributes of timbre\nand rhythm (Aucouturier and Pachet, 2002; Yang, 2002).\nThese methods are usually limited by the simplicity of\ntheir selected feature set, which can be often regarded\nas low-level. Dixon et al. (2004) demonstrated that it is\npossible to successfully characterize music according to\nrhythm by adding higher-level descriptors to a low-level\nfeature set. These descriptors are more readily available\nfor rhythm than for harmony as the state-of-the-art in beat,\nmeter tracking and tempo estimation has had more success\nthan similar efforts on chord and melody estimation.\nPickens et al. (2002) showed success at identifying\nharmonic similarities between a polyphonic audio query\nand symbolic polyphonic scores. The approach relied on\nautomatic transcription, a process which is partially effe c-\ntive within a highly constrained subset of musical record-\nings (e.g. mono-timbral, no drums or vocals, small poly-\nphonies). To effectively retrieve despite transcription e r-\nrors, all symbolic data was converted to harmonic distri-\nbutions and similarity was measured by computing the\ndistance between two distributions over the same event\nspace. This is an inefﬁcient process that goes to the un-\nnecessary step of transcription before the construction of\nan abstract representation of the harmony of the piece.\nIn this paper we propose a method for semantically\ndescribing harmonic content directly from music signals.\nOur goal is not to do a formal harmonic analysis but to\nproduce a robust and consistent harmonic description use-\nful for similarity-based applications. We do this with-\nout attempting to estimate the pitch of notes in the mix-\nture. By avoiding the transcription step, we also avoid\nits constraints, allowing us to operate on a wide variety\nof music. The approach combines a chroma-based repre-\nsentation and a hidden Markov model (HMM) initialized\nwith musical knowledge and partially trained on the sig-\nnal data. The output, which is a function of beats (tactus)\ninstead of time, represents the sequence of major and mi-\nnor triads that describe the harmonic character of the input\nsignal.\nThe remainder of this paper is organized as follows:\nSection 2 reviews previous work on this area; Section 3\ngives details about the construction of the feature vector;\nSection 4 explains the used model and justiﬁes our ini-\n304tialization and training choices; Section 5 evaluates the\nrepresentation against a database of annotated pop music\nrecordings; Section 6 discusses the application of our rep-\nresentation to long-term segmentation; and ﬁnally, Sec-\ntion 7 presents our conclusions and directions for future\nwork.\n2 Background\nWe are by no means the ﬁrst to use either chroma-based\nrepresentations or HMMs for automatically estimating\nchords, harmony or structure from audio recordings. Pre-\nvious systems (Gomez and Herrera, 2004; Pauws, 2004)\ncorrelate chromagrams1, to be explained in 3.1, with\ncognition-inspired models of key proﬁles (Krumhansl,\n1990) to estimate the overall key of music signals. Sim-\nilarly Harte and Sandler (2005) correlate tuned chroma-\ngrams with simple chord templates for the frame-by-frame\nestimation of chords in complex signals. While differing\nin their goals, these studies identiﬁed the lack of contex-\ntual information about chord/key progressions as a weak-\nness of their approaches, as at the level of analysis frames\nthere are a number of factors (e.g. transients, arpeggios,\nornamentations) that can negatively affect the local esti-\nmation.\nIn their research on audio thumbnailing, Bartsch and\nWakeﬁeld (2001) found that the structure of a piece,\nas seen by calculating a similarity matrix, is more\nsalient when using beat-synchronous analysis of chromas.\nLonger analysis frames help to overcome the noise intro-\nduced by transients and short ornamentations. However,\nthis solution still does not make use of the fact that in a\nharmonic progression certain transitions are more likely\nto occur than others.\nAn alternative way of embedding the idea of harmonic\nprogression into the estimation is by using HMMs. The\nwork by Raphael and Stoddard (2003) is a good exam-\nple of successfully using HMMs for harmonic analysis;\nalthough their analysis is done from MIDI data, they do\nadopt beat-synchronous observation vectors.\nPerhaps the approach which is most similar to ours is\nthat proposed by Sheh and Ellis (2003) for chord estima-\ntion. In this approach an HMM is used on Pitch Class Pro-\nﬁle features (PCP) estimated from audio. Both the mod-\nels for chords (147 of them) and for chord transitions, are\nlearned from random initializations using the expectation\nmaximization (EM) algorithm. Importantly, this approach\ndiffers from ours on that no musical knowledge is explic-\nitly encoded into the model, something that, as will be\ndemonstrated in future sections, has a notable impact on\nthe robustness of the estimation. Also, our choice of fea-\nture set and use of a beat-synchronous analysis frame min-\nimizes the effect of local variations. Finally, our proposa l\ndiffers in scope, we are not trying to achieve chord tran-\nscription but to generate a robust harmonic blueprint from\naudio, and to this end we limit our chord lexicon to the\n24 major and minor triads, a symbolic alphabet that we\nconsider to be sufﬁcient for similarity-based application s.\n1also referred to as Harmonic Pitch Class Proﬁles: HPCP3 Features\nThe ﬁrst stage of our analysis is the calculation of a se-\nquence of suitable feature vectors. The process can be\ndivided into four main steps: 36-bin chromagram calcula-\ntion, chromagram tuning, beat-synchronous (tactus) seg-\nmentation and 12-bin chromagram reduction.\n3.1 Chromagram calculation\nA standard approach to modeling pitch perception is as a\nfunction of two attributes: height andchroma . Height re-\nlates to the perceived pitch increase that occurs as the fre-\nquency of a sound increases. Chroma, on the other hand,\nrelates to the perceived circularity of pitched sounds from\none octave to the other. The musical intuitiveness of the\nchroma makes it an ideal feature representation for note\nevents in music signals. A temporal sequence of chro-\nmas results in a time-frequency representation of the sig-\nnal known as chromagram.\nIn this paper we use a common method for chro-\nmagram generation known as the constant Q transform\n(Brown, 1991). It is a spectral analysis where frequency-\ndomain channels are not linearly spaced, as in DFT-based\nanalysis, but logarithmically spaced, thus closely resem-\nbling the frequency resolution of the human ear. The con-\nstant Q transform Xcqof a temporal signal x(m)can be\ncalculated as:\nXcq(k) =N(k)−1/summationdisplay\nn=0w(n,k)x(n)e−j2πfkn(1)\nwhere both, the analysis window w(k)and its length\nN(k), are functions of the bin position k. The center fre-\nquency fkof the kthbin is deﬁned according to the fre-\nquencies of the equal-tempered scale such that:\nfk= 2k/βfmin (2)\nwhere βis the number of bins per octave, thus deﬁning the\nresolution of the analysis, and fmin deﬁnes the starting\npoint of the analysis in frequency. From the constant Q\nspectrum Xcq, the chroma for a given frame can then be\ncalculated as:\nChroma (b) =M/summationdisplay\nm=0|Xcq(b+mβ)| (3)\nwhere b∈[1,β]is the chroma bin number, and Mis the\ntotal number of octaves in the constant Q spectrum. In\nthis paper, the signal is downsampled to 11025Hz, β=\n36and analysis is performed between fmin = 98 Hz and\nfmax = 5250 Hz. The resulting window length and hop\nsize are 8192 and 1024 samples respectively.\n3.2 Chromagram tuning\nReal-world recordings are often not perfectly tuned, and\nslight differences between the tuning of a piece and the\nexpected position of energy peaks in the chroma represen-\ntation can have an important inﬂuence on the estimation of\nchords.\n305frame chroma\nCDEFGAB\ntactus chroma\nCDEFGAB\nD E G D A Bmtrue\nDBm DA E G BmD A Bmest1\n56 57 58 59 60 61 62 63 64 65 66D A E G D A Bmest2\ntime (s)\nFigure 1: Frame and tactus-based feature vectors for Eight days a week by The Beatles. At the bottom the estimated chord\nlabels can be observed: “true” corresponds to the ground-tr uth chord annotation, “est1” corresponds to the chord label s\nestimated using frame-based features, and “est2” correspo nds to the chords estimated using tactus-based features.\nThe 36-bin per octave resolution is intended to clearly\nmap spectral components to a particular semitone regard-\nless of the tuning of the recording. Each note in the octave\nis mapped by 3 bins in the chroma, such that bias towards\na particular bin (i.e. sharpening or ﬂattening of notes in\nthe recording) can be spotted and corrected. To do this we\nuse a simpler version of the tuning algorithm proposed by\nHarte and Sandler (2005). The algorithm starts by pick-\ning all peaks in the chromagram. Resulting peak positions\nare quadratically interpolated and mapped to the [1.5,3.5]\nrange. A histogram is generated with this data, such that\nskewness in the distribution is indicative of a particular\ntuning. A corrective factor is calculated from the distribu -\ntion and applied to the chromagram by means of a circular\nshift. Finally, the tuned chromagram is low-pass ﬁltered\nto eliminate sharp edges.\n3.3 Beat-synchronous segmentation\nAs mentioned before, beat-synchronous analysis of the\nsignal helps to overcome the problems caused by transient\ncomponents in the sound, e.g. drums and guitar strum-\nming, and short ornamentations, often introduced by vo-\ncals. Both these cases are quite common in pop music\nrecordings, hence the relevance of this processing step.\nFurthermore, harmonic changes often occur at a longer\ntime span than that deﬁned by the constant Q analysis,\nthus the default temporal resolution results unnecessary\nand often detrimental.\nIn our approach we use the beat tracking algorithm\nproposed by Davies and Plumbley (2005). This method\nhas proven successful for a wide variety of signals. Using\nbeat-synchronous segments has the added advantage that\nthe resulting representation is a function of beat, or “tac-\ntus”, rather than time. These facilitates comparison with\nsongs in different tempos.3.4 Observation Vectors\nFinally, the chromagram is averaged within beat segments\nand further reduced from 36 to 12 bins by simply sum-\nming within semitones. A piece of music is thus repre-\nsented as a sequence of these 12 dimensional vectors.\n4 Chord Labeling\nLet us turn our attention to the chord labeling of the\nchroma sequence. Recall, however, that our goal is not\ntrue harmonic analysis, but a mid-level representation\nwhich we believe will be useful for music similarity and\nmusic retrieval tasks. For this we apply the HMM frame-\nwork (Rabiner, 1989). As mentioned in section 2, we are\nnot the ﬁrst to use this framework, but we utilize it in a\nrelatively new way, based largely on music theoretic con-\nsiderations.\n4.1 Chord lexicon\nThe ﬁrst step in labeling the observations in a data stream\nis to establish the lexicon of labels that will be used. We\ndeﬁne a lexical chord as a pitch template. Of the 12\noctave-equivalent (mod 12) pitches in the Western canon,\nwe select some n-sized subset of those, call the subset a\nchord , give that chord a name, and add it to the lexicon.\nNot all possible chords belong in a lexicon and we must\ntherefore restrict ourselves to a musically-sensible subs et.\nThe chord lexicon used in this work is the set of 24 major\nand minor triads, one each for all 12 members of the chro-\nmatic scale: C Major, c minor, C ♯Major, c ♯minor . . . B ♭\nMajor, b ♭minor, B Major, b minor. Assuming octave-\ninvariance, the three members of a major triad have the\nrelative semitone values n,n+ 4 andn+ 7; those of a\nminor triad n,n+ 3 andn+ 7. No distinction is made\nbetween enharmonic equivalents (C ♯/D♭, A♯/B♭, etc.).\n306(a) (b)\n(c) (d)\nFigure 2: State-transition distribution A: (a) initializa-\ntion of Ausing the circle of ﬁfths, (b) trained on Another\nCrossroads (M. Chapman), (c) trained on Eight days a\nweek (The Beatles), and (d) trained on Love me do (The\nBeatles). All axes represent the 24 lexical chords (C →B\nthen c →b)\nWe have chosen a rather narrow space of chords. We\ndid not include dyads nor other more complex chords such\nas augmented, diminished, 7thor9thchords. Our intu-\nition is that by including too many chords, both complex\nand simple, we run the risk of “overﬁtting” our models\nto a particular piece of music. As a quick thought exper-\niment, imagine if the set of chords were simply the en-\ntire/summationtext\nn=1..12/parenleftbig12\nn/parenrightbig\n= 212−1possible combinations of 12\nnotes. Then the set of chord labels would be equivalent\nto the set of 12-bin chroma and one would not gain any\ninsight into the harmonic “substance” of a piece, as each\nobservation would likely be labeled with itself. This is\nan extreme example but it illustrates the intuition that the\nricher the lexical chord set becomes, the more our feature\nselection algorithms might overﬁt one piece of music and\nnot be useful for the task of determining music similarity.\nWhile it is clear that the harmony of only the crudest\nmusic can be reduced to a mere succession of major and\nminor triads, as this choice of lexicon might be thought to\nassume, we believe that this is a sound basis for a proba-\nbilistic approach to labeling. In other words, the lexicon i s\na robust mid-level representation of the salient harmonic\ncharacteristics of many types of music, notably popular\nmusic.\n4.2 HMM initialization\nIn this paper we are not going to cover the basics of hid-\nden Markov modeling. This is far better covered in works\nsuch as (Rabiner, 1989) and even by previous music HMM\npapers cited above. Instead, we begin by describing the\ninitialization procedure for the model. As labeled trainin g\ndata is difﬁcult to come by, we forgo supervised learning\nand instead use the unsupervised mechanics of HMMs for\nparameter estimation. However, with unsupervised train-\ning it is crucial that one start the model off in a reason-able state, so that the patterns it learns correspond with\nthe states over which one is trying to do inference.\n4.2.1 Initial state distribution [ π]\nOur estimate of πis1\n24for each of the 24 states in the\nmodel. We have no reason to prefer, a priori, any state\nabove any other.\n4.2.2 State transition matrix [ A]\nPrior to observing an actual piece of music we also do not\nknow what states are more likely to follow other states.\nHowever, this is where a bit of musical knowledge is use-\nful. In a song, we might not yet know whether a C major\ntriad is more often followed by a B♭major or a D ma-\njor. But it is reasonable to assume that both hypotheses\nare more likely than an F♯major. Most music tends not to\nmake large, quick harmonic shifts. One might gradually\nwander from the C to the F♯, but not immediately. We use\nthis notion to initialize our state transition matrix.\nCaFdBb\ngEb\ncAb\nfC#\nbb\nF#\neb\nBab\nEc#\nAf#\nDbGe\nThe ﬁgure above is a doubly-nested circle of ﬁfths,\nwith the minor triads (lower case) staggered throughout\nthe major triads (upper case). Triads closer to each other\non the circle are more consonant, and thus receive higher\ninitial transition probability mass than triads further aw ay.\nSpeciﬁcally, the transition C →C is given a probability\n12+ǫ\n144+24 ǫ, where ǫis a small smoothing constant, C →e =\n11+ǫ\n144+24 ǫand then clockwise in a decreasing manner, un-\ntil C→F♯=0+ǫ\n144+24 ǫ. At that point, the probabilities be-\ngin increasing again, with C →b♭=1+ǫ\n144+24 ǫand C →a =\n11+ǫ\n144+24 ǫ.\nThe entire 24 ×24 transition matrix, as seen in Figure\n2(a), is constructed in a similar manner for every state,\nwith a state’s transition to itself receiving the highest in i-\ntial probability estimate, and the remaining transitions r e-\nceiving probability mass relative to their distance around\nthe 24-element circle above.\n4.2.3 Observation (output) distribution [ B]\nEach state in the model generates, with some probability,\nan observation vector. We assume a continuous observa-\ntion distribution function modeled using a single multi-\nvariate Gaussian for each state, each with mean vector µ\nand covariance matrix Σ.\nSheh and Ellis (2003) use random initialization of µ\nand a Σcovariance matrix with all off diagonal elements\nset to 0, reﬂecting their assumption of completely uncor-\nrelated features. We wish to avoid this assumption. One\nof the main purposes of this paper is to argue that musical\n307(a) (b)\n(c) (d)\nFigure 3: Initializations for µandΣ. Top-left is µfor\nall states (a). Then for a C major chord: diag-only (b),\nweighted-diag (c), and off-diag(d) initializations of Σ.\nThexaxis in (a) corresponds to the 24 lexical chords. All\nother axes refer to the 12 notes in the chroma circle.\nknowledge needs to play an important role in music infor-\nmation retrieval tasks. Thus if we are using triads as our\nhidden state labels, µandΣshould reﬂect this fact.\nLet us take for example the C major triad state. Instead\nof initializing µrandomly, we initialize it to 1.0 in the C,\nE, and G dimensions, and 0.0 elsewhere. This reﬂects the\nfact that the triad is grounded in those dimensions. Initial -\nizations of µfor all states can be seen in Fig. 3(a).\nThe covariance matrix should also reﬂect our musi-\ncal knowledge. Covariance is a measure of the extent to\nwhich two variables move up or down together. Thus, for\na C major triad, it is reasonable that pitches which com-\nprise the triad are more correlated than pitches which do\nnot belong to the triad. Naturally, the pitches C, E, and\nG are strongly correlated with themselves. Furthermore,\nthese pitches are also strongly correlated with each other.\nWe symmetrically use the knowledge, gained both from\nmusic theory as well as empirical evidence (Krumhansl,\n1990), that the dominant is more important than the medi-\nant in characterizing the root of the triad. We set the co-\nvariance of the tonic with the dominant to 0.8, the mediant\nwith the dominant to 0.8, and the tonic with the mediant\nto 0.6. The actual values are heuristic, but the principle\nwe use to set them is not.\nThe remainder of covariances in the matrix are set to\nzero, reﬂecting the fact that from the perspective of a C\nmajor triad there is little useful correlation between, say ,\nan F♯and an A♯. The non-triad member diagonals are\nset to 0.2 both to indicate that non-triad pitches need not\nbe as strongly self-correlated, as well as to insure that the\nmatrix is positive, semi-deﬁnite. Figure 3(d) shows the\ncovariance matrix used for the C major triad state.\nThe covariance for C minor is constructed almost ex-\nactly the same way, but with the mediant on D♯/E♭rather\nthan on E, as would be expected. The remainder of the\nmatrices for all the states are constructed by circularly\nshifting the major/minor matrix by the appropriate num-ber of semitones.\n4.3 HMM Training\nA key difference between our approach and previous sys-\ntems is our use of musical knowledge for model initializa-\ntion. There are two important pieces of information that\nwe are providing the system: a template for every chord\nin the lexicon, as given by µandΣ, and cognitive-based\nknowledge about likely chord progressions, as given by\nthe state transition probability matrix A.\nIt is relatively safe to say that the template for a chord\nis almost universal, e.g. a C major triad is always sup-\nposed to have the notes C, E and G. If we were to change\nour chord models from song to song we cannot longer\nassume that a certain state will always map to the same\nmajor or minor triad. Our labels would not have univer-\nsal value. Furthermore, it is very unlikely that all chords\nin our lexicon will be present in any given song (or on\nany reasonably sized training set), and in training, this\nsituation gives rise to the undesirable effect of different\ninstances of existing chords being mapped to different\n(available) states, usually those that are initialized clo sely,\ne.g. relative and parallel minors and majors.\nOn the other hand, chord progressions are not univer-\nsal, changing from song to song depending on style, com-\nposer, etc. Our initial state transition probability matri x\nprovides a reference, founded in music cognition and the-\nory, on how certain chord transitions are likely to occur in\nmost western tonal music, especially pop music. We be-\nlieve that this knowledge captures the a-priori harmonic\nintuition of a human listener. However, we want to pro-\nvide the system with the adaptability to develop models\nfor the particular chord progression of a given piece (see\nFig. 2), much as people do when exposed to a piece of\nmusic they have never heard before.\nWe therefore propose selectively training our model\nusing the standard expectation maximization (EM) algo-\nrithm for HMM parameter estimation (Rabiner, 1989),\nsuch that we disallow adjustment of B={µ,Σ}, while\nπandAare updated as normal. We believe this kind of\nselective training to provide a good trade-off between the\nneed for a stable reference for chords, and a ﬂexible, yet\nprincipled, modeling of chord sequences.\n4.4 Chord Labeling (Inference)\nOnce we have both a trained model and an observation se-\nquence, we can apply standard inference techniques (Ra-\nbiner, 1989) to label the observations with chords from our\nlexicon. The idea is that there are many sequences of hid-\nden states that could have been responsible for generating\nthe chroma vector observation sequence.\nThe goal is to ﬁnd that sequence that maximizes the\nlikelihood of the data without having to enumerate the\nexponentially many ( 24n, for a sequence of length n, in\nour model) number of sequences. To this end a dynamic\nprogramming algorithm known as Viterbi is used (Forney,\n1973). This algorithm is well covered in the literature and\nwe do not add any details here.\n308Parameters TP %\nFeature π A B Training CD1 CD2 TOTAL\nscope µ Σ\ntactus1\n24random template diag-only π,A,B 22.88 29.83 26.36\ntactus1\n24random template weighted-diag π,A,B 34.14 36.24 35.19\ntactus1\n24random template off-diag π,A,B 33.13 44.36 38.74\ntactus1\n24circle of 5thstemplate off-diag π,A,B 38.09 47.75 42.93\nframe1\n24circle of 5thstemplate off-diag π,A 58.96 74.78 66.87\ntactus1\n24circle of 5thstemplate off-diag π,A 68.55 81.54 75.04\nFigure 4: Results for various model parameters\n5 Evaluation and Analysis\nIn summary, our system, for a single piece of music, is:\n1. Compute the 36-bin chromagram for the music piece.\n2. Tune the chromagrams (globally) to remove slight\nsharpness or ﬂatness and avoid energy leaking from\none pitch class into another\n3. Segment the signal frames into tactus-sized win-\ndows, average the chroma within each window, and\nﬁnally reduce each chroma from 36 to 12 bins by\nsumming all three bins for each pitch class\n4. Selectively train the HMM to get a sense of the har-\nmonic movement of the piece\n5. Decode the HMM (do inference) to give a good mid-\nlevel harmonic characterization of the piece\nDespite our stated goal of harmonic description rather\nthan analysis, we found that it is still useful to attempt\nquantitative evaluation of the goodness of our represen-\ntation by comparing the generated labels to an anno-\ntated collection of music. We use the test set proposed\nand annotated by Harte and Sandler (2005). It contains\n28 recordings (mono, fs= 44.1kHz ) from the Beatles\nalbums: Please Please Me (CD1) and Beatles for Sale\n(CD2). Note that all recordings are polyphonic and multi-\ninstrumental containing drums and (multi-part) vocals.\nThe majority of chords (89.51%) in the manually la-\nbeled test set belong to our proposed lexicon of major and\nminor triads. However, the set also contains more com-\nplex chords such as major and minor 6ths,7thsand9ths.\nFor simplicity, we map any complex chord to its root triad,\nso for example C#m7sus4 becomes simply C#m. If any-\nthing, this mapping has the effect of overly penalizing our\nresults, as chords of 4 or more notes could contain triads\nother than its root triad, e.g. Fm7 (F, G#, C, D#) has 100%\noverlap with G# (G#, C, D#) and Fm (F, G#, C). Compar-\nisons are made on a frame-by-frame basis, such that a true\npositive is deﬁned as a one-to-one match between estima-\ntion and annotation.\nTo quantitatively demonstrate some of the hypotheses\nput forward on this paper, we evaluate a series of incre-\nmental improvements to our approach. Figure 4 shows the\nmodel parameters for each experiment and its correspond-\ning results for the test set (in percentage of true positives ).\nResults are presented per CD and in total. The considered\nmodel parameters are:•Feature scope: Whether it is a frame-by-frame\n(time-based) or a beat-synchronous (tactus-based)\nchroma feature set.\n•Initialization of A: Whether it is randomly initial-\nized or initialized according to the circle of ﬁfths.\n•Initialization of B: Whether Σis initialized as a di-\nagonal matrix with elements equal to 1.0 (diag-only,\nFig. 3(b)), whether it is the diagonal with weighted\ntriad elements, as in Fig. 3(c), and off-diagonal el-\nements set to 0.0 (weighted-diag), or whether it in-\ncludes the mediant and dominant off-diagonal ele-\nments, i.e. the Fig. 3(d) matrix (off-diag).\n•Training: Whether π,AandBare updated in the\nexpectation-maximization step of HMM training or\nwhether Bis left ﬁxed and only πandAare adjusted.\nResults in Figure 4 clearly support the choices made\nin this paper. The ﬁrst three rows show how initializing Σ\nwith a weighted diagonal and off-diagonal elements out-\nperforms diagonal-only initializations. This supports th e\nview that the feature set is highly correlated along the di-\nmensions of the elements of a chord. The weighted diag-\nonal in itself introduces a noticeable amount of improve-\nment over the unitary diagonal, a further indication of the\nstrong correlation between the tonic, mediant and domi-\nnant of a chord.\nThe initialization of Ausing the circle of ﬁfths brings\nabout more than 10% relative improvement when com-\npared to the random initialization. This shows how the\nuse of musical knowledge is crucial. .\nFrom the analysis of the last two rows in Figure 4\ntwo more observations can be made. The ﬁrst is that se-\nlective training introduces considerable beneﬁts into our\napproach. The huge accuracy increase (from 42.93%\nto 75.04%) supports the view that the knowledge about\nchords encoded in Bis universal, and as such it should\nnot be modiﬁed during training. This accuracy increase\noccurs for every song, showing the generality of this as-\nsertion.\nThe second observation is that the use of a tactus-\nbased feature set clearly outperforms the frame-by-frame\nestimation. This point is further illustrated by the chord\nestimation example in Fig. 1, where the frame-by-frame\nestimation is subject to small variations due to phrasing or\nornamentation (as shown by the spurious estimations of\nB minor chords between 56 and 60.5 seconds), while the\n30920 40 60 80 100 120CEmGBmDF#mAC#mchords\nsegmentation\ntime (s)20 40 60 80 100 120B\nABmDF#mAC#mEG#mBD#mchords\n    A   B    A   B   C    A   B   C    Aseg1verse trans chorus verse trans chorus insttrue\n50 100 150 200  A   B   C   A   B   C   Dseg2\ntime (s)\nFigure 5: (left) Love me do by The Beatles: estimated chord sequence (top) and estimate d segments, showing the long-\nterm structure “ABABA”\nFigure 6: (right) Estimated chord sequence (top) and long-t erm segment boundaries from Wonderwall by Oasis: “true”\nrefers to ground-truth annotation, “seg1” to segments obta ined using our raw chord label sequence and “seg2” to segment s\nobtained by collapsing our chord label sequence into a simpl e chord sequence by removing contiguous duplicates\ntactus-based estimation shows more stability and, there-\nfore, accuracy when compared to the ground-truth annota-\ntion. Furthermore, chord changes are more likely to occur\non the beat, thus chords detected using the tactus-based\nfeature set tend also to be better localized.\nOur results compare favorably to those reported by\nSheh and Ellis (2003) and Harte and Sandler (2005). The\nmaximum true positives rate in the collection is 90.86%\nforEight days a week . Conversely, the worst estimation\nis for Love me do , with only 49.27% of chords correctly\nidentiﬁed. For the latter case almost all errors are due to\nrelative minor confusions: C being confused with E minor\nconsistently through the song. As we will see in the next\nsection the consistency of the representation, even when\nwrong, can be useful for certain applications.\n6 Application to Segmentation\nTo show the applicability of our chord labels to long-term\nsegmentation of songs we use a histogram clustering al-\ngorithm developed by Abdallah et al. (2005). The algo-\nrithm calculates a sequence of unlabeled states (e.g. A\nand B) that represent the long-term sections of a song\n(e.g. chorus, verse, bridge, etc) from a sequence of his-\ntograms computed from our labeled sequence. It consists\nof a phase of simulated annealing to learn the state transi-\ntion probability matrix (Puzicha et al., 1999) and a second\nphase of combined annealing and Gibbs sampling to com-\npute the posterior probabilities of segments belonging to\ngiven states, and thus the sequence of states. See (Robert\nand Casella, 1999) for an introduction.\nThe top plot of Fig. 5 shows the resulting chord label-\ning for Love me do , the song on which our labeling per-\nformed the worst. The bottom plot shows, for each time\nstep, the marginal posterior probabilities obtained from\nthe segmentation algorithm, such that white indicates zero\nprobability and black indicates a probability of 1. Fromboth these plots we can clearly see the simple structure of\nthe song, of the form “ABABA”. This demonstrates how,\neven when imperfect, our representation is consistent, al-\nlowing for successful clustering of its symbols. To our\nknowledge, this success is the ﬁrst example of long-term\nsegmentation using a mid-level harmonic feature set.\nFigure 6 shows segmentation results for a more com-\nplicated structure, that of “Wonderwall” by Oasis. The\ntop plot shows our calculated sequence of chord labels\n(“chords”). The next line (“true”) shows the manually an-\nnotated segments of the song. The middle line depicts\nthe automatically segmented sections using our chord la-\nbels (“seg1”). Finally, the bottom line (“seg2”) shows\nthe automatically segmented sections obtained after ﬁrst\ncollapsing our tactus-based chord labels (e.g. CCG-\nGFFFEAAAA) into a simple sequence of chords (e.g.\nCGFEA) by removing contiguous duplicates.\nAs can be seen in “seg1”, there are some problems\nwith the segmentation: the verse is segmented as to in-\nclude parts of the transition, the chorus section and a ﬁ-\nnal instrumental Coda , creating some confusion between\nthem, and thus resulting in errors. On the other hand, seg-\nmentation on the collapsed chord sequence is more accu-\nrate, both in terms of temporal localization and segrega-\ntion between states. We suggest that this is because the\nresulting chord groupings can be thought of as equivalent\nto musical phrases. Indeed, some informal testing seems\nto support the idea that when the number of segmenta-\ntion states is increased and the length of our histograms is\nreduced, we start to pick up segments that are related to\nsections at a shorter temporal scale (e.g. phrases). While\na proper study on segmentation is beyond the scope of\nthis paper, we suggest that this increased granularity is\npotentially a major asset of harmonic-based segmentation,\nin opposition to timbre-based segmentation, where short-\nterm structures are not necessarily indicative of musical\ngestures.\n3107 Conclusion\nThe main contribution of this work is the creation of an\neffective mid-level representation for music audio signal s.\nWe have shown that by considering the inherent musical-\nity of audio signals one achieves results far greater than\nraw signal processing and machine learning techniques\nalone (Figure 4). Our hope is that these ideas and their re-\nsults will encourage those in the ﬁeld working on raw au-\ndio to build more musicality into their techniques. At the\nsame time, we hope it also encourages those working on\nthe symbolic side of music retrieval to aide in the creation\nof additional musically sensible mid-level representatio ns\nwithout undue concern over whether such representations\nstrictly adhere to formal music theory guidelines.\nIn support of this goal, we have integrated into a single\nframework a number of state-of-the-art music processing\nalgorithms. Speciﬁcally, we build our algorithms upon a\nmusical foundation in the following ways: (1) The audio\nsignal is segmented into tactus windows rather than time-\nbased frames. (2) Pitch chroma are tuned. (3) A lexicon\nof 24 triads is used, which is neither too speciﬁc or too\ngeneral, in an attempt to describe harmonic movement in\na piece rather than doing a formal harmonic analysis. (4)\nInitialization of the machine learning (HMM) algorithm\nis done in a manner that respects the dependency between\ntonic, mediant, and dominant pitches in a triad, as well as\nthe consonance between neighboring triads in a sequence.\nFinally, (5) the machine learning algorithm itself is mod-\niﬁed with an eye toward musicality; updates to model pa-\nrameters are done so as to maintain the relationship be-\ntween pitches in a chord, but be amenable to changing\nchord transitions in a sequence.\nIn the future we are planning a series of audio-to-audio\nmusic retrieval experiments to further show the validity\nof our approach. We will also continue to develop and\nintegrate techniques that emphasize the musical nature of\nthe underlying source. We believe that this mindset is vital\nto continuing development in the ﬁeld.\n8 Acknowledgments\nThe authors wish to thank Chris Harte, Matthew Davies,\nKaty Noland and Samer Abdallah for making their code\navailable. We also wish to thank Geraint Wiggins and\nChristopher Raphael for their insights regarding the train -\ning and music-based initializations of HMMs. This\nwork was partially funded by the European Commission\nthrough the SIMAC project IST-FP6-507142.\nReferences\nS. Abdallah, K. Noland, M. Sandler, M. Casey, and\nC. Roads. Theory and evaluation of a Bayesian mu-\nsic structure extractor. In Proceedings of the 6th ISMIR\nConference, London, UK , 2005.\nJ.-J. Aucouturier and F. Pachet. Music similarity mea-\nsures: What’s the use? In Proceedings of the 3rd IS-\nMIR, Paris, France. , pages 157–163, 2002.\nM. A. Bartsch and G. H. Wakeﬁeld. To catch a chorus:\nUsing chroma-based representations for audio thumb-nailing. In Proceedings of the IEEE Workshop on Ap-\nplications of Signal Processing to Audio and Acoustics.\nNew Paltz, NY, , pages 15–18, 2001.\nJ. Brown. Calculation of a constant Q spectral transform.\nJournal of the Acoustical Society of America, , 89(1):\n425–434, 1991.\nM. E. P. Davies and M. D. Plumbley. Beat tracking with a\ntwo state model. In Proceedings of the 2005 IEEE Inter-\nnational Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP), Philadelphia, Penn., USA , pages\n241–244, 2005.\nS. Dixon, F. Gouyon, and Gerhard Widmer. Towards char-\nacterisation of music vian rhythmic patterns. In Pro-\nceedings of the 5th ISMIR, Barcelona, Spain. , pages\n509–516, 2004.\nG. D. Forney. The viterbi algorithm. Proc. IEEE , 61:268–\n278, 1973.\nE. Gomez and P. Herrera. Estimating the tonality of poly-\nphonic audio ﬁles: Cognitive versus machine learning\nmodelling strategies. In Proceedings of the 5th ISMIR,\nBarcelona, Spain. , pages 92–95, 2004.\nC. A. Harte and M. B. Sandler. Automatic chord identiﬁ-\ncation using a quantised chromagram. In Proceedings\nof the 118th Convention of the Audio Engineering Soci-\nety, Barcelona, Spain, May 28-31 2005.\nC. L. Krumhansl. Cognitive Foundations of Musical Pitch .\nOxford University Press, New York, 1990.\nS. Pauws. Musical key extraction from audio. In Proceed-\nings of the 5th ISMIR, Barcelona, Spain. , pages 96–99,\n2004.\nJ. Pickens, J. P. Bello, G. Monti, T. Crawford, M. Dovey,\nM. Sandler, and D. Byrd. Polyphonic score retrieval\nusing polyphonic audio queries: A harmonic modeling\napproach. In Proceedings of the 3rd ISMIR , pages 140–\n149, Paris, France, October 2002.\nJ. Puzicha, J. M. Buhmann, and T. Hofmann. Histogram\nclustering for unsupervised image segmentation. In\nProceedings of IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR), Ft.\nCollins, CO, USA , pages 2602–2608, 1999.\nL. R. Rabiner. A tutorial on HMM and selected applica-\ntions in speech recognition. Proceedings of the IEEE ,\n77(2):257–286, 1989.\nC. Raphael and J. Stoddard. Harmonic analysis with prob-\nabilistic graphical models. In Proceedings of the 4th\nISMIR , pages 177–181, Baltimore, Maryland, October\n2003.\nC. P. Robert and G. Casella. Monte Carlo Statistical Meth-\nods. Springer, New York, 1999.\nA. Sheh and D. P. W. Ellis. Chord segmentation and recog-\nnition using em-trained hidden markov models. In Pro-\nceedings of the 4th ISMIR , pages 183–189, Baltimore,\nMaryland, October 2003.\nC. Yang. MACSIS: A scalable acoustic index for content-\nbased music retrieval. In Proceedings of the 3rd ISMIR,\nParis, France. , pages 53–62, 2002.\n311"
    },
    {
        "title": "Scalable Metadata and Quick Retrieval of Audio Signals.",
        "author": [
            "Nancy Bertin",
            "Alain de Cheveigné"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417079",
        "url": "https://doi.org/10.5281/zenodo.1417079",
        "ee": "https://zenodo.org/records/1417079/files/BertinC05.pdf",
        "abstract": "Audio search algorithms have reached a degree of speed and accuracy that allows them to search efficiently within large databases of audio. For speed, algorithms generally depend on precalculated indexing metadata. Unfortunately, the size of the metadata follows the same exponential trend as the audio data itself, and this may lead to an exponential increase in storage cost and search time. The concept of scalable metadata has been introduced to allow metadata to adjust to such trends and alleviate the effects of forseeable increases of data and metadata size. Here, we argue that scalability fits the needs of the hierarchical structures that allow fast search, and illustrate this by adapting a state-of-the-art search algorithm to a scalable indexing structure. Scalability allows search algorithms to adapt to the increase of database size without loss of performance. Keywords: Search, indexing, scalability, audio retrieval, scalable metadata. 1",
        "zenodo_id": 1417079,
        "dblp_key": "conf/ismir/BertinC05",
        "keywords": [
            "search algorithms",
            "speed",
            "accuracy",
            "audio databases",
            "metadata",
            "exponential trend",
            "storage cost",
            "search time",
            "scalable metadata",
            "search performance"
        ],
        "content": "SCALABLE METADATA AND QUICK RETRIEVAL OF AUDIO SIGNALS\nNancy Bertin, Alain de Cheveign ´e\nEquipe Audition\nCNRS UMR 8581 - ENS (DEC)\n29, rue d’Ulm\n75005 PARIS\nNancy.Bertin@ens.fr, Alain.de.Cheveigne@ens.fr\nABSTRACT\nAudio search algorithms have reached a degree of speed\nand accuracy that allows them to search efﬁciently within\nlarge databases of audio. For speed, algorithms gener-\nally depend on precalculated indexing metadata. Unfor-\ntunately, the size of the metadata follows the same expo-\nnential trend as the audio data itself, and this may lead to\nan exponential increase in storage cost and search time.\nThe concept of scalable metadata has been introduced to\nallow metadata to adjust to such trends and alleviate the\neffects of forseeable increases of data and metadata size.\nHere, we argue that scalability ﬁts the needs of the hi-\nerarchical structures that allow fast search, and illustrate\nthis by adapting a state-of-the-art search algorithm to a\nscalable indexing structure. Scalability allows search al-\ngorithms to adapt to the increase of database size without\nloss of performance.\nKeywords: Search, indexing, scalability, audio re-\ntrieval, scalable metadata.\n1 INTRODUCTION\nThis paper investigates the usefulness of scalable content-\nbased metadata for quick retrieval of music and audio\ndata. In agreement with the well-known law of Moore\nand its avatars (Odlyzko, 1999), we observe an exponen-\ntial growth of multimedia content available on the web,\ncable networks, DVDs, hard disks, etc. This constitutes\na problem for those who manipulate the content (produc-\ners and consumers alike), and also for the algorithms and\ntools that deal with such data. The concept of metadata\nwas introduced as an attempt to alleviate problems associ-\nated with manipulating large quantities of data. Metadata\nare designed to summarize data in a format that is com-\npact so as to minimize storage costs, and optimized for\nmanipulations such as search and retrieval. Metadata are\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonhowever likely to grow at the same rate as the data itself.\nGrowth of metadata may be addressed by designing new\nmetadata formats, either more compact descriptions of the\ndata, or else second-order descriptions of the metadata.\nHowever new formats introduce interoperability problems\nas well as ergonomy problems for the user who must learn\nnew tools to deal with these new formats.\nThe concept of scalable metadata was introduced to\naddress these issues (de Cheveign ´e, 2002). A deﬁnition\nof scalability is given in Sect. 2; in brief, metadata are\nscalable if they have the same semantics at all scales, can\nbe converted from high- to low-resolution scale (rescaled),\nand do not depend on the intervening rescaling operations.\nScalable metadata can thus be adjusted to ﬁt databases of\nany size, and the needs of any application, while retaining\nconsistent semantics across scale. Metadata storage for-\nmats, and tools to manipulate them, thus remain the same\nwhatever the future increases in data size. Arguably, meta-\ndata must be scalable to follow future increases in data size\nand so non-scalable formats will eventually be superseded\n(de Cheveign ´e, 2002).\nA crucial function in any system that handles large\nquantities of data is search . Search is needed for query\noperations, but also for signal processing, display, house-\nkeeping, etc. Without adequate attention to efﬁciency and\nasymptotic properties, the cost of search operations can\nvery easily grow out of bounds. For example a search\ntime linear in size is obviously prohibitive if size increases\nexponentially. Efﬁcient algorithms have been developed\nfor e.g. strings or trees, but relatively fewer are effective\nfor multimedia data. Examples are (Luettgen and Willsky,\n1995; Spence and Parra, 2000; Jain et al., 1999; Crestani\net al., 1998; Kashino et al., 1999, 2003). Efﬁcient search\nmethods usually involve construction of an index struc-\nture within which search proceeds. The indices can be\nunderstood as metadata, and this raises the issue of their\nscalability. It also raises the question of the usefulness of\nmetadata for search in general. For all practical purposes,\nsearch within a multimedia database is search within the\nmetadata associated with the content. The effectiveness of\nscalable metadata as a substrate for search is an essential\nquestion.\nHere, we start from a state-of-the-art audio search al-\ngorithm (Kashino et al., 1999, 2003) and show that it can\nbe adapted to take advantage of scalable metadata. Scal-\nability eases the implementation of a hierarchical meta-\n238data structure that speeds search. Efﬁcient search in-\nvolves pruning of the search space at the earliest possi-\nble stage. The statistical operations involved in the deﬁ-\nnition of scalable metadata (extrema, mean, variance, his-\ntogram, etc.) are effective for this purpose, as these statis-\ntics allow inference of the presence (or better: the ab-\nsence) of a search token within the subset of data that\nthey summarize. Such statistics have been exploited previ-\nously (Gaede and G ¨unther, 1998; Spence and Parra, 2000;\nSivakumaran et al., 2001) for search within audio or image\ndomains.\nThe notion of scalability is deﬁned in the next section.\nPrinciples of efﬁcient search are outlined in Sect. 3. Ex-\nperiments to test the effectiveness of hierarchical search\nare described in Sect. 4.\n2 SCALABLE METADATA\n2.1 Properties and design\nThree properties characterize scalable metadata. First,\nthey may be instantiated at any resolution. Second, an\nexisting description may be converted automatically to a\nlower-resolution one. Third, the description at a given res-\nolution depends only on the resolution, and not on the his-\ntory of scaling operations that led to it. Scaling entails loss\nof information: low resolution metadata contain less infor-\nmation than high resolution, and conversion from lower to\nhigher resolution is not possible. Lower resolutions are\nusually imposed by application or system-dependent stor-\nage or processing constraints. The scalability property al-\nlows low and high resolution metadata to be compared,\nthus ensuring interoperablity.\nScalability is provided by a set of well-deﬁned opera-\ntions that transform series of numerical descriptor values\nfrom one resolution to the next. Among them are: ex-\ntrema ( minandmax),sum,mean andhistogram .Vari-\nance andcovariance are also scalable if associated with\nthe mean. Others are described in (de Cheveign ´e, 2002;\nde Cheveign ´e and Peeters, 1999a; ISO/IEC JTC 1/SC 29,\n2001), in particular a “scalewise variance” that measures\nvariance across scale, as well as “weighted” versions of\nthe above operations. Weighted operations allow certain\nsamples to be discounted before scaling, for example if\nthey are known to be locally undeﬁned or unreliable. A\nscalable data structure for metadata consists of the meta-\ndata (represented as a series of scalars, vectors or matri-\nces) together with information that describes its “geom-\netry”: name of the scaling operation that was (or should\nbe) applied to it, number of samples, scale ratio, etc. The\nstructure contains all that is needed by an application to\nmake sense of the data. It also contains all that that is\nneeded to rescale it. Metadata storage or transfer systems\ncan thus adjust to the resolution according to storage ca-\npacity or bandwidth constraints.\nThe scalable structure is designed to be “plugged in”\nto a content-based descriptor, to serve as a container for\nthe series of descriptor values. Typically, an audio de-\nscriptor such as spectrum or power is calculated from au-\ndio content and then stored with on one hand descriptor-\nspeciﬁc parameters (frame rate, spectral resolution, etc.)\nand on the other a series of descriptor values. The latter isstored in a scalable data structure. In some cases it is use-\nful to think of the scaling operation as part of the deﬁnition\nof the descriptor. For example, the power spectrum of a\nsegment of audio is equal to the mean of the power spectra\nof the frames within that segment. Power can be deﬁned\nwith whatever granularity. In other cases, the scaling op-\nerations are best understood as statistics that describe the\ndistribution of values that they summarize. For example,\nmean and covariance can be used to parametrize a gaus-\nsian distribution. A given descriptor may eventually be\nscaled according to several different operations, each at\nits own scale, all stored within the same description.\nScalability was introduced as a design principle of the\naudio part of the MPEG-7 metadata standard. Content-\nbased audio descriptors are built using a “Scalable-\nSeries” datatype that stores the series of descriptors\nvalues, and confers to each MPEG-7 audio descrip-\ntion the scalability property (de Cheveign ´e and Peeters,\n1999a,b, 2000; ISO/IEC JTC 1/SC 29, 2001). Audio de-\nscriptors used in the MPEG-7 standard are deﬁned in\n(ISO/IEC JTC 1/SC 29, 2001). A useful property of the\nMPEG-7 data structures is that they can accommodate a\ndescriptor instantiated at multiple scales, and with multi-\nple scaling rules. Some examples of scalable audio de-\nscriptors and associated scaling operations are given in\nTab. 2.1, others are described in (de Cheveign ´e, 2002).\nTable 1: Descriptor examples.\nDescriptor Typical operations Useful for\nWaveform Min, Max Display, search\nPower Mean, variance Search\nFundamental Weighted mean, Query by\nfrequency Histogram humming\nPower Mean, Covariance, Display, search\nspectrum Histogram\n2.2 Scalability and the life-cycle of metadata\nAs evoked previously, scalability addresses issues related\nto the life-cycle of multimedia content and metadata. It\nenhances the useful properties of interoperability andre-\nusability of metadata descriptions, that are further en-\nhanced by being standardized. Metadata resolution re-\nquirements typically vary across applications and across\ntime, and the same is true of storage and transport con-\nstraints. Metadata produced at one time for one applica-\ntion may need to be reused at another time by another ap-\nplication. A level of detail that is appropriate at one time\nmay later be excessive, when the volume of data to de-\nscribe has increased. Scalability allows the same metadata\nformats to be used for every application. It frees the appli-\ncation designer from the difﬁcult decision of the “right”\nresolution. It also allows rescaling operations to be sched-\nuled at the level of the transport or storage system, so that\nmetadata may be thinned rather than deleted when space\ncomes to lack. Scalability thus extends the life-cycle of\nmetadata.\n2392.3 Typical applications\nScalable metadata are useful for a range of applications.\nThey can be used for display of audio data within a\nuser interface (music browser or editor). For example,\na “waveform” descriptor may be deﬁned as a time-series\nof min/max pairs. This is sufﬁcient to produce a full-\nresolution graphical display of the waveform within an au-\ndio ﬁle. As long as there are as many pairs as pixels hori-\nzontally, the result is identical to plotting all samples of the\noriginal waveform, and yet cheaper in terms of storage,\ntransport and drawing. Likewise a sonagram-like spec-\ntrum descriptor (time-frequency representation). Descrip-\ntors may be combined, for example a low-resolution spec-\ntrogram associated with fundamental frequency and har-\nmonicity measure, to approximate a high-resolution spec-\ntrogram. Descriptors may serve also to produce icons.\nScalable audio metadata may also be used to provide\naudio feedback for browsing. For example, a combina-\ntion of a low-resolution power spectrum with descriptors\nof fundamental frequency, harmonicity and roughness al-\nlows a rough rendering of the texture of audio content to\nbe restituted. By focusing on the large scale structure of\ndocuments or collections, such auditory or visual feed-\nback is complementary to the more common practice of\nproviding a human-edited clip, for instance. A synthetic\n“earcon” may thus be used to characterize ﬁle content\nThe most important function, however, is search . The\nimportance of search increases as content grows, and man-\nual operations become less feasable. Search is needed to\nrespond to user demands to “ﬁnd” speciﬁc content. It is\nalso needed to support “house-keeping” operations at the\nsystem level, or to organize data into useful synthetic rep-\nresentations for a user interface. For example, a display\nthat represents the content of a hard disk with duplicates\ncolored in red would be of great use to a user. Search\nsupports such functionality.\n3 SCALABILITY AND SEARCH\nEfﬁcient search is based on pruning , that is, early elim-\nination of the parts of the search space where the query\nis known not to be. This is typically done by organizing\nthe search space hierarchically as a tree and attaching in-\nformation to each node, such that by consulting that infor-\nmation the algorithm can know that the token is notwithin\nthe subtree spanned by the node.\nOrganizing the data in hierarchical search structures is\na principle shared in numerous domains: string matching\n(Fredriksson, 2004), image browsing and search (Spence\nand Parra, 2000; Chen et al., 2000), bioinformatics and\nDNA analysis (Eisen et al., 1998), speech processing\n(Zotkin and Duraiswami, 2004), etc. Many standard\nsearch algorithms use structures such as binary trees, red-\nblack trees, B-trees (Cormen et al., 1990), hierarchical\nclustering (Krishnamachari and Abdel-Mottaleb, 1999).\nMultiple-resolution representations of the data may be\nused to label such trees to perform hierarchical search,\neither deterministic (Chen et al., 2000; Li et al., 1996)\nor probabilistic (Luettgen and Willsky, 1995; Spence and\nParra, 2000).\nScalable metadata are well suited for this purpose. Theindex at each node is a scaled summary of the indices of\nlower nodes. The scaling operations mentioned above (ex-\ntrema, mean, variance, etc.) offer statistics that describe\nthe set of data that they summarize, and thus each node\nallows inference as to whether a search token is included\nin the subtree.\nExtrema statistics support deterministic pruning: a\nnode is pruned if the search token is out of bounds. As\nan example, an audio waveform comparison algorithm\nbased on minima and maxima of the waveform is de-\nscribed in (de Cheveign ´e, 2002; ISO/IEC JTC 1/SC 29,\n2001). Other statistics such as mean and covariance al-\nlow probabilistic inference based on a parametric model\nof the data set, for example multivariate gaussian. The\n“scalewise variance” statistics described in (de Cheveign ´e,\n2002; ISO/IEC JTC 1/SC 29, 2001) allows yet ﬁner char-\nacterization of the distribution, as do various combinations\nof these statistics.\nSearch proceeds from the root of the tree (lowest reso-\nlution) to the leaves (highest resolution). At each node, the\ndecision is made whether to prune the subtree as a result\nof an inference based on the index of that node. Search\nis fast near the root, and more slow but reliable as it pro-\nceeds towards the leaves. For a very large database (or\none that is distributed across the network), only the high-\nlevel nodes may be available on line, the off-line lower-\nlevel nodes being retrieved on demand, if available. Early\npruning reduces the number of times that this potentially\ncostly operation needs to be performed. Scalable meta-\ndata thus provide the hierachical search structures needed\nto support efﬁcient search.\n4 EXPERIMENT\nThe aim of this experiment is to demonstrate that scal-\nable metadata can support effective content-based audio\nsearch. For that purpose we took a state-of-the-art search\nalgorithm, adapted it to make use of a hierarchical struc-\nture based on scalable metadata, and compared its perfor-\nmance with that of the original.\n4.1 Baseline algorithm\nAn algorithm that achieves efﬁcient pruning without a hi-\nerarchical search structure is the “active search” algorithm\nof Kashino et al. (1999, 2003). The algorithm, based on\nhistograms of vector-quantized spectra, allows a segment\nof audio (query token) to be found within a database of\naudio documents. In brief, the database to be searched\nis indexed by calculating power spectra at a given frame\nrate. Spectra warped to a logarithmic frequency axis are\nvector-quantized using a code book to form a time-series\nof VQ code indices. This series constitutes an index\nwithin which the search proceeds.\nAt search time, indices of the database are accumu-\nlated over a running window to form a time series of his-\ntograms . At each frame, the histogram is compared to\na similar histogram formed from the query token, until\na match is found. The efﬁciency of search stems from\nthe fact that, after each comparison between the query\nhistogram and the current histogram, the algorithm can\n240skip forward a number Nof index samples equal to the\nlargest mismatch between corresponding bins of token\nand database histograms. Indeed, a time shift of at least N\nframes is required to resorb that mismatch. The algorithm\nis thus fast.\nAs just described, the algorithm is sure to ﬁnd a match\nif the analysis frames of query and database are tempo-\nrally aligned. However, perfect alignment is not always\nguaranteed, and misalignment may cause the VQ series to\ndiffer slightly. Taking this into account, the algorithm uses\na measure of similarity deﬁned as:\nS(hQ, hC) =1\nLL/summationdisplay\nk=1min(hQ(k), hC(k))\nwhere hQ(k)is the k-th coefﬁcient of the query his-\ntogram, hC(k)the current histogram (taken from the\ndatabase), and Lthe codebook size. The token is con-\nsidered to have been found when the similarity exceeds a\ncertain threshold.\nFFT FFT FFTfrequency\ntime\nVQ\n1 3 4\nhistograms\nFigure 1: Schematic diagram of the indexing phase. Over-\nlapping frames are windowed and Fourier-transformed to\nobtain power spectra that are then warped to a logarithmic\nfrequency axis and vector-quantized to produce a series of\ncodes that are then aggregated into histograms.\nA useful feature of the “active search” algorithm of\nKashino et al. (1999, 2003) in our context is that it is basedon histograms. The histogram operation is scalable, and\ntherefore this efﬁcient algorithm is a good starting point\nto develop a scalable search algorithm also based on his-\ntograms. The similarity between the two histogram-based\nalgorithms makes it easy to draw insights from their com-\nparison.\n4.2 Hierarchical histogram-based search\nThe ﬁrst steps of the scalable algorithm (Fig. 1), are simi-\nlar to the active search algorithm of Kashino et al. (1999,\n2003). Log-frequency spectra are vector-quantized to\nform a series of VQ codes that are then grouped into a\nseries of histograms. In contrast to the active search algo-\nrithm, a series of histograms, rather than VQ codes, forms\nthe searchable index. The histogram series are then scaled\nby successive powers of two to form a hierarchical search\nstructure (binary tree) that indexes the database (Fig. 2).\nAt search time, a histogram is similarly calculated from\nthe query token and compared to the histogram attached\nto each node of the database index structure, starting from\nthe root. Histogram comparison determines whether the\nquery token is included within the segment spanned by a\nnode. More precisely, if at frame jand for all codebook\nindices kwe have:\nhQ(k)≤hC(k, j),\nthen all spectra within the query token are also found\nwithin the database segment spanned by this node. If the\ntest fails, that segment of the database may be pruned. If\nthe test succeeds, it is repeated at successively ﬁner reso-\nlutions until it fails or the token is found.\nActually the algorithm is slightly more complex than\njust described. So far, a match is guaranteed only if\nthe query token happens to be temporally aligned with a\nsame-sized database interval spanned by a node, an un-\nlikely event. To allow arbitrary aligment, the above com-\nparison is replaced by hQ(k)≤hC(k, j) +hC(k, j+ 1).\nIf the segment spanned by the latter two histograms is\nshorter than twice the query token, the query token is split\nin two and search proceeds with each half. A further com-\nplication arises because, as in the active search algorithm\nof Kashino et al. (1999, 2003), the temporal aligment of\nsearch token analysis frames and database analysis frames\nis not guaranteed. To allow for slight misalignments, the\nperfect match criterion must be replaced by an approxi-\nmate match criterion involving a threshold.\n4.3 Database and implementation\nExperiments were performed using the RWC database\n(Goto et al., 2002, 2003) that contains a total of approxi-\nmately 24 hours sampled at 44100 kHz with 16-bit resolu-\ntion of various types of music (classical, Japanese popular\nmusic...).\nSpectral features were calculated every 16 ms by ap-\nplying an FFT to a 32 ms window shaped as a raised\ncosine. The frequency axis was warped to a logarith-\nmic scale by grouping power spectrum bins into 8 one-\noctave bands ranging from 65.2 to 16000 Hz. Spec-\ntrum values were raised to a power of 1/3 to improve the\n241shape of their distribution. The advantage of a logarith-\nmic scale is that it parallels the spectral resolution of the\near (although with cruder resolution) and distributes signal\npower evenly among bands (most spectra are low-pass).\nThe 1/3 exponent simulates partial loudness (Hartmann,\n1997). This relatively crude resolution (about three times\npoorer than the ear) is motivated by the need for concision.\nThe procedure is similar to that speciﬁed in the MPEG-7\nstandard.\n4\n32\n2\n3\n3\n2\n31\n1\n1\n3\n412\n2?\nCompatible histogram bin\nIncompatible histogram binscale ratio4 8 16\nFigure 2: Schematic diagram of the hierachical search al-\ngorithm. The database is indexed by a series of codevec-\ntors (right) from which is derived a binary tree structure of\naggregate histograms. Search proceeds from the root. The\nquery (represented by a histogram) is compared to each\nhistogram in turn. In case of failure, the subtree spanned\nby that node is pruned (gray). In case of success (white),\nsearch proceeds within the subtree until the token is found,\nor the search returns unsuccessful.\nA codebook was prepared by applying the LBG vec-\ntor quantization algorithm (Linde et al., 1980) to a sub-\nset of the database (about 20%). The codebook size was\n512, adequate for a 8-dimension feature space. Codebook\nsize and spectral resolution result from a compromise be-\ntween discriminative power and practical constraints. For\ncomparison, 2-bin quantization of each dimension of a\n1/3 or 1/4 octave spectrum would require a codebook of\nsize224or232which is clearly unweildy. The codebook\n(dictionary) must be included with the indexing metadata\nattached to the database to be searched (or else it must\nbe “well-known”, for example deﬁned by a standard).A codebook prepared in this way will obviously depend\nupon the database from which it was prepared. Ideally one\nwould like to have a sort of “universal” codebook, built on\na very large database and re-usable. Whether such a goal\nis feasable remains to be determined.\nThe time series of spectral features was quantized us-\ning the VQ code book and used to implement both the\nactive search algorithm of Kashino et al. (1999, 2003)\nand the hierarchical algorithm. To implement the hier-\narchical search algorithm, series of VQ codes were aggre-\ngated to form histograms, and then scaled repeatedly and\nstored within a hierarchical data structure. For simplicity,\nboth algorithms were implemented so that analysis frames\nof query and database were aligned, and thresholds (see\nabove) were set to the ideal value corresponding to an ex-\nact match without any tolerance. This affects both algo-\nrithms equally. A third algorithm, exhaustive search, was\nalso implemented for comparison. It simply consists in\na direct comparison of the sequence of vector-quantized\nspectra of the query with same-sized subsequences in the\ndatabase. This naive algorithm is useful as an upper bound\nfor calculation time.\nSubsets of the database of varying size were extracted\nin order to investigate the relation between search speed\nand database size. A set of 48 query excerpts of 10s dura-\ntion was used as search tokens for both algorithms. Both\nalgorithms were set up to search for multiple occurrences\n(i.e. search did not terminate after the ﬁrst match). Search\nspeed was quantiﬁed by counting the average number of\nhistogram comparisons, or average CPU time necessary\nto ﬁnd a query token. Simulations were performed using\nMatlab 7.0.1 on a PC with a Pentium 4 - 3 GHz processor.\n5 RESULTS AND DISCUSSION\n5.1 Results\nTable 2 and Fig. 3 summarize the results. The size of the\nsearch space is varied as a parameter. Cost of exhaustive\nsearch (top line) varies linearly with space size. Active\nsearch (middle line) also follows an approximately linear\ntrend, but is over two orders of magnitude faster than ex-\nhaustive. Hierarchical search (bottom line) follows a less-\nthan-linear trend, with a speedup ratio of 7 to 40 over ac-\ntive search. Results for CPU time are similar.\nTable 2: Search time and speed-up ratio (SU) for active\nsearch (AS) and hierarchical search (HS).\n1h 2h 6h 13h 24h 48h\nAS 525 803 2726 5043 9794 19588\nHS 69 72 123 158 272 481\nSU 7,6 11,2 22,2 31,9 36 40,7\nThe hierarchical algorithm based on scalable metadata\nis faster than the baseline algorithm, itself known to be\ncompetitive. This shows that scalable metadata can sup-\nport efﬁcient search. The result is welcome, as we argued\nearlier that scalability is a necessary property of metadata.\nBoth algorithms attained 100% accuracy (deﬁned as\nthe mean between precision and recall rate). This high\n242106107101102103104105106107108\nNumber of codes in database indexNumber of examined histogramsExhaustive search\nActive search\nHierarchical searchFigure 3: Search time as a function of database size.\nlevel of accuracy was obtained for the case where analy-\nsis frames of query and database were temporally aligned.\nWhile this can be assumed in some applications, in oth-\ners there is no guarantee of temporal alignment. Mis-\nalignment produces a mismatch that must be compensated\nby allowing histogram comparisons to be tolerant. This\nin turn increases the proportion of “false positives” and\ncauses subtree pruning to be less effective, thus increas-\ning search time. The active search and hierarchical search\nalgorithms are equally affected. Similar “fuzzy-match”\nschemes are required if search is expected to tolerate dis-\ntortions (such as produced by lossy coding).\n5.2 Discussion\nScalability puts strong constraints on the semantics and\nstructure of metadata. Here we found that these con-\nstraints are compatible with the important function of\nsearch, and that scalability may actually help to imple-\nment hierarchical structures that support very efﬁcient\nsearch. Efﬁciency stems from the hierarchical organiza-\ntion of the search space induced by scaling.\nScalability implies that the temporal order of labels at\neach level must be conserved. This leads to a hierarchical\nstructure that less efﬁcient than, say, hierarchical cluster-\ning by similarity. It is conceivable that efﬁciency could be\nenhanced by reorganizing the search tree at search time.\nMore research is needed to develop index schemes that\nare both scalable and efﬁcient.\nSpeedups may also be expected by improving the dis-\ntributional properties of the descriptor, for example by\nnormalization by power (separately vector-quantized), or\ntransformation to a “cepstrum-like” representation with\na covariance matrix closer to diagonal. The index may\nalso be improved by including additional descriptors to\ncapture features not well represented by the octave-band\nspectrum: fundamental frequency, harmonicity, rough-\nness, etc.\nAn attractive feature of histogram-based search is that\nit can be applied to any descriptor. Useful examples are\nfundamental frequency, chroma, event counts, or even\ntext-based descriptors. All can be quantiﬁed and storedas histograms. A constraint is that the coding (quantiz-\ning) stage requires a dictionary. Scalability requires that\nthe same dictionary be used to quantize all data, and this\nimplies the existence of a well-known and accepted (i.e.\nstandardized) dictionary. Whether a universal dictionary\ncan be found for each descriptor that can cover all needs\nis an important issue.\nThe present paper examined only algorithms based on\nthe histogram. Other scalable operations as extrema, mean\nand covariance, scalewise variance, etc. produce statistics\nthat can be used to quantify the distribution of values that\nthey summarize. These too are expected to support efﬁ-\ncient search. More research should be devoted to the issue\nof search within scalable representations.\n6 CONCLUSIONS\nIn this paper, we investigated search within audio content\nbased on scalable metadata. We veriﬁed that a standard\nsearch algorithm can be applied to scalable metadata, and\nfound additionally that search could be speeded by mak-\ning use of the hierarchical structure offered by scalable\nmetadata. While we did not test other search algorithms, it\nis reasonable to believe that they too can be implemented\nand beneﬁt from scalability. Scalability is a property that\nis necessary to allow metadata to follow future trends in\ndata size. It is a welcome result that it can support essen-\ntial operations such as search.\nACKNOWLEDGEMENTS\nThe authors wish to thank Daniel Pressnitzer, Dan Gnan-\nsia and Maria Chait for their useful comments and friendly\nsupport during the conduct of this research.\nREFERENCES\nJ.-Y . Chen, C. Bouman, and J. Dalton. Hierarchical\nbrowsing and search of large image databases. In IEEE\ntransactions on Image Processing , volume 9, pages\n442–455, march 2000.\nT. Cormen, C. Leiserson, and R. Rivest. Introduction to\nalgorithms . The MIT Press, 1990.\nF. Crestani, C. Rijksbergen, and I. Campbell. Is this doc-\nument relevant ?... probably: a survey of probabilistic\nmodels in information retrieval. In ACM Computing\nSurveys , volume 30, pages 528–552, 1998.\nA. de Cheveign ´e. Scalable metadata for search, soniﬁca-\ntion and display. In Proceedings of the 2002 Interna-\ntional Conference on Auditory Display , Kyoto, Japan,\nJuly 2002.\nA. de Cheveign ´e and G. Peeters. Core set of audio sig-\nnal descriptors. Technical Report JTC1/SC29/WG11,\nMPEG00/m5885 technical report, ISO/IEC, 2000.\nA. de Cheveign ´e and G. Peeters. Scale tree. Technical\nReport JTC1/SC29/WG11, MPEG99/m5076 technical\nreport, ISO/IEC, 1999a.\nA. de Cheveign ´e and G. Peeters. Scale tree up-\ndate. Technical Report ISO/IEC JTC1/SC29/WG11,\nMPEG99/m5443 technical report, ISO/IEC, 1999b.\n243M. Eisen, R. Spellman, P. Brown, and D. Botstein. Cluster\nanalysis and display of genome-wide expression pat-\nterns. In Proc. Natl. Acad. Sci. USA, Genetics , vol-\nume 95, pages 14863–14868, december 1998.\nK. Fredriksson. Metric indexes for approximate string\nmatching in a dictionary. In Proceedings of the 11th\nInternational Symposium on String Processing and In-\nformation Retrieval (SPIRE’2004) , LNCS 3246, pages\n212–213. Springer–Verlag, 2004.\nV . Gaede and O. G ¨unther. Multidimensional access meth-\nods. In ACM Computing Surveys , volume 30, pages\n170–231, 1998.\nM. Goto, H. H., N. T., and R. Oka. RWC music database:\nPopular, classical and jazz music databases. In Proc.\nof International Conference of Music Information Re-\ntrieval (ISMIR) , pages 287–288, 2002.\nM. Goto, H. H., N. T., and R. Oka. RWC music database:\nMusic genre database and musical instrument sound\ndatabase. In Proc. of International Conference of Music\nInformation Retrieval (ISMIR) , 2003.\nW. Hartmann. Signals, Sounds and Sensation . AIP Press,\n1997.\nISO/IEC JTC 1/SC 29. Information technology - mul-\ntimedia content description interface - part 4: Audio.\nTechnical Report ISO/IEC FDIS 15938-4, 2001.\nA. Jain, M. Murty, and P. Flynn. Data clustering: a review.\nInACM Computing Surveys , volume 30, pages 265–\n321, 1999.\nK. Kashino, G. Smith, and H. Murase. Time-series active\nsearch for quick retrieval of audio and video. In Proc. of\nInternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP) , volume 6, pages 2993–2996,\nMarch 1999.\nK. Kashino, T. Kurozumi, and H. Murase. A quick search\nmethod for audio and video signals based on histogram\npruning. IEEE Transactions on Multimedia , 5:348–\n357, Sep. 2003.\nS. Krishnamachari and M. Abdel-Mottaleb. Hierarchi-\ncal clustering algorithm for fast image retrieval. In\nIS&T/SPIE Conference on Storage and Retrieval for\nImage and Video Databases VII , pages 427–435, San\nJose, California, January 1999.\nC.-S. Li, P. Yu, and V . Castelli. Hierarchyscan: A hierar-\nchical similarity search algorithm for databases of long\nsequences. In Proc. of IEEE Conference on Data Engi-\nneering , 1996.\nY . Linde, B. A., and R. Gray. An algorithm for vector\nquantizer design. IEEE Transactions on Communica-\ntions , pages 702–710, January 1980.\nM. Luettgen and A. Willsky. Likelihood calculation for\na class of multiscale stochastic models, with applica-\ntion to texture discrimination. In IEEE Transactions on\nImage Processing , volume 4, pages 194–207, 1995.\nA. Odlyzko. The current state and likely evolution of the\ninternet. In Proc. Globecom ’99, IEEE , pages 1869–\n1875, 1999.P. Sivakumaran, J. Fortuna, and A. M. Ariyaeeinia. On\nthe use of the bayesian information criterion in multi-\nple speaker detection. In Proc. Eurospeech , pages 795–\n798, 2001.\nC. Spence and L. Parra. Hierarchical image probability\n(HIP) models. In Proc. Advances in Neural Information\nProcessing Systems , pages 848–854, 2000.\nD. Zotkin and R. Duraiswami. Accelerated speech source\nlocalization via a hierarchical search of steered re-\nsponse power. In IEEE Transactions on Speech and\nAudio Processing , volume 12, pages 499–508, 2004.\n244"
    },
    {
        "title": "Distributed Audio Feature Extraction for Music.",
        "author": [
            "Stuart Bray",
            "George Tzanetakis"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417563",
        "url": "https://doi.org/10.5281/zenodo.1417563",
        "ee": "https://zenodo.org/records/1417563/files/BrayT05.pdf",
        "abstract": "One of the important challenges facing music information retrieval (MIR) of audio signals is scaling analysis algorithms to large collections. Typically, analysis of audio signals utilizes sophisticated signal processing and machine learning techniques that require significant computational resources. Therefore, audio MIR is an area were computational resources are a significant bottleneck. For example, the number of pieces utilized in the majority of existing work in audio MIR is at most a few thousand files. Computing audio features over thousands files can sometimes take days of processing. In this paper, we describe how Marsyas-0.2, a free software framework for audio analysis and synthesis can be used to rapidly implement efficient distributed audio analysis algorithms. The framework is based on a dataflow architecture which facilitates partitioning of audio computations over multiple computers. Experimental results demonstrating the effectiveness of the proposed approach are presented. Keywords: distributed processing, dataflow networks, large-scale music information retrieval 1",
        "zenodo_id": 1417563,
        "dblp_key": "conf/ismir/BrayT05",
        "keywords": [
            "distributed processing",
            "dataflow networks",
            "large-scale music information retrieval",
            "audio analysis algorithms",
            "computational resources",
            "signal processing",
            "machine learning techniques",
            "audio features",
            "audio MIR",
            "experimental results"
        ],
        "content": "Distributed audio feature extraction for music\nStuart Bray\nComputer Science Department\nUniversity of Victoria\n3800 Finnerty Rd\nVictoria BC, Canada\nsbray@csc.uvic.caGeorge Tzanetakis\nComputer Science Department (also in Music)\nUniversity of Victoria\n3800 Finnerty Rd\nVictoria BC, Canada\ngtzan@cs.uvic.ca\nABSTRACT\nOneoftheimportantchallengesfacingmusicinformation\nretrieval (MIR) of audio signals is scaling analysis algo-\nrithms to large collections. Typically, analysis of audio\nsignals utilizes sophisticated signal processing and ma-\nchine learning techniques that require signiﬁcant compu-\ntational resources. Therefore, audio MIR is an area were\ncomputational resources are a signiﬁcant bottleneck. For\nexample, the number of pieces utilized in the majority of\nexistingworkinaudioMIRisatmostafewthousandﬁles.\nComputing audio features over thousands ﬁles can some-\ntimes take days of processing. In this paper, we describe\nhowMarsyas-0.2 , a free software framework for audio\nanalysis and synthesis can be used to rapidly implement\nefﬁcientdistributedaudioanalysisalgorithms. Theframe-\nwork is based on a dataﬂow architecture which facilitates\npartitioning of audio computations over multiple comput-\ners. Experimental results demonstrating the effectiveness\nof the proposed approach are presented.\nKeywords: distributed processing, dataﬂow networks,\nlarge-scale music information retrieval\n1 INTRODUCTION\nAdvances in storage capacity, network speed, and audio\ncompression have made possible the storage of large col-\nlections of audio and music on personal computers and\nportable devices. Projecting these trends it is likely that\nin the near future all of recorded music in human history\nwill be available digitally. In the last few years, there has\nbeen a number of publications exploring ways of analyz-\ning audio signals for music information retrieval (MIR)\napplications.\nDeveloping audio and music analysis systems is chal-\nlenging. Existing algorithms for extracting content infor-\nmation from music tend to use sophisticated signal pro-\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londoncessing and machine learning techniques which require\nsignﬁcant computational resources. Audio signal pro-\ncessing for music signals utilizes computationally inten-\nsive time-frequency analysis techniques such as the short\ntime Fouriertransform, wavelets and auditory ﬁlterbanks.\nMoreover, machine learning algorithms such as Support\nVector Machines (SVMs) and Artiﬁcial Neural Networks\n(ANNs) require large amounts of training data in order to\nbuild reliable classiﬁers that generalize well. Currently\ndevelopers of audio analysis applications are faced with\na hard dilemma. They can use an interpreted program-\nming environment such as Matlab that provides a lot of\nthe necessary components for building audio analysis ap-\nplicationsbutisnotasefﬁcientascompiledcode,orwrite\nthe code from scratch which requires a signiﬁcant invest-\nment of time just to build the necessary infrastructure.\nDistributed computation can be used to speed up com-\nputations and deal with the large data sets required for\nthe analysis of audio signals. Existing mechanisms for\nparallelizing computer programs are designed for general\nprogramming tasks and tend to be complicated and hard\nto use. Therefore in audio analysis typically distributed\ncomputingisnotusedatallorwhenitisuseditistailored\nto very speciﬁc projects.\nIn this paper, we describe how Marsyas-0.2 , a\ndataﬂow-based audio analysis and synthesis framework\ncan be used for rapid prototyping and developing of dis-\ntributed audio analysis systems. Although audio analy-\nsis is a challenging application domain, it provides some\ndomain-speciﬁc constraints that can inform the design\nof an effective framework. Using Marsyas-0.2 , music\nanalysis systems are assembled from component using a\ndataﬂow-based approach. An important advantage of the\ndataﬂow approach proposed in this work is that it sim-\npliﬁes distributing audio and music analysis computation\namong various computers. The dataﬂow model empha-\nsizes the parallel computational structure of a particular\nproblem. Using a declarative dataﬂow speciﬁcation ap-\nproach the programmer can distribute audio analysis al-\ngorithms with minimum effort. In contrast, distributing\ntraditionalsequentialprogramsplacesasigniﬁcantburden\nto the programmer who has to decide which parts of the\ncode can be parallelized and deal with load distribution,\nscheduling, synchronization andcommunication.\n4342 RELATED WORK\nDataﬂow programming has a long history. The origi-\nnal (and still valid) motivation for reasearch into dataﬂow\nwas to take advantage of parallelism. Motivated by criti-\ncismsoftheclassicalvonNeumannhardwarearchitecture\ndataﬂow architectures for hardware were proposed as an\nalternative in the 1970s and 1980s. During the same pe-\nriod a number of textual dataﬂow languages such as Lu-\ncid (Wadge and Ashcroft, 1985) were proposed. During\nthe 1990s there was a new direction of growth in the ﬁeld\nof dataﬂow visual programming languages especially in\nspeciﬁc application domains. Succesful commercial ex-\namplesincludeLabview1andSimuLink2. Arecentcom-\nprehensivereviewofthehistoryofdataﬂowprogramming\nlanguagescanbefoundbyJohnstonetal.(2004). Arecent\ntrend is to view dataﬂow computation as a software engi-\nneering methodology for building systems using existing\nlanguages (Manolescu, 1997). A small scale case study\nof distributed music analysis using the Geddei processing\nframework is described by Wood and Keefe (2004). The\ndescription of a large scale evaluation of different acous-\ntic measures of similarity is provided by Berenzeig et al.\n(2003). A similar dataﬂow approach to audio processing\nisutilizedinCLAM(Amatriain,2005). Morerecentlythe\nM2K/D2K dataﬂow framework (Downie et al., 2004) has\nbeenusedforevaluationofMIRsystems. TheMax/MSP3\ndataﬂowvisualprogramminglanguagehasalsobeenused\nextensively in computer music.\n3 DATAFLOW ARCHITECTURE\nMarsyas-0.24is a software framework, written in C++,\nfor rapid prototyping and experimentation with audio\nanalysis and synthesis with speciﬁc emphasis on process-\ning music signals. A variety of existing building blocks\nthat form the basis of many published algorithms are pro-\nvided as dataﬂow components that can be composed to\nformmorecomplicatedalgorithms(black-boxfunctional-\nity). In addition, it is straightforward to extend the frame-\nwork with new building blocks(white-box functionality).\nInMarsyas terminology the processing nodes of the\ndataﬂow network are called MarSystems and provide the\nbasicbuildingblocksoutofwhichmorecomplicatedsys-\ntems are built. Existing components include: I/O (au-\ndio ﬁles, Matlab, audio playback and recording), feature\nextraction (short-time Fourier transform, wavelets, mel-\nfrequency cepstral coefﬁcients), synthesis (wavetable,\nphasevocoder) and machine learning classiﬁers. cluster-\ning). Similarlyto CLAM(Amatriain, 2005), Marsyas-0.2\nmakesacleardistinctionbetweendata-ﬂowwhichissyn-\nchronous and control-ﬂow which is asynchronous. Be-\ncauseMarsyas-0.2 has synchronous dataﬂow (i.e at every\n“tick” a speciﬁc data slice is propagated across the entire\nnetwork) there is no need for queues between nodes and\nshared buffers can be used for better performance (simi-\nlarly to Univ pipes).\n1http://www.ni.com/labview/\n2http://www.mathworks.com/products/\nsimulink/\n3http://www.cycling74.com\n4http://marsyas.sourceforge.net\nFigure 1: Marsyas-0.2 VisualPatch Builder\nFigure 2: Feature Extraction Network for real-time Mu-\nsic/Speech classiﬁcation\nMarSystems can be instantiated at run-time. There-\nfore any complicated audio computation expressed as a\ndataﬂow network can be instantiated at run-time. For ex-\nample multiple instances of any complicated network can\nbe created as easily as the basic primitive MarSystems .\nThisisaccomplishedbyusingthe Prototype andCompos-\nitedesign patterns (Gamma et al., 1995). Currently there\nare three ways to build audio analysis and applications in\nMarsyas-0.2 . Theﬁrstisthetraditionalmethodofwriting\ndirectly C++ code and compiling an executable. The sec-\nond is based on a simple scripting language that provides\nsyntacticconstructsforbuildingthedataﬂownetwork,set-\ntingappropriatelythecontrolsandmovingsoundthrough\nthenetwork. Thethirdwayistouseavisualpatchbuilder\nwhich uses the scripting language “under the hood”.\nFigure 1 shows the visual patchbuilder that can be\nused for specifying dataﬂow networks and controls. Fig-\nure 2 shows a dataﬂow network for extracting audio fea-\ntures for real-time music/speech classiﬁcation. The en-\ntire network can be created at run-time without requiring\nany code compilation. The feature extraction front-end of\nTzanetakis and Cook (2002), has been implemented as a\ndataﬂow network in Marsyas-0.2 .\n4353.1 Distributed audio computation\nThere are two standard data communication protocols\nusedontheInternet: transmissioncontrolprotocol(TCP),\nanduserdatagramprotocol(UDP).TCPprovidesreliabil-\nity mechanisms to ensure that all packets are received ex-\nactlyastheyaresent;ontheotherhand,UDPprovidesno\nsuchmechanismsbutissigiﬁcantlyfasterduetolessover-\nhead. UDPisthereforetheprotocolofchoiceforreal-time\nstreaming applications where data is time critical.\nMarsyas-0.2 supports both the UDP and TCP proto-\ncols. Inordertosenddatatoanothermachine,a“network\nsink”objectissimplyinsertedsomewhereintheﬂowofa\nMarSystem . In order to receive data, a “network source”\nobject is inserted. Control ﬂow and data ﬂow are man-\naged seperately so that controls can be changed from the\nsenderand propagatethrough thesystem. Theidea isthat\na user can operate several worker machines and the view\nof the distributed system is abstracted as one large com-\npositeMarSystem .\n4 EXPERIMENTS\nAudio features can be calculated either in realtime using\nUDP, or non-real time using TCP. One of the main goals\nof our experimentation was to demonstrate the cost ben-\neﬁts of parallelization accross multiple computers; thus,\nwe used TCP and non-realtime feature extraction. For all\nthe experiments a feature vector consisting of the means\nand variances of smoothed Mel-frequency Cepstral Coef-\nﬁcients as well as STFT-based features such as spectral\ncentroid and rolloff was used. The data consists of 30-\nsecond audio clips and a 35-dimensional feature vector is\ncalculated for each clip. The actual audio waveform sam-\nples are transmitted over thenetwork.\n4.1 Optimal Vector Size\nFile transfer throughput using TCP is highly affected by\nthe size of the packet generated at the application layer.\nRequeststhatarelargerthantheTCPMaximumSegment\nSize (MSS) get buffered and partitioned in the transport\nlayer; this can be costly. Additional overhead can come\nfrom several sources, some of which are the handling\nof out-of-order segments, retransmissions, and checksum\ncalculations. A detailed analysis of the segmentation cost\nis beyond the scope of this paper, but is well documented\nand available from other sources.\nOur experimentation was done on a 100Base-T Eth-\nernet local area network of Apple G5 computers, where\nthe MSS is 1460 bytes. Although the details of TCP are\nlargely implementation speciﬁc, typically segments that\nare smaller than the MSS will be sent immediately. Con-\nsequently, we found that the optimal vector size to use\nwouldbeasclosetotheMSSaspossible,asshowninﬁg-\nure 3. From the graph you will also note the signiﬁcant\nmeasured cost increase of over-stepping the boundary of\ntheMSS.Thecostincreasewhensmallerpacketsareused\nis likely due to sending more packets than necessary, in-\nvolvingextraprocessinginalllayersofTCP/IP,andsatu-\nration on the network.\nFigure3: OptimalVectorSizesforMaximumThroughput\n4.2 Parallelization of FeatureExtraction\nInthissectionwedemonstrateourresultsfromparalleliz-\ning audio computation across multiple computers using\nthe data-ﬂow architecture of Marsyas 0.2. The model in-\nvolvesadispatchernodethatsendsseperateclipsfroman\naudio collection to each worker node in the network. The\njob of a worker node is to simply calculate features for\neachﬁleitreceivesandthensendtheresultstoacollector\nprocess(possiblyrunningonthesamemachineasthedis-\npatcher) that gathers the results. In our ﬁrst experiment,\nthe audio collection was partitioned into sub-collections\nwhich were sent to each worker. All the audio clips are\nstored on the dispatcher. We found that the optimal num-\nber of worker nodes in this model was three, after which\ntherewasnotimebeneﬁtofusingextramachines. Infact,\nitwascostlytoaddanymorethanﬁveworkernodesdueto\nthenetworkcapacityofthedispatcher/collector. Laterin\nthis section it is shown that collection partitioning across\nmultiple dispatchers can improve results. Table 1 shows\nresults of using the collection dispatcher model, using up\ntoﬁveworkernodesandaudiocollectionsofupto10,000\nﬁles. The format is hours:minutes:seconds.\nTable 1: Parallelization results for Collection Dispatcher\n10100100010000\nLocal00:0500:5809:391:36:49\n1W00:0701:1011:481:58:49\n2W00:0300:3806:011:10:46\n3W00:0400:3405:4959:46\n4W00:0300:3405:521:04:56\n5W00:0400:3605:541:08:36\nTheproblemwiththeﬁrstapproachisthatsomenodes\nmay complete processing the features of their respective\nsub-collection before others, sitting idle. Thus the time\nittakestoprocesstheentireaudiocollectionisdependent\nontheslowestnodeinthesystem. Inordertoalleviatethis\nproblemandmakeuseofidlenodes,anadaptiveapproach\nis used where the dispatcher sends data as necessary to\neach worker. That way, each node in the system is work-\n436Table 2: Parallelization results for Adaptive Dispatcher\n10100100010000\nLocal00:0500:5809:391:36:49\n1W00:0701:1011:481:58:49\n2W00:0400:4006:211:02\n3W00:0300:3305:3357:10\n4W00:0300:3105:2454:20\n5W00:0300:3105:2554:15\nFigure 4: Comparison of each Dispatcher Model\ninguntilthedispatcherhasﬁnishedprocessingtheﬁlesin\nthe collection. Table 2 shows the increase in performance\nbased on this approach.\n4.3 Data Partitioning and Multiple Dispatchers\nWhen using one dispatcher the bottleneck is the the net-\nwork capacity of the Ethernet card on the dispatcher. To\ndemonstrate this fact we decided to partition the audio\ncollectionacrosstwodispatchers,essentiallydoublingthe\nnetworkcapacity. Inthismodelfourworkernodesarestill\nused, as well as a seperate collector node whose only job\nwas to gather the feature results from each worker. As\nexpected, the time it took to process the same size audio\ncollectionwascutinhalf. Inﬁgure 4weshowtheresults\nof processing features on an audio collection of 10,000\nﬁles, using four worker nodes for each type of dispatcher.\nTheoretically this model can be further improved by par-\ntitioning the data across even more dispatchers in an hier-\narchical fashion.\n4.4 Large Scale Experimentation\nTypically feature extraction tests run on audio collections\nof around 10,000 ﬁles. Based on our results we expect\na linear trend as collection sizes increase. To test that\nhypothesis a large-scale test using the data-partitioning\nmodel with the adaptive dispatcher was conducted on\n100,000 ﬁles. As expected, it took approximately ten\ntimestheamountoftimetocompletethe100,000ﬁletest\nas it took to complete the 10,000 ﬁle test (5:00:44).5 CONCLUSIONS-FUTURE WORK\nDistributedaudiofeatureextractionusingthedataﬂowap-\nproach exempliﬁed by Marsyas-0.2 can result in signif-\nicant savings in computation time without signiﬁcantly\nburdening the programmer. Experimental results show\nthat using 5 computers we can perform audio feature ex-\ntraction for 100,000 30-second clips in 5 hours.\nThere are many directions of future work. More\ncomputationally-intensive feature front-ends such as au-\nditory ﬁlterbanks can also be distributed. In many appli-\ncations audio feature extraction is followed by machine\nlearning. We are conducting experiments in distributed\nmachine learning and it’s integration with feature extrac-\ntion. The ability to scale to large datasets in reasonable\namounts of time will enable us to perform detailed ad-\njusting of parameters without risking overﬁtting. We are\nalso exploring the use of multi-threaded collectors and\ndispatchers to take advantage of mutiple-cpu and multi-\ncore workstations.\nACKNOWLEDGEMENTS\nOur thanks to Yvonne Coady and Dan Hoffman for their\nhelpwiththenetworkingandsystemaspectsofthiswork.\nReferences\nX.Amatriain. AnObject-OrientedMetamodelfor Digital\nSignal Processing with a focus on Audio and Music .\nPhD thesis, Univ. of Pompeu Fabra, 2005.\nA. Berenzeig, B. Logan, D. Ellis, and B. Whitman. A\nlarge scale evaluation of acoustic and subjective music\nsimilaritymeasures. In Proc.Int.Conf.onMusicInfor-\nmaiton Retrieval (ISMIR) , Baltimore, USA, 2003.\nJ.S.Downie,J.Futrelle,andD.Tcheng. Theint.musicin-\nformation retrieval system evaluation laboratory: Gov-\nernance, access, and security. In Int. Conf. on Music\nInformation Retrieval (ISMIR) , Spain, 2004.\nE. Gamma, R. Helm, R. Johnson, and J. Vlissides. De-\nsign Patterns: Elements of Reusable Object-Oriented\nSoftware. Addison Wesley, 1995.\nW. Johnston, J.R. Paul Hanna, and R.J. Millar. Advances\nin dataﬂow programming languages. ACM Computing\nSurveys, 36(1):1–34, March 2004.\nD. Manolescu. A data ﬂow pattern language. In Pro-\nceedingsofthe4thPatternLanguagesofProgramming ,\nMonticello, Illinois, September 1997.\nG. Tzanetakis and P. Cook. Musical Genre Classiﬁcation\nof Audio Signals. IEEE Trans. on Speech and Audio\nProcessing , 10(5), July 2002.\nW. Wadge and E.A. Ashcroft. Lucid, the dataﬂow pro-\ngramminglanguage . APICStudiesinDataProcessing.\nAcademic Press, New York, NY, 1985.\nG. Wood and S. Keefe. A case study of distributed mu-\nsic analysis using the geddei processing framework. In\nProc. Int. Conf. on Music Information Retrieval (IS-\nMIR), pages 275–276, Barcolona, Spain, 2004.\n437"
    },
    {
        "title": "Learning Harmonic Relationships in Digital Audio with Dirichlet-Based Hidden Markov Models.",
        "author": [
            "John Ashley Burgoyne",
            "Lawrence K. Saul"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414870",
        "url": "https://doi.org/10.5281/zenodo.1414870",
        "ee": "https://zenodo.org/records/1414870/files/BurgoyneS05.pdf",
        "abstract": "Harmonic analysis is a standard musicological tool for understanding many pieces of Western classical music and making comparisons among them. Traditionally, this analysis is done on paper scores, and most past research in machine-assisted analysis has begun with digital representations of them. Human music students are also taught to hear their musical analyses, however, in both musical recordings and performances. Our approach attempts to teach machines to do the same, beginning with a corpus of recorded Mozart symphonies. The audio files are first transformed into an ordered series of normalized pitch class profile (PCP) vectors. Simplified rules of tonal harmony are encoded in a transition matrix. Classical music tends to change key more frequently than popular music, and so these rules account not only for chords, as most previous work has done, but also for the keys in which they function. A hidden Markov model (HMM) is used with this transition matrix to train Dirichlet distributions for major and minor keys on the PCP vectors. The system tracks chords and keys successfully and shows promise for a real-time implementation. Keywords: Dirichlet, harmony, PCP, HMM, Mozart 1",
        "zenodo_id": 1414870,
        "dblp_key": "conf/ismir/BurgoyneS05",
        "keywords": [
            "Harmonic analysis",
            "Western classical music",
            "pitch class profile (PCP)",
            "digital representations",
            "machine-assisted analysis",
            "corpus of recorded Mozart symphonies",
            "ordered series of normalized pitch class profile (PCP) vectors",
            "simplified rules of tonal harmony",
            "transition matrix",
            "hidden Markov model (HMM)"
        ],
        "content": "LEARNING HARMONIC RELATIONSHIPS IN DIGITAL AUDIO WITH\nDIRICHLET-BASED HIDDEN MARKOV MODELS\nJ. Ashley Burgoyne and Lawrence K. Saul\nDepartment of Computer and Information Science\nUniversity of Pennsylvania, Philadelphia, PA 19104 USA\n{burgoyne,lsaul }@cis.upenn.edu\nABSTRACT\nHarmonic analysis is a standard musicological tool for\nunderstanding many pieces of Western classical music\nand making comparisons among them. Traditionally, this\nanalysis is done on paper scores, and most past research\nin machine-assisted analysis has begun with digital repre-\nsentations of them. Human music students are also taught\nto hear their musical analyses, however, in both musical\nrecordings and performances. Our approach attempts to\nteach machines to do the same, beginning with a corpus\nof recorded Mozart symphonies. The audio ﬁles are ﬁrst\ntransformed into an ordered series of normalized pitch\nclass proﬁle (PCP) vectors. Simpliﬁed rules of tonal har-\nmony are encoded in a transition matrix. Classical music\ntends to change key more frequently than popular music,\nand so these rules account not only for chords, as most\nprevious work has done, but also for the keys in which\nthey function. A hidden Markov model (HMM) is used\nwith this transition matrix to train Dirichlet distributions\nfor major and minor keys on the PCP vectors. The system\ntracks chords and keys successfully and shows promise\nfor a real-time implementation.\nKeywords: Dirichlet, harmony, PCP, HMM, Mozart\n1 INTRODUCTION\nMachine-assisted harmonic analysis has a long history\nand continues to spawn active research [1, 2]. Much of\nthat research, however, has focused on digital analogues\nto the symbols on a paper score [1, 3, 4, 5]. We choose\nto begin with sound, using models drawn from the speech\nand signal processing communities.\nThe project shares ground with Christopher Raphael’s\nwork on automatic transcription of piano music [6], al-\nthough the “harmonic” space of his problem domain en-\njoys less a priori structure. Its inspiration is Alexan-\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonder Sheh and Daniel Ellis’s chord recognition project for\nsongs by the Beatles [7]. We begin with a hidden Markov\nmodel, as do Sheh and Ellis, but replace the more tradi-\ntional Gaussian emission distributions with Dirichlet dis-\ntributions. Dirichlet distributions have properties partic-\nularly well suited to recognizing chords and are the key\nadvance made in our work. They also bring the project\ncloser to several recent key tracking algorithms for audio\ndata [8, 9], and in a second important departure from Sheh\nand Ellis, we choose a more complex harmonic model,\nrooted in traditional music theory, that enables the system\nto track key simultaneously with chord.\nThe second section of this paper outlines the theoret-\nical background necessary for understanding the model,\nthe third discusses the details of implementation and the\nresults, and the fourth offers ideas for further develope-\nment.\n2 THEORETICAL BACKGROUND\n2.1 Hidden Markov Models\nHidden Markov models (HMMs) are a family of statistical\nmodels that have proven very useful for speech recogni-\ntion and certain tasks in robotics and are growing increas-\ningly popular for musical problems. They are deﬁned by a\ndiscrete state space Sthat cannot be observed directly but\nis assumed to generate a set of possibly multidimensional\nand continuous observations, a complete set of transition\nprobabilities between these states as time passes, and a\nformula for computing the probability of any observation\ngiven some state in S. The simplest of these models – and\nours – make the ﬁrst-order Markov assumption that for a\ntime-ordered set of random state variables S1,...,ST∈S,\nP(St+1|S1,...,St) =P(St+1|St). (1)\nUnder this assumption, the transition probabilities can be\nstored in matrix form.\n2.2 Harmonic Space\nMuch previous work either assumes that the music ana-\nlyzed will remain in a single key from start to ﬁnish or\ndisregards the notion of key altogether, tackling the chord\nrecognition problem explicitly or considering transitions\nbetween chords independent of their tonal context. Al-\n438though it is frequently necessary to make simplifying as-\nsumptions, these ones are especially limiting. Western\nclassical music modulates frequently, and contemporary\ntonal theory rests on the assumption that it is not merely\nlocal conﬁgurations of pitches that create musical mean-\ning but also the contexts in which these conﬁgurations\narise. A D major chord in the key of G major should yield\na PCP similar to that of a E /flat/flatmajor chord in the key of\nD/flatmajor, but quite different harmonies (and correspond-\ning PCPs) are likely to follow. Our model thus considers\nchord and key to be inseparable properties of any given\nharmony.\nAnother group of previous work attempts only to iden-\ntify the prevailing key. Most of these models are based on\nCarol Krumhansl’s probe-tone key proﬁles or V os and van\nGeenen’s derivative algorithm [10, 5], although there have\nbeen interesting alternatives using Elaine Chew’s spiral\nmodel [11]. By choosing a different model, we are in\nsome sense attempting to relearn proﬁles from the ground\nup: see also [12].\nFor ease of implementation and training, the model is\nrestricted to major and minor triads only, ignoring aug-\nmented and diminished triads as well as sevenths, ninths,\nand other additions. A full range of chromatic alterations,\nhowever, are available within each key. Following Ald-\nwell and Schachter [13], we divide the triads into four\ngroups based on their degree of “mixture.” The diatonic\nchords within each key employ no mixture. Primary mix-\nture accounts for chords that are “borrowed” from the par-\nallel major or minor.1Secondary mixture describes other\nchromatic alterations of the third or ﬁfth of the diatonic tri-\nads. Double mixture, as its name implies, includes both a\nborrowing and a further alteration of the third of ﬁfth. Ta-\nble 1 lists the chords included in the model and the types\nof mixture necessary to produce them.\n2.3 Pitch Class Proﬁles\nOne of the primary challenges of the project is to work\nwith digitized audio rather than a score-like format.\nAmong the many useful features that can be computed\nfrom audio are pitch class proﬁles (PCPs) [14], which\nSheh and Ellis found to perform signiﬁcantly better than\nseveral popular alternatives. The computation of PCPs\nbegins with a windowed short-term Fourier transform\n(STFT):\nXSTFT[k,n] =N−1\n∑\nm=0x[n−m]w[m]e−j(2π/N)km.(2)\nwhere nis the index of the edge of a window of length\nNin the discrete time series x[·],wis a discrete window-\ning function of length N, and kindexes the frequency axis\nfrom DC to the Nyquist frequency. The values of the\nSTFT for each bin are squared to generate a power spec-\ntrum and then mapped to the musical pitch class closest to\nthe frequency of the bin. For each window, these squared\n1Aldwell and Schachter consider IV and V in minor to be\ninstances of primary mixture but note that they are essential to\nthe the key; we consider them to be native to minor keys for the\npurposes of our model.Major Minor\nI none primary\ni primary none\n/flatII primary none\n/flatii double secondary\nII secondary double\nii none primary\n/flatIII primary none\n/flatiii double secondary\nIII secondary double\niii none primary\nIV none none\niv primary none\nV none none\nv primary none\n/flatVI primary none\n/flatvi double secondary\nVI secondary double\nvi none primary\n/flatVII primary none\n/flatvii double secondary\nVII secondary double\nvii secondary double\nTable 1: Tonal vocabulary and classiﬁcation of mixture.\nvalues are summed over the pitch class labels to gener-\nate a twelve-dimensional PCP. The PCPs then represent\nthe total amount of spectral energy in each musical pitch\nclass at regularly sampled points in time. They are similar\nto Tzanetakis, Ermolinskyi, and Cook’s pitch histograms\n[15] but with a more localized scope.\nAn alternative calculation of PCPs might begin with\nthe constant Q transform, which has had some success\nin chord identiﬁcation and the related task of polyphonic\npitch tracking [16, 17].\n2.4 Dirichlet Distributions\nA Dirichlet distribution is a probability distribution over\na set of discrete probability distributions. It is the con-\njugate prior of the multinomial distribution, which is a\ngeneralization of the binomial distribution from a binary\ndecision to a set of nalternatives. If we label the probabil-\nities of choosing each of these alternatives /vectorp=p1,..., pn,\n∑n\ni=1pi=1 and pi>0∀i, then a Dirichlet distribution\nwith parameters /vectoru=u1,...,un,ui>0∀iis deﬁned as\nDir(/vectorp,/vectoru)/defines1\nZ(/vectoru)n\n∏\ni=1pui−1\ni . (3)\nTheZ(/vectoru)term is a normalization constant deﬁned as\nZ(/vectoru)/defines∏n\ni=1Γ(ui)\nΓ(∑n\ni=1ui)(4)\nwhere Γrepresents the standard gamma function\nΓ(x)/defines/integraldisplay∞\n0dte−ttx−1. (5)\n439Major chords\n Minor chords\n(a) Initial Dirichlet parameters, rotated for C major and minor. These parameters were estimated from synthesized chords\nof four sawtooth waves within the standard playing frequency range of the Western classical orchestra. The shapes of the\ndistributions illustrate the mean distribution (see Eq. 6) while the parameter values are inversely proportional to the variance.\nMajor chords\n Minor chords\n(b) Trained Dirichlet parameters, rotated for C major and minor. These parameters were estimated from the training corpus\nusing the EM algorithm seeded with the parameters above. The distribution shapes remain unmistakably major and minor\ntriads; variance has increased considerably.\nFigure 1: Dirichlet parameters before and after training.\n440The mean of a Dirichlet distribution is the normalization\nof its parameters:\n/angbracketleft/vectorp/angbracketright=/vectoru\n∑n\ni=1ui. (6)\nIts variance decreases as the magnitude of /vectoruincreases.\nDirichlet distributions are more attractive than Gaus-\nsian models for systems where the relations among out-\nputs are more important than their magnitude. PCP vec-\ntors are a good example of this sort of system: what de-\nﬁnes a C major chord is not that C, E, and G are sounding\nloudly but that they are relatively louder than any other\npitch classes sounding at the time. Although there have\nbeen successes with Gaussian models for chord recogni-\ntion, because Dirichlet distributions model the underlying\nphenomenon more accurately, we expect an improvement\nin performance. In order to use them, all PCP vectors must\nbe normalized such that their components sum to unity.\n3 IMPLEMENTATION AND RESULTS\nThe implemented HMM draws its states from the har-\nmonic space outlined in Section 2.1, derives a transition\nmatrix from the mixture classiﬁcations, accepts ordered\nsets of normalized PCP vectors as observation data, and\nparameterizes the observation distributions as Dirichlet\ndistributions tied to the underlying states.\nThe transition matrix is deﬁned by ﬁve hand-tuned\nparameters: pk, the probability of remaining within the\ncurrent key, and pd1>pd2>pd3>pd4, the probabilities\nof remaining within a harmony when it employs no, pri-\nmary, secondary, and double mixture. All other entries\nare set uniformly with respect to these constraints and\nthe “pivot region” constraints outlined by Fred Lerdahl in\nTonal Pitch Space [18]. Lerdahl’s constraints allow major\nkeys to modulate only to the related keys i, ii, iii, IV , V ,\nvi and minor keys only to I, /flatIII, iv, v, /flatVI, and /flatVII, and\neven in these cases, at least one of the chords must be a\ntonic.\nIn order to maximize the utility of the training data,\nour system deﬁnes only two Dirichlet parameter vectors,\n/vectorumajor and/vectoruminor. To get the expected obervation distribu-\ntion for any given harmony, the base parameter vector for\nits mode is rotated until the root of the parameter vector\nmatches the root of the harmony. Note that triads share the\nsame observation distribution regardless of their key: all\nthe information for tracking keys must be encoded in the\ntransition matrix. Figure 1(a) shows our initial parame-\nters for the Dirichlet vectors, standardized on C major and\nminor. Their overall shapes illustrate the expected distri-\nbutions, as per Equation 6, and the high magnitude repre-\nsents a high conﬁdence level in these initial selections.\nThese initial estimates were estimated from a ran-\ndomly generated four-note chords composed of sawtooth\nwaves from across the orchestral range of frequencies.\nThirds and ﬁfths were alternately doubled. Modulo the\ndownsampling to 11,025 Hz and conversion to mono nec-\nessary to make computation on the CD audio ﬁles in our\ncorpus tractable, the random samples were processed ex-\nactly like the training data. First, they were broken into\nwindows of 2765 samples (250 ms) with a 50 percentoverlap. We tuned a Gaussian windowing function\nw[k] =e−1\n2/parenleftBig\nα·k−N/2\nN/2/parenrightBig\n(7)\nand zero-padded the STFT to 4096 samples. Setting αto\n1.3 yields a main lobe width of 3.7 Hz with a leakage fac-\ntor of 2.3 percent, allowing us to discard only frequency\ninformation below MIDI note 36 (65.4 Hz) while preserv-\ning the integrity of the PCP vectors. Any vectors whose\ntotal energy after this processing fell below 1e-6 were re-\nmoved – in most cases, the leading and trailing samples of\neach track.\nThe training corpus comprised professional compact\ndisc recordings of ﬁve Mozart symphonies in ﬁfteen\nmovements altogether with a total duration of 70”39’:\nSymphony No. 21 in A (K. 134), Symphony No. 22 in\nC (K. 162), Symphony No. 23 in D (K. 181), Symphony\nNo. 24 in B /flat(K. 182), and Symphony No. 25 in G mi-\nnor (K. 183) [19]. After processing, the expectation max-\nimization (EM) algorithm was used to tune the Dirich-\nlet parameters. The algorithm converges very quickly on\nthese data, and to avoid overtraining, we limited the algo-\nrithm to ﬁve iterations.\nGraphs of the trained Dirichlet parameters are in Fig-\nure 1(b). Although the variance has grown much greater\nfor experience with noisier, real-world data, the triadic\nproﬁles are clear. The third of minor chords stands out,\nsuggesting correctly that in the absence of deﬁning in-\nformation, major chords should be the default predic-\ntion. The relative lack of prominence of the ﬁfth of minor\nchords can be explained by the predominance of dimin-\nished triads in the corpus. There is no corresponding state\nin the model, and the system usually guesses that they are\nminor triads instead, enabled by this weaker emphasis on\nhaving a perfect ﬁfth. Unfortunately, these guesses can\nmake poor harmonic sense – when vii◦is functioning as a\ndominant, for example – and it will be worthwhile to ex-\ntend the model to account for diminished triads directly.\nThe system was tested on a recording of the Minuet\nfrom Mozart’s Symphony No. 40 in G minor (K. 550)\nfrom the same boxed set. Figure 2(a) includes a con-\ndensed excerpt from the score. Figure 2(b) underneath\ndisplays a matrix of the corresponding PCP distributions\nfrom the ﬁrst time through the repeat; it is scaled linearly\nin time, not necessarily with the score above it. Figure 2(c)\nincludes a ground truth harmonic analysis of the excerpt\ngiven the limitations of our model to major and minor tri-\nads; chords are in the top row in roman type and the keys\nunderneath them in boldface. The phrase modulates from\nG minor to D minor with some chromatic trickery in the\nsecond half as well as a number of suspensions and other\nnon-harmonic tones. Also notice the paucity of well artic-\nulated triads in the PCP plot despite the fully orchestrated\ntexture above.\nUnderneath the ground truth in Figure 2(c) are the\nchord and key sequences predicted by the model for both\nthe ﬁrst and second passes through repeat in the recording\n(without timing data). The free chord identiﬁcation results\nare excellent. The model fails to identify only 4 of the 24\nharmonies in this example (17 percent), comparable with\nprevious work, and these are “good” mistakes. The ﬁrst\nerror is g for E /flatin m. 7, which is almost a viable alter-\n441gDggDgggDgggDgEbg/dAdddgdAdBbdCdDdgdAdddEdAdBb\u0000dedAddd\n(a) Reduced musical score.\n\t\nTime (s)Pitch class\n2.5 5.0 7.5 10.0 12.5 15.0CDEFGAB\nStudent Version of MATLAB\n(b) PCP distributions (ﬁrst repeat only), scaled linearly in time. Note the lack of obvious triads.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \nGround truth:D\ng\nD\ng\ng\nD\ng D\nE/flat\nA\nd g A\nB/flatC D\ng A d\nE A B /flat\ne A\nd\ng\ng\ng\ng\ng\ng\ng g\ng/d\nd\nd d d\nd d d\nd d d\nd d d\nd d\nd\nFirst repeat:D\nG g\nD\nG g\ng\nD\ng D\nG g\nc/sharpA\nd g A\nd C g D\ng c /sharpd\nE A d\ne A\nD\ng\ng g\nd\nd d\nd\nd\nd d\nd d\nd d\nd d d\nd d d d\nd d d\nd d d\nd d\nd\nSecond repeat:D\nG g\nD\ng G\ng\nD\ng D\nG g\ne A\nd g A\nd C g\ng b /flatd\nE A d\ne A\nD\nd\nd d\nd\nd d\nd\nd\nd d\nd g\ng g\ng g g\ng g g\ng g g\ng g g\ng g\ng\n(c) Ground truth harmonic labels and machine analysis. The results are especially impresive considering the few well deﬁned triads above: clearly, the harmonic model is\nworking.\nFigure 2: Symphony No. 40 (K. 550), III, mm. 1–14.\n442native analysis. It misses two other deceptive cadence ﬁg-\nures, one in m. 10 and other in m. 12, mistaking /flatVI for the\ntonic in both cases. These mistakes are again reasonable:\nthe submediant in deceptive cadences is meant to mimic a\ntonic. It would have been almost impossible for our model\nto ﬁgure out that it could label the c /sharp◦7in m. 11 as A, and\nits guesses of c /sharpthe ﬁrst time and b /flatthe second each make\nthe diminished-ﬁfth compromise discussed earlier.\nThe key ﬁnding results are less impressive but a\nmarked improvement from before training. G minor and\nD minor are very closely related keys, and with so much\nchromaticism in the second half of the phrase, it is un-\nderstandable that the repeat would have confused the sys-\ntem. Moreover, because of the way in which harmonic\nstates are tied to the Dirichlet parameters, the system al-\nmost has to determine the chord sequence a priori and\nthen try to match a smooth progression of keys beneath\nit using just the rudimentary information contained in the\ntransition matrix. A more advanced harmonic model is\nneeded.\n4 CONCLUSIONS AND FUTURE WORK\nDirichlet distributions on PCP vectors are an efﬁcient and\neffective means for chord recognition in recorded perfor-\nmances of symphonic music, and their accuracy should\nimprove further with more sophisticated harmonic tran-\nsition models like Lerdahl’s or Raphael’s. Our current\nsystem could be adapted for real-time performance easily\nusing a belief-updating network. Because it is an unsuper-\nvised framework, it also lends itself to incorporating more\ntraining data in the future, perhaps enough data to learn\nmore sophisticated harmonic models on its own.\nACKNOWLEDGEMENTS\nThis work was supported by NSF Award 0238323.\nREFERENCES\n[1] H. Christopher Longuet-Higgins and Mark J. Steed-\nman. On interpreting Bach. Machine Intelligence ,\n6:221–41, 1971.\n[2] Robert Rowe. Machine Musicianship . MIT Press,\nCambridge, MA, 2001.\n[3] Christopher Raphael. Harmonic analysis with prob-\nabilistic graphical models. In Proceedings of the\nInternational Symposium on Music Information Re-\ntrieval , Baltimore, MD, 2003.\n[4] David Temperley. The Cognition of Basic Musical\nStructures . MIT Press, Cambridge, MA, 2001.\n[5] Piet G. V os and E. W. van Geenen. A parallel-\nprocessing key-ﬁnding model. Music Perception ,\n14:185–224, 1996.\n[6] Christopher Raphael. Automatic transcription of\npiano music. In Proceedings of the International\nSymposium on Music Information Retrieval , Paris,\nFrance, 2002.[7] Alexander Sheh and Daniel P. W. Ellis. Chord seg-\nmentation and recognition using EM-trained hidden\nMarkov models. In Proceedings of the International\nSymposium on Music Information Retrieval , Balti-\nmore, MD, 2003.\n[8] Hendrik Purwins, Benjamin Blankertz, and Klaus\nObermayer. A new method for tracking modulations\nin tonal music in audio data format. In C. L. Giles,\nM. Gori, and V . Piuri, editors, Proceedings of the\nInternational Joint Conference on Neural Networks ,\nvolume 6, 2000.\n[9] Steffen Pauws. Musical key extraction from au-\ndio. In Proceedings of the International Conference\non Music Information Retrieval , Barcelona, Spain,\n2004.\n[10] Carol L. Krumhansl. Cognitive Foundations of Mu-\nsical Pitch . Oxford University Press, New York,\n1990.\n[11] Ching-Hua Chan and Elaine Chew. Polyphonic au-\ndio key-ﬁnding using the spiral array CEG algo-\nrithm. In Proceedings of the International Confer-\nence on Multimedia and Expo , Amsterdam, Nether-\nlands, 2004.\n[12] Samer A. Abdallah. Towards Music Perception\nby Redundancy Reduction and Unsupervised Learn-\ning in Probabilistic Models . Doctoral thesis, De-\npartment of Electronic Engineering, King’s College\nLondon, 2002.\n[13] Edward Aldwell and Carl Schachter. Harmony and\nVoice Leading . Harcourt Brace Javanovich, New\nYork, 1978.\n[14] Takuya Fujishama. Real-time chord recognition of\nmusical sound: A system using Common Lisp Mu-\nsic. In Proceedings of the International Computer\nMusic Conference , Beijing, 1999.\n[15] George Tzanetakis, Andrey Ermolinskyi, and Perry\nCook. Pitch histograms in symbolic and audio mu-\nsic information retrieval. Journal of New Music Re-\nsearch , 32(2):143–52, 2003.\n[16] S. Hamid Nawab, Salma Abu Ayyash, and Robert\nWotiz. Identiﬁcation of musical chords using\nconstant-Q spectra. In Proceedings of the Interna-\ntional Conference on Acoustics, Speech, and Signal\nProcessing , Salt Lake City, UT, 2001.\n[17] Ozgur Izmirli. A hierarchical constant Q transform\nfor partial tracking in musical signals. In Proceed-\nings of the 2nd COST G-6 Workshop on Digital Au-\ndio Effects , Trondheim, Norway, 1999.\n[18] Fred Lerdahl. Tonal Pitch Space . Oxford University\nPress, New York, 2001.\n[19] Neville Marriner and the Academy of St. Martin-in-\nthe Fields. Mozart: Early symphonies. Compact disc\nrecording, 1990. Philips 4225012.\n443"
    },
    {
        "title": "Automatic X Traditional Descriptor Extraction: the Case of Chord Recognition.",
        "author": [
            "Giordano Cabral 0001",
            "François Pachet",
            "Jean-Pierre Briot"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415702",
        "url": "https://doi.org/10.5281/zenodo.1415702",
        "ee": "https://zenodo.org/records/1415702/files/CabralPB05.pdf",
        "abstract": "Audio descriptor extraction is the activity of finding mathematical models which describe properties of the sound, requiring signal processing skills. The scientific literature presents a vast collection of descriptors (e.g. energy, tempo, tonality) each one representing a significant effort of research in finding an appropriate descriptor for a particular application. The Extractor Discovery System (EDS)  [1] is a recent approach for the discovery of such descriptors, which aim is to extract them automatically. This system can be useful for both non experts – who can let the system work fully automatically – and experts – who can start the system with an initial solution expecting it to enhance their results. Nevertheless, EDS still needs to be massively tested. We consider that its comparison with the results of problems already studied would be very useful to validate it as an effective tool. This work intends to perform the first part of this validation, comparing the results from classic approaches with EDS results when operated by a completely naïve user building a guitar chord recognizer.",
        "zenodo_id": 1415702,
        "dblp_key": "conf/ismir/CabralPB05",
        "keywords": [
            "audio descriptor extraction",
            "signal processing skills",
            "vast collection of descriptors",
            "scientific literature",
            "discovery of descriptors",
            "automatic extraction",
            "non experts",
            "expertise",
            "Extractor Discovery System (EDS)",
            "validation"
        ],
        "content": "AUTOMATIC X TRADITIONAL DESCRIPTOR EXTRACTION: THE \nCASE OF CHORD RECOGNITION \nGiordano Cabral François Pachet Jean-Pierre Briot \nLIP6 – Paris 6 \n8 Rue du Capitaine Scott \n75018 Paris \nGiordano.CABRAL@lip6.fr  Sony CSL Paris \n6 Rue Amyot  \n75005 Paris \npachet@csl.sony.fr  LIP6 – Paris 6 \n8 Rue du Capitaine Scott \n75018 Paris \nJean-Pierre.BRIOT@lip6.fr  \nABSTRACT \nAudio descriptor extraction is the activity of find ing \nmathematical models which describe properties of th e \nsound, requiring signal processing skills. The scie ntific \nliterature presents a vast collection of descriptor s (e.g. \nenergy, tempo, tonality) each one representing a si gnifi-\ncant effort of research in finding an appropriate d escrip-\ntor for a particular application. The Extractor Dis covery \nSystem (EDS)  [1] is a recent approach for the disc overy \nof such descriptors, which aim is to extract them a uto-\nmatically. This system can be useful for both non e xperts \n– who can let the system work fully automatically –  and \nexperts – who can start the system with an initial solution \nexpecting it to enhance their results. Nevertheless , EDS \nstill needs to be massively tested. We consider tha t its \ncomparison with the results of problems already stu died \nwould be very useful to validate it as an effective  tool. \nThis work intends to perform the first part of this  valida-\ntion, comparing the results from classic approaches  with \nEDS results when operated by a completely naïve use r \nbuilding a guitar chord recognizer. \n \nKeywords: descriptor extraction, chord recognition. \n1 INTRODUCTION \nAudio descriptors express by a mathematical formula  a \nparticular property of the sound. Such a property m ay be \nfor example the tonality of a musical piece, the am ount \nof energy in a given moment, or whether a song is i n-\nstrumental or sung. Although the creation of each d e-\nscriptor means a different study, the design of a d escrip-\ntor extractor normally follows the process of combi ning \nthe relevant characteristics of acoustic signals (f eatures) \nusing machine learning algorithms. These features a re \noften low-level descriptors (LLD), and the task usu ally \nrequires important signal processing knowledge. \nSince last year, a heuristic-based approach became \navailable through the Computer Science Lab of Sony in Paris, which developed the Extractor Discovery Syst em \n(EDS). The system is based on genetic programming, \nand machine learning algorithms employed to automat i-\ncally generate a descriptor from a database of soun d files \nexamples and their respective perceptive values. ED S \ncan be used by non experts or expert users. Non exp erts \ncan use it as a tool to extract descriptors, even w ith \nminimal or no knowledge at all in signal processing . For \nexample, movie makers have created classifiers of s ound \nsamples to be used in their films (explosions, car breaks, \netc.). Experts can use the system to improve their results, \nstarting from their solution and then controlling a nd \nguiding EDS. For instance, the perceived intensity of \nmusic titles can be more precisely detected, taking  as a \nstarting point the mpeg7 audio features  [2]. \nWe are currently designing a guitar accompanier for  \n“bossa nova” style. During the application developm ent \nprocess, we ran into the problem of recognising a c hord, \nwhich turns out to be a good opportunity to compare  \nclassical and EDS approaches. On the one hand, chor d \nrecognition is a well studied domain, with solid re sults \nthat can be considered as reference. On the other h and, \ncurrent techniques use background knowledge that ED S \n(initially) does not have (pitches, harmony). Good EDS \nresults would indicate the capacity of the system t o deal \nwith real world musical description cases. \nThis paper presents a comparison between a standard  \ntechnique to chord recognition (knn learner over pi tch \nclass profiles) and an EDS solution performed by an  \ninexperienced, naïve user. In the next section, we intro-\nduce the chord recognition problem. In section 3 we  \nexplain the most widely used technique. In section 4 we \nexamine EDS, how it works and how to use it. Sectio n 5 \ndetails the experiment. Section 6 shows and discuss  the \nresults. Finally, we draw some conclusions and poin t \nfuture works. \n2 CHORD RECOGNITION \nThe ability of recognizing chords is important for many \napplications, such as interactive musical systems, con-\ntent-based musical information retrieval (finding p articu-\nlar examples, or themes, in large audio databases),  and \neducational software. Chord recognition means the t ran-\nscription of a sound into a chord, which can be cla ssified \naccording to different levels of precision, from a simple \ndistinction between maj and min chords to a complex  set \nof chord types (maj, min, 7th, dim, aug, etc).  \n Permission to make digital or hard copies of all or  part of this \nwork for personal or classroom use is granted witho ut fee pro-\nvided that copies are not made or distributed for p rofit or com-\nmercial advantage and that copies bear this notice and the full \ncitation on the first page. \n© 2005 Queen Mary, University of London \n444   \n \n Many works can be mentioned here as the state of th e \nart.  [4] and  [5] automatically transcribes chords  from a \nCD recorded song.  [3] deals with a similar problem : \nestimating the tonality of a piece (which is analog ous to \nthe maj/min). In these cases and in most part of th e lit-\nerature the same core technique is used, even if so me \nvariations may appear during the implementation pha se. \nThis technique has been applied to our problem. We \nexplain it in the next section. \n3 PITCH CLASS PROFILE \nMost part of the works involving harmonic content \n(chord recognition, chord segmentation, tonality es tima-\ntion) uses a feature called Pitch Class Profile (PC P)  [6]. \nPCPs are vectors of low-level instantaneous feature s, \nrepresenting the intensity of each pitch of the ton al scale \nmapped to a single octave. This intensity can be ca lcu-\nlated by the magnitude of the spectral peaks, or by  sum-\nming the magnitudes of all frequency bins that are lo-\ncated within a certain frequency band. Each frequen cy \nband corresponds to a pitch, and may change to deal  with \ndifferences in tuning and/or to gain in performance . The \nequivalent pitches from different octaves are summe d, \nproducing a vector of 12 values, consequentially un ifying \nvarious dispositions of a single chord class. \n \n \n \nFigure 1. Examples of PCPs for a Amaj7 (above) \nand a Cmaj7 (below). \nThe idea is that the PCPs of a chord follow a patte rn, and \nthat pattern can be learned from examples. Machine \nlearning (ML)  [9] techniques are used to generaliz e a \nclassification model from a given database of label led \nexamples, and then new examples can be automaticall y \nclassified. The original PCP implementation from Fu -\njishima used a KNN learner  [9], and more recent wo rks \n [3] successfully used other machine learning algor ithms. \n 4 EDS \nEDS (Extractor Discovery System), developed at Sony  \nCSL, is a heuristic-based generic approach for auto mati-\ncally extracting high-level music descriptors from acous-\ntic signals. EDS approach is based on Genetic Progr am-\nming  [11], used to build extraction functions as c omposi-\ntions of basic mathematical and signal processing o pera-\ntors. Given a database of audio signals with their associ-\nated perceptive values, EDS is capable to generaliz e a \ndescriptor. Such descriptor is built by running a g enetic \nsearch to find relevant signal processing features to \nmatch the description problem, and then machine lea rn-\ning algorithms to combine those features into a gen eral \ndescriptor model. \n \n \nFigure 2. EDS main interface. \nThe genetic search performed by the system is inten ded \nto generate functions that may eventually be releva nt to \nthe problem. The best functions in a population are  se-\nlected and iteratively transformed (by means of rep ro-\nduction, i.e., constant variations, mutations, and/ or cross-\nover), always respecting the pattern chosen by the user. \nThe default pattern is !_x(Testwav) , which means a func-\ntion presenting any number of operations but a sing le \nvalue as result. The populations of functions repro duce \nuntil no improvement is found.  \nAt this point, the best functions are selected to b e \ncombined. This selection can be made both manually or \nautomatically. For example, given a database of aud io \nfiles labeled as ‘voice’/‘instrumental’, kept the d efault \npattern, these are some possible functions that mig ht be \nselected by the system: \n \nLog10  (Range (Derivation  (Sqrt  (Blackman  \n(MelBands  (Testwav, 24.0)))))) \n \nSquare  (Log10  (Mean  (Min  (Fft  \n(Split (Testwav, 4009)))))) \nFigure 3 . Some possible EDS functions for the \ndefault pattern. \nThe final step in the extraction process is to choo se and \ncompute a model (linear regression, model trees, kn n, \nlocally weighted regression, neural networks, etc.) . Al-\n445   \n \n ternatively, the user can choose the option test and opti-\nmize all classification methods . As the output, EDS cre-\nates an executable file, which classifies an audio file \npassed as argument. \n5 BOSSA NOVA GUITAR CHORDS \nOur final goal is to create a guitar accompanier in  Brazil-\nian “bossa nova” style; consequently our chord reco g-\nniser has examples of chords played with nylon guit ar. \nThe data was taken from D’accord Guitar Chord Data-\nbase  [10], a guitar midi based chord database. The  pur-\npose of using it was the richness of the symbolic i nfor-\nmation present (chord root, type, set of notes, pos ition, \nfingers, etc.), which was very useful for labelling  the data \nand validating the results. Each midi chord was ren dered \ninto a wav file using Timidity++  [13] and a free n ylon \nguitar patch, and the EDS database was created acco rd-\ning to the information found in D’accord Guitar dat a-\nbase. Even though a midi-based database may lead to  \ndistortions in the results, we judge that the compa rison \nbetween approaches is still valid. \n5.1 Chord Classes \nWe tested the solutions with some different dataset s, \nreflecting the variety of nuances that chord recogn ition \nmay show: \nAMaj/Min –classifies between major and minor \nchords, given the root is A. 101 samples, 2 classes . \nChord Type, fixed root – classifies among major, mi-\nnor, seventh, minor seventh and diminished chords, \ngiven it is a fixed root (A or C). 262 samples, 5 c lasses, \nChord Recognition – classifies major, minor, seventh, \nminor seventh and diminished chords, in any root. 1 885 \nsamples, 60 classes. \n80% of each database is settled on as the training \ndataset and 20% as the testing dataset. \n5.2 Pitch Class Profile \nIn our implementation of the pitch class profile, f re-\nquency to pitch mapping is achieved using the logar ith-\nmic characteristic of the equal temperament scale.  \nThe intensity of each pitch is computed by summing \nthe magnitude of all frequency bins that correspond  to a \nparticular pitch class. The same computation is app lied \nto a white noise and the result is used to normaliz e the \nother PCPs. \nFor the chord recognition database, PCPs were ro-\ntated, meaning that each PCP was computed 12 times,  \none time for each possible rotation (for instance, a Bm is \nequivalent to a Am rotated twice). \nAfter the PCP extraction, several machine learning \nalgorithms could be applied. We implemented 2 simpl e \nsolutions. The first one calculates a default, or a  tem-\nplate PCP to each chord class. Then, the PCP of a n ew \nexample can be matched up to the template PCP, and the \nmost similar one is retrieved as the chord. \n \nFigure 4. Example of a template PCP for a C chord \nclass. \n \nThe second one uses the k-nearest neighbours algo-\nrithm (KNN), with maximum of 3 neighbours. KNNs \nhave been used since the original PCP implementatio n \nand have proved to be at least one of the best lear ning \nalgorithms for this case  [3]. \n5.3 EDS \nThe same databases were loaded in EDS. We ran a \nfully automated extraction, keeping all default val ues. \nThe system generated the descriptor without any hel p \nfrom the user, obtaining the results we call EDS Na ïve, \nbecause they correspond to the results that a naïve  user \nwould achieve. \n6 RESULTS AND DISCUSSION \nThe results achieved by us are presented in the tab le \nabove. Rows represent the different databases. Colu mns \nrepresent the different learning techniques. The pe rcent \nvalues indicate the number of correctly classified in-\nstances over the total number of examples in the te sting \ndatabase. \nTable 1. Percentage of correctly classified in-\nstances for the different databases using the stud-\nied approaches. \nApproach  \nDatabase PCP  \nTemplate KNN EDS \nMaj/Min (fixed \nroot) 100% 100% 90.91% \nChord Type (fixed \nroot) 89% 90.62% 87.5% \nChord Recognition 53.85% 63.93% 40.31% \n \nAs we can see, EDS gets really close to classical a p-\nproaches when the root is known, but disappoints wh en \nthe whole problem is presented. It seems that a com bina-\ntion of low level functions is capable of finding d ifferent \npatterns in the same root, but the current palette of signal \nprocessing functions in EDS is not sufficient to ge neral-\nize harmonic information. \n446   \n \n 6.1 Case 1: Major/Minor classifier, fixed root. \nFigure 5 shows the selected features for the Amaj/m in \ndatabase. The best model obtained was a KNN of 1 ne ar-\nest neighbour, equally weighted, absolute error (se e  [9] \nfor details). The descriptor reached 90.91% of the per-\nformance of the best traditional classifier. \nEDS1: Power (Log10 (Abs (Range (Integration \n(Square (Mean (FilterBank (Normalize (Testwav), \n5.0))))))), -1.0) \nEDS2: Power (Log10 (Abs (Range (Sqrt (Bart-\nlett (Mean (FilterBank (Normalize (Testwav), \n9.0))))))), -1.0) \nEDS3: Sqrt (Range (Integration (Hanning \n(Square (Mean (Split (Testwav, 3736.0))))))) \nEDS4: Arcsin (Sqrt (Range (Integration (Mean \n(Split (Normalize (Testwav), 5862.0)))))) \nEDS5: Log10 (Variance (Integration (Bartlett \n(Mean (FilterBank (Normalize (Testwav), \n5.0)))))) \nEDS6: Power (Log10 (Abs (Range (Integration \n(Square (Sum (FilterBank (Normalize (Testwav), \n9.0))))))), -1.0) \nEDS7: Square (Log10 (Abs (Mean (Normalize \n(Integration (Normalize (Testwav))))))) \nEDS8: Arcsin (Sqrt (Range (Integration (Mean \n(Split (Normalize (Testwav), 8913.0)))))) \nEDS9: Power (Log10 (Abs (Range (Sqrt (Bart-\nlett (Mean (FilterBank (Normalize (Testwav), \n3.0))))))), -1.0) \nFigure 5 . Selected features for the Amaj/min \nchord recogniser. \n6.2 Case 2: Chord Type Recognition, fixed root. \nFigure 6 shows the selected features for the chord type \ndatabase. The best model obtained was a GMM of 14 \ngaussians and 500 iterations (see  [9] for details) . The \ndescriptor reached 96,56% of the performance of the  best \ntraditional classifier. \nEDS1: Log10 (Abs (RHF (Sqrt (Integration \n(Integration (Normalize (Testwav))))))) \nEDS2: Mean (Sum (SplitOverlap (Sum (Bartlett \n(Split (Testwav, 1394.0))), 4451.0, \n0.5379660839449434))) \nEDS3: Power (Log10 (Abs (RHF (Normalize (In-\ntegration (Integration (Normalize (Test-\nwav))))))), 6.0) \nEDS4: Power (Log10 (RHF (Testwav)), 3.0) \nEDS5: Power (Mean (Sum (SplitOverlap (Sum \n(Bartlett (Split (Testwav, 4451.0))), 4451.0, \n0.5379660839449434))), 3.0) \nFigure 6 . Selected features for the Chord Type \nrecogniser. \n6.3 Case 3: Chord Recognition. \nFigure 7 shows some of the selected features for th e \nchord recognition database. The best model obtained  was \na KNN of 4 nearest neighbours, weighted by the inve rse of the distance (see  [9] for details). The descrip tor \nreached 63,05% of the performance of the best tradi -\ntional classifier. It is important to notice that 4 0,31 % is \nnot necessarily a bad result, since we have 60 poss ible \nclasses. In fact, 27,63% of the wrongly classified in-\nstances were due to mistakes between relative major s and \nminors (e.g; C and Am); 40,78% due to other usual m is-\ntakes (e.g. C and C7; C° and Eb°; C and G); only 31 ,57% \nwere caused by unexpected mistakes. Despite these r e-\nmarks, the comparative results are significantly wo rse \nthan the previous ones. \nEDS1: Square (Log10 (Abs (Sum (SpectralFlat-\nness (Integration (Split (Testwav, 291.0))))))) \nEDS4: Power (Log10 (Abs (Iqr (SpectralFlat-\nness (Integration (Split (Testwav, 424.0)))))), \n-1.0) \nEDS9: Sum (SpectralRolloff (Integration \n(Hamming (Split (Testwav, 4525.0))))) \nEDS10: Power (Log10 (Abs (Median (Spec-\ntralFlatness (Integration (SplitOverlap (Test-\nwav, 5638.0, 0.7366433546185794)))))), -1.0) \nEDS12: Log10 (Sum (MelBands (Normalize \n(Testwav), 7.0))) \nEDS13: Power (Median (Normalize (Testwav)), \n5.0) \nEDS14: Rms (Range (Hann (Split (Testwav, \n9336.0)))) \nEDS15: Power (Median (Median (Split (Sqrt \n(Iqr (Hamming (Split (Testwav, 2558.0)))), \n4352.0))), 1.5) \nEDS17: Power (HFC (Power (Correlation (Nor-\nmalize (Testwav), Testwav), 4.0)), -2.0) \nEDS18: Square (Log10 (Variance (Square \n(Range (Mfcc (Square (Hamming (Split (Testwav, \n9415.0))), 2.0)))))) \nEDS19: Variance (Abs (Median (Hann (Filter-\nBank (Peaks (Normalize (Testwav)), 5.0))))) \nEDS21: MaxPos (Sqrt (Normalize (Testwav))) \nEDS22: Power (Log10 (Abs (Iqr (SpectralFlat-\nness (Integration (Split (Testwav, \n4542.0)))))), -1.0) \nFigure 7 . Some of the selected features for the \nchord recogniser. \n6.4 Other cases \nWe also compared the three approaches on other data -\nbases, as we can see in the table 2. MajMinA is the ma-\njor/minor classifier, root fixed to A. ChordA is the chord \ntype recogniser, root fixed to A. ChordC is the chord \ntype recogniser, root fixed to C. RealChordC  is the same \nchord type recogniser in C, but the testing dataset  is \ncomposed by real audio recordings (samples of less than \n1 second of chords played in a nylon guitar), inste ad of \nmidi rendered audio. Curiously, in this case, the E DS \nsolution worked better than the traditional one (pr obably \ndue to an alteration in tuning in the recorded audi o). \nChord is the chord recognition database. SmallChord is a  \nsmaller dataset (300 examples) for the same problem . \nNotice that in this case EDS outperformed KNN and \n447   \n \n PCP Template. In fact, the EDS solution does not im -\nprove very much when passing from 300 to 1885 exam-\nples (from 38,64% to 40,31%), while the KNN solutio n \ngoes from 44% to 63,93%. Finally, RealChord  has the \nsame training set from the Chord database, but is tested \nwith real recorded audio.  \nTable 2. Comparison between the performance of \nthe EDS and the best traditional classifier for a \nlarger group of databases. Comparative perform-\nance = eds performance / traditional technique \nperformance. \nDB NAME Comparative Per-\nformance \nMajMinA 91,00% \nChordA 94,38% \nChordC 96,56% \nRealChordC 116,66% \nChord 63,05% \nSmallChord 87,82% \nRealChord 55,16% \n \nThe results from these databases confirm the trend of \nthe previous scenario. The reading of the results i ndi-\ncates that the effectiveness of the EDS fully autom ated \ndescriptor extraction depends on the domain it is a pplied \nto. Even admitting that EDS (in its current state) is only \npartially suited to non expert users, we must take into \naccount that EDS currently uses a limited palette o f sig-\nnal processing functions, which is being progressiv ely \nenhanced. Since EDS didn’t have any information abo ut \ntonal harmony, it was already expected that it woul d not \nreach the best results. Even though, the results ob tained \nby the chord recogniser with a fixed root show the \npower of the tool. \n7 CONCLUSION AND FUTURE WORKS \nIn this paper we compared the performance of a stan dard \nchord recognition technique and the EDS approach. T he \nchord recognition was specifically related to nylon  gui-\ntar samples, since we intend to apply the solution to a \nBrazilian style guitar accompanier. The standard te ch-\nnique was the Pitch Class Profiles, in which freque ncy \nintensities are mapped to the twelve semitone pitch  \nclasses, and then uses KNN classification to chord tem-\nplates. EDS is an automatic descriptor extractor sy stem \nthat can be employed even if the user does not have  \nknowledge about signal processing. It was operated in a \ncompletely naïve way so that the solution and the r esults \nwould be similar to those obtained by a non expert user.  \nThe statistical results reveal a slight deficit of EDS for a \nfixed root, and a greater gap when the root is not known \na priori, showing its dependency on primary operato rs. \nAn initial improvement is logically the increase of  the \npalette of functions. Currently, we are implementin g tonal harmony operators such as chroma and \npitchBands, which we suppose will provide much bett er \nresults. Additionally, as the genetic search in EDS  is \nindeed an optimisation algorithm, if the user start s from \na good solution, it will be expected that the algor ithm \nmakes it even better. The user can also guide the f unc-\ntion generation process, via more specific patterns  and \nheuristics. \nWith these actions, we intend to perform the second  part \nof the comparison we started in this paper – betwee n the \ntraditional techniques and EDS operated by a signal  \nprocessing expert. \nACKNOWLEDGEMENTS \nWe would like to thank all the team at Sony CSL Par is, \nparticularly Anthony Beurivé, Jean-Julien Aucouturi er \nand Aymeric Zils for their support and assistance w ith \nEDS; and a special thanks to Tristan Jehan for his help in \nthe conception and implementation of the algorithms . \nREFERENCES \n[1] Pachet, F. and Zils, A. “Automatic Extraction of \nMusic Descriptors from Acoustic Signals”, \nProceedings of Fifth International Conference on \nMusic Information Retrieval (ISMIR04), Barcelona, \n2004. \n[2] Zils, A. & Pachet, F. “Extracting Automatically the  \nPerceived Intensity of Music Titles”, Proceedings o f \nthe 6th COST-G6 Conference on Digital Audio \nEffects (DAFX03), 2003. \n[3] Gómez, E. and Herrera, P. “Estimating the tonality of \npolyphonic audio files: cognitive versus machine \nlearning modelling strategies”, Proceedings of  the \n5th International Conference on Music Information \nRetrieval (ISMIR04), Barcelona, 2004. \n[4] Sheh, A. and Ellis, D. “Chord Segmentation and \nRecognition using EM-Trained Hidden Markov \nModels”, Proceedings of the 4th International \nSymposium on Music Information Retrieval \n(ISMIR03), Baltimore, USA, 2003. \n[5] Yoshioka, T., Kitahara, T., Komatani, K., Ogata, T.  \nand Okuno, H. “Automatic chord transcription with \nconcurrent recognition of chord symbols and \nboundaries”, Proceedings of  the 5th International \nConference on Music Information Retrieval \n(ISMIR04), Barcelona, 2004. \n[6] Fujishima, T. “Real-time chord recognition of \nmusical sound: a system using Common Lisp Music”, \nProceedings of International Computer Music \nConference (ICMC99), Beijing, 1999. \n[7] Bartsch, M. A. and Wakefield, G. H. “To catch a \nchorus: Using chromabased representation for audio \nthumbnailing”, Proceedings of International. \n448   \n \n Workshop on Applications of Signal Processing to \nAudio and Acoustics, Mohonk, USA, 2001. \n[8] Pardo, B., Birmingham, W. P. “The Chordal Analysis \nof Tonal Music”, The University of Michigan, \nDepartment of Electrical Engineering and Computer \nScience Technical Report CSE-TR-439-01, 2001. \n[9] Mitchell, T. “Machine Learning”, The McGraw-Hill \nCompanies, Inc. 1997. \n[10] Cabral, G., Zanforlin, I., Santana, H., Lima, R., \n& Ramalho, G. “D'accord Guitar: An Innovative \nGuitar Performance System”, in Proceedings of \nJournées d'Informatique Musicale (JIM01), Bourges, \n2001. \n[11] Koza, J. R. \"Genetic Programming: on  the \nprogramming of  computers  by  means  of  natural  \nselection\",  Cambridge, USA, The MIT Press. \n[12] Gómez, E. Herrera, P. “Automatic Extraction of \nTonal Metadata from Polyphonic Audio Recordings”, \nProceedings of 25th International AES Conference, \nLondon, 2004. \n[13] Website: http:// timidity.sourceforge.net/ \n \n449"
    },
    {
        "title": "Melodic Similarity Algorithms -Using Similarity Ratings for Development and Early Evaluation.",
        "author": [
            "Margaret Cahill",
            "Donncha Ó Maidín"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415642",
        "url": "https://doi.org/10.5281/zenodo.1415642",
        "ee": "https://zenodo.org/records/1415642/files/CahillM05.pdf",
        "abstract": "This paper focuses on gathering similarity ratings for use in the construction, optimization and evaluation of melodic similarity algorithms. The approach involves conducting listening experiments to gather these ratings for a piece in Theme and Variation form.",
        "zenodo_id": 1415642,
        "dblp_key": "conf/ismir/CahillM05",
        "keywords": [
            "melodic",
            "similarity",
            "ratings",
            "melodic",
            "algorithms",
            "listening",
            "experiments",
            "Theme",
            "and",
            "Variation"
        ],
        "content": "MELODIC SIMILARITY ALGORITHMS – USING SIMILARITY \nRATINGS FOR DEVELOPMENT AND EARLY EVALUATION\nMargaret Cahill Donncha Ó Maidín \nCentre for Computational Musicology and Computer Music, \nDepartment of Computer Science and Information Systems, \nUniversity of Limerick, \nIreland. \nmargaret.cahill, donncha.omaidin@ul.ie  \nABSTRACT   \nThis paper focuses on gathering similarity ratings for use \nin the construction, optimization and evaluation of me-lodic similarity algorithms. The approach involves con-ducting listening experiments to gather these ratings for a piece in Theme and Variation form. \n \nKeywords: melodic similarity, score, algorithm, percep-\ntion, similarity ratings, listening experiments \n1 OVERVIEW OF PROBLEM \nThe MIR research community draws its members from many research disciplines, including musicology and music analysis. It is of benefit for musicologists to be able to search digitized score databases (corpora) for exact and similar melodies. Melodic similarity algo-rithms play an important role in automating this process. Such algorithms calculate a measure that reflects the de-\ngree of similarity/dissimilarity between a pair of melo-\ndies or melodic segments. \nMany algorithms used to measure melodic similarity \nare text-based string-matching algorithms that have ei-ther been adopted directly or somewhat altered to suit this new role [1-4]. One of the most commonly used of these is the edit-distance family of algorithms (along with variations), which essentially calculates the “cost” of taking one string/melody and converting it into the other [5-7]. However, the issue of identifying melodies that are perceptually similar means that there is not a clear analogy between comparing words/sentences in text to comparing musical melodies which are multi-dimensional, and for which operations such as simple addition or deletion of notes is problematic.  \nThis research is concerned with identifying successful \nalgorithms for determining melodic similarity using mu-sic perception principles as a guide and employing a relevant testbed in the development stage to aid the process. We are currently focussing on monophonic music. 2 WHICH MUSICAL FEATURES TO USE \nAND HOW TO COMBINE THEM. \nThe most basic features that can be used to describe a \nnote are pitch and duration. In its most basic form, a melody could be described as a sequence of such pitches and durations. Further features that could also be used to describe a notated melody in more detail include rests, phrasing, dynamics, tempo, articulation and other ex-pression indications. Metrical accents (the pattern of \nstrong and weak beats relating to the time signature of a \npiece) are implicit in scores and are instinctively per-formed by musicians. These accents are also included in the list of possible features. In the past researchers have mainly focussed on using pitch sequences alone [8-9] or both pitch and duration sequences [1, 5, 10-11]. Some researchers have used other melodic features include metrical stress [12-13] and dynamics [14]. \nMusic perception and cognition research provides a \nuseful starting point for deciding which features might be most relevant. Quite a bit of work has been done to discover the ways in which we remember, identify and recall melodies. Much of this work has focussed on pitch aspects of music only [15-19], although rhythm is con-sidered to a lesser extent [20]), as well as the effects of melodic and rhythmic accents [21-24], among other fea-tures.  \nIf more than one musical feature is used, it becomes \nimportant to consider the relative importance of each and to explore fruitful ways of combining the associated measures. If only pitch and duration are used, a decision to weight them equally would require some justifica-\ntions. There is also the issue of how to combine such features (e.g. by addition, or by multiplication, or by some other means). When a decision is taken on ways in which the various feature measures are to be combined, there is still the remaining task of selecting weights to apply to each measure, Finding the proportionate weights for internal parameter values forms part of the development and “tweaking” stage of an algorithm. Of-ten a researcher may simply pick arbitrary values, com-bination schemes, and weightings that instinctively make sense in order to get some satisfactory results. Others have “tweaked” settings as a direct result of the output of the algorithm and thus tried to improve and optimise the algorithm in some way [7]. Again, research in the music perception area may be useful here to indicate the relative importance certain musical features play.  \nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-vided that copies are not made or distributed for profit or com-mercial advantage and that copies bear this notice and the full citation on the first page. \n© 2005 Queen Mary, University of London \n450   \n \n The approach adopted here is to use relevant research \nas a guide to inform choices made in the design and de-\nvelopment stage of an algorithm, rather than to attempt to create a perceptual model of melodic similarity. Schulkind et. al., in a paper that deals with how people identify melodies, urges caution as “features that are easily perceived may not necessarily be those used to \ndistinguish melodies”[25]. In this research we pr opose \nto examine the extent to which such features may be \nuseful in melodic similarity algorithms.  \n3 EXPERIMENTAL METHOD \nWe propose to use a small test collection based on a real \npiece of music during the development stage of this pro-ject. As Pampalk indicates “..even a tiny music collection can be used to identify weaknesses of a measure and compare measures to each other”[26]. Theme and Varia-tion form are very good candidates for this early assess-ment of algorithm performance because they consist of \nmusical material that is of varying degrees of similarity, \nwhere some variations may be very similar to the theme and others greatly alter the theme. In order to assess the performance of algorithms and various combinations of weighting of features we need to have an objective idea of the similarity of each variation. We propose gathering similarity ratings by conducting listening experiments that ask subjects to rate the similarity of the theme to each variation. \nVarious kinds of rating scales, including similarity \nrating scales, have been commonly used in music per-ception research but often as a mechanism to derive in-formation other than melodic similarity [18].  Also, some \nrecent research in the area of melodic similarity has also used similarity ratings gathered from subjects so there is a body of research with which to compare methods and results with [14, 28-31]. Another recent project involved asking subjects to rank melodies in order of similarity [32]. The current focus of our research is on carrying out listening tests to gather the similarity ratings we need for future work. The methods used for the test are briefly described in section 4. The above mentioned papers \npresent a number of mechanisms for ensuring the reli-ability and consistency of user ratings and some guid-ance is taken from this research.  \nOnce a set of reliable similarity ratings has been iden-\ntified we will focus on creating and evaluating algo-rithms that produce comparable results with these rat-ings. \n4 GATHERING SIMILARITY RATINGS \nFOR A SMALL TESTBED \nFor the initial phases of this research we are using a set \nof variations on Twinkle, Twinkle, Little Star composed for recorder by Duschenes [33]. This piece was used by Mongeau and Sankoff [7] for evaluating the success of their distance measure algorithm. Initially this collection was considered because the Mongeau and Sankoff paper provided some discussion and comment on the musical \nmaterial and research findings.  On closer examination this test collection demonstrated very good examples of the various kinds of problems that a melodic similarity algorithm is faced with. Different time signatures, differ-ent keys, augmentation of theme (1 bar stretched to 2 bars), notes replaced by shorter repeated notes, triplets, elaborations of theme by stepwise motions and by leaps, notes occurring an octave higher and hiding of theme notes are all included in the 10 short variations (see Fig-ure 1 for an example). This set of Theme with nine varia-\ntions consists mostly of 12 bars in\n44time. One variation \nis in 43time and is 24 bars long and a further variation is \nin 86time. The test material tends to segment quite natu-\nrally, and this allows us to ignore an issue that would be \nmore problematic with other music forms and pieces. \nThe form of the piece is very obviously ABA/ABA’, with each section lasting for four bars.  \nOne of the key issues when using similarity ratings is \nthe reliability or consistency of the data. As we intend to develop improved versions of existing algorithms based  \n \n              \n \nFigure 1:  The first two bars of the Theme, Varia-\ntion 4 and Variation 7, illustrating some of the \nvariations of the theme found in this piece.   \non these gathered ratings it is essential to ensure that the data is as “true” as possible. As previously mentioned, the form of this set of Theme and Variations easily al-lows for dividing each into short four bar phrases, which makes it easier for subjects to compare than the entire 12 \nbars and therefore provides more reliable data. The mel-ody is also very well known which means that the sub-jects should not have difficulty remembering the refer-ence melody. Many perception experiments use un-known melodies but there are examples of using known melodies [34-35] and we believe it increases the useful-ness of the data we are gathering.  \n5 THE LISTENING EXPERIMENT \nThe main part of the listening experiment is structured as \nshown in Table 1 below and is run on computer using Roger Kendall’s MEDS (Music Experiment Develop-ment System [36]). Subjects hear a series of pairs of melodies and are asked to rate the similarity of the melo-dies in each pair. Each segment is 8 seconds long, al-though one variation is 12 seconds long.  There is a .5 \nsecond pause between the first and second melody of \n451   \n \n each pair then a rating scale is shown. As soon as the \nsubject inputs their rating (there is no time limit), the next pair of melodies is played. Re-testing, split-testing or repeated random trials are often used in these kinds of experiments and later checked for consistent results from subjects. We decided in this case to repeat the basic test in random order for later comparison. There is a one \nminute pause between Part A and B. \n \nTable 1: layout of listening test \nPart A Theme & Variations 1-9 \nBars 1-4 sequential  \n Theme & Variations 1-9 \nBars 1-4 random \nPart B Theme & Variations 1-9 \nBars 5-9 sequential \n Theme & Variations 1-9 \nBars 5-9 random \n \nIn a pilot experiment we used a 7-point scale that \nranged from “very dissimilar” to “very similar” and did not tell the subjects that all the pairs they heard would be similar in some way. This did include an introductory description of the test with played examples and a prac-tice test with three pairs. We found that subjects found \nthe use of the words dissimilar and similar confusing and \noften were reluctant to use the extremes of the scale until they had heard all melodies played through once.  \nWe are currently in the process of running these lis-\ntening experiments and due to findings in the pilot test have changed the rating scale to a 7-point scale using the descriptors “hardly similar at all” and “very similar” at the opposite poles of the scale (1 and 7 respectively). Additionally we are now spending c.10 minutes of preparation time with the subject describing and discuss-ing the test. In the introduction they are told that all the melodies they will be comparing to the theme will be similar in some way to it and a demonstration using 7 variations on another well known tune are played and discussed with the subject. These variations are based on the sort of modifications made to the “Twinkle, Twin-kle” melody and three pairs are used in a practice run before the test proper begins. Subjects can ask questions and discuss the issue of similarity during this period but no comments were made about which musical features subjects should use to make their judgements.  Subjects are encouraged to use the full range of the scale. Sub-\njects are asked to complete a short questionnaire on mu-sical experience and comment sheet the end of the ex-periment so that we can determine if significant effects occur due to musical background. \n6 INITIAL RESULTS \nAt the time of writing 13 subjects have participated in the \nexperiment, all of whom are musicians but not all are from a classical music background. Initial results show that there is general agreement between subjects on the ratings given to each variation. As might have been ex-pected, subjects were most in agreement regarding varia-\ntions that were least similar and most similar (i.e. ratings of 1 and 7), while the results for variations that were only somewhat similar were less clear. Many subjects showed very high correlation between their ratings for the se-quential and random playings (only one subject was less than .7) and there was high inter-subject correlation for the most part. Most subjects did consistently use the full range of the scale. We have yet to fully analyse the re-sults gathered so far but are confident that these initial results show that reliable data can be obtained from such an experiment and intend to continue these listening tests with further subjects.  \nREFERENCES \n[1] Crawford, T., and Iiliopoulos, C. Strong-Matching \nTechniques for Musical Similarity and Melodic \nRecognition. Computing in Musicology II, Melodic Similarity - Concepts, Procedures and Applications, MIT Press, 1998.  \n[2] Downie, J.S. Music Retrieval as Text Retrieval: \nSimple yet Effective. SIGIR 1999, Berkeley, California. \n[3] Droettboom, M., Fujinaga, I., MacMillan, K., Patton, \nM., Warner, J., Choudhury, S., and DiLauro, T. \nExpressive and Efficient Retrieval of Symbolic Musical Data. ISMIR 2001, Indiana.  \n[4] Lemstrom, K. String Matching Techniques for Music \nRetrieval. PhD Thesis, Series of Publications A, Report A-2000-04, Department of Computer Science, University of Finland. \n[5] Smith, L., McNab, R., and Witten, I. Sequence –\nBased Comparison: A Dynamic-Programming Approach. Computing in Musicology II, Melodic Similarity - Concepts, Procedures and Applications, MIT Press, 1998. \n[6] Uitdenbogard, A., and Zobel, J. Melodic Matching \ntechniques for Large Music Databases. ACM Multimedia 1999, Orlando, Florida.  \n[7] Mongeau., M, and Sankoff, D. Comparison of \nMusical Sequences. Computers and the Humanities, \n24, (1990), 161-175. \n[8] Blackburn, S., and DeRoure, D. A Tool for Content \nBased Navigation of Music. ACM Multimedia 1998, Bristol, UK.  \n[9] Downie, S., and Nelson, M. Evaluation of a Simple \nand Effective Music Information Retrieval Method. SIGIR 2000, Athens, Greece.  \n[10] Hoos, H., Renz, K., and Gorg, M. GUIDO/MIR \n– an Experimental Musical Information Retrieval System based on GUIDO Music Notation. ISMIR 2001, Indiana.  \n452   \n \n [11] McNab, R., Smith, L., Bainbridge, D., and \nWitten, I. \nwww.dlib.org/dlib/may97/meldex/05w itten.html \n[12] Typke, R., Giannopoulos, P., Veltkamp, R., \nWiering, F., and Oostrum, R. Using Transportation Distances for Measuring Melodic Similarity. \nTechnical Report, UU-CS-2003-024, Institute of \nInformation and Computing Sciences, Utrecht University. \n[13] Ó Maidín, D. A Geometrical Algorithm for \nMelodic Difference. Melodic Similarity - Concepts, Procedures and Applications, Computing in Musicology II, MIT Press. 1998 \n[14] Hofman-Engl, L. Melodic Similarity and \nTransformations: A Theoretical and Empirical Approach. PhD Thesis, Department of Psychology, Keele University, January 2003. \n[15] Levitin, D. Absolute Memory for Musical Pitch: \nEvidence from the Production of Learned Melodies.  Perception and Psychophysics, 56, 4 (1994), 414-423.  \n[16] Edworthy., J. Interval and Contour in Melodic \nProcessing. Music Perception, 2 (1985), 375-388.  \n[17] Dowling, J. and Fujitani, D.  Contour, Interval, \nand Pitch Recognition in Memory for Melodies. \nJournal of the Acoustical Society of America, 49, 2 (1971), 524-531.  \n[18] Dowling, J., Scale and Contour: Two \nComponents of a Theory of Memory for Melodies. Psychological Review, 85, 4 (1978), 341-354. \n[19] Dewitt, L. and Crowder, R., Recognition of \nnovel melodies after brief delays., L., Bainbridge, D., and Witten, I. The New Zealand Digital Library MELody inDEX.  D-Lib Magazine, May 1997,  Music Perception, 3, 3 (1986), 259-274. \n[20] Kidd, G., Boltz, M., & Jones, M. R. Some \neffects of rhythmic context on melody recognition. American Journal of Psychology, 97, (1984), 153-173. \n[21] Jones, M. Dynamic Pattern Structure in Music: \nRecent Theory and Research. Perception & Psychophysics, 41, 6 (1987), 621-634.  \n[22] Thomassen, J. Melodic Accent: Experiments \nand a Tentative Model. Journal of the Acoustical Society of America., 71, 6 (1982), 1596-1605.  \n[23] Huron, D., and Royal, M. What is Melodic \nAccent? Converging Evidence from Musical Practice. Music Perception, 13, 4 (1996), 489-516 \n[24] Pfordresher, P. The Role of Melodic and \nRhythmic Accents in Musical Structure. Music Perception, 20, 4 (2003), 431-464. [25] Schulkind., M., Posner, R., and Rubin, D. \nMusic Features that Facilitate Melody Identification: \nHow do you Know it’s “your” Song when they Finally Play It? Music Perception, 21, 2 (2003), 217-249. \n[26] Pampalk, E., Dixon, S., and Widmer, G. \nExploring Music Collections by Browsing Different \nViews. DAFX-03, London. \n[27] Rosner, B., and Meyer, L. The Perceptual Roles \nof Melodic Process, Contour, and Form. Music Perception, 4, 1 (1986), 1-40.  \n[28] Eerola, T., Jarvinen, T., Louhivuori, J. and \nToivainen, P. Statistical Features and Perceived Similarity of Folk Melodies. Music Perception, 18, 3 (2001), 275-296 \n[29] Mullensiefen, D., and Frieler, K. Measuring \nMelodic Similarity: Human vs. Algorithmic Judgements. Proceedings of the Conference on Interdisciplinary Musicology, Graz, Austria, April 2004. \n[30] Schmuckler, M. Testing Models of Melodic \nContour Similarity. Music Perception, 16, 3 (1999), 295-326.  \n[31] McAdams, S., and Matzkin, D. Similarity, \nInvariance, and Musical Variation. The Biological \nFoundations of Music, New York Academy of Sciences, 2001 \n[32] Typke, R., Hoed, M., de Nooijer, J., Wiering, \nF., and Veltkamp, R. A Ground Truth for Half a Million Incipits. Proceedings of DIR 05, 5th Dutch-Belgian Information Retrieval Workshop, Utrecht, The Netherlands, January 2005. \n[33] Duschenes, M. Variations on Twinkle, Twinkle, \nLittle Star from Method for the Recorder – Tunes and Exercises. Berandol Music Limited, Ontario, Canada, 1962.  \n[34] White., B. Recognition of Distorted Melodies. \nAmerican Journal of Psychology, 73, (1960), 100-107.  \n[35] Dowling, W. J., and Hollombe, A. The \nPerception of Melodies Distorted by Splitting into Several Octaves: Effects of Increasing Proximity and Melodic Contour. Perception and Psychophysics, 21, 1 (1977), 60-64.  \n[36] Kendall, R. MEDS – Music Experiment \nDevelopment System.    www.ethnomusic.ucla.edu/systematic/Faculty/Kendall/meds.htm \n \n \n453"
    },
    {
        "title": "A Pattern Extraction Algorithm for Abstract Melodic Representations that Allow Partial Overlapping of Intervallic Categories.",
        "author": [
            "Emilios Cambouropoulos",
            "Maxime Crochemore",
            "Costas S. Iliopoulos",
            "Manal Mohamed 0001",
            "Marie-France Sagot"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415008",
        "url": "https://doi.org/10.5281/zenodo.1415008",
        "ee": "https://zenodo.org/records/1415008/files/CambouropoulosCIMS05.pdf",
        "abstract": "REPRESENTATIONS THAT ALLOW PARTIAL OVERLAPPING OF INTERVALLIC CATEGORIES Emilios Cambouropoulos1, Maxime Crochemore2,3, Costas Iliopoulos3, Manal Mohamed3, Marie-France Sagot4 1 Department of Music Studies, University of Thessaloniki, 540006, Thessaloniki, Greece emilios@mus.auth.gr 2 Institut Gaspard-Monge, University of Marne-la-Vall´ee, 77454 Marne-la-Vall´ee CEDEX 2, France maxime.crochemore@univ-mlv.fr 3 Department of Computer Science, King’s College London, London WC2R 2LS, England {mac,csi,manal}@dcs.kcl.ac.uk 4 INRIA Rhˆone-Alpes, Universit´e Claude Bernard, 43 Bd du 11 novembre 1918, 69622 Villeurbanne cedex, France Marie-France.Sagot@inria.fr This paper proposes an efficient pattern extraction algorithm that can be applied on melodic sequences that the melodic representation introduces special “don’t care” symbols for intervals that may belong to two partially overlapping intervallic categories. As a special case the well established “step-leap” representation is examined. In the step-leap representation, each melodic diatonic interval is classified as a step (±s), a leap (±l) or a unison (u). Binary don’t care symbols are introduced to reprecategories e.g. ∗= s, ∗= l and # = −s, # = −l. For such a sequence, we are interested in finding maximal repeating pairs and repetitions with a hole (two matching subsequences separated with an intervening non-matching symbol). We propose an O(n + d(n −d) + z)-time algorithm for computing all such repetitions in a given sequence x = x[1..n] with d binary don’t care symbols, where z is the output size. Keywords: string, don’t care, repetitions, suffix tree, lowest common ancestor. 1",
        "zenodo_id": 1415008,
        "dblp_key": "conf/ismir/CambouropoulosCIMS05",
        "keywords": [
            "efficient",
            "pattern",
            "algorithm",
            "melodic",
            "sequences",
            "melodic",
            "representation",
            "special",
            "“don’t care”",
            "symbols"
        ],
        "content": "A PATTERN EXTRACTION ALGORITHM FOR ABSTRACT MELODIC\nREPRESENTATIONS THAT ALLOW PARTIAL OVERLAPPING OF\nINTERVALLIC CATEGORIES\nEmilios Cambouropoulos1, Maxime Crochemore2,3, Costas Iliopoulos3, Manal Mohamed3, Marie-France Sagot4\n1Department of Music Studies, University of Thessaloniki, 5 40006, Thessaloniki, Greece\nemilios@mus.auth.gr\n2Institut Gaspard-Monge, University of Marne-la-Vall ´ee, 77454 Marne-la-Vall ´ee CEDEX 2, France\nmaxime.crochemore@univ-mlv.fr\n3Department of Computer Science, King’s College London, Lon don WC2R 2LS, England\n{mac,csi,manal }@dcs.kcl.ac.uk\n4INRIA Rh ˆone-Alpes, Universit ´e Claude Bernard, 43 Bd du 11 novembre 1918, 69622 Villeurban ne cedex, France\nMarie-France.Sagot@inria.fr\nABSTRACT\nThis paper proposes an efﬁcient pattern extraction al-\ngorithm that can be applied on melodic sequences that\nare represented as strings of abstract intervallic symbols ;\nthemelodicrepresentationintroducesspecial“don’tcare ”\nsymbols for intervals that may belong to two partially\noverlapping intervallic categories. As a special case the\nwell established “step-leap” representation is examined.\nIn the step-leap representation, each melodic diatonic in-\nterval is classiﬁed as a step(±s), aleap(±l) or aunison\n(u). Binary don’t care symbols are introduced to repre-\nsentthepossibleoverlappingbetweenthevariousabstract\ncategories e.g.∗=s,∗=land# =−s,# =−l.\nForsuchasequence,weareinterestedinﬁndingmaximal\nrepeating pairs and repetitions with a hole (two matching\nsubsequencesseparatedwithaninterveningnon-matching\nsymbol). We propose an O(n+d(n−d) +z)-time al-\ngorithm for computing all such repetitions in a given se-\nquence x=x[1..n]withdbinary don’t care symbols,\nwhere zis the output size.\nKeywords: string, don’t care, repetitions, sufﬁx tree,\nlowest common ancestor.\n1 INTRODUCTION\nRecently, there have been different proposals in the liter-\nature to develop an effective music information retrieval\nsystem. The goal of these proposals is to take advantage\nof appropriate computer science techniques. For exam-\nple, representing the musical surface as a string or set of\nstringsmaymakeitpossibleinsomecasestoapplyexist-\ning algorithms from the ﬁeld of stringology. For instance,\nin order to discover similarities between different musi-\ncal entities ortoestablishmotivic“signatures”, musican -\nalysts may use algorithms that extract repetitions from\nstrings. Such similarities often involve ﬁnding approx-\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrstpage.\nc/circlecopyrt2005 Queen Mary, University of Londonimate repetitions (Crawford et al., 1998). This requires\ndeveloping new approximation measures that meet musi-\ncians’ needs.\nOne commonly used representation for music is the\nnumeric representation MIDI. For such a representation,\ndifferent approximation measures have been developed,\nsuch as, δ-,γ- and{δ,γ}-approximate. For example, in\nδ-approximate matching, equal-length strings consisting\nof integers match if each corresponding integer differs by\nnotmorethan δ–e.g. aC-major{60,64,65,67}andaC-\nminor{60,63,65,67}sequence can be matched if a tol-\nerance δ= 1is allowed in the matching process. Using\nthese approximation measures, algorithms for ﬁnding ap-\nproximate repetitions in musical sequences have been de-\nveloped (Iliopoulos et al., 2000; Cambouropoulos et al.,\n2002). These algorithms are based on approximate pat-\ntern matching techniques. For an overview refer to Clif-\nford and Iliopoulos (2004).\nAlthough MIDI is the most common representation\nin the computational domain, it has certain well-known\nshortcomings,forinstance,manyimportantmusicalprop-\nerties are not explicitly represented (e.g. note durations ,\naccidentals etc.) and almost all information on musical\nstructure is lost. Therefore, different representations h ave\nbeen proposed in the literature. For example, Hawley\n(1993) proposed representing the musical signal as a se-\nquence of pitch intervals. In order to allow tolerance in\ninterval matching, Ghias et al. (1995) used the reduced\ninterval alphabet of the“melodic contour” representation .\nLemstr¨om and Laine (1998) proposed classifying the in-\ntervals into seven partially overlapping classes: small,\nmedium and large, up- or downwards, and prime.\nIn this paper, we propose an alternative method to\nusing approximate pattern matching techniques for ﬁnd-\ning approximate repetitions in a musical string. Our ap-\nproach is based on using exact pattern matching tech-\nniques to extract repetitions from an abstract level of\na musical sequence. As an abstract representation, we\nwill use the “reﬁned contour” (or step-leap) representa-\ntion - see, for instance, application of this representatio n\nin<http://www.themeﬁnder.org >. In the step-leap repre-\nsentation, intervals are classiﬁed into ﬁve distinct equiv a-\nlence classes: up- or downwards stepandleap, anduni-\nson. An interval with magnitude a= 0is a unison (u),\na <2is a step (s), and any other interval a≥2is a leap\n(l); the direction of intervals is preserved – see second\n167Figure 1: Melodic pattern-matching example (the pitches of this example are taken from Bach’s Well-Tempered Clavier,\nBook I, Fugue in F# major)\nstring of symbols in Figure 1.\nIn the second string of symbols in Figure 1, two re-\npeated substrings are found: −s s−landl−s s s−l\neach occurring twice. However, for a listener/musician,\nthesecondhalfofthisstringofintervalsisanapproximate\nrepetition of the ﬁrst half (two approximately matching\nsubstrings separated by a “hole” of size one) because in-\ntervals a= 1anda= 2areconsidered similar(i.e. astep\nissimilartoasmallleap). Thisisnotsimplysomerareex-\nception in music. It is a rather common phenomenon es-\npecially when themes appear in their dominant form (see,\nfor instance, the tonal answers of almost half of Bach’s\nfugue themes from the two books of the Well-Tempered\nClavier). In Figures 2 and 3 some melodic examples are\npresented.\nThe problem in the step-leap representation is that\nthe abstract interval classes (u,s,l)have sharp bound-\naries and no diatonic pitch interval instance may belong\nto more than one class. In other words, borderline mem-\nbers can never be matched to other ‘similar’ members of\nother classes (e.g. an a= 2interval as a member of leap\ncanneverbematchedtoa‘similar’ a= 1intervalwhichis\nastep),i.e,asmallleapcanneverbeconsideredasastep.\nA way to overcome this problem is to allow partial over-\nlappingbetweenthevariousclasses(thisisalsosuggested\nin Lemstr ¨om and Laine (1998)). For instance, an interval\na= 2may be classiﬁed as either step or leap. A special\n“binary don’t care” symbol ∗that matches either sorlis\nused (see third string of symbols in Figure 1). Similarly,\nanotherdon’tcaresymbol #thatmatchesboth−sand−l\nis also used. Note that this idea can be easily extended to\nanyconstantnumberofpartiallyoverlappingclasses,such\nas those proposed in Lemstr ¨om and Laine (1998).\nA pattern extraction algorithm may ﬁnd a large num-\nberofrepeatingpatterns. Manyofthesearenotmusically\nor perceptually important. A mechanism, therefore, for\nselecting important patterns is required. A relatively so-\nphisticated method for ﬁnding beginnings of potentially\nsalient repetitions is proposed by Cambouropoulos. Fol-\nlowing this proposal, for each pattern a prominence value\nis calculated based on frequency of occurrence, pattern\nlength and degree of pattern overlapping; these promi-\nnence values contribute to establishing a segmentation\nprominence proﬁle for a melody whereby the most likely\npositions of important repetitions are highlighted. In thi s\npaper,wheretheaimisnotsegmentationbuttheextraction\nof interesting patterns, simpler criteria are set: extract ionof maximal repeating pairs and repetitions with a ‘hole’.\nThe later involves extracting immediately repeated non-\noverlapping melodic patterns - in order that the patterns\ndo not overlap over one note a ‘hole’ is necessary in the\npitchintervalrepresentation(forexample,inFigure1,th e\nsecond half of the melody is an approximate repetition of\nthe ﬁrst half - in the interval representations beneath the\nmelody it is necessary to skip one symbol in the middle\nso that the repetition is consecutive and non-overlapping\nat the note level).\nFor strings with don’t cares, several string matching\nalgorithmshavebeenproposed(see(FischerandPaterson,\n1974;ApostolicoandPreparata,1983;Amiretal.,2001)).\nRecently,Iliopoulosetal.(2003)presentedalgorithmsfo r\ncomputing typical regularities in strings with don’t cares .\nHere, we consider binary don’t care symbols that each\nmatches beside itself two additional symbols. For string\nx=x[1..n]withdbinary don’t cares, we propose an al-\ngorithm for computing special kinds of repetition that we\nrefer to as “maximal-pairs” and “repetitions with a hole”.\nThe proposed algorithm uses O(n+d(n−d) +z)time,\nwhere zis the output size.\nThe paper is organized as follows: in Section 2, we\nstate the preliminaries used throughout the paper. In Sec-\ntion 3, we deﬁne all approximate repetitions problem and\ndescribe in general how to ﬁnd them. In Section 4, we\ndetail our algorithm. Finally, in Section 5, we analyze the\nrunning time of the algorithm.\n2 PRELIMINARIES\nThroughout the paper, x=x[1..n]denotes a stringof\nlength noverΣ∪{∗,#}, where Σ ={s,−s,l,−l,u}.\nThesymbols‘∗’and‘ #’arecalled binarydon’tcare sym-\nbols. Each binary don’t care symbol matchesitself and\ntwo different symbols, that is, ∗=∗,∗=s,∗=l,# =\n#,# =−sand# =−l.\nWe use x[i], fori= 1,2,... ,n, to denote the i-th\nsymbol of x, and x[i..j]as a notation for the substring\nx[i]x[i+ 1]···x[j]ofx. Ifx=uvthenxis said to be\ntheconcatenation of the two strings uandv. A string yis\nsaid tooccurinxat position iify[j] =x[i+j−1], for\n1≤j≤|y|.\nArepeating pair inxis represented by (p;i,j)where,\nx[i..i+p−1] =x[j..j+p−1]forsome i/negationslash=j. Thepositive\ninteger piscalledthe periodoftherepeatingpair. If x[i−\n1]/negationslash=x[j−1]then(p;i,j)isleft-maximal . Respectively,\n168Opening melody of Beethoven’s Piano Sonata Op.10, No.2.\nUpper voice ‘stream’ (theme and tonal answer) from the openi ng of Bach’s Well-Tempered Clavier, Book I, Fugue in F#\nmajor.\nUpper voice ‘stream’ (theme and tonal answer) from the openi ng of Bach’s Well-Tempered Clavier, Book I, Fugue in C\nminor.\nSecond theme from Shostakovich’s String Quartet No 4 in D maj or, Op. 83, Mov. 4\nFigure 2: Melodic examples where an a= 1interval (step) and an a= 2(leap) should be matched (these positions are\nindicated by asterisks in the melodic examples). Brackets i ndicate extracted melodic repetitions\nFigure 3: The opening melody of Mussorgsky’s, Pictures from an exhibition, Promenade. Extracted maximal repeating\npairs and repetitions with a ‘hole’ are indicated by bracket s\n169Ifx[i+p]/negationslash=x[j+p]then(p;i,j)isright-maximal . If\n(p;i,j)isbothleft-andright-maximalthenitis maximal-\npair. Arepetition with a hole is a repeating pair (p;i,j)\nsuch that j=i+p+ 1.\nHere, we present a method for ﬁnding all maximal-\npairs and all repetitions with a hole in a given string x,\nwhere xmay have occurrences of binary don’t cares. Our\nmethod uses the sufﬁx tree of xas a fundamental data\nstructure. A complete description of sufﬁx trees is be-\nyond the scope of this paper, and can be found in (Gus-\nﬁeld, 1997) or (Crochemore and Rytter, 2002). However,\nfor the sake of completeness, we will brieﬂy review the\nnotion.\nDeﬁnition 1 (Sufﬁx tree) A sufﬁx treeT(x)of the string\nx$ =x[1..n]$is a rooted directed tree with exactly n\nleaves numbered 1 to n+ 1, where $/∈Σ. Each internal\nnode, other than the root, has at least two children and\neachedgeislabelledwithanon-emptysubstringof x. No\ntwoedgesoutanodecanhaveedge-labelsbeginningwith\nthe same symbol. The key feature of the sufﬁx tree is that\nfor any leaf i, the concatenation of the edge-labels on the\npath from the root to leaf iexactly spells out i-th sufﬁx of\nx, withn+ 1denotes the empty sufﬁx.\nSeveral algorithms construct the sufﬁx tree T(x)in\nΘ(n)timeandspace,assumingconstantsizealphabet(see\nfor example (Crochemore and Rytter, 2002) and (Gus-\nﬁeld, 1997)). For any node v, thepath-label ofvis the\nlabel of the path from the root of T(x)tov; it is denoted\nbylabel(v). Thestring-depth ofvis the number of sym-\nbolsin v’spath-label;itisdenotedby depth(v). Theleaf-\nlistofvisthesetoftheleafnumbersinthesubtreerooted\natv; it is denoted by LL(v).\nOur method makes use of the Schieber and Vishkin\n(1988)’s Lowest Common Ancestor algorithm. For a\ngiven rooted treeT, thelowest common ancestor (LCA)\noftwonode uandvisthedeepestnodein Tthatisances-\ntorofboth uandv. Afteralinearamountofpreprocessing\nof a rooted tree, any two nodes can be speciﬁed and their\nlowest common ancestor is found in constant time. That\nis,arootedtreewith nnodesisﬁrstpreprocessedin O(n)\ntime, and thereafter any lowest common ancestor query\ntakes only a constant time to be solved, independent of n.\nIn the context of sufﬁx trees, the situation commonly\narises that both uandvare leaves inT(x), where x[i..n]\nandx[j..n]arethesufﬁxesrepresentedby uandvrespec-\ntively, for integers iandjin the range 1..n+ 1. In this\ncase,thenode w=LCA(u,v)istherootoftheminimum\nsize subtree contains uandv. Note that the path-label of\nw(label(w)) is thelongest common preﬁx ofx[i..n]and\nx[j..n]. Theabilityofﬁndingsuchlongestcommonpreﬁx\nis an important primitive in many stringproblems.\n3 FINDING ALL REPETITIONS\nPROBLEM\nHere, we study the problem of ﬁnding in a given sting x\noverΣ∪{∗,#}, the following two kinds of repetitions:\nall maximal-pairs and all repetitions with a hole, where\neach repetition with a hole is a repeating pair (not neces-\nsary maximal) in which an intervening symbol separates\nthe two matching substrings. In fact, there is a very closerelationbetweenthesetwokindsofrepetitions. Forexam-\nple, if a given string xis as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13\nx=s s l s s #l s∗ −l l s s\nThenthemaximal-pair (6;3,7)–whichrepresentstwo\noverlapping matching substrings– represents four repeti-\ntions with a hole: (3;3,7),(3;4,8), (3;5,9), (3;6,10).\nThe relation between maximal-pairs and repetitions\nwith a hole is presented in the following lemma:\nLemma 1 Let(p;i,j)beamaximal-pairin xandlet g=\nj−i−p.\n1. Ifg= 1then(p;i,j)is a repetition with a hole.\n2. Ifg≤0andj/negationslash=i+ 1then(p/prime;i+k,j+k)are\nrepetitions with a hole, where p/prime=j−i−1and\nk= 0..|g|+ 1.\n3. Ifg≤0andj=i+ 1then some repetitions with a\nhole may be found in x[i..j+p−1].\n4. Ifg >1thenx[i..i+p−1]andx[j..j+p−1]gare\ntwo matching substrings separated by intervening g\nsymbols .\nIn the following we will introduce Gusﬁeld’s algo-\nrithm for ﬁnding all maximal-pairs in a given string with-\nout don’t cares. The basic tool behind Gusﬁeld’s algo-\nrithm is the sufﬁx tree. The algorithm starts by construct-\ning the sufﬁx tree for a given string. The algorithm then\nuses a bottom-up approach (from leaves to root) to report\nfor each internal node the maximal-pairs associated with\nit. Thisisaccomplishedbymaintainingtheleaflist LL(v)\nof each internal node vas a collection of disjoint sublists\nLLα(v), where αis the symbol preceding the sufﬁx as-\nsociated to a leaf in the subtree rooted by v. Thus, each\ninternal node is attached at most |Σ|sublists. Reporting\nthe maximal-pairs is accomplished by the cartesian prod-\nuctofaleaf-sublistwithalltheleaf-sublistsofitsbroth ers\nthat correspond to different symbols. The algorithm runs\ninO(n+z),where zisthenumber ofreported maximal-\npairs. Gusﬁeld’s algorithm can be easily modiﬁed to ﬁnd\nall repeating pairs (not necessary maximal) in x.\nThe presence of the binary don’t care symbols in x\ncomplicates the construction of the sufﬁx tree. Hence,\nGusﬁeld’s algorithm cannot be used directly to solve the\nproblem. The dynamic programming seems to be an ob-\nvious solution. The cost of this method is quadratic. For\nexample,if x=s s#l l∗−l s∗−l l l s. Thenusingdy-\nnamic programming, the following maximal-pairs can be\nfound: (1;1,2), (1;1,6), (6;1,8), (1;1,9), (1;1,13), (2;2 ,6),\n(1;2,8), (1;2,13), (2;4,5), (1;4,6), ..., (1;11,12). Note that\nonly (6;1,8), (1;4,6), (1;6,8), (1;9,11) and (1;4,6) are th e\nonly repetitions with a hole in x(see Table 1).\nInthenextsection,wewillexplainhowthesufﬁxtree\ncan be used to speed up the dynamic programming cal-\nculations. Independently of the size of the alphabet, our\nalgorithm works for any string that have occurrences of\nﬁnite number of binary don’t cares.\n170Table 1: Using dynamic programming to ﬁnd all rep-\netitions. The bold values represent the lengths of all\nmaximal-pairs\n12345678910111213\nss#ll∗−ls∗−llls\n1s-100010110001\n2s -00010120001\n3# -0002003000\n4l -110010410\n5l -20010150\n6∗ -0110126\n7−l -002000\n8s -10001\n9∗ -0111\n10−l -000\n11l -10\n12l -0\n13s -\n4 ALGORITHM\nGiven a string xoverΣ∪{∗,#}, we construct two new\nstrings xsandxl. Where string xs(respectively, xl) is\nobtained by substituting each ∗bysand#by−s(re-\nspectively, each∗byland#by−l). The idea is to con-\nstructtwostringsbothover Σeachisacomplementofthe\nother in a sense that for each binary don’t care symbol in\ntheoriginalstringeachofthetwonewconstructedstrings\ncontains one of the two possible matching symbols. Note\nthat the sufﬁx trees of the two constructed strings can be\nbuilt in linear time.\nGiven xsandxl, each maximal-pair (p;i,j)inxcan\nbe considered as the concatenations of mright-maximal\nrepeating pairs:\n(p1;i,j),(p2;i+p1,j+p1),...,(pm;i+m−1/summationdisplay\nk=0pk,j+m−1/summationdisplay\nk=1pk),\nwhere\n1. thestarting-pair (p1;i,j)isamaximal-pair(i.e. left-\nand right-maximal) in either xsorxl,\n2. the collection of these right-maximal repeating pairs\nis distributed between xsandxli.e. one right-\nmaximalrepeatingpairisin xsandthefollowingre-\npeating pair is in xl,\n3.p=/summationtextm\nk=1pk.\nThe above states the main idea of our algorithm. The\nalgorithmiteratestwice. Intheﬁrstiteration,allmaxima l-\npairs in xwhose starting-pairs are in xsare calculated.\nIn the second iteration, all maximal-pairs in xwhose\nstarting-pairs are in xlare calculated.\nRecall that the starting-pair needs to be maximal.\nThus, each iteration starts by calculating all maximal-\npairs using the sufﬁx tree (as in Gusﬁeld). Then, each\nmaximal-pair is extended to the right by a sequence of\nright-maximal pairs using a series of jumpsfrom one suf-\nﬁx tree to another. In each attempt of jump we calculate\nthe depth of the lowest common ancestor of two nodes.\nFor example, if the starting-pair (p1;i,j)is a maximal-\npair in xsthenp2is equal to the depth of lowest common\nancestor of the two leaves i+p1andj+p1inT(xl).Similarly, p3is the depth of the lowest common ancestor\nofleaves i+p1+p2andj+p1+p2inT(xs)andsoon.\nFor example, if x=s s#l l∗−l s∗−l l l sthen\nxs=s s−s l l s−l s s−l l l sandxl=s s−l l l l−\nl s l−l l l s. Thesufﬁxtreesof xsandxlarerepresented\nin Figures 4 and 5.\nConsidernode u1∈T(xs). Duringthebottom-uptra-\nversal ofT(xs)and at node v1, the maximal-pair (2;1,8)\nis calculated. To check whether this starting-pair can be\nextended to the right, that is whether x[1 + 2]matches\nx[8 + 2]. Sincethey match, thealgorithmjumps to T(xl)\nand calculates the lowest common ancestor of leaves 1+2\nand8+2. Thelowestcommonancestorofthesetwoleaves\nisv2. Since depth(v2) = 3, the current repeating pair is\nextended to the right by the right-maximal repeating pair\n(3;3,10). Since x[3 + 3]matches x[10 + 3], the algo-\nrithm jumps back to T(xs)calculating the lowest com-\nmon ancestor of the two leaves 3+3 and 10 +3, that is\nv3. Since depth(v3) = 1, the current repeating pair\nis extended further to the right by the right-maximal re-\npeating pair (1;6,13). Because x[6 + 1]does not match\nx[13 + 1], no more jumps are possible. Then, the algo-\nrithm reports (6;1,8) as a maximal-pair in x. Moreover,\nsinceg= 8−1−6 = 1, then (6;1,8) is a repetition with\na hole in x(Lemma 1).\nThedetailsofthealgorithmarepresentedinFigures6\nand 7. For simplicity, algorithm Find-Maximal-Pairs as-\nsumes that bothT(xs)andT(xl)are binary sufﬁx trees.\nThis is always a valid assumption since that any sufﬁx\ntree can be transformed into binary one in O(n)time.\nThe function Report (p;i,j)reports the given maximal-\npair and checks according to Lemma 1 for all repetitions\nwitha hole. Note that if g=j−i−p≤0andj=i+ 1\nthenAll-Pairs isusedtoﬁndallpairs(notnecessarymax-\nimal) in x[i..j+p−1]. Algorithm All-Pairs is a simple\nmodiﬁcation of All-Maximal-Pairs .\n5 RUNNING TIME\nIn this section, we analyze the running time of All-\nRepetitions algorithm. Recallthat,forconstantsizealpha-\nbet,asufﬁxtreecanbebuiltin O(n)-time. Thus,creating\nbothT(xs)andT(xl)costsO(n)-time. Creatingtheleaf-\nlistsofallleavescosts O(n)-time. Ateveryinternalnode,\nthe algorithm reports the repetitions associated with this\nnode and constructs the leaf-sublists by concatenating the\nleaf-sublistsofthechildrenofthisnode. Thetotalcostfo r\ncreating the leaf-listsover all internal node is O(n)-time.\nForeachpossiblestarting-pairthealgorithmperforms\nseries of jumps form one tree to another. Each jump costs\nconstant time which is the cost of the lowest common an-\ncestor (LCA) query (Schieber and Vishkin, 1988). In the\nfollowingwewillestimateanupperboundforthenumber\nof jumps performed by the algorithm.\nObservethat,wejumpfrom T(xs)toT(xl)toextend\nthe current repeating pair to the right by a right-maximal\nrepeating pair in xl. This only possible if and only if the\nﬁrst two symbols of both two copies of this new right-\nmaximal repeating pair are either ∗andlor#and−l.\nSimilarly,wejumpbackfrom T(xl)toT(xs)ifandonly\nifthecurrentrepeatingpaircanbeextendedtotherightby\n171v3s\nv1s\n1xs[3..13]\n8\n−\nl l l s2xs[3..13]−\nl\n6xs[8..13]\n9\nlls13\n$3xs[3..13]l\ns\n5xs[7..13]\n12\n$\nls\n4xs[7..13]\n11\n$\n−l\n7xs[8..13]\n10\nlls\nFigure 4: The sufﬁx tree of xs$\ns\n1xl[2..13]\n8xl[9..13]\n2\nx\nl[3\n..13]\n13\n$\nl\n12sl\n11s\n4\nx\nl[6\n..13]\n5\nx\nl[7\n..13]\n−l\n6xl[8..13]\n9\nlls\n−l\n7xl[8..13]\nv2\nll\n10s\n3\nx\nl[6\n..13]\nFigure 5: The sufﬁx tree of xl$Algorithm All-Repetitions (x)\nInput:A string x[i..n]overΣ∪{∗,#}\nOutput:Allmaximal-pairsandallrepetitionswithahole\ninx\n1.fori= 1ton\n2. ifx[i] = ‘∗’\n3. thenxs[i] = ‘s’\n4. else if x[i] = ‘#’\n5. thenxs[i] =‘−s’\n6. elsexs[i] =x[i]\n7.fori= 1ton\n8. ifx[i] = ‘∗’\n9. thenxl[i] = ‘l’\n10. else if x[i] = ‘#’\n11. thenxl[i] =‘−l’\n12. elsexl[i] =x[i]\n13. Build the sufﬁx trees T(xs)andT(xl)\n14. Find-Maximal-Pairs( x,T(xs))\n15. Find-Maximal-Pairs( x,T(xl))\nFigure 6: All-Repetitions algorithm\nAlgorithm Jump&Report (x,i,j,c,d )\n1.length←d\n2.while x[i+length ] =x[j+length ]\n3. ifc= ‘s’thenc←‘l’\n4. elsec←‘s’\n5. v←T(xc).LCA(i+length,j +length )\n6. length←length +depth(v)\n7.Report (length ;i,j)\nAlgorithm Find-Maximal-Pairs (x,T(xc))\n1.foreach leaf node u∈T(xc)\n2. ifurepresents the ith sufﬁx of xc\n3. thenLLx[i−1](u)←{i}\n4. foreachα∈Σ∪{∗,#}andα/negationslash=x[i−1]\n5. LLα(u)←∅\n6.foreachinternalnode u∈(xc)inbottom-up(depth-\nﬁrst)manner\n7. u1,u2←the left and the right children of u\n8. foreach (i∈LLα1(u1)andj∈LLα2(u2))\nwhere α1/negationslash=α2\n9. if(x[i+depth(u)] =x[j+depth(u)])\n10. thenJump&Report (x,i,j,c,depth(u))\n11. elseReport (depth(u);i,j)\n12. foreachα∈Σ∪{∗,#}\n13. LLα(u)←LLα(u1)∪LLα(u2)\nFigure 7: Jump&Report andFind-Maximal-Pairs subrou-\ntines\n172a right-maximal repeating pair in xs, where the ﬁrst two\nsymbols of both copies of this repeating pair are either ∗\nandsor#and−s. Thus, the total number of jumps is\nO(d(n−d)),where disthetotalnumberofdon’tcaresin\nx. Summing the above gives that the total running time is\nas follows:\nTheorem 1 Given string x[1..n]∈ {Σ∪{∗,#}}∗, al-\ngorithmAll-Repetitions reports all maximal-pairs and\nall repetitions with a hole in xin space O(n)and time\nO(n+z+d(n−d)), where zis the output size and dis\nthe total number of binary don’t cares.\nClearly, the algorithm might have a quadratic running\ntime if the input string has n/2binary don’t care sym-\nbols. For example, ﬁnding all repetitions in string x=\n{sl}n/4∗n/2will cost O(n2)-time. This is asymptotically\nequal to the running time of the dynamic programming.\nIn practice, we expect our algorithm to have a better per-\nformance. Table 2 shows the values in the dynamic pro-\ngrammingmatrixthatarecalculatedusing All-Repetitions\nalgorithmtocomputeallmaximal-pairsandallrepetitions\nwithaholeinstring x=s s#l l∗−l s∗−l l l s. Note\nthat, in addition to the 22 reported repetitions, only 4 in-\ntermediate values have been calculated by our algorithm.\nTable 2: The values calculated and reported by All-\nRepetitions algorithm. The bold values represent the\nlengths of the reported maximal-pairs\n12345678910111213\nss#ll∗−ls∗−llls\n1s-1 1 1 1\n2s - 1 12 1\n3# - 2\n4l -11 1 1\n5l -2 1 15\n6∗ - 1 126\n7−l - 2\n8s -1 1\n9∗ - 111\n10−l -\n11l -1\n12l -\n13s -\n6 CONCLUSIONS\nIn this paper we have presented an algorithm that enables\nextraction of melodic patterns from abstract strings of\nsymbols; this abstract representation allows partial over -\nlappingbetweenthevariousabstractsymbolicclasses. As\naspecialcase,wehaveappliedtheproposedalgorithmon\nthe commonly used “step-leap” interval representation.\nIntermsofmelodicrepresentation,itissuggestedthat\na more reﬁned representation that comprises of a larger\nnumber of abstract interval classes (e.g. unison, step,\nsmall leap, medium leap, large leap) may actually enable\nthe extraction of better melodic patterns from the stand-\npoint of musical analysis or, even, music perception. Ad-\nditionally,theuseofrhythmic‘contour’,intermsofrhyth -\nmic abstract classes (e.g. equal, slightly larger, larger,\nmuch larger), may improve results further. Such repre-\nsentationshaveyettobestudied,implementedandtested.\nThe proposed algorithm requires extensive testing on\npattern extraction tasks, and its performance has yet to becompared withother similaralgorithms. This study, how-\never, has presented a novel problem in terms of melodic\nrepresentation and pattern extraction, and has attempted\nto provide an efﬁcient solution to it that can be used for\nfurther testing and evaluation.\nACKNOWLEDGEMENTS\nMaxime Crochemore is partially supported by CNRS,\nWellcomeFoundation,andNatogrants. CostasS.Iliopou-\nlos is partially supported by a Marie Curie fellowship,\nWellcome Foundation, Nato and Royal Society grants.\nManal Mohamed is supported by an EPSRC studentship.\nMarie-France Sagot is partially supported by French Pro-\ngramme BioInformatique Inter EPST, Wellcome Founda-\ntion, Royal Society and Nato grants.\nREFERENCES\nA. Amir, E. Porat, and M. Lewenstein. Approximate\nsubset matching with don’t cares. In Proceedings of\nthe twelfth annual ACM-SIAM symposium on Discrete\nalgorithms , pages 305–306. Society for Industrial and\nApplied Mathematics, 2001. ISBN 0-89871-490-7.\nA.Apostolico and F. P. Preparata. Optimal off-linedetec-\ntionofrepetitionsinastring. Theoret.Comput.Sci. ,22:\n297–315, 1983.\nE. Cambouropoulos. Musical parallelism and melodic\nsegmentation: A computational approach. music per-\nception.(forthcoming) .\nE. Cambouropoulos, M. Crochemore, C. Iliopoulos,\nL. Mouchard, and Y. Pinzon. Algorithms for comput-\ning approximate repetitions in musical sequences. J.\nComputer Mathematics , 79(11):1135–1148, 2002.\nR. Clifford and C. Iliopoulos. Approximate string match-\ning for music analusis. Soft Computing - A Fusion\nof Foundations, Methodologies and Applications , 8(9),\n2004.\nT. Crawford, C. Iliopoulos, and R. Raman. String\nmatchingtechniquesformusicalsimilarityandmelodic\nrecognition. Computing in Musicology , 11:73–100,\n1998.\nM. Crochemore and W. Rytter. Jewels of Stringology .\nWorld Scientiﬁc, 2002.\nM. Fischer and M. Paterson. String matching and other\nproducts. In R. Karp, editor, Complexity of Computa-\ntion SIAM-AMS Proceedings , pages 113–125, 1974.\nA. Ghias, J. Logan, D. Chamberlin, and B. Smith. Query\nbyhumming: Musicalinformationretrievalinanaudio\ndatabase. In ACM Multimedia , pages 231–236, 1995.\nD.Gusﬁeld. AlgorithmsonStrings,TreesandSequences:\nComputer Science and Computational Biology . Cam-\nbridge University Press, 1997.\nM. Hawley. Structure of Sound . PhD thesis, MIT, 1993.\nC. Iliopoulos, T. Lecroq, L. Mouchard, and Y. Pin-\nzon. Computingapproximaterepetitionsinmusicalse-\nquences. In Proc. of Prague Stringology Club Work-\nshop (PSCW’00) , pages 49–59, 2000.\n173C. S. Iliopoulos, M. Mohamed, L. Mouchard,\nK. Perdikuri, W. F. Smyth, and A. Tsakalidis.\nString regularities with don’t cares. Nordic Journal of\nComputing , 10(1):40–51, 2003.\nK. Lemstr ¨om and P. Laine. Musical information retrieval\nusing musical parameters. In Proc. International Com-\nputer Music Conference (ICMC ’98) , pages 341–348,\n1998.\nB. Schieber and U. Vishkin. On ﬁnding lowest common\nancestors: Simpliﬁcation and parallelization. SIAM\nJournal on Computing , 17(6):1253–1262, 1988.\n174"
    },
    {
        "title": "On Tuning the (\\delta, \\alpha)-Sequential-Sampling Algorithm for \\delta-Approximate Matching with Alpha-Bounded Gaps in Musical Sequences.",
        "author": [
            "Domenico Cantone",
            "Salvatore Cristofaro",
            "Simone Faro"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416892",
        "url": "https://doi.org/10.5281/zenodo.1416892",
        "ee": "https://zenodo.org/records/1416892/files/CantoneCF05a.pdf",
        "abstract": "The δ-approximate matching problem arises in many questions concerning musical information retrieval and musical analysis. In the case in which gaps are not allowed between consecutive pitches of the melody, transposition invariance is automatically taken care of, provided that the musical melodies are encoded using the pitch interval encoding. However, in the case in which nonnull gaps are allowed between consecutive pitches of the melodies, transposition invariance is not dealt with properly by the algorithms present in literature. In this paper, we propose two slightly different variants of the approximate matching problem under transposition invariance and for each of them provide an algorithm, obtained by adapting an efficient algorithm for the δ-approximate matching problem with α-bounded gaps. Keywords: approximate string matching, musical information retrieval, experimental algorithms. 1",
        "zenodo_id": 1416892,
        "dblp_key": "conf/ismir/CantoneCF05",
        "keywords": [
            "δ-approximate",
            "matching",
            "problem",
            "musical",
            "information",
            "retrieval",
            "musical",
            "analysis",
            "pitch",
            "interval"
        ],
        "content": "SOLVING THE ( δ,α)-APPROXIMATE MATCHING PROBLEM UNDER\nTRANSPOSITION INV ARIANCE IN MUSICAL SEQUENCES\nDomenico Cantone Salvatore Cristofaro Simone Faro\nUniversit `a di Catania, Dipartimento di Matematica e Informatica\nViale Andrea Doria 6, I-95125 Catania, Italy\ncantone,cristofaro,faro@dmi.unict.it\nABSTRACT\nTheδ-approximate matching problem arises in many\nquestions concerning musical information retrieval and\nmusical analysis. In the case in which gaps are not al-\nlowed between consecutive pitches of the melody, trans-\nposition invariance is automatically taken care of, pro-\nvided that the musical melodies are encoded using the\npitch interval encoding. However, in the case in which\nnonnull gaps are allowed between consecutive pitches of\nthe melodies, transposition invariance is not dealt with\nproperly by the algorithms present in literature.\nIn this paper, we propose two slightly different vari-\nants of the approximate matching problem under transpo-\nsition invariance and for each of them provide an algo-\nrithm, obtained by adapting an efﬁcient algorithm for the\nδ-approximate matching problem with α-bounded gaps.\nKeywords: approximate string matching, musical infor-\nmation retrieval, experimental algorithms.\n1 INTRODUCTION\nString matching is an extensively studied problem in com-\nputer science, mainly due to its direct applications to such\ndiverse areas as text, image and signal processing, speech\nanalysis and recognition, musical analysis, information\nretrieval, computational biology and chemistry, etc. For-\nmally the string matching problem consists in ﬁnding all\noccurrences of a given pattern Pin a text T, over some\nalphabet Σ.\nThe string matching problem has been generalized\nwith various notions of approximate matching.\nIn this paper we focus on the δ-approximate string\nmatching problem with α-bounded gaps , or (δ,α)-\nmatching problem for short, which arises in many ques-\ntions on musical information retrieval and musical anal-\nysis. This is especially true in the retrieval of given\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonmelodies from complex musical scores in the context of\nmonophonic music.\nMusical sequences can be viewed schematically as se-\nquences of integer numbers, representing either the notes\nin the chromatic or diatonic notation ( absolute pitch en-\ncoding , abbreviated as a.p.e. ), or the intervals between\nconsecutive notes, expressed as number of semitones\n(pitch interval encoding , abbreviated as p.i.e. ).\nThe latter representation is generally of greater interest\nin applications in tonal music, since absolute pitch encod-\ning disregards tonal qualities of pitches. In particular, t he\npitch interval encoding is transposition invariant , i.e. any\ntwo melodies differing only by an interval transposition\nare encoded in p.i.e. by a same numerical sequence.\nNote durations and note accents can also be encoded\nin numeric form, yielding richer alphabets whose symbols\ncan really be regarded as sets of parameters. For this rea-\nson alphabets used for music representation are generally\nquite large.\nIn the δ-approximate matching problem two integer\nstrings of the same length match if the corresponding inte-\ngers differ by at most a ﬁxed bound δ. Notice that if δ= 0,\ntheδ-approximate string matching problem reduces to the\nexact string matching problem.\nA signiﬁcant amount of research has been devoted to\nadapt solutions for exact string matching to δ-approximate\nmat chi ng ( s ee, f or i ns t ance, Cambour opoul os et al .,  1999 ;\nCr ochemor e et al ., 2001, 2002b ; Cant one et al .,  2004 ) .\nTheδ-approximate matching problem has been gen-\neralized by Crochemore et al. (2002a) by admitting also\nbounded gaps. They proposed an algorithm based on the\ndynamic programming approach. More recently, faster\nsolutions have been presented by Cantone et al. (2005a,b).\nAt an intuitive level, we say that a melody has a δ-\napproximate occurrence with α-bounded gaps within a\ngiven musical score, if the melody has a δ-approximate\nmatching with a subsequence of the musical score, in\nwhich it is allowed to skip up to a ﬁxed number αof sym-\nbols (the gap) between any two consecutive positions.\nFor example, in classical music compositions, and\nin particular in compositions for Piano Solo , it is quite\ncommon to ﬁnd musical pieces based on a sweet ground\nmelody, whose notes are interspaced by rapidly executed\narpeggios. In such cases, if one wants to retrieve all ap-\nproximate occurrences of a given melody from a musical\nscore, the value for αmust be chosen large enough so that\n460Figure 1: Bars Nr. 20 and Nr. 21 of the study Opus 10\nNr. 11 for Piano Solo by F. Chopin.\nthe secondary notes of each arpeggio can be skipped.\nFigure 1 shows two (partial) bars of the study Opus\n10 Nr. 11 for Piano Solo by F. Chopin illustrating such\na point. The notes of the melody are the higher notes of\neach arpeggiated chord. In this case a value α= 5would\nsufﬁce to retrieve the melody.\nThe above musical technicality is not by any means\nthe only one for which approximate matching with\nbounded gaps turns out to be very useful. Other examples\nare given by the use of musical ornaments ,pedal-notes ,\netc.\n2 MATCHING UNDER\nTRANSPOSITION INV ARIANCE\nIn the problem of ﬁnding musical similarities between\ngiven melodies, it is quite natural, among other things,\nto regard as similar any two melodies when they differ\nby an interval transposition. However, the deﬁnition of\n(δ,α)-matching used in previous works does not take into\naccount the requirement of transposition invariance.\nSince transposition invariance is a property which de-\npends only on the intervals between the pitches, rather\nthan on the pitches themselves, when gaps are not allowed\nthe approximation problem under transposition invariance\ncan be solved by any algorithms for the δ-approximate\nmatching problem, provided that the melodies are repre-\nsented using the pitch interval encoding.\nFor instance, let us consider the melodies S1andS2in\nFigure 2. If S1andS2are represented using the absolute\npitch encoding, then an approximation bound of δ≥7\nis needed to retrieve a match, whereas a value of δ= 1\nwould sufﬁce if both melodies were represented using the\npitch interval encoding.\nHowever, the use of the pitch interval encoding works\nwell only in the case in which gaps between pitches are\nnot allowed. Referring again to Figure 2, although the\nmelody S3has plainly a transposed δ-approximate occur-\nrence with α-bounded gaps in melody S2(withα= 1),\nsuch an occurrence would not be retrieved by the existing\n(δ,α)-approximate matching, unless a quite large value\nforδwere used. The main reason behind this fact is that\nwhen gaps are allowed, one has to devise a way to keep\nrelevant tonal information contained in the pitches of the\ngap which otherwise would be lost.\nFigure 2: Two excerpts from Toccata and Fuga in D minor\nby J.S. Bach.\nIn this section we propose two slightly different def-\ninitions of the δ-approximate matching problem with α-\nbounded gaps, which take into account transposition in-\nvariance.\n2.1 (δ,α)-Matching Under Transposition Invariance\nLetΣbe an alphabet of integer numbers and let δ,α, and\nτbe nonnegative integers. Two symbols aandbofΣare\nsaid to be δ-approximate if|a−b| ≤δ. In this case we\nsay that aandbhave a δ-match and we write a=δb.\nGiven a text Tof length nand a pattern Pof length\nm, aτ-transposed (δ,α)-occurrence of PinTat position\niis an increasing sequence of indices (i0,i1,... ,i m−1)\nsuch that\n1.0≤i0andim−1=i≤n−1;\n2.ih−ih−1≤α+ 1, for 1≤h≤m−1;\n3.|P[0]−T[i0]|=τ;\n4.P[j]−P[j−1] = δT[ij]−T[ij−1], for 1≤j≤\nm−1.\nThe above deﬁnition of δ-approximate occurrence\nwithα-bounded gaps is slightly different from the one\ngiven in Cantone et al. (2005a) in that it takes into ac-\ncountδ-matching of intervals between (immediately con-\nsecutive) pitches, rather than δ-matching of the pitches\nthemselves.\nGiven a text Tand a pattern P, we write P⊳i\nδ, α ,τT\nto mean that Phas aτ-transposed (δ,α)-occurrence in T\nat position i.\nThus we give the following deﬁnition\nDeﬁnition 1 Given a text T, a pattern P, and two non-\nnegative integers δandα, theδ-APPROXIMATE MATCH -\nING PROBLEM WITH α-BOUNDED GAPS UNDER TRANS -\nPOSITION INVARIANCE consists in ﬁnding all positions i\ninTsuch that P⊳i\nδ, α ,τT, for some τ≥0.\nAccording to the above deﬁnition, the melody S3of\nFigure 2 has a transposed (δ,α)-match within S1, pro-\nvided that both melodies are represented in a.p.e. and that\nδ, α≥1.\nNotice that when α= 0, theδ-approximate match-\ning problem with α-bounded gaps under transposition in-\nvariance reduces to the δ-approximate matching problem\nrelative to melodies represented in p.i.e.\n4612.2 (δ,α)-Matching Under Ranged-Transposition\nInvariance\nThe notion of τ-transposed (δ,α)-occurrence given be-\nfore takes into consideration only the intervals between\nimmediately consecutive pitches in the pattern melody P,\nnamely the intervals P[j]−P[j−1], forj= 1,...,m −1.\nThis deﬁnition allows to bound the local tonal varia-\ntions between pitches, but may lead to large global tonal\nvariations along the whole sequence. More precisely, a\ndifference δbetween any two consecutive pitches in a se-\nquence Pof length mcan lead to an overall difference of\nδ·(m−1)between the ﬁrst and the last symbols of the\nsequences. For instance, consider Figure 3, which shows\ntheCmajor scale (ﬁrst score) and the Awhole tone scale\n(second score). If we set δ= 2 andα= 1, then we can\nretrieve a transposed (δ,α)-occurrence of the whole tone\nscale as part of the major scale (in fact, we retrieve the\nDm13chord in arpeggiated form) indicated in the ﬁrst\nscore by the squared notes. Observe, however, that the\noverall difference between the ﬁrst and the last symbols\nof the sequences is 9semitones.\nHowever, to take care of transposition invariance,\nwe do not need to consider necessarily intervals formed\nbetween immediately consecutive pitches in the pattern\nmelody. To bound the global tonal variations of a matched\nsequence it may be more convenient to ﬁx a particular\npitch of the melody —the pivot— and then consider the\nintervals relative to such a pitch.\nChoosing the ﬁrst pitch of a melody as pivot leads to a\nnew deﬁnition of (δ,α)-occurrence of a pattern Pin a text\nT, which we call τ-ranged-transposed (δ,α)-occurrence\nofPinT(at a given position i). The new deﬁnition differs\nfrom the previous one in that clause 4 is replaced by the\nfollowing:\n4′.P[j]−P[0] = δT[ij]−T[i0], for 1≤j≤m−1.\nGiven a text Tand a pattern P, we write Pq⊳i\nδ, α ,τT\nto mean that Phas a τ-ranged-transposed (δ,α)-\noccurrence in Tat position i. Thus we end up with the\nfollowing deﬁnition:\nDeﬁnition 2 Given a text T, a pattern P, and two\nnonnegative integers δandα, the δ-APPROXIMATE\nMATCHING PROBLEM WITH α-BOUNDED GAPS UNDER\nRANGED -TRANSPOSITION INVARIANCE consists in ﬁnd-\ning all positions iinTsuch that Pq⊳i\nδ, α ,τT, for some\nτ≥0.\nIn general, approximate matching under ranged-\ntransposition invariance leads to more accurate match-\ning results than approximate matching under transposi-\ntion invariance deﬁned previously in Section 2.1. For in-\nstance, in Figure 3, to retrieve a ranged-transposed (δ,α)-\noccurrence, we need a value of δ≥9for the approxima-\ntion bound.\n3 TWO SEQUENTIAL-SAMPLING\nALGORITHMS\nIn this section we present two algorithms which solve the\napproximate matching problems introduced in the previ-\nFigure 3: The Cmajor scale and the Awhole tone scale.\nThe squared notes in the ﬁrst score represent an approx-\nimate occurrence under transposition invariance of the\nwhole tone scale.\nous section. Our algorithms, respectively named (δ,α)-T-\nSequential-Sampling and (δ,α)-RT-Sequential-Sampling,\nare modiﬁed versions of the (δ,α)-Sequential-Sampling\nalgorithm presented by the authors in Cantone et al.\n(2005a).\nWe begin by considering ﬁrst the (δ,α)-matching\nproblem under transposition invariance (cf. Deﬁnition 1),\nand then we turn our attention to the (δ,α)-matching\nproblem under ranged-transposition invariance (cf. Deﬁ-\nnition 2).\nGiven a text Tof length n, a pattern Pof length m,\nand two integers δ,α≥0, fori= 0,1,... ,n we put\nSi={(j,k):0≤j < i,\n0≤k < m , and\nPk⊳i\nδ, α ,τTfor some τ≥0},\nwhere Pkdenotes the preﬁx of Pof length k+ 1.\nNotice that S0=∅. If we put S=Sn, the problem\nof ﬁnding all positions iinTsuch that Pk⊳i\nδ, α ,τTfor\nsomeτ≥0translates into the problem of ﬁnding all val-\nuesisuch that (i,m−1)∈ S. To begin with, note that\nfori= 0,1,... ,n −1we have\nSi+1=Si∪ {(i,0)}\n∪ {(i,k) :k >0,(i−h,k−1)∈ Siand\nT[i]−T[i−h] =δP[k]−P[k−1]\nfor some 1≤h≤α+ 1}.\nThe above recursive relation, coupled with the initial con-\ndition S0=∅, allows one to compute the set Sin an iter-\native fashion, by computing in turn the sets S0,S1, ...,Sn,\nstarting from S0. Such computation can be accomplished\nby means of the (δ,α)–T-Sequential-Sampling algorithm\nshown in Figure 4, whose time and space complexities are\neasily seen to be respectively O(mnα )andO(mn).\nSince during each iteration of the for-loop at line 4\nof the (δ,α)-T-Sequential-Sampling algorithm at most\nm·(α+ 1) pairs of Sneed to be considered, it would\nbe enough to maintain only m·(α+ 1) pairs of Sat\neach step of the computation. With this modiﬁcation, the\nspace complexity of the (δ,α)-T-Sequential-Sampling\nalgorithm can be reduced to O(mα).\nTo solve the (δ,α)-matching problem under ranged-\ntransposition invariance, for each pair (i,k)with 0≤i <\nnand0≤k < m , we deﬁne a set Γ(i,k)which col-\n462(δ, α)-T-Sequential-Sampling (T,P,δ,α)\n1.n=length( T)\n2.m=length( P)\n3.S=∅\n4.fori= 0ton−1do\n5. fork=m−2downto 0do\n6. if∃h∈ {1, . . . , α + 1}: (i−h, k)∈Sand\n7. P[k+ 1]−P[k] =δT[i]−T[i−h]then\n8. S=S∪ {(i, k+ 1)}\n9. S=S∪ {(i,0)}\n10. fori= 0ton−1do\n11. if(i, m−1)∈Sthen output( i)\nFigure 4: The algorithm (δ,α)-T-Sequential-Sampling\nfor the δ-approximate matching problem with α-bounded\ngaps under transposition invariance.\n(δ, α)-RT-Sequential-Sampling (T,P,δ,α)\n1.n=length( T)\n2.m=length( P)\n3.fori= 0ton−1do\n4. fork=m−2downto 0do\n5. Γ(i, k+ 1) = ∅\n6. forh= 1toα+ 1 do\n7. foreachj∈Γ(max(0 , i−h), k)do\n8. ifP[k+ 1]−P[0] = δT[i]−T[j]then\n9. Γ(i, k+ 1) = Γ( i, k+ 1)∪ {j}\n10. Γ(i,0) ={i}\n11. fori= 0ton−1do\n12. ifΓ(i, m−1)/ne}ationslash=∅then output( i)\nFigure 5: The (δ,α)-RT-Sequential-Sampling algorithm\nfor the δ-approximate matching problem with α-bounded\ngaps under ranged-transposition invariance.\nlects the starting positions jof all τ-ranged-transposed\n(δ,α)-occurrences of PkinTending at position i. Then,\nthe problem of ﬁnding all positions iinTsuch that\nPq⊳i\nδ, α ,τT, for some τ≥0, translates into the prob-\nlem of ﬁnding all values isuch that Γ(i,m−1)/ne}ationslash=∅.\nWe observe that Γ(i,0) ={i}, for0≤i < n . In addition,\nwe have:\nΓ(i,k) ={j:P[k]−P[0] = δT[i]−T[j]and\nj∈Γ(max(0 ,i−h),k−1)for some\nh∈ {1,... ,α + 1}},\nfor0≤i < n and0< k < m .\nThus the computation of the sets Γ(i,k)can be done iter-\natively, by the (δ,α)-RT-Sequential-Sampling algorithm\nshown in Figure 5, whose time and space complexities are\neasily seen to be respectively O(m2nα)andO(mn).\n4 CONCLUSIONS\nExisting solutions to the approximate matching problem\nwith bounded gaps do not cover well the case in which\ntransposition invariance is enforced, regardless of the en -\ncoding used (either a.p.e. or p.i.e.).\nIn this paper, we have proposed two slightly differ-\nent variants of the approximate matching problem under\ntransposition invariance and for both of them we have\nshown how to adapt the (δ,α)-Sequential-Sampling algo-\nrithm to take care of transposition invariance.We plan to further optimize the algorithms presented.\nWe also intend to address other variants of the approxima-\ntion matching problem, relevant to applications in musical\nanalysis and musical information retrieval.\nREFERENCES\nE. Cambouropoulos, M. Crochemore, C. S. Iliopoulos,\nL. Mouchard, and Y . J. Pinzon. Algorithms for com-\nputing approximate repetitions in musical sequences.\nIn R. Raman and J. Simpson, editors, Proceedings of\nthe 10th Australasian Workshop on Combinatorial Al-\ngorithms , pages 129–144, Perth, WA, Australia, 1999.\nD. Cantone, S. Cristofaro, and S. Faro. Efﬁcient algo-\nrithms for the δ-approximate string matching problem\nin musical sequences. In M. ˇSim´anek and J. Holub, edi-\ntors, Proceedings of the Prague Stringology Conference\n’04, pages 69–82, Czech Technical University, Prague,\nCzech Republic, 2004.\nD. Cantone, S. Cristofaro, and S. Faro. An efﬁcient al-\ngorithm for δ-approximate matching with α-bounded\ngaps in musical sequences. In S.E. Nikoletseas, editor,\nProceedings of the 4th International Workshop on Ex-\nperimental and Efﬁcient Algorithms (WEA 2005) , num-\nber 3503 in Lecture Notes in Computer Science, pages\n428–439. Springer-Verlag, Berlin, 2005a.\nD. Cantone, S. Cristofaro, and S. Faro. On tuning the ( δ,\nα)-sequential-sampling algorithm for δ-approximate\nmatching with α-bounded gaps in musical sequences.\nSubmitted to ISMIR 2005. 2005b. Available at\nthe URLhttp://www.dmi.unict.it/ ∼cantone/\nCCF-ISMIR05.pdf .\nM. Crochemore, C. S. Iliopoulos, T. Lecroq, and Y . J.\nPinzon. Approximate string matching in musical se-\nquences. In M. Bal ´ık and M. ˇSim´anek, editors, Pro-\nceedings of the Prague Stringology Conference ’01 ,\npages 26–36, Prague, Czech Republic, 2001. Annual\nReport DC–2001–06.\nM. Crochemore, C. Iliopoulos, C. Makris, W. Rytter,\nA. Tsakalidis, and K. Tsichlas. Approximate string\nmatching with gaps. Nordic Journal of Computing , 9\n(1):54–65, 2002a.\nM. Crochemore, C. S. Iliopoulos, T. Lecroq,\nW. Plandowski, and W. Rytter. Three heuristics\nforδ-matching: δ-BM algorithms. In A. Apostolico\nand M. Takeda, editors, Proceedings of the 13th An-\nnual Symposium on Combinatorial Pattern Matching ,\nnumber 2373 in Lecture Notes in Computer Science,\npages 178–189. Springer-Verlag, Berlin, 2002b.\n463"
    },
    {
        "title": "Solving the (\\delta, \\alpha)-Approximate Matching Problem Under Transposition Invariance in Musical Sequences.",
        "author": [
            "Domenico Cantone",
            "Salvatore Cristofaro",
            "Simone Faro"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.11202953",
        "url": "https://doi.org/10.5281/zenodo.11202953",
        "ee": "https://zenodo.org/records/11202953/files/uoi-Gegiou-Zorba-Papaioannou_ppt.pdf",
        "abstract": "The academic librarian's role shifting from administrative staff providing access to scientific information to training staff in the management of that content is increasingly identified as resources digital dissemination has been established, especially under Open Access Dissemination arrangement. The role of the information professional as an educator is expanding to that of a producer of learning material for the management of scientific information, a teacher trainer and an educational partner, making a substantially higher quality contribution to achieving the objectives set by the Institutions. Information literacy skills are placed at the heart of academic skills development models, as the educational and research process and performance is qualitatively determined by the management of information needs. The educational work of academic librarians and their related collaborations has been explored for many years. Positive results of these collaborations have been documented, but still the need for teaching competence of information professionals is highlighted with particular emphasis. This article discusses the role of the academic librarian as an information content producer and as a teaching partner. At the same time, special reference is made to challenges that sometimes act as an impediment to the success of collaborative activities between libraries and educational staff. The Library of the University of Ioannina responded to an invitation for cooperation from the Center for Teaching and Learning Support (KEDIMA) to develop learning content in the context of the goals and needs of KEDIMA. With the main objective of mutual contribution, a memorandum of understanding was signed in which a model of cooperation between an academic library and a learning and teaching unit was defined. This is a recent collaborative action, which is evolving dynamically and reinforces the optimistic scenario for consolidating the role of the Library as a producer of learning content and a teaching partner.",
        "zenodo_id": 11202953,
        "dblp_key": "conf/ismir/CantoneCF05a",
        "keywords": [
            "academic librarian",
            "role shifting",
            "administrative staff",
            "scientific information",
            "management of content",
            "Open Access",
            "information professional",
            "educator",
            "producer of learning material",
            "educational partner"
        ],
        "content": "Γέγιου , Α. ( MSc ), Ζορμπά. Ι. (MA, PhD), Παπαϊωάννου, Α. ( MRes , PhD)Βιβλιοθήκη & Κέντρο Πληροφόρησης Πανεπιστημίου ΙωαννίνωνΗ ακαδημαϊκή βιβλιοθήκη ως \nπαραγωγός μαθησιακού περιεχομένου: \nη περίπτωση συνεργασίας με Κέντρο \nΔιδασκαλίας και Μάθησης (ΚΕΔΙΜΑ)\n29ο Πανελλήνιο Συνέδριο Ακαδημαϊκών Βιβλιοθηκών\n\"Ακαδημαϊκές Βιβλιοθήκες: Επιταχύνοντας την έρευνα και την ανάπτυξη, \nεμπλουτίζοντας την ακαδημαϊκή ζωή\" 27 -29 Νοεμβρίου 2023, ΑΠΘΡόλος : ο βιβλιοθηκονόμος εκπαιδευτής·\nΔιεύρυνση ρόλου : ο βιβλιοθηκονόμος \nπαραγωγός πληροφοριακού περιεχομένου και \nεκπαιδευτικός συνεργάτης·\nΖητούμενο : εμπλουτισμός προσόντων·\nΠαράδειγμα : το Πανεπιστήμιο Ιωαννίνων·\nΠροοπτική : ευελιξία, ανοικτότητα και \nσυνεργατικότηταΤι θα δούμε σήμερα εν τάχει;ο βιβλιοθηκονόμος\nεκπαιδευτήςΡόλος [1]\nΠηγή: Blue Skies: A new definition of information literacy. A New Curriculum for Information Literacy. (2012, January 13)ο βιβλιοθηκονόμος\nεκπαιδευτήςΡόλος [2]\nΠηγή: Standards and Proficiencies for Instruction Librarians and Coordinators Revision Task Force. (2018, March 9). Roles and stren gths of teaching \nlibrarians. Association of College & Research Libraries (ACRL). https://www.ala.org/acrl/standards/teachinglibrarians…από διαθέτη οδηγιών χρήσης υπηρεσιών \nΒιβλιοθήκης & αναζήτησης πηγών\n…σε:\nπαραγωγό μαθησιακού περιεχομένου·\nεκπαιδευτή διδασκόντων·\nεκπαιδευτικό συνεργάτη.Διεύρυνση ρόλουπαραγωγός μαθησιακού περιεχομένου\n◼Mount Royal University (Καναδάς)\nεκπαιδευτής διδασκόντων\n◼University of Kentucky (ΗΠΑ)\n◼University of North Carolina Wilmington (ΗΠΑ)\nεκπαιδευτικός συνεργάτης\n◼Tampere University of Technology (Φιλανδία)\n◼Park University (ΗΠΑ)\n◼University of Louisville (ΗΠΑ)Διεύρυνση ρόλου : παραδείγματαανάπτυξη διδακτικών δεξιοτήτων·\nαναγνώριση του εκπαιδευτικού ρόλου από την \nπλευρά των διδασκόντων·\nμετατόπιση από τη διάθεση τυποποιημένου \n«γενικού μαθήματος» οδηγιών στη διάθεση \nποικίλου, επίκαιρου και ευέλικτου μαθησιακού \nπεριεχομένου.Ζητούμενοτο Πανεπιστήμιο ΙωαννίνωνΠαράδειγμα\n… τα πρώτα παράγωγα [1]\n… τα πρώτα παράγωγα [2]\n✓Μεταδεδομένα\n✓Πλοήγηση στα περιεχόμενα\n✓Επιλεκτικός υποτιτλισμός\n✓Συνοδευτικό προσβάσιμο αρχείο κειμένουΒιβλιοθήκη: ανάπτυξη μαθησιακού \nπεριεχομένου\n◼Πνευματικά δικαιώματα\n◼Δικαίωμα επαναδημοσίευσης\n◼Δικαίωμα επικαιροποίησης περιεχομένου\nΚΕΔΙΜΑ: τεχνική υποστήριξη & διάθεση \nπαραγόμενου υλικούΠαράδειγμα συνεργασίαςαπό την αποκλειστική διάθεση τυποποιημένων \nοδηγιών ενός σεμιναρίου ( one-shot library \ninstruction sessions)\nσε διαρκή διάθεση  συνεδριών μικρής διάρκειας \nπολυθεματικού επικαιροποιημένου και \nεπίκαιρου περιεχομένουΜετάβασηΣυνεργατικότητα\nΑνοικτότητα\nΕυελιξίαΠροοπτική\nCredit: Lightspring  / Shutterstock.com © 2022Γέγιου , Αγάθη  -agegiou@uoi.gr\nΒιβλιοθήκη & Κέντρο Πληροφόρησης Πανεπιστημίου ΙωαννίνωνΕυχαριστούμε!\nΗ ακαδημαϊκή βιβλιοθήκη ως παραγωγός μαθησιακού περιεχομένου:  \nη περίπτωση συνεργασίας με Κέντρο Διδασκαλίας και Μάθησης (ΚΕΔΙΜΑ)Ζορμπά, Ιωάννα  -izorba@uoi.gr\nΠαπαϊωάννου, Αριέττα  - apapaioan@uoi.gr \n29ο Πανελλήνιο Συνέδριο Ακαδημαϊκών Βιβλιοθηκών\n\"Ακαδημαϊκές Βιβλιοθήκες: Επιταχύνοντας την έρευνα και την ανάπτυξη, \nεμπλουτίζοντας την ακαδημαϊκή ζωή\" 27 -29 Νοεμβρίου 2023, ΑΠΘ"
    },
    {
        "title": "Frame-Level Audio Feature Extraction Using AdaBoost.",
        "author": [
            "Norman Casagrande",
            "Douglas Eck",
            "Balázs Kégl"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414718",
        "url": "https://doi.org/10.5281/zenodo.1414718",
        "ee": "https://zenodo.org/records/1414718/files/CasagrandeEK05.pdf",
        "abstract": "In this paper we adapt an AdaBoost-based image processing algorithm to the task of predicting whether an audio signal contains speech or music. We derive a frame-level discriminator that is both fast and accurate. Using a simple FFT and no built-in prior knowledge of signal structure we obtain an accuracy of 88% on frames sampled at 20ms intervals. When we smooth the output of the classifier with the output of the previous 40 frames our forecast rate rises to 93% on the Scheirer-Slaney (Scheirer and Slaney, 1997) database. To demonstrate the efficiency and effectiveness of the model, we have implemented it as a graphical real-time plugin to the popular Winamp audio player. 1",
        "zenodo_id": 1414718,
        "dblp_key": "conf/ismir/CasagrandeEK05",
        "keywords": [
            "AdaBoost",
            "image processing",
            "audio signal",
            "speech or music",
            "frame-level discriminator",
            "FFT",
            "accuracy",
            "Scheirer-Slaney database",
            "smooth output",
            "Winamp audio player"
        ],
        "content": "Frame-LevelSpeech/Music Discrimination using AdaBoost\nNorman Casagrande\nUniversityof Montreal\nDepartmentof Computer Science\nCP6128, Succ. Centre-Ville\nMontreal,Quebec H3C 3J7 Canada\ncasagran@iro.umontreal.caDouglas Eck\nUniversityof Montreal\nDepartmentof Computer Science\nCP 6128, Succ. Centre-Ville\nMontreal,Quebec H3C 3J7 Canada\neckdoug@iro.umontreal.caBal´azsK´egl\nUniversityof Montreal\nDepartment of Computer Science\nCP 6128, Succ. Centre-Ville\nMontreal,Quebec H3C 3J7 Canada\nkegl@iro.umontreal.ca\nABSTRACT\nInthispaperweadaptanAdaBoost-basedimageprocess-\ning algorithm to the task of predicting whether an audio\nsignal contains speech or music. We derive a frame-level\ndiscriminator that is both fast and accurate. Using a sim-\nple FFT and no built-in prior knowledge of signal struc-\nture we obtain an accuracy of 88% on frames sampled at\n20ms intervals. When we smooth the output of the clas-\nsiﬁer with the output of the previous 40 frames our fore-\ncastraterisesto93%ontheScheirer-Slaney(Scheirerand\nSlaney,1997)database. Todemonstratetheefﬁciencyand\neffectiveness of the model, we have implemented it as a\ngraphical real-time plugin to the popular Winamp audio\nplayer.\n1 Introduction\nTheabilitytoautomaticallydiscriminatespeechfrommu-\nsic in an audio signal is useful in domains where a partic-\nulartypeofinformationisofinterest,suchasinautomatic\naudio news transcription of a radio broadcast, where non-\nspeech would presumably be discarded. Previous mod-\nels have employed a mixture of simple features that cap-\nture certain temporal and spectral features of the signal\n(Scheirer and Slaney, 1997; Saunders, 1996). including\nfor example pitch, amplitude, zero crossing rate, cepstral\nvaluesandlinespectralfrequencies(LSF).Morerecently,\nother approaches have used the posterior probability of a\nframe being in a particular phoneme class (Williams and\nEllis, 1999), HMMs that integrate posterior probability\nfeaturesbasedonentropyand“dynamism”(Ajmeraetal.,\n2002), and a mixture of Gaussians on small frames (Ez-\nzaidiand Rouat, 2002).\nWehaveadaptedasuccessfulandrobustapproachfor\nobject detection (Viola and Jones, 2001) to this task. Our\nmodel works by exploiting regular geometric patterns in\nspeech and non-speech audio spectrograms. These reg-\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitationon the ﬁrst page.\nc/circlecopyrt2005Queen Mary,Universityof Londonularities are detectable visually, as demonstrated by the\nability of trained observers to identify speech structure\n(e.g. vowel formant structure, consonant onsets) and mu-\nsical structure (e.g. note onsets and harmonic pitch struc-\nture) through visual inspection of a spectrogram. We\ndemonstrateinthispaperthatbyexploitinggeometricreg-\nularitiesinatwo-dimensionalrepresentationofsound,we\nare able to obtain good accuracy results (88%) for 20ms\nframecategorizationwithnobuilt-inpriorknowledgeand\nat very low computational cost. When smoothing is em-\nployed over 40 previous frames (800ms), our accuracy\nrises to 93%. This compares favorably with other mod-\nels on the same dataset.\nDespitebeingmotivatedbyworkinvision,thismodel\niswellsuitedforaudiosignalprocessing. Thoughittreats\nindividual20msslicesofmusicashavingﬁxedgeometry,\nit places no limitations on the geometry of entire songs.\nFor example, it places no constraints on song length nor\ndoesitrequirerandomaccesstotheaudiosignal. Inother\nwords,thisapproachiscausalandisabletoprocessaudio\nstreams online and in real time.\n2 The algorithm\nIn order to build a good binary discriminator, one must\nﬁrst ﬁnd a set of salient featuresthat separate the two\nclasseswiththelargestmarginpossible. Todetectobjects\nin an image, Viola and Jones employed a set of simple\nHaar-like (ﬁrst proposed by Papageorgiou et al. (1998))\nrectangles depicted in Figure 1. These features compute\nand subtract the sum of pixels in the white area from the\nsum of pixels in the black area. The areas can have dif-\nferent shapes and sizes, and can be placed at different x\nandycoordinates of the image. A discriminator using a\nsingle of these features is called a weak learner because,\nused alone, it cannot achieve very good discrimination.\nHowever,whenthesefeaturesarecombinedinanadditive\nmodel, the resulting classiﬁer can perform very well. In\ntheir work on two-dimensional images, Viola and Jones\nshowed that with enough features, it is possible to detect\ncomplexobjects likefaces.\n2.1 AdaBoost\nTo additively combine the weak learners, we use the\nADABOOSTalgorithm (Freund and Schapire, 1996),\n345Figure 1: The two Haar-like features used in our additive\nmodel.\nwhich is one of the best general purpose learning meth-\nods developed in the last decade. It has inspired several\nlearningtheoreticalresultsand,duetoitssimplicity,ﬂexi-\nbility,andexcellentperformanceonreal-worlddata,ithas\ngainedpopularity among practitioners.\nADABOOSTis anensemble (ormeta-learning )\nmethod that constructs a classiﬁer in an iterative fashion.\nIn each iteration, it calls a simple learning algorithm (the\nweak learner ) that returns a classiﬁcation. The ﬁnal clas-\nsiﬁcationwillbedecidedbyaweighted“vote”oftheweak\nclassiﬁers, where each weight is proportional to the cor-\nrectness of the corresponding weak classiﬁer. This incre-\nmental process of combining weak classiﬁers weighed by\ntheirperformanceiscalled boosting. Theweakclassiﬁers\nneed only be slightly better than a random guess, which\nlends great ﬂexibility to the design of the weak classiﬁer\n(orfeature)set. Ifthereisnoparticulara-prioriknowledge\navailableonthedomainofthelearningproblem,smallde-\ncision trees or, in the extreme case, decision stumps (de-\ncision trees with two leaves) are often used. A decision\nstump can be deﬁned by three parameters, the index jof\nthe attribute1that it cuts, the threshold θof the cut, and\nthesign of the decision. Formally,\nhj,θ+(x) =(\n1ifxj≥θ,\n−1otherwise,(1)\nand\nhj,θ−(x) =−hj,θ+(x) =(\n1ifxj< θ,\n−1otherwise.(2)\nAlthough decision stumps may seem very simple, when\nboosted, they yield excellent classiﬁers in practice. Also,\nﬁnding the best decision stump using exhaustive search\ncan be done efﬁciently in O(nd)time, where nis the\nnumber of training points, and dis the dimension of the\ninputspace(thenumberofHaar-likefeaturesinourcase).\nAlternatively, by using a random sampling and a gradi-\nentfollowingapproach(asexplainedbelow),eventheex-\nhaustivesearch can be avoided.\nFor the formal description of ADABOOST, let the\ntraining set be Dn=©\n(x1, y1), . . . , (xn, yn)ª\n, where\nxiis the observation vector, and yiis its binary ( +1\nor−1, representing speech or music, respectively) la-\nbel. The algorithm maintains a weight distribution wt=¡\nwt\n1, . . . , wt\nn¢\nover the data points. The weights are ini-\ntialized uniformly at the beginning, and are updated in\neach iteration. The weight distribution remains normal-\nized in each iteration, that is,Pn\ni=1wt\ni= 1for all t. In\ngeneral, the weight of a point will be proportional to how\nhardit is to correctly classify.\n1In our case, the jth attribute is the output of the jth ﬁlter in\ntheﬁlter bank (see Section 2.2).Given: Dn=©\n(x1, y1), ...,(xn, yn)ª\nwhere xi∈X, y i∈Y={−1,+1}\nInitializeweights w1(i) = 1 /n.\nFort= 1, ..., T:\n•Findthe feature htthatminimizes the error /epsilon1t.\n•Computeconﬁdence αt=1\n2ln1−/epsilon1t\n/epsilon1t.\n•Updatethe weight vector w:\nwt+1(i) = wt(i)×(\n1\n2(1−/epsilon1t)ifht(xi) =yi\n1\n2/epsilon1tifht(xi)/negationslash=yi\nOutputﬁnal hypothesis:\nH(x) = signÃTX\nt=1αtht(x)!\n.\nFigure2: Pseudocode of the ADABOOSTalgorithm.\nWe suppose that we are given a set Hof weak classi-\nﬁers and a weak learner algorithm that, in each iteration\nt, returns the weak classiﬁer ht∈ Hthat minimizes the\nweighted error\n/epsilon1t=nX\ni=1I{ht(xi)/negationslash=yi}wt\ni, (3)\nwhere the indicator function I{A}is1if its argument\nAis true and 0otherwise. The coefﬁcient αtofhtis\ntheconﬁdence we have in our weak learner. It is set to\nαt=1\n2ln1−/epsilon1t\n/epsilon1tin each iteration. Since /epsilon1t<1/2(other-\nwise we would ﬂip the labels and return −ht), the weight\nupdateformulas(seeinthepseudocodeinFigure2),indi-\ncate that we increase the weights of misclassiﬁed points\nand decrease the weights of correctly classiﬁed points.\nAs the algorithm progresses, the weights of frequently\nmisclassiﬁed points will increase, so weak classiﬁers will\nconcentrate more and more on these “hard” data points.\nAfter Titerations2,thealgorithmreturnstheweightedav-\nerage fT(·) =PT\nt=1αtht(·)of the weak classiﬁers. The\nsign of fT(x)is then used as the ﬁnal classiﬁcation of x.\n2.2 The features\nOur goal is to classify 20ms frames of audio as being ei-\nther speech or music. We represent each training sample\nby its spectrogram Si={S(t, φ)}i, where S(t, φ)is the\nsignal intensity at time tand frequency φ. We then con-\nvolve the image of the spectrogram with Haar-like ﬁlters\n(depictedinFigure1),ﬁndthebestﬁlterthatdiscriminates\nthetrainingdata,andcomputeastump(1-2)overtheout-\nput of the ﬁlter.\nEach ﬁlter contains two or three rectangular black or\nwhite blocks with different sizes and locations. For a\nblack block with its upper left corner placed at (t, φ), and\n2Tisanappropriatelychosenconstantthatcanbesetby,for\nexample,cross-validation.\n346withsize wt×wφ, we compute the convolution\nBt,φ,w t,wφ(S) =t+wtX\ni=tφ+wφX\nj=φS(i, j).\nFora white block, we compute the negativeconvolution\nWt,φ,w t,wφ(S) =−Bt,φ,w t,wφ(S) =−t+wtX\ni=tφ+wφX\nj=φS(i, j).\nSo, for example, a three block white-black-white feature\nplacedat (t, φ),andwithblocksize wt×wφwouldoutput\nthevalue\nWt,φ,w t,wφ(S)+Bt,φ+wφ,wt,wφ(S)+Wt,φ+2wφ,wt,wφ(S).\nThe major advantage of these features over more compli-\ncatedﬁltersusuallyusedinsound-processingthattheycan\nbe computed at an extremely low cost. The main trick is\ntopre-compute the so called integralimage\nΣ(t, φ) =tX\ni=1φX\nj=1S(i, j) (4)\nfor each spectrogram in the training set. Then any convo-\nlution BorWcanbecomputedinconstanttimebyusing\ntheequation\nBt,φ,w t,wφ(S) = Σ( t, φ) + Σ( t+wt, φ+wφ)\n−Σ(t, φ+wφ)−Σ(t+wt, φ).\nThis allows us to evaluate a very large number of candi-\ndate features in every boosting iteration. Formally, each\nHaar-like ﬁlter gjreturns a real number gj(Si)for each\nspectrogram Si, which is the jth attribute xj\niin the obser-\nvation vector xi. Then for each ﬁlter, the best decision\nstump is found. Finally, we select the weak learner ht\nwhich minimizes the weighted training error (3) among\nallthe candidates.\nDespitethesimplicityoftheﬁlters,theycandiscrimi-\nnate between speech and music by capturing local depen-\ndencies in the spectrogram. For example, the three-block\nfeature depicted in Figure 3 is well-correlated with the\nspeech signal and quasi-independent of the music signal.\nFigure 4 displays the real-valued output of the ﬁlter for\nall training points, and the threshold of the optimal deci-\nsion stump. This feature, selected in the ﬁrst iteration of\nADABOOST,has a 30% error rate on the test set.\n2.3 Reducing the training time\nOneof the problemswith the Haar-likeﬁltersapproach is\nthat at each iteration only one optimal feature is selected,\namonghundredsofthousandsofpossiblecandidates. This\nresultsinacomputationalcost,forthetrainingprocess,in\nthe order of days on a single Pentium 4. To reduce the\ncomputational demand, we have implemented a solution\nthat comes from the observation that small changes in the\nfeature’s property do not change the overall error signiﬁ-\ncantly. Figure5showstheerrormapofthehorizontalfea-\nture at the ﬁrst iteration. Notice that the two-dimensional\n0 2000 4000 6000 8000050100150AmplitudeSpeech\n0 2000 4000 6000 8000050100150\nMusicAmplitude\nFrequency (Hz)\nFigure 3: This three-block feature can distinguish speech\nfrom music. It is well correlated with the speech signal\n(its output is 347), and independent of the music signal\n(its output is -577).\n−400 −200 0 200 40000.10.20.30.40.50.60.70.80.91\nFeature 1 outputspeech\nmusic\nthreshold\nFigure4: TheoutputofthefeatureshowedinFigure3for\nalltrainingpoints,anditsdecisionstump’sthreshold. The\ndatahasbeenrandomlydistributedontheverticalaxisfor\nclarity.\n347Feature's Position\nFeature's Height2040608010012050\n100\n150\n200\n2503035404550\nFigure 5: The error as a function of the two feature’s pa-\nrameters. Dark areas represents local minima that the al-\ngorithmislookingfor. Theheightofthefeaturedecreases\nlinearly with the position, therefore the map is triangu-\nlar. Observe that the resulting two-dimensional terrain is\nrelatively smooth, suggesting that exhaustive sampling of\ntheseparameters may be unnecessary.\nspaceissmoothlychanging. Thissuggeststhatitisunnec-\nessarytoexhaustivelysampleallparametercombinations.\nAfter ﬁrst selecting randomly a set of initial sampling pa-\nrameters, we iteratively adjust them until we arrive at a\nlocalminimum in a gradientfollowing fashion.\nThe feature thus discovered is suboptimal with re-\nspect to the exhaustive search. However, ADABOOST\nrequires only that features support discrimination better\nthan chance. For instance, Escudero (2000) showed with\nhisLazyBoosting thatevenwithrandomly(homogeneous)\nfeatures it was possible to achieve results comparable to\nthose obtained from the optimal features. To compensate\nforthe poorer features, a largernumber wasrequired.\n2.4 Experimental results\nIn the experiments, we used 240 digital audio ﬁles of 15\nsecond radio extracts published by Scheirer and Slaney\n(1997)3. We extracted 11200 20ms frames from this\ndata,normalizedthem,andthenprocessedthemwithFFT,\nRASTA (Hermansky et al., 1992), and log-scale FFT. We\nchose the ﬁrst because it is the simplest representation of\nthefrequencyspectrum,andtheothertwobecauseoftheir\npopularity in speech processing. The FFT represents the\nbiggest frequency space with 256 points, followed by the\nother two (respectively 26 and 86). The size of this space\nhas an important impact on the training time, but thanks\nto the gradient approach it is limited. During detection,\nmoreover, the size of the space does not play any role,\nthanksto the integralimage representation (4).\nFigure 6 shows the training and test errors using the\nFFT. The choice for the optimal number of features does\n3The data was collected at random from the radio by Eric\nScheirer during his internship at Interval Research Corporation\ninthesummerof1996underthesupervisionofMalcolmSlaney.\n0 50 100 150 20005101520253035\n# of iterations (features)% of errorTrain − No smoothing\nTest − No smoothing\nTrain − Smoothing\nTest − Smoothing\nFigure 6: The results on a 20ms FFT frame, without and\nwith smoothing. The beneﬁts of smoothing are clearly\nseen both in test error and train error.\nTable1: Testingerror rate using single frame ﬁlters.\nFFT\nRASTA\nLog-FFT\nWithoutsmoothing\n11.9%\n10.8%\n12.6%\nWithsmoothing\n6.7%\n7.2%\n7.4%\nnot have the same importance as in other machine learn-\ning algorithms (e.g., neural networks) because of the in-\ntrinsic resistance of ADABOOSTto over-ﬁtting: even if\nthe training error tends to zero, the test set error does not\nincrease. It is therefore less important to ﬁnd a speciﬁc\nstopping point, except for efﬁciency reasons. We can ob-\nservethatataframelevel,onasimpleFFTwealreadyob-\ntain an error rate of about 13% after 150 iterations, which\nis far better than the 37% of the best frame-level feature\nin (Scheirer and Slaney, 1997). We then decided to tie\nthe classiﬁcation of a frame to classiﬁcation at previous\nframes with a simple smoothing function. Let f(xτ)be\nthe output of the strong learner, where xτis a frame at\ntimeτ. Then, the newoutput used for classiﬁcation is\ng(xτ) =Pτ\ni=max(τ−nframes +1,0)aτ−if(xi)\nPτ\nj=max(τ−nframes +1,0)aτ−j,(5)\nwhere ais a decay parameter between 0and1and where\nnframes is an integer that corresponds to the number of\npast frames to consider. In order to ﬁnd the best values\nofaandnframes , we randomly concatenated the audio\n(wave) ﬁles of the validation set and measured the classi-\nﬁcationerrorratesforseveralvaluesforthedecayparam-\neter. We chose this procedure in order to approximately\nsimulate audio streaming from a radio station and get the\nbest values at a= 0.98andnframes = 40. With these\nsettings the error reaches a value less than 7% with less\nthan a second of information. The error converges after\n150 iterations, but even with a much smaller number of\nfeatures, such as 75, the error levelis below10%.\nSurprisingly, RASTA and logarithmic scale FFT rep-\nresentation did not perform as well, even though the re-\nsults are still below 10%. Table 1 summarizes the errors\n348on the test set with these representations. Also, RASTA\nand Log-FFT converged much faster than simple FFT,\nwhichcanbeexplainedinbothcasesbythehigherquality\nofthe information and the limited dimensionality.\n3 A WinampPlugin\nFigure 7: A Winamp plugin version of our model. The\nlong thin horizontal line displays the decision boundary\nbetweenSpeech(top)andMusic(bottom). Thelongthick\nhorizontal line displays the decision of the model, with\nvertical position proportional to conﬁdence. Here, for ex-\nample, the model is moderately conﬁdent that the stream\nis Music (in this case, a correct prediction). The shorter\nhorizontal lines on the left provide a histogram of the de-\ncisions and conﬁdence ratings of the weak learners. See\ntextfor details.\nWinamp ( www.winamp.com ) from Nullsoft is a\npopular audio player for the Windows operating system.\nLike many multimedia software applications, Winamp\nuses modular “plug ins” that extend the functionality of\nthe basic player. Because the development kit for writing\nthese plugins is available for public use, we were able to\ndevelop a version of our model that runs within Winamp.\nThis plugin is able to predict in real time whether the au-\ndio stream being played in the Winamp player contains\nmusic or speech.4Because the plugin is distributed al-\nreadytrained,itrequiresfewcomputationalresourcesand\nshouldrun well on anycomputer that can run Winamp.\nIn Figure 7 a screenshot of the plugin is shown. The\nlong thick horizontal line displays the decision of the\nmodel. The distance of this decision line from the bound-\nary between Speech (top) and Music (bottom) is propor-\ntionaltotheconﬁdencethemodelhasinitsdecision. This\nconﬁdence is computed as a sum of weak learner votes\n(+1forSpeech,-1forMusic)multipliedbytherespective\nweak learner conﬁdences (alpha, α, inADABOOST). See\nSection2.1 for details.\nThe shorter horizontal lines on the left in the screen-\nshot display a histogram of the weak learner votes where\nthe vertical position yof each of these shorter lines is\n4The plugin is available for free download at www.iro.\numontreal.ca/˜casagran/winamp/index.html .given by the vote (+1 for Speech, -1 for Music) multi-\nplied by conﬁdence. In addition, conﬁdence is also used\ntoscalethewidthofeachoftheselines. Iftwoweaklearn-\nersfallonthesameverticalpositionintheplot,theircon-\nﬁdence is summed, generating a single longer line. The\nthree horizontal lines near the bottom of the plot are indi-\nvidual weak learners having particularly high conﬁdence\nthat the stream is music.\nTheplotisrefreshedevery20ms,resultinginasmooth\nanimation of model performance. Though the graphical\ninterface is relatively primitive, it can be instructive to\nwatch model behavior evolve over time in response to an\naudio stream.\nFor the Winamp plugin, we trained a version of the\nclassiﬁcation model using randomly-selected segments\nfrom a number of Internet radio streams. Labeling\nwas achieved implicitly by treating talk-radio streams as\nspeechexamplesandmusic-radiostreamsasmusicexam-\nples. Though training streams were not labeled by hand,\nwe did control against contamination of speech with mu-\nsic and vice-versa through careful listening. We did not\ncreateahand-labeledtestingsetofmixedspeechandmu-\nsic radio streams. For this reason we are unable to report\na reliable error rate for the Winamp plugin (nor was this\nour goal). We were able to demonstrate that the model\nruns efﬁciently and perform well in a real-world applica-\ntion. We observed that the output of the model is stable\nand reliable for a range of input streams.\n4 Conclusions\nWe have showed how a simple generic object recognition\nalgorithm can be used also to perform frame-level classi-\nﬁcation of audio by exploiting geometric regularities in a\nﬁxed-sized two-dimensional representation of frame con-\ntents. Because of the strong relationship among frames\nin time, we can increase the performance of the classiﬁer\nwith a simple smoothing on the output of the frame-level\nclassiﬁer. Itisalsopossibletodotrainingdirectlyonaset\nofsubsequent framesto capture localdependencies in the\ntime domain. However, such an approach would have to\ndeal with the problem of the absolute position of the fea-\ntures in time, and therefore would probably be working if\nthe number of frames is limited.\nAlso it may be helpful to explore the use of different\nbasicfeatures(suchasGaussiansorband-passes),anddif-\nferentrepresentationssuchaswaveletsorsine-waverepli-\ncas (Liebenthal et al., 2001).\nFinally,whilethecurrentmodelislimitedtotwo-class\ncategorization, we are exploring a multi-class version of\nADABOOST(Schapire,1999). Thiswouldallowustoex-\ntendourworktomorechallengingclassiﬁcationproblems\nsuch as speaker identiﬁcation singer identiﬁcation, music\ninstrument identiﬁcation and music genre classiﬁcation.\nACKNOWLEDGEMENTS\nWe would like to thank Stanislas Lauly for help with\nworking with data, and Hugo Larochelle for his sugges-\ntions. We would also like to thank Dan Ellis for helpful\ncomments and for providingthe dataset.\n349References\nJitendra Ajmera, Iain A. McCowan, and Herv ´e Bourlard.\nRobustHMMbasedspeech/musicsegmentation. Proc.\nofICASSP-02 , 2002.\nGerard Escudero, Lluis Mrquez et German Rigau. Boost-\ning Applied to Word Sense Disambiguation Proceed-\nings of the 12th European Conference on Machine\nLearning,ECML. Barcelona ,Spain. 2000.\nH. Ezzaidi and J. Rouat. Speech, music and songs dis-\ncriminationinthecontextofhandsetsvariability. Proc.\nofICSLP 2002, 16-20 September 2002.\nYoav Freund, Robert E. Schapire. A decision-theoretic\ngeneralization of on-line learning and an application to\nboosting. Journal of Computer and System Sciences ,\n55(1):119-139,August 1997.\nHynek Hermansky, Nelson Morgan, Arun Bayya, Phil\nKohn. RASTA-PLP speech analysis technique. IEEE\nInternational Conference on Acoustics, Speech, and\nSignalProcessing ,1992. ICASSP-92.\nEinat Liebenthal, Jeffrey R. Binder, Rebecca L.\nPiorkowski and Robert E. Remez. Sinewave\nspeech/nonspeech perception: An fMRI study. The\nJournal of the Acoustical Society of America May 1,\n2001- Volume109, Issue 5, pp. 2312-2313.\nConstantineP.Papageorgiou,MichaelOren,TomasoPog-\ngio. A general framework for object detection. Inter-\nnationalConferenceon Computer Vision ,1998.\nJohn Saunders. Real-time discrimination of broadcast\nspeech/music. Proc. IEEE Int. Conf. on Acoustics,\nSpeech, Signal Processing (Atlanta, GA) , pp. 993-996,\nMay1996.\nRobert E. Schapire. A Brief Introduction to Boosting.\nProc.oftheSixteenthInternationalJointConferenceon\nArtiﬁcialIntelligence , 1999.\nEric Scheirer, Malcolm Slaney. Construction and evalua-\ntion of a robust multifeature speech/music discrimina-\ntor.Proc. of the 1997 IEEE International Conference\non Acoustics, Speech, and Signal Processing . Volume\n2,pp. 1331, 1997.\nPaul A. Viola, Michael J. Jones. Robust Real-time Ob-\nject Detection. International Conference on Computer\nVision,pp. 747, 2001.\nGethin Williams and Daniel P.W. Ellis. Speech/music\ndiscrimination based on posterior probability features.\nProc.EuropeanConferenceonSpeechCommunication\nandTechnology , Sept. 1999, pp. 687-690.\n350"
    },
    {
        "title": "Foafing the Music: A Music Recommendation System based on RSS Feeds and User Preferences.",
        "author": [
            "Òscar Celma",
            "Miquel Ramírez",
            "Perfecto Herrera"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414800",
        "url": "https://doi.org/10.5281/zenodo.1414800",
        "ee": "https://zenodo.org/records/1414800/files/CelmaRH05.pdf",
        "abstract": "In this paper we give an overview of the Foafing the Music system. The system uses the Friend of a Friend (FOAF) and Rich Site Summary (RSS) vocabularies for recommending music to a user, depending on her musical tastes. Music information (new album releases, related artists’ news and available audio) is gathered from thousands of RSS feeds —an XML format for syndicating Web content. On the other hand, FOAF documents are used to define user preferences. The presented system provides music discovery by means of: user profiling —defined in the user’s FOAF description—, context-based information —extracted from music related RSS feeds— and content-based descriptions —extracted from the audio itself. 1",
        "zenodo_id": 1414800,
        "dblp_key": "conf/ismir/CelmaRH05",
        "keywords": [
            "Friend of a Friend (FOAF)",
            "Rich Site Summary (RSS)",
            "music recommendation",
            "user profiling",
            "context-based information",
            "content-based descriptions",
            "music discovery",
            "music information",
            "RSS feeds",
            "XML format"
        ],
        "content": "FOAFING THE MUSIC: AMUSIC RECOMMEND ATION SYSTEM BASED\nONRSS FEEDS AND USER PREFERENCES\n\u001eOscar Celma\nMusic Technology Group\nUniversitat Pompeu Fabra\nPg.Circumv al\u0001laci´o8,08003\nBarcelona, SPAIN\nocelma@iua.upf.eduMiquel Ram ´\u0011rez\nMusic Technology Group\nUniversitat Pompeu Fabra\nPg.Circumv al\u0001laci´o8,08003\nBarcelona, SPAIN\nmramirez@iua.upf.eduPerfecto Herr era\nMusic Technology Group\nUniversitat Pompeu Fabra\nPg.Circumv al\u0001laci´o8,08003\nBarcelona, SPAIN\npherrera@iua.upf.edu\nABSTRA CT\nInthispaper wegiveanovervie woftheFoa\u0002ng theMusic\nsystem. Thesystem uses theFriend ofaFriend (FOAF)\nand RichSite Summary (RSS) vocabularies forrecom-\nmending music toauser,depending onhermusical tastes.\nMusic information (newalbumreleases, related artists'\nnewsandavailable audio) isgathered from thousands of\nRSS feeds an XML format forsyndicating Webcon-\ntent. Ontheother hand, FOAFdocuments areused to\nde\u0002ne user preferences.\nThe presented system provides music disco veryby\nmeans of:user pro\u0002ling de\u0002ned intheuser' sFOAF\ndescription, conte xt-based information extracted\nfrom music related RSS feeds andcontent-based de-\nscriptions extracted from theaudio itself.\n1INTR ODUCTION\nThe World WideWebhasbecome thehost anddistrib u-\ntion channel ofabroad variety ofdigital multimedia as-\nsets. Although theInternet infrastructure allowssimple,\nstraightforw ardacquisition, thevalue ofthese resources\nlacks ofpowerful content management, retrie valandvi-\nsualization tools. Music content isnoexception: although\nthere isasizeable amount oftext-based information about\nmusic (albumreviews,artist biographies, etc.) thisinfor -\nmation ishardly associated totheobjects theyrefer to,\nthatismusic music \u0002les (MIDI and/or audio). Music isan\nimportant vehicle forcommunicating other people some-\nthing relevantabout ourpersonality ,history ,etc.\nIntheconte xtoftheSemantic Web,there isaclear in-\nterest tocreate aWebofmachine-readable homepages de-\nscribing people, thelinks among them, andthethings they\ncreate anddo.TheFOAF(Friend OfAFriend )project1\nprovides conventions andalanguage totell amachine\nthesort ofthings that auser says about herself inher\n1http://www .foaf-project.or g\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonhomepage. FOAFisbased ontheRDF/XML2vocabu-\nlary.Wecanforesee thatwith theuser' sFOAFpro\u0002le,\nasystem would getabetter representation oftheuser' s\nmusical needs. Ontheother hand, theRSS vocabulary3\nallowstosyndicate Webcontent onInternet. Syndicated\ncontent includes data such asnewsfeeds, events listings,\nnewsstories, headlines, project updates, etc.\nFoa\u0002ng-the-music initiati veiscovered bytheSIMA C\nISTproject4.The main goal ofSIMA Cproject isdoing\nresearch onsemantic descriptors ofmusic contents, inor-\ndertousethem, bymeans ofasetofprototypes, forpro-\nviding song collection exploration, retrie valandrecom-\nmendation services. These services aremeant forhome\nusers, music content producers anddistrib utors, andaca-\ndemic users. One special feature isthat these descrip-\ntions arecomposed bysemantic descriptor s.Music will\nbetagged using alanguage close totheuser' sownwayof\ndescribing itscontents mo ving thefocus from low-level\ntohigher -level(i.e.semantic) descriptions.\n2BACKGR OUND\nRecommender Systems aresoftw areapplications whose\npurpose istodeliverinformation topeople thatneeds\nit.Inthisway,onecannot tellthedifference between a\nrecommender system andasearch engine both softw are\ntypes share thesame purpose: toselect objects (oritems)\nfrom arepository whose features were found tosatisfy the\nquerying user' sneeds.\nHowever,there aretwosubtle butmeaningful differ-\nences between search engines andrecommender sys-\ntems. The \u0002rst oneliesinthedesign intention, orbet-\ntersaid: thewording oftheproblem toaddress when de-\nsigning thesystem. Isthatinformation need related to\nsolving acontingent situation, oristhatneed something\nperiodic orstatic? Thesecond difference isalso another\ndesign intention difference, which liesintheuseoftwo\ndifferent words todescribe thesystem: does itretrieve\ninformation from arelati velystatic repository ofinforma-\ntion? Ordoes it\u0002lter objects embedded inanincoming\nstream ofinformation?\nThe term recommender system emer gedasalogi-\ncalevolution oftheresearch oninformation retrie val(IR)\n2http://www .w3.or g/RDF\n3http://web .resource.or g/rss/1.0/\n4http://www .semanticaudio.or g\n464systems. This evolutivemain feature wastheemphasis put\nonthequery concept de\u0002nition andrepresentation. Rec-\nommender systems were initially thought asinformation\n\u0002ltering systems, whose technological frame workbase-\nlinestemmed from information retrievalsystems (Belkin\nandCroft, 1997). This, then, effectivelyimplies that a\nrecommender system isaninherently dual purpose appli-\ncation: theuserpro\u0002ling ofstatic information needs might\nbeused tobetter understanding andattending immediate,\nunforeseen needs.\nThere aretwomain approaches torecommend items\ntousers: collaborati ve\u0002ltering andcontent-based \u0002lter-\ning. Nextsection explains thedifferences between both\napproximations.\n2.1 Collaborati ve\u0002ltering versus Content-based\n\u0002ltering\nCollaborati ve\u0002ltering consists ofmaking useoffeedback\nfrom users toimpro vethequality ofmaterial presented to\nusers. Obtaining feedback canbeexplicit orimplicit. Ex-\nplicit feedback comes intheform ofuser ratings oranno-\ntations, whereas implicit feedback canbeextracted from\nuser' shabits. One ofthemain caveats ofthisapproach\nisthefactthat theonly waytorecommend brand new\nitems isthatsome user hastopreviously rateorreview\nthatitem. There aresome examples thatsucceed based on\nthisapproach. Forinstance, Amazon isagood illustration\nsystem (Linden etal.,2003).\nOntheother hand, content-based \u0002ltering tries toex-\ntract useful information from theitems oftheuser' s\ncollection thatcould beuseful torepresent user' sneeds.\nThis approach solvesthelimitation ofcollaborati ve\u0002lter-\ningasitcanrecommend newitems (evenbefore thesys-\ntemdoes notknowanything from thatitem), bycompar -\ningtheactual setofuser' sitems andcalculating adistance\nwith some sortofsimilariy measure. Inthemusic \u0002eld,\nextracting musical semantics from therawaudio andcom-\nputing similarities between music pieces isachallenging\nproblem. Traditional music similarity measures uselow-\nlevelmainly timbre-based features. Webelivethat\nadding cultural metadata terms tosuch asimilarity mea-\nsure canhelp togetbetter results.\nPachet (2005) proposes aclassi\u0002cation ofmusical\nmetadata, andhowthisclassi\u0002cation affects music con-\ntentmanagement, aswell astheproblems tofacewhen\nelaborating aground truth reference formusic similarity\n(both incollaborati veandcontent-based \u0002ltering).\n2.2 Friend ofafriend initiati ve\nTheFriend ofafriend (FOAF)initiati veprovides aframe-\nworkforrepresenting information about people, their in-\nterests, relationships between them and social connec-\ntions. TheFOAFvocabulary contains terms fordescrib-\ningpersonal information name, nick, mailbox, inter-\nest, images, etc., membership ingroups member ,\ngroup, organization, etc.FOAFisbased ontheRDF/XML\nvocabulary .Listing 1showsapossible input \u0002le5.A\n5Arealexample extracted from http://www .livejournal.com ,\nonly changing theuser' snameFOAFdescription, then, describes in amachine read-\nable format aperson. Currently ,there aremore then 6\nmillion ofFOAFdocuments describing people ontheweb\n(Golbeck, 2005).\n<rdf:RDF xml:lang =\"en\">\n<foaf:Person >\n<foaf:nick >a_user</foaf:nick >\n<foaf:dateOfBirth >04-17</\nfoaf:dateOfBirth >\n<foaf:mbox_sha1sum >ce24ca...a1f0</\nfoaf:mbox_sha1sum >\n<foaf:page >\n<foaf:Document\nrdf:about =\"http://www.livejournal .\ncom/userinfo .bml?user=a_user\">\n<dc:title >LJProfile</dc:title >\n</foaf:Document >\n</foaf:page >\n<foaf:weblog rdf:resource =\"http://www.\nlivejournal .com/users/a_user/\"/>\n<foaf:interest dc:title =\"gretsch \"\nrdf:resource =\"http://www.livejournal\n.com/interests .bml?int=gretsch \"\n/>\n<foaf:interest dc:title =\"dogsd'amour\"\nrdf:resource =\"http://www.livejournal\n.com/interests .bml?int=dogs+d%27\namour\"/>\n<foaf:interest dc:title =\"social\ndistortion \"\nrdf:resource =\"http://www.livejournal\n.com/interests .bml?int=social+\ndistortion \"/>\n<foaf:interest dc:title =\"beer\"\nrdf:resource =\"http://www.livejournal\n.com/interests .bml?int=beer\"/>\n<foaf:interest dc:title =\"themisfits\"\nrdf:resource =\"http://www.livejournal\n.com/interests .bml?int=the+\nmisfits\"/>\n<foaf:interest dc:title =\"thepogues\"\nrdf:resource =\"http://www.livejournal\n.com/interests .bml?int=the+\npogues\"/>\n</foaf:Person >\n</rdf:RDF >\nListing 1:Example ofauser' sFOAFpro\u0002le\nToourknowledge, nowadays itdoes notexistanysys-\ntemthatrecommends items toauser,based onherFOAF\npro\u0002le (Celma etal.,2004). The FilmTrust system6is\napart ofaresearch study aimed tounderstanding how\nsocial preferences might help web sites topresent infor -\nmation inamore useful way.The system collects user\nreviewsandratings about movies, andholds them intothe\nuser' sFOAFpro\u0002le. Although ithasnotyetimplemented\narecommendation system, itincludes arating algorithm\nfor\u0002lms based onatrust-based algorithm (Golbeck and\nParsia, 2005).\n2.3 Music recommendation systems\nThe main goal ofamusic recommendation system isto\npropose, totheend-user ,interesting andunkno wnmusic\nartists (and their available tracks if possible), based\nonhermusical taste. Butmusical taste andmusic pref-\nerences areaffected byseveralfactors, evendemographic\nandpersonality traits. Then, thecombination ofmusic\n6http://trust.mindsw ap.or g/FilmT rust\n465preferences and personal aspects such as:age, gen-\nder,origin, occupation, musical education, etc. could\nimpro vemusic recommendations (Uitdenbogerd andvan\nSchyndel, 2002).\nMoreo ver,amusic recommendation system should be\nable togetnewmusic dynamically ,asitshould recom-\nmend newitems totheuser once inawhile. Inthissense,\nthere isalotoffreeavailable (interms oflicensing) mu-\nsiconInternet, performed byunkno wn artists thatcan\nsuitperfectly fornewrecommendations. Nowadays, mu-\nsicwebsites arenoticing theuser about newreleases or\nartist' srelated news,mostly intheform ofRSS feeds.\niTunes Music Store7provides anRSS (version 2.0) feed\ngenerator8,updated once aweek, thatpublishes newre-\nleases ofartists' albums. Amusic recommendation sys-\ntemshould takeadvantage ofthese publishing services, as\nwell asintegrating them intothesystem, inorder to\u0002lter\nandrecommend newmusic totheuser.\nMost ofthecurrent music recommenders arebased on\ncollaborati ve\u0002ltering approach, oronahybrid version in-\ncluding clustering andusers' communities Examples of\nsuch systems are: Audioscrobbler9,iRate10,Goombah\nEmer gent Music11,MusicStrands12,andinDisco ver13.\nThebasic idea ofamusic recommender system based\noncollaborati ve\u0002ltering is:\n1.Tokeep track ofwhich artists auser listens to\n(through WinAmp, XMMS, etc.plugins),\n2.To\u0002ndother users with similar tastes, and\n3.Torecommend similar artists totheuser,according\ntothese similar listeners' taste.\nBut, digital music collections canbehuge (thousands\nof\u0002les) andveryheterogeneous. Thus, thisapproach to\nrecommend music cangenerate some silly (orobvious)\nanswers.\nContrastingly ,themain goal oftheFoa\u0002ng theMu-\nsicsystem istorecommend, todisco verand toex-\nplore music content; based onuser pro\u0002ling via FOAF\ndescriptions, conte xt-based information extracted\nfrom music related RSS feeds, andcontent-based de-\nscriptions extracted from theaudio itself.\n3SYSTEM OVER VIEW\nMusic recommendations, intheFoa\u0002ng theMusic system,\naregenerated through thefollowing steps:\n1.Getinterests from user' sFOAFpro\u0002le,\n2.Detect artists andbands,\n3.Select related artists, from artists encountered inthe\nuser' sFOAFpro\u0002le, and\n7http://www .apple.com/itunes\n8http://phobos.apple.com/W ebObjects/MZSearch.w oa/w o/0.1\n9http://www .audioscrobbler .com\n10http://irate.sourcefor ge.net\n11http://goombah.emer gentmusic.com/\n12http://www .musicstrands.com\n13http://www .indisco ver.net/4.Rate results byrelevance.\nThesystem reads aninput FOAFpro\u0002le that is,an\nRDF/XML \u0002le, andextracts user' sinterests. Then, it\nqueries amusic repository inorder todetect whether the\ninterest isamusic artist (oraband), andselects similar\nartists totheones found. Togetartists' similarities, a\nfocused web crawled hasbeen implemented tolook for\nrelationships between artists (such as:related to,in\u0003u-\nenced by,followers of,etc.). Moreo ver,amusic similarity\ndistance isused torecommend tracks thataresimilar to\ntracks composed orplayed byartists found intheFOAF\npro\u0002le.\nBased ontheprevious FOAFexample (see listing 1),\nthesystem detects thefollowing artists from theuser' s\npro\u0002le: Dogsd'Amour ,Social Distortion ,The Mis\u0002ts\nandThePogues.Starting from these artists, thesystem\nsearches forsimilar artists andforartists in\u0003uenced by\nthem. Then, itscores them interms ofcounting artist oc-\ncurrences. Ifthere areanytracks inthemusic repository\nfrom artists declared intheFOAFpro\u0002le, itcomputes the\nsimilarity andgetsthemost signi\u0002cant similar tracks from\nother artists. Figure 1showstheoutput recommended\nartists.\nOnce therelated artists havebeen selected, Foa\u0002ng the\nMusic \u0002lters music related information coming form RSS\nfeeds to:\n\u000fGetnewmusic releases,\n\u000fDownload (orstream) audio from MP3-blogs and\nPodcast sessions, and\n\u000fCreate, automatically ,playlists based onaudio simi-\nlarity .\nAnother component ofthesystem isthe(music re-\nlated) newsfeeds \u0002ltering. Nextsection explains themain\ncharacteristics ofthiscomponent.\n3.1 Music related news \u0002ltering\nThe music related news\u0002ltering component queries a\nnewsfeeds system that\u0002lters newsrelated artists found in\nuser' sFOAFpro\u0002le. Todoso,thiscomponent permits\ntocommunicate with thePubSub server14,viatheJab-\nberprotocol, andcreates anRSS feed with agivenquery\nthe user musical preferences found intheFOAF\u0002le.\nPubSub isamatching service thatinstantly noti\u0002es auser\nwhene vernewcontent matching user' ssubscription iscre-\nated. PubSub reads over13million weblogs andmore\nthan 50,000 internet newsgroups. Jabber15isanopen se-\ncure protocol, anad-free alternati vetoconsumer instant\nmessaging services likeICQ, MSN, andYahoo. Jabber\nmakesuseofXML protocols thatenable anytwoentities\nontheInternet toexchange messages, presence, andother\nstructured information innearly realtime.\nOnce thesubscription hasbeen created, itispossible\ntovisualize allthemusic related newsforagivenuser.\nEach newsitem hasabarscore thatshowshowmuch itis\n14http://www .pubsub .com\n15http://www .jabber .org\n466Figure 1:Recommended artists from artists detected inauser' sFOAFpro\u0002le.\nrelated with user' smusical interests. Scoring isdone using\ntheTF/IDF ranking algorithm (Baeza-Y atesandRibeiro-\nNeto, 1999). TF/IDF ranks documents bycounting the\nnumber ofocurrences ofuser' sterm query intoeach doc-\nument.\nFoa\u0002ng the Music system isavailable at:\nhttp://www .semanticaudio.or g/foa\u0002n-the-music\n4CONCLUSIONS\nWehaveproposed asystem that\u0002lters music related infor -\nmation from RSS based onagivenuser' spro\u0002le. Asys-\ntembased onFOAFpro\u0002les allowstounderstand auser\nintwocomplementary ways; psychological factors \npersonality ,demographic preferences, socio-economics,\nsituation andexplicit musical preferences. This system,\nthen, isable to\u0002lter andtoconte xtualize users' queries.\nInthemusic \u0002eld conte xt,weexpect thatusing news\n\u0002ltering about newmusic releases, artists' intervie ws,al-\nbumreviews,etc.canimpro vearecommendation system\ninadynamic way.Finally ,thisapproach opens awide\nrange ofpossible usages andapplications, such asnoti-\nfying auser theforthcoming gigs byanartist playing\nclose touser' slocation whose music issimilar touser' s\nmusical taste.\nFinally ,theevaluation ofthesystem isplanned tobe\ndone byrealusers, bycomparing themusic recommenda-\ntionresults ofoursystem (intheartist level)with amusic\nrecommender based oncollaborati vesystem, such asAu-\ndioscrobbler .ACKNO WLEDGEMENTS\nThis workispartially funded bytheSIMA CIST-FP6-\n507142 European project.\nREFERENCES\nR.Baeza-Y atesandB.Ribeiro-Neto. Modern Information\nRetrie val.Addison-W esley,\u0002rstedition, 1999.\nN.J.Belkin andW.B.Croft. Information \u0002ltering and\ninformation retrie val:Twosides ofthesame coin?\nCommunications oftheACM,35(12):2939, December\n1997.\nO.Celma, M.Ram´\u0011rez, andP.Herrera. Semantic interac-\ntionwith music content using foaf. InProceedings of\n1stWorkshop onFriend ofaFriend, Social Networking\nandtheSemantic Web,Galway,Ireland, 2004.\nJ.Golbeck. Computing andApplying TrustinWeb-based\nSocial Networks .PhD thesis, 2005.\nJ.Golbeck andB.Parsia. Trustnetw ork-based \u0002ltering of\naggre gated claims. InInternational Journal ofMeta-\ndata, Semantics, andOntolo gies(toappear) ,2005.\nG.Linden, B.Smith, andJ.York. Amazon.com recom-\nmendations: Item-to-item collaborati ve\u0002ltering. IEEE\nInternet Computing ,4(1), 2003.\nF.Pachet. Knowledg eMana gement andMusical Meta-\ndata.Idea Group, 2005.\nA.Uitdenbogerd andR.vanSchyndel. Areviewoffac-\ntorsaffecting music recommender success. InISMIR\n3rdInternational Confer ence onMusic Information Re-\ntrieval,October 13-17. ,2002.\n467"
    },
    {
        "title": "Detection of Key Change in Classical Piano Music.",
        "author": [
            "Wei Chai",
            "Barry Vercoe"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415538",
        "url": "https://doi.org/10.5281/zenodo.1415538",
        "ee": "https://zenodo.org/records/1415538/files/ChaiV05.pdf",
        "abstract": "Tonality is an important aspect of musical structure. Detecting key of music is one of the major tasks in tonal analysis and will benefit semantic segmentation of music for indexing and searching. This paper presents an HMM-based approach for segmenting musical signals based on key change and identifying the key of each segment. Classical piano music was used in the experiment. The performance, evaluated by three proposed measures (recall, precision and label accuracy), demonstrates the promise of the method.",
        "zenodo_id": 1415538,
        "dblp_key": "conf/ismir/ChaiV05",
        "keywords": [
            "tonality",
            "musical structure",
            "key detection",
            "tonal analysis",
            "semantic segmentation",
            "music indexing",
            "searching",
            "HMM-based approach",
            "key change",
            "label accuracy"
        ],
        "content": "DETECTION OF KEY CHANGE IN  CLASSICAL PIANO MUSIC\nWei Chai Barry Vercoe \nMIT Media Laboratory \nCambridge MA, USA \n{chaiwei, bv}@media.mit.edu  \nABSTRACT \nTonality is an important aspect of musical structure. \nDetecting key of music is one of the major tasks in tonal \nanalysis and will benefit semantic segmentation of music \nfor indexing and searching. This paper presents an HMM-based approach for segmenting musical signals \nbased on key change and identifying the key of each \nsegment. Classical piano music was used in the experiment. The performance, evaluated by three \nproposed measures (recall, precision and label accuracy), \ndemonstrates the promise of the method.  \nKeywords: key detection, music segmentation, Hidden \nMarkov Models.  \n1 INTRODUCTION \nTonality is an important aspect of musical structure. It describes the relationships between the elements of \nmelody and harmony. Detecting key of music is one of \nthe major tasks in tonal analysis. Developing \ncomputational models to mimic the perception and detection of key will help automate the analysis of \ndevelopment of musical themes and emotion.  \nFrom the practical perspective, semantic \nsegmentation of music, including segmentation based on key change, will benefit intelligent music editing \nsystems and automatic indexing of music repository.  \nFurthermore, detection of key is a critical step for \nfinding repeated patterns in music for music indexing and searching. For example, Foote [1] proposed a representation called self-similarity matrix for analyzing \nthe recurrent structure of music, where a repetition \ntypically will result in a diagonal pattern in the self-similarity matrix. However, if a theme repeats at a \ndifferent key, without considering the key change, the \ndiagonal pattern will not appear and the repetition will not be detected. For example, Figure 1 shows the self-\nsimilarity matrix zoomed in at a repetition of the theme \nat a different key in Mozart’s piano sonata. The diagonal \npattern could not be seen from the original self-\nsimilarity matrix representation without considering key change. However, if know the key change in advance \nand adjust accordingly when  comparing two frequency vectors to get the self-similarity matrix, the diagonal \npattern will come out. \n \nji\n2580 2600 2620 2640 2660 2680 2700 272020\n40\n60\n80\n100\n120\n140\nji\n2580 2600 2620 2640 2660 2680 2700 272020\n40\n60\n80\n100\n120\n140\n \nFigure 1.  Zoom in of the last repetition in  \n“Mozart: Piano Sonata No. 15 In C” (left: original \nself-similarity matrix; right: key-adjusted self-\nsimilarity matrix). \n \nThis paper presents an HMM-based generative model \nfor automatic key detection of music.  Specifically, given a musical piece (or part of it), the system will segment it into sections based on key change and \nidentify the key of each section. Please note that here we \nwant to segment the piece and identify the key of each segment at the same time. A simpler task could be, \ngiven a segment of a particular key, detecting the key of \nit. In fact, previous research on key detection of acoustic musical signals typically assumes that the musical \nsegment remains the same key,  so that the algorithm can \nanalyze the pitch profile of the segment to infer the key \n[2,3,4].    \nAnother related work was done by Sheh [5], who \ninvestigated a similar problem of segmenting musical \nsignals based on chord change and identifying the chord \nof each segment, where EM -trained Hidden Markov \nModels were employed. \nThe remainder of this paper is organized as follows. \nSection 2 gives a brief introduction of musical key and other relevant terms. Section 3 presents the chromagram \nrepresentation and the framework of using Hidden \nMarkov Models (HMMs) for detecting key change. Section 4 demonstrates the promise of the method by \nexperiments using classical piano music and some \nevaluation metrics. Section 5 concludes the paper and proposes future work. \n2 MUSICAL KEY AND MODULATION \nIn Music theory, the key is the tonal center of a piece. It \ncan be either in major or minor mode. A scale  is an \nascending or descending series of notes or pitches The \nchromatic scale  is a musical scale that contains all \ntwelve pitches of the Western tempered scale. The diatonic scale  is most familiar as the major scale or the \n\"natural\" minor scale. The major mode has half-steps Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made  or distributed for profit or \ncom-mercial advantage and that copies bear this notice and the \nfull citation on the first page. \n© 2005 Queen Mary, University of London \n468   \n \n between scale steps 3 and 4, and 7 and 8. The natural \nminor mode has half-steps between 2 and 3, and 5 and 6. \nA piece may change key at some point. This is called \nmodulation . Modulation to the dominant (a fifth above \nthe original key) or the subdominant (a fourth above) is \nrelatively easy, as are modulations to the relative major \nof a minor key or to the relative minor of a major key. A \nthing needs to mention is that there might be ambiguity \nof key. It can be hard to determine the key of a quite \nlong passage. Some music is even atonal , meaning there \nis no tonal center. Thus, in this paper, we will focus on tonal music with least ambuiguity of tonal center. \n3 APPROACH \nThis section presents an  HMM-based approach for \ndetecting key change in classical piano music. \n3.1 Chromagram Representation \nChromagram, also called the P itch Class Profile features \n(PCP), is a frame-based representation very similar to \nShort-time Fourier Transform (STFT). It combines the \nfrequency components in STFT belonging to the same \npitch class and results in a 12-dimensional \nrepresentation, corresponding to C, C#, D, D#, E, F, F#, \nG, G#, A, A#, B in music, or a generalized version of 24-dimensional representation simply for higher \nresolution. Specifically, for the 24-dimensional \nrepresentation, let \n],[nK XSTFT denote the magnitude \nspectrogram of signal ][nx. The chromagram of ][nx is \n \n∑\n==\n' )(:],[ ],'[\nK KPKSTFT PHP nK X nK X (1) \n \nThe mapping between frequency index K in STFT \nand frequency index K’ in PCP is \n \n24 mod)]/ /(log24[)(1 2 ff NFFTK KPs⋅ ⋅=     (2) \n \nwhere NFFT is the FFT length, sf is the sampling rate, \n1f is the reference frequency corresponding to a note in \nthe standard tuning system, for example, MIDI note C3 \n(32.7031956626Hz). In the following, we will use the \n24-dimensional PCP representation for better resolution.  \nIn the following, we will focus on the chromagram \nrepresentation for key analysis of classical piano music, simply because of its advantage of direct mapping to the \nmusical meaning. It doesn’t mean it is best for any types \nof applications or any musical genres. However, all the \nfollowing approaches should be generalized fairly easily using other representations. \n3.2 Parameters and Configuration of HMM \nIn the following, the task of key detection will be \ndivided into two steps: \n \n1. Detect the key without considering its mode. For \nexample, both C major and A minor will be denoted as key 1, C# major and A# minor will be denoted as \nkey 2, and so on. Thus, there could be 12 different keys in this step. \n 2. Detect the mode (major or minor).  \nThe task is divided in th is way, because diatonic \nscales are assumed and relative modes share the same diatonic scale. Thus, step 1 attempts to determine the \nheight of the diatonic scale. And again, both steps \ninvolve segmentation based on key (mode) change as \nwell as identification of keys (modes). \nThe model used for key change detection should be \nable to capture the dynami c of sequences, and to \nincorporate prior musical knowledge easily since large \nvolume of training data is normally unavailable. Thus, we propose to use Hidden Markov Models for this task, \nbecause HMM is a generative model for labelling \nstructured sequence and sa tisfies both of the above \nproperties [6]. \n  \nS1 S2 St ST-1 STO1 O2 Ot OT-1 OT\n)(1Sπ)(t SOb\nt\n \n \nFigure 2.  Demonstration of Hidden Markov Models. \n \nFigure 2 shows a graph of HMM used for key change \ndetection. The hidden states correspond to different keys \n(or modes). The observations correspond to each frame \nrepresented as 24-dimensiona l chromagram vectors. The \ntask will be decoding the underlying sequence of hidden \nstates (keys or modes) from the observation sequence \nusing Viterbi approach. \nThe parameters of HMM need to be configured \ninclude:  The number of states N corresponding to the \nnumber of different keys (=12) or the number of \ndifferent modes (=2), respectively, in the two steps. \n  The state transition probability distribution \n}{ija=A  corresponding to the probability of \nchanging from key (mode) i to key (mode) j. Thus, \nA is a 1212× matrix (in step 1) and a 22× matrix \n(in step 2), respectively. \n \n The initial state distribution }{iπ=Π  corresponding \nto the probability at whic h a piece of music starts \nfrom key (mode) i. \n \n The observation probability distribution )}({vbj=B  \ncorresponding to the probability density at which a \nchromagram v is generated by key (mode) j. \n \n469   \n \n Due to the small amount of labeled audio data and \nthe clear musical meanings of the parameters, Π and A \nwere empirically set as follows: \n \n1⋅=Π121 \n \nwhere 1 is a 12-dimensional vector in step 1 and a 2-\ndimensional vector in step 2. This configuration denotes \nequal probabilities of starti ng from different keys \n(modes). \n \nddstayprob b bb b bb stayprob bb b stayprob\n×⎥⎥⎥⎥\n⎦⎤\n⎢⎢⎢⎢\n⎣⎡\n=\n............\nA \n \nwhere d is 12 in step 1 and is 2 in step2. stayprob  is the \nprobability of staying in the same state and \n1 )1(=⋅−+ b d stayprob . For step 1, this configuration \ndenotes equal probabilities of changing from a key to a \ndifferent key. It can be easily shown that when stayprob  \ngets smaller, the state sequence gets less stable (changes \nmore often). In our experiment, stayprob  will be varying \nwithin a range (e.g., [0.9900 0.9995]) in step 1 and be \nset to 20101−−  in step 2 to see how it impacts the \nperformance. \nFor observation probability distribution, instead of \nGaussian probabilistic models, commonly used for \nmodeling observations of continuous random vectors in \nHMM, the cosine distances between the observation (the 24-dimensional chromagram vector) and the pre-defined \ntemplate vectors were used to represent how likely the \nobservation was emitted by the corresponding keys or modes, i.e., \n \n||||.||||.)(\njj\njvvvbθθ=  (3) \n \nwhere jθ is the template of state j (corresponding to the \njth key or mode). Note that, strictly speaking, the model \nusing cosine distances is not a probability density, \nbecause it does not integrate to 1; however, since we \nonly care about the relative likelihood of being at \ndifferent keys, it is still a reasonable model.  \nThe advantage of using cosine distance instead of \nGaussian distribution is that the key (or mode) is more correlated with the relativ e amplitudes of different \nfrequency components rather than the absolute values of \nthe amplitudes. Figure 3 shows an example for \ndemonstrating this. Suppose points A, B and C to be three chromagram vectors. Based on musical \nknowledge, B and C are more likely to be generated by \nthe same key (or, mode) than A and C, because B and C have more similar energy profile. However, if we look \nat the Euclidean space, A and C are closer to each other \nthan B and C; thus, if we use Gaussian distribution to model the observation probability distribution, A and C will be more likely to be generated by the same key, which is not true. \n \nO11\nC\noooAB\no\noo\n \nFigure 3.  Comparison of observation \ndistributions of Gaussian and cosine distance. \n \nFor step 1, the template of a key was empirically set \ncorresponding to the diatonic  scale of that key. For \nexample, the template for key 1 (C major or A minor) is \nT odd]101010110101[1=θ  (Figure 4), 0=even\n1θ , where \nodd\n1θ  denotes sub-vector of 1θ with odd indexes (i.e., \n)23:2:1(1θ ) and even\n1θ  denotes sub-vector of 1θ with \neven indexes (i.e., )24:2:2(1θ ). This means we ignore \nthe elements with even in dexes when calculating the \ncosine distance. The templates of other keys were set \nsimply by rotating 1θ accordingly: \n \n))1(2,(1−⋅= j rjθθ   (4) \n \n]24 mod) [( ][..),,( ik itskr += = αβαβ  \n \nwhere j=1, 2, …, 12 and i, k=1, 2, …, 24. Let us also \ndefine 24 24 mod24 =. \n \n \n \nFigure 4.  Configuration of the template for C \nmajor (or A minor). \nFor step 2, the templates of modes were empirically \nset as follows:  \n \nT odd\nmajor ]000010000000[=θ ,  \n \nT odd\nminor ]001000000000[=θ ,  \n \n470   \n \n 0= =even\nminoreven\nmajorθ θ ,  \n \nThis setting comes from musical knowledge that \ntypically in a major piece, the dominant (G in C major) \nappears more often than the submediant (A in C major), \nwhile in a minor piece, the to nic (A in A minor) appears \nmore often than the subtonic (G in A minor). Please \nnote the templates need to be rotated accordingly \n(Equation 4) based on its key detected from step 1. \nApparently, the above is a simplified model and there \ncan be several refinements of it. For example, if we consider the prior knowledge of modulation, we can \nencode in \nAthe information that each key tends to \nchange to its “close” keys rather than the other keys. \nThe initial key or mode of a piece may not be uniformly \ndistributed as well. But to quantize the numbers, we will \nneed a very large corpus of pre-labeled musical data, which is not available here. \n4 EXPERIMENTAL EVALUATION \n4.1 Data Set \nTen classical piano pieces (Tab le 1) were used in the \nexperiment of key detection, since the chromagram \nrepresentation of piano music has very clear mapping \nbetween its structure and its musical meaning (Section 3.1). These pieces were chosen  randomly as long as they \nhave fairly clear tonal structure (relatively tonal instead \nof atonal). The truth was manually labeled by the author \nbased on the score notation to be compared with the \ncomputed results.  \nThe data were mixed into 8-bit mono and down-\nsampled to 11kHz. Each piece was segmented into \nframes of 1024 samples with 512 samples overlap. \n \nTable 1. Ten classical pian o pieces in the experiment. \n1.  Mozart: Piano Sonata No. 15 In C (I. Allegro)  \n2.  Schubert: Moment Musical No. 2  \n3.  Dvorak: Humoresque No. 7  \n4.  Rubenstein: Melody In F  \n5.  Paderewski: Menuett  \n6.  Chopin: ‘Military’ Polonaise  \n7.  Beethoven: Minuet In G  \n8.  Mozart: Sonata No. 11 In A ‘Rondo All Turca’  \n9.  Schumann: From Kinderszenen (1. Von Fremden    \nLandern Und Menschen)  \n10. Chopin: Waltz In D-flat, Op. 64 No. 1 ‘Minute Waltz’  \n4.2 Evaluation Measures \nTo evaluate the results, two aspects need to be \nconsidered: label accuracy (h ow the computed label of \neach frame is consistent with the actual label) and \nsegmentation accuracy (how th e detected locations of \ntransitions are consistent with the actual locations). \nLabel accuracy is defined as the proportion of frames \nthat are labeled correctly, i.e.,  \nframes totalcorrectly labeled framesaccuracy Label##=  (5) \n \nTwo metrics were proposed and used for evaluating \nsegmentation accuracy. Precision  is defined as the \nproportion of detected tran sitions that are relevant. \nRecall  is defined as the proportion of relevant transitions \ndetected. \nThus, if B={relevant transitions}, C={detected \ntransitions} and CBA∩= , from the above definition,  \n \nCAPrecision=  (6)  \n \nBARecall=  (7) \n \n \n \n \nFigure 5 . An example for measuring \nsegmentation performance (above: detected \ntransitions; below: relevant transitions).  \n \nTo compute precision and recall, we need a \nparameter w: whenever a detected transition t 1 is close \nenough to a relevant transition t 2 such that | t1-t2|<w, the \ntransitions are deemed identical (a hit). Obviously, \ngreater w will result in higher precision and recall. In the \nexample shown in Figure 5,  the width of each shaded \narea corresponds to 2 w-1. If a detected transition falls \ninto a shaded area, there is a hit. Thus, the precision in \nthis example is 3/6=0.5; the recall is 3/4=0.75. Given w, \nhigher precision and recall in dicates better performance. \nIn my experiment (512 window step at 11kHz sampling rate), w will vary within a range to see how precision \nand recall vary accordingly: for key detection, w varies \nfrom 10 frames (~0.46s) to 80 frames (~3.72s). The range of w for key detection is fairly large because \nmodulation of music (change from one key to another \nkey) is very often a smooth process that may take \nseveral bars.  \nAssume we randomly segment a piece into (k+1)  \nparts, i.e., k random detected transitions. Let n be the \nlength of the whole piece (n umber of frames) and let m \nbe the number of frames “close enough” to each \nrelevant transition, i.e., m=2w-1. Also assume there are l \nactual segmenting points. To compute average precision and recall of random segmentation, the problem can be categorized as a hyper-geom etric distribution: if we \nchoose k balls from a box of ml black balls (i.e., m black \nballs corresponding to each segmenting point) and ( n-\nml) white balls, assuming no overlap occurs, what is the \ndistribution of the number of black balls we get. Thus, \n \n471   \n \n nml\nnmlk\nk ks chosen black ball=⋅= =1] E[#Precision (8) \n \n)11)...(11)( 1(11)0 ( 1)0 ( ] [#Recall\n0 0\n+−−−−−−=−==−=>⋅= =\n−\n−\nmnk\nnk\nnkCCCBPlBPl\nlpoints segmenting detectedE\nk\nnk\nmn m\n     ( 9 )  \n \nwhere B denotes the number of black balls chosen \ncorresponding to a particular segmenting point. If we know the value of l in advance and make k=l (thus, not \ncompletely random), and n>>m ,  \n \nm\nnl) 1(1 Recall −−≈   (10) \n \nThe equations shown that, given n and l, precision \nincreases by increasing w (i.e., increasing m); and recall \nincreases by increasing k or w .  Equation 8 and 10 will \nbe used later as the baseline (upper bound of the \nperformance of random segmentation) to be compared \nto the performance of the segmentation algorithm. \n4.3 Results \nFigure 6 shows key detection result of Mozart’s piano \nsonata No. 11 with stayprob =0.996 for step 1 and \nstayprob2 =1-.1-20 in step 2. The figure above presents \nthe result of key detection without considering mode (step 1) and the figure below presents the result of mode \ndetection (step 2). \nTo show label accuracy, recall and precision of key \ndetection averaged over all the pieces, we can either fix w and change stayprob (Figure 7), or fix stayprob  and \nchange w (Figure 8). In Figure 7, two groups of results are shown in the \nplot: one corresponds to the performance of step 1 \nwithout considering modes; the other corresponds to the \noverall performance of key detection with mode into \nconsideration. It clear ly shows that when stayprob  is \nincreasing, precision is also  increasing while recall and \nlabel accuracy are decreasing. \n \n0 500 1000 1500 2000 2500036912Ex tract from CD 31 - Track 19.wavkey (1-12)\n0 500 1000 1500 2000 250001\ntime (frame#)mode ( m= 0/M=1)\n \nFigure 6.  Key detection of  “Mozart: Sonata No. \n11 In A ‘Rondo All Turca’” (solid line: computed \nkey; dotted line: truth) \n  In Figure 8, three groups of results are shown in the \nplot: one corresponds to the performance of step 1 \nwithout considering modes; one corresponds to the \noverall performance of key detection with mode into \nconsideration; and one co rresponds to recall and \nprecision based on random segmentation (Equation 8 \nand 10). Additionally, label accuracy based on random \nshould be around 8%, without considering modes.  \nIt clearly shows that when w is increasing, recall and \nprecision are also increasi ng. Please note that label \naccuracy is irrelevant to w.  \nThe above two figures show that the segmentation \nFigure 8 . Performance of key detection with \nvarying w (stayprob =0.996; stayprob2 =1-.1-20). 10 20 30 40 50 60 70 8000.10.20.30.40.50.60.70.80.91stayprob=0.996; stayprob2=1-. 1-20\nwRecall\nPrecision\nLa bel a ccu racy\nRecall M/ m\nPrecision M /m\nLa bel a ccu racy M/m\nRecall (random )\nPrecision (random)\nFigure 7.  Performance of key detection with \nvarying stayprob (w=10; stayprob2 =1-.1-20). 0.99 0.992 0.994 0.996 0.998 10.50.60.70.80.9\nstayprobwidth threshold=10 frames; stayprob2=1-.1-20\nrecall\nprecision\nlabel acc uracy\nrecall M /m\nprecision M /m\nlabel acc uracy M/m\n472   \n \n performance (recall and precisi on) base on the algorithm \nis significantly better than random segmentation. \n5 DISCUSSION \nIdeally, all the HMM parameters should be learned from \na labeled musical corpus. The training can be made \n(efficiently) using a maximum likelihood (ML) estimate since all the nodes are obser ved. Especially, if the \ntraining set has the similar timbre property as the test set, the observation distribution can be more accurately \nestimated employing the timbre information besides \nprior musical knowledge, an d the overall performance \nshould be further improved.  \nHowever, this training data set should be very huge. \nManually labelling it will involve tremendous amount of work. For example, if the training data set is not big \nenough, the state transition ma trix will be very sparse \n(0’s at many cells) and this may result in many test \nerrors, because any transition that does not appear in the \ntraining set will not be recognized. One possibility for \nfuture improvement is using Bayesian approach to combine the prior know ledge (via empirical \nconfigurations) and the information obtained from a small amount of training data.  \n \ncomputed key (1-12)original key (1-12)confusion matrix of key detection\n2 4 6 8 10 122\n4\n6\n8\n10\n12\n \nFigure 9.  Confusion matrix of key detection. \n \nAnother interesting thing to investigate is how the \nalgorithm was confused with keys and whether the \nerrors make a musical sense. Figure 9 shows the \nconfusion matrix of key detection (without considering \nmodes; stayprob=0.996; stayprob2=1-.1-20). It shows \nthat most errors came fr om confusion between the \noriginal key and the dominant or sub-dominant key (e.g., F Æ C, G Æ C, F# Æ C#). This is consistent with \nmusic theory presented in Se ction 2 that these keys are \ncloser to each other and share more common notes.   \n6 CONCLUSION AND FUTURE WORK \nThis paper presented an HMM-based approach for \ndetecting key change. Experimental result, evaluated by three proposed measures, demonstrates the promise of \nthe method. Although constraints on music have been made to build simplified models, e.g., diatonic scales, \nthe framework should be easily generalized to handle \nother types of music.  \nEach step in the presented framework has been \ncarefully designed with consideration of its musical meaning: from using chromagram representation, to employing cosine-distance observation probability \ndistribution, to empirical  configurations of HMM \nparameters. The experimental result is fairly robust and significantly better than random segmentation. \nFuture improvement could be adding a training stage \n(if training data is available) to make this general model customized to specific types of music. More \nrepresentations need to be explored for other music genres. Furthermore, the HMM parameters should be \nchosen most appropriate for different applications: for \nsegmentation-based applications, we should maximize \nprecision and recall; for key relevant applications (such \nas detecting repeated patterns that was presented is \nSection 1), we should maximize label accuracy. \nSimilar framework has also been applied to chord \ndetection task for classical piano music, which will not be covered in this paper. \n \nREFERENCES \n[1] Foote, J. and Cooper , M. “Visualizing Musical \nStructure and Rhythm via Self-Similarity,” \nProceedings of International Conference on \nComputer Music, Habana, Cuba, September 2001. \n[2] Chuan, Ching-Hua and Chew, Elaine. “Polyphonic \nAudio Key-Finding Using the Spiral Array CEG Algorithm,” Proceedings of International Conference \non Multimedia and Expo, Amsterdam, Netherlands, \nJuly 6-8, 2005. \n[3] Gomez, E. and Herrera, P. “Estimating The Tonality \nOf Polyphonic Audio Files: Cognitive Versus Machine Learning Modelling Strategies,” \nProceedings of Internatio nal Conference on Music \nInformation Retrieval, 2004. \n[4] Pauws, S. “Musical key extraction from audio,” \nProceedings of Internatio nal Conference on Music \nInformation Retrieval, 2004. \n[5] Sheh, A. and Ellis, D. “Chord Segmentation and \nRecognition using EM-Trained Hidden Markov \nModels,” Proceedings of International Symposium \non Music Information Retrieval ISMIR-03, \nBaltimore, October 2003. \n[6] Rabiner, L. R. “A Tutorial on Hidden Markov \nModels and Selected Applications in Speech Recognition,” Proceedings of  the IEEE, Volume: 77 \nIssue: 2, Feb. 1989, Page(s): 257 –286. \n \n \n473"
    },
    {
        "title": "Segmentation and Recognition of Tabla Strokes.",
        "author": [
            "Parag Chordia"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416000",
        "url": "https://doi.org/10.5281/zenodo.1416000",
        "ee": "https://zenodo.org/records/1416000/files/Chordia05.pdf",
        "abstract": "A system that segments and labels tabla strokes from real performances is described. Performance is evaluated on a large database taken from three performers under different recording conditions, containing a total of 16,834 strokes. The current work extends previous work by Gillet and Richard (2003) on categorizing tabla strokes, by using a larger, more diverse database that includes their data as a benchmark, and by testing neural networks and treebased classification methods. First, the time-domain signal was segmented using complex-domain thresholding that looked for sudden changes in amplitude and phase discontinuities. At the optimal point on the ROC curve, false positives were less than 1% and false negatives were less than 2%. Then, classification was performed using a multivariate Gaussian model (mv gauss) as well as non-parametric techniques such as probabilistic neural networks (pnn), feed-forward neural networks (ffnn), and tree-based classifiers. Two evaluation protocols were used. The first used 10-fold cross validation. The recognition rate averaged over several experiments that contained 10-15 classes was 92% for the mv gauss, 94% for the ffnn and pnn, and 84% for the tree based classifier. To test generalization, a more difficult independent evaluation was undertaken in which no test strokes came from the same recording as the training strokes. The average recognition rate over a wide variety of test conditions was 76% for the mv gauss, 83% for the ffnn, 76% for the pnn, and 66% for the tree classifier. Keywords: instrument recognition, tabla, automatic transcription, timbre. 1",
        "zenodo_id": 1416000,
        "dblp_key": "conf/ismir/Chordia05",
        "keywords": [
            "tabla",
            "performance",
            "recording",
            "database",
            "classification",
            "neural",
            "networks",
            "tree-based",
            "classification",
            "evaluation"
        ],
        "content": "SEGMENT ATION AND RECOGNITION OFTABLA STR OKES\nParag Chordia\nCCRMA, Stanford University\n660Lomita Dr.\nStanford CA94305\npchordia@ccrma.stanford.edu\nABSTRA CT\nAsystem thatsegments andlabels tabla strok esfrom real\nperformances isdescribed. Performance isevaluated on\nalargedatabase takenfrom three performers under dif-\nferent recording conditions, containing atotal of16,834\nstrok es.Thecurrent workextends previous workbyGillet\nandRichard (2003) oncategorizing tabla strok es,byus-\ningalarger,more diversedatabase thatincludes their data\nasabenchmark, andbytesting neural netw orks andtree-\nbased classi\u0002cation methods. First, thetime-domain sig-\nnalwassegmented using comple x-domain thresholding\nthat lookedforsudden changes inamplitude andphase\ndiscontinuities. Attheoptimal point ontheROCcurve,\nfalsepositi veswere lessthan 1%andfalsenegativeswere\nless than 2%. Then, classi\u0002cation wasperformed us-\ningamulti variate Gaussian model (mv gauss) aswell\nasnon-parametric techniques such asprobabilistic neu-\nralnetw orks (pnn), feed-forw ardneural netw orks (ffnn),\nandtree-based classi\u0002ers. Twoevaluation protocols were\nused. The\u0002rstused 10-fold cross validation. Therecogni-\ntionrateaveraged overseveralexperiments thatcontained\n10-15 classes was92% forthemvgauss, 94% fortheffnn\nandpnn, and84% forthetreebased classi\u0002er .Totestgen-\neralization, amore dif\u0002cult independent evaluation was\nundertak eninwhich noteststrok escame from thesame\nrecording asthetraining strok es.Theaverage recognition\nrateoverawide variety oftestconditions was76% forthe\nmvgauss, 83% fortheffnn,76% forthepnn, and66% for\nthetreeclassi\u0002er .\nKeywords: instrument recognition, tabla, automatic\ntranscription, timbre.\n1INTR ODUCTION\nLabeling tabla strok esisatimbre recognition problem\nthat isverysimilar toinstrument recognition. There is\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonagrowing body ofworkdealing with recognizing isolated\npercussion timbres (Herrera etal.,2003, 2002; Sandv old\netal.,2004; Sillanpaa etal.,2000; Tindale etal.,2004).\nThese studies haveseveralmotivations. Isolated percus-\nsion timbre recognition isuseful forautomatically search-\ningorcategorizing largecollections ofsounds, which can\nbeused formultimedia search ororganizing sound li-\nbraries insample collections. Itcanalso beused tolabel\nmonophonic audio. Beyond that, itishoped thattheiso-\nlated tone recognition problem willgiveinsights intoiden-\ntifying components ofsound mixtures. There aremany\napplications, such assearching and editing audio \u0002les,\nwhere wewould liketoidentify andlabel percussion sec-\ntions inpolyphonic music.\nTabla strok etranscription isoneofthefewreal-w orld\ntranscription tasks where monophonic timbre recognition\nisgenuinely useful. Although there aresimultaneous\nstrok es,these areconceptualized assingle entities. Some-\ntimes, notes will overlap because ofringing, buttabla is\nessentially asingle stream ofconstantly changing timbres.\nInmost music, wearefaced either with asolo passage\nfrom oneinstrument, oramixture ofinstruments.\n2INTR ODUCTION TOTABLA\nTabla iscomprised ofapairofdrums, atreble drum, re-\nferred toasthetabla ordayan ,andthebass drum called\nthebayan .Thebayan ismade ofcopper andsometimes\nterracotta. The right-hand dayan isatapering cylinder\ncarvedfrom ablock ofdense wood. Each ofthedrums\niscovered with goatskin. Aunique feature ofthetabla is\nthetuning paste called thesyahi ,which isapplied tothe\ncenter ofthedayan ,andoff-center onthebayan .When\namembrane stretched overaresonating body isstruck,\nthere isgenerally noclear sense ofpitch because thesound\nproduced isrichininharmonic overtones. When properly\napplied, thesyahi causes thealignment ofsome ofthein-\nharmonic partials, giving thedayan aclear sense ofpitch\nifstruck correctly (Fletcher andRossing, 1998).\nThe twodrums ofthetabla produce manydifferent\ntimbres. Manyofthese sounds havebeen named, forming\navocabulary oftimbres. Thenaming ofstrok eshasfacili-\ntated thedevelopment andtransmission ofasophisticated\nsolo repertoire. Inaddition totherhythmic comple xity\noftabla music, itisitstimbral beauty anddiversity that\ndistinguish itfrom other percussion instruments. Tabla\n107sounds canberoughly classi\u0002ed intothefollowing three\nmajor sound groups:\n1.Ringing bell-lik etones played onthetreble drum.\nThese tones aredistinguished byaclear sense of\npitch, sharp attack, andlong sustain. Ta,tinandtun\nareexamples.\n2.Resonant bass strok esplayed onthebass drum. This\nisthestrok eghe.Unlik eother sounds, gheistheonly\nsound thatiscontinuously varied. The tabla player\nmodulates thepitch bycontrolling thetension onthe\nskin ofthebass drum using thebase ofhispalm.\n3.Closed, crisp sounds. These sounds havesharp at-\ntacks anddecays andsound damped. Katplayed on\nthebayan ,andte,tak,dhe,andre,which areplayed\nonthedayan ,areexamples ofthisfamily .\nInaddition tothese, strok escanbeplayed simultaneously\nforming acompound strok e.Typically ,astrok efrom the\ndayan iscombined with abass tone (ge)oraclosed tone\n(ke)onthebayan .Themost common compound strok es\naredha(na+ghe),dhin (tin+ghe),anddun(tun+ghe).\nThese areconceptualized asdistinct strok esandnotas\nmixtures.\nIntotal there areapproximately twenty different tim-\nbres. Isolated examples ofeach sound can befound\nathttp://ccrma.stanford.edu/ pchordia/ismir05/. Inmost\ncompositions, asmaller subset ofthetotal number of\navailable timbres isused. This issimilar totonal music\ninwhich certain notes areemphasized tocreate atonal\nframe work. The mapping between timbres andsymbols\nisnotone-to-one: onename canrefer toseveraltimbres\ndepending ontheconte xt,andasingle timbre canbere-\nferred tobymore than onename. Inthiswork, each dis-\ntinct timbre isgivenaunique identifying label.\nIndividual strok esarecombined into largerphrases,\nsuch asterekite ,denegene,kitetak e,gegenage.These\ncombinations areakin towords inlanguage; theycom-\nbine hierarchically tomakelargerphrases. Because of\nthese building blocks, certain strok estend tofollowother\nstrok es.This sequential structure canbemodeled asa\nMark ovprocess.\n3RELA TED WORK\nTheclosest workisthatofGillet andRichard (2003) who\ndescribe atabla transcription system. Segmentation was\ndone bysimple amplitude thresholding. Theyreported\n98.3% segmentation accurac y,with themajority oferrors\ncoming from falsenegatives.Intheir system each strok e\nisparametrized bythemean, variance andrelati veweight\noffour frequenc ypeaks. This wasdone bymodeling\nthemagnitude spectrum using aGaussian mixture model\n(gmm). Thegmm wasused notasaprobability distrib u-\ntion, butrather asasimple waytoparametrize theshape\nofthefrequenc yspectrum: theposition ofthepeaks, their\nbandwidth, andtheir relati veweight. Inthisway,each\nstrok ewasrepresented bya12-dimensional feature vec-\ntor.\nClassi\u0002cation wasdone using naveBayes, 5-nearest\nneighbor (5-nn), orbykernel density estimation. Inthe\u0002rst, thefeature vector wasmodeled asbeing drawnfrom\namulti variate gaussian distrib ution. Ina10-fold cross-\nvalidation experiment thebestreported accurac ywas83%\nusing 5-nn ina10category problem. Byperforming lan-\nguage modeling, using ahidden Mark ovmodel (hmm),\ntheywere abletoattain a93.6% recognition rate. This was\ndone bytreating thestrok elabels ashidden states, andthe\nfeature vectors asemissions from those states. Transitions\nbetween strok eswere estimated directly from thetrain-\ningdata. Forthesequence offeature vectors theViterbi\npath wascalculated giving thesequence ofstrok es.Ina\nmore dif\u0002cult generalization test, inwhich training and\ntestsamples arefrom different conditions, performance\ndropped to79.8% without language modeling, and90.2%\nwith language modeling.\nEarly work inpercussion transcription wasdone\nbySchloss (1985) who described anamplitude-based\nthresholding algorithm foronset detection. Amethod to\nclassify severaldifferent cong astrok esbased ontherel-\nativeenergyindifferent frequenc ybands wasdescribed.\nRecognition rates were notreported.\nSillanpaa etal.(2000) attempted toidentify ran-\ndom mixtures ofupto3sounds from \u0002vedifferent drum\nclasses. Atotal of129drum sounds were used. Anit-\nerativetemplate matching scheme wasused inwhich the\nnearest neighbor wasfound andsubtracted. The proce-\ndure wasrepeated until theenergyfellbelowsome thresh-\nold.Forisolated notes, thecorrect label wasgiven87% of\nthetime, with extradrums (falsepositi ves)occurring 32%\nofthetime. Intwodrum mixtures, both drums were found\n49% ofthetime, andatleast onedrum 100% ofthetime.\nFormixtures ofthree, therecognition rates were 8%for\nallthree, 60% fortwo,and100% foratleast one.\nRecent workinpercussion transcription hasbeen done\nbyHerrera etal.(2003, 2002). Inthe\u0002rstwork, isolated\npercussion notes from 9different instruments were classi-\n\u0002ed. Awide variety ofspectral andtemporal features were\nused, including spectral centroid, mel-frequenc ycepstral\ncoef\u0002cients (mfccs), log-attack time and zero-crossing\nrate(zcr). Recognition of90% using ak-nearest neighbor\n(k-nn) classi\u0002er wasreported, howeverthedatabase only\nincluded 634examples from commercial sample CDs. In\nthesecond study ,thenumber oftimbres wasincreased to\n33andthetotal number ofsamples to1976. Arecogni-\ntionrateof85% wasattained using ak-nn algorithm ina\n10-fold cross-v alidation test.\nTindale etal.(2004) described asystem that clas-\nsi\u0002ed sounds produced bystriking different areas ofa\nsnare drum, intotal producing sevendistinct timbres. Us-\ningadatabase of1260 strok esrecorded, under veryuni-\nform recording conditions, arecognition rateof95% was\nachie vedwith afeed-forw ardneural netw ork. Testing was\ndone using aleave-one-out protocol. Thehigh recognition\naccurac ywaslikelydurtothesimilarity ofmanyofthe\nstrok esinthedatabase.\nAmore dif\u0002cult percussion recognition task wasun-\ndertak enbySandv oldetal.(2004), who collected 1136\ntraining samples from 25different recordings and1419\ntestsamples from 17different recordings. Thechallenge\ncame from thediversity ofthedatabase, andthefactthat\nrealperformance data were used. Sandv oldshowed that\n108performance could bedramatically impro ved(20-30%) by\nmanually labeling asmall subset ofthetones inthetest\nset.\nvanSteelant etal.(2004) dealt with mixtures ofeither\nthesnare drum orthebass drum with three other percus-\nsivesounds, leading tosixpossible timbres. Thesystem\nattempted todetect thepresence orabsence ofthebass and\nsnare drum ineach ofthesixpossible mixtures. Using\nsupport vector machines theyreported accuracies of94%\nand96% forthebass andsnare drum respecti vely.When\nevaluating these results, itisimportant tonote thatonly six\ntimbres areused, andthatonly adecision about thepres-\nence orabsence ofthebass andsnare drums istaken.Nev-\nertheless, forpopular music, identifying bass andsnare\ndrum patterns canrevealmuch about thestructure ofthe\npiece. Features such astherelati veenergyindifferent fre-\nquenc ybands, mfccs, temporal centroid, andmeasures of\ntheshape ofthefrequenc yspectrum were used.\n4TABLA STR OKE DATABASE\nOriginal recordings were made oftwohighly skilled tabla\nplayers1using different drums, with agood quality con-\ndenser microphone onaDATrecorder .The recordings\nwere made inasmall room, about 13X13, which was\nmildly reverberant. Each player played several qaidas2\natslowandfasttempos. This wasaqualitati veterm that\nplayers were free tointerpret, although constrained by\ntypical performance practice. This wasdone toensure that\nthesystem wasevaluated onthefullrange ofdurations\nthatoccur inrealperformances. Qaida wasused because\nofitscentral roleintabla repertoire, because itcanbeex-\ntensivelyelaborated, andbecause itissimpler toannotate\nthan fully impro vised sections. Most qaidas emphasize a\nsubset ofallpossible strok es,soanattempt wasmade to\nchose qaidas thattakentogether included awide variety\nofstrok es.Each recording wasthen manually segmented\nTable 1:Strok eDatabase\nSour ce id##strokes #targets\nGillet 1 1678 10\n2 1821 10\n3 2216 10\nTanmo y 4 2475 16\n5 2612 16\nRavi 6 3035 9\n7 2997 9\nTotal 16834\nandannotated. Annotation wasverytime consuming and\nlimited thetotal amount ofdata thatcould begathered.\nTheannotation alsomade clear some ofthedif\u0002culties\nofevaluating theautomatic transcription systems. Even\nwithin awell-established musical tradition naming con-\nventions differ.Aswenoted, different names aregivento\n1Thetabla players were Tanmo yBose andRaviGutala. Tan-\nmoyBose isaninternationally knownartist, who performs with\nPandit RaviShankar .RaviGutala isasenior disciple ofPandit\nSwapan Chaudhari.\n2Atheme andvariation formthesame timbre andthesame name canbegiventodif-\nferent timbres. Unless onehasmanually auditioned each\nstrok ethiscanintroduce structural errors thatcannot be\nimpro vedbyimpro ving theclassi\u0002cation system. Also,\nevenforexperienced musicians, itcansometimes bedif\u0002-\nculttoassign thelabel from thesound alone (i.e. without\nanyvisual reference). Perhaps themost subtle andsubjec-\ntiveissue ishowtodeal with playing errors. When should\nastrok ebeconsidered anerror ,andwhen should itbe\nconsidered aplausible variation ofaparticular strok e?If\nwediscard toomanymishit strok eswelose theability to\nlearn these variations. Theintuiti veanswer istoconsider\nastrok eamishit, rather than avariation, ifitisunlik elyto\noccur inthefuture. Ifitisacommon error ,then itshould\nbeincluded. This sortofjudgment requires extensi vedo-\nmain knowledge bytheannotator .\nInaddition tothese data sets, adata setthatwasused\npreviously inGillet andRichard (2003) wasobtained from\ntheauthors, hereafter referred toastheGillet data set.The\ngoal wastousethedata tobenchmark thecurrent system.\nTheGillet data setiscomprised ofthree data setsrecorded\nunder different conditions. The \u0002rst data setuses alow\nquality tabla under low\u0002delity recording conditions, the\nsecond setuses ahigher quality tabla andbetter micro-\nphone, while thelastsetislessclosely miked,leading to\nsome reverbintherecording.\nIngeneral, theGillet dataarerecorded inlower\u0002delity\nthan theprevious data sets, andtheplayer isclearly an\namateur .While low\u0002delity ispotentially aninteresting\nchallenge fortheclassi\u0002cation algorithm, thetabla play-\nerslevelproduces some obvious mistak esthatdegrade the\nquality ofthedata set. Twoproblems emer ged. First,\ntheplayed strok edoes notalwayssound liketheintended\nstrok e,whereas thedatabase annotation records thein-\ntended strok elabel. Second, theconsistenc yofsound pro-\nduction foranamateur player issigni\u0002cantly lessthan a\nprofessional, leading togreater acoustic variation within\nthesame strok eclass. However,since these problems\nwere also faced byGillet andRichard, itisstillvalidto\nusetheir data asabenchmark. Itis,however,animportant\nfactor inevaluating theabsolute error rate.\nInaddition tolabeled strok es,theGillet data con-\ntained onset location information and recorded theor-\nderofstrok es.Table 1summarizes thedata sets. Ex-\nample phrases from each ofthesources canbefound at\nhttp://ccrma.stanford.edu/ pchordia/ismir05/.\n5METHOD\nThesystem works inthefollowing three stages: 1)thesig-\nnalissegmented by\u0002nding onset points 2)thesegmented\nstrok esareparametrized bycomputing features thatsum-\nmarize thetimbre 3)aclassi\u0002er thathasbeen trained ona\nsubset ofthedata isused tolabel thestrok es.Each stepis\ndescribed indetail below.\n5.1 Segmentation\nSuccessful transcription depends onaccurate segmenta-\ntion. Because thesystem makesdecisions atthenote level,\nandnotattheframe level,itisimportant thateach strok eis\n109segmented atthecorrect point. Fortabla music, theseg-\nmentation only needs to\u0002nd onset points. Offsetpoints\naremuch lesssalient, andtheinclusion ofsome tailsi-\nlence willnotnegativelyimpact theclassi\u0002er .Falsepos-\nitives(detecting onsets that arenotthere) will result in\nthestrok ebeing divided inunexpected places, possibly\nleading totheomission ofacritical portion ofthestrok e.\nFalse negatives(failing todetect onset points) will lead\ntosegments thatcontain more than onestrok e.Forthese\nreasons, eveniftheclassi\u0002er isworking well onisolated\nnotes, inaccurate segmentation will lead toanincreased\nerror rate. Although notdiscussed here, accurate detection\nofonsets isalsoimportant forbeat andtempo detection.\nThesimplest andmost common approach toonset de-\ntection issome form ofamplitude thresholding. Thegist\nofthese algorithms istoformalize ourvisual intuition that\nonset points correspond toregions where theamplitude\nenvelope suddenly rises. Aversion ofthisapproach that\nwasused bySchloss (1985); Gillet andRichard (2003).\nAproblem thatisoften encountered inthese approaches\nisfalse positi vescaused byringing. Tocorrect this, the\nthreshold change value hastobesetcorrectly ,andarest\nparameter ,which disallo wsonsets thataretooclose to-\ngether ,hastobeused. Generally ,adjustment ofthese pa-\nrameters varies with therecording andhastobedone man-\nually .More recently ,thisapproach hasbeen re\u0002ned by\nbreaking thesignal intovarious frequenc ybands andthen\ncombining theamplitude thresholding information from\neach band totakeadecision. This isdescribed inKlapuri\n(1999), andafrequenc y-domain implementation ofthisis\nfound inGoto (2001).\nTheamplitude thresholding methods described above\nwere implemented butdidnotyield satisf actory results.\nAttheoptimal point ontheROCcurve,falsenegativesand\nfalsepositi veseach exceeded 3%. Duxb uryetal.(2003)\npropose asystem thatcombines amplitude andphase in-\nformation. Thealgorithm works byusing thesteady-state\nassumptions topredict themodulus andargument ofthe\ncomple xamplitude. Consider theFourier transform ofthe\nsignal: forthek-th bin,themodulus givestheamplitude\nofthesinusoid, andtheargument givesthephase. By\nde\u0002nition, inasteady-state region, thephase changes at\naconstant ratethatisgivenbyradian frequenc y.Soat\nthenexttime frame, weexpect theamplitude tobethe\nsame andthephase toequal thephase attheprevious time\nframe plus theradian frequenc ymultiplied bythetime\nstep. The expected change canbeestimated bytaking\nthephase difference between theprevious twoframes. So\n\u001ek(m)=\u001ek(m\u00001)+[\u001ek(m\u00001)\u0000\u001ek(m\u00002)]in\nsteady-state regions.\nThe deviations ineach binofthecomple xampli-\ntude from theexpected comple xamplitude attimemare\nsummed tocompute anewdetection function. The new\ndetection function islessnoisy ,more clearly showing on-\nsetpoints. This leads tomore accurate segmentation,\nwithout requiring signi\u0002cant parameter optimization. This\nsystem hadafalsenegativerateoflessthan 2%andafalse\npositi verateoflessthan 1%.5.2 Featur eExtraction\nMost current instrument recognition studies usealarge\nnumber (>100)oftemporal andspectral features Es-\nsidetal.(2004); Herrera etal.(2003, 2002); Livshin and\nRodet (2004); vanSteelant etal.(2004). Commonly used\nspectral features include spectral centroid, mfccs, mea-\nsures ofspectral shape such asskewness and kurtosis,\nspectral rollof fandothers. Inaddition tofeatures thatare\ncalculated overthewhole note, some features arecalcu-\nlated inframes sothat their evolution overtime canbe\nused. Because most classi\u0002cation techniques require fea-\nturevectors ofequal length, wecannot simply append the\nvalues ofthefeatures ineach frame toform alargefeature\nvector since notes varyinduration. Because ofthis, the\ntemporal evolution offeatures isoften summarized bythe\naverage amount ofchange andthestandard deviation of\nthatchange. Alternati vely,feature vectors from consecu-\ntiveframes canbemodeled using hidden Mark ovmodels,\nbutthismethod hasbeen used infrequently todate.\nInorder toavoidover\u0002tting, dimension reduction is\nusually performed onthefullfeature set.Welimited the\ninitial setoffeatures bycarefully examining themagni-\ntude spectra andamplitude envelopes oftabla strok esfrom\neach category.Themain temporal features were temporal\ncentroid (which givesthebalancing point oftheampli-\ntude envelope), attack time, andzero-crossing rate. The\nspectral features used were spectral centroid, skewness (of\nthemagnitude spectrum), kurtosis, andthirteen mfccs. In\ntotal, thirty-one initial features were calculated foreach\nstrok e.These were further reduced using principal com-\nponent analysis.\n5.3 Classi\u0002cation\nFourdifferent classi\u0002ers were trained asdescribed below.\nAfter being presented with thefeature vectors ofeach test\nsample, theclassi\u0002ers returned thecorresponding labels.\n5.3.1 Multivariate Gaussian (mvgauss)\nThe feature vector wasmodeled asbeing drawnfrom\namvgauss distrib ution. The labeled samples from the\ntraining data were used toestimate themean and co-\nvariance ofthelikelihood distrib ution foreach class, i.e.\nPr(datajclassi).The covariance matrix wascomputed\nonthepooled data, rather than being estimated separately\nforeach class. Theprior probability ofeach class wasde-\ntermined bycalculating therelati veproportion ofstrok es\ninthetraining database. Using Bayes rule, thelikelihood\nandprior probabilities were multiplied, anddivided bythe\nevidence, tocalculate theposterior probability foreach\nclass: Pr(classijdata).The label wasselected accord-\ningtoBayes rule; theclass thathadthelargest posterior\nprobability waschosen.\n5.3.2 Feed-forwar dneuralnetwork (ffnn)\nNeural netw orks, rather than explicitly estimating thelike-\nlihood andprior distrib utions, usetraining examples to\ncompute anon-linear mapping from thefeature space to\ncategories. The great advantage ofsuch architectures is\ntheir ability tonon-linearly combine features torepresent\n110comple xdecision boundaries. The decision boundaries\nneed notbeconvexorcontiguous.\nNeural netw orks arecomprised ofnodes andassoci-\nated weights. Inputs toeach node aremultiplied bya\nweight andfedtoanon-linear function thatemits avalue\nclose tooneiftheinput exceeds some threshold, andclose\ntozero otherwise. Inaffnn, theinput layers consists of\nasmanynodes asthere arefeatures. Foreverynode, each\nfeature ismultiplied byaweight andsummed before being\nfedtothenon-linear function. Each node intheinput layer\nisconnected toeach node inahidden layer ,where thepro-\ncess isrepeated. The output layer hasasmanynodes as\nthere areclasses. Intheideal case, when presented with a\nsample from agivenclass, thenetw orkoutputs avalue of\noneinthecorresponding output node, andzero elsewhere.\nTheffnnisessentially acollection ofweights andnon-\nlinear functions. Itturns outthat theparticular choice\nofnon-linearity isusually notcritical, leaving thecen-\ntralquestion ofhowtosettheweights. This isdone by\ntheback-propag ation algorithm. Weights areinitially as-\nsigned randomly .During training, asample ispresented\ntothenetw ork,andeach output node emits avalue. Since\nweareaiming foraoneatoneofthenodes, andzeros at\ntheother nodes, wecancalculate thedifference between\ntheoutputted values andthedesired values. The square\nofthisisourerror function. Theweights areadjusted to\nminimize theerror function. Inparticular ,weperform a\ngradient descent intheweight space. Details oftheback-\npropag ation algorithm canbefound inDuda etal.(2001).\nArchitecture plays acrucial roleinthenetw ork'sabil-\nitytogeneralize. Thenumber ofhidden layer nodes must\nbeselected sothattheffnnissuf\u0002ciently expressi vewith-\noutover\u0002tting thedata. Intheffnn, themain architec-\ntural decisions concern thenumber ofhidden layers and\nthenumber ofnodes ineach hidden layer .Inthisstudy ,\nffnns with 1and2hidden layers were tried. Both worked\nwell, the\u0002rst with 50hidden nodes andthesecond with\nbetween 10and20nodes ineach layer .\n5.3.3 Probabilistic neuralnetwork (pnn)\nApnn isaneural netw ork that isused tonon-\nparametrically learn aprobability density function. In\nclassi\u0002cation problems, training samples areused tolearn\ntheposterior distrib ution ofeach class. Thepnnisaneural\nnetw orkimplementation ofParzen windo wsdensity esti-\nmation (Duda etal.,2001).\nApnnconsists ofinput units, pattern units, andout-\nputunits. Forclassi\u0002cation problems, thenumber ofin-\nputunits isequal tothedimension ofthefeature vector .\nApattern unitconsists ofweights thatarethenormalized\nfeature values ofatraining sample. Pattern units arecon-\nnected toeach oftheinput units. Each output unit rep-\nresents acategory andapattern unit connects totheone\noutput unitofthesame category.Training consits ofset-\nting theweights ofeach pattern unit equal tothefeature\nvalues ofthecorresponding training sample.\nWhen afeature vector ispresented tothenetw ork,the\ninner product ofthefeature vector andtheweights ofa\ngivenpattern unit arecalculated: wtx.Ifthesample is\nsimilar tothesample represented bythepattern unit, this\nvalue will belarge.This isrepeated foreach ofthepat-tern units that emit anon-linear function ofthisvalue:\nf(wtx).Each category unitsums theoutputs ofthepat-\nternunits thatconnect toit.Thetestsample isgiventhe\nlabel ofthecategory unit thatemits thelargest value. It\ncanbeshownthatife(wtx\u00001)=r2isused asthefunction,\nwhere risaconstant thecontrols thespread, then thepat-\nternunitwillemit avalue thatisequal totheprobability\nthatthetestpoint wasgenerated from aGaussian centered\nonthetraining example (Duda etal.,2001).\n5.3.4 Treeclassi\u0002er\nAbinary treebased classi\u0002er wasconstructed. Treebased\nclassi\u0002ers workbyconsidering questions thatdivide the\ntraining data. Thetreeconsists ofnodes andbranches. At\neach node aquestion isposed thatdivides thedata. They\nareessentially formalizations ofaseries ofquestions, giv-\ningthem acertain intuiti veappeal. Attheend, wewould\nliketohavealltheexamples ataterminal node belong\ntothesame class. This iscalled apure node. Ateach\nnode thegoal istosplit thedata inawaythatmaximally\npuri\u0002es thedata. Acommon wayofdoing thisisbyus-\ninganentrop ycriterion. Apure node haszero entrop y,\nwhile anode thathasseveralclasses uniformly distrib uted\nwillhavenormalized entrop yvalue ofone. Splits arecho-\nsenthatmaximally reduce entrop y.Although purity can\ngenerally beincreased byfurther splitting, overlycomple x\ntrees willlead toover\u0002tting. Toavoidthis, traning isusu-\nallyhalted when theerror ratebegins toincrease onan\nindependent veri\u0002cation set.\n5.3.5 Hidden Mark ovmodel\nAnattempt wasalso made toimpro veperformance using\nlanguage modeling. Asnoted before, tabla strok eshave\nasequential structure. Wecanaskwhat theprobability of\nthenextstrok eisgiventheprevious strok e,ortheprevious\nseveralstrok es.These transition probabilities canbeesti-\nmated from training data inwhich thesequence ofstrok es\nisknown. Forexample, ifweareestimating thetransi-\ntionprobabilities from dha,wesimply look attherelati ve\nnumber oftimes itcontinues toother strok es.Based on\nthese transition probabilities, wecancalculate thelikeli-\nhood ofagivensequence ofstrok es.Most sequences will\nbehighly unlik ely.\nInourproblem, however,wecannot directly observ e\nthestrok elabels; weonly seeasequence offeature vec-\ntors. Thegoal istodecide what sequence ofstrok eswas\nmost likelytohaveproduced thefeature vectors weob-\nserved.Foreach strok eclass, there isacertain probabil-\nitythatthefeature vector wasdrawnfrom it;thisissim-\nplythelikelihood probability ,i.e.Pr(datajclassi).We\nneed tocombine thetransition probabilities with thelike-\nlihood probabilities inorder tocalculate themost likely\npath. This isdone bytheViterbi algorithm. Forthesake\nofbrevity,thereader isreferred toDuda etal.(2001);\nRabiner andJuang (2003) foranexplanation. Language\nmodelling wasused with themvgauss classi\u0002er andthe\nffnn. Inthelatter ,thevalues oftheoutput nodes were\nnormalized andtreated asestimates oftheposterior prob-\nabilities.\n1116EXPERIMENT ALRESUL TS\n6.1 TestConditions\nTobetter understand thegeneralization abilities ofthesys-\ntem, twotesting methods were used. The \u0002rst method\nused 10-fold cross-v alidation inwhich each experiment\nwasrepeated tentimes andtheaverage error ratereported.\nIneach trial, 90% ofthesamples were randomly selected\nfortraining, andtheremaining 10% were used fortesting.\nThesecond method tested theability oftheclassi\u0002ca-\ntion algorithm togeneralize tocompletely newdata. In\nthisprotocol, which wewillrefer toasnovelgeneraliza-\ntion, training samples andtestsamples were takenfrom\ndifferent data sets. Incross-v alidation tests (including\nTable 2:Recognition accurac y(10-fold CV)\nDatabase mvgauss pnn nnet tree\n1 0.86 0.90 0.93 0.79\n2 0.86 0.95 0.96 0.84\n3 0.86 0.93 0.95 0.80\n123 0.92 0.93 0.93 0.80\n4 0.86 0.90 0.90 0.74\n5 0.86 0.89 0.87 0.64\n45 0.86 0.89 0.87 0.72\n6 0.94 0.94 0.95 0.84\n7 0.91 0.94 0.94 0.82\n67 0.92 0.94 0.94 0.84\naverage 0.89 0.92 0.92 0.78\n12345 0.79 0.90\n12367 0.83 0.77\n4567 0.85 0.93\nleave-one-out), itislikelythatforeach category anexam-\nplethatisverysimilar toeach testexample willbefound\ninthetraining database. Forexample, thetabla strok e\ndatabase iscomposed ofstrok esthathavebeen takenfrom\nvarious tabla compositions. Because phrases areoften re-\npeated, there isahigh likelihood ofgetting severalvery\nsimilar strok esinagivendatabase, that is,strok esthat\nareplayed onthesame instrument, bythesame player ,\nunder thesame recording conditions, andwith nearly the\nsame articulation. This ispartially responsible forthehigh\nrecognition rates thatarereported instudies using thistest\nmethod. Nevertheless, thisdoes notnecessarily invalidate\nthistestmethod, asthemethod givesusthebest-case per-\nformance scenario andisuseful inpredicting recognition\nrates insituations inwhich weknowthatthetraining and\ntestsetsaresimilar .\nNovelgeneralization issigni\u0002cantly harder because\ntheclassi\u0002cation algorithm hastogeneralize across differ-\nentinstruments, players, andrecording conditions. This\npoint hasbeen made inmanyprevious instrument recog-\nnition studies (Essid etal.,2004; Gillet andRichard, 2003;\nLivshin andRodet, 2004; Sandv oldetal.,2004).\n6.2 Recognition accuracy\nResults using thecross-v alidation experiments aregiven\ninTable 2andfrom thenovelgeneralization experimentsinTable 3.\nTherecognition rateforthecombined Gillet data was\n93% forboth theffnnandthepnn, and92% forthemv\ngauss. This wassigni\u0002cantly higher than theGillet and\nRichard results (83%) using nolanguage modeling, and\nnearly equal tothe93.6% recognition rateobtained with\nlanguage modeling. Thebestrecognition ratefortheRavi\ndata was94% using theffnnandpnn. OntheTanmo y\ndata, which contained themost number ofclasses (16), the\nrecognition ratewas89%. Therecognition rateaveraged\noverseveralexperiments was92% forthemvgauss, 94%\nfortheffnnandpnn, and84% forthetreebased classi-\n\u0002er.Inthecross-v alidation experiments, alltheclassi\u0002ers\nperformed similarly ,except forthetree, which did10%\nworse onaverage.\nTable 3makeitclear thatthetwotesting paradigms\nlead todifferent recognition rates. Recognition rates for\nthenovelgeneralization experiments ranged widely ,from\nalowof15% toahigh of95%. Theaverage recognition\nrateoverawide variety oftestconditions was76% forthe\nmvgauss, 83% fortheffnn, 76% forthepnn, and66%\nforthetreeclassi\u0002er .Evenwhen taking intoaccount the\nwidely varying results, theffnnseems togeneralize bet-\nter.When Gillet sets1and2were used fortraining, and\n3fortesting, theaccurac ywas88% forboth themvgauss\nandthennet. This compares with 80% intheGillet and\nRichard study without language modelling andissimilar\ntotheir \u0002gure of90% accurac yusing language modeling.\nWhen Gillet sets2and3were used fortraining, and1for\ntesting, theaccurac ywas88%, ascompared with 78% re-\nported byGillet andRichard without language modeling,\nand88% with language modeling.\nOneclear trend thatemer gesfrom thedataisthattrain-\ningonTanmo yandRaviandtesting ontheGillet data led\ntopoor results, ranging from 29-45% using themvgauss\nclassi\u0002er .Ontheother hand, performance ranged between\n60-80% when theTanmo yandRavisetswere used totest\neach other .Thebestresults were when thedatacame from\nthesame recording conditions. Forexample, therecogni-\ntion accurac yreached ahigh of95% when theshort du-\nration Ravistrok es(database 7)were used totrain affnn\nclassi\u0002er thatwasused onthelong duration Ravistrok es\n(database 6).\nAninteresting point isthat, although alargeandvaried\ndatabase isimportant forgeneralization, what ismore im-\nportant ishaving strok esinthetraining database thatare\nsimilar tostrok esinthetestdatabase. Increasing thesize\nofthedatabase candecrease performance. Forexample,\nusing theTanmo ydata toclassify samples from theRavi\ndatabase gavea65% recognition rate. Adding theGillet\ndata tothetraning setdecreased theaccurac yto56%.\nTable 3showsthat language modelling didnotim-\nproveclassi\u0002cation performance inanycases where itwas\ntried, anditmarginally decreased performance inmost\ncases.\n7DISCUSSION\nThe system' sperformance issimilar tothat reported\nbyGillet andRichard (2003), andsubstantially better in\nthecase when nolanguage modeling isused.\n112Table 3:Recognition Accurac y(NovelGeneralization)\nTrain Test mvgauss mvgauss hmm nnet nnet hmm pnn tree\n23 1 0.88 0.88 0.87 0.81 0.82 0.71\n13 2 0.88 0.89 0.89 0.86 0.85 0.73\n2 3 0.86 0.84 0.82 0.78 0.86 0.73\n12 3 0.88 0.89 0.88 0.84 0.87 0.76\n5 4 0.84 0.89 0.85 0.71\n4 5 0.80 0.85 0.80 0.66\n7 6 0.91 0.95 0.92 0.84\n12345 6 0.56 0.65 0.44 0.56\n45 6 0.65 0.84 0.76 0.59\n6 7 0.84 0.90 0.87 0.78\n12345 7 0.58 0.63 0.41 0.45\n45 7 0.58 0.79 0.67 0.51\n45 67 0.61 0.78 0.70 0.53\naverage 0.76 0.83 0.76 0.66\n12345 67 0.57 0.65 0.49\n4567 1 0.37 0.45\n4567 2 0.45 0.60\n4567 3 0.29 0.15\n4567 123 0.39 0.20\n45 123 0.26\n67 123 0.37\n123 67 0.50\nThedifferent results intheno-language modeling case\naremost likelyduetotheincorporation ofmore sophisti-\ncated features. Another factor isthat, intheGillet study ,\ntimbres thatwere aurally thesame butnominally different\nretained their nominal labels. Forexample, tiandteareal-\nmost alwaysacoustically indistinguishable. Inthecurrent\nstudy ,theywere giventhesame label, whereas Gillet and\nRichard (2003) retained theoriginal labels. Because teal-\nmost alwaysfollowtiinnotation, thisdistinction could\nonly becaptured bythelanguage model. Inboth studies,\nitislikelythat, fortheGillet data, theupper performance\nbound isclose tobeing reached. Most oftheremaining\nerrors aredatabase issues, such asmishits andmislabeled\nstrok es.\nTable 4showsatypical confusion matrix. This isfrom\nacross-v alidation experiment using alltheGillet datawith\nthemvgauss classi\u0002er .There arethree common confu-\nsions, naanddha,tunanddun,andkatandte.The\u0002rst\nandsecond areduetothefactthat, inmanynastrok es,\nthebass strok eisstillringing from aprevious strok e,be-\ncause dha=na+ge.This makesitveryeasy toconfuse na\nwith dha.This isalso thecase with thetun,dunconfu-\nsion. Because ofthehigh prior probability ofte,several\noftheother closed stokesaresometimes confused with\nit.Although notshown,confusion matrices forother data\nsetswere similar .IntheTanmo ydata, themost common\nconfusion wasagainbetween strok esandtheir compound\nversion (i.e. thestrok e+ge).Again,thehigh prevalence\nofteinthetraining data ledtomanystrok esbeing occa-\nsionally incorrectly classi\u0002ed aste.\nTheposterior probabilities generated bythemvgauss\nmodel were examined totryandunderstand whylanguage\nmodeling didnothelp. Wewould expect language mod-eling tohaveanaffectwhere theposterior isdistrib uted\namong twoormore strok es.Inmost cases, theposterior\ndistrib ution ishighly concentrated ononestrok e,with the\nprobably ofallother strok esbeing much less than one\npercent. Because theposterior andthetransition proba-\nbilities aremultiplied intheViterbi algorithm, thetotal\nprobability isdominated bytheposterior inthose cases.\nAhighly concentrated posterior indicates thecon\u0002dence\nwith which theclassi\u0002er isassigning alabel. Examination\noftheposterior distrib utions revealed nosigni\u0002cant dif-\nference intheentrop yofcorrectly andincorrectly labeled\nstrok es.Incases where theposterior distrib ution isspread\nout, theentrop ywill begreater .The posterior wasusu-\nallyhighly concentrated evenincases where theclassi\u0002er\nfailed, which means thatthetransition probabilities ofthe\nlanguage model didnotchange theclassi\u0002cation decision.\nAuditioning some oftheincorrectly classi\u0002ed strok es\nrevealed thatmanyoftheerrors were duetoambiguities\ninthedata. Forexample, theconfusion matrix showsthat\nmanynastrok eswere classi\u0002ed aste,eventhough thetwo\nstrok esaretheoretically distinct. Naisaringing, pitched\nstrok e,wheras teisanon-resonant, non-pitched strok e.\nListening tosome ofthemisclassi\u0002ed nastrok esrevealed\nthatsome them were played poorly ,resulting inaslapping\nrather than ringing sound. Although, noteveryerror was\nauditioned, itislikelythatlanguage modeling didnothelp\nbecause manyoftheerrors hadtodowith thedata source,\nandnottheaccurac yoftheclassi\u0002er .\n8FUTURE WORK\nThecurrent workisthestart ofbuilding anautomatic tabla\ntranscription system. Toaccomplish this, amuch larger,\n113Table 4:Confusion Matrix forGillet Data\ndhadhin natintundhun dhedhec rerecneckatgetetas\ndha 357 122000 0 000002220\ndhin 227 03013 000001110\nna 76 0418 10 0 300024010\ntin 00070 0 000004012\ntun 0000106 11 000050350\ndhun 0300074 000010400\ndhe 00000 027 07010030\ndhec 00000 0 130103070\nre 00000 0 5335230060\nrec 00000 0 200900020\nnec 00004 2 00003900120\nkat 70050 2 10000347 3160\nge 3200014 0000720400 20\nte 100112 0 24585111 5988 0\ntas 00000 0 000000017\nmore varied database needs tobecreated. Thetotal num-\nberofstrok eswilllikelyhavetobeincreased byanorder\nofmagnitude inorder tocapture thesuf\u0002cient strok eva-\nriety.This will also require drawing strok esfrom many\ndifferent players andrecording conditions.\nOnce thisisdone, there willbesuf\u0002cient data tode-\nvelop ahierarchical Mark ovmodel. The current work\ndoes nottakeintoaccount thetemporal evolution offea-\ntures. This could bemodeled with anhierarchical hmm.\nInalmost allrealperformances oftabla solo, thetabla\nisaccompanied byamelodic instrument such asthehar-\nmonium orsarangi.Future workwill attempt tolabel\nstrok esfrom recordings thatcontain such accompaniment.\nSuccess willlikelydepend onsuccessfully expanding the\ndatabase andusing dynamic features.\nACKNO WLEDGEMENTS\nMysincere thanks toOlivierGillet andGael Richard for\nproviding their data tome.\nREFERENCES\nR.Duda, P.Hart, andD.Stork. Pattern Reco gnition and\nScene Analysis .John Willey,2001.\nB.Duxb ury,J.Bello, M.Davies, andM.Sandler .Com-\nplexdomain onset detection formusical signals. InIn\nProc.Digital Audio Effects Workshop (DAFx) ,2003.\nS.Essid, G.Richard, andD.Bertrand. Musical instrument\nrecognition based onclass pairwise feature selection. In\nInProc.ofthe5thISMIR Conf .,2004.\nN.Fletcher andT.Rossing. Thephysics ofmusical instru-\nments .Springer ,NewYork,1998.\nO.Gillet andG.Richard. Automatic labelling oftabla\nsignals. InInProc.ofthe4thISMIR Conf .,2003.\nM.Goto. Anaudio-based real-time beat tracking system\nformusic with orwithout drum sounds. Journal ofNew\nMusic Resear ch,pages 159171, 2001.P.Herrera, A.Yeterian, andF.Gouyon. Automatic classi-\n\u0002cation ofdrum sounds: acomparison offeature selec-\ntion andclassi\u0002cation techniques. InProc.ofSecond\nInt.Conf .onMusic and Arti\u0002cial Intellig ence,pages\n7991, 2002.\nP.Herrera, A.Dehamel, andF.Gouyon. Automatic label-\ningofunpitched percussion sounds. InProceedings of\ntheAudio Engineering Society ,pages 137891, 2003.\nA.Klapuri. Sound onset detection byapplying psychoa-\ncoustic knowledge. InInternational Confer ence on\nAcoustics, Speec h,andSignal Processing ,1999.\nA.Livshin and X.Rodet. The importance ofcross\ndatabase evaluation inmusical instrument sound clas-\nsi\u0002cation: Acritical approach. InInProc.ofthe5th\nISMIR Conf ,2004.\nL.Rabiner andB.Juang. Fundamentals ofSpeec hReco g-\nnition .Prentice-Hall, 2003.\nV.Sandv old,F.Gouyon, andP.Herrera. Drum sound clas-\nsi\u0002cation inpolyphonic audio recordings using local-\nized sound models. InInProc.ofthe5thISMIR Conf ,\n2004.\nA.Schloss. OntheAutomatic transcription ofpercus-\nsivemusic- Fromacoustic signal tohigh-le velanalysis .\nCCRMA, 1985.\nJ.Sillanpaa, A.Klapuri, J.Seppanen, andT.Virtanen.\nrecognition ofacoustic noise mixtures bycombined\nbottom-up andtop-do wnapproach. InInProc.ofEU-\nSIPCO ,2000.\nA.Tindale, A.Kapur ,G.Tzanetakis, andI.Fujinag a.Re-\ntrievalofpercussion gestures using timbre classi\u0002cation\ntechniques. InInProc.ofthe5thISMIR Conf .,2004.\nD.vanSteelant, K.Tanghe, S.Degroeve,B.DeBaets,\nM.Leman, andJ.Martens. Classi\u0002cation ofpercus-\nsivesounds using support vector machines. InProc.\noftheannual machine learning confer ence ofBelgium\nandTheNetherlands ,2004.\n114"
    },
    {
        "title": "Fuzzy Analysis in Pitch-Class Determination for Polyphonic Audio Key Finding.",
        "author": [
            "Ching-Hua Chuan",
            "Elaine Chew"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417297",
        "url": "https://doi.org/10.5281/zenodo.1417297",
        "ee": "https://zenodo.org/records/1417297/files/ChuanC05.pdf",
        "abstract": "This paper presents a fuzzy analysis technique for pitch class determination that improves the accuracy of key finding from audio information. Errors in audio key finding, typically incorrect assignments of closely related keys, commonly result from imprecise pitch class determination and biases introduced by the quality of the sound. Our technique is motivated by hypotheses on the sources of audio key finding errors, and uses fuzzy analysis to reduce the errors caused by noisy detection of lower pitches, and to refine the biased raw frequency data, in order to extract more correct pitch classes. We compare the proposed system to two others, an earlier one employing only peak detection from FFT results, and another providing direct key finding from MIDI. All three used the same key finding algorithm (Chew’s Spiral Array CEG algorithm) and the same 410 classical music pieces (ranging from Baroque to Contemporary). Considering only the first 15 seconds of music in each piece, the proposed fuzzy analysis technique outperforms the peak detection method by 12.18% on average, matches the performance of direct key finding from MIDI 41.73% of the time, and achieves an overall maximum correct rate of 75.25% (compared to 80.34% for MIDI key finding).",
        "zenodo_id": 1417297,
        "dblp_key": "conf/ismir/ChuanC05",
        "keywords": [
            "fuzzy analysis",
            "pitch class determination",
            "audio information",
            "key finding",
            "imprecise pitch class",
            "noisy detection",
            "lower pitches",
            "biased raw frequency",
            "Chew’s Spiral Array CEG algorithm",
            "410 classical music pieces"
        ],
        "content": "FUZZY ANALYSIS IN PITCH CLASS DETERMINATION FOR \nPOLYPHONIC AUDIO KEY FINDING\nChing-Hua Chuan Elaine Chew  \nDepartm ent of Com puter Science  \nUniversity  of Southern California  \nIntegrated Media Sy stem s Center \nLos Angeles, CA90089, USA  \nchinghuc@usc.edu  Epstein Dep of Industrial & Sy stem s Eng  \nUniversity  of Southern California  \nIntegrated Media Sy stem s Center \nLos Angeles, CA90089, USA  \nechew@usc.edu  \nABSTRACT \nThis paper presents a fuzzy analysis techni que for pi tch \nclass determ ination that improves the accuracy of key \nfinding from  audi o inform ation. Errors in audio key \nfinding, t ypically incorrect  assi gnments of closely \nrelated  keys, co mmonly resu lt fro m imprecise p itch class \ndetermination and biases introduced by the quality o f the \nsound. Our techni que i s motivated by  hypotheses on t he \nsources of audio key  finding errors, and uses fuzzy  \nanalysis to reduce the errors caused by  noisy detection of \nlower p itches, an d to refine the biased raw frequency  \ndata, in order to extract m ore correct pitch classes. We \ncompare t he proposed sy stem to two others, an earlier \none em ploying onl y peak det ection from  FFT resul ts, \nand another provi ding di rect key finding from  MIDI. Al l \nthree used t he sam e key  finding al gorithm (Chew’s \nSpiral Array  CEG algorithm) and t he sam e 410 cl assical \nmusic pieces (ranging from  Baroque to Contem porary). \nConsidering only the first 15 seconds of m usic in each \npiece, the proposed fuzzy analysis technique \noutperform s the peak det ection m ethod by  12.18% on \naverage, m atches the perform ance of direct key finding \nfrom  MIDI 41.73% of t he time, and achi eves an overal l \nmaximum correct  rate of 75.25% (com pared to 80.34% \nfor M IDI key  finding). \n \nKeyw ords: audio key  finding, pi tch classes, fuzzy  \nanalysis, key  proxi mity.  \n1 MOTIVATION \nPolyphoni c audio key finding has gai ned interest in \nrecent years, with several researchers proposing system s \nfor ext racting key  from  audi o inform ation [1, 2, 3, 4] . \nKey finding from  audio typically requi res several  steps, \nincluding pitch class det ermination from  audi o \n(som etimes wi th pitch spel ling), and key  finding from  \npitch classes. Th e pitch class determination step \nprovi des pitch class i nform ation from  audi o signals; \nonce t he pitch class di stribution has been ascertained, \nthen a key  finding al gorithm can use t his inform ation to \ndetermine the key  of t he excerpt . In order t o improve the performance of existing system s, it is imperative that \nwe shoul d be abl e to segregat e the sources of errors and \nimprove on each m odule of the system .   \nIn this paper, it is o ur goal to examine pitch class \ndetermination for key finding, t o study the sources of \nerrors, and propose a m ethod t hat is tailored to reduce \nkey finding error from  audi o signals. To t his end, we \npropose a fuzzy  anal ysis techni que t o refine the pitch \nclass distribution extracted from the au dio sam ple in  \norder t o improve t he likelihood of correct  key  \nidentification, and t o avoi d closely related key s. \nPitch cl ass det ermination for audi o key  finding \ndiffers in several ways from  pitch det ection for \ntranscri ption. Key  finding i s concerned only with \ndetermining the tonal context and not with  identifyin g \nevery  individual not e. Transcri ption requi res t he \nrecogni tion of every  single pitch, whi ch includes pi tch \nclass and regi ster inform ation, and i ts durat ion, whereas \nkey finding onl y needs pi tch class inform ation. \nFurtherm ore, contextual inform ation such as key can \noften be assisted by  the phy sical product ion (for \nexam ple, a m usician stressi ng st ructurally important \npitches) and acoust ic interactions (for exam ple, the \nstrongest  harm onics tend to be pi tches i n the sam e key) \nof musical sound.  \nWe have i dentified several  sources of errors i n pitch \nclass det ermination for audi o key  finding. It is \nstraightforward to obtain frequency  inform ation (pi tch \nclasses) from  audi o signals using frequency  anal ysis \nmethods such as the Fast  Fouri er Transform  (FFT) [5] . \nSources of errors i n pitch class det ermination include: \nuneven loudness of pi tches, i nsufficient resol ution of \nlower frequency  pitches, t uning probl ems, and the \nharm onic series effect . Although frequency  anal ysis can \nidentify all frequencies present in an audio segment, the \nlouder pitches will have higher frequency spectrum  \nvalues than ot hers. Pi tch percept ion operat es on a \nlogari thmic frequency  scal e, resulting in the fact that \nlower pi tches are cl oser i n frequency  and hence harder \nto discriminate than hi gher frequenci es. Pitches are \noften descri bed as corres pondi ng to discret e frequency  \nvalues, which is probl ematic when one encount ers \nsounds produced by  instruments that are mistuned. Last \nbut not least, each tone pr oduced by an instrum ent \nconsi sts not only of the fundam ental frequency , but also \na sequence of frequencies that are the effects of the \nharm onic seri es.  Permission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or\ncommercial advantage and that copies  bear this notice and the\nfull citation on the first page. \n© 2005 Queen Mary , University  of London Based on experi mental resul ts, we found t hat audi o \nkey finding, m ore frequent ly than sy mbolic key finding, \nresults in the m islabeling of pieces as being in keys \n296   \n \nclosely related  to the co rrect one. Such errors occur \nbecause closely related ke ys have large overlapping \npitch class sets. For exam ple, the relative major/m inor \nkeys share exactly the sam e pitch classes, t he onl y \ndifference bei ng t heir typical distributions. A sl ight \ntipping of t he bal ance of pi tch class distributions can \nlead to close but incorrect  answers. The m ost com mon \nerror i n audi o key  finding is the mislabeling of a sample \nas being in a key that is th e dominant of the actual one, \nfor exam ple, labeling a sam ple in C major as G major.  \nTwo key s related in this manner share all but one pitch \nclass, t hat of the leading tone (or t he sevent h note in its \nscale).  The pitch classes {C, G, E and F} feature \nstrongl y in a typical distribution of pi tch classes in C \nmajor; the correspondi ng pi tch class set  for G major is \n{G, D, B and C}.  Because both G and C are im portant \npitches in G m ajor, a sm all ch ange in the pitch class \ndistribution can resul t in G bei ng sel ected as t he key .  In \naudio key finding, the probl em is exacerbated by the \nfact that the dom inant is typically the strongest  \nharm onic of a t one ot her than the pitch class of the tone \nitself. Rath er th an elim inating the harmonics that help \nconstrain the answers to closely related keys, we focus \non reduci ng noi se in the dat a and on t he refi ning of the \npitch class di stribution to improve key  recogni tion. \nOur approach t o this probl em uses a fuzzy analysis \ntechni que t o adjust  the pitch class distribution, in light \nof the chal lenges m entioned above, so as to emphasi ze \nthe correct tonal context for accurate key finding. In \naddition to the use of a fuzzy analysis technique to \nimprove pitch class d etermination, in this paper, we also  \nprovi de a m ethodol ogy for eval uating the effect iveness \nof various pitch class det ermination st rategies for audi o \nkey finding.  \nAlthough sy mbolic key finding has been st udied for \nmore than a decade and various evaluation \nmethodol ogies have been em ployed, the evaluation for \naudio key finding, especi ally the pi tch cl ass \ndetermination part , rem ains obscure. There are two main \nprobl ems that make the eval uation for audi o key  finding \nill-defined. Firstly, th e degree of incorrectn ess of closer \nkeys (th e dominant, relativ e, an d parallel) is difficu lt to \ndecide. Secondl y, it is uncl ear how one shoul d judge the \nperform ance of pi tch class det ermination m ethods when \none does not  requi re exact  pitch det ection. \nThe difficulty of key  finding vari es wi dely across \nmusical styles. Usi ng the key  denot ed by  the composer \nin the title, when such an answer ex ists, is o nly useful \nup to a degree. Consider a sym phony with m ultiple \nmovem ents: the piece stays in the m ain key only in the \nfirst and l ast movem ents. The second and third \nmovem ents are t ypically com posed i n other keys. Even \nin the first m ovem ent, it is not uncom mon for the piece \nto modulate to closely related key s, such as t he \ndominant and relative. Furtherm ore, in classical music, \nthe language get s progressi vely more com plicated over \nthe course of tim e. For exam ple, the music in  the late \nRomantic peri od i s tonally much m ore diverse and \ncomplex than that in the Baroque peri od. The increasi ng \ncomplexity of tonal struct ure in pieces poses another \nchallenge for eval uation of audi o key  finding. However, the eradication of closer key errors in single-key \nexam ples m ust to be sol ved before audio key finding \ncan advance furt her to account  for m odulations. \nIn this paper, we provi de an eval uation m ethodol ogy \nfor pitch class det ermination in audi o key  finding by  \ncomparing the key  assignment resul ts for sy mbolic and \naudio music using C hew’s Spi ral Array  Center of Effect  \nGenerator (CEG) key  finding algorithm  (see [6, 7]). The \ntest sets for audio are rendered from  MIDI so that the \naudio key fin ding will u se exactly th e sam e test set as \nthat in MIDI. B y using the sam e key finding al gorithm \nand test sets, we can com pare the results of MIDI and \naudio key  finding to eval uate the perform ance of t he \nproposed pi tch class det ermination techni que.  \nWe used our system  to te st 410 pieces of classical \nmusic across a wi de spect rum of t ime peri ods rangi ng \nfrom  Baroque to Contemporary . The sel ection consi sts \nof a wi de range of t onal music so that the aggregat e \nresults will be as unbiased as possible. Detailed analysis \nof results for each m usical period is also provided.  \nThe rem ainder of t he paper i s organi zed as follows: \nSectio n 2 provides a literatu re rev iew o f work in audio \nkey finding, Sect ion 3 descri bes t he overal l system \ndiagram  and introduces each part of the system , \nincluding the new fuzzy an alysis techni que, t he \nexperi mental design and resul ts are present ed in Sect ion \n5, and concl usions and fut ure work fol low in Sect ion 6. \n2 BACKGROUND \nThe approach i n this paper di ffers from  previ ous effort s \nin three way s, the obvi ous t wo being the fuzzy  analysis \ntechni que for pitch class det ermination and t he use of t he \nSpiral Array  CEG algorithm for key  finding. The t hird \ndistinguishing feat ure i s the systematic exam ination of \nthe part s of a key  finding system, and careful  evaluation \nto isolate and m easure t he improvem ents provi ded by \nchanges t o a part icular com ponent . \nAudi o key  finding sy stems requi re two basic \ncomponent s: one com prising of som e pitch class \ndetermination m ethod such as t he generat ing of pitch \nclass distributions, and anot her consi sting of som e key \nfinding al gorithm that determines the key  given a pitch \nclass di stribution. In t his paper, we aim to improve the \npitch cl ass det ermination m ethod by  using a fuzzy  \nanalysis technique and to system atically evaluate its \neffect iveness by  com paring it to symbolic key finding. \nMost research i n audi o key  finding fails to discriminate \namong t he sources of errors, report ing only the overal l \nsystem’s key  finding resul ts [1, 2, 4] .   \nIn Góm ez [1]  and in Góm ez and Herrera [2], the \nauthors det ected pi tches usi ng t hrice t he standard \nresol ution of t he pitch frequency  spect rum of t he FFT \nmethod, and di stributed the frequency  values am ong t he \nadjacent frequency bins usi ng a weighting function to \nreduce boundary  errors. They  generat ed a Harm onic \nPitch C lass Profi le as i nput to Krum hansl  and \nSchm uckler’s (K-S) key  finding m ethod [8] . Thei r \ntemplate pitch class profi le gives t he dom inant a hi gher \nweight than the tonic, a counterintuitive assignm ent. \n \n297   \n \nThey  report ed an overal l correct  rate of 66.1% when \ntesting on 833 pieces of classi cal and jazz pieces [2].  \nPauws [4]  incorporat ed rul es for avoi ding noi se and \nemphasi zing pi tch loudness i n his pi tch class \ndetermination m ethod, and appl ied the K-S m ethod t o \ngenerat e the key . Pauws used 237 cl assical piano \nsonat as as t he test set and hi s method resul ted in a \ncorrect  rate of 59.1% within 5 seconds, and achi eved a \nmaximum correct  rate of 66.2% wi thin 15 seconds.  \nIt is unclear if the errors report ed by  the aut hors of \nthese two audio key finding sy stems are due i n larger \npart to their pitch class det ermination techni ques, or to \nthe key finding al gorithm.  The t wo sy stems differ \nprimarily in  the pitch class d etermination step , in the \nway in  which the system s generate th e pitch class \nprofile fo r the stan dard FFT resu lts.  Both used the K-S \nprobe tone method [8] for key  finding, wi th Góm ez \nusing a m odified template profi le. \nIn 1996, Izmirli and B ilgen present ed a m odel that \nanalyses tonal cont ext as a continuous funct ion [3]. \nThey  used a const ant Q transform  to generat e pitch \nclasses and proposed a l eaky  integrator based on the K-\nS model to determine the tonal cen ter. Ho wever, in their \nstudy, only two m usic excerpts are evaluated, a less than \nrepresentative sam ple size. \n There exi sts only a limited num ber of m odels for key \nfinding. In 1986, Krum hansl  and Schm uckler (K-S \nmodel) [8] proposed the probe t one profi le method t hat \nmatches pitch duration profiles to  tem plate pitch class \nprofi les for major and m inor key s, acqui red from  user \nratings of probe t one experi ments. The key is \ndetermined as the one with  the highest correlatio n value. \nIn 1999, Tem perley improved upon t he K-S m ethod by \nmodifying t he template pitch class profi les through \nmusical reasoni ng [9]. Tem perley modified the profi les \nto em phasize the differences between diatonic and \nchrom atic scales, and also adjusted the weights of the \nforth and sevent h pitches so as t o different iate the key s \nwith highly sim ilar p itch class sets.  \nIn this paper, we em ploy the Spi ral Array  CEG \nalgorithm proposed by  Chew [6, 7]  to determine key  for \nMIDI and audi o. The Spi ral Array  Model is a 3-\ndimensional model that rep resents pitches, intervals, \nchords and keys in the sam e space for easy comparison. \nOn the Spiral Array , pitches are represent ed as poi nts on \na helix, and adjacent pitches are related by intervals of \nperfect  fifths, whi le vert ical neighbors are rel ated by \nmajor thirds. In the CEG alg orithm, key selectio n is \nperform ed by  sum marizing m usical inform ation as a \nspatial p oint in the in terior of the sp iral and by \nconduct ing a nearest  neighbor search i n the Spi ral Array  \nspace. Although the K-S m odel is one of the most \nwidely used key  finding m ethods, t he Spi ral Array  CEG \nmodel has been dem onstrated to achi eve bet ter key  \nfinding resul ts using sy mbolic data sets [6, 7]. We \nimplemented both the Spi ral Array  CEG method and t he \nK-S model in an earlier audi o key  finding sy stem that \nemployed si mply peak det ection from  FFT [10]. We \nobserved t hat the CEG m ethod agai n consi stently \noutperform ed the K-S m ethod wi th few except ions. In \nthis paper, we will use the Spiral Array CEG m ethod as a const ant among t he three sy stems we t est: key finding \nfrom  MIDI, key  finding from  audi o using only peak \ndetection, and key finding from  audi o usi ng peak \ndetection and fuzzy analysis. \n3 SYSTEM DESCRIPTION \nIn our proposed audi o key  finding sy stem, we em ploy a \nfuzzy analysis technique, with adaptive weig hts an d \nperiodic cleanup, t o generat e a pitch class distribution \nthat reduces cl oser-key  errors i n key finding. Figure 1 \nshows a diagram  of the system for pol yphoni c audi o key  \nfinding.  \nThe sy stem consi sts of t wo major part s. The fi rst \ngenerat es a di stribution of wei ghts for t he twelve pitch \nclasses from  audi o signals, as shown i n the upper \ndashed box i n Figure 1. W e use the FFT to extract the \nfrequency  inform ation, and em ploy the peak detection \nmethod descri bed in [10] and i n Sect ion 3.1. W e appl y a \nfuzzy  anal ysis techni que (det ailed in Sect ion 3.2) and a \nperiodic cleanup procedure (expl ained i n Section 3.3) to \ngenerat e refined weight distributions for t he key  finding \nalgorithm in order t o increase t he likelihood of \nobtaining the correct  key. This fuzzy  analysis method \nwill b e describ ed in detail in  Sectio n 3. \nThe second part  of t he sy stem cont ains the key  \nfinding al gorithm. Thi s module consi sts of pitch \nspelling and key  finding. To represent  pitch class \ninform ation for com parison to key represent ations, we \nuse Chew’s Spi ral Array  Model [6, 7] . The pi tch \nspelling method for mapping num eric pitch classes t o Figure 1. Graph of audi o key finding system.FFT\npeak detection \nfuzzy anal ysis  \n00.10.20.30.40.50.6\npolyphonic audi o \nperiodic cleanup\n1 2 3 4 5 6 7 8 9 10 11 12pitch class di stribution \nCEG Al gorithm and Spi ral Array  Model \nKEY!\n \n298   \n \nletter n ame pitch class rep resentations on the pitch spiral \nis descri bed i n [11]  and [12] .  Finally, we employ the \nCEG algorithm [6, 7, 13]  to determine the key . \nIn Sectio n 4, we will co mpare three system s for key \nfinding.  The first, key finding from  MIDI is cont ained \nwithin the lower dashed box i n Figure 1:  this system \ntakes M IDI fi les as i nput, generat es pitch classes, and \nuses the Spiral Array CEG alg orithm to determine the \nkey. The second sy stem perform s audio key finding \nusing the FFT and a peak det ection m ethod [10]  in the \npitch class det ermination phase (hencefort h referred to \nas the audi o key  finding wi th peak detection); this \nsystem is represent ed by  all modules in the diagram  \nexcept  for the gray  boxes i n Figure 1. The t hird is the \naudio key finding system with the fuzzy  anal ysis \ntechni que and peri odic cleanup procedure. The entire \nsystem is shown i n Figure 1. \nIn the fol lowing sect ions we descri be the methods \ndesigned to reduce noise and refi ne the pitch class \ndistribution in the pitch class det ermination phase.  \n3.1 Pi tch frequency detecti on from audi o signal \nWe use standard FFT wi th the peak det ection m ethod \ndescri bed in [10] to extract the correspondi ng frequency  \nmagnitude for each pitch. The peak detection method \nselects th e local m aximum with in the freq uency ran ge \npre-defined for each pitch.  In [10], we sum med these \nmaximum values for all p itches in a class to  get the pitch \nclass distribution. Instead of di rectly using the local \nmaximum values to generat e the pitch class di stributions, \nhere, we apply a fuzzy analys is technique (described in \nthe next sectio n) to refin e the local maxima so as to \nobtain m ore accurate pitch cla ss distributions that avoid \ncloser-key  errors.  \n3.2 Fuz zy anal ysis with adapti ve level weights \nWe use a fuzzy analysis technique to clarify pitch \ninform ation from  the frequency  spectrum . Pitch class \ndetection from  audio signal is inherent ly noisy for t he \nreasons outlined in Section 1 – uneven loudness of \npitches, harm onic seri es effect, and insufficient \nresolution in lower freq uencies, in crease th e difficu lty of \nrecognizing polyphoni c audio pitch – resulting in the \nincorrect detection of closely related keys, such as the \ndominant, relativ e an d parallel k eys. Th e erro rs also  \naccum ulate, thus worsening the perform ance of key \nfinding sy stems over t ime.  \nOur m ethod consi sts of three st eps. The fi rst two aims \nto clarify in formation in the lower freq uencies. Th e fist \nstep, det ailed in Sect ion 3.2.1, uses knowl edge of the \novertone series to clarify m embership in the lower \nranges. The second st ep, descri bed in Section 3.2.2, \nscales the FFT results in each pre-defined range by the \ndensi ty of t he signal in that range so as to properl y \nacknowl edge the presence of important pitches i n that \nfrequency  range. This second st ep, cal led adapt ive level \nweig hting, is p articularly effectiv e in the clarifyin g of \nlow pitches in late ro mantic m usic, esp ecially m usic th at \nbegins almost excl usively in the low frequency  ranges.  \nAfter th e pitch values have been folded into pitch class values, we em ploy the third and fi nal step to refine the \npitch class d istribution. The th ird step , outlined in \nSection 3.2.3, set s all norm alized pi tch class val ues 0.2 \nand bel ow to zero, and al l values 0.8 and above t o one. \nThe reason that we use fuzzy analysis is that one can \nview the results of an FFT analysis as a fuzzy \ndescrip tion, not a calcu lated  probability, o f the \nlikelihood t hat a pi tch is played. The fuzzy  analysis \ntechni que provi des several  advant ages:  (1) i t generat es \nmore accurate weight distribu tions for pitch classes to \ndeterm ine the correct key, inst ead of cl osely related ones;  \nand, (2) B y em ploying adapt ive level wei ghts, the \ntechni que i s robust  agai nst the impact of different  \nmusical arrangem ents (the vary ing of regi sters and \ninstrumentation) and st yles. \n3.2.1 C larifying low frequenci es \nIn Step 1, we use the overtone series as the basis for \nfuzzy an alysis to  clarify p itches of frequencies below \n261 Hz (t he pi tch C 4). Pitch frequenci es are defined on a \nlogari thmic scal e; thus, t he frequenci es of lower pitches \nare more closely spaced than higher ones. The m apping \nof lower frequenci es to their pitches, defi ned by  discret e \nfrequency  ranges, i s part icularly noisy and error prone. \nIn contrast, assignm ent of higher-frequencies to pitches \nis more accurate because of the relatively wider \nfrequency ranges. Therefore, we use the presence of the \nfirst overt one to determine and t o refine the wei ghts for \nlower p itches. \nWe use the idea of the m embership value in fuzzy \nlogic to represent  the likelihood t hat a pi tch has been \nsounded. The m embershi p values are based on t he FFT \nresults after p eak detectio n. We set th e highest peak, \nPmax, the pitch membership value of the largest FFT \nresult, to one, which one can interpret as assu ming that \nthis pitch is definitely sounded. W e get the membershi p \nvalues for al l other pi tches by  dividing their peak values \nby Pmax. We set th e values less th an 0.1 to zero  to \neliminate some noise. The value 0.1 was chosen after \nperforming some preliminary tests. Assu me that Pi,j \nrepresents the pitch of class j at register i, for exam ple, \nmiddle C (C 4) is P4,0. Let FFT(P i,j) be the local peak for \npitch Pi,j after the FFT. Then, the membership value for \nPi,j is defined as:  \n    mem ,                              (1) max , , P/)( )( ji ji P FFT P=\nwhere i = 2, 3, 4, 5, 6, and j = 1…12, whi ch allows for \npitches rangi ng from  C2 (65 Hz) t o B6 (2000 Hz). \nBy examining the membership values of the pitch an \noctave above, t he pitch one ha lf-step above and i ts first \novert one, we can remove t he com mon errors caused by  \ninsufficient frequency  resolu tion and di scret e frequency  \ndefinition in lower pitche s. The m ethod nullifies the \nmembership value of the lower p itch for which the pitch \none half step above has a m embershi p value higher t han \nits own, or t he pitch one oct ave higher or the pitch one \nhalf step and one oct ave hi gher has a m embershi p value \nhigher than its o wn. A reaso n for this nullifyin g step is \nthat if the pitch one half step above has t he higher \nmembership value, then it is lik ely th at the present pitch \nis in correctly d etected .  An other reaso n is that pitch \n \n299   \n \nmembership values are m ore accurate in the higher \nregisters th an in lower o nes.  Hen ce, if the membership \nvalue of the pitch an oct ave above i s higher, t hen this \nhigher val ue already  account s for t hat pitch class in the \nfinal distribution.  If the membershi p value of t he pitch \na hal f step pl us an oct ave above i s higher, then it is \nlikely that the current  one was not  sounded.  \nMath ematically, we first define the membership \nnegat ion val ue for lower p itches, a q uantity th at \nrepresent s the fuzzy  likelihood t hat a pitch is not \nsounded. The m embershi p negat ion val ue is the \nmaximum of the membershi p values of t he pitch one \nhalf step above ( Pi,j+1), and t he first overt ones of t he \npitch itself ( Pi+1,j) and t hat of the pitch one hal f step \nabove ( Pi+1,j+1 ):  \n{ })1,1( ),,1( ),1,( max),( ~ ++ + + = jiP memjiP memjiP mem jiP mem ,    (2) \nwhere i = 2, 3 and j = 1…12, because we consider only \nthe lower frequency  pitches, pi tches bel ow C 4. \nNext, we set th e m embership values of lower-\nfrequency  pitches t o zero i f its membershi p negat ion \nvalue is larg er than its m embership value: \n\n≤>=) mem(P) mem(P if P mem) mem(P) mem(P if , P mem\ni,j i,j jii,j i,j\nji,~ ), (~ 0) (\n,,   (3) \nwhere i = 2, 3 and j = 1…12 . \n3.2.2 Adaptive level weighting \nThe fuzzy  anal ysis techni que descri bed i n the previ ous \nsection pri oritizes t he membershi p values of higher \nfrequency  pitches. Thi s becom es probl ematic in key  \nevaluation of pieces containi ng large segm ents of m usic \nwith only lower freq uency pitches. The adaptive level \nweighting schem e descri bed here scal es the FFT resul ts \nin each pre-defined range by the density of the signal in \nthat range so as to better detect the presence of \nimportant pitches in that frequency  range.  \nThe adapt ive level wei ght for a given range, a \nscalin g facto r, is th e relativ e density o f signal in that \nrange.  For exam ple, the adaptive level weight for \nregister i (whi ch includes pi tches C i through Bi), Lw i, is \ndefined as:   \n∑∑∑\n== ==6\n212\n1,12\n1, ) ( ) (\nkjjk\njji i P FFT P FFT Lw ,               (4) \nFinally, we generate the we ight for each pitch class, \nmem (Cj), by summing the membershi p values of t hat \npitch across all reg isters, m ultiplied by th e \ncorrespondi ng adapt ive level weight:  \n∑\n==6\n2) ( * )(,\niP mem Lw C memji i j , where j = 1…12.       (5) \n3.2.3 Fl atten high and l ow values \nTo reduce m inor differences in the membership values \nof important pitch classes an d to elim inate lo w-lev el \nnoise, we i ntroduce t he final step descri bed i n this \nsectio n.  We set th e pitch class membership values equal \nto one if th ey are larg er th an 0.8, and equal to zero if they are less than 0.2. The fl at out put for hi gher \nmembershi p values prevent s louder pi tches from  \ndominating the wei ght distribution. Last  but not least, \nwe normalize th e membership values fo r all p itch \nclasses by scaling them  to sum  to one.  \n3.3 Periodic cleanup \nBased on our observations, errors tend to accum ulate \nover time. To count er this effect , we i mplemented a \nperiodic cleanup procedure th at takes place every 2.5 \nseconds. In this cleanup step, we sort  the pitch classes i n \nascendi ng order and i solate the four pitches with the \nsmallest m embership values. W e set th e two  smallest \nvalues to zero, a reasonable choice since m ost scales \nconsi st of onl y seven pi tch classes. For the pitch classes \nwith the third and fourth smallest membership values, \nwe consul t the current  key  assi gned by the CEG \nalgorithm; if the pitch class does not belong to the key, \nwe set th e membership value to zero  as well. \n4 EXPERIMENTS AND RESULTS \nTo eval uate the fuzzy  anal ysis techni que, we choose \nexcerpts from  410 classical music pieces by various \ncomposers across di fferent  time and stylistic periods, \nrangi ng from  Baroque t o Contemporary .  M ost the \npieces are concertos, preludes, and symphonies, \ncomprising of polyphoni c sounds from  a vari ety of \ninstrum ents. The key of each piece is stated explicitly by \nthe composer in  the title. W e use only the first fifteen  \nseconds of t he first movem ent, so t hat the test samples \nare h ighly likely to  remain in the stated key for the entire \ndurat ion of t he sam ple. \nIn order to facilitate th e co mpariso n of audio key \nfinding from  symbolic and audi o data, we started with \nMIDI sam ples from  www.classicalarchieves.com , and \nused t he Winamp software wi th a sam pling rat e of 44.1 \nkHz to render M IDI files to audio (wave form at). We \ntested three different system s on the sam e pieces.  The \nfirst system  applied the CEG alg orithm to MIDI files, \nthe second appl ied the CEG al gorithm to pitch class \ndistributions generat ed by peak det ection, and t he third \napplied the CEG algorithm to pitch class distributions \ngenerated by fuzzy analysis. Each system  returned a key \nanswer every  0.37 seconds and t he answers are \nclassified into five cate gories: correct, dom inant, \nrelativ e, parallel, an d others.   \n4.1 Overall res ults \nThe correct rates of the three system s over tim e are \nshown in Figure 2. For th e 410 classical m usic pieces, \nthe co rrect rate u sing fuzzy an alysis is consisten tly \nhigher than that for the peak det ection m ethod, except  \nfor the first 0.37 seconds. The difference exceeds 10% \nfrom  5.55 seconds onwards. From  6.66 t o 12.95 seconds, \nthe resul ts of t he audi o key  finding system with fuzzy  \nanalysis perform  alm ost as well as that for MIDI key \nfinding. \n \n300   \n \n5.5 se c 12.95  sec 6.66 sec\n Figure 2 . Comparison of overal l key finding resul ts. \n \n(a) 5 seconds into the pieces \n \n(b) 10 seconds into the pieces \n (c) 15 seconds into the pieces \n \nFigure 3 . Detailed  analysis o f overall resu lts. \nThe det ailed anal yses of t he resul ts at 5, 10 and 15 \nseconds are shown i n Figures 3(a), (b), and (c) \nrespectively. In Figure 3, the results are classified into \nfive categories. Notice that m ost of the closer-key errors \nfor audio key finding were due t o the mislabelling of t he \npieces as being in the dom inant key (a perfect fifth \nabove the correct  one). The fuzzy  anal ysis techni que \nimproves the correct  rate by reduci ng the closer-key  \nanswers in the Dom inant and the Others categories. The \naudio key finding m ethods (bot h peak det ection and \nfuzzy  analy sis) suffer m ore from  parallel key  errors, \nwhile the sy mbolic (M IDI) key  finding suffers m ore \nfrom  relative key  errors. These results are probably  due \nto the fact th at relativ e keys share the sam e pitch classes \nas the correct  one, whi le the audi o dom inant and paral lel \nkey errors m ost likely  result from  incorrect weight \ndistributions i n the pitch classes. The other important observat ion is that symbolic key \nfinding suffers m ost from  erro rs in the Others category \n15 seconds into the pieces, as shown in Figure 3(c). One \nexplanation for t his coul d be t hat symbolic key finding \nis distracted by extrane ous i nform ation such as \naccidentals, while the am plitude and frequency \ncharact eristics of m usical audi o signals const rain audio \nkey finding resul ts to mostly the closer key s.  \nThe resulting maximum correct percentage, average \ncorrect percentage, and m edian correct percentage for \nkey identificatio n by the three system s are sum marized \nin Table 1. Notice that the fuzzy analysis technique \nsignificantly improves t he peak det ection resul ts, \nespecially in  term s of the average correct percentage \nand m edian correct percentage. \nTable 1. Sum mary of overal l resul ts. \n MIDIAudio \n(peak detection) Audio \n(fuzzy  analy sis)\nMax correct \npercentage (%)80.34 70.17 75.25 \nAverage correct\npercentage (%)73.91 62.23 69.81 \nMedian correct\npercentage (%)72.97 61.98 70.22 \n4.2 Res ults sorted by stylistic period  \nWe classify our test data according to the stylistic \nperiods defi ned by  www.cla ssica larchives.co m: Baroque \n(Bach and Vivaldi), Classical  (Hay dn and Mozart), Late \nClassical and Earl y Romantic (Beethoven and Schubert ), \nRomantic (Chopin, M endelssohn, and Schum ann), Lat e \nRomantic (B rahm s and Tchai kovsky ), and \nContemporary  (Copland, Gershwi n, and Shost akovich).  \n    The key finding results  for 95 pieces by Bach and \nVivaldi (concert os) are shown i n Figure 4. The audio \nkey finding system s perform  as well as, som etimes even \nsupercedi ng, t he MIDI key  finding resul ts in the first 5 \nseconds. The resul ts for t he peak det ection m ethod drop \nafter 6.66 seconds. In cont rast, the correct  rate of the \nfuzzy analysis technique rem ains comparable to that fo r \nMIDI. The correct rate for th e fuzzy analysis technique \nis more than 20% hi gher t han the peak det ection m ethod \nfrom  10 seconds t o 15 seconds. \n \n6.66 sec 10 sec \n20%\n Figure 4 . Key finding results for 95 Baroque pieces. \nThe key finding results fo r 115 pieces by Haydn and \nMozart (symphoni es) are shown i n Figure 5. The test \nsamples from  the classical period are the only cases \nwhere t he audi o key  finding sy stems outperform  that for \n \n301   \n \nMIDI. No te th at th e system  with fuzzy analysis has a \nhigher correct rate than that  with  peak detectio n from \n5.55  t o 15 seconds.   \n5.55 s ec 11.1sec \nFigure 5 . Key finding results for 115 classical pieces by \nHaydn and M ozart. \nAt 11.1 seconds, we observe t he largest  difference \nbetween M IDI and audi o key  finding. Fi gure 6 present s \nthe detailed breakdown of the key  finding resul ts at 11.1 \nseconds. The fi gure shows t hat most of the MIDI errors \nare due to assig nments to  the dominant key. It is \ndifficult to judge from  the sum mary statistics whet her \nsome of the pieces actually change to their dom inant \nkeys at this tim e, a lik ely scenario. However, the \nsymbolic key  finding sy stem produces m ore errors i n \nthe Others cat egory  than the audi o key  finding sy stems.  \n \nFigure 6 . Detailed analysis of key  finding resul ts \nfor 115 classical pieces  at 11.1 seconds. \nThe results o f Late Classical-Early Ro mantic, \nRomantic, an d Late Ro mantic are p resented in Figure \n7(a), (b), and (c) respect ively. Compared t o the Baroque \nand Classical periods, the correct rates are significantly \ndiminished for exam ples from  each of these later \nperiods. The shape of the correct percentage line reflects \nthe less structured m usic style. For exam ple, in \nRomantic p eriod, the resu lts fo r all system s start with  \nlower correct rates and gradually increase over time. In \nthe Late Romantic p eriod, the resu lts also  start with  a \nlower co rrect rate, th en increase significantly within \n2.59 seconds, but drop agai n towards 15 seconds. The \nresults im ply that in the Romantic period, the music may \nstart with  a key other than the one stated , while in the \nLate Romantic period, the music changes qui ckly to \nother key s. \nFigure 8 shows the key finding results for 29 pieces \nby Copland, Gershwi n and Shost akovich, t wentieth \ncentury classical com posers. Observe that the resu lts in  \nFigures 7(a), (b), (c), and 8 show hi gher correct  rates for \nthe fuzzy  techni que t han the peak det ection method.  In \neach case, the fuzzy technique resulted in correct rates closer or equal  to the MIDI results. In Figure 7(a), the \naudio resul ts decrease at  3.7 seconds, but  fuzzy  anal ysis \nhas a benefi cial effect  on audi o key  finding bet ween \n4.81 and 15 seconds. For music from  the Romantic \nperiod (Figure 7(b)), the fuzzy analysis results are \ncomparabl e to that for sy mbolic key finding between \n4.44 and 9.25 seconds. For m usic from  the Late \nRomantic period (Fi gure 7(c)), M IDI and audi o key  \nfinding perform  similarly , with  results that are difficult \nto verify o bjectiv ely d ue to the tonal complexity of the \npieces. In Figure 8, the fuzzy analysis technique obtains \nbetter resul ts than the peak det ection method from  2.22 \nseconds to 8.51 seconds, whi le the MIDI resul ts are \nconsi stently better.  This coul d be due i n part  to pitch \nspelling errors, as the pitch spel ling techni que gi ves \npriority to  pitches in the sam e key (as does the periodic \ncleanup procedure). \n(a) Late Classical and Early Rom antic: 87 pieces by \nBeethoven and Schubert  \n3.7 s ec 4.81 s ec\n \n(b) Rom antic: 50 pieces by Chopin, Mendelssohn, and \nSchum ann \n4.44 s ec9.25 s ec \n \n(c) Late Rom antic: 34 pieces  by  Brahm s and \nTchaikovsky\n2.59 s ec 8.88 sec \n \nFigure 7.  Key finding resul ts for l ate classical and \nromantic works. \n \n302   \n \n \n2.22 s ec 8.51 s ec \nFigure 8 . Key finding resul ts for 29 C ontemporary  \npieces by Copland, Gers hwin and Shostakovich. \n5 CONCLUSIONS  \nWe have present ed a fuzzy  anal ysis techni que for pitch \nclass determ ination to im prove the accuracy of key \nfinding for pol yphoni c audi o. W e eval uate the techni que \nby com paring the resul ts of sy mbolic (MIDI) key finding, \naudio key  finding wi th peak det ection, and audi o key  \nfinding with fuzzy analysis. We showed that the fuzzy \nanalysis techni que was superi or to a si mple peak \ndetection pol icy, increasi ng the percent age of correct  key \nidentifications by  12.18% on average. In fact, the \npercentage correct for the fuzzy analysis technique \nmatched that of symbolic key finding 41.73% of t he time. \nREFERENCES \n[1] Góm ez, E., “Tonal  Descri ption of Pol yphoni c Audi o \nfor Music Content Processi ng”, INFOR MS Journal  \non C omputing, to appear. \n[2] Gómez, E. Herrera, P., “Estim ating The To nality Of \nPolyphonic Audio Files: C ognitive Versus Machine \nLearni ng M odelling St rategies”, ISM IR 2004 – 5th \nInternational Conference on Music Inform ation \nRetrieval, 2004. \n[3] Izmirli, O. and Bilg en, S., “A Mo del for To nal \nContext Time Course C alculation from  Acoust ical \nInput ,” Journal  of New M usic Research, Vol . 25(3), \n1996. \n[4] Pauws, S., “M usical Key  Ext raction from  Audi o”, \nISMIR 2004 – 5th International Conference on Music \nInform ation R etrieval, 2004. \n[5] Mitra, San jit K., Dig ital Signal Processin g: A \nComputer B ased Approach, 2nd Edition. McGraw-\nHill, 2001. \n[6] Chew, E. Toward s a Math ematical Mo del of Tonality. \nDoct oral dissertation, Depart ment of Operat ions \nResearch, Massachusetts Institute of Technology, \nCambridge, M A, 2000. \n[7] Chew, E., “Mo deling To nality: Applicatio ns to \nMusic Cognition”, Proceedings of the 23rd Annual \nConference of the Co gnitive Scien ce So ciety, \nEdinburgh, Scot land, 2001. [8] Krum hansl , C.L., Quant ifying Tonal  Hierarchi es and \nKey Distances. Cognitive Foundations of Musical \nPitch, chapt er 2, 16-49, 1990. \n[9] Temperley , D., “W hat’s Key  for Key? The \nKrum hansl -Schm uckler Key -Finding Algorithm \nReconsi dered,” M usic Percept ion, 17(1), 65-100, \n1999. \n[10] Chuan, C . H. and C hew, E., “Polyphoni c Audi o Key \nFinding Usi ng the Spi ral Array  CEG Algorithm”, \nProceedings of the IEEE In ternational Conference on \nMultim edia and Expo, Am sterdam , The Netherlands, \n2005. \n[11] Chew, E. and C hen, Y. C., “Mapping MIDI to the \nSpiral Array: Disam biguating Pitch  Spellings”, H. K. \nBhargava and Nong Ye (Eds.), C omputational \nModeling and Probl em Solving in the Net worked \nWorld, Kluwer, pp.259-275. Proceedings of the 8th \nINFOR MS Computer Soci ety Conference, ICS2003, \nChandler, AZ, Jan 8-10, 2003. \n[12] Chew, E. and Chen, Y. C., Real-Tim e Pitch Spelling \nUsing the Spi ral Array . Computer Music Journal . \n29:2, Sum mer 2005. \n[13] Chew, E. “Slicin g it all ways: m athematical m odels \nfor tonal induct ion, approxi mation and segm entation \nusing the Spiral Array ”, INFOR MS Journal  on \nComputing, to appear. \n \n \n303"
    },
    {
        "title": "Using a Pitch Detector for Onset Detection.",
        "author": [
            "Nick Collins"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417309",
        "url": "https://doi.org/10.5281/zenodo.1417309",
        "ee": "https://zenodo.org/records/1417309/files/Collins05.pdf",
        "abstract": "A segmentation strategy is explored for monophonic instrumental pitched non-percussive material (PNP) which proceeds from the assertion that human-like event analysis can be founded on a notion of stable pitch percept. A constant-Q pitch detector following the work of Brown and Puckette provides pitch tracks which are post processed in such a way as to identify likely transitions between notes. A core part of this preparation of the pitch detector signal is an algorithm for vibrato suppression. An evaluation task is undertaken on slow attack and high vibrato PNP source files with human annotated onsets, exemplars of a difficult case in monophonic source segmentation. The pitch track onset detection algorithm shows an improvement over the previous best performing algorithm from a recent comparison study of onset detectors. Whilst further timbral cues must play a part in a general solution, the method shows promise as a component of a note event analysis system. Keywords: onset detection, pitch detection, segmentation 1",
        "zenodo_id": 1417309,
        "dblp_key": "conf/ismir/Collins05",
        "keywords": [
            "segmentation strategy",
            "monophonic instrumental pitched non-percussive material",
            "human-like event analysis",
            "pitch percept",
            "constant-Q pitch detector",
            "post processing",
            "note transitions",
            "vibrato suppression",
            "evaluation task",
            "human annotated onsets"
        ],
        "content": "USING A PITCH DETECTOR FOR ONSET DETECTION\nNick Collins\nUniversity of Cambridge\nCentre for Music and Science\n11 West Road, Cambridge, CB3 9DP, UK\nnc272@cam.ac.uk\nABSTRACT\nA segmentation strategy is explored for monophonic in-\nstrumental pitched non-percussive material (PNP) which\nproceeds from the assertion that human-like event analy-\nsis can be founded on a notion of stable pitch percept. A\nconstant-Q pitch detector following the work of Brown\nand Puckette provides pitch tracks which are post pro-\ncessed in such a way as to identify likely transitions be-\ntween notes. A core part of this preparation of the pitch\ndetectorsignalisanalgorithmforvibratosuppression. An\nevaluation task is undertaken on slow attack and high vi-\nbrato PNP source ﬁles with human annotated onsets, ex-\nemplarsofadifﬁcultcaseinmonophonicsourcesegmen-\ntation. Thepitchtrackonsetdetectionalgorithmshowsan\nimprovementoverthepreviousbestperformingalgorithm\nfromarecentcomparisonstudyofonsetdetectors. Whilst\nfurthertimbralcuesmustplayapartinageneralsolution,\nthemethodshowspromiseasacomponentofanoteevent\nanalysis system.\nKeywords: onset detection, pitch detection, segmenta-\ntion\n1 INTRODUCTION\nArecentpaper(Collins,2005)comparedanumberofmu-\nsicalonsetdetectionfunctionswithrespecttoonsetdetec-\ntionperformanceonsetsofnon-pitchedpercussive(NPP)\nand pitched non-percussive (PNP) sound ﬁles. Whilst\nmany algorithms performed successfully at the NPP task,\nwith few false positives for a large number of correct de-\ntections, the ability of the same algorithms to parse the\nPNP set was substantially reduced. The most successful\nattempt was that of the phase deviation algorithm (Bello\net al., 2004), which uses a measure of the change of in-\nstantaneous frequency. It was proposed that this suc-\ncess could be linked to the use of stable pitch cues as a\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonsegmentation feature, a tactic also highlighted by Tris-\ntan Jehan in his event analysis/synthesis work (Jehan,\n2004). Fundamentalfrequencytrailshavebeensegmenta-\ntion features in work by teams from IRCAM (Rossignol\net al., 1999b,a) and Universitat Pompeu Fabra (G ´omez\net al., 2003b,a). Whilst many signal attributes, particu-\nlarlytimbraldescriptors,maycontributetoonsetdetection\nand event parsing (Handel, 1995; Yost and Sheft, 1993;\nMoore,1997),theuseofacentralpitchperceptisinvesti-\ngated in this paper as one component of a plausible strat-\negy, and a signiﬁcant one for the source material tackled\nherein.\nIn this paper I attempt to explore the basis of an\nimproved onset detection algorithm for pitched material\nwhich uses the stability of a pitch percept as the deﬁning\nproperty of a sound event. In order to obtain a clean de-\ntection signal, the output of a pitch detection algorithm is\nprocessedinvariousways,includingbythesuppressionof\nvibrato,followingRossignoletal.(1999b). Thechoiceof\npitchdetectionalgorithmisopen,butthespeciﬁcdetector\nconsidered in this paper is Brown and Puckette’s constant\nQ transform pitch tracker (Brown and Puckette, 1993).\nThe material with which I am concerned provides the\nhardest case of monophonic onset detection, consisting\nof musical sounds with slow attacks and containing vi-\nbrato, such as the singing voice (Saitou et al., 2002). Vi-\nbratoassociatedfrequencyandamplitudemodulationpro-\nvidesproblemstotraditionalenergybasedonsetdetectors,\nwhich tend to record many false positives as they follow\nthe typically 4-7 Hz oscillation. For such material, the\nsought after performance is a segmentation as a human\nauditor would perceive sound events. Better than human\nlistenerperformance,aspossibleforsomehighspeedper-\ncussive sequences via non-real-time digital editing or by\nalgorithm (Collins, 2005) is unlikely.\nThe applications of such an algorithm are multifold.\nOnset detection is a frontend to beat induction algorithms\n(Klapuri et al., 2004), empowers segmentation for rhyth-\nmic analysis and event manipulation both online and of-\nﬂine (Jehan, 2004; Brossier et al., 2004), and provides a\nbasis for automatically collating event databases for com-\npositional and information retrieval applications (Rossig-\nnoletal.,1999b;Schwarz,2003). Extractionofnoteevent\nlocations from an audio signal is a necessary component\nofautomatictranscription,andthevibratosuppressionin-\nvestigated here may assist clear f0 estimation. For music\n100Figure 1: Overview of the algorithm\ninformation retrieval, the ’query by humming’ approach\nrequires the parsing of monophonic vocal melodies from\naudio signal alone.\n2 ALGORITHM OUTLINE\nFigure1givesanoverviewofthedetectionalgorithmand\nthe associated signal features based on the extracted fun-\ndamental frequency f0. The following subsections will\naddress successive stages of the onset detector.\n2.1 Pitch Detection\nBrown and Puckette (1993) describe an efﬁcient FFT\nbased pitch detection algorithm which cross correlates a\nharmonictemplatewithaconstantQspectruminasearch\nfor the best ﬁtting fundamental frequency f0. The form\nofthetemplateisdevisedsoastominimiseoctaveerrors;\nthe template consists of the ﬁrst 11 harmonics, weighted\nfrom1.0to0.6. Afurtherstageevaluatesphasechangein\nthe winning FFT bin to get a more accurate value for the\npitch unconstrained by the limited bin resolution. Since\nthefulldetailsaregivenintheirpapers(BrownandPuck-\nette, 1992, 1993) and my implementation follows that\nwork I shall avoid a fuller discussion of this pitch detec-\ntion method. Alternative pitch detection algorithms may\neasily be placed as front-ends to the analysis system now\nto be described.\nThe4096pointFFTdrivingthepitchdetectorwasrun\nwithastepsizeof512samples,foraframerateofaround\n86 Hz (all the audio signals involved had 44100Hz sam-\nplingrate). Thepitchdetectoroutputwastakenfrom150-\n2000Hz, with values outside this range shifted by octave\nsteps into this compass, and values outside 22050Hz sent\nto 1 Hz, where they are easily cleaned up with the algo-\nFigure 2: The upper f0track is cleaned up and the result\nis the lower track\nrithm next described.\nA post processing stage was added to clean up some\nsmall blips in the signal, consisting of momentary oc-\ntave errors and rogue outliers. Whilst a jump to an oc-\ntave which is then maintained could indicate a true oc-\ntave leap in the music, some obvious short-term octave\nerrors were seen, with lengths of one or two frames. The\noriginal Brown/Puckette algorithm also occasionally cre-\nated some strange values during otherwise relatively sta-\nble held pitches. The pseudocode in ﬁgure 3 reveals the\ntactic employed to clean up these short-term errors. The\nMATLABindexingconventionofcountingfrom1isused.\nThetwotestscheckagainsttheratioofanequaltempered\nsemitone.\nFigure2demonstratestheapplicationofthealgorithm\non a signal which has out of bound pitches and instanta-\nneous errors against the general trend.\nIt is convenient to transform the fundamental fre-\nquency track to pitch in semitones prior to vibrato sup-\npression, as a musically normalised representation. An\narbitraryreferencepointisselectedsuchthat0Hzistrans-\nformed to 0 semitones.\np= 12 ∗log2((f+ 440) /440) (1)\n2.2 Vibrato Suppression\nThef0track is perturbed by vibrato, and this can be at-\ntributed as the chief cause of noise on that signal disrupt-\ningitsuseinsegmentation. Rossignoletal.(1999b)noted\nthis in their event segmentation paper, and sketch a vi-\nbrato suppression algorithm. Herrera and Bonada (1998)\nhave also outlined both frequency domain and time do-\nmain vibrato suppression methods within the context of\nthe SMS (Spectral Modeling Synthesis) framework, us-\ning an FFT to isolate 6-7Hz vibrato by analysing peaks\nin the frequency domain before suppression and IFFT re-\nsynthesis, and in the time domain, a 10Hz high pass ﬁl-\nter on a 200mS window. These methods require the be-\nfore application identiﬁcation of the mean around which\na vibrato ﬂuctuates, and utilise ﬁxed windows. Rossignol\n101postprocessing(arg input)\nfor jj= 2 to 7 {\nfor ii= 1 to (length(input)-jj) {\ntestratio= input(ii)/input(ii+jj);\nif testratio <1.059 AND testratio >0.945 {\nfor kk=1 to (jj-1) {\nmid = (input(ii)+input(ii+jj))*0.5;\ntestratio2= input(ii+kk)/mid;\nif testratio2 >1.059 OR testratio <0.945\ninput(kk) = mid;\n}\n}\n}\n}\noutput=input;\nFigure 3: Pseudocode for theoutlier removal algorithm\net al. (1999a) also expands upon a selection of methods\nfor suppression; I followed the ‘minima-maxima detec-\ntion’ method as in common with Rossignol et al. (1999b)\nas the most plausible for my purposes.\nAttemptstoimplementtheRossignoletal.(1999b)al-\ngorithm, however, were somewhat thwarted by the ques-\ntionofthebestwindowingstrategytouse;theiralgorithm\nis underspeciﬁed. A vibrato suppression algorithm is de-\nscribedherewhichisinspiredbytheirworkbutmakesex-\nplicithowthesearchforregionsofvibratowilltakeplace,\nand uses some variation in the criteria for a vibrato detec-\ntion and substituting value, along with variable window\nsize to encompass vibrato regions.\nVibratoremovalproceedsinwindowsof300mS,with\na step size of 100mS. If the difference of the maximum\nandminimumvalueoftheinputwithinthiswindowisless\nthan 1.5 semitones, a search for vibrato ensues. All max-\nima and minima within the (open) window range form a\nlist of extrema. Lists of differences in time and in ampli-\ntude of the extrema are taken, and the variances of these\nlists calculated. Note that this is different to Rossignol\netal.(1999b)wherethemaximaandminimalistsarecon-\nsidered separately. The quantity pextrema is calculated\nastheproportionofthetimedifferencesbetweenextrema\nthat fall within the vibrato range of 0.025 to 0.175 sec-\nonds, corresponding to 2.86 to 20 Hz frequency modula-\ntion. Avibratoisdetectedwhen pextrema islargeandthe\nvariances are sufﬁciently small.\nGiven a vibrato detected in a window, the window is\nnowgraduallyextendedsoastotakeinthewholeduration\nofthisvibrato;thisguaranteesthatthecorrectionswillnot\nbe piecemeal, giving rise to some erroneous ﬂuctuations.\nA number of conditions are checked as the window is in-\ncrementally widened, so as not to confuse a vibrato with\na jump to a new pitch. The mean of the input has been\nprecalculatedin21framesegmentscentredoneachpoint.\nThis mean allows a guide as to the centre point of any\nvibrato oscillation; if this mean changes during the win-\ndow extension, it is likely that a new note event has com-\nmenced. This test was particularly important in cases of\nsingingwherethemagnitudeofvibratoononetonecould\nencompassthesmallervibratomagnitudeonasucceeding\nFigure 4: Vibrato suppression for an ascending arpeg-\ngiatedviolinsignal. TheFFTframesareontheabscissae,\npitch in semitones or a 0/1 ﬂagfor the ordinate\ntone. Secondly, the window is only extended where no\nvalue departs more than a semitone from the mean of the\nextrema list. The correction is applied, replacing all val-\nuesinthewindowwiththemeanoftheextremalist. After\nsuppressingavibrato,thesearchforvibratorecommences\nwith the window positioned at the next frame unaffected\nby the changes.\nFigure4showsanexamplewherethevibratosuppres-\nsionworkseffectively. Thetoppartoftheﬁgureshowsthe\ninput, the centre marks areas where vibrato was detected\nand shows the length of the windows after extension, and\nthe bottom shows the vibrato suppressed output. Figure\n5 shows a less clean case where the suppression does not\nremove all the frequency modulation. The heuristical al-\ngorithm given in this paper could likely be extended via\nsuch tactics as a cross correlation search for matches to\nsinusoidal variation exhaustively through appropriate fre-\nquenciesorbyfurtherrulesbasedonastudyofinstrumen-\ntal vibrato. It works well enough, however, for evaluation\npurposes herein.\n102Figure 5: Vibrato suppression for a solo soprano signal.\nThe FFT frames are on the abscissae, pitch in semitones\nor a 0/1 ﬂag for the ordinate\nFigure 6: The upper cleaned and vibrato suppressed pitch\ntrack is converted to a detection function\n2.3 Assessing Peaks of Instability\nGiven the vibrato suppressed pitch tracks, note events\nmust be distinguished by jumps of pitch. A procedure is\napplied to rate the strength of changes in the pitch track p\nover time.\nd f(i) =8/summationdisplay\nj=1min ( |p(i)−p(i+j)|,2)(2)\nThe minoperator disregards the size of changes\ngreater than a tone to avoid overly biasing the output de-\ntectionfunction d fbasedonthesizeofleapbetweennotes\ninvolved. Figure 6 demonstrates d ffor a soprano signal.\nBecausechangesaresoughtout,cuesformultiplenote\neventsinarowofthesamepitcharethemostdifﬁcultcase\nto spot (particularly questionable are the case of smooth\ntransitions between same pitch notes- how little energy\ndrop can a player get away with?). It is assumed that\nnoteonsetsshouldshowsomeslightperturbationinpitch,\nthough the pitch integration area is around 90mS in theFFT. The pitch track test may have to be combined with\nother features, to be described next. However, one inter-\nesting case, that is not particularly well dealt with by the\nvibrato suppression stage at the present time, is that the\nendandrestartofavibratoitselfmayindicateatransition\nbetween successive notes.\n2.4 Correction for Signal Power\nBecausethedetectionfunctiondidnottakeaccountofsig-\nnal power, onsets would often appear at the very tails of\nevents,foreventswhichendinsilence. Tocounteractthis,\namultiplierwasintroducedbasedonthesignalpowerim-\nmediately following a given frame. A basic temporal in-\ntegration was carried out, taking a weighted sum over 10\nframes, and compressing to 1 for all reasonably large val-\nues. Smallvaluesunder0.01ofthemaximumpowerwere\nleft unaffected and downweighted troublesome points in\nthe pitch detector based detection function.\n2.5 Peak Picking\nA detection function must yield onset locations via some\npeakpickingprocess. Belloetal.(2004)provideanadap-\ntive peak picking algorithm based on a median ﬁlter on a\nmovingwindow. Theirpeakpickerwasusedasacommon\nstage in the evaluation, following (Collins, 2005; Bello\net al., 2004), and the algorithm is not discussed further\nhere.\n3 EVALUATION\n3.1 Procedure\nAn evaluation of the pitch detection based onset detec-\ntor was carried out using the same methodology as pre-\nviouscomparativestudiesofonsetdetectioneffectiveness\n(Collins,2005;Belloetal.,2004). Pitchednon-percussive\n(PNP) soundﬁles originally prepared and annotated by\nJuan Bello formed the test set. 11 source ﬁles were se-\nlected,containing129onsets,comprisingslowattackand\nhigh vibrato sounds from strings and voices. The on-\nsets were sparse in relatively long sound ﬁles, providing\na great challenge; with amplitude modulation associated\nwith vibrato, it is unsurprising that loudness based detec-\ntion functions fared so poorly in Collins (2005). The tol-\nerance for matches between algorithm and hand-marked\nonsets was set at a very tolerant 100mS, though this win-\ndow was small compared to the average distance between\nnote events.\nThepitchtrackonsetdetectionfunctionwascompared\nto the phase deviation detection function with a common\nadaptive peak picking stage. The peak picker has a pa-\nrameter δwhich acts like an adaptive threshold; this was\nvaried between -0.1 and 0.53 in steps of 0.01, giving 64\nruns on the test set for each detection function. A Re-\nceiver Operating Characteristics curve was drawn out as\ndelta is varied. This ROC curve is given in ﬁgure 7. The\nclosestpointstothetopleftcornerindicatethebetterper-\nformance,withmanycorrectdetectionsforfewfalsepos-\nitives.\n103Table 1: NPP test set comparison of detection functions with Bello et al. (2004) peakpicker\ndetection function score (eqn 4) CDROnsets Detected False Positives bestδ\n1. pitch track detection function 42.6 -17 58.1 36.4 0.13\n2. phase deviation (Bello etal., 2004) 32.8 -36.4 45.0 37.0 0.13\nFigure 7: ROC curve of false positives against correct de-\ntections comparing phase deviation and pitch track onset\ndetector functions over varying δ\nResults for the best δfor each algorithm are given in\ntable1withratingswithrespecttotwomeasuresofperfor-\nmance. Liuetal.(2003)’sCorrectDetectionRatio(CDR)\nis described by the equation:\nCDR =total −missing −spurious\ntotal∗100% (3)\nbut is not constrained, however, to return values between\n0-100. I also introduce therefore an evaluation formula\nfromDixon (2001), originally used for the assessment\nof beat tracking algorithm performance as an alternative\nscoringmechanism,combiningmatches m,falsepositives\nF+(spurious) and false negatives F−(missing).\nscore =m\nm+F−+F+∗100% (4)\nThe denominator includes the term for the number of on-\nsetsinthetrial nasm+F−. Thesemeasuresarethesame\nas in (Collins, 2005).\n3.2 Discussion\nAsmalladvanceisshownbythepitchdetectionbasedon-\nset detector, its performance being marginally better than\nthe phase deviation and by extension all the energy based\ndetection functions considered in (Collins, 2005). The\nsuccess of a pitch detection cue gives corroborative evi-\ndencethatnoteeventsdeﬁnedbystablepitchperceptarea\nplausible segmentation strategy. The fact that vibrato had\nto be suppressed for effective performance shows the im-\nportance of higher level feature extraction in human seg-\nmentation. As noted above, the onset and offset of a vi-\nbrato may be a feature that helps to segment successivenotes of the same pitch. It might even be speculated that\nthe appearance of vibrato in long notes can be linked to\na human desire for stimulation over time, for the con-\nfound given by vibrato and associated amplitude modu-\nlation (often at 4-7 Hz) is comparable to new amplitude\ncued events at the same rate. The central pitch around\nwhichthevibratooscillatesmaintainstheidentityofasin-\ngle note event.\nVariousproblemswiththeevaluationtaskwerenoted,\nwhich may have underrated the performance of the pitch\ndetector. First, the annotations were at their most subjec-\ntive for this type of note event; as Leveau et al. (2004)\nnote, the annotation task involves some variability in\ndecisions between human experts, particularly for com-\nplexpolyphonicmusicandinstrumentswithslowattacks.\nHowever, at the time of writing, the Bello database pro-\nvided a larger test set (11 as opposed to 5 ﬁles), and the\nLeveau database could not be made to function properly\nwithin MATLAB.\nHuman pitch perception shows different time resolu-\ntioncapabilitiestothecomputerpitchtrackerusedherein.\nWhilst the qualitative agreement of onset locations with\nthe hand marked ones was much more impressive for the\nstable pitch detector than the phase deviation (for exam-\nple, ﬁgure 8), these would often be early with respect to\nthe human marked positions (though could also appear\nlate). To compensate somewhat, a delay of 7 frames had\nbeen introduced in the detection function for the compar-\nison test. The time resolution of the new onset detection\nalgorithmisdependentonthelowertimeresolutionofthe\npitch detection algorithm, with a 4096 point FFT (pitch\ndetection accuracy degrades with a shorter window); the\nphasedeviationwasmuchlesssusceptibletothisproblem,\nbased on a 1024 point FFT. Localisation could perhaps\nbe improved by zero padded FFTs for the pitch detector,\nparallel time domain autocorrelation and timbrally mo-\ntivated onset detection (differentiating transient regions\nfrom smooth wherever possible) and remains an area for\nfurther investigation.\nThe selection of the test set also played a role. When\nonsets are sparse, false positives count for proportionally\nmoreovertherun. Acombinationofsoundﬁlesrequiring\nmany onsets to be detected and those with sparse onsets\nis a difﬁcult combination, for onset detectors built to risk\nmore will score very poorly on the sparse regions. It can\nbe speculated that additional contextual clues due to tim-\nbreandmusicalconventionareutilisedbyhumanlisteners\nto focus their event detection strategy. An onset detection\nalgorithm which performed well for both NPP and PNP\nmaterialwouldmostlikelyrequiresomeswitchingmech-\nanism based on the recognition of instrument and playing\nstyle. The evocation of a pitch percept and the detection\nof vibrato cues may provide knowledge for deciding the\nevent segmentation tactic.\n104Figure8: Comparisonofpitchdetector(middle)andpitch\ndeviation (bottom) on a violin signal. The top shows the\nsource signal with onsets marked- those on the top line\nshowthehumanannotation,abovethemiddlethosedueto\nthepitchdetectoralgorithmandbelowthephasedeviation\nFor the determination, given arbitrary material, of the\nbest algorithm to use, a computer program might assess\nthestabilityofpitchcues(amountofﬂuctuation)andgen-\neral inharmonicity to decide if pitched material is being\ntargeted. Attack time cues through the ﬁle may distin-\nguish whether to apply a combined pitch and amplitude\nalgorithm or a pure pitch algorithm for slow attacks, and\nhow to deal with confounds from the recognition of the\nspeciﬁc shape of vibrato or other playing conventions (on\nwhich much further work might be done).\nIn testing the algorithm, it was found that the quality\nof pitch detection tracks was worse for lower register in-\nstruments, as for double bass or bass voice. This could\nbe traced to inadequacies in the constant Q pitch detec-\ntor for tracking fundamentals below around 150Hz. False\nmatches to higher harmonics could skew the pitch tracks\nand the algorithm consistently gave the worst detection\nscores for such cases. Leaving these troublesome sound\nﬁlesoutofthetestsetledtomuchimprovedperformance.\nOnareducedtestsetof6ﬁles,thealgorithmthenachieved\n58.7%correctdetectionsfor21.4%falsepositives(Dixon\nscore of 48.3, CDR 1.3) as opposed to 45.3% correct to\n38.2% false positives (Dixon score 32.8, CDR -37.3) for\nthe phase deviation.\n4 CONCLUSIONS\nIn this paper, a pitch detection algorithm was adapted for\nan onset detection task on pitched non-percussive source\nmaterial. This often slow attacking and vibrato-ridden\nmonophonic music provides a challenging case for event\nsegmentation. A very high correct identiﬁcation to low\nfalse positive rate is yet to be exhibited commensurate\nwith the success rates on the easier NPP task, but the tac-\ntic introduced shows some promise for the PNP task. It is\nthe most promising of detection functions assessed so far,\nparticularly by qualitative comparison of results from the\nnew detector with that of thephase deviation algorithm.Whilstthepitchdiscriminationcapabilitiesofhumans\nare much more reﬁned than a semitone, a semitone has\nbeen used above as a practical working value for the size\nof pitch changes, as opposed to vibrato. In fact, the or-\nder of vibrato can approach that of note events, and some\ntighter heuristics for the vibrato suppression which take\nintoaccountthenatureofthevibratoperceptmayneedto\nbe applied.\nGeneral improvements may arise from investigating\ncomputational auditory models, for the goal on such mu-\nsical material as targeted in this paper is to match a hu-\nman auditor’s segmentation. A better pitch detection al-\ngorithm as a frontend to event segmentation may be one\nmodeledmorethoroughlyonneuralcodingofperiodicity,\nwith realistic pitch reaction time and stability characteris-\ntics. For example, a perceptually plausible pitch detector\nis proposed by Slaney and Lyon (1990).\nIt is likely that human auditors use instrument recog-\nnition cues to decide on a segmentation strategy. Prior\nknowledge of instrument timbre and associated playing\nconventions provide situations where human segmenta-\ntion may continue to out perform machine in the near fu-\nture.\nACKNOWLEDGEMENTS\nThanks are due to Juan Bello for providing the evaluation\ntest set, and the helpful comments of four anonymous IS-\nMIR reviewers.\nREFERENCES\nJ. P. Bello, L. Daudet, S. Abdallah, C. Duxbury,\nM. Davies, and S. B. Sandler. A tutorial on onset de-\ntection in music signals. IEEE Transactions on Speech\nand Audio Processing , 2004.\nP. Brossier, J. P. Bello, and M. D. Plumbley. Real-time\ntemporalsegmentationofnoteobjectsinmusicsignals.\nInProc. Int. Computer Music Conference , 2004.\nJ. C. Brown and M. S. Puckette. An efﬁcient algorithm\nforthecalculationofaconstantQtransform. J.Acoust.\nSoc. Am., 92(5):2698–701, November 1992.\nJ. C. Brown and M. S. Puckette. A high-resolution\nfundamental frequency determination based on phase\nchanges of the Fourier transform. J. Acoust. Soc. Am. ,\n94(2):662–7, 1993.\nN. Collins. A comparison of sound onset detection algo-\nrithms with emphasis on psychoacoustically motivated\ndetectionfunctions.In AESConvention118 ,Barcelona,\nMay 28-31 2005.\nS. Dixon. Automatic extraction of tempo and beat from\nexpressive performances. Journal of New Music Re-\nsearch, 30(1):39–58, 2001.\nE. G´omez, M. Grachten, X. Amatriain, and J. Arcos.\nMelodiccharacterizationofmonophonicrecordingsfor\nexpressive tempo transformations. In Proceedings of\nStockholm Music Acoustics Conference 2003 , Stock-\nholm, Sweden, 2003a.\n105E. G´omez, A. Klapuri, and B. Meudic. Melody descrip-\ntion and extraction in the context of music content pro-\ncessing.JournalofNewMusicResearch ,32(1),2003b.\nS. Handel. Timbre perception and auditory object identi-\nﬁcation. In Moore (1995), pages 425–61.\nP. Herrera and J. Bonada. Vibrato extraction and param-\neterization in the spectral modeling synthesis frame-\nwork. InProc.DigitalAudioEffectsWorkshop(DAFx) ,\nBarcelona, 1998.\nT.Jehan. Event-synchronousmusicanalysis/synthesis. In\nProc. Digital Audio Effects Workshop (DAFx) , Naples,\nItaly, Oct. 2004.\nA. P. Klapuri, A. J. Eronen, and J. T. Astola. Analysis\nof the meter of acoustic musical signals. IEEE Trans.\nSpeech and Audio Processing , forthcoming, 2004.\nP. Leveau, L. Daudet, and G. Richard. Methodology and\ntoolsfortheevaluationofautomaticonsetdetectional-\ngorithms in music. In Proc. Int. Symp. on Music Infor-\nmation Retrieval , 2004.\nR. Liu, N. Griffth, J. Walker, and P. Murphy. Time do-\nmain note average energy based music onset detection.\nInProceedings of the Stockholm Music Acoustics Con-\nference, Stockholm, Sweden, August2003.\nB. C. J. Moore, editor. Hearing. Academic Press, San\nDiego, CA, 1995.\nB. C. J. Moore. An Introduction to the Psychology of\nHearing. Academic Press, San Diego, CA, 1997.\nS. Rossignol, P. Depalle, J. Soumagne, X. Rodet, and\nJ. Collette. Vibrato: Detection, estimation, extraction\nand modiﬁcation. In Proc. Digital Audio Effects Work-\nshop (DAFx) , 1999a.\nS. Rossignol, X. Rodet, J. Soumagne, J.-L. Collette, and\nP. Depalle. Automatic characterisation of musical sig-\nnals: Feature extraction and temporal segmentation.\nJournal of New Music Research , 28(4):281–95, 1999b.\nT. Saitou, M. Unoki, and M. Akagi. Extraction of f0\ndynamic characteristics and development of f0 control\nmodel in singing voice. In Proc. of the 2002 Int. Conf.\non Auditory Display , Kyoto, Japan, July 2002.\nD. Schwarz. New developments in data-driven concate-\nnative sound synthesis. In Proc. Int. Computer Music\nConference , 2003.\nM.SlaneyandR.F.Lyon. Aperceptualpitchdetector. In\nProc. ICASSP , pages 357–60, 1990.\nW. A. Yost and S. Sheft. Auditory perception. In W. A.\nYost,A.N.Popper,andR.R.Fay,editors, HumanPsy-\nchophysics ,pages193–236.Springer,NewYork,1993.\n106"
    },
    {
        "title": "&quot;The Pain, the Pain&quot;: Modelling Music Information Behavior and the Songs We Hate.",
        "author": [
            "Sally Jo Cunningham",
            "J. Stephen Downie",
            "David Bainbridge 0001"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417209",
        "url": "https://doi.org/10.5281/zenodo.1417209",
        "ee": "https://zenodo.org/records/1417209/files/CunninghamDB05.pdf",
        "abstract": "The paper presents a grounded theory analysis of 395 user responses to the survey question, “What is the worst song ever?”  Important factors uncovered include: lyric quality, the “earworm” effect, voice quality, the influence of associated music videos, over-exposure, perceptions of pretentiousness, and associations with unpleasant personal experiences.",
        "zenodo_id": 1417209,
        "dblp_key": "conf/ismir/CunninghamDB05",
        "keywords": [
            "grounded theory analysis",
            "survey question",
            "worst song ever",
            "user responses",
            "lyric quality",
            "earworm effect",
            "voice quality",
            "associated music videos",
            "over-exposure",
            "perceptions of pretentiousness"
        ],
        "content": "“THE PAIN, THE PAIN”: MODELLING MUSIC INFORMATION \nBEHAVIOR AND THE SONGS WE HATE\nSally Jo Cunningham J. Stephen Downie David Bainbridge \nDept. of Computer Science \nUniversity of Waikato \nHamilton, New Zealand \nsallyjo@cs.waikato.ac.nz GSLIS \nUniversity of Illinois at \nUrbana-Champaign \njdownie@uiuc.edu Dept. of Computer Science \nUniversity of Waikato \nHamilton, New Zealand \ndavidb@cs.waikato.ac.nz \nABSTRACT \nThe paper presents a grounded theory analysis of 395 \nuser responses to the survey question, “What is the \nworst song ever?”  Important factors uncovered include: \nlyric quality, the “earworm” effect, voice quality, the \ninfluence of associated music videos, over-exposure, \nperceptions of pretentiousness, and associations with \nunpleasant personal experiences. \n \nKeywords: User study, music information behaviour, \nmusic recommender systems.  \n1 INTRODUCTION \nTo understand music information retrieval (MIR) tasks, \nparticularly music recommender systems, the issue of \nwhy we dislike a song can be as important as why we \nlike one. In this paper we focus on this issue, presenting \nresults collated from an on-line survey that asked re-\nspondents to provide details over song they did not like. \nUsing  a  grounded  theory  approach  to draw out si g-\nnificant features and characteristics, we discuss their \nimplications and relevance to MIR. \nMusic psychology research to date has primarily fo-\ncussed on music preference—music that individuals and \ngroups like. An earlier review of this literature [1] has \ndemonstrated that a deep understanding of music prefer-\nence to have practical value in suggesting features and \nfunctionality for MIR software, particularly for music \nrecommender systems. It would seem reasonable that an \nunderstanding of musical aversions could also be useful \nin informing MIR design, since the music seeker is si-\nmultaneously trying to locate desirable music and avoid \ndisagreeable music. Music dislikes, however, have been \nrelatively neglected as a subject of study. An exception \nis the work of artists Komar and Melamid, who based \nthe composition of a song predicted to be liked by fewer \nthan 200 people in the world on input from a survey of \nsong feature dislikes [2]. \n \n 2 DATA COLLECTION \nUsing Survey Builder,1 an online survey was constructed \nto gather descriptions of the music characteristics that \ndefine why people dislike particular pieces of music. The \nsurvey was posed as an opportunity to vote to determine \nthe worst song ever, and was modelled on a similarly \nthemed, postal survey organized by humorist and news-\npaper columnist Dave Barry [3]. Respondents were \nasked to enter the title and artist of the song that they \nthemselves particularly hated, and to explain why. The \nrespondents could also optionally enter their age, sex and \nnationality. The survey was publicized primarily by post-\ning notices in music-related newsgroups, online discus-\nsion forums, and mailing lists. \nThe responses analyzed in this paper were submitted \nfrom March 21 to April 17 2005. 395 usable responses \nwere collected. The majority of respondents were from \nEnglish-speaking countries, and this language bias is \nreflected in the songs nominated—the overwhelming \nmajority of which were North American and European, \nEnglish language songs. For respondents volunteering \ndemographic details, the number of male and females is \nnearly even (182 female, 177 male), and the bulk of \nrespondents are adults in the 20–45 years old  range \n(average age 32). \nFigure 1 shows this data displayed as a scatter graph, \nplotting the decade of the song against the age of the \nsubject (decade information for 12 of the songs could \nnot be determined). The graph shows a strong weighting \ntowards songs from the last five years (as many as from \nthe previous decade). Around the age of 50 and above, \nthe nomination of older songs becomes more noticeable. \n \nFigure 1. Decade of song versus age of subject. \n \nThe explanations detailing why respondents loathed \ntheir nominated song were analysed using a grounded \n                                                             \n1 http://chnm.gmu.edu/tools/ Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full \ncitation on the first page. \n© 2005 Queen Mary, University of London \n 190019101920193019401950196019701980199020002010\n1020304050607080Age of subjectDecade of song (with +/- 1% jitter)\n474   \n \n theory approach [4]. With this technique researchers \nattempt to approach the data without prior assumptions, \nand to generate theory from the data. \n3 ANALYSIS AND DISCUSSION \nApproximately 7% (30) of the survey respondents \nseemed to have difficulty in singling out particular fea-\ntures of a song to explain why they hated it. Some sim-\nply dismiss the nominated song as ‘horrible’, ‘indescrib-\nable’, ‘lamest song’; others clearly wrestle—\nunsuccessfully—with the problem of providing a rea-\nsoned, rational analysis of a visceral response:  \nI HATE this song [George Gershwin’s ‘Summer-\ntime’], it really irritates me. I know it's a 'classic' \nand all that and I generally like the Gershwins' \noutput. I can't find any aesthetic or logical reason \nwhy i've taken against this song but it just sets \nmy teeth on edge. \nThis section explores the factors contributing to dis-\nliking a song as suggested in the remaining 93% of sur-\nvey responses. \n3.1 Descriptive Data \nTable 1. Top-ranked explanatory terms. \nFeature Terms Descriptive Terms \nTerm  Response \nCount  (%) Term Response \nCount    (%) \nlyric  152 (38%) bad 48 (12%) \nmusic 67 (17%) annoy 34   (9%) \nsing 52 (13%) hate 32   (8%) \nvideo 38 (10%) really 31   (8%) \nvoice 37   (9%) inane 30   (8%) \nsinger 33   (8%) horrible 28   (7%) \nsound 30   (8%) stupid 27   (7%) \nrepetitive 22   (6%) worst 25   (6%) \ntune 21   (5%) awful 23   (6%) \nperform 18   (5%) crap 22   (6%) \nchorus 16   (4%) bore 18   (5%) \n \nIn Table 1 we present the top-ranked terms taken from \nthe “Explanation” field of the online survey form. These \ndata represent the number of responses which included \nthe given term root (i.e., the terms “lyric” and “lyrics” \nwere collapsed into one term category). The Feature \nTerms data highlight the most-frequently used terms that \nattempt to illustrate the components of the song under \ndiscussion. The Descriptive Terms data represent the \nmost-frequently used terms which give a sense of the \nrespondents’ feelings about the pieces. \n3.2 The Importance of Lyrics \nIssues surrounding the lyrics of a given song appear to \nbe the single most consistent factor in motivating the \nnomination of the song with 53% (209) of the responses \ncommenting on some aspect of the lyrics. 77 (19%) of \nrespondents provided lyric fragments to illustrate their \nexplanations. The two descriptive terms commonly used to describe the lyrics which jump out of the data based \nupon their frequency of use are “inane” and “stupid” \nwith 30 (8%) and 27 (7%) responses including these \nterms respectively. Beyond the poetic and stylistic mer-\nits of the lyrics in question, 41 (10%) respondents made \nexplicit negative comments about the underlying stories, \nideas and themes expressed in the lyrics.  \nOverly simple, repetitious lyrics (cited in 5 responses, \n9%) can be a factor in disliking a song; given this, it \nwould seem reasonable that a song with a more complex \nstory or message—requiring more complex lyrics—\nwould be viewed more favourably. This appears to be \nthe case only if the listener agrees with or enjoys the \nstory or message. One source of objection is overly sen-\ntimental or clichéd lyrics (“cheesy”, “corny”, “cloy-\ning”). Other storylines or messages that inspired dislike \ninclude sad stories (“Makes me cry just to think of this \nawful song.”); songs expressing misogynous sentiments \nand other biases (“The words are sexist and derogatory \nto almost everyone that breathes”, “because it was so \ndiscriminant against short people, and i am short”);  and \ncontradictory or unintelligible messages (“There's a line \nin the song that says \"it can't be taught,\" while the cho-\nrus says \"I can teach you, but I'd have to charge”). \n3.3 The Earworm Effect \n‘Earworm’ is a literal translation of the German Ohr-\nwurm, meaning a song that gets stuck in your head. 30 \nrespondents (7.5%) cited this common, but exceptionally \nirritating, phenomenon (“Oh god it's in my head...the \npain the pain!”). The earworm is not a recent phenome-\nnon; the first extensive description of this condition dates \nto 1876 [5]. \nEarworms are frequently prompted by hearing a \nsong, but they can also invade if one simply thinks \nabout it—for example, when filling out an online sur-\nvey: “I cant stand the bloody chorus. Now look! Its in \nmy head and I cant get it out”. Respondents note that \nearworms are ‘catchy’ and appealing, at least initially.  \nA song is more likely to be an earworm if it has one \nor more of the following characteristics [6, 7]: \n• the song is overly repetitive, either in tune, lyrics, or \nboth (“crude lyrics, repeated over and over and over \nagain in a mind-numbing manner”; “the repetitive \nlyrics over and over that creep inside your brain”). \n• the tune or the lyrics lack complexity—the song is \nmusically simplistic, or the lyrics are predictable and \nundemanding (“the tune never changes”; “no musi-\ncal variety”). Children’s music is particularly suscep-\ntible to becoming an earworm.  \n• the song contains incongruous or unexpected ele-\nments—for example, irregular beats, unpredicted \nmelodic patterns, or unusual effects. ‘Who let the \ndogs out’ is cited for its ‘woof, woof, woof’ chorus. \n• the song does not resolve, or the resolution is not as \npredicted by the listener; for example, one respon-\ndent nominated “Anything by Phil Colins or Gene-\nsis” because their albums frequently include “Tunes \nthat don't resolve properly, or when I expect.” The \nnominated song that provides the most extreme ex-\nample of this property is “The song that never ends” \n(Shari Lewis and Lamb Chop).  \n475   \n \n Note that some of these characteristics can be extracted \nfrom the music or lyrics to identify potential earworms. \nOne can imagine, for example, measuring the degree of \nrepetition in lyrics or melody, developing a metric for \nmusical complexity, or noting how the song ends. \n3.4 ‘The Voice’ \n110 responses (28%) cite aspects of the vocal elements \nof songs as a significant factor in disliking the nominated \nsong. Singing is characterized as ‘annoying’, ‘yowling’, \n‘monotonous’, and ‘whiny’. Again, sentimentality \n(‘sickly sweet’) comes under fire. Over-dramatic vocal \neffects draw rebuke: ‘relentless vocal gymnastics that \nspawned a generation of Pop Idol wannabes who com-\npensate for their inability to sing by turning one note into \n144!’  \n3.5 Music Videos \n37 respondents—over 9%—mentioned the nominated \nsong’s accompanying music video as a factor in why \nthey dislike the song. Most references simply dismiss the \nvideo (‘stupid’, ‘horrible’). More detailed references are \nto the overall stylistic effect of the video (“flashy”; “see-\ning a video with all the possible cliches (especially the \nslow-motion) is just psychological terrorism”).  \nImages and video are not currently included in most \nMIR systems to support searching and browsing. One \nnotable exception is an interface based on a ‘collage \nmachine’, which allows the user to serendipitously ex-\nplore a music collection based on an interactive collage \nof images associated with song and album titles [8]. \nThis neglect is surprising, given the long history of al-\nbum and CD cover art; printing innovations in the late \n1940s were quickly exploited by record companies to \nprovide distinctive, attractive covers, and conventions of \nimagery, colour, and style emerged to represent different \nmusical genres [9].  Shoppers in CD stores frequently \nuse visual cues from CD covers to identify potentially \ninteresting  music when browsing, or as a memory aid to \nquickly pick out a particular desired CD from a stack \n[10]. And, of course, the artist is usually featured in a \nsong’s video, supporting easy visual identification of the \nperformer. \nGiven these strong associations of still and video im-\nages with music, it appears promising to use CD covers \nand video images to support browsing in MIR systems. \nImages can be scanned more quickly than text, and sup-\nport rapid relevant/not relevant decisions—in this case, \nto identify musical genres, artists, and mood that are not \nof interest, or to winnow out specific songs. \n3.6 Over-Exposure \nWhile the number of times an individual hears a song \nclearly influences whether that person likes/dislikes the \nsong, the impact of repetition is neither straightforward \nnor clearly understood. The most widely accepted theory \nstates that repeated exposure to a song tends to increase \nthe degree of ‘liking’, until a peak is reached. After that \npeak, continued exposure to the song is associated with a \nlessening of ‘liking’ [11]. A similar pattern, dubbed \n‘thrashing’ and ‘sickness’, has been noted in the acquisi-\ntion of CDs for a personal music collection; a new pur-chase is played frequently (‘thrashing’) until saturation is \nreached, at which point ‘sickness’ sets in and the CD is \nset aside [12]. The song may become more acceptable \nwhen (or if) the sense of over-familiarity wears off. \n Approximately 6% (25) responses cited over-playing \nas a factor in disliking a song. One respondent declined \nto select a single song and instead nominated “any song \nthat is at one of the first 5 chart positions. i just can't \nhear something 20 times a day.” Over-exposure could \ncome about through radio play (“The very worst about \nthat song is that it was played on the radio all summer \nand you just couldn't hide from it.”), or through other \nsources (“all those damn ipod commercials”; “seems to \nbe in every movie during some cheesy driving scene”; \n“Its on nearly every bleeding love song compilation”; \n“worst of all its really popular for karaoke”).  \nIt is initially tempting to suggest associating a meas-\nure of the current degree of airplay with a song as meta-\ndata. Unfortunately, this would provide only a rough \nindication of over-exposure. Airplay misses other poten-\ntial venues for contact with a song, and individuals will \nhave different thresholds for at which over-familiarity \nsets in.  \nA more promising approach is to require the audio \nplayer application to keep usage logs of the music lis-\ntened to, and to analyse this information to detect pat-\nterns of usage.  Given that most contemporary audio \nplayers integrate audio streaming this would also natu-\nrally cover the user’s listening pattern to on-line radio \nstations noting, for example, when they effectively \n“change channel.”  Further research is required to de-\ntermine whether an airplay measurement is too crude to \nbe useful in filtering over-exposed songs. \n3.7 Pretentiousness: Wannabes and Posers \nPop artists and their music can be seen as symbols for \nsub-cultures, outlooks on life, even entire generations. \nThe perception that an artist is a ‘wannabe’, that he or \nshe is merely copying the appearance, behaviour, or \nstyle of another group is cited as being particularly off-\nputting in 15 (3.7%) responses: ‘just reeks of stupid \nwhite boy…misguidedly trying to channel prince’; ‘faux \n\"street\" gestures’; ‘faux spiritualism’. A similar distain \nis felt for songs perceived as pretentious or the product \nof ‘posers’ (15 responses, 3.7%):    ‘Overblown preten-\ntious cliched rock, attempting to be poetic’; ‘The inane \nlyrics and the fact that in popular opinion it is regarded \nas a deep and meaningful song’; “if I have to hear an-\nother U2 fanboy mumbling on about how Deep U2 are \nsomeone is going to hurt.”; ‘The pretentious artist’.  \nGiven this frequent association of pretentiousness \nwith particular artists, it appears that the ability to filter \nout songs from certain artists would be a useful facility \nfor a music retrieval system. \n3.8 Clashing Taste Cultures \nA colleague drops by to tell me that she’s been looking \nthrough the posted survey responses.  She agrees with \nmost of them, but The Rasmus!  How could anyone hate \nthat group!  And ‘Milkshake’—that song is cute! \nNo song is universally loved, or universally hated. \nSome respondents express a sense of surprise and dis-\n476   \n \n may that their nominated song is not universally \nloathed: ‘IT'S SO ANNOYING HOW MANY PEOPLE \nLOVE THIS DRIVEL’. This response can be inter-\npreted in the context of ‘taste cultures’—groups of peo-\nple with similar likes and dislikes for products of sig-\nnificance to the group (such as music and clothing) [13]. \nA song may be particularly disliked if it is associated \nwith a taste culture that the listener finds objectionable:  \n‘Perhaps I really loath it because I don't like the sort of \npeople that really get into it.’ \nCollaborative filtering systems make recommenda-\ntions based on taste information collected from other \nusers. Generally, it is easier to gather information on \nuser likes than on dislikes (for example, a ‘like’ may be \nassumed if a user downloads a song). It would be inter-\nesting to explore whether recommendations could be \nrefined by gathering dislike data as well. \n3.9 Unfortunate Personal Associations \nSongs can become strongly associated in our memories \nwith people or events; think of the couples who hold \nhands when ‘our song’ comes on the radio, or remember \na particularly enjoyable party from your youth and note \nthat a song from that time pops into your head. For 5% \n(21) of the survey respondents, their nominated song has \nan unfortunate association that is evoked whenever they \nhear the song: \nThis song was played 5 times in a row at my \nAuntie's funeral. I can't listen to the song without \nthinking of her. (She died young) \nClearly such personal associations are impossible to \ncapture in software. It is important to be aware of such \nidiosyncratic characteristics, however, as they are limit-\ning factors to the success of any attempt to provide mu-\nsic recommendations. \n4 CONCLUSIONS AND FUTURE WORK \nUltimately, the songs that we dislike depend as much \nupon ourselves as upon characteristics of the songs. The \nearworm effect suggests features of songs whose appeal \nis likely to fade, with further evidence supporting fea-\ntures of the lyrics and tune that particular individuals \nmay dislike. As the survey continues gathering user in-\nput, we intend to gather up a research collection of the \nnominated works so we can begin a more in-depth map-\nping of the content-based (i.e., lyrics, audio) and extra-\nmusical features (i.e., videos, radio play history, chart \npositions, etc.) to the rather passionately described ex-\nplanations of the respondents. This mapping should con-\ntribute greatly to the improvement of both MIR recom-\nmender (positive) and user-defined MIR filtering (nega-\ntive) algorithms.  \nOh, and the song that received the most nominations \nfor ‘worst song ever’? Achy Breaky Heart, by Billy Ray \nCyrus: respondents objected to its earworm qualities, \nlyrics, overly-simple melody, its taste culture, and yes, \neven cited personal associations (“My ex used to try to \ndance to it when we went out, and I have hated it and \nhim since”). REFERENCES \n[1] Uitdenbogerd, A. and van Schyndel, R. \"A review \nof factors affecting music recommender success\",  \nProceedings of ISMIR '02, Paris. 2002, 204-208. \n[2] Soldier, D., Komar, and Melamid, People's Choice \nMusic [CD-Single]. 2002, Mulatta. \n[3] Barry, D. Dave Barry's Book of Bad Songs. An-\ndrews McMeel, Kansas City, 1997. \n[4] Glaser, B. and Struass, A. The Discovery of \nGrounded Theory:  Strategies for Qualitative Re-\nsearch. Chicago, 1967. \n[5] Twain, M. A Literary Nightmare. The Atlantic \nMonthly, 37, 20, (1876), 167-170,  \n[6] Kellaris, J.J. \"Dissecting earworms: Further evi-\ndence on the song-stuck-in-your-head phenome-\nnon\". Proceedings of Society for Consumer Psy-\nchology, New Orleans, LA, American Psychologi-\ncal Society, 2003, 220-222. \n[7] Kellaris, J.J. \"Identifying properties of tunes that \nget stuck in your head: toward a theory of cognitive \nitch\". Presented at the Society for Consumer Psy-\nchology Conference, Scottsdale, AZ, American \nPsychological Society, 2001. \n[8] Bainbridge, D., Cunningham, S.J., and Downie, \nJ.S. \"Visual collaging of music in a digital library\". \nProceedings of ISMIR '04, Barcelona, 2004, 397-\n402. \n[9] Ruffins, F.D., Reflecting on ethnic imagery in the \nlandscape of commerce, 1945-1975, in Getting and \nspending:  European and American consumer so-\ncieties in the Twentieth Century, S. Strasser, C. \nMcGovern, and M. Judt, Eds., Cambridge, Cam-\nbridge University Press, 1998. \n[10] Cunningham, S.J., Reeves, N., and Britland, M. \n\"An ethnographic study of music information seek-\ning: implications for the design of a music digital \nlibrary\".  Proceedings of International Joint \nACM/IEEE Conference on Digital Libraries, Hous-\nton, TX, 2003, 5-16. \n[11] North, A.C. and Hargreaves, D.J., Experimental \naesthetics and everyday music listening, in The So-\ncial Psychology of Music, D.J. Hargreaves and \nA.C. North, Eds., Oxford, Oxford University Press, \n1997, 84-103. \n[12] Cunningham, S.J., Jones, M., and Jones, S. \"Organ-\nizing digital music for use: An examination of per-\nsonal music collections\". Proceedings of ISMIR \n'04, Barcelona,  2004, 447-454. \n[13] Zillman, D. and Gan, S.-l., Musical taste in adoles-\ncence, in The Social Psychology of Music, D.J. \nHargreaves and A.C. North, Eds., Oxford, Oxford \nUniversity Press, 1997, 161-187. \n \n477"
    },
    {
        "title": "Using the Gamera Framework for Building a Lute Tablature Recognition System.",
        "author": [
            "Christophe Dalitz",
            "Thomas Karsten"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416378",
        "url": "https://doi.org/10.5281/zenodo.1416378",
        "ee": "https://zenodo.org/records/1416378/files/DalitzK05.pdf",
        "abstract": "In this article we describe an optical recognition system for historic lute tablature prints that we have built with the aid of the Gamera toolkit for document analysis and recognition. We give recognition rates for various historic sources and show that our system works quite well on printed tablature sources using movable types. For engraved and manuscript sources, we discuss some principal current limitations of our system and Gamera. Keywords: Optical Music Recognition, Lute Tablature. 1 LUTE TABLATURE From the 16th and early 17th century a large body of lute tablature sources is extant. As a major part of this music is derived from vocal models, it can be an ideal investigation object for music information retrieval questions. Consequently there are efforts like the ECOLM project [1] to build a data base of machine readable tablature encodings of lute music sources. Usual optical music recognition (OMR) systems designed for common music notation (CMN) cannot be used for this purpose because lute music is written in tablature rather than CMN. Figure 1 shows the characteristics of lute tablature notation: rather than specifying the sound of the music, it specifies when and in which frets the strings of the instrument are stopped. The symbols used for fret and rhythm had not been standardized, but almost every historic source used its own unique set of symbols. This is an important difference to CMN, which consists of a limited set of symbols which are consistent across different music scores. Consequently a system for optical tablature recognition (OTR) must not be designed to work with a single set of a priori known symbols, but to be adaptable to differing tablature symbols. We shall see below that the conception of training in Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. c⃝2005 Queen Mary, University of London d a d d a cd a c d a c d a 8 course 4, fret 0 course 3, fret 3 string (course) 6 5 4 3 2 1 Figure 1: Lute tablature sample and music transcription statistical pattern recognition provides an adequate framework for this. Besides the above problem, the complexity of recognizing a particular source depends on two factors: how much the individual symbols vary within the source and whether symbols overlap and intersect. While the former can hinder the identification of individual symbols, the latter poses problems on their proper segmentation. In our present work we only consider sources printed with movable types because in these the symbol variance is limited to random defects and symbols are not overlapping. 2 THE GAMERA FRAMEWORK The Gamera framework [2] has been developed by M. Droettboom et al. as a flexible toolkit for building recognition systems. Gamera essentially is a library for the Python programming language and has a number of distinctive features: • Gamera already provides functions for image segmentation (projections, connected component analysis) and classification (kNN) and it has a classifier training interface • all methods can be combined flexibly because they are provided as python modules. Own C++ functions for image processing can be added as plugins • platform independence, which has let us develop parallel on both Linux and MacOS X without noticeable differences 478 • Gamera’s source code is freely available under the GNU general public license The last point is particularly important for research projects, because it allows for full access to the underlying techniques and enables other researchers to participate in the development of Gamera. Consequently we have also made the full source code of our system freely available as a Gamera toolkit from our project homepage [3]. 3 OUR RECOGNITION SYSTEM The recognition process in our system consists of the four steps preprocessing, segmentation, classification and postprocessing. Table 1 provides an overview over the individual operations and shows what we could use from Gamera and what we had to implement ourselves. The following sections describe the individual steps in more detail. Table 1: Operations used from Gamera and implemented by ourselves Preprocessing: rotation correction own implementation (now in Gamera) smoothing Gamera (convolution) staff line removal own implementation Segmentation: symbol isolation Gamera (CC analysis) Classification: heuristic own implementation statistic Gamera (kNN, grouping) Postprocessing: semantic interpretation own implementation tablature coding own implementation (abc) (midi with abc2midi, postscript with abctab2ps)",
        "zenodo_id": 1416378,
        "dblp_key": "conf/ismir/DalitzK05",
        "keywords": [
            "Gamera toolkit",
            "optical music recognition",
            "lute tablature",
            "CMN",
            "tablature notation",
            "fret",
            "rhythm",
            "symbol variance",
            "symbol overlap",
            "postprocessing"
        ],
        "content": "USING THE GAMERA FRAMEWORK FOR BUILDING A LUTE\nTABLATURE RECOGNITION SYSTEM\nChristoph Dalitz Thomas Karsten\nNiederrhein University of Applied Sciences\nReinarzstr. 49, 47805 Krefeld, Germany\nchristoph.dalitz@hsnr.de, thomas.karsten@gmx.de\nABSTRACT\nIn this article we describe an optical recognition system\nfor historic lute tablature prints that we have built with\nthe aid of the Gamera toolkit for document analysis and\nrecognition. We give recognition rates for various his-\ntoric sources and show that our system works quite well\non printed tablature sources using movable types. For en-\ngraved and manuscript sources, we discuss some principal\ncurrent limitations of our system and Gamera.\nKeywords: Optical Music Recognition, Lute Tablature.\n1 LUTE TABLATURE\nFrom the 16th and early 17th century a large body of lute\ntablature sources is extant. As a major part of this music is\nderived from vocal models, it can be an ideal investigation\nobject for music information retrieval questions. Conse-\nquently there are efforts like the ECOLM project [1] to\nbuild a data base of machine readable tablature encodings\nof lute music sources.\nUsual optical music recognition (OMR) systems de-\nsigned for common music notation (CMN) cannot be used\nfor this purpose because lute music is written in tablature\nrather than CMN. Figure 1 shows the characteristics of\nlute tablature notation: rather than specifying the sound o f\nthe music, it speciﬁes when and in which frets the strings\nof the instrument are stopped.\nThe symbols used for fret and rhythm had not been\nstandardized, but almost every historic source used its own\nunique set of symbols. This is an important difference to\nCMN, which consists of a limited set of symbols which\nare consistent across different music scores. Consequentl y\na system for optical tablature recognition (OTR) must not\nbe designed to work with a single set of a priori known\nsymbols, but to be adaptable to differing tablature sym-\nbols. We shall see below that the conception of training in\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of London/french_d\n/french_a/french_d/french_d\n/french_a/french_c /french_d/french_a /french_c\n/french_d\n/french_a/french_c\n/french_d\n/french_a\n8course 4, fret 0course 3, fret 3\nstring (course)654321\nFigure 1: Lute tablature sample and music transcription\nstatistical pattern recognition provides an adequate fram e-\nwork for this.\nBesides the above problem, the complexity of recog-\nnizing a particular source depends on two factors: how\nmuch the individual symbols vary within the source and\nwhether symbols overlap and intersect. While the former\ncan hinder the identiﬁcation of individual symbols, the lat -\nter poses problems on their proper segmentation. In our\npresent work we only consider sources printed with mov-\nable types because in these the symbol variance is limited\nto random defects and symbols are not overlapping.\n2 THE GAMERA FRAMEWORK\nThe Gamera framework [2] has been developed by M.\nDroettboom et al. as a ﬂexible toolkit for building recog-\nnition systems. Gamera essentially is a library for the\nPython programming language and has a number of dis-\ntinctive features:\n•Gamera already provides functions for image seg-\nmentation (projections, connected component anal-\nysis) and classiﬁcation (kNN) and it has a classiﬁer\ntraining interface\n•all methods can be combined ﬂexibly because they\nare provided as python modules. Own C++ functions\nfor image processing can be added as plugins\n•platform independence, which has let us develop par-\nallel on both Linux and MacOS X without noticeable\ndifferences\n478•Gamera’s source code is freely available under the\nGNU general public license\nThe last point is particularly important for research\nprojects, because it allows for full access to the underlyin g\ntechniques and enables other researchers to participate in\nthe development of Gamera. Consequently we have also\nmade the full source code of our system freely available\nas a Gamera toolkit from our project homepage [3].\n3 OUR RECOGNITION SYSTEM\nThe recognition process in our system consists of the\nfour steps preprocessing, segmentation, classiﬁcation and\npostprocessing . Table 1 provides an overview over the in-\ndividual operations and shows what we could use from\nGamera and what we had to implement ourselves. The\nfollowing sections describe the individual steps in more\ndetail.\nTable 1: Operations used from Gamera and implemented\nby ourselves\nPreprocessing:\nrotation correction own implementation\n(now in Gamera)\nsmoothing Gamera (convolution)\nstaff line removal own implementation\nSegmentation:\nsymbol isolation Gamera (CC analysis)\nClassiﬁcation:\nheuristic own implementation\nstatistic Gamera (kNN, grouping)\nPostprocessing:\nsemantic interpretation own implementation\ntablature coding own implementation (abc)\n(midi with abc2midi,\npostscript with abctab2ps)\n3.1 Preprocessing and Segmentation\nIn order to be independent of the scanning resolu-\ntion we ﬁrst determine the characteristic dimensions\nstafﬂine height (staff line thickness) and staffspace height\n(distance between adjacent staff lines) as the most fre-\nquent black and white vertical runlengths [4].\nWhen scanning a tablature book the image is usually\nslightly rotated. Such a rotation has a negative effect\non both the detection of staff lines and the classiﬁcation\nof the tablature symbols and hence should be corrected,\nwhich we have done with Postl’s projection proﬁle method\n[5]. Although a naive implementation is rather slow, we\nhave obtained reasonable performance by scaling the im-\nage down to a stafﬂine height of two pixels and by search-\ning the maximum with a golden section search [6].\nThe image quality of most historic lute tablatures is far\nfrom ideal. Typical defects are random black speckles and\nbroken symbols. We identify black speckles as connected\ncomponents with a small black area. Small white holes\nwithin tablature symbols or staff lines can be ﬁlled with\na convolution operation. We have chosen a kernel width\nequal to stafﬂine height and a height half as high. This\nFigure 2: Staff line removal\nrectangular shape has the effect that stafﬂines become bet-\nter connected.\nWhile the above operations merely improve the image\nquality and are optional, there is one ﬁnal obligate prepro-\ncessing operation: staff line removal. Figure 2 shows that\nthe symbols are only separable with a connected compo-\nnent analysis after the staff lines have been removed, be-\ncause otherwise all symbols are connected by staff lines.\nOur removal method is based on Fujinaga [4] with two\nmajor modiﬁcations: we have omitted the line rectifying\nshearing step and have added an option to remove a staff\nline section even when it touches a symbol. The latter is\nused for fret letters between the lines. When they are on\nthe lines we simply keep vertical runlengths longer than\ntwo times stafﬂine height . Although this distorts the line\ncrossing part of symbols considerably, the effect on sym-\nbol classiﬁcation is less negative than one might expect,\nbecause the distorted symbols are also used for training.\nBefore the staff lines are removed, we remember their\nposition, which we ﬁnd via an analysis of the horizon-\ntal projection proﬁle. Although this only yields a single\nrow value per staff line, it is sufﬁcient because the ver-\ntical variation of a staff line after the skew correction is\nconsiderably less than staffspace height .\n3.2 Symbol Classiﬁcation\nOnce the tablature symbols are isolated they are passed\none by one to the classiﬁer which assigns each symbol to\na speciﬁc class like “eighth ﬂag”, “bar line”, “fret number\nzero”. For all symbols (except bar lines) we use a sta-\ntistical classiﬁcation method, which means that the fea-\nture distribution of the symbols needs to be trained before\nrecognition. This makes the system adaptable to a wide\nrange of tablature prints because no a priori assumptions\nabout the actual form of the individual symbols are made.\nFor classiﬁcation we use the k nearest neighbor (kNN)\nmethod [8] in combination with Gamera’s grouping algo-\nrithm [7], which cares for disjoined symbols like the letter\n“g” in ﬁgure 2. As this grouping algorithm requires the\ngroup to be pretty close to a glyph from the training data,\nit did not work well with bar lines, which are often broken\nin historic tablature prints. Hence we classify bar lines\nheuristically based upon their aspect ratio, width and tota l\nheight of a group of bar line fragments.\nThe kNN classiﬁer recognition rate strongly depends\non the chosen set of features. Of Gamera’s 15 built in fea-\ntures (see [2] for their description) 14 are useful for seg-\nmentation based classiﬁcation, resulting in 214possible\nfeature combinations. We have used Morlaye’s tablature\n479book from 1552 with a training set of 3303 training sym-\nbols and 1403 test symbols (four pages with a total of 16\nstaves) to measure the recognition rate for all possible fea -\nture sets. The results for single features are given in table\n2. Note that some features are vector values; hence the\ndimension is also given in the second column.\nTable 2: Recognition rates for individual features\nFeature Dimension Recognition Rate\narea 1 70.4%\naspect ratio 1 74.5%\nblack area 1 33.9%\ncompactness 1 13.2%\nmoments 9 98.0%\nncols 1 52.8%\nnholes 2 67.3%\nnholes extended 8 87.0%\nnrows 1 58.2%\nskeleton features 5 70.5%\nvolume 1 42.8%\nvolume16regions 16 96.6%\nvolume64regions 64 97.3%\nzernike moments 26 96.7%\nFor feature combinations, ﬁgure 3 shows how the\nrecognition rate depends on the feature set size. For all\nsets of the same size the highest recognition rate value\nis displayed. Our results nicely illustrate the point that\nthe addition of features does not necessarily increase the\nrecognition rate.\nThe highest recognition rates were achieved by sev-\neral feature combinations followed closely by other com-\nbinations. Hence we have based our decision for a par-\nticular feature set not on the recognition rate alone, but\nalso on the complexity of the feature computation, and\nsettled on the combination aspect ratio, moments, nrows,\nvolume64regions (measured recognition rate: 99.2%).\nAnother parameter that needs to be chosen in kNN\nclassiﬁcation is the number kof neighbors taken into ac-\ncount. According to theory [8], kshould be considerably\nsmaller than the smallest class population among the train-\ning samples. Moreover the class population ratios in the\ntraining set should be representative for the original data ,\nwhich implicitly takes the a priori probabilities into ac-\ncount. These constraints have the effect that kcould be 0.97 0.98 0.99\n 2  4  6  8  10  12  14maximum recognition rate\nnumber of features\nFigure 3: Maximum recognition rate for different feature\nset sizes\nhardly larger than one, because there are some compara-\nbly rare symbols in lute tablature like high fret numbers or\nvery long or very short rhythm values.\n3.3 Postprocessing\nAfter the individual symbols are recognized, their musical\nsemantics needs to be determined. Essentially this means\nto organize the symbols as a linear sequence of chords .\nHere “chord” does not only mean “set of simultaneous\nnotes”, but any set of symbols which belong together at\na single point on the time axis. For instance a “chord” can\nbe a bar line, a time signature or an actual chord of notes\ntogether with its rhythm value.\nIn order to establish this chord sequence, we assign\neach symbol a staff and course number based upon the\nstaff line positions determined during the preprocessing\nphase. Then all symbols within the same staff can be\ngrouped into chords. A natural grouping criterion is\nwhether symbols overlap horizontally. In order not to miss\nsmall symbols like dots we do not use an absolute over-\nlapping threshold, but a threshold for the ratio of overlap\nand symbol width. When using this criterion it is neces-\nsary to ﬁrst ignore wide symbols that span more than one\nchord and take care of them after grouping. The thresh-\nold whether a symbol is “wide” can be based upon staff-\nspace height or the median symbol width.\nOnce the chord sequence is established, the tablature\ncode can be generated. Unfortunately there is currently\nno widely accepted open lute tablature encoding format.\n/french_a\n/french_a\n/french_a\n/french_c/french_a\n/french_a\n/french_a\n/french_c /french_a\n/french_d /french_c/french_d\n/french_b\n/french_a\n/french_d/french_c/french_a /french_b /french_d/french_a /french_b /french_d/french_a\n/french_d\n/french_c\n/french_d/french_d/french_b /french_a\n/french_d /french_b /french_a\n/french_d\n/french_c/french_a\n/french_a\n/french_a/french_d /french_c /french_d\n/french_f\n/french_c/french_e\nFigure 4: Sample result of our OTR system\n480ECOLM uses T. Crawford’s TabCode [1], but apart from\nthe internal ECOLM software there is currently no soft-\nware available that can read this format. Hence we have\nused C. Dalitz’ tablature extension of the abcformat, for\nwhich free software is available [9]. It should be noted\nhowever that the actual tablature encoding does not mat-\nter, because conversion programs could be easily written\nor our OTR system could be easily extended to produce\nother codes.\nWe also generate a music transcription in a “literal”\nrather than an “interpretative” way [10]. Figure 4 shows\nthe OTR result after it has been processed with the pro-\ngram abctab2ps . The key signature is automatically cho-\nsen in such a way that the number of accidentals is mini-\nmized. In ﬁgure 4 this leads to an E ﬂat, because there are\ntwo ﬂat E’s, but only one natural E.\n3.4 Performance and Limitations\nWe have tested our system with several sources using dif-\nferent tablature types (both “french” and “italian” lute ta b-\nlature) and obtained the error rates given in table 3. Note\nthat this is not just the error rate for classifying individu al\nsymbols but also includes errors introduced by our post-\nprocessing operations.\nTable 3: Error rates of our system for various historic tab-\nlature prints (for details about the sources see [11])\nSourceTraining Test Error\nSymbols Symbols Rate\nMorlaye 1552a 3303 1673 0.6%\nDenss 1594 3390 1827 3.6%\nFrancisque 1600 2959 1645 4.1%\nSpinacino 1507a 3023 2286 1.8%\nGiovanni Maria\nda Crema 1546a2275 1850 3.8%\nCasteliono 1536 1577 1713 3.9%\nMost of the individual recognition errors can be traced\ndown to the following reasons:\n•artifacts from the staff line removal procedure\n(da Crema, Casteliono)\n•errors in bar line recognition (Francisque)\n•wrong chord grouping\n•touching symbols (Denss)\nIn order to improve the staff line removal, we have begun\nto investigate different staff line removal techniques sug -\ngested in the literature. The bar line recognition might be\nimproved by a semi-heuristic approach: ﬁrst vertical line\nfragments could be classiﬁed statistically and among these\ngroups forming bar lines could be searched.\nThe last problem of touching symbols however is\nmore fundamental. It is due to our approach of a symbol\nagnostic segmentation via a connected component analy-\nsis. Although Gamera can deal to a certain extent with\nover-segmented images [7], it currently cannot cope with\nunder-segmented symbols. Consequently our system is\nnot applicable to manuscripts or engravings, which typi-\ncally do not only have touching symbols but also a lot of\nintersecting symbols.4 CONCLUSION\nAccording to the Gamera homepage [2], Gamera is a tool\nfor “domain experts, who have a strong knowledge of the\ndocuments in a collection, but may not have a formal tech-\nnical background”. This statement should be taken with a\ngrain of salt. It holds as long as everything can be achieved\nwith methods that Gamera provides out of the box, be-\ncause Python is an easy to learn language particularly for\npeople without a training in computer science. The addi-\ntion of custom plugins however requires some knowledge\nof image processing techniques and advanced C++ pro-\ngramming concepts.\nThus we consider Gamera ideal for “domain experts”\nwho also have some technical background. That we have\nimplemented our OTR system within the Gamera frame-\nwork has reduced our development time considerably. Ac-\ntually we consider our OTR system as a demonstration\nthat Gamera’s modular and extensible conception is suc-\ncessful.\nREFERENCES\n[1] G. Wiggins, T. Crawford, M. Gale, D. Lewis:\n“An Electronic Corpus of Lute Music (ECOLM)”.\nhttp://www.ecolm.org/\n[2] M. Droettboom: “The Gamera Project Homepage”.\nhttp://gamera.sourceforge.net/\n[3] Homepage of our OTR project:\nhttp://lionel.kr.hsnr.de/ ˜dalitz/data/projekte/otr/\n[4] I. Fujinaga: “Staff Detection and Removal”. In S.\nGeorge (editor): Visual Perception of Music Nota-\ntion, pp. 1-39, Idea Group, 2004\n[5] W. Postl: “Detection of linear oblique structures and\nskew scan in digitized documents”. Proc. 8th Int.\nConf. on Pattern Recognition , pp. 687-689, 1986\n[6] W.H. press, B.P. Flannery, S.A. Teukolsky, W.T. Vet-\nterling: “Numerical Recipes in Pascal”. Cambridge\nUniversity Press , 1993\n[7] M. Droettboom: “Correcting broken characters in\nthe recognition of historical printed documents”.\nJoint Conference on Digital Libraries , pp. 364-366,\n2003\n[8] B.V . Dasarathy: “Nearest Neighbor (NN) Norms:\nNN Pattern Classiﬁcation Techniques”. IEEE Com-\nputer Society Press , 1991\n[9] C. Dalitz: “abctab2ps”.\nhttp://www.lautengesellschaft.de/cdmm/\n[10] M. Orph ´ee: “A History of Transcriptions of Lute\nTablature from 1679 to the Present”. The Lute, Jour-\nnal of the Lute Society , V olume XLIII, pp. 1-43,\n2003\n[11] R ´epertoire international de sources musicales\n(RISM), Series A/I: “Single Prints before 1800”.\nB¨arenreiter , 1971-1981\n481"
    },
    {
        "title": "Toward Automated Holistic Beat Tracking, Music Analysis and Understanding.",
        "author": [
            "Roger B. Dannenberg"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415246",
        "url": "https://doi.org/10.5281/zenodo.1415246",
        "ee": "https://zenodo.org/records/1415246/files/Dannenberg05.pdf",
        "abstract": "Most music processing attempts to focus on one particular feature or structural element such as pitch, beat location, tempo, or genre. This hierarchical approach, in which music is separated into elements that are analyzed independently, is convenient for the scientific researcher, but is at odds with intuition about music perception. Music is interconnected at many levels, and the interplay of melody, harmony, and rhythm are important in perception. As a first step toward more holistic music analysis, music structure is used to constrain a beat tracking program. With structural information, the simple beat tracker, working with audio input, shows a large improvement. The implications of this work for other music analysis problems are discussed. Keywords: Beat tracking, tempo, analysis, music structure 1 INTRODUCTION Music is full of multi-faceted and inter-related information. Notes of a melody fall into a rhythmic grid, rhythm is hierarchical with beats, measures, and phrases, and harmony generally changes in coordination with both meter and melody. Although some music can be successfully decomposed into separate dimensions of rhythm, harmony, melody, texture, and other features, this kind of decomposition generally loses information, making each dimension harder to understand. In fact, it seems that musicians deliberately complicate individual dimensions to make them more interesting, knowing that listeners will use other information to fill in the gaps. Syncopation can be exaggerated when the tempo is very steady, but we hear less syncopation when tempo is more variable. Confusing rhythms are often clarified by an unmistakeable chord change on the first beat of a measure. Repetition in music often occurs in some power-of-two number of measures, providing clear metrical landmarks even where beats and tempo might be ambiguous. It is easy to notice these interrelationships in music, but difficult to take advantage of them for automatic music analysis. If everything depends on everything else, where does one start? If perception is guided by expectations, will we fail to perceive the “truth” when it is unexpected? Music analysis produces all kinds of data and representations. How can the analysis of one dimension of music inform the analysis of another, given the inevitable errors that will occur? These are all difficult questions and certainly will form the topic of much future research. This paper describes a small step in this general direction. I will show how information about music structure can be used to inform a beat tracker. In all previous beat trackers known to the author, an algorithm to identify beats is applied uniformly, typically from the beginning to the end of a work. Often times, beat trackers have a tendency to be distracted by syncopation and other musical complexities, and the tracker will drift to some faster or slower tempo, perhaps beating 4 against 3 or 3 against 4. In contrast, when musical structure is taken into account, the beat tracker can be constrained such that when a beat is predicted in one section of music, a beat is also predicted at the corresponding place in all repetitions of that section of music. In practice, these are not absolute constraints but probabilistic tendencies that must be balanced against two other goals: to align beats with sonic events and to maintain a fairly steady tempo. It might seem that if a beat tracker can handle one section of music, it can handle any repetition of that section. If this were the case, the additional constraint of music structure would not help with the beat-tracking problem. Tests with real data, however, show a dramatic improvement when music structure is utilized. How can this be? A simple answer is that the input data is audio, and the detection of likely beat events is error prone. Music structure helps the beat tracker to consolidate information from different sections of music and ultimately do a better job. This will be explained in greater detail in the discussion section. The next section describes related work. Then, in Section 3, I explain the basic beat tracker used for Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or com-mercial advantage and that copies bear this notice and the full citation on the first page. © 2005 Queen Mary, University of London 366",
        "zenodo_id": 1415246,
        "dblp_key": "conf/ismir/Dannenberg05",
        "keywords": [
            "Music perception",
            "Multi-faceted information",
            "Interconnectedness",
            "Melody",
            "Harmony",
            "Rhythm",
            "Syncopation",
            "Tempo",
            "Repetition",
            "Music structure"
        ],
        "content": "TOWARD AUTOMATED HOLISTIC BEAT TRACKING, \nMUSIC ANALYSIS, AND UNDERSTANDING\n Roger B. Dannenberg  \n School of Computer Science \nCarnegie Mellon University \nPittsburgh, PA 15213 USA \nrbd@cs.cmu.edu  \nABSTRACT \nMost music processing attempts to focus on one \nparticular feature or structural element such as pitch, beat \nlocation, tempo, or genre. This hierarchical approach, in \nwhich music is separated into elements that are analyzed \nindependently, is convenient for the scientific researcher, \nbut is at odds with intuition about music perception. \nMusic is interconnected at many levels, and the interplay \nof melody, harmony, and rhythm are important in \nperception. As a first step toward more holistic music \nanalysis, music structure is used to constrain a beat \ntracking program. With structural information, the simple \nbeat tracker, working with audio input, shows a large \nimprovement. The implications of this work for other \nmusic analysis problems are discussed. \nKeywords: Beat tracking, tempo, analysis, music \nstructure \n1 INTRODUCTION \nMusic is full of multi-faceted and inter-related \ninformation. Notes of a melody fall into a rhythmic grid, \nrhythm is hierarchical with beats, measures, and phrases, \nand harmony generally changes in coordination with \nboth meter and melody. Although some music can be \nsuccessfully decomposed into separate dimensions of \nrhythm, harmony, melody, texture, and other features, \nthis kind of decomposition generally loses information, \nmaking each dimension harder to understand.  \nIn fact, it seems that musicians deliberately \ncomplicate individual dimensions to make them more \ninteresting, knowing that listeners will use other \ninformation to fill in the gaps. Syncopation can be \nexaggerated when the tempo is very steady, but we hear \nless syncopation when tempo is more variable. \nConfusing rhythms are often clarified by an \nunmistakeable chord change on the first beat of a measure. Repetition in music often occurs in some \npower-of-two number of measures, providing clear \nmetrical landmarks even where beats and tempo might \nbe ambiguous. \nIt is easy to notice these interrelationships in music, \nbut difficult to take advantage of them for automatic \nmusic analysis. If everything depends on everything \nelse, where does one start? If perception is guided by \nexpectations, will we fail to perceive the “truth” when it \nis unexpected? Music analysis produces all kinds of data \nand representations. How can the analysis of one \ndimension of music inform the analysis of another, given \nthe inevitable errors that will occur? These are all \ndifficult questions and certainly will form the topic of \nmuch future research. \nThis paper describes a small step in this general \ndirection. I will show how information about music \nstructure can be used to inform a beat tracker. In all \nprevious beat trackers known to the author, an algorithm \nto identify beats is applied uniformly, typically from the \nbeginning to the end of a work. Often times, beat \ntrackers have a tendency to be distracted by syncopation \nand other musical complexities, and the tracker will drift \nto some faster or slower tempo, perhaps beating 4 \nagainst 3 or 3 against 4. \nIn contrast, when musical structure is taken into \naccount, the beat tracker can be constrained such that \nwhen a beat is predicted in one section of music, a beat \nis also predicted at the corresponding place in all \nrepetitions of that section of music. In practice, these are \nnot absolute constraints but probabilistic tendencies that \nmust be balanced against two other goals: to align beats \nwith sonic events and to maintain a fairly steady tempo. \nIt might seem that if a beat tracker can handle one \nsection of music, it can handle any repetition of that \nsection. If this were the case, the additional constraint of \nmusic structure would not help with the beat-tracking \nproblem. Tests with real data, however, show a dramatic \nimprovement when music structure is utilized. How can \nthis be? A simple answer is that the input data is audio, \nand the detection of likely beat events is error prone. \nMusic structure helps the beat tracker to consolidate \ninformation from different sections of music and \nultimately do a better job. This will be explained in \ngreater detail in the discussion section. \nThe next section describes related work. Then, in \nSection 3, I explain the basic beat tracker used for Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or \ncom-mercial advantage and that copies bear this notice and the \nfull citation on the first page. \n© 2005 Queen Mary, University of London \n366  \n \nexperiments. In Section 4, music structure analysis is \ndescribed, and the additions to the beat tracker to use \nstructure information are described in Section 5. In \nSection 6, I describe tests performed and the results. \nSection 7 presents a discussion, which is followed by a \nsummary and conclusions. \n2 RELATED WORK \nThe literature has many articles on beat tracking. \nGouyon and Dixon have written an excellent overview \nwith an extensive list of references. [1] For this work, I \nrelied especially on the HFC (high frequency content) \nfeature [2] for detecting likely beat events, as used by \nDavies and Plumbley [3] and also by Jensen and \nAndersen [4]. The general structure of the beat tracker is \nrelated to that of Desain and Honing [5] in that the \ntracker relies on gradient descent. Desain and Honing \nadjust the times of actual beat events to fit an expected \nmodel, whereas my system adjusts a tempo estimate to \nfit the actual times. \nThis work is not unique in attempting to incorporate \nmusic structure and additional features to analyze music. \nIn particular, Goto and Muraoka used knowledge of \ndrum beat patterns to improve beat tracking of popular \n(rock) music with drums [6], and Goto used some music \nclassification techniques to handle music with drums \ndifferently from music without drums [7]. \n3 THE BASIC BEAT TRACKER \nIn order to show that music structure can help with the \nbeat tracking problem, I first constructed a “baseline” \nbeat tracker to measure performance without any music \nstructure information. This beat tracker is based on \nstate-of-the-art designs, but it has not been carefully \ntuned. \nAs is common, the beat tracker consists of two parts. \nThe first part computes likely beat events from audio. \nLikely beat events are time points in the audio that \nsuggest where beats might occur. These are represented \nas a discrete set of (time, weight) pairs. The second part \nattempts to identify more-or-less equally spaced beats \nthat correspond to the likely beat events. Not all likely \nbeat events will turn out to be beats, and some beats will \nnot coincide with a likely beat event. The baseline beat \ntracker attempts to balance the two criteria of steady \ntempo and good matches to likely beat events. \n3.1 Likely beat event detection. \nOne might expect that beats would be simple to \ndetect in popular music, given the typically heavy-\nhanded rock beat. Unfortunately, the loud snare hits are \nnot so different spectrally from rhythm guitar chords or \neven vocal onsets and consonants. Furthermore, much \npopular music exhibits heavy dynamic compression, \ngiving the music an almost constant energy level, so \nlooking for peaks in the amplitude envelope is unreliable for detecting beats. High frequency content (HFC) [2] \nand spectral flux [8] are alternatives to RMS amplitude. \nI use an HFC feature to detect likely beat events. \nMusic audio is mixed from stereo to a single channel \nand downsampled to 16 kHz. FFTs of size 1024 are \ntaken using a Hanning window applied to each (possibly \noverlapping) segment of 512 samples to yield a \nsequence Xn of complex spectra1. The per-frame HFC \nfeature is the sum of the magnitudes weighted by the \nsquare of the bin number [4]: \n \u0001\n=× =512\n12][\nin n iiX hfc  (1)\n \nwhere |Xn[i]| is the magnitude of the ith bin of the nth \nframe. Note that some authors use the square of the \nmagnitude and others weight linearly with bin number. \nTo detect events, some thresholding is necessary. A \nrunning average is computed as: \n 1 1 1.0 9.0 - - × + × = n n n hfc avg avg  (2) \nThe ratio hfcn/avgn exhibits peaks at note onsets, drum \nhits, and other likely beat locations. Unfortunately, even \nafter normalizing by a running average, there will be \nlong stretches of music with no prominent peaks. This \nproblem is solved by a second level of thresholding \nwhich works as follows: \n \n\u0002\u0003\u0004\n×> ×==\n+otherwise  99.0   if ))95.0, max( ,2 min(  /\n1nn n nnnn n n\nthrthr r r thrthravg hfc r (3) \nThus, the nominal threshold is 2, which captures every \nstrong peak (rn > 2) that occurs. When strong peaks are \nnot present, the threshold adapts to detect smaller peaks. \nWhenever the threshold thrn is exceeded by rn, the time \nis recorded along with rn, which serves as a weight in \nfurther computation. (In the next section, these pairs of \n(n/framerate, rn) will be referred to as (ti, wi), a \ntime/weight pair.) Since some peaks are broad and span \nmultiple samples, no further times are recorded until rn \ndips below the threshold. \nThe adaptive median threshold method [9] offers an \nalternative method for picking peaks from hfcn. This \nmethod essentially replaces avgn with a local median \nvalue of hfcn, and it does not adapt when peaks are close \nto the median. \n3.2 Beat tracking: initialization. \nThe beat tracking algorithm works from an initial beat \nlocation and tempo estimation, so the next step is to \nsearch for good initial values. This is not an on-line or \nreal-time algorithm, so the entire song can be searched \nfor good starting values. It is assumed that the likely \nbeat events will be highly correlated with a “beat \npattern” function shown at the top of Figure 1. This \n                                                           \n1 A step size of 64, yielding a frame rate of 250 Hz, was used to \nminimize any time quantization effects. However, there does not \nappear to be any significant difference when using even the lowest \nframe rate tried, 31.25 Hz. \n367  \n \npattern represents the expected locations of quarter notes \n(full peaks) and eighth notes (half peaks), and is biased \nso that the integral is zero. The pattern is not meant to \nmodel a specific musical pattern such as a drum pattern. \nIt merely models alternating strong and weak beats at a \nfixed tempo, and only this one pattern is used. The \npattern is stretched in 2% increments from a beat period \nof 0.3s (200 bpm—beats per minute) to 1.0s (60 bpm)1. \nAt each tempo, the function is shifted by 5 increments of \n1/5 beat. Given a tempo and shift amount, the “goodness \nof fit”, gf, to the data is given by:  \u0001 ×- - =\nii i w ttbp tgf ) /) (( ),,( 0 0 fr fr \n(4) \nwhere t0 is used to center the beat pattern over some \ninterior point in the song, \u0001 is the period, f is the shift \n(in beats), bp is the beat pattern function (top of Figure \n1), and (ti, wi) are the likely beat event times and weights \ncalculated in Section 3.1.2 \n \n \nFigure 1. Beat patterns used to search for initial \nbeat location and tempo. \nEach configuration of tempo and shift is further \nrefined using a gradient descent algorithm to find the \nbest local fit to the data. Then the peaks of the beat \npattern function are sharpened as shown in the lower \nhalf of Figure 1 to reduce the weight on outliers, and the \ngradient descent refinement is repeated. \nAll this estimates a tempo and offset for a general \nneighborhood in the song near t0. We want to find a \nplace where beats are strong and the data is as \nunambiguous as possible, so we estimate the tempo and \nbeat offset at 5 second intervals (t0=5, 10, 15, …) \nthroughout the entire song. The values that maximize gf \nare used to initialize the beat tracker. \n3.3 Beat tracking. \nBeat tracking is accomplished by extending the idea of \nthe beat pattern function and gradient decent. Imagine \nbroadening the window on the beat pattern function \n(Figure 1) to expose more peaks and using gradient \ndecent to align the function with increasingly many \nlikely beat events. This is the general idea, but it must be \nmodified to allow for slight tempo variation.  \nTempo (and period) is assumed to be constant within \neach 4-beat measure, so a discrete array of period values \nserves to record the time-varying tempo. Given a vector \n                                                           \n1 These are, of course, parameters that could be changed to accept a \nlarger range of tempi. In practice, the tracker will tend to find multiples \nor submultiples when the “correct” tempo lies out of range. \n2 Note that we can consider the entire, continuous HFC signal simply \nby including every sample point rn in the set of data points (ti, wi). At \nleast on a small sample of test data, this does not improve \nperformance. of beat periods, pv, and an origin, t0, it is not difficult to \ndefine a function from time (in seconds) to beat (a real \nnumber). Call this the “time warp” function \u0002pv, t0(t). The \ngoodness of fit function can then be modified to \nincorporate this “time warping:”  \u0001 × =\nii itpv w t bp tpv gfw ))( ( ),(0, 0 t \n(5) \nThis function maps each likely beat event from time to \nbeat, then evaluates the beat pattern at that beat. Recall \nthat the beat pattern has peaks at integer beat and sub-\nbeat locations. \nIf the only criterion was to match beats, we might see \nwild tempo swings to fit the data, so we add a “tempo \nsmoothness” that penalizes tempo changes: \n ))) () (2,1.0,0( ln( )(\n11\u0001\n--\n+-× =\ni i ii i\npv pvpv pvgauss pvts \n(6) \nwhere gauss(m, s, x) is the Gaussian with mean m and \nstandard deviation s, evaluated at x.  \nThe beat tracking algorithm performs a gradient \ndescent to fit the predicted beats to the likely beat \nevents. The goal is to optimize the sum of gfw and ts, \nwhich represent a good fit to the beat pattern and a \nsmooth tempo curve. Notice, however, that the beat \npattern function shown in Figure 1 rapidly goes to zero, \nso likely beat events outside of a small window will be \nignored. Although not described in detail, the beat \npattern bp consists of a periodic beat pattern multiplied \nby a window function. The window function can be \nwidened to consider more beats. \nThe beat tracking algorithm alternately widens the \nwindow function for the beat pattern to consider a few \nmore beats at the left and right edges of the window. \nThen, gradient descent is used to make slight \nadjustments to the period vector (tempo curve), possibly \ntaking into account more likely beat events that now fall \nwithin the wider window. This alternation between \nwidening the window and gradient descent continues \nuntil the window covers the entire song. \n3.4 Beat tracking performance. \nAs might be expected, this algorithm performs well \nwhen beats are clear and there is a good correspondence \nbetween likely beat events and the “true” beat. In \npractice, however, many popular songs are full of high \nfrequency content from drums, guitars, and vocals, and \nso there are many detected events that do not correspond \nto the beat pattern. This causes beat tracking problems. \nIn particular, it is fairly common for the tempo to \nconverge to some integer ratio times the correct tempo, \ne.g. 4/3 or 5/4. This allows the beat pattern to pick up \nsome off-beat accents as well as a number of actual \ndownbeat and upbeat events. \nOne might hope that the more-or-less complete search \nof tempi and offsets used to initialize the beat tracker \nmight be used to “force a reset” when the tempo drifts \noff course. Unfortunately, while the best match overall \nusually provides a good set of initial values, the best \n368  \n \nmatch in the neighbourhood of any given time point is \nnot so reliable. Often, it is better not to reset the beat \ntracker when it disagrees with local beat information. \nHuman listeners can use harmonic changes and other \nstructural information to reject these otherwise plausible \ntempi, and we would like to use structural information to \nimprove automatic beat tracking, perhaps in the same \nway. The next two sections look at ways of obtaining \nstructure and using structure to guide beat tracking. \n4 STRUCTURAL ANALYSIS \nPrevious work on structural analysis identified several \napproaches to music analysis. [10] This work aimed to \nfind “explanations” of songs, primarily in the form of \nrepetition, e.g. a standard song form is AABA. For this \nstudy, I use the chroma vector representation [11], \nwhich is generally effective for the identification of \nharmony and melody. [12] The chroma vector is a \nprojection of the discrete Fourier transform magnitude \nonto a 12-element vector representing energy at the 12 \nchromatic pitch classes. [13] \nA self-similarity matrix is constructed from chroma \nvectors and a distance function: every chroma frame is \ncompared to every other chroma frame. Within this \nmatrix, if music at time a is repeated at time b, there will \nbe roughly diagonal paths of values starting at locations \n(a, b) and (b, a), representing sequences of highly \nsimilar chroma vectors and extending for the duration of \nthe repetition. (See Figure 2.) \nbb\naa\n \nFigure 2. Paths of high similarity in the similarity \nmatrix. Sections starting at a and b in the music are \nsimilar. \nIn many cases, it is possible to determine a good \n“explanation” that covers the entire song, e.g. \nABABCA. One can imagine inferring the length of \nsections, e.g. 8 or 16 measures, and this could be \nextremely helpful for beat tracking. However, not all \nsongs have such a clear structure, and we cannot make \nsuch strong assumptions. For this study, only the paths \nin the similarity matrix are used, but even this small \namount of structural information can be used to make \nlarge improvements in beat-tracking performance. \n5 BEAT TRACKING WITH STRUCTURE \nWhen two sections of music are similar, we expect them \nto have a similar beat structure. This information can be combined with the two previous heuristics: that beats \nshould coincide with likely beat events and tempo \nchanges should be smooth.  \nThe structure analysis finds similar sections of music \nand an alignment, as shown in Figure 2. The alignment \npath could be viewed as a direct mapping from one \nsegment to the other, but an even better mapping can be \nobtained by interpolating over multiple frames. \nTherefore, to map from time t in one segment to another, \na least-squares linear regression to the nearest 5 points \nin the alignment path is first computed. Then, the time is \nmapped according to this line. \nBut how do we use this mapping? Note that if beat \nstructures correspond, then mapping from one segment \nto another and advancing several beats should give the \nsame result as advancing several beats and then mapping \nto the other segment.1 The formalization of this \n“structural consistency” is now described. \n5.1 Computing Structural Consistency. \nThe “structural consistency” function is illustrated in \nFigure 3 and will be stated as Equation 9. The roughly \ndiagonal line in the figure represents an alignment path \nbetween two sections of music starting at a and b. (Note \nthat the origins of the time axes are not zero, but close to \na and b, respectively, to make the figure more compact.) \nThe time t1 is the time of the first measure beginning \nafter a. This is mapped via the alignment path to a \ncorresponding moment in the music u1. Next, we \nadvance 4 beats beyond t1. To accomplish this, we use \nthe time warp function: \u0001pv,t0(t1), add 4 beats, and then \nmap back to time using the inverse function: \n )4)( ( 1,1\n, 2 0 0+ =-t t tpv tpvt t (7) \nThen, t2 is mapped via the alignment path to u2 as shown \nby dashed lines. The resulting time should be consistent \nwith u1 plus 4 beats, which is computed in the same way \nas t2: \n )4)( ( 1 ,1\n, 2 0 0+ =-u u tpv tpvt t (8) \nIn practice, there will be some discrepancy between u2 \nand the mapping of t2. This is illustrated and labeled \n“error” in Figure 3. \nHaving computed an error value for a 4-beat offset, a \nsimilar procedure is used to compute the error at 8 beats \nand every other measure that falls within the alignment \npath. There may be multiple alignment paths, so all \nerrors for these alignment paths are also computed. The \noverall “structural consistency” function is then:  \u0001\u0001\nÎ Î=\nw wp Pp Eew e gauss sc\n,),2.0,0( \n(9) \nwhere w indicates a range of the song (a “window”) over \nwhich the function is computed, Pw is the set of \n                                                           \n1 We could state further that every beat in one segment should map \ndirectly to a beat in a corresponding segment, but since alignment may \nsuffer from quantization and other errors, this constraint is not \nenforced. Future work should test whether this more direct constraint \nis effective. \n369  \n \nalignment paths that overlap the window w, and Ep,w is \nthe set of error values computed for alignment path p \nwithin window w. Although not mentioned explicitly, \nscw also depends upon the period vector pv as implied by \nEquations 7 and 8. \nt1time (s)time (s)\n4 beats8 beatserror\nerror\n4 beats8 beats\nab\nt2u1u2\n \nFigure 3. If beat locations are consistent with \nstructure, then advancing 4 or 8 beats in one section of \nmusic and mapping to the corresponding point in \nanother section will be equivalent to mapping to the \ncorresponding point (u1) first, and then advancing 4 or \n8 beats. \n5.2 Beat Tracking With Structure Algorithm. \nNow we have three functions to guide our beat tracker: \ngfw is the “goodness of fit with time warping” function \nthat evaluates how well the likely beat events line up \nwith predicted beats, given a period vector that maps \nreal time to beats. ts is the “tempo smoothness” function \nthat evaluates how well the period vector meets our \nexpectation for steady tempo. sc is the structural \nconsistency function that measures the consistency of \nbeats and tempo across similar sections of music. These \nthree functions are simply summed to form an overall \nobjective function. Recall that sc is parameterized by a \nwindow (a starting and ending time); this is set to match \nthe window of the beat pattern function used in gfw. \nIt remains to describe an algorithm that performs beat \ntracking utilizing these three functions. The algorithm is \nsimilar to the beat tracking algorithm of Section 3.3 \n(among other things, using a similar algorithm will help \nus to isolate and assess the impact of structural \nconsistency). We begin with a small window around the \nsame t0 found in Section 3.2 and, as before, alternately \nwiden the window and perform a gradient descent \noptimization of the period vector pv.  \nWhat is different now is that the existence of music \nstructure will force us to “jump” to other locations in the \nsong to evaluate the structural consistency function. \nThese other sections will need a well-defined period \nvector, and because of the coupling between similar \nsections of music, all similar sections will need to be considered when attempting to use gradient descent to \noptimize the objective function. \nThe new algorithm uses the concept of “islands,” \nwhich are simply regions of the song that are relevant to \nthe computation. Each island has an associated period \nvector and time offset. The “time warp” function, t, is \ndefined on a per-island basis.  \nInitially, there is one island centered on t0, and the \nperiod vector is only defined within the “shores” of the \nisland. When this initial island grows to overlap an \nalignment path (or if the island already overlaps an \nalignment path when it is initialized), the structural \nconsistency function will need to examine some other \nplace in the song, quite possibly “off the island.” When \nthis happens (see Figure 4), a new island is created. It is \ninitialized with a small window using an offset and \nperiod vector that makes it consistent with the initial \nisland. \nsimilar sections of music\ninitial island new island  \nFigure 4. New islands are created when parts of an \nexisting island are similar to music elsewhere in the \nsong. This allows for the computation and evaluation \nof structural consistency as part of the beat-tracking \nprocess. \nComputation proceeds in a round-robin fashion, \nlooking at each island in turn. The island’s window is \nwidened and gradient descent is used to optimize the \nisland’s period vector. Then the next island is \nconsidered. \nAt some point, islands will begin to overlap. \nOverlapping islands are merged by consolidating their \nperiod vectors. Ideally, islands will meet on an exact \nmeasure boundary, but this does not always happen in \npractice. To avoid large discontinuities, one of the \nvectors is shifted by some integer number of beats so \nthat the vectors are maximally consistent at their \nmeeting point. When the vectors are merged, beat times \nare preserved and it is assumed that gradient descent will \nfix any remaining inconsistencies. \nSince islands never grow smaller, the algorithm \neventually terminates with one island covering the entire \nsong. At this point, all beat times are determined from \nthe single remaining period vector and time origin t0. \n5.3 Implementation. \nThe HFC feature extraction is implemented in Nyquist \n[14], and the structure analysis is implemented in \nMatlab, while the beat tracking algorithms are \nimplemented in C++. Nyquist is then used to synthesize \n“tap” sounds and combine these with the original songs \nfor evaluation. The total CPU time to process a typical \npopular song is on the order of a few minutes. Using a \ncompiled language, C++, for the gradient-descent beat \ntracking algorithms is important for speed, but other \nlanguage choices were just for convenience. \n370  \n \nThe beat tracking program logs the current period \nvector and other information so that when the \ncomputation completes, the user can display a plot of the \nwarped and windowed beat pattern(s) against the \nexpected beat events. The user can then visualize the \niterative search and optimization by stepping forward or \nbackward in time, and by zooming in or out of various \nregions of the song. This feature proved invaluable for \ndebugging and verifying the behaviour of the program. \n6 EVALUATION \nSince beats are a perceptual construct, there is no \nabsolutely objective way to determine where beats \noccur. Some listeners may perceive the tempo to be \ntwice or half the rate of other listeners. Furthermore, if \nthe tempo is slightly fast or slow, it will appear to be \ncorrect almost half the time, as estimated beats go in and \nout of phase with “true” beats.  \nFor this study, the goal is to compare beat tracking \nperformance with and without the use of structural \nconsistency. To evaluate beat tracking, the beat-tracker \noutput is used to synthesize audio “taps,” which are \nmixed with the original song. The audio mix is then \nauditioned and subjective judgements are made as to \nwhen the beat tracker is following the beat and when it is \nnot. Tapping on the “upbeat” and/or tapping at twice or \nhalf the preferred rate are considered to be acceptable; \nhowever, tapping at a slightly incorrect tempo, causing \nbeats to drift in and out of phase (which is a common \nmode of failure) is not acceptable even though many \npredicted beats will be very close to actual (perceived) \nbeats. Beat tracking is rated according to the percentage \nof the song that was correctly tracked, and percentages \nfrom a number of songs are averaged to obtain an \noverall performance score. Although human judgement \nis involved in this evaluation, the determination of \nwhether the beat tracker is actually tracking or not seems \nto be quite unambiguous, so the results are believed to \nbe highly repeatable. \nSixteen (16) popular songs were tested. Using the \nbasic beat tracking algorithm without structural \nconsistency, results ranged from perfect tracking \nthrough the entire song to total failure. The average \npercentage of the song correctly tracked was 30%. With \nstructural consistency, results also ranged from perfect \nto total failure, but the number of almost perfectly \ntracked songs (> 95% correct) doubled from 2 to 4, the \nnumber of songs with at least 85% correctly tracked \nincreased from 2 to 6, and the overall average increased \nfrom 30% to 59% (p < 0.0034). (See Table 1.) \n7 DISCUSSION \nThe results are quite convincing that structural \nconsistency gives the beat tracker a substantial \nimprovement. One might expect that similar music \nwould cause the beat tracker to behave consistently \nanyway, so it is surprising that the structural consistency \ninformation has such a large impact on performance. However, one of the main problems with beat tracking \nin audio is to locate the “likely beat events” that guide \nthe beat tracker. Real data is full of sonic events that are \nnot on actual beats and tend to distract the beat tracker. \nBy imposing structural consistency rules, perhaps \n“random” events are averaged out, essentially bringing \nthe law of large numbers into play: structural \nconsistency considers more information and ultimately \nallows for better decisions. \nTable 1. Performance of basic beat tracker and \nbeat tracker using music structure information. \n Basic \nTracker Tracker Using \nMusic Structure \nPercentage tracked 30 59 \nNumber tracked at \nleast 95% correct 2 4 \nNumber tracked at \nleast 85% correct 2 6 \n \nAnother advantage of music structure is that by \npropagating good tempo information to new “islands,” \nthe beat tracker can more successfully approach regions \nof uncertainty between the islands. Looked at another \nway, regions that are difficult to track do not have as \nmany opportunities to “throw off” the beat tracker to the \nextent that it cannot recover the correct tempo later in \nthe song. To further isolate this factor, one could use the \nislands to determine the order in which beat tracking is \nperformed, but ignore the structural consistency function \nsc when optimizing the period vectors. \n7.1 Absolute Quality of Beat Tracker \nOne possible criticism of this work is that if the basic \nbeat tracker had better performance, structural \nconsistency might not be so useful. Are we seeing great \ntracking improvement because the basic tracker is \nentirely inadequate? The basic beat tracker is based on \nrecent published work that claims to be successful. \nReaders should recognize that correlating the beat \npattern function with beat events is closely related to \nautocorrelation and wavelet techniques used by other \nbeat induction programs [1] to detect periodicity. My \nmethod of widening the beat pattern window and then \noptimizing the beat period vector is closely related to \nother methods of entrainment for beat tracking. While \nwe do not have shared standards for measuring beat-\ntracking performance, it seems likely that any technique \nthat can substantially improve the basic beat tracker will \noffer some improvement to most others.  \nFor comparison, Scheirer’s beat tracker [15] was used \nto identify beats in the same test set of songs. The results \nare difficult to interpret because Scheirer’s program \ndoes not actually fit a single smooth tempo map to the \ndata. Instead, there are multiple competing internal \ntempo hypotheses that can switch on or off at any time. \nAs a result, the output beats are often correct even when \nthere is no underlying consistent tempo. In many cases, \n371  \n \nhowever, it seems that a little post-processing could \neasily recover a steady tempo. Giving the output this \nsubjective benefit of the doubt, Scheirer’s tracker \ncorrectly tracked about 60% of the songs. This is \nsignificantly better than my baseline tracker, and \nessentially the same as my tracker using music structure.  \nThis may indicate that the baseline tracker could be \nimproved through tuning. It may also indicate that \nsearching for periodicity independently in different \nfrequency bands (as in the Scheirer tracker) is \nadvantageous. A third possibility is that using \ncontinuous features rather than discrete peaks may be \nimportant; however, modifying the baseline tracker to \nuse continuous hfc values appears not to make any \nsignificant difference. Much more investigation is \nneeded to understand the many factors that affect beat \ntracker performance in general. This investigation was \ndesigned to explore only one factor, the use of music \nstructure, while keeping other factors the same. \n7.2 The Non-Causal Nature \nThis algorithm is non-causal. It searches for a strong \nbeat pattern as a starting point and expands from there. \nWhen music structure is considered, the algorithm jumps \nto similar musical passages before considering the rest \nof the music. Certainly, human listeners do not need to \nperform multiple passes over the music or jump from \none location to another. However, musical memory and \nfamiliarization are part of the listening process, and \ncomposers use repetition for good reasons. Although \ninspired by intuitions about music listening, this work is \nnot intended to model any more than a few interesting \naspects of music cognition. \n7.3 Other Comments \nBecause the goal of this work was to explore the use \nof structure in beat tracking, I did not try the system on \njazz or classical music, where the repetitions required \nfor structure detection are less common. Most of the test \nset is music with drums. Further work will be needed to \nexpand these ideas to work with different types of music \nand to evaluate the results. \nThe main goal of this work is to show that music \nstructure and other high-level analysis of music can \ncontribute to better detection of low-level features. \nUltimately, there should be a bi-directional exchange of \ninformation, where low-level features help with high-\nlevel recognition and vice-versa. For example, beat and \ntempo information can help to segment music, and \nmusic segmentation [16-20] can in turn help to identify \nmetrical structure. Metrical structure interacts closely \nwith beat detection. One of the fascinating aspects of \nmusic analysis is the many levels of interconnected \nfeatures and structures. Future automatic music analysis \nsystems will need to consider these interconnections to \nimprove performance. This work offers a first step in \nthat direction. 8 SUMMARY AND CONCLUSIONS \nTwo beat-tracking algorithms were presented. Both use \nhigh frequency content to identify likely beat events in \naudio data. The first is a basic algorithm that begins by \nsearching for a good fit between the likely beat event \ndata and a windowed periodic “beat pattern” function. \nAfter establishing an initial tempo and phase, the beat \npattern window is gradually widened as gradient descent \nis used to find a smoothly varying tempo function that \nmaps likely beat events to predicted beat locations. \nA second algorithm is based on the first, but adds the \nadditional constraint that similar segments of music \nshould have corresponding beats and tempo variation. \nThe beat tracking algorithm is modified to incorporate \nthis heuristic, and testing shows a significant \nperformance improvement from an average of 30% to an \naverage of 59% correctly tracked. \nThis work is based on the idea that human listeners \nuse many sources of information to track beats or tap \ntheir feet to music. Of course, low-level periodic audio \nfeatures are of key importance, but also high-level \nstructure, repetition, harmonic changes, texture, and \nother musical elements provide important “musical \nlandmarks” that guide the listener. This work is a first \nstep toward a more holistic approach to music analysis \nand in particular, beat tracking. I have shown that \nmusical structure can offer significant performance \nimprovements to a fairly conventional beat tracking \nalgorithm. It is hoped that this work will inspire others \nto pursue the integration of high-level information with \nlow-level signal processing and analysis to build more \ncomplete and effective systems for automatic music \nunderstanding. \n9 ACKNOWLEDGEMENTS \nThe author would like to thank the Carnegie Mellon \nSchool of Computer Science where this work was \nperformed. \nREFERENCES \n[1] Gouyon, F. and Dixon, S. \"A Review of Automatic \nRhythm Description Systems\", Computer Music Journal, \n29, 1, (Spring 2005), 34-54. \n[2] Masri, P. and Bateman, A. \"Improved Modeling of Attack \nTransients in Music Analysis-Resynthesis\", Proceedings \nof the 1996 International Computer Music Conference, \nHong Kong, 1996, 100-103. \n[3] Davies, M.E.P. and Plumbley, M.D. \"Causal Tempo \nTracking of Audio\", ISMIR 2004 Fifth International \nConference on Music Information Retrieval Proceedings, \nBarcelona, 2004, 164-169. \n[4] Jensen, K. and Andersen, T.H. \"Beat Estimation on the \nBeat\", 2003 IEEE Workshop on Applications of Signal \nProcessing to Audio and Acoustics (WASPAA), New \nPalz, NY, 2003, 87-90. \n[5] Desain, P. and Honing, H. \"The Quantization of Musical \nTime: A Connectionist Approach\", Computer Music \nJournal, 13, 3, (Fall 1989), 55-66. \n372  \n \n[6] Goto, M. and Muraoka, Y. \"Music Understanding at the \nBeat Level: Real-Time Beat Tracking of Audio Signals\", \nin Rosenthal, D. and Okuno, H. eds. Computational \nAuditory Scene Analysis, Lawrence Erlbaum Associates, \nNew Jersey, 1998. \n[7] Goto, M. \"An Audio-Based Real-Time Beat Tracking \nSystem for Music with or without Drums\", Journal of \nNew Music Research, 30, 2, (2001), 159-171. \n[8] Alonso, M., David, B. and Richard, G. \"Tempo and Beat \nEstimation of Musical Signals\", ISMIR 2004 Fifth \nInternational Conference on Music Information Retrieval \nProceedings, Barcelona, 2004, 158-163. \n[9] Bello, J.P., Duxbury, C., Davies, M. and Sandler, M. \"On \nthe Use of Phase and Energy for Musical Onset Detection \nin the Complex Domain\", IEEE Signal Processing Letters, \n11, 6, (June 2004), 553-556. \n[10] Dannenberg, R.B. and Hu, N. \"Pattern Discovery \nTechniques for Music Audio\", Journal of New Music \nResearch, 32, 2, (June 2003), 153-164. \n[11] Bartsch, M. and Wakefield, G.H. \"Audio Thumbnailing of \nPopular Music Using Chroma-based Representations\", \nIEEE Transactions on Multimedia, 7, 1, (Feb. 2005), 96-\n104. \n[12] Hu, N., Dannenberg, R.B. and Tzanetakis, G. \"Polyphonic \nAudio Matching and Alignment for Music Retrieval\", \n2003 IEEE Workshop on Applications of Signal \nProcessing to Audio and Acoustics (WASPAA), New \nPalz, NY, 2003, 185-188. \n[13] Wakefield, G.H. \"Mathematical Representation of Joint \nTime-Chroma Distributions\", International Symposium on Optical Science, Engineering, and Instrumentation, \nSPIE'99, Denver, 1999. \n[14] Dannenberg, R.B. \"Machine Tongues XIX: Nyquist, a \nLanguage for Composition and Sound Synthesis\", \nComputer Music Journal, 21, 3, (Fall 1997), 50-60. \n[15] Scheirer, E. \"Tempo and Beat Analysis of Acoustic Music \nSignals\", Journal of the Acoustical Society of America, \n104, (January 1998), 588-601. \n[16] Tzanetakis, G. and Cook, P. \"Multifeature Audio \nSegmentation for Browsing and Annotation\", Proceedings \nof the Workshop on Applications of Signal Processing to \nAudio and Acoustics (WASPAA), New Paltz, NY, 1999. \n[17] Logan, B. and Chu, S. \"Music Summarization Using Key \nPhrases\", Proceedings of the 2003 IEEE International \nConference on Acoustics, Speech, and Signal Processing \nProceedings (ICASSP 2000), Istanbul, Turkey, 2000, II-\n749-752. \n[18] Foote, J. \"Automatic Audio Segmentation Using a \nMeasure of Audio Novelty\", Proceedings of the \nInternational Conference on Multimedia and Expo (ICME \n2000), 2000, 452-455. \n[19] Aucouturier, J.-J. and Sandler, M. \"Segmentation of \nMusical Signals Using Hidden Markov Models\", \nProceedings of the 110th Convention of the Audio \nEngineering Society, Amsterdam, The Netherlands, 2001. \n[20] Peeters, G., Burthe, A.L. and Rodet, X. \"Toward \nAutomatic Audio Summary Generation from Signal \nAnalysis\", ISMIR 2002 Conference Proceedings, Paris, \nFrance, 2002, 94-100. \n \n373"
    },
    {
        "title": "A Simulated Annealing Optimization of Audio Features for Drum Classification.",
        "author": [
            "Sven Degroeve",
            "Koen Tanghe",
            "Bernard De Baets",
            "Marc Leman",
            "Jean-Pierre Martens"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417311",
        "url": "https://doi.org/10.5281/zenodo.1417311",
        "ee": "https://zenodo.org/records/1417311/files/DegroeveTBLM05.pdf",
        "abstract": "Current methods for the accurate recognition of instruments within music are based on discriminative data descriptors. These are features of the music fragment that capture the characteristics of the audio and suppress details that are redundant for the problem at hand. The extraction of such features from an audio signal requires the user to set certain parameters. We propose a method for optimizing the parameters for a particular task on the basis of the Simulated Annealing algorithm and Support Vector Machine classification. We show that using an optimized set of audio features improves the recognition accuracy of drum sounds in music fragments. Keywords: drum classification, Mel Frequency Cepstral Coefficients, Support Vector Machine, Simulated Annealing 1",
        "zenodo_id": 1417311,
        "dblp_key": "conf/ismir/DegroeveTBLM05",
        "keywords": [
            "Simulated Annealing",
            "Support Vector Machine",
            "drum classification",
            "Mel Frequency Cepstral Coefficients",
            "Recognition Accuracy",
            "audio features",
            "optimizing parameters",
            "music fragments",
            "classification",
            "drum sounds"
        ],
        "content": "A Simulated Annealing Optimization of Audio Features\nfor Drum Classiﬁcation\nSven Degroeve1, Koen Tanghe2, Bernard De Baets1,\nMarc Leman2and Jean-Pierre Martens3\n1Department of Applied Mathematics, Biometrics and ProcessControl, Ghe nt University, Belgium\n2Department of Musicology (IPEM),Ghent University, Belgium\n3Department ofElectronics and Information Systems(ELIS),Ghent Un iversity, Belgium\nSven.Degroeve@UGent.be\nABSTRACT\nCurrent methods for the accurate recognition of instru-\nments within music are based on discriminative data de-\nscriptors. These are features of the music fragment that\ncapture the characteristics of the audio and suppress de-\ntails that are redundant for the problem at hand. The ex-\ntractionofsuchfeaturesfromanaudiosignalrequiresthe\nuser to set certain parameters. We propose a method for\noptimizingtheparametersforaparticulartaskonthebasis\noftheSimulatedAnnealingalgorithmandSupportVector\nMachine classiﬁcation. We show that using an optimized\nsetofaudiofeaturesimprovestherecognitionaccuracyof\ndrum sounds in music fragments.\nKeywords: drum classiﬁcation, Mel Frequency Cep-\nstral Coefﬁcients, Support Vector Machine, Simulated\nAnnealing\n1 INTRODUCTION\nWith the tremendous growth of the amount of digital mu-\nsic available either locally or remotely through networks,\nMusic Information Retrieval (MIR) has become a topic\nthat has attracted the attention of researchers in a wide\nrange of disciplines. An important part of MIR research\nis concerned with automatic methods for (musical) audio\ncontent description.\nAutomatic localization and classiﬁcation of the per-\ncussivecontentofmusicalaudiocanbeemployedinvari-\nousways. Therecognitionofisolateddrumsoundswould\nbe beneﬁciary for the organization of sample libraries\nwhile the more challenging task of transcribing mixtures\nof percussive sounds (such as drum loops, break beats or\ncomplete drum tracks) can assist in the process of music\nproduction. If the percussive content of complete songs\n(containing other instruments, voices and audio effects)\ncan be analyzed, this information can be used for the de-\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrstpage.\nc/circlecopyrt2005 Queen Mary, University of Londontermination of beat/tempo and genre/style.\nDigital audio corresponds to a very high data rate,\ne.g. 88Kbyte/s for mono CD quality. Current instrument\nrecognition methods require the extraction of discrimina-\ntive data descriptors, known as features. These features\nrepresent one speciﬁc property of the signal. They cap-\nture the characteristics of the audio and suppress details\nthat are redundant for the problem at hand.\nOne set of features that is known to provide valuable\ninformation for the recognition of music are the Mel Fre-\nquency Cepstral Coefﬁcients (MFCC). They were shown\ntobeappropriatefore.g. music/speechclassiﬁcation(Lo-\ngan, 2000; Tzanetakis and Cook, 2002; West and Cox,\n2004). Theyarealsointerestingforcomplexmusicanaly-\nsisbecausetheycombinelow-dimensionalityandtheabil-\nity to discriminate between different spectral contents.\nRecent studies in which MFCC features are compared to\nother signal representations have shown the potential of\nMFCC features for speaker/sound recognition and audio\nsegmentation (McKinney and Breebaart, 2004; Kim and\nSikora, 2004).\nThe extraction of MFCC features from an audio sig-\nnal requires the user to set certain parameters such as the\nlength of the windows used to extract the information. A\ndetailed description of these parameters is given in Sec-\ntion 3.1. Until now people have used values for these pa-\nrameters that seem intuitively acceptable. In this researc h\nwe investigate the impact of optimizing these parameter\nvalues for the recognition of drum sounds using a Linear\nSupport Vector Machine algorithm.\nThe rest of this paper is organized as follows. Sec-\ntion 2 describes the drum data sets used in the experi-\nments. The optimization procedure we propose is based\non the Simulated Annealing (SA) algorithm (Kirkpatrick\net al., 1983) and is described in Section 3. Section 4\npresents experimental results demonstrating the inﬂuence\nofanoptimizedsetoffeatureparametervaluesonthetask\nof drum recognition.\n2 DATA\nThe musical data used for this paper was collected from\nvariouscommercialmusicCDs,mostlyfrompopulargen-\nres. We chose to use ‘real, fully produced music’ because\nthat is exactly the type of music that will be handled by\nmusic information retrieval systems, which is the appli-\n482cation domain of our research. We selected 49 fragments\nof30secondslengtheachin16bit44100HzstereoPCM\nWAVformatandaskedexperienceddrummerstoannotate\nthedrumsinthesefragments(Tangheetal.,2005). Anno-\ntationinvolvedlocalizingandlabelingallthedrumevents\nthat are present in the fragments. This was done through\na combination of live playing on a MIDI keyboard and\nvisualediting/correctinginastandardaudioandMIDIse-\nquencer. During the annotations, 18 different drum types\nwereavailable,butforthispaperwehavereducedthemto\nthe5maintypes: bassdrums(BD),snaredrums(SD),hi-\nhats (HH), cymbals (CY) and toms (TM). 25 music frag-\nments were randomly selected as a training set and the\nother 24 fragments as a test set. The total number of in-\nstancesforeachdrumineachsetisshownintheﬁrstthree\ncolumns of Table 1.\n3 METHOD\nThe recognition of drum sounds in an audio signal can\nbe formulated as a context classiﬁcation task. For each\ndrum sound a separate data set is created from the an-\nnotated music fragments, i.e. the task of recognizing the\ndrums in an audio signal is reduced to the recognition of\neach drum sound individually. Each data set contains one\nfeature vector for each annotated onset. A feature vector\n(known as an instance) is computed from the signal prop-\nerties found in the neighbourhood of the onset. The true\nclassiﬁcation of an instance (known as the label) depends\non the task. For instance, for the task of recognizing BD\na data set is created in which all onsets annotated as BD\nare represented by an instance labeled as positive, and all\nother onsets are represented by instances labeled as neg-\native. If more than one drum sound is annotated for the\nsame onset then this onset is used only once in the data\nset: as a positive instance if one of the annotated drums is\nthe drum sound that needs to be recognized, negative oth-\nerwise. So,foreachtaskthedatasetcontains9848feature\nvectors (knownasinstances)whereeach instancehasone\nlabel: positive or negative.\nAn inductive learning algorithm is adapted to create\na drum classiﬁcation system. Given that the data and the\ninductivelearningmethodareﬁxed,theclassiﬁcationper-\nformance of this classiﬁcation system can be used as an\nestimation of how suitable a certain feature vector repre-\nsentation is for the classiﬁcation of drums.\n3.1 Feature Vector Representation\nFor each annotated drum event a feature vector is ex-\ntracted from the signal properties found in the neighbour-\nhood of that event. This neighbourhood is deﬁned as an\ninterval of length p1(measured in milliseconds) starting\nat the onset. We will refer to the signal in this interval as\nthecontextoftheonset. Foreachdrumsound,allfeatures\nwill be computed from this context. As such, the length\np1inﬂuencesthevalueofallthefeaturesdescribedbelow\nand it is the ﬁrst parameter that requires optimization.\nThe amplitude of the onset context is described by\nmeans of a Root Mean Square (RMS) formula. When\ninspecting the accumulated spectra of hundreds of bassdrums, snare drums and hihats, it can be seen that the\nspectralenergydistributionsofthesesoundsarelocatedi n\nmore or less distinct frequency bands (although not com-\npletely separated). Hence we divide the spectrum into\nthree frequency bands and compute energy-related fea-\ntures over these bands: RMS in the whole signal; RMS\nper frequency band; ratio RMS per band to overall RMS;\nandRMSperﬁlterbandrelativetoRMSofotherbands(1\nto 2, 1 to 3 and 2 to 3).\nThe temporal nature of the onset context is described\nby the following features: Zero Crossing Rate (ZCR):\nnumberoftimespersecondthesignalchangessign;Crest\nFactor: ratio of the maximum signal amplitude value to\nthe RMS of the signal; and Temporal Centroid: the cen-\nter of gravity of the power values of the samples in the\nsegment.\nThe spectral features of the onset context are com-\nputed from a Fast Fourier Transform (FFT) computed on\nthe whole onset context signal. The following features\nare added to the feature vector: Spectral Centroid: the\ncentre of gravity of the power spectrum; Spectral Skew-\nness: the third order central moment of the power spec-\ntrum; Spectral Kurtosis: the fourth order central moment\nof the power spectrum; Spectral Flatness: the ratio of the\ngeometricmeantothearithmeticmeanofthepowerspec-\ntrum; and Spectral Rolloff: The value Rsuch that\nR/summationdisplay\ni=1P[fi] = 0.85N/summationdisplay\ni=1P[fi]\nwhere P[fi]is the power value for the frequency at bin i\nandNis the number of frequency bins.\nAnothersetoffeaturesaretheMFCCsandtheirderiv-\natives. The MFCCs are derived from a sequence of ﬁxed\nlengthaudioframes;theﬁrstframestartsatthedrumonset\nt0,andthelastoneendsat t0+p1,andconsecutiveframes\nare shifted over a ﬁxed length frame step. For each frame\nthe MFCCs are calculated using the following FFT-based\nmethod: applyaHanningwindowtotheaudioframe,per-\nform an FFT, apply a triangular shaped Mel ﬁlter bank\nto the FFT bin values and sum the results per band, (op-\ntionally) apply the log operator to the ﬁlter outputs and\nﬁnally apply a Discrete Cosine Transform (DCT). In or-\nder to capture the temporal changes of the MFCCs, we\nalso calculate their ﬁrst and second order deltas. The fea-\ntures we considered are the mean and standard deviation\nfor each of these values over the whole onset context.\nSeveral parameters come into play when calculating\ntheseMFCC-relatedfeatures,andtogetherwiththeabove\nmentionedcontextlength p1,thesearetheparametersthat\nwerevariedduringtheoptimization(seealsoTable2). Pa-\nrameter p2speciﬁesthewidthoftheaudioframes(inmil-\nliseconds)forwhichthespectrumiscalculated,parameter\np3speciﬁes the frame step (in milliseconds) and parame-\nterp4speciﬁesthesizeoftheFFT.FortheMelﬁlterbank,\nthenumberofﬁlterscanbechosen(parameter p5)anditis\npossibletonormalizetheFFTbinweightssothatthetotal\nweight is the same for each ﬁlter (parameter p6). Fur-\nthermore,parameter p7speciﬁeswhetherthelogarithmof\neach ﬁlter band output should be taken or not, and para-\nmeter p8speciﬁes the number of coefﬁcients that should\n483Table 1: Data set statistics and baselines. For each drum classiﬁcation task the table s hows the number of instances in the training set\n(25 music fragments) in the second column and the number of instances in the test set (24 music fragments) in the third column. For\nboththeformatis(positives/negatives). Thefourthcolumnshowsthep recisionratioforabaselineclassiﬁerthatclassiﬁesallinstances\nin the test set as positive (recall=1). The ﬁfth column shows the FN5% ratio obtained on the test set using an LSVM trained on the\ntraining set with all features except for the MFCC features ( p1= 0.1). For the ﬁfth column results shown in bold indicate a statistical\nsignifcant difference as compared to the baseline method. The last colu mn shows the FN5% ratio obtained on the test set using an\nLSVM trained on the training set with all features extracted using default pa rameter settings as shown in Table 2. For the last column\nresultsin bold indicate a statisticalsigniﬁcant difference as compared to no t usingthe MFCCfeatures (column ﬁve).\ndrum train test baseline no MFCC default MFCC\nBD972/3334 1230/4310 22.2 49.9 52.0\nSD563/3743 919/4621 16.6 23.0 28.4\nCY 42/4264 164/5376 2.9 3.2 5.9\nHH1656/2650 2128/3412 38.4 54.0 55.2\nTM123/4183 81/5459 1.5 – –\nbe kept after the DCT. Then ﬁnally, for the calculation of\nthe derivatives, parameter p9speciﬁes the type of delta,\nandparameter p10thewindowsize(innumberofframes)\noverwhichthedeltasarecalculated(see(Youngetal.) for\ndetailed info).\n3.2 Linear Support Vector Machines\nThe drum classiﬁcation system is a Linear Support Vec-\ntor Machine (LSVM), trained by means of the inductive\nlearningalgorithmthatisexplainedinSection3.2 (Boser\netal.,1992;Vapnik,1995). TheLSVMhasbeenshownto\nperform well for the task of classifying BD and SD drum\nsounds (Van Steelant et al., 2004). The LSVM separates\nthe two classes in a data set Dusing a hyperplane such\nthat:\n(a) the “largest” possible fraction of instances of the\nsameclassisonthesamesideofthehyperplane,and\n(b) the distance of either class from the hyperplane is\nmaximal.\nThe algorithm has a parameter Cthat needs to be set\nby the user and regulates the effect of outliers and noise,\ni.e. it deﬁnes the meaning of the word “largest” in (a).\nFor the induction of the LSVM we used SVMlight5.0\n(Joachims,1999)1inclassiﬁcationmodewithallparame-\nters at their default values, except for the cost parameter\nC, which will be optimized fromthe data.\n3.3 Measure of Classiﬁcation Performance\nThe classiﬁcation performance of an LSVM on a data set\nDis measured in terms of recall and precision. Recall\nquantiﬁestheproportionofpositivevectorsthatareclass i-\nﬁed correctly while precision quantiﬁes the proportion of\npositive classiﬁcations that are correct. Both are require d\ntoaddresstheperformanceoftheclassiﬁcationsystem. To\nallow for an automated optimization procedure, we quan-\ntifytheperformanceofanLSVMbyitsprecisionata95%\nrecall, i.e. we allow for only 5% false negative classiﬁca-\ntions. The precision measure is referred to as the FN5%\nratio (5% False Negatives).\n1http://svmlight.joachims.org/Giventhedataset Dtheclassiﬁcationperformanceof\nanLSVMinducedclassiﬁcationsystemiscomputedusing\n10-fold cross-validation which divides Dinto ten parti-\ntions, uses each partition in turn as a test set and the other\nnine as the training set, and which computes evaluation\ncriteria as averages over the ten test sets. The partitions\nwe use are equally sized, and have the same class distrib-\nution.\nMcNemar’s test is applied to decide whether any ap-\nparent difference in error rates between two algorithms\n(feature vector representations in our experiments) teste d\non the same set of music fragments is statistically signiﬁ-\ncant (Dietterich, 1998). The test uses those classiﬁcation s\nthat are correct for only one of the algorithms ( n01and\nn10). Let h0be the hypothesis that the underlying error\nrates are the same. Then under h0an error is as likely\nto be made by either of the two algorithms and the dis-\ntribution of n01andn10is the distribution obtained when\ntossing a fair coin. This is a binomial distribution and the\np-valuesareeasilyobtainedfromtables. Resultsthathave\nap-valuegreaterthan0.05arenotstatisticallysigniﬁcant.\n3.4 Optimization Strategy\nSimulated Annealing (SA) is an optimization algorithm\nbased on a Metropolis Monte Carlo simulation. The goal\nis to ﬁnd a parameter setting sminin the space of all can-\ndidate settings Sfor which a real-valued energy function\nEis minimized. The algorithm performs a series of ran-\ndom walks through Sat successively lower temperatures\nT, where the probability Pof making a step from soldto\nsnewis given by a Boltzmann distribution:\nP=/braceleftbigg1if∆E≤0\ne−∆E\nTotherwise(1)\nwith\n∆E=E(snew)−E(sold). (2)\nThe function E(s)in our optimization strategy quantiﬁes\nthe classiﬁcation error of an LSVM induced in a feature\nspace deﬁned by s. To avoid features in greater numer-\nical ranges to dominate those in smaller ranges, the data\nis scaled such that every feature lies within the range of\n[−1,1]. Let the function 10CV(s,c)return the FN5%\n484Table 2: Feature vector parameters. For each parameter piin the ﬁrst column the table shows the default value in the second col-\numn. The third column shows the values considered during the optimization. The fourth column provides a short description of the\nparameters. The last ﬁve columns show optimized parameter settings for each of the drum sounds.\nParameter Default Values Description BDSDCYHHTM\np1 0.1 grid search onset context length 0.10.1 0.14–\np2 0.02 {0.01,0.02,0.03}FFT window length 0.030.03 0.03–\np3 0.01 {0.005,0.01,0.015}FFT window interval 0.010.005 0.01–\np4 1024 {1024,2048,4096}FFTsize 20481024 4096–\np5 40 [5,40] number of ﬁlters 3731 34–\np6 1 {0,1} normalize 10 0–\np7 1 {0,1} logarithm 10 0–\np8 13 [1,p5] number of coefﬁcients 1622 30–\np9 0 {0,1} delta type 00 0–\np10 2 {1,2,3,4} delta window length 43 3–\nTable 3: Optimal classiﬁcation performance for all SA procedures. Each row r epresents one of the ﬁve drum classiﬁcation tasks. The\ncolumns represent values for p1(in milliseconds). All results (FN5% ratio) are obtained on the test set. Resu lts in bold represent the\nbest performing context lengths forthe training set.\n0.060.080.100.120.140.160.180.200.220.240.26\nBD47.9–51.6–51.4––––––\nSD27.0–27.8–25.7––––––\nCY4.1–6.45.56.0–5.98.98.98.96.1\nHH55.3–55.255.554.855.0–––––\nTM–––––––––––\nratio of a 10-fold cross-validation procedure in a feature\nspace deﬁned by susing an LSVM with cost C=c, then\nthe energy E(s)is computed as:\nE(s) = 1 −max\nc10CV(s,c). (3)\nWe chose to optimize Cwithin each computation of\nE, as opposed to adding Cto the set of parameters (Ta-\nble 2) that are optimized within the SA procedure. The\nreason is that most of the computation time required for\ncomputing Eisspentattheextractionofthefeaturesfrom\nthe music fragments, and not at the cross-validation pro-\ncedure. Assuch,optimizing Cwithintheenergyfunction\ncan be done at littleadditional computation cost.\nThis computation cost is further reduced by par-\nallelizing the SA search using a grid search based on\nthe onset context length p1. This is done by perform-\ning three SA optimizations (optimizing all parameters\nexcept p1), one for p1= 0.06, one for p1= 0.1, and\none for p1= 0.14. Ifp1=vis the best performing\ncontext length, then two other SA optimizations are\nperformed for p1=v−0.02andp1=v+ 0.02.\nThis process is repeated until no further improvement is\nobserved. TheSAprocedurewasimplementedasfollows:\n(1) initialize sold\n(2)T= 0.043\n(3) repeat 100 times\n(4) repeat 10 times\n(5) snew=step(sold)\n(6) sold=snewwith probability P\n(7) T=T\n1.05Theprobability Pin(6)dependson Tandiscomputedas\ninEq.(1). Theprocedure stepin(5)randomlyselectsone\nof the features from {p2,... ,p 10}and changes the value\nofthisparameteratrandom. Fortheparameters p5andp8\nthe random increase or decrease was limited to 5 to keep\nthe step local.\nIt is known that choosing proper values for the vari-\nous SA parameters is hard and depends very much on the\ncharacteristics of the energy landscape deﬁned by S. Ini-\ntializing T= 0.043means that during the ﬁrst iterations\na step is taken with P= 0.5when∆E= 0.03. As the\ncomputational cost for computing the energy function is\nhigh,thenumberofSAiterationsislimitedto100,which\nresults in a total of 1000 energy computations for each\ndrum data set and each context length p1considered dur-\ning the grid search.\n4 RESULTS\nAllLSVMclassiﬁersinthefollowingexperimentsarein-\nduced from the training set. The results shown in the ta-\nbles are obtained on the test set. LSVM parameter Cwas\noptimized using 10-fold cross-validation on the training\nset. The signiﬁcance of the differences between the re-\nsults is computed using McNemar’s test.\nIn a ﬁrst experiment we evaluated the importance\nof MFCC features for the task of drum classiﬁcation.\nWe evaluated the classiﬁcation performance of an LSVM\ncomputed from feature sets including and excluding the\nMFCC features with default parameter settings (p1=\n0.1). These are values often used in literature and are\nshown in column 3 of Table 2 (Logan, 2000). The per-\nformance was measured as the FN5% ratio. The results\n485werecomparedtoabaselineclassiﬁerthatclassiﬁesallin-\nstancesaspositive,i.e. recall=1. Theseresultsareshown\nin the last three columns of Table 1. For HH and TM, the\nbaseline method does not perform worse than the LSVM\nclassiﬁer that uses all but the MFCC features. Adding the\nMFCC features improves the classiﬁcation signiﬁcantly\nfor SD: the number of of false drum classiﬁcations (false\npositives) decreased by 21.5%. Also for HH and TM we\nobserve minor improvements.\nNext, the SA optimization method was executed on\nthe 25 music fragments in the training set. The proce-\ndurewasrunforeachoftheﬁvedrumclassiﬁcationtasks.\nThisresultedinanoptimalparametersettingforeachcon-\ntextlength p1testedduringthegridsearchprocedure(de-\nscribed in Section 3.4) for each of the ﬁve drum sounds.\nWethentestedtheseoptimalparametersettingsonthetest\nset. This was done for each drum sound by computing\ntheFN5%testsetclassiﬁcationperformanceofanLSVM\ninduced from the training set using the optimal parame-\nter settings for that drum sound. Table 3 shows the op-\ntimal FN5% ratio for each onset context length p1and\neach drum sound. For each row one result is shown in\nbold. The column associated with the result in bold rep-\nresents the context length p1that showed the best 10-fold\ncross-validationperformanceonthetrainingsetattheend\nof each SA procedure. For BD, SD and CY the SA pro-\ncedure ﬁnds parameter settings that perform signiﬁcantly\nbetter than the default ones that are shown in Table 2. For\nBD the number of false drum classiﬁcation decreased by\n28.5%, for CY this was 24.4%. Also, for BD and CY\nwe observe that when using optimized audio features, the\naddition of MFCC features does actually improve classi-\nﬁcation performance. This was not the case when using\nthedefaultfeatureparametersettings. IfweuseTable3to\nevaluate the ﬁnal solutions found by the SA optimization\nprocedure,thenwenoticethattheprocedurefailedtoﬁnd\nthe best parameter settings for most of the tasks.\nFigure 1 summarizes the SA procedure for the best\nperforming(onthetrainingset)contextlengths. Theopti-\nmalparametersettingsforeachofthedatasetsareshown\ninthelastﬁvecolumnsofTable2. FromFigure1wecon-\nclude that, given the limitation of 100 iterations for each\nSA run, the SA algorithm converges nicely at around 750\nenergy evaluations. The ﬁgure also indicates that, next\ntop1, other parameters have an impact on the drum clas-\nsiﬁcation performance of the LSVM. If we look at the\noptimal parameter settings in Table 2 we notice that the\nboolean parameter p7has the same optimal value for all\nﬁve drum sounds. Also p2shows a stable behaviour as\nthe length of the FFT windows is preferred to be larger.\nAll other parameters can differ signiﬁcantly between the\ntasks.\nNext, we evaluated the impact of each optimal para-\nmeter value on the classiﬁcation performance individu-\nally. For each pi(i= 2...10)we quantiﬁed the drum\nclassiﬁcation performance using the optimal parameter\nvaluesfoundbytheSA,butwith pisettoitsdefaultvalue\nif this was not already the case. Again all training was\ndoneonthetrainingsetandtheFN5%ratiosshowninthe\nTable 4 are computed on the test set. We observe that\nslight changes (setting parameters back to their defaultTable4: Impactofsettingeachparameter pitoitsdefaultvalue\n(Table 2) in the optimal parameter setting for each of the drum\nsounds. The results shown are the FN5% ratios obtained using\n10-foldcross-validationonthetestset. Resultsinboldshowdif-\nferencesthatarestatisticallysigniﬁcant (p <0.05)ascompared\nto using the optimal parameter settings found by the SA proce-\ndure. The symbol ’-’ means that this parameter was already set\nto itsdefault value.\nBDSDCYHHTM\np249.426.49.154.6-\np3-27.39.1--\np451.326.9-54.7-\np551.1-9.554.6-\np6-24.98.954.6-\np7-27.1-54.4-\np852.927.28.754.6-\np9-----\np1050.727.8-54.9-\nvalues) in the optimal parameter settings can improve the\nclassiﬁcation of SD, CY and TM further. The SA opti-\nmization procedure proposed in this paper does not nec-\nessarily ﬁnd the optimal solution, but it does ﬁnd good\nsub-optimalsolutionsthatimprovedrumclassiﬁcationac-\ncuracy.\nIn a ﬁnal experiment we wanted to evaluate to what\nextent the (sub-)optimal parameter settings for each drum\nsound are speciﬁc to this drum classiﬁcation task. As the\nSA algorithm does not ﬁnd the optimal solution, there\nmightbemorethanonesub-optimalsolution. Thismeans\nthattheparametersettingsfoundforacertaindrumsound\nmight also work well for other drum classiﬁcation tasks.\nTo check this, we compared optimal parameter settings\nbetween the different drum tasks. For each pair of drum\nsounds(X,Y)weevaluatedtheclassiﬁcationperformance\nonthetaskofrecognizingdrumXwithanLSVMinduced\nusingtheoptimalparametersettingsofdrumsoundY.Per-\nformance was again measured as the FN5% ratio com-\nputed on the test set. Table 5 shows the results with drum\nsound X as rows and drum sound Y as columns. For all\n 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1\n 0  20  40  60  80  100energy value E\nSA iterationsBD\nSD\nCY\nHH\nTM\nFigure1: SummaryoftheSAprocedure. Thex-axisrepresents\nthe number of SA iterations. It shows the energy of the current\nparametersettingeverytimethetemperatureisdecreasedinstep\n(4) of the SA procedure. The y-axis shows the energy of the\ncurrent parameter setting.\n486Table 5: A comparison of different optimal parameter settings\nfor the classiﬁcation of different drum sounds. Each row repre-\nsentsadrumclassiﬁcationtask. ValuesthenrepresenttheFN5%\nperformanceratiosobtainedusingtheoptimalparametersettings\nfor the drum soundsin the associated columns.\nBDSDCYHHTM\nBD31.932.933.033.232.9\nSD20.222.122.322.622.2\nCY4.13.44.03.74.4\nHH38.639.238.038.439.1\nTM1.51.61.71.61.6\ntasks we observe statistically signiﬁcant differences be-\ntween the different sub-optimal parameter settings. But\nthese differences are not all in favour of the SA optimiza-\ntion procedure. For instance, for CY we ﬁnd that using\nthe TM optimal parameter settings increases performance\nto4.4%,ascomparedto4.0%whenusingtheCYoptimal\nparameters found using the SA procedure. When using\ntheseoptimizedaudiofeaturesinarealsituationinwhich\nall instruments in an audio signal need to be recognized,\nother performance criteria arise. Using different feature\nextraction parameter settings for each of the instruments\nthat need to be classiﬁed increases the computation cost\nfor analyzing the signal signiﬁcantly. The optimal para-\nmeter setting for the TM classiﬁcation task (last column\nofTable5)performswellforallﬁvetasks. Usingthisset-\nting for all ﬁve drum sounds would drastically reduce the\ncomputation cost.\n5 CONCLUSIONS\nIn this study we evaluated the importance of using opti-\nmizedaudiofeaturesfordrumclassiﬁcationinmusicfrag-\nments. We proposed an optimization procedure based on\nSimulated Annealing and we have shown that the feature\nparameters evaluated in the research have a statistically\nsigniﬁcant impact on the drum sound classiﬁcation per-\nformance. We believe that this should be taken into ac-\ncount when comparing the use of different types of audio\nfeatures. When using non-optimized features, the results\nmight not fully capture the true discriminative potential.\nWe have also shown that the optimization procedure\nproposedinthisresearchdoesnotalwaysﬁndtheoptimal\nsolution. Better optimization algorithms should be inves-\ntigated, but the optimization task is hard because of the\nlongcomputationtimerequiredtoevaluateoneaudiofea-\nture parameter setting.\nACKNOWLEDGEMENTS\nThis work was done in the context of the Musical Audio\nMining (MAMI) project, which is funded by the Flemish\nInstituteforthePromotionofScientiﬁcandTechnological\nResearchinIndustry. TheauthorswishtothankMicheline\nLesaffre, Liesbeth De Voogdt and Dirk Van Steelant for\ntheir contribution to the creation of the music fragments\ndata set.References\nB.E. Boser, I. Guyon, and V. Vapnik. A training algo-\nrithmforoptimalmarginclassiﬁers.In Proc.oftheFifth\nAnnual Workshop on Computational Learning Theory ,\npages 144–152, 1992.\nT.G. Dietterich. Approximate statistical tests for com-\nparing supervised classiﬁcation learning algorithms.\nNeural Computation , 10:1895–1924, 1998.\nT. Joachims. Making large-scale SVM learning practical.\nIn B. Sch ¨olkopf, C. Burges, and A. Smola, editors, Ad-\nvances in Kernel Methods – Support Vector Learning ,\npages 169–185. MIT Press, 1999.\nH.-G. Kim and T. Sikora. Comparison of MPEG-7 au-\ndio spectrum projection features and MFCC applied to\nspeakerrecognition,soundclassiﬁcationandaudioseg-\nmentation. In Proc. of the IEEE International Con-\nference on Acoustics, Speech, and Signal Processing\n(ICASSP 2004) , Montreal, Canada, 2004.\nS. Kirkpatrick, C.D. Gelatt, and M.P. Vecchi. Optimiza-\ntion by simulated annealing. Science, (2):671–680,\n1983.\nB. Logan. Mel frequency cepstral coefﬁcients for music\nmodeling. In Proc. of the Internatonal Conference on\nMusic Information Retrieval (ISMIR 2000) , Plymouth\nMA, 2000.\nM.F. McKinney and J. Breebaart. Features for audio and\nmusic classiﬁcation. In Proc. of the Internatonal Con-\nference on Music Information Retrieval (ISMIR 2004) ,\npages 151–158, Plymouth MA, 2004.\nK. Tanghe, M. Lesaffre, S. Degroeve, M. Leman, B. De\nBaets, and J.-P. Martens. Collecting ground truth an-\nnotations for drum detection in polyphonic music. In\nProc. of the Internatonal Conference on Music Infor-\nmation Retrieval , London, 2005.\nG.TzanetakisandP.Cook. Musicalgenreclassiﬁcationof\naudiosignals. IEEETransactionsonSpeechandAudio\nProcessing , 10(5):293–302, 2002.\nD. Van Steelant, K. Tanghe, S. Degroeve, B. De Baets,\nM. Leman, and J.-P. Martens. Classiﬁcation of percus-\nsive sounds using support vector machines. In Proc. of\nthe Annual Machine Learning Conference of Belgium\nand The Netherlands (BENELEARN) , pages 146–152,\nBrussels, 2004.\nV. Vapnik. The Nature of Statistical Learning Theory .\nSpringer-Verlag, 1995.\nK. West and S. Cox. Features and classiﬁers for the auto-\nmatic classiﬁcation of audio signals. In Proc. of the In-\nternatonal Conference on Music Information Retrieval\n(ISMIR2004) , Barcelona, 2004.\nS. Young, D. Kershaw, J. Odell, D. Ollason,\nV. Valtchev, and P. Woodland. The HTK Book .\nhttp://htk.eng.cam.ac.uk/.\n487"
    },
    {
        "title": "Automatic Prediction of Hit Songs.",
        "author": [
            "Ruth Dhanaraj",
            "Beth Logan"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417571",
        "url": "https://doi.org/10.5281/zenodo.1417571",
        "ee": "https://zenodo.org/records/1417571/files/DhanarajL05.pdf",
        "abstract": "We explore the automatic analysis of music to identify likely hit songs. We extract both acoustic and lyric information from each song and separate hits from non-hits using standard classifiers, specifically Support Vector Machines and boosting classifiers. Our features are based on global sounds learnt in an unsupervised fashion from acoustic data or global topics learnt from a lyrics database. Experiments on a corpus of 1700 songs demonstrate performance that is much better than random. The lyricbased features are slightly more useful than the acoustic features in correctly identifying hit songs. Concatenating the two features does not produce significant improvements. Analysis of the lyric-based features shows that the absence of certain semantic information indicates that a song is more likely to be a hit. Keywords: hit song detection, music classification. 1",
        "zenodo_id": 1417571,
        "dblp_key": "conf/ismir/DhanarajL05",
        "keywords": [
            "automatic analysis",
            "identify likely hit songs",
            "extract acoustic and lyric information",
            "standard classifiers",
            "Support Vector Machines",
            "boosting classifiers",
            "corpus of 1700 songs",
            "experiments",
            "performance",
            "lyric-based features"
        ],
        "content": "AUTOMA TIC PREDICTION OFHIT SONGS\nRuth Dhanaraj\nResearch Science Institute Intern\nHewlett Packard Labs\nOne Cambridge Center\nCambridge MAUSA\nruthdhan@mit.eduBeth Logan\nHewlett Packard Labs\nOne Cambridge Center\nCambridge MAUSA\nBeth.Logan@hp.com\nABSTRA CT\nWeexplore theautomatic analysis ofmusic toidentify\nlikelyhitsongs. Weextract both acoustic andlyric in-\nformation from each song andseparate hitsfrom non-hits\nusing standard classi\u0002ers, speci\u0002cally Support Vector Ma-\nchines andboosting classi\u0002ers. Our features arebased\nonglobal sounds learnt inanunsupervised fashion from\nacoustic dataorglobal topics learnt from alyrics database.\nExperiments onacorpus of1700 songs demonstrate per-\nformance that ismuch better than random. The lyric-\nbased features areslightly more useful than theacoustic\nfeatures incorrectly identifying hitsongs. Concatenat-\ningthetwofeatures does notproduce signi\u0002cant impro ve-\nments. Analysis ofthelyric-based features showsthatthe\nabsence ofcertain semantic information indicates thata\nsong ismore likelytobeahit.\nKeywords: hitsong detection, music classi\u0002cation.\n1INTR ODUCTION\nOnApril 41964 theBeatles accomplished what noband\nhadachie veduntil then; indeed what noother band has\nachie vedsince. Inaddition toholding theNo.1USA sin-\nglewith Can' tBuy MeLovetheBeatles also held the\nNo. 2slotandtheNo. 3slot. InfactBeatles songs oc-\ncupied the\u0002rst\u0002vepositions onthecharts. What exactly\nwasitthatfueled theBeatles' risetofame? Isthere an\nintrinsic quality inmusic thatpredisposes ittogreatness?\nInthispaper weexamine these questions bystudying au-\ntomatic methods toidentify hitsongs.\nThegrowthoftherecording industry hasresulted inan\nabundance ofmusic thatrequires automated methods of\norganization andclassi\u0002cation. Compression algorithms\nsuch astheMP3 \u0002leformat coupled with connecti vityto\ntheinternet andimpro vements inmass storage havecon-\ntributed tothewidespread availability ofmusic indigital\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonform. Thesizeofcollecti verepositories clearly showsa\nneed fororganization, butitisalso evident thatonly an\nautomated system isfeasible forsuch massi vecollections.\nSeveralareas ofclassi\u0002cation havebeen proposed and\nstudied. Grouping songs bygenre orartist similarity are\ntwocommon types ofclassi\u0002cation (e.g.Tzanetakis and\nCook, 2002; Berenzweig etal.,2003b). Howeverseem-\ningly little workhasbeen done onthespeci\u0002c subject of\nclassi\u0002cation andgrouping bypopularity .\nAlthough societal, cultural, andother qualitati vefac-\ntorsundoubtedly play apart insongs' popularity ,inthis\nworkwesearch forsome factor thatcanbequanti\u0002ed in\nsongs which makesthem more likelytoreach thetopof\nthecharts. That is,weassume thegroup psychology that\nmakesasong popular isnotentirely unpredictable, butis\nsomeho wbased onthequalities ofmusic thatappeal toa\nbroad spectrum ofpeople.\nDetecting popular songs hastremendous commercial\npotential; infactHitSong Science1claims tohavealready\nsucceeded. Ifproperly developed, such technology could\nhelp record companies pinpoint themost promising songs\nandartists andthus better focus their mark eting. Inthis\npaper weseek todetermine ifsuch technology isfeasible.\n2Methods\nThere isalargebody ofliterature onsong writing andin-\ndeed some books evenclaim toteach howtowrite ahit\nsong (e.g.Blume, 2004). Clearly itisn'taseasy assup-\nposed butcertainly melody ,chords, lyrics andinstrumen-\ntation play arole. Inthisstudy ,weuseaverysimple ap-\nproach. Weextract verygeneral acoustic andlyric-based\nfeatures from songs then usestandard classi\u0002ers tosepa-\nratehitsfrom non-hits. Ourmethods aredescribed below.\n2.1 Acoustic Featur es\nRawacoustic waveforms areoftoohigh dimension and\naretooredundant fordirect use. Therefore, manyfea-\ntures havebeen proposed torepresent thesalient proper -\ntiesofsongs (e.g.seeTzanetakis andCook, 2002, andref-\nerences). Forhitsong classi\u0002cation, weseek anunkno wn\nintrinsic universal quality .Wetherefore extract features\nfrom each song describing themain sounds present, where\nthese sounds arepre-learned from acorpora ofwidely\n1http://www .hitsongscience .com\n488varying music. Ourmethod issimilar tothose which con-\nvertsongs tovectors according tosounds learnt bysu-\npervised clustering (e.g.Foote, 1997; Berenzweig etal.,\n2003a). Weuseunsupervised clustering since wedon't\nknowwhich sounds would beoptimal forourclassi\u0002ca-\ntiontask.\nSimilar toprevious studies, wecharacterize sounds us-\ningMFCC features thusfocusing ontimbrel aspects ofthe\nmusic. Manyother characterizations arepossible however\nsuch asthose based onrhythmic ormelodic features. We\n\u0002rst convertthesongs inthetraining corpora toasetof\nMFCC features then useK-means clustering to\u0002ndtheN\nmost prominent clusters. Wecanthen convertanysong\ntoanN-dimensional vector representation bycomputing\nthelikelihoods ofthesound represented byeach cluster\noccuring inthatsong.\n2.2 Lyric-Based Featur es\nLyrics arethought tobealargecomponent ofwhat makes\nasong ahitsowetherefore study features based onsong\nlyrics. Inthiswork,weassume thatforeach song thatwe\ncanobtain atranscription ofthelyrics, most likelyfrom\ntheinternet. Future systems may trytoextract lyrics di-\nrectly from audio. Howeverthisisbeyond state oftheart\natpresent.\nTextcanbeanalyzed using various methods. Al-\nthough weoriginally tried features based onrepeating\nphrases -inspired bysong writing guides thatclaim hits\ncontain catch y,often repeated phrases -wefound these\ndidnotgivegood results. Instead weusefeatures which\ndescribe thesemantic content ofeach song. Each song is\nconverted toavector using Probabilistic Latent Semantic\nAnalysis (PLSA) (Hofmann, 1999). Similar totheacous-\nticfeatures described above,each component ofthevector\nrepresents thelikelihood thatthesong isabout apre-learnt\ntopic. Thetopics arecharacterized bythewords thatap-\npear frequently inthem andarelearned during anauto-\nmatic training process. Wehavepreviously shownthat\nthistechnique canbeeffectivefordetermining similarity\nbetween songs based onlyrics (Log anetal.,2004).\n2.3 Support Vector Machine Classi\u0002ers\nThe \u0002rst classi\u0002er weusetoseparate hitsfrom non-hits\nisaSupport Vector Machine (SVM) (e.g.Burges, 1998).\nSVMs arestandard classi\u0002ers used inmanyapplications.\nIntheir simplest implementation theylearn aseparating\n`thick' hyper-plane between twoclasses which maximizes\nthe`margin'. This marginroughly corresponds tothedis-\ntance between thedata points residing attheedges ofthe\nhyper-plane.\nSVMs haveseveraladvantages which makethem the\nclassi\u0002er ofchoice inmanysituations. First theydonot\nrequire anycomple xtuning ofparameters. Second they\nexhibit agreat ability togeneralize givenasmall training\ncorpora. Finally ,theyareparticularly suited tolearning in\nhigh dimensional spaces.2.4 Boosting Classi\u0002ers\nAnother popular classi\u0002cation technique isboosting\n(Schapire, 1990; Freund andSchapire, 1995). Boosting\ncombines hundreds oreventhousands ofweak learners\ninanoptimal way.These weak learners could beanyclas-\nsi\u0002er (evenSVMs) although forcomputational reasons\ntheyaretypically verysimple. Each weak learner focuses\nitsattention onthose training vectors where theprevious\nweak learners failed.\nWeuseavariant ofboosting proposed in(Tieuand\nViola, 2000) inwhich theweak learners aresimple linear\nclassi\u0002ers ononedimension. This offerstheadvantage of\nbeing lesssensiti vetospurious features. Components of\nthefeature vector thatdonotaddanyadvantage areig-\nnored attheexpense ofmore promising components. Ad-\nditionally ,weareable toanalyze therelati veimportance\nofeach feature inaprincipled way;asimple inspection of\ntheweak learners highlights those features thatcontrib ute\nmost toclassi\u0002cation.\n3Databases\nInthissection wedescribe thedatabases used inourstudy .\n3.1 Ground Truth\nSince wearenotawareofanypublicly available database\nofhitsandnon-hits, weuseasground truth data from the\nOzNetMusic Chart TriviaPage2.This sitelistsallsongs\nwhich reached theNo. 1ranking ineither theUnited\nStates, theUnited Kingdom, orAustralia since records of\nNo.1songs were kept.Weusedata from January 1956 to\nApril 2004 producing alistof4439 hitsongs. Note that\nweonly consider number No.1songs rather than sayTop\n40data.\n3.2 Acoustic Data\nWeuseanin-house database ofapproximately 18,500\nsongs asacoustic data. This data, wasobtained bypooling\nthepersonal music collections ofseveralmembers ofstaff\natHewlett Packard' sCambridge Research Lab.Thecol-\nlection coversmanygenres ranging from ReggaetoClas-\nsical although rock songs form themajority ofthecollec-\ntion, totaling around 13,000 songs.\n3.3 Lyric Data\nLyrics aremuch more easily obtained than audio data as\nseverallyrics repositories arefreely available onthein-\nternet. Howevermanyofthese arenotstandardized orin\naformat conduci vetoautomated retrie val.Additionally ,\nmanyofthese sites arenotcomprehensi veenough foran\neffectivedatabase.\nOne site with standardized pages istheAstra web\nLyrics Search site3.Weused data from thissiteinour\nexperiments. Intotal wedownloaded lyrics forabout 500\nartists, totaling around 47000 songs, although some songs\n2http://www .onmc.iinet.net.au/trivia/hit list.htm\n3http://lyrics.astr aweb .com\n489were repeated ondifferent albums. Westripped allHTML\ntags, advertising, andexcess information toobtain raw\nlyrics forthesongs.\nNote thatinmanycases, thelyrics from thissiteare\nnotnecessarily averbatim transcript ofeach song. For\nexample, sometimes repeats ofthechorus aresimply de-\nnoted bychorus ornottranscribed atall.This may have\nbeen whyourattempts toconstruct features based onre-\npeating properties ofthelyrics were lessthan successful\nsince thetranscripts didnotalwaysfaithfully represent the\nlyrics.\n3.4 Experimental Database\nForourexperiments weconsidered songs forwhich we\nhadboth lyric andacoustic data. Ofthe4000 orsohit\nsongs, weunfortunately only hadboth acoustic andlyric\ndata available for91songs. Tocomplete theexperimental\ndatabase wesampled from theremaining (non-hit) songs\nforwhich both acoustics andlyrics were available tomake\nasetofaround 1700 songs total. Ouraiminrestricting the\nnumber ofnon-hits wastoavoidaseverely unbalanced\ndatabase.\n4Experiments\nToinvestig atetheperformance ofourproposed hitsong\nclassi\u0002ers weconduct aseries ofexperiments described\nbelow.\n4.1 Featur eExtraction\nWe\u0002rstconverteach song inthe1700 song experimental\ndatabase toacoustic andlyric-based representations.\nAsdescribed earlier ,the\u0002rst step inconverting each\nsong toanacoustic representation istolearn theNmost\nprominent clusters inageneral setofaudio. Speci\u0002cally ,\nwe\u0002rst converteach song inourtraining settoaseries\nof20dimensional MFCC vectors computed from over-\nlapping 25ms windo wssampled each 10ms. Wediscard\nthe0th(DC) component ofeach vector then perform K-\nmeans clustering tolearn theNmost prominent sounds.\nForcomputational reasons, wedonotlearn these clusters\nusing thefull18,500 songs forwhich wehaveacoustic\ndata. Instead wesample from thisdatabase using around\n200songs tolearn theK-means models.\nWethen converteach song inour experimental\ndatabase toaN-dimensional vector asfollows.Asbefore,\nweconverteach song toaseries ofMFCC vectors. For\neach vector ,wescore itagainst each oftheNclusters and\nincrement acounter forthecluster which scores highest.\nThe normalized setofcounts forms theN-dimensional\nrepresentation forthatsong.\nSimilar totheacoustic case, the\u0002rststepinconverting\nsong lyrics toanN-dimensional representation istolearn\nasetoftopics from atextcorpus. Weused oursetoflyrics\nasthecorpus andafter eliminating stop words trained top-\nicsaccording tothealgorithm described in(Hofmann,\n1999) using adictionary ofaround 91,000 words. Wethen\nscore thelyrics foreach song against these models topro-\nduce anormalized vector ofcounts similar totheacoustic\ncase above.4.2 Classi\u0002cation\nWerunexperiments using 10-fold cross validation. This\nmitig atessome what theeffectofexperimenting with such\nasmall database since byaveraging over10cuts ofthe\ndata into testing andtraining sets, wereduce theimpact\nofaparticularly easy orparticularly hard set.Our\u0002gure\nofmerit foreach classi\u0002er isthearea under theRecei ver\nOperating Characteristic (ROC) curve.AnROCcurve\nplots sensiti vityvs.(1-speci\u0002city), essentially describing\nthetrade-of fbetween false negativesandfalse positi ves\nastheclassi\u0002er' sthreshold isvaried. Random classi\u0002ers\nhaveROCarea 0.5andperfect classi\u0002ers haveROCarea\n1.0.\nFigures 1and2showtheROCarea averaged over\nthe10cross validation cuts oftheexperimental database\nforSVM andboosting classi\u0002ers trained ontheacoustic-\nbased andlyrics-based features. Weshowresults forvary-\ningnumbers ofaudio sounds andtopics. Also shownfor\nreference isthe0.5ROCarea which would result from\nrandom classi\u0002cation.\n0 20 40 60 80 100 120 14000.20.40.60.81\nNumber SoundsROC area SVM     \nBoosting\nRandom  \nFigure 1:Average ROCarea foracoustic-based features\nwith various numbers ofsounds forSVM andboosting\nclassi\u0002ers\n0 20 40 60 80 100 120 14000.20.40.60.81\nNumber TopicsROC area SVM     \nBoosting\nRandom  \nFigure 2:Average ROCarea forlyric-based features with\nvarious numbers oftopics forSVM andboosting classi-\n\u0002ers\nFrom these plots weseethatevenwith thesimple fea-\ntures andclassi\u0002ers proposed, wecanachie vebetter than\nrandom performance. Wealso seethatthere theresults\narecomparable forthetwoclassi\u0002ers. Lyric-based fea-\ntures appear togiveslightly better performance overall\nthan acoustic-based features. Thebest result obtained us-\ninglyrics features isaverage ROCarea 0.68 obtained us-\ning8-topic models. This isslightly better than 0.66, the\nbest result obtained using acoustic features obtained for\nboth 32and128-sound models.\nWenowconsider combining acoustic andlyric-based\nfeatures. Weachie vethisbyconcatenating thevectors for\nthetworepresentations. Figure 3showsresults forthisex-\nperiment. Forsimplicity ,thisplotonly showsresults for\n490concatenating equal length vectors. Forexample theVec-\ntorSize 16result isobtained using feature vectors formed\nbyconcatenating length 8acoustic vectors andlength 8\nlyric vectors. Wealso investig ated non-equal combina-\ntions ofacoustic andlyric vectors andsawcomparable\nresults.\n0 50 100 150 200 250 30000.20.40.60.81\nVector sizeROC area SVM     \nBoosting\nRandom  \nFigure 3: Average ROC area for combined acoustic and\nlyrics features with varying vector sizes forSVM and\nboosting classi\u0002ers\nThe best result forcombining acoustic andlyric fea-\ntures isaverage ROCarea 0.69. This isobtained by\nconcatenating 32-sound audio features and8-topic lyric\nfeatures. This isonly moderately better than average\nROCarea 0.68, thebestresult obtained using lyrics alone.\nGiventherelati velysmall sizeoftheexperiment, itisun-\nclear then whether thecombined feature setimpro vesper-\nformance.\n5Analysis ofResults\nAsdiscussed earlier ,anice property oftheboosting clas-\nsi\u0002er used isthatbyanalyzing theweights oftheweak\nlearners wecanidentify which dimensions ofthefeature\nvectors aremost helpful forclassi\u0002cation. Wetherefore\nperformed thisanalysis forthe8-topic lyric vectors.\nTable 1showsthemost frequent words which char-\nacterize each topic inthe8topic case. Byanalyzing the\nboosting models, wefound thatbyfarthemost important\nfeatures fordistinguishing hitswere Topic 1andTopic\n6.These appear todescribe heavymetal andpeace-\nful/ne wage music. Interestingly ,nearly alloftheweak\nlearners learnt negative boundaries. That is,theabsence\nofTopic 1orTopic 6meant thesong wasmore likelytobe\nahit.Topic 4which describes generic lovesongs wasthe\nmain topic whose presence meant thesong waslikelyto\nbeahit.However,itsimpact wasmuch lowerthan Topics\n1or6.\nTable 1:Most frequent words characterizing 8-topic mod-\nels\nTopic Characterizing Words\n0 N*GGA SH*T F*CK YADONT B*TCH\n1 BLOOD CHILDREN WARDANCE HES\n2 DONT YOURE SAYTHA TSMONEY\n3 YODONT CAUSE EMTHA TSXYALL\n4 YEAH OHGIRL HEY SHES BABY\n5 LOVEDONT OHYOURE BABY SAY\n6 AWAYDAYEYES THERES IVE GONE\n7 LAQUEDEYTEBYE MITUESYOEN6Conclusions andFutur eWork\nOurresults suggest thatthere isindeed some distinguish-\nable thread connecting hitsongs. More experimentation\nisneeded, butevenour\u0002rstattempts inthisstudy ledto\nclassi\u0002ers thatarebetter than random. Itseems then that\nwecannot simply dismiss claims bycompanies such as\nHitSong Science asimpossible.\nOur results indicate thatforthefeatures used, lyric-\nbased features areslightly more effectivethan audio-based\nfeatures atdistinguishing hits. Combining features does\nnotsigni\u0002cantly impro veperformance. Analysis ofthe\nbestlyric-based system showsthattheabsence rather than\nthepresence ofcertain semantic information inthelyrics\nmean asong ismore likelytobeahit.\nNumerous extensions ofthisresearch arepossible. For\nexample, future workshould examine different weight-\nings oftheaudio andlyrical data, uselargerdata sets, and\nattempt classi\u0002cation within smaller groupings, such as\nmusic bydecade, ormusic bystyle. Different kinds of\nacoustic andlyric features should also bestudied. Inpar-\nticular ,rhythmic andmelodic features merit exploration.\nFinally ,since popular music iscontinually evolving, time-\nvarying classi\u0002ers should bestudied.\nRefer ences\nA.Berenzweig, D.P.W.Ellis, andS.Lawrence. Anchor\nspace forclassi\u0002cation andsimilarity measurement of\nmusic. InICME 2003 ,pages 2932, 2003a.\nA.Berenzweig, B.Logan,D.P.W.Ellis, andB.Whitman.\nAlarge-scale evaluation ofacoustic andsubjecti vemu-\nsicsimilarity measures. InProceedings International\nConfer ence onMusic Information Retrie val(ISMIR) ,\npages 103109, 2003b.\nJ.Blume. 6Steps toSongwriting Success: theCompr e-\nhensive Guide toWriting andMark eting HitSongs .Bil-\nboard Books, revised andexpanded edition, 2004.\nC.Burges. ATutorial onSupport Vector Machines for\nPattern Recognition. Data Mining andKnowledg eDis-\ncovery,2(2):121167, 1998.\nJ.T.Foote. Content-based retrie valofmusic andaudio.\nInSPIE ,pages 138147, 1997.\nY.Freund andR.E.Schapire. Adecision-theoretic gener -\nalization ofon-line learning andanapplication toboost-\ning. InComputational Learning Theory: Eurocolt '95,\npages 2337. Springer -Verlag, 1995.\nT.Hofmann. Probabilistic latent semantic analysis. In\nUncertainty inArti\u0002cial Intellig ence,1999.\nB.Logan,A.Kositsk y,andP.Moreno. Semantic analysis\nofsong lyrics. InICME 2004 ,2004.\nR.E.Schapire. The strength ofweak learnability .Ma-\nchine Learning ,5(2):197227, 1990.\nK.TieuandP.Viola. Boosting image retrie val.InIEEE\nInternational Confer ence onComputer Vision ,pages\n228235, 2000.\nG.Tzanetakis andP.Cook. Music genre classi\u0002cation of\naudio signals. IEEE Transactions onSpeec handAudio\nProcessing ,5(10):293302, July 2002.\n491"
    },
    {
        "title": "MATCH: A Music Alignment Tool Chest.",
        "author": [
            "Simon Dixon",
            "Gerhard Widmer"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416952",
        "url": "https://doi.org/10.5281/zenodo.1416952",
        "ee": "https://zenodo.org/records/1416952/files/DixonW05.pdf",
        "abstract": "We present MATCH, a toolkit for aligning audio recordings of different renditions of the same piece of music, based on an efficient implementation of a dynamic time warping algorithm. A forward path estimation algorithm constrains the alignment path so that dynamic time warping can be performed with time and space costs that are linear in the size of the audio files. Frames of audio are represented by a positive spectral difference vector, which emphasises note onsets in the alignment process. In tests with Classical and Romantic piano music, the average alignment error was 41ms (median 20ms), with only 2 out of 683 test cases failing to align. The software is useful for content-based indexing of audio files and for the study of performance interpretation; it can also be used in real-time for tracking live performances. The toolkit also provides functions for displaying the cost matrix, the forward and backward paths, and any metadata associated with the recordings, which can be shown in real time as the alignment is computed. Keywords: audio alignment, content-based indexing, dynamic time warping, music performance analysis 1",
        "zenodo_id": 1416952,
        "dblp_key": "conf/ismir/DixonW05",
        "keywords": [
            "MATCH",
            "toolkit",
            "audio recordings",
            "music",
            "dynamic time warping",
            "forward path estimation",
            "alignment error",
            "performance interpretation",
            "real-time tracking",
            "metadata"
        ],
        "content": "MATCH: A MUSIC ALIGNMENT TOOL CHEST\nSimon Dixon\nAustrian Research Institutefor Artiﬁcial Intelligence\nFreyung 6/6\nVienna 1010, Austria\nsimon@ofai.atGerhard Widmer\nDepartment of ComputationalPerception\nJohannes Kepler University Linz\nAltenberger Str 69, A-4040 Linz, Austria\ngerhard.widmer@jku.at\nABSTRACT\nWe present MATCH, a toolkit for aligning audio record-\nings of different renditions of the same piece of music,\nbased on an efﬁcient implementation of a dynamic time\nwarping algorithm. A forward path estimation algorithm\nconstrains the alignment path so that dynamic time warp-\ning can be performed with time and space costs that are\nlinear in the size of the audio ﬁles. Frames of audio are\nrepresentedbyapositivespectraldifferencevector,which\nemphasises note onsets in the alignment process. In tests\nwith Classical and Romantic piano music, the average\nalignmenterrorwas41ms(median20ms),withonly2out\nof 683 test cases failing to align. The software is use-\nful for content-based indexing of audio ﬁles and for the\nstudy of performance interpretation; it can also be used\nin real-time for tracking live performances. The toolkit\nalso provides functions for displaying the cost matrix, the\nforwardandbackwardpaths,andanymetadataassociated\nwith the recordings, which can be shown in real time as\nthe alignment is computed.\nKeywords: audio alignment, content-based indexing,\ndynamic time warping, music performance analysis\n1 INTRODUCTION\nTheuseofrandomaccessmediaforaudiodata,makingit\npossible to jump immediately to any point in the data, is\nadvantageous only to the extent that the data is indexed.\nFor example, content-based indexing of CDs is typically\nlimited to the level of tracks (songs or movements), the\ninformation provided by the manufacturer. The index-\ning cannot be determined by the user, who might be in-\nterested in a more ﬁne-grained or special purpose index.\nFor example, a piano student or music lover might want\ntocomparehowseveraldifferentpianistsplayaparticular\nphrase,whichwouldinvolveamanualsearchfortherele-\nvantphraseineachrecording. Oralternatively,amusicol-\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonogist studying the relationship between tempo and phrase\nstructure painstakingly marks the times of beats in each\nrendition of a work, not having any way of transferring\nthe metadata from one version to the next, since the beats\noccur at different times in each performance.\nTo address these and similar needs, we developed\nMATCH, a system for accurate automatic alignment of\nmultiple renditions of the same piece of music. This tool\ncanbeusedinmusicologyandmusicpractice,tocompare\ndifferent interpretations of a work, or for annotation of\nmusic with content-based metadata (e.g. section, phrase,\nbeat or note indexes), which could then be transferred au-\ntomatically from one recording to the corresponding po-\nsitions in another recording. Another use would be in an\naudio recording system to provide intelligent editing op-\nerations such as aligning splice points in corresponding\nﬁles. The toolkit also provides functions for displaying\nthe alignment as it is computed.\nMATCH is based on an efﬁcient dynamic time warp-\ning algorithm which has time and space costs that are lin-\near in the lengths of the performances. This effectively\nallows arbitrarily long pieces to be processed faster than\nreal time, that is, in less time than the duration of the au-\ndio ﬁles. The audio data is represented by positive spec-\ntral difference vectors. Frames of audio input are con-\nverted to a frequency domain representation using a short\ntime Fourier transform, and then mapped to a non-linear\nfrequency scale (linear at low frequencies and logarith-\nmicathighfrequencies). Thetimederivativeofthisspec-\ntrumisthenhalf-waverectiﬁedandtheresultingvectoris\nemployedinthedynamictimewarpingalgorithm’smatch\ncost function, using a Euclidean metric.\nIn the next section, we review the standard dynamic\ntime warping algorithm and describe the modiﬁcations\nnecessary for an efﬁcient implementation for audio align-\nment. We also present the cost function used to evaluate\nthesimilarityofframesofaudiodata,andabriefdescrip-\ntion of the user interface and implementation details of\nMATCH. Section 3 reports on the results of testing with\nthree different data sets, which indicate that the current\naudio alignment algorithm works well for a range of mu-\nsic. The ﬁnal section provides a discussion of the work, a\ncomparison with other audio alignment methods, and an\noutline of planned future work.\n4922 EFFICIENT TIME WARPING\nDynamic time warping (DTW) is a technique for align-\ning time series which has been well known in the speech\nrecognition community since the 1970’s (Rabiner and\nJuang,1993). DTWalignstwotimeseries U=u1, ..., u m\nandV=v1, ..., v nby ﬁnding a minimum cost path\nW=W1, ..., W l, where each Wkis an ordered pair\n(ik, jk), such that (i, j)∈Wmeans that the points ui\nandvjarealigned. Thealignmentisassessedwithrespect\nto a local cost function dU,V(i, j), usually represented as\nanm×nmatrix, which assigns a match cost for aligning\neachpair (ui, vj). Thecostis0foraperfectmatch,andis\notherwise positive. The path cost D(W)is the sum of the\nlocal match costs along thepath:\nD(W) =l/summationdisplay\nk=1dU,V(ik, jk)\nSeveral local path constraints are placed on W,\nnamely that the path is bounded by the ends of both se-\nquences, and it is monotonic and continuous. Addition-\nally, global path constraints are often used, such as the\nSakoe-Chibabound(SakoeandChiba,1978),whichcon-\nstrains the path to lie within a ﬁxed distance of the diag-\nonal (typically 10% of the total length of the time series).\nBy limiting the slope of the path, either globally or lo-\ncally, these constraints prevent pathological solutions and\nreduce the search space.\nTheminimumcostpathcanbecalculatedinquadratic\ntime by dynamic programming, using the recursion:\nD(i, j) =d(i, j) + min\n\nD(i, j−1)\nD(i−1, j)\nD(i−1, j−1)\n\n\nwhere D(i, j)isthecostoftheminimumpathfrom (1,1)\nto(i, j),and D(1,1) = d(1,1). Thepathitselfisobtained\nby tracing the recursion backwards from D(m, n).\nSome formulations of DTW introduce various biases\nin addition to the slope constraints, by multiplying d(i, j)\nby a weight which is dependent on the direction of the\nmovement. In fact, the above formulation is biased to-\nwards diagonal steps: the greater the number of diago-\nnal steps, the shorter the total path length (Sankoff and\nKruskal,1983,p.177). WefollowSakoeandChiba(1978)\ninusingaweightof2fordiagonalstepssothatthereisno\nbias for any particular direction.\n2.1 A Linear Time Implementation of DTW\nThequadratictimeandspacecostisoftencitedasalimit-\ningfactorfortheuseofDTWwithlongsequences. How-\never the widely used global path constraints can be triv-\niallymodiﬁed tocreatea lineartimeand spacealgorithm.\nFor instance, if the width of the Sakoe-Chiba bound is set\nto a constant rather than a fraction of the total length, the\nnumberofcalculationsbecomeslinearinthelengthofthe\nsequences. The danger with this approach is that it is not\nknown how close to the diagonal the optimal solution is,\nsothedesiredsolutioniseasilyexcludedbyabandaround\nthe diagonal which is too narrow.To avoid missing the optimal path, we use a forward\npath estimation algorithm to compute the centre of the\nband of the cost matrix which is to be calculated. This\nis based on the on-line time warping algorithm presented\nin (Dixon, 2005), which estimates the alignment of a live\nperformancewitharecordinginrealtime. TheDTWpath\nis constrained to lie within a ﬁxed distance of the forward\npath, which ensures that the computation is bounded by\nlineartimeandspacecosts. Ifwehadusedstandardglobal\npath constraints, a wider band would have been required,\nin order to cater for the estimated maximum possible de-\nviationfromthediagonal. Withan“adaptivediagonal”,it\nispossibletouseanarrowerbandwithlessriskofmissing\nthe optimal solution. This enables the system to perform\nwith greater efﬁciency and accuracy than a system based\non global path constraints.\nTheintuitionbehindtheforwardpathalgorithmcanbe\nexplained with reference to Figure 1, where a band width\nofw= 4is used for illustrative purposes. (In practice, a\nband width of w= 500is used.) At any time the active\nareaof the matrix is the top row and the right column of\nthe calculated area. The minimum cost path to each of\nthese cells is evaluated and the cell with the lowest mini-\nmum cost path (normalised by length) is used as an indi-\ncation of the direction in which the optimal path appears\nto be heading. (The true optimal path cannot be known\nuntil the complete matrix is calculated.) If this cell is in\nthe top right corner, the algorithm is considered to be on\ntarget. If it is to the left of the target (for example, after\nexpansions 7 and 8 in Figure 1), then the calculated part\nof the matrix is expanded upwards until the algorithm is\non target again (expansions 9 to 11). Likewise if the cell\nis below the target, expansion isperformed to the right.\nThealgorithmisinitialisedbycomputingasquarema-\ntrix of size w; then the calculated area is iteratively ex-\npanded by evaluating rows or columns of length w. The\ndirection of expansion (i.e. whether a new row or a new\ncolumn is calculated) is determined by the location of the\ncell in the active area with lowest minimum path cost. If\nthis cell is in the top row, a new row is calculated, and\nif it is in the right column, a new column is calculated.\nTo avoid pathological solutions, limits are placed on the\nnumber of successive row (respectively column) compu-\ntations. A complete description of the forward path al-\ngorithm can be found in (Dixon, 2005). When the ends\nof both ﬁles are reached, the optimal path is traced back-\nwards using the standard DTW algorithm, constrained by\nthefactthatonlythecellscalculatedpreviouslyduringthe\nforward path calculation can beused.\n2.2 A Cost Function for Comparing Audio Frames\nThe alignment of audio ﬁles is based on a cost function\nwhich assesses the similarity of frames of audio data. We\nuse a low level spectral representation of the audio data,\ngenerated from a windowed FFT of the signal. A Ham-\nming window with a default size of 46 ms (2048 points)\nis used, with a default hop size of 20 ms. The spectral\nrepresentation was chosen over a higher level symbolic\nrepresentationofthemusicinordertoavoidapitchrecog-\nnition step, which is notoriously unreliable in the case of\npolyphonic music. The frequency axis was mapped to a\n493123\n45\n67\n891011\n1213\n14151617\n181920\n21Figure 1: An example of the on-line time warping algo-\nrithm with band width w= 4, showing the order of eval-\nuation for a particular sequence of row and column in-\ncrements. The axes represent time in the two ﬁles. All\ncalculated cells are framed in bold, and the optimal path\nis coloured grey.\nscale which is linear at low frequencies and logarithmic\nat high frequencies. This achieved a signiﬁcant data re-\nduction without loss of useful information, at the same\ntimemimickingthelinear-logfrequencysensitivityofthe\nhuman auditory system. The lowest 34 FFT bins (up to\n370Hz, or F /sharp4) were mapped linearly to the ﬁrst 34 ele-\nments of the new scale. The bins from 370Hz – 12.5kHz\nweremappedontoalogarithmicscalewithsemitonespac-\ning by summing energy in each bin into the nearest semi-\ntone element. Finally, the remaining bins above 12.5kHz\n(G9) were summed into the last element of the new scale.\nThe resulting vector contained a total of 84 points instead\nof the original 2048.\nThe most important factor for alignment is the timing\nof the onsets of tones. The subsequent evolution of the\ntone gives little information about its timing and is dif-\nﬁcult to align using energy features, which change rela-\ntively slowly over time within a note. Therefore the ﬁnal\naudio frame representation uses a half-wave rectiﬁed ﬁrst\norder difference, so that only the increases in energy in\neach frequency bin are taken into account, and these pos-\nitive spectral difference vectors are compared using the\nEuclidean distance:\nd(i, j) =/radicaltp/radicalvertex/radicalvertex/radicalbt84/summationdisplay\nb=1(E/primeu(b, i)−E/primev(b, j))2\nwhere E/prime\nx(f, t)represents the increase in energy Ex(f, t)\nof the signal x(t)in frequency bin fat time frame t:\nE/prime\nx(f, t) = max( Ex(f, t)−Ex(f, t−1),0)\n2.3 Interpretation of the DTW Path\nThe path returned by the DTW alignment algorithm is\nused as a lookup table between the two audio ﬁles to ﬁndthe location in the second ﬁle corresponding to a selected\nlocation in the ﬁrst ﬁle. Since the path is continuous and\ncovers the full extent of both ﬁles, there is for each time\nindex in one ﬁle at least one corresponding time point in\nthe other. If there is more than one corresponding point,\nan average is taken. This deﬁnes a bidirectional mapping\nbetween the time variables in the two ﬁles, with the reso-\nlution of the frame hop size.\n2.4 Implementation Details\nMATCH has a familiar graphical user interface which is\nsimilar to most media players (Figure 2). When ﬁles are\nloaded, the ﬁrst ﬁle is used as the reference ﬁle, and sub-\nsequent ﬁles are each aligned to the reference ﬁle. Corre-\nsponding time points between arbitrary pairs of ﬁles can\nthenbecomputedviathereferenceﬁle,usingcomposition\nof the respective time maps. One unfamiliar function (the\n‘*’ button) marks positions of interest in a piece, which\nare mapped to the corresponding locations in the other\nversions, so that the user can compare performances of\na particular section or test the operation of the alignment\nalgorithm. MATCH has functions for displaying the cost\nmatrix, the forward and backward paths, and any other\nmetadata associated with the ﬁles. The audio from one\nﬁle can be played as matching is performed, with the ma-\ntrixscrollinginrealtimeanddisplayingacausalestimate\nof the alignment.\nMATCH is implemented in Java 1.5, and on a 3GHz\nLinux PC, alignment of two audio ﬁles takes approxi-\nmately 4% of the sum of durations of the ﬁles, using a\ntimeresolutionof20ms. Alowerframeratecouldbeused\nwithoutsigniﬁcantlossofprecision. MATCHisavailable\nfor download at:\nhttp://www.ofai.at/˜simon.dixon/match\n3 TESTING AND RESULTS\nWe report the results from 3 sets of test data: a pre-\ncise quantitative evaluation using data recorded on a\nB¨osendorfer computer-monitored piano; a quantitative\nevaluation based on semi-automatic annotation of vari-\nous CD recordings; and a qualitative evaluation based on\nunannotated CD recordings.\n3.1 B¨osendorfer Data\nThe B¨osendorfer SE290 is a grand piano with sensors\nwhich measure the precise timing and dynamics of every\nnote with a time resolution of 1.25ms. This test set con-\nsistsofrecordingsof22pianistsplaying2excerptsofsolo\npiano music by Chopin (Etude in E Major, Op.10, no.3,\nbars 1–21; and Ballade Op.38, bars 1–45) (Goebl, 2001).\nTheEtudeperformancesrangedfrom70.1to94.4seconds\nduration,andtheBalladerangedfrom112.2to151.5sec-\nonds, so the differences in execution speeds were by no\nmeans trivial. Alignment was performed on all pairs of\nperformances of each piece (a total of22×21\n2×2 = 462\ntest cases).\nIn order to estimate the correctness of the alignment,\nwe compared it with the onset times of the corresponding\nnotes in each interpretation. If we consider the alignment\n494Figure 2: Screenshot of MATCH showing the user inter-\nface.\nas a mapping from time in one interpretation to time in\nthe other interpretation, a correct alignment should map\ntheonsettimeofeachnoteintheﬁrstinterpretationtothe\nonset time of the same note in the second interpretation.\nTwo factors make this difﬁcult: differences in the per-\nformed notes, which might be due to different score ver-\nsions,ornaments,ormistakes;andasynchroniesinchords\n(setsofsimultaneousnotesaccordingtothemusicalnota-\ntion),whicharetypicallyaround30ms,butsometimesup\nto 150 ms, and not necessarily in any ﬁxed temporal or-\nder. In these cases there is no unique “correct” alignment\nof the notes involved.\nTherefore, we deﬁne a score event to be a set of si-\nmultaneous notes according to the score, and for each in-\nterpretation iwe calculate the average onset time t(i, e)\nof the performed notes in each score event e. The cor-\nrect alignment is then deﬁned in terms of the accuracy of\nthemappingofscoreeventsfromoneinterpretationtothe\nother,ignoringtimepointsbetweenscoreevents. Foreach\nscore event e, the alignment path should pass through the\npoint (t(i1, e), t(i2, e)), and the error is calculated as the\nManhattandistanceofthispointfromthenearestpointon\nthealignmentpath. ThetotalerrorofanalignmentpathisError≤ Cumulative error counts\nFrames Seconds Count Percent\n00.00 38655 46.1%\n10.02 72934 87.1%\n20.04 79126 94.5%\n30.06 80540 96.2%\n50.10 81343 97.1%\n10 0.20 82325 98.3%\n25 0.50 83292 99.4%\n50 1.00 83658 99.9%\nTable1: Distributionofalignmenterrors,shownascumu-\nlativecountsandpercentagesofscoreeventswithanerror\nup to the given value. The averageerror was 23ms.\nthe average of the point-wise errors over all score events.\nTable1showsthedistributionofpoint-wiseerrorsless\nthan or equal to 0,1,2,3,5,10,25 and 50 frames, where a\nhop size of 20 ms was used. The median and average er-\nrors are below the human temporal order threshold (the\nability to distinguish the order of two sounds occurring\ncloselyintime),whichisapproximately40ms,andcanbe\nmuch worse in the context of annotating musical record-\nings (Dixon et al., 2005). The success of the system with\nthis data was aided by the fact that the audio recordings\nwereallmadeunderidenticalconditions(samepiano,mi-\ncrophone, room and settings). In the following subsec-\ntions we describe tests using data with a large variety of\nrecording conditions.\n3.2 BeatRoot Data\nThe second set of test data involved music with a large\nrangeofrecordingconditions,pianos,piecesandinterpre-\ntations,wherethebeathadbeenannotatedusingtheinter-\nactivebeattrackingsystemBeatRoot(Dixon,2001). This\ndata set is larger, complexer and more varied, containing\nClassicalandRomanticPeriodpianomusicrecordedover\nthe second half of the twentieth century by great pianists\nsuch as those listed in Figure 2. However, the data is only\nannotated at beat times (not note onsets) and is less pre-\ncise, having an estimated accuracy of 30ms.\nThe results are summarised in Table 2, showing the\nmaximum, mean and median error for each piece. In 2\nof the 221 test cases, the alignment failed, and these re-\nsultswerenotincludedinthestatistics. Inmostcases,the\nmaximumerroroccurredattheendofapiece,wherethere\nis no further data to orient the alignment. The mean er-\nror tends to be biased by the maximum errors, so we also\nshow the median error, which is less biased, but it gives\nno indication of the spread of the errors. The overall av-\nerageerrorof64msisworsethanfortheprevioustestset,\nwherethecontrolledrecordingconditionsmadesimilarity\njudgements much easier.\n3.3 Further Tests\nThe above tests consisted only of piano music, which\ncould be easier to align than other instruments, due to the\nsharpness of onsets and the ﬁxed timbre of piano tones.\nSince we do not have any annotated non-piano music, in-\n495Composer Piece Versions TestEvents Error (seconds)\n(work, section) Pairs(total) Maximum MeanMedian\nBeethoven Op.15, No.2, bar 1–8 4 6366 0.70 0.085 0.040\nChopin Op.15, No.1 13* 7716863 7.48 0.061 0.020\nMozart KV279, 1st movt 5105510 5.26 0.036 0.020\nMozart KV279, 2nd movt 4 61836 2.26 0.058 0.020\nMozart KV279, 3rd movt 5104474 1.38 0.025 0.020\nMozart KV280, 1st movt 5105990 8.12 0.037 0.020\nMozart KV280, 2nd movt 5105012 8.90 0.102 0.020\nMozart KV280, 3rd movt 5102783 4.08 0.044 0.020\nSchubert D899, No.3 12 6622506 5.74 0.071 0.020\nSchumann Op.15, No.7 6* 143570 2.28 0.087 0.020\nTable 2: Alignment results for commercial CDs of the given works. Two lines are marked with ‘*’, indicating that one\npair failed to align and was not included in the statistics.\nformal tests on other music were performed by marking\npositions in one ﬁle, and listening to the aligned ﬁles to\ncheck that the marks were transferred to the correspond-\ning positions in each recording. This method has several\ndisadvantages: it can only detect errors of at least several\nhundred milliseconds, it relies on human judgement, it is\nnot automated, and it only checks speciﬁc points on the\nalignment path, not the complete path.\nWetestedsomesoloclassicalguitarpiecesbyAlbeniz\n(Asturias, Cordoba, Sevilla), Granados (Spanish Dances\n4 and 5), Tarrega (Capricho Arabe) and Villa Lobos (Pre-\nlude 1). The alignments of Asturias and Spanish Dance\nNo. 4 were partially unsuccessful, due to many differ-\nences in the arrangements. The other works were suc-\ncessfully aligned. Tests with orchestral music, including\nTchaikovsky’s Piano Concerto No. 1 and 10 different in-\nterpretations of the ﬁrst movement of Schumann’s Piano\nConcerto, revealed no problems in alignment. Some er-\nrorswereapparentinthealignmentofotherworks,partic-\nularlyatthebeginningsandendsofthepieces. Twopopu-\nlarBeatlessongs( IWannaHoldYourHand andSheLoves\nYou) in English and German versions were also aligned\nsuccessfully. These tests suggest that the similarity mea-\nsure is not restricted to piano tones, but is applicable to a\nvariety of instruments.\n4 DISCUSSION AND CONCLUSION\nThis paper presented an audio alignment toolkit which\nuses a modiﬁed DTW algorithm. The average alignment\nerror for solo piano music was 41ms, with only 2 out of\n683 test cases failing to align. Informal tests with guitar,\norchestral and popular music conﬁrmed the generality of\nthe system.\nA low-level audio representation was used in prefer-\nence to a high-level representation, which would enable\na more efﬁcient DTW computation, but is less reliable in\nits extraction of features. The cost function was based on\nderivativespectralfeatures,inordertoemphasisetoneon-\nsets. Derivative features have been used in speech recog-\nnition(SakoeandChiba,1978)andscorefollowing(Orio\nand Schwarz, 2001). A distance measure calculated di-\nrectly from the short time spectrum was used for comput-\ning audio similarity in (Foote and Uchihashi, 2001). This\nusedamuchsmallerwindowsize(11ms),sinceitwasfo-cussed on rhythmic analysis, where timing is critical and\npitch not so important. In tests using spectral values in-\nstead of the spectral difference, we found that the results\nwere clearly better using spectral difference.\nDannenbergandHu(2003)proposetheuseofachro-\nmagram, which reduces the frequency scale to twelve\npitch classes, independent of octave. This might be suit-\nable for retrieval by similarity, where absolute identity of\nmatchingmusicalpiecesisnotassumed,andalargenum-\nberofcomparisonsmustbeperformedinashorttime,but\nit discards more information than is necessary. Other fea-\ntures such as MFCCs are often used in speech and audio\nresearch,buttheycapturethespectralshape(reﬂectingthe\ntimbre of the instrument) rather than the pitch (reﬂecting\nthe notes that were played).\nDTW has been used for score-performance alignment\n(Orio and Schwarz, 2001; Soulez et al., 2003; Turet-\nsky and Ellis, 2003) and query by humming applica-\ntions (Mazzoni and Dannenberg, 2001; Zhu and Shasha,\n2003). Theearliestscorefollowingsystemsuseddynamic\nprogramming (Dannenberg, 1984), based on a high-level\nsymbolic representation of the performance which was\nonly usable with monophonic audio. Alternative ap-\nproaches to music alignment use hidden Markov mod-\nels (Cano et al., 1999; Orio and D ´echelle, 2001) and hy-\nbrid graphical models (Raphael, 2004), which both re-\nquire training data for each piece. The test data used in\nthisworkissomewhatexceptional;ingeneral,wewillnot\nhave access to multiple labelled performances.\n4.1 Future Work\nWe conclude with some ideas for extending and im-\nproving this work. Experiments with normalisation have\nproved it to be a double-edged sword. Since we have no\ncontrol of recording levels, some form of normalisation\nbetween ﬁles is essential. The frame to frame normali-\nsation of energy is however more problematic, since it is\nmore important that salient parts of the audio match, and\nas notes decay to silence, it is not desirable that they play\nan equally signiﬁcant role as the tone onsets in determin-\ning the alignment. The use of positive spectral difference\nsolves part of this problem, but further experimentation is\nrequired to determine the bestaudio representation.\nThe output from the DTW algorithm is not at all\n496smooth at the local level, but we perceive most tempo\nchanges as being smooth. Many irregularities in the path\narise because the cost function is tuned to match note on-\nsets, and therefore the frames where no new notes appear\nhaveverylittletodistinguishthem. Someformofsmooth-\ning or interpolation could be performed in order to create\na path which is musically plausible. However, smoothing\ntendstoworsenthenumericalresults,astheonlyimprove-\nments are between the evaluated points, and some outly-\ning points adversely inﬂuence correctly aligned note on-\nsets. Our current smoothing algorithm uses interpolation\nto remove outlying points, replacing adjacent horizontal\nand vertical path segments with diagonal segments.\nMost of the large errors occur at the beginnings and\nends of ﬁles; no example has been found where the align-\nment is correct at the beginning and then incorrect for the\nbulk of the ﬁle. Part of the reason for this is that the off-\nsetfromtheﬁrst(respectivelylast)frametotheﬁrst(last)\nnote onset varies greatly between ﬁles, and the DTW al-\ngorithm is required to ﬁnd a path from the ﬁrst to the last\nframe. Ifwespeciﬁcallydetectedtheﬁrstandlastnote,or\nalternatively detected silence in the audio ﬁles, many of\nthese errors could be avoided.\nOne issue that has not been addressed is the prob-\nlem of structural differences between performances. For\nexample, if one performer repeats the ﬁrst section of a\nmovement and another performer does not, there is no\nway for the DTW algorithm to recover, since the width\nof the search band is only 5 or 10 seconds. In order to\nﬁnd structural differences and perform partial matches,\nthe complete similarity matrix would need to be calcu-\nlated, which would then limit the size of pieces which\ncould be matched, due to memory and time limitations.\nThis work stemmed from a real-time audio alignment\ntool for live performance analysis (Dixon, 2005). Since\nthecurrentworkdoesnotrequireon-lineprocessing,some\nimprovements could be made to the off-line system in or-\nder to reduce the number of tracking errors, for example,\nby computing a default slope (relative tempo) from the\ndurations of the audio ﬁles, and biasing the forward al-\ngorithm to favour this slope. In future work, we intend\nto extend MATCH to include score-audio alignment, so\nthat it can be used as a score-following system in real-\ntime, and so that symbolic metadata can be automatically\naligned with performances and recordings.\nACKNOWLEDGEMENTS\nThis work was supported by: the Vienna Science and\nTechnology Fund, project CI010 Interfaces to Music ; the\nAustrian Ministry BMBWK, START project Y99-INF;\nand the European Union, project EU-FP6-IST-507142\nSIMAC. The Austrian Research Institute for Artiﬁcial\nIntelligence acknowledges the support of the ministries\nBMBWK and BMVIT.\nREFERENCES\nP. Cano, A. Loscos, and J. Bonada. Score-performance\nmatching using HMMs. In Proceedings of the Inter-\nnational Computer Music Conference , pages 441–444.\nInternational Computer Music Association, 1999.R. Dannenberg. An on-line algorithm for real-time ac-\ncompaniment. In Proceedings of the International\nComputer Music Conference , pages 193–198, 1984.\nR.DannenbergandN.Hu. Polyphonicaudiomatchingfor\nscore following and intelligent audio editors. In Pro-\nceedings of the International Computer Music Confer-\nence, pages 27–34, 2003.\nS. Dixon. Automatic extraction of tempo and beat from\nexpressive performances. Journal of New Music Re-\nsearch, 30(1):39–58, 2001.\nS. Dixon. Live tracking of musical performances using\non-line time warping. In Proceedings of the 8th Inter-\nnational Conference on Digital Audio Effects , 2005.\nS. Dixon, W. Goebl, and E. Cambouropoulos. Smoothed\ntempo perception of expressively performed music.\nMusic Perception , 23, 2005. To appear.\nJ. Foote and S. Uchihashi. The beat spectrum: A new ap-\nproach to rhythm analysis. In IEEE International Con-\nference on Multimedia and Expo , 2001.\nW.Goebl. Melodyleadinpianoperformance: Expressive\ndevice or artifact? Journal of the Acoustical Society of\nAmerica, 110(1):563–572, 2001.\nD. Mazzoni and R. Dannenberg. Melody matching di-\nrectly from audio. In 2nd International Symposium on\nMusic Information Retrieval , pages 73–82, 2001.\nN. Orio and F. D ´echelle. Score following using spectral\nanalysisandhiddenMarkovmodels. In Proceedingsof\nthe International Computer Music Conference , pages\n151–154, 2001.\nN. Orio and D. Schwarz. Alignment of monophonic and\npolyphonic music to a score. In Proceedings of the\nInternationalComputerMusicConference ,pages155–\n158, 2001.\nL. R. Rabiner and B. H. Juang. Fundamentals of Speech\nRecognition . Prentice, Englewood Cliffs, NJ, 1993.\nC. Raphael. A hybrid graphical model for aligning poly-\nphonic audio with musical scores. In Proceedings of\nthe 5th International Conference on Musical Informa-\ntion Retrieval , pages 387–394, 2004.\nH.SakoeandS.Chiba. Dynamicprogrammingalgorithm\noptimisationforspokenwordrecognition. IEEETrans-\nactions on Acoustics, Speech and Signal Processing ,\n26:43–49, 1978.\nD. Sankoff and J. Kruskal. Time warps, string ed-\nits, and macromolecules: The theory and prac-\ntice of sequence comparison . Addison-Wesley, New\nYork/Menlo Park/Reading, 1983.\nF. Soulez, X. Rodet, and D. Schwarz. Improving poly-\nphonicandpoly-instrumentalmusictoscorealignment.\nIn4th International Conference on Music Information\nRetrieval, pages 143–148, 2003.\nR. Turetsky and D. Ellis. Ground-truth transcriptions\nof real music from force-aligned MIDI syntheses. In\n4thInternationalConferenceonMusicInformationRe-\ntrieval, pages 135–141, 2003.\nY. Zhu and D. Shasha. Warping indexes with envelope\ntransforms for query by humming. In ACM SIGMOD\nConference , 2003.\n497"
    },
    {
        "title": "Extracting Quality Parameters for Compressed Audio from Fingerprints.",
        "author": [
            "Peter Jan O. Doets",
            "Reginald L. Lagendijk"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416080",
        "url": "https://doi.org/10.5281/zenodo.1416080",
        "ee": "https://zenodo.org/records/1416080/files/DoetsL05.pdf",
        "abstract": "An audio fingerprint is a compact yet very robust representation of the perceptually relevant parts of audio content. It can be used to identify audio, even when of severely distorted. Audio compression causes small changes in the fingerprint. We aim to exploit these small fingerprint differences due to compression to assess the perceptual quality of the compressed audio file. Analysis shows that for uncorrelated signals the Bit Error Rate (BER) is approximately inversely proportional to the square root of the Signal-to-Noise Ratio (SNR) of the signal. Experiments using real music confirm this relation. Further experiments show how the various local spectral characteristics cause a large variation in the behavior of the fingerprint difference as a function of SNR or the bitrate set for compression. 1",
        "zenodo_id": 1416080,
        "dblp_key": "conf/ismir/DoetsL05",
        "keywords": [
            "audio fingerprint",
            "perceptually relevant parts",
            "audio content",
            "identify audio",
            "severely distorted",
            "compression",
            "perceptual quality",
            "signal-to-noise ratio",
            "Bit Error Rate (BER)",
            "Signal-to-Noise Ratio (SNR)"
        ],
        "content": "EXTRACTING QUALITY PARAMETERS FOR COMPRESSED AUDIO\nFROM FINGERPRINTS\nP.J.O. Doets and R.L. Lagendijk\n{p.j.doets, r.l.lagendijk }@ewi.tudelft.nl\nDept. of Mediamatics, Information and Communication Theor y Group,\nFaculty of Electrical Engineering, Mathematics and Comput er Science,\nDelft University of Technology, P.O. Box 5031, 2600 GA Delft\nABSTRACT\nAn audio ﬁngerprint is a compact yet very robust rep-\nresentation of the perceptually relevant parts of audio\ncontent. It can be used to identify audio, even when\nof severely distorted. Audio compression causes small\nchanges in the ﬁngerprint. We aim to exploit these small\nﬁngerprint differences due to compression to assess the\nperceptual quality of the compressed audio ﬁle. Anal-\nysis shows that for uncorrelated signals the Bit Error\nRate (BER) is approximately inversely proportional to the\nsquare root of the Signal-to-Noise Ratio (SNR) of the sig-\nnal. Experiments using real music conﬁrm this relation.\nFurther experiments show how the various local spectral\ncharacteristics cause a large variation in the behavior of\nthe ﬁngerprint difference as a function of SNR or the bi-\ntrate set for compression.\n1 INTRODUCTION\nIdentiﬁcation of music on the Internet is usually done by\nsearching in the metadata describing the music content.\nMetadata like song title, artist, etc., however, is often in co-\nherent or misleading [1], especially on popular unmoder-\nated Peer-to-Peer (P2P) ﬁle-sharing networks like KaZaA\n(www.kazaa.com) and eDonkey (www.edonkey.com). A\nsolution is to identify the music based on the content.\nIdentiﬁcation, however, is often not enough. The per-\nceptual quality of a song compressed using MP3 at a bi-\ntrate of 32 kbps is totally different from the perceptual\nquality of the CD-recording of the same song. There-\nfore, a content-based indication for the perceptual qualit y\nis needed. The Music2Share project proposes to use audio\nﬁngerprints for both identiﬁcation and quality assessment\nof unknown content on a P2P network [2].\nAudio ﬁngerprints are compact representations of the\nperceptually relevant parts of audio content that can be\nused to identify music based on the content. A ﬁngerprint-\nPermission to make digital or hard copies of all or part of thi s\nwork for personal or classroom use is granted without fee pro -\nvided that copies are not made or distributed for proﬁt or com -\nmercial advantage and that copies bear this notice and the fu ll\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londoning system consists of two parts: ﬁngerprint extraction and\na matching algorithm. The ﬁngerprints of a large number\nof songs are usually stored in a database. A song is iden-\ntiﬁed by comparing its ﬁngerprint with the ﬁngerprints in\nthe database. The procedure for music identiﬁcation using\nﬁngerprints is schematically shown in Figure 1.\n/G46 /G69 /G6E /G67 /G65 /G72 /G70 /G72 /G69 /G6E /G74\n/G65 /G78 /G74 /G72 /G61 /G63 /G74 /G69 /G6F /G6E/G44 /G61 /G74 /G61 /G62 /G61 /G73 /G65\n/G6C /G6F /G6F /G6B /G75 /G70/G44 /G65 /G63 /G69 /G73 /G69 /G6F /G6E\n/G41 /G75 /G64 /G69 /G6F\n/G73 /G69 /G67 /G6E /G61 /G6C/G41 /G75 /G64 /G69 /G6F\n/G6D /G65 /G74 /G61 /G64 /G61 /G74 /G61/G46 /G69 /G6E /G67 /G65 /G72 /G70 /G72 /G69 /G6E /G74 /G73\n/G41 /G6E /G64\n/G4D /G65 /G74 /G61 /G64 /G61 /G74 /G61\nFigure 1: Music identiﬁcation using an audio ﬁnger-\nprint. The extracted audio ﬁngerprint is matched against a\ndatabase with pre-computed ﬁngerprints and metadata.\nFingerprinting applications are e.g. identiﬁcation of\nsongs or commercials being played on radio or television,\nmusic identiﬁcation using a cell phone (e.g. Shazam [6]),\nand ﬁltering for ﬁle sharing applications [3, 4].\nMost audio ﬁngerprinting systems derive their ﬁnger-\nprint from a time-frequency representation, e.g. using\nshort-term Fourier transforms. They mainly differ in their\nchoice of features to construct the ﬁngerprint, e.g. spec-\ntral ﬂatness features [5], spectral peaks [6], Fourier coef ﬁ-\ncients [7], Mel-Frequency Cepstrum Coefﬁcients (MFCC)\n[8], and energy differences between frequency bands [3].\nFingerprints are robust to many kinds of processing:\nencoding using different coding schemes or bit rates, sub-\nsequent Digital-to-Analog (D/A) and Analog-to-Digital\n(A/D) conversions, small changes in play-out speed, etc.\nThe ﬁngerprints of two arbitrary pieces of music are very\ndifferent, while ﬁngerprints originating from the same\nmusic recording, but which differ due to a limited amount\nof processing or distortion, are only slightly different.\nWe aim to exploit the small ﬁngerprint differences due\nto compression to assess the perceptual quality of the com-\npressed audio ﬁle. For the time being we limit ourselves\nto compression using the popular MP3 format. This setup\nis shown schematically in Figure 2. FX(n, m )denotes\nthe ﬁngerprint bits of the original, undistorted recording ,\nX, andFY(n, m )denotes the ﬁngerprint bits of the com-\n498/G46 /G50 /G20 /G73 /G79 /G73 /G74 /G65 /G6D /G53 /G6F /G6E /G67 /G20 /G58 /G46/G58 /G20 /G28 /G6E /G2C /G6D /G29\n/G64 /G28 /G46/G58 /G28 /G6E /G2C /G6D /G29 /G3B /G20 /G46/G59 /G28 /G6E /G2C /G6D /G29 /G20 /G29 /G64/G70 /G28 /G53 /G6F /G6E /G67 /G58 /G3B /G20 /G53 /G6F /G6E /G67 /G59 /G29/G46 /G50 /G20 /G73 /G79 /G73 /G74 /G65 /G6D /G53 /G6F /G6E /G67 /G20 /G59 /G46/G59 /G20 /G28 /G6E /G2C /G6D /G29/G46 /G50 /G20 /G73 /G79 /G73 /G74 /G65 /G6D /G53 /G6F /G6E /G67 /G20 /G58 /G46/G58 /G20 /G28 /G6E /G2C /G6D /G29\n/G64 /G28 /G46/G58 /G28 /G6E /G2C /G6D /G29 /G3B /G20 /G46/G59 /G28 /G6E /G2C /G6D /G29 /G20 /G29 /G64/G70 /G28 /G53 /G6F /G6E /G67 /G58 /G3B /G20 /G53 /G6F /G6E /G67 /G59 /G29/G46 /G50 /G20 /G73 /G79 /G73 /G74 /G65 /G6D /G53 /G6F /G6E /G67 /G20 /G59 /G46/G59 /G20 /G28 /G6E /G2C /G6D /G29\nFigure 2: Relating differences in audio ﬁngerprints of two\nversions of the same recording, XandY, to differences\nin perceptual quality of these recordings.\npressed recording, Y. The difference between the ﬁnger-\nprints d(FX(n, m );FY(n, m ))is related to the (percep-\ntual) difference between the songs dπ(SongX;SongY).\nWe use the Philips audio ﬁngerprinting system [3] be-\ncause it is well documented, highly robust against com-\npression and differences in ﬁngerprint can be related to\nparameters used in MP3 compression. Recently, we have\nworked on a model of the ﬁngerprint generation of an un-\ncorrelated signal using the Philips system [9]. This paper\nfocuses on the difference between ﬁngerprints of a song\nand a distorted version of that same song.\nSection 2 of the paper presents details of the audio ﬁn-\ngerprinting system used for identifying songs, Section 3\nanalyzes the robustness of a ﬁngerprint to MP3 compres-\nsion of a song and presents experimental results, Section\n4 draws conclusions and outlines future work.\n2 PHILIPS AUDIO HASH\nFigure 3 shows an overview of the ﬁngerprint extraction\nstage of the Philips system [3]. The audio signal is ﬁrst\nsegmented into frames of 0.37 seconds with an overlap\nfactor of 31/32, weighted by a Hanning window. The\ncompact representation of a single frame is called a sub-\nﬁngerprint. In this way, it extracts 32-bit sub-ﬁngerprint s\nfor every interval of 11.6 ms (370/32 ms). Due to the large\noverlap, subsequent sub-ﬁngerprints have a large similar-\nity and slowly vary in time. The ﬁngerprint of a song con-\nsists of a sequence of sub-ﬁngerprints, which are stored in\na database.\nTo extract a 32-bit sub-ﬁngerprint for every frame, 33\nnon-overlapping frequency bands are selected from the es-\ntimated Power Spectral Density (PSD). These bands range\nfrom 300 Hz to 2000 Hz and are logarithmically spaced.\nHaitsma and Kalker report that experiments have\nshown that the sign of energy differences is a property that\nis very robust to many kinds of processing [3]. We denote\nthe energy of frequency band mof frame nbyE(n, m ).\nEnergy differences are computed in time and frequency:\nED(n, m ) =E(n, m )−E(n, m +1)\n−(E(n−1, m)−E(n−1, m+1)).(1)\nThe bits of the sub-ﬁngerprint are derived by\nF(n, m ) =/braceleftBigg\n1ED(n, m )>0\n0ED(n, m )≤0, (2)\nwhere F(n, m )denotes the mthbit of sub-ﬁngerprint n./G46 /G6F /G75 /G72 /G69 /G65 /G72\n/G54 /G72 /G61 /G6E /G73 /G66 /G6F /G72 /G6D/G46 /G72 /G61 /G6D /G69 /G6E /G67\n/G7C /G46 /G46 /G54 /G7C/G45 /G6E /G65 /G72 /G67 /G79\n/G43 /G6F /G6D /G70 /G75 /G74 /G61 /G74 /G69 /G6F /G6E/G42 /G61 /G6E /G64\n/G44 /G69 /G76 /G69 /G73 /G69 /G6F /G6E\n/G46 /G28 /G6E /G2C /G30 /G29/G42 /G69 /G74 /G20 /G44 /G65 /G72 /G69 /G76 /G61 /G74 /G69 /G6F /G6E\n/GE5 /G78/G32\n/GE5 /G78/G32\n/GE5 /G78/G32\n/GE5 /G78/G32/G2B\n/G2D/G54/G2D\n/G2B/G3E /G30\n/G2B\n/G2D/G54/G2D\n/G2B/G3E /G30\n/G2B\n/G2D/G54/G2D\n/G2B/G3E /G30/G46 /G28 /G6E /G2C /G31 /G29\n/G46 /G28 /G6E /G2C /G33 /G31 /G29/G46 /G6F /G75 /G72 /G69 /G65 /G72\n/G54 /G72 /G61 /G6E /G73 /G66 /G6F /G72 /G6D/G46 /G72 /G61 /G6D /G69 /G6E /G67\n/G7C /G46 /G46 /G54 /G7C/G45 /G6E /G65 /G72 /G67 /G79\n/G43 /G6F /G6D /G70 /G75 /G74 /G61 /G74 /G69 /G6F /G6E/G42 /G61 /G6E /G64\n/G44 /G69 /G76 /G69 /G73 /G69 /G6F /G6E\n/G46 /G28 /G6E /G2C /G30 /G29/G42 /G69 /G74 /G20 /G44 /G65 /G72 /G69 /G76 /G61 /G74 /G69 /G6F /G6E\n/GE5 /G78/G32\n/GE5 /G78/G32\n/GE5 /G78/G32\n/GE5 /G78/G32/G2B\n/G2D/G54/G2D\n/G2B/G3E /G30/G2B\n/G2D/G54/G54/G2D\n/G2B/G3E /G30\n/G2B\n/G2D/G54/G2D\n/G2B/G3E /G30/G2B\n/G2D/G54/G54/G2D\n/G2B/G3E /G30\n/G2B\n/G2D/G54/G2D\n/G2B/G3E /G30/G2B\n/G2D/G54/G54/G2D\n/G2B/G3E /G30/G46 /G28 /G6E /G2C /G31 /G29\n/G46 /G28 /G6E /G2C /G33 /G31 /G29\nFigure 3: Philips audio ﬁngerprinting extraction [3]. T\nindicates a unit-time delay.\nFigure 4(a) shows an example of a ﬁngerprint. White\nparts indicate positive energy differences (i.e. F(n, m ) =\n1). The small side of the ﬁngerprint block is the frequency\ndirection, consisting of the 32 bits corresponding to the\ndifferences between the 33 frequency bands. The long\nside of the block corresponds to the temporal dimension.\nThe system is capable of identifying a segment\nof about 3.3 seconds of music - generating 256 sub-\nﬁngerprints - in a large database, even if the segment is\ndegraded due to a variety of signal processing operations.\nA match is found if the Bit Error Rate (BER) between\nthe extracted ﬁngerprint and the ﬁngerprint in the database\nfalls below a threshold of 0.35.\n3 FINGERPRINT ROBUSTNESS\nANALYSIS TO MP3 COMPRESSION\nWhen the song is subject to compression, the ﬁngerprint\nchanges slightly. To indicate the effect of MP3 compres-\nsion on the ﬁngerprint extraction, Figures 4(c)-4(e) show\nthe difference patterns of the ﬁngerprint of a recording\nat different bit-rates relative to the ﬁngerprint of the CD-\nquality recording of the same song. The difference be-\ntween ﬁngerprints can be deﬁned as:\nFdiff(n, m ) =XOR (FX(n, m ), FY(n, m )) (3)\nThe black sections mark the ﬁngerprint differences, white\nsections indicate similarity between the ﬁngerprints.\nThe goal is to relate the perceptual quality of the com-\npressed version of the song (relative to the original record -\ning) to features of the observed difference in the corre-\nsponding ﬁngerprints, f(Fdiff(n, m )). The intended use\nis illustrated in Figure 4(f). The central research questio n\nis how to deﬁne the quality distance measure and the func-\ntionf(·)operating on Fdiff(n, m )(e.g. BER).\nSection 3.1 presents a simple model to analyze the re-\nlation between Signal-to-Noise Ratio (SNR) and BER for\nuncorrelated signals, Section 3.2 discusses the relation b e-\ntween the spectral content of a song and the robustness of\nthe ﬁngerprint bits, Section 3.3 presents details about two\nﬁngerprint distance measures used in the experiments pre-\nsented in Section 3.4.\n3.1 Analysis using uncorrelated signals\nIn previous work we have modeled the Philips ﬁngerprint\nextraction for uncorrelated, stationary data sources [9].\n499(a)\n (b)\n (c)\n (d)\n (e)/G53 /G6D /G61 /G6C /G6C\n/G44 /G69 /G66 /G66 /G65 /G72 /G65 /G6E /G63 /G65/G4C /G61 /G72 /G67 /G65\n/G44 /G69 /G66 /G66 /G65 /G72 /G65 /G6E /G63 /G65/G48 /G69 /G67 /G68\n/G51 /G75 /G61 /G6C /G69 /G74 /G79/G4C /G6F /G77\n/G51 /G75 /G61 /G6C /G69 /G74 /G79/G51 /G75 /G61 /G6C /G69 /G74 /G79\n/G49 /G6E /G64 /G69 /G63 /G61 /G74 /G6F /G72\n/G66 /G20 /G28 /G46/G64 /G69 /G66 /G66 /G29 /G53 /G6D /G61 /G6C /G6C\n/G44 /G69 /G66 /G66 /G65 /G72 /G65 /G6E /G63 /G65/G4C /G61 /G72 /G67 /G65\n/G44 /G69 /G66 /G66 /G65 /G72 /G65 /G6E /G63 /G65/G48 /G69 /G67 /G68\n/G51 /G75 /G61 /G6C /G69 /G74 /G79/G4C /G6F /G77\n/G51 /G75 /G61 /G6C /G69 /G74 /G79/G51 /G75 /G61 /G6C /G69 /G74 /G79\n/G49 /G6E /G64 /G69 /G63 /G61 /G74 /G6F /G72\n/G66 /G20 /G28 /G46/G64 /G69 /G66 /G66 /G29\n(f)\nFigure 4: Fingerprints for an excerpt of ’Anarchy in the U.K. ’ by the Sex Pistols. (a)-(b) Fingerprints of (a) the origina l\nand (b) of an MP3 compressed version encoded at 128 kbps; whit e indicates F(n, m ) = 1 (c-e) Differences between the\nﬁngerprints of the original and an MP3 compressed version en coded at (c) 128 kpbs (d) 80 kpbs and (e) 32 kbps. The\nblack positions mark the differences. (f) Relating differe nces between two ﬁngerprints Fdiff to a quality indication.\nOf course, music is strongly correlated and highly non-\nstationary. These models, however, help to understand the\neffect of key signal and ﬁngerprint parameters such as the\nframe length and the amount of frame overlap.\nHere we extend the analysis to the situation where\nboth the signal and the additive noise are assumed to be\nzero-mean Gaussian, independent identically distributed\n(iid) data sources. Such an analysis relates the BER to\nSNR. Although SNR is not a realistic real-life quality\nmeasure, it is a suitable distortion measure at the abstrac-\ntion level of this analysis.\nTo simplify the analysis, the ﬁngerprints are subjected\nto two constraints. First, for these data sources the BER is\nindependent of the number of frequency bands. Therefore,\nwithout loss of generality, we limit the analysis to two fre-\nquency bands. Second, for these data sources, the amount\nof frame overlap has no inﬂuence on the BER when the\nﬁngerprints are perfectly aligned. Therefore, we assume\nnon-overlapping windows.\nOur ﬁrst analysis starts with a simple model for the\nenergy differences, ED(n, m ), that lead to the ﬁngerprint\nbits using Eq. (2). Index mis omitted, since the analysis\nassumes two frequency bands, resulting in energy differ-\nences ED(n, m )having just one frequency index. Let\nEDX(n)denote the energy differences of signal X, and\nEDW(n)denote the energy differences of the noise, W.\nIn case of additive noise, the signal EDY(n)becomes:\nEDY(n) =EDX(n) +EDW(n). (4)\nFX(n)is generated by taking the sign of EDX(n):\nFX(n) =/braceleftBigg\n1EDX(n)>0\n0EDX(n)≤0. (5)\nThe BER can now be expressed in terms of probabilities:\nBER temp =P[FX(n)∝ne}ationslash=FY(n)]\n= 2P[EDX(n)>0, ED Y(n)≤0]\n= 2P[EDX(n)>0,\nEDW(n)≤ −EDX(n)]. (6)Since both EDX(n)andEDW(n)are mutually indepen-\ndent, zero-mean Gaussian iid data sources, all signals are\nfully characterized by their variance:\nV AR [EDX(n)] =σ2\nED X∝σ2\nX\nV AR [EDW(n)] =σ2\nED W∝σ2\nW\nV AR [EDY(n)] =σ2\nED X+σ2\nED W(7)\nWe will now express both SNR and BER in terms of the\nvariances σ2\nXandσ2\nW. The SNR is deﬁned as:\nSNR =V AR [X]\nV AR [Y−X]=σ2\nX\nσ2\nW(8)\nBy simple geometrical arguments using the joint PDF\nofEDX(n)andEDW(n),fED X,ED W(x, w), it can be\nshown that the BER is equal to:\nBER temp = 2P[EDX(n)>0,\nEDW(n)≤ −EDX(n)]\n= 2/integraldisplay0\n−∞fED W(w)/braceleftbigg/integraldisplay−w\n0fED X(x)dx/bracerightbigg\ndw\n=1\nπarctan/parenleftbiggσED W\nσED X/parenrightbigg\n=1\nπarctan/parenleftBigg/radicalBigg\nV AR [EDX]\nV AR [EDY−EDX]/parenrightBigg\n(9)\n=1\nπarctan/parenleftbigg1√\nSNR/parenrightbigg\n(10)\nTo illustrate the geometrical argument, Figure 5(a)\nshows the joint PDF fED X,ED W(x, w). The axes of the\nground plane represent the unit-variance variablesED X\nσEDX\nandED W\nσEDW. The PDF is now rotation-symmetric. The vol-\numeV ol=P[EDX>0, ED W≤−EDX]can be computed\nby rotating the light shaded area around the fED X,ED W-\naxis over an angle φ. Since the total volume of the PDF is\nequal to 1, the relation between φand the shaded volume\nis given by V ol =φ\n2π. The line EDX=−EDWhas an\nangleφ= arctan/parenleftBigσEDW\nσEDX/parenrightBig\nwith the EDW-axis.\n500−20 2   \n  −20 2   00.10.2\nED X\nσEDXED W\nσEDWfED X,ED W(x, w)\n(a)−20 −10 0 10 20 30 40 50 6010−410−310−210−1100\nSNR  [dB]Fingerprint BERAnalytical: BERtime\nAnalytical: BERfreq\nGenerated Data\nMotorhead @ Noise\nMotorhead @ MP3\n(b)\nFigure 5: Robustness to additive noise (analytical and expe rimental) (a) Illustrating the geometrical argument used t o\ncompute P[FX(n) = 1, FY(n) = 0] , which is a volume in the joint-PDF fX,W(x, w)(b) SNR vs. BER for a simple\nmodels using iid Gaussian random variables.\nFigure 5(b) shows the SNR vs. BER temp plot for\nboth experimental ﬁngerprint of Gaussian zero-mean iid\ndata and the analytical results of Eq. (10). The analytical\ncurve is shifted with respect to the experimental curve.\nThis deviation is caused by the computation of the ﬁnger-\nprints in the frequency domain instead of the time domain.\nIn the frequency domain, the spectrum of Y(n)is\nrelated to the spectra of X(n)andW(n):\n|ˆY(k)|2=|ˆX(k) + ˆW(k)|2\n=|ˆX(k)|2+|ˆW(k)|2+ 2Re/parenleftBig\nˆX(k)ˆW(k)/parenrightBig\n(11)\nwhere ˆY(k)denotes the Fourier transforms of Y(n)and\nˆW(k)denotes the complex conjugate of ˆW(k).\nIn order to compute the BER using Eq. (9), the vari-\nances of EDXandEDYare expressed in terms of fre-\nquency and time domain signal variances [10]:\nV AR [EDX]∝V AR/bracketleftBig\n|ˆX(k)|2/bracketrightBig\n∝σ4\nX\nV AR [EDY−EDX]∝V AR/bracketleftBig\n|ˆY(k)|2− |ˆX(k)|2/bracketrightBig\n∝σ4\nW+ 2σ2\nXσ2\nW\nNow the BER can be expressed as:\nBER freq =1\nπarctan/parenleftBigg/radicalBigg\nσ4\nW\nσ4\nX+ 2σ2\nW\nσ2\nX/parenrightBigg\n(12)\nFigure 5(b) also shows the curve for the BER freq expres-\nsion, which ﬁts perfectly to the experimental data. For\nSNR≫1the expression can be further simpliﬁed to:\nBER freq≈1\nπarctan/parenleftbigg√\n2σW\nσX/parenrightbigg\n≈√\n2\nπσW\nσX(13)\nThis implies that for sufﬁciently large SNR, the ﬁn-\ngerprint BER is reduced by a factor 10 when the SNR is\nincreased by 20 dB.3.2 Content dependence\nFigures 6(a) and 6(d) shows the BER between original and\na compressed version for blocks of 64 sub-ﬁngerprints of\ntwo songs: ‘Requiem - Pie Jesu’ composed by Faur´ e and\n‘Mot¨ orhead’ by Mot¨ orhead, respectively. Two observa-\ntions can be made from this graph: First, there is a large\ninter -song variance. Different songs compressed at the\nsame bit-rate show different average behavior. Second, a\nsong can have a large intra -song variance as well. Faur´ e\nshows a large spread in BER behavior, while the ﬁnger-\nprint blocks of Mot¨ orhead show a very small spread.\nThe spectrograms are shown in Figures 6(b) and 6(e).\nThe horizontal axis shows the starting time of a frame,\nthe vertical axis shows the frequency dimension and the\ngray-value indicates the magnitude in the energy spec-\ntrum of each frame in decibel (dB). Lighter values indi-\ncate larger magnitude. Comparing Figures 6(a) and 6(d)\nwith Figures 6(b) and 6(e), respectively, clearly relates t he\nBER of a block to spectral characteristics of that region\nin time. The spectrogram of Faur´ e shows distinct peaks\nand regions which have near-zero energy. These regions\nin the spectrogram containing near-zero energy generate\nnear-zero ED(n, m )signals which are rather sensitive to\ncompression artifacts. Since these valleys in the spectro-\ngram do not occur uniformly over time, they cause a large\nspread in time of the BER.\nThe difference in behavior of these regions is also re-\nﬂected in the ﬁngerprint blocks. Figures 6(c) and 6(f)\nshow two differences between ﬁngerprint blocks having\nthe same number of bit errors. Figure 6(c) corresponds\nto a part of Faur´ e (having relatively little energy) while\nFigure 6(f) corresponds to a part of Mot¨ orhead.\n3.3 Distance measures\nThe large intra-song variance might be reduced by using\nother distance measures than the BER, e.g. average length\nor average area of runs of ﬁngerprint errors. In this paper\nwe use two distance measures: BER based on the ham-\nming distance and BER based on the weighted hamming\n5010 5 10 15 20 25 30 3500.050.10.150.20.250.30.35\nStarting time of the fingerprint block  [sec]Average BER\n(a)\nTime  [sec]Frequency  [Hz]\n0 5 10 15 20 25 30 35400600800100012001400160018002000\n(b)\n (c)\n0 5 10 15 20 25 30 3500.050.10.150.20.250.30.35\nStarting time of the fingerprint block  [sec]Average BER\n(d)\nTime  [sec]Frequency  [Hz]\n0 5 10 15 20 25 30 35400600800100012001400160018002000\n(e)\n (f)\nFigure 6: (a,d) Bit Error Rate as function of time, (b,e) Spec trograms and (c,f) ﬁngerprint differences between origina l\nrecording and MP3@32 kbps of (a-c) Faur´ e and (d-f) Mot¨ orhe ad.\ndistance. The former is deﬁned as:\nBER =1\n32NN−1/summationdisplay\nn=031/summationdisplay\nm=0Fdiff(n, m ), (14)\nthe latter is deﬁned as:\nBER W=N−1/summationdisplay\nn=031/summationdisplay\nm=0H(n, m )Fdiff(n, m )\nN−1/summationdisplay\nn=031/summationdisplay\nm=0H(n, m ). (15)\nCertain regions can be excluded by setting the weights\nH(n, m )to 0. For the experiments, the weights are set\nbased on the ED(n, m )signal.\nLetEDmax\nh(n, m )denote the maximum value of\n|ED(n, m )|within a sliding window of size h. The binary\nweight H(n, m )is zero if its corresponding ED(n, m )\nvalue is smaller than a signal dependent threshold T:\nH(n, m ) =/braceleftBigg\n0 EDmax\nh(n, m )≤T\n1 EDmax\nh(n, m )> T(16)\n3.4 Experiments\nExperiments have been performed on fragments of 39 sec-\nonds for 11 songs. To keep the ﬁgures comprehensible\nthe results presented here are limited to the 2 songs men-\ntioned earlier, viz. Faur´ e and Mot¨ orhead. The ﬁngerprint sof these fragments were split into 13 non-overlapping ﬁn-\ngerprint blocks consisting of 256 sub-ﬁngerprints.\nWhen compressing a song, the perceptual quality can\nbe controlled by selecting the bitrate. This indirectly inﬂ u-\nences the difference between the ﬁngerprints. Two quality\nmeasures have been used: the MP3 bitrate and the SNR.\nThe weights H(n, m )were assigned using a threshold\nTequal to the global median value of |ED(n, m )|and a\nwindow size hof 32 frames. Depending on (the part of)\nthe song, the 8-35% of the bits was excluded.\nFigure 7 shows the experimental results. Figures 7(a)\nand 7(c) use BER, while 7(b) and 7(d) use BER W; Figure\n7(a) and 7(b) use MP3 bitrate as quality indicator while\n7(c) and 7(d) use SNR. The lines indicate the results av-\neraged over the 13 ﬁngerprint blocks, the errorbars and\nshaded regions indicate the corresponding standard devi-\nations. Both SNR - in dB - and BER are displayed on\na logarithmic scale. A straight line in a plot using loga-\nrithmical scales indicates a power law relation ship. From\nFigures 7(c) and 7(d), the relation between the expected\nvalue of the BER and the SNR conﬁrms Eq. (13):\nE[BER ]∝1√\nSNR=σW\nσX. (17)\nFrom the experiments it can be concluded that the\nBER between ﬁngerprints originating from the same au-\ndio ﬁle is inversely proportional to the square root of the\nSNR of one song with respect to the other. Relating BER\nto bitrate is less straightforward, since compressing diff er-\nent songs at the same bitrate yield different SNR.\n5020 50 100 150 200 250 30000.050.10.150.20.25\nMP3 Bit rate  [kbps]Fingerprint BERFaure\nMotorhead\n(a)0 50 100 150 200 250 30000.050.10.150.20.25\nMP3 Bit rate  [kbps]Fingerprint BERFaure\nMotorhead\n(b)\n10 15 20 25 30 35 40 45 50 5510−310−210−1100\nSNR  [dB]Fingerprint BERFaure\nMotorhead\n(c)10 15 20 25 30 35 40 45 50 5510−310−210−1100\nSNR  [dB]Fingerprint BERFaure\nMotorhead\n(d)\nFigure 7: Average BER in 13 ﬁngerprint blocks for two songs. T he errorbars and shadings indicate the standard deviation\nof the BER at a speciﬁc bitrate or SNR level. (a-b) MP3 bitrate vs. (a) BER (b) BER W; (c-d) SNR vs. (c) BER (d) BER W\n4 CONCLUSION AND FUTURE WORK\nExperiments have indicated how differences in audio ﬁn-\ngerprint due to compression are related to the spectral\ncharacteristics of the audio signal. Variations in these lo -\ncal spectral characteristics cause a large variation in the\nbehavior of ﬁngerprint differences for a given compres-\nsion bitrate or SNR. We have shown that this variation\ncan be reduced when the ﬁngerprint bits related to spec-\ntral regions with near-zero energy are excluded. It was\ndetermined both theoretically and experimentally that the\nBER is approximately inversely proportional to the SNR\nof the signal.\nFuture work concerns the further exploration and the-\noretical foundation SNR-BER relationship, its expansion\nto bitrate-BER relations, and the deﬁnion of a similarity\nmetric which is suitable for quality assessment using ﬁn-\ngerprints.\nREFERENCES\n[1] J. Liang et al. Pollution in P2P ﬁle sharing systems.\nInIEEE Infocom , March 2005.\n[2] T. Kalker et al. Music2Share copyright-compliant\nmusic sharing in P2P systems. Proceedings of the\nIEEE , 92(6):961 – 970, 2004.[3] J. Haitsma and T. Kalker. A highly robust audio ﬁn-\ngerprinting system. In 3rd Int. Symp. on Music In-\nformation Retrieval (ISMIR) , October 2002.\n[4] P. Cano, E. Batlle, T. Kalker, and J. Haitsma. A re-\nview of algorithms for audio ﬁngerprinting. In IEEE\nInt. Workshop on Multimedia Sig. Proc. , Dec. 2002.\n[5] E. Allamanche et al. Content-based identiﬁcation of\naudio material using mpeg-7 low level description.\nIn2ndISMIR , October 2001.\n[6] A. Wang. An industrial strength audio search algo-\nrithm. In 4thISMIR , October 2003.\n[7] Y . Cheng. Music database retrieval based on spectral\nsimilarity. In 2ndISMIR , October 2001.\n[8] P. Cano, E. Batlle, H. Mayer, and H. Neuschmied.\nRobust sound modeling for song detection in broad-\ncast audio. In 112th AES Convention , 2002.\n[9] P.J.O. Doets and R.L. Lagendijk. Stochastic model\nof a robust audio ﬁngerprinting system. In 5thISMIR ,\nOctober 2004.\n[10] A. Leon-Garcia. Probability and Random Processes\nfor Electrical Engineering . ISBN 0-201-50037-X.\nAddison-Wesley, 2ndedition, 1994.\n503"
    },
    {
        "title": "The 2005 Music Information retrieval Evaluation Exchange (MIREX 2005): Preliminary Overview.",
        "author": [
            "J. Stephen Downie",
            "Kris West",
            "Andreas F. Ehmann",
            "Emmanuel Vincent 0001"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416044",
        "url": "https://doi.org/10.5281/zenodo.1416044",
        "ee": "https://zenodo.org/records/1416044/files/DownieWEV05.pdf",
        "abstract": "preliminary overview of the 2005 Music Information Retrieval Evaluation eXchange (MIREX 2005). The MIREX organizational framework and infrastructure are outlined. Summary data concerning the 10 evaluation contests is provided. Key issues affecting future MIR evaluations are identified and discussed. The paper concludes with a listing of targets items to be undertaken before MIREX 2006 to ensure the ongoing success of the MIREX framework.",
        "zenodo_id": 1416044,
        "dblp_key": "conf/ismir/DownieWEV05",
        "keywords": [
            "preliminary overview",
            "2005 Music Information Retrieval Evaluation eXchange (MIREX 2005)",
            "MIREX organizational framework and infrastructure",
            "10 evaluation contests",
            "future MIR evaluations",
            "identified and discussed",
            "targets items",
            "MIREX 2006",
            "ongoing success",
            "MIREX framework"
        ],
        "content": "THE 2005 MUSIC INFORMATION RETRIEVAL EVALUATION \nEXCHANGE (MIREX 2005): PRELIMINARY OVERVIEW\nJ. Stephen Downie Kris West Andreas Ehmann Emmanuel Vincent \nGSLIS \nUniversity of Illinois at \nUrbana-Champaign  \njdownie@uiuc.edu  School of Computing \nSciences, University of \nEast Anglia \nkw@cmp.uea.ac.uk  Electrical Engineering \nUniversity of Illinois at \nUrbana-Champaign \naehmann@uiuc.edu  Electronic Engineering \nQueen Mary \n University of London  \nemmanuel.vincent\n@elec.qmul.ac.uk \nABSTRACT \nThis paper is an extended abstract which provides a brief \npreliminary overview of the 2005 Music Information Retrieval Evaluation eXchange (MIREX 2005). The MIREX organizational framework and infrastructure are outlined. Summary data concerning the 10 evaluation contests is provided. Key issues affecting future MIR evaluations are identified and discussed. The paper con-\ncludes with a listing of targets items to be undertaken \nbefore MIREX 2006 to ensure the ongoing success of the MIREX framework.   \nKeywords: MIREX 2005, evaluation.  \n1 INTRODUCTION \nThis extended abstract provides a brief overview of the \n2005 Music Information Retrieval Evaluation eXchange (MIREX 2005) contest run in conjunction with the 6th International Conference on Music Information Retrieval (ISMIR 2005) held in London, UK, 11 September to 15 September, 2005. Although 2005 is the inaugural year for MIREX, MIREX 2005 should be considered a direct \ndescendant of the very successful Audio Description \nContest (ADC 2004) organized by the Music Technol-ogy Group (MTG), Universitat Pompeu Fabra (UPF), in Barcelona, Spain, as part of the ISMIR 2004 conference (see http://ismir2004.ismir.net/ISMIR_Contest.html).  \nBoth ADC 2004 and MIREX 2005 were convened in \nresponse to the long-held desire of the MIR community to establish formal evaluation frameworks and metrics with which researchers could scientifically compare and \ncontrast their wide variety of approaches to solving MIR tasks. Downie [1] provides an introduction to the issues involved in the establishment of such frameworks and metrics.  \nIn Section 2 we outline the basic organizational \nscheme and infrastructure for MIREX 2005. In Section 3 we discuss some of the important issues brought to the fore while organizing MIREX 2005. Section 4 is de-voted to listing the set of target items that are designed to build upon the successes of MIREX 2005 so that fu-ture iterations of MIREX can be more useful to the MIR community. \n2 MIREX 2005 ORGANIZATION \n2.1 Defining the MIREX 2005 contests \nMIREX 2005 was co-chaired by Downie of the Gradu-\nate School of Library and Information Science (GSLIS), \nUniversity of Illinois at Urbana-Champaign (UIUC) and \nVincent of Electronic Engi neering, Queen Mary, Uni-\nversity of London. Like ADC 2004, the choice of evaluation scenarios and metrics for MIREX 2005 was based on the expressed interests of the MIR community \nitself. The two primary me dia for community decision \nmaking were the MIREX mailing list (157 subscribers) (https://mail.isrl.uiuc.edu/mailman/listinfo/evalfest) and the MIREX Wiki (http://www.mu sic-ir.org/mirexwiki/). \nThe mailing list and Wiki were established to help or-ganize MIREX 2005 and through these interfaces pro-posals for evaluation tasks to be performed at MIREX 2005 were received. Each propos al included an approach \nto evaluating a MIR task, one or more evaluation metrics to be used in scoring performance on that task, and the nomination of potential databases that could be used to evaluate performance. Each proposal underwent signifi-cant refinement through this community dialogue proc-ess. Refinements included the addition of new data sets and major/minor modifications to the evaluation metrics. \nAfter lively community debate on both the Wiki and \nthe mailing list, a roster of 10 evaluation contests was settled upon for MIREX 2005. Special mention must be made of the “contest leaders”  for they played pivotal \nroles in the moderation of the community dialogue and the finalizing of the contest scenarios. The contest names and summary data about each contest can be found in Table 1. \n2.2 MIREX 2005 Infrastructure \nThe locus of the MIREX 2005 evaluation work was the \nInternational Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) housed in GSLIS, UIUC [2]. IMIRSEL provided MIREX 2005 with three key components:  1. the aforementioned communications mechanisms; \n2. the computational infrastructure; and, \n3. the M2K evaluation frameworks for each contest (see Section 2.2.2).  \nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full citation on the first page. \n© 2005 Queen Mary, University of London \n320   \n \n 2.2.1 Computational infrastructure \nThe submissions to MIREX 2005 were designed to run \non one or more of the Windows, Linux/Unix and Mac OS X architectures. Table 2 summarizes the hardware setup at IMIRSEL that was used to run the evaluation experiments. Post-evaluati on standardized benchmarking \nof the individual computer processing speeds is planned so contestants can better ascer tain the relative speed per-\nformances for those contests that spanned different com-puter architectures. \n2.2.2 Evaluation frameworks infrastructure using M2K In order to enable, coordinate and evaluate submissions \nto MIREX, a software framework was developed by the IMIRSEL team. This software framework had to be able to support submissions in a variety of formats including (but not limited to): C, C++, Java, Python and Matlab. The final solution is based in the Data-to-Knowledge (D2K) Toolkit and is included as part of the Music-to-Knowledge (M2K) Toolkit [2]. Both D2K and M2K are implemented in Java, giving them near total platform independence. M2K is an open-source initiative, mean-ing that any individual or group may leverage or modify this software and it can be evolved to support future evaluations. M2K is freely available from http://music-ir.org/evaluation/m2k. \nThe MIREX evaluation frameworks are implemented \nin M2K’s modular format. Modules are connected by an \nXML-based itinerary which describes the particular process flow for each evaluation task. Figure 1 is a sample M2K MIREX evaluation itinerary. These frame-works are extremely flexible and can be customized by \nparticipants to suit the specific topologies of their sub-missions. This represents a significant advance over \ntraditional evaluation frameworks and supports the cen-tral evaluation paradigm necessitated by the unique challenges posed by MIR evaluation. \n3 DISCUSSION \n3.1 Importance of MIREX \nTo get a sense of the impor tance that the MIR commu-\nnity has attached to MIREX 2005—in particular—and to \nthe need for scientific evaluation—in general—it is \nworthwhile to note the strong evidence of growth be-tween ADC 2004 and MIREX 2005. For example, ADC 2004 attracted 20 individual participants from 12 re-search labs; whereas, MIREX 2005 has 82 individual \nparticipants representing 41 different labs. ADC 2004 comprised 5 audio-based contests: Melody Extraction (4 submissions), Artist Identification (2 submissions), Rhythm Classification (1 submission), Music Genre Classification (5 submissions) and Tempo Induction (6 submissions) for a total of 18 primary submissions\n1 [3]. \nA comparison of these data with the 72 primary submis-\nsions, distributed across 10 contests, for MIREX 2005 reveals an encouraging broadening of community inter-est in formal evaluation tasks. Furthermore, the number of primary submissions per contest for MIREX 2005 ranges from 5 (Symbolic Key Detection) to 13 (Audio \nGenre Classification) and repr esents a healthy deepening \nof researcher participation.  \n                                                          \n \n1 The submission counts given above reflect only the “primary” sub-\nmissions as some teams submitted for evaluation several algorithmic \nvariants of their techniques to each contest.  Table 1. MIREX 2005 summary data. \nContest Name Submissions Countries Individuals Contest Leaders \nAudio Artist Identification   8 5 13  K. West \nAudio Drum Detection   7 7 10  K. Tanghe \nAudio Genre Classification  13 11 21  K. West \nAudio Key Detection  5 3 6  C.-H. Chuan & E. Chew \nAudio Melody Extraction   8 7 12  G. Poliner & D. Ellis \nAudio Onset Detection 7 5 11  P. Leveau, P. Brossier & E. Vincent \nAudio Tempo Detection    8 6 12  M. McKinney & D. Moelants \nSymbolic Genre  5 4 9  C. McKay \nSymbolic Key Detection 5 3 6  A. Mardirossian & E. Chew \nSymbolic Melodic Similarity 6 6 15  R. Typke  \nTable 2. Computational infrastructure for MIREX 2005. \nMachine Names OS Processor RAM Disk(s) \nFAST WIN XP  AMD Athlon XP 2600+ 1.9 GHz 2GB 80 GB\nRED, YELLOW, GREEN WIN XP  Intel Pentium 4 3.0 GHz 3GB 80 GB + 80 GB\nBIBE OS X  PowerPC G4 450 MHz 768 MB 20 GB + 20 GB\nLINUX RedHat 9 AMD Athlon XP 2600+ 1.9 GHz 1GB 80 GB\nBeerClusterHead  CentOS Dual AMD Opteron 64 1.6 GHz 4GB 1.8 TB NFS RAID\nBeerClusterSlaves (x4) CentOS Dual AMD Opteron 64 1.6 GHz 4GB 160 GB Local disks \n321   \n \n 3.2 Continuing Challenges \nNotwithstanding the advancements made by ADC 2004 \nand MIREX 2005 on deepening and broadening the scope of formal MIR evaluations, there remain several serious challenges facing the MIR community that must be overcome in order to conduct future MIREX contests that consistently provide meaningful and fair scientific evaluations. These challenges include: 1. the continued near impossibility of establishing a common set of evaluation databases or the sharing of databases among researchers due primarily to in-\ntellectual property restrictions and the financial im-\nplications of those restrictions;  \n2. the ongoing lack of established evaluation metrics for the plethora of tasks in MIR; and,  \n3. the general tendency in the field to use small data-bases for evaluation with the difficulties associated with the creation of ground-truth data being a pri-mary cause of this state of affairs.  \nThe MIREX 2005 organizers recognize that these \naforementioned hurdles will not be overcome in the near future. We do, however, want to briefly highlight some of the implications of these ongoing challenges. \n3.2.1 The constant ad hoc evolution of metrics More than half of the evaluation tasks proposed for \nMIREX 2005 had evaluation metrics established or sig-nificantly refined through the MIREX communication process. While many of the metrics decided upon are based on principled evaluation procedures used in other fields, a close reading of the ongoing metrics discussions reveals a decidedly ad hoc  approach to MIR evaluation \nmetrics: in almost every case the initially proposed met-\nrics were significantly refined through participant dis-cussion and in several cases completely new metrics were established. Furthermore, tasks that enjoyed rela-tively well-established evaluation procedures, such as Audio and Symbolic Genre Classification and Artist Identification, had interesting evaluation metrics added to them, such as the discounting of confusion in the clas-\nsifications through the use of hierarchical taxonomies.  \nThe MIREX 2005 team fully appreciates the delicate \nbalancing act between the necessity of community input on metric decisions (which tend to generate ever chang-ing evaluation metrics) and the need to establish —\nperhaps even impose— universal, standardized metrics \nso that multi-year comparisons can be made. At this \npoint, we have no simple solution to offer. We are, how-ever, flagging the “ ad hoc  evolution of metrics” issue as \na high-priority “target item” (Section 4). \n3.2.2 The need for tests of statistical significance Due to the financial implications of collecting large au-\ndio databases for evaluation and the significant burden of annotating them, there is a general tendency to use relatively small databases for evaluation. The establish-ment of central evaluation paradigms like ADC 2004 and MIREX 2005, has helped to alleviate, but not elimi-nate, this problem. Audio databases and annotation sets are valuable resources and a reluctance to surrender that \ndata to a wider community is understandable. The MIR community has been remarkably open with their re-sources and has allowed the establishment of databases of much greater magnitude and coverage than existed prior to MIREX 2005. Despite this show of community goodwill, however, these data bases are still relatively \nlimited when compared to industrial-scale real-world problems. Because of these lim itations we need to inter-\npret the contest results achieved with care. \nTo mitigate this database-size limitation problem the \nIMIRSEL team has established tools in M2K that allow multiple, principled statistical significance tests to be applied to the comparison of results in every evaluation task performed at MIREX 2005. These techniques in-clude the:   1. Student’s t-Test;  \n2. Sign Test; and,  \n3. McNemar’s Test (see [4]).  \nThe Student’s t and the sign tests are methods of as-\nsessing the significance of differences in overall  per-\nformance between two systems. McNemar’s test, how-ever, takes into account the use of the same dataset in the comparative evaluation of two algorithms and as-\nsesses the significance of differences in performance on \nan item-by-item  basis. Thoughtful application of these \ntests in combination can yield important insights into \ntrue system performance. For ex ample, if a t-test yields \na non-significant difference between two algorithms, the \n                           Figure 1. A sample MIREX evaluation framework implemented in M2K.  \n322   \n \n results from a McNemar’s test on these algorithms can \nhelp determine whether the examined systems are ex-\nhibiting similar error functions. Opening dialogue con-cerning the application of test s of statistical significance \nis being flagged as another “target item” (Section 4). \n3.2.3 Need for collaborative annotations Due to the large number of tasks to be evaluated, the \nMIREX organizers could not possibly annotate all the evaluation data themselves. As a consequence, partici-pants were encouraged to contribute their annotated data and to conduct new annotations. Some of these partici-pants could be suspected of using the annotated data they contributed to fine tune their algorithms. However, if they were not trusted by the majority of other partici-pants, only a small subset of evaluations could have been run. This issue will become even more stringent when new tasks are evaluated. In the future, collaborative an-\nnotation of the testing data by a large number of partici-pants (all if possible) will be needed. The creation of an online collaborative annotation tool is another of our \"target items\" (Section 4). \n4 FUTURE WORK: KEY TARGET ITEMS \nThe MIREX 2005 organizers and the IMIRSEL team \nhave set up a list of 8 priority target items designed to improve upon the successes of ADC 2004 and MIREX 2005. These are items that we believe should be imple-mented prior to MIREX 2006. We have tasked ourselves to: 1. Establish a communications mechanism specifically \ndevoted to the establishment of standardized and \nstable evaluation metrics to replace the undesirable ad hoc  procedures currently being used. \n2. Open discussions on the selection of more statistical significance testing procedures. The current three implemented are only a beginning and are not uni-versally applicable because evaluation result data do not always conform to their underlying assumptions. \n3. Work with the MIR community to establish new annotation tools and procedures to overcome the shortage of available ground-truth data. Ideally, these tools should be open-sourced or perhaps made available via M2K in conjunction with the proposed webservices system mentioned in Item #8. \n4. Establish a more formal organizational structure for future MIREX contests. This year, the contest lead-ers became so by “default”. We need to have contest \nleaders who have formally accepted the various ad-ministrative tasks associated with setting up the in-dividual contests including acting as liaisons be-tween participants and the MIREX organizers. \n5. Convene an online forum to produce a high-level development plan for the future of the M2K Toolkit to solicit advice and opinions from the members of the MIR community on the se rvices and formats that \nwould be desirable in later versions of M2K.  \n6. Continue to develop the evaluator software and es-tablish an open-source evaluation API. This will in-volve the redevelopment of the existing evaluation modules, adding a greater degree of abstraction to allow for optimal reuse of code and aid in the de-velopment of evaluators for new MIR tasks. This may include the provision of ‘command line’ ver-sions of the evaluation systems. \n7. Make useful evaluation data publicly available year round. Care will have to be taken in doing this as making all of the data used to evaluate a task avail-able will preclude its use in future, fair evaluations as fine-tuning or over-fitting could be performed on this data. Therefore, di stributable “development” \ndata sets must also be established. Again, Item #8 should play a major role in making this a reality. \n8. Establish a webservices-based IMIRSEL/M2K online system prototype which would allow MIR re-searchers to run evaluations on centrally held data-sets and to compare their results against the earlier \nresults achieved by others on those data and query \nsets. Mounting community -accessible annotation \ntools should be seen as part of these webservices. \nACKNOWLEGMENTS \nThe IMIRSEL team is supported by the Andrew W. Mellon Foundation and the National Science Foundation (NSF) Grant Nos. NSF IIS-0340597 and NSF IIS-0327371. E. Vincent is funded by the EPSRC grant GR/S75802/01.We thank those who provided content and ground-truth data. We thank the Automated Learn-ing Group (ALG) at the National Center for Supercom-\nputing Applications (NCSA) at UIUC and Paul Lamere \nof Sun Labs for their kind assistance. IMIRSEL’s X. Hu, J. Futrelle, M. Callahan, C. Jones, D. Tcheng, M. McCrory, S. Kim and J-H. Lee are also thanked. Special thanks to the GSLIS technology services team. Finally, we thank the ISMIR 2005 organizing committee for making both ISMIR and MIREX 2005 a success. \nREFERENCES \n[1] Downie, J. The scientific evaluation of music \ninformation retrieval systems: Foundations and future. Computer Music Journal, 28, 2, (2004), 12-33. \n[2] Downie, J., Futrelle, J., and Tcheng., D. “The International Music Information Retrieval Systems Evaluation Laboratory: Governance, access and security”, Proceeding of the Fifth International \nConference on Music Information Retrieval \n(ISMIR), Barcelona, Spain, 2004. \n[3] Cano, P., Gómez, E., Gouyon, F., Herrera, P., Koppenberger, M., Ong, B., Serra, X., Streich, S., and Wack, N. ISMIR 2004 Audio Description Contest. Under review for journal publication. \n[4] Gillick, L., and Cox, S. “Some statistical issues in the comparison of speech recognition algorithms”, Proceedings of IEEE Conference on Acoustics, Speech and Signal Processing, Glasgow, UK, 1989. \n323"
    },
    {
        "title": "Finding Meter in Music Using An Autocorrelation Phase Matrix and Shannon Entropy.",
        "author": [
            "Douglas Eck",
            "Norman Casagrande"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415650",
        "url": "https://doi.org/10.5281/zenodo.1415650",
        "ee": "https://zenodo.org/records/1415650/files/EckC05.pdf",
        "abstract": "This paper introduces a novel way to detect metrical structure in music. We introduce a way to compute autocorrelation such that the distribution of energy in phase space is preserved in a matrix. The resulting autocorrelation phase matrix is useful for several tasks involving metrical structure. First we can use the matrix to enhance standard autocorrelation by calculating the Shannon entropy at each lag. This approach yields improved results for autocorrelationbased tempo induction. Second, we can efficiently search the matrix for combinations of lags that suggest particular metrical hierarchies. This approach yields a good model for predicting the meter of a piece of music. Finally we can use the phase information in the matrix to align a candidate meter with music, making it possible to perform beat induction with an autocorrelation-based model. We present results for several meter prediction and tempo induction datasets, demonstrating that the approach is competitive with models designed specifically for these tasks. We also present preliminary beat induction results on a small set of artificial patterns. Keywords: Meter prediction, tempo induction, beat induction, autocorrelation, entropy 1",
        "zenodo_id": 1415650,
        "dblp_key": "conf/ismir/EckC05",
        "keywords": [
            "metrical structure",
            "autocorrelation",
            "phase space",
            "energy distribution",
            "Shannon entropy",
            "tempo induction",
            "metrical hierarchies",
            "meter prediction",
            "beat induction",
            "autocorrelation-based model"
        ],
        "content": "Finding Meter inMusic Using anAutocorr elation Phase Matrix andShannon\nEntr opy\nDouglas Eck\nUniversity ofMontreal\nDepartment ofComputer Science\nCP6128, Succ. Centre-V ille\nMontreal, Quebec H3C 3J7CANADA\neckdoug@iro.umontreal.caNorman Casagrande\nUniversity ofMontreal\nDepartment ofComputer Science\nCP6128, Succ. Centre-V ille\nMontreal, Quebec H3C 3J7CANADA\ncasagran@iro.umontreal.ca\nABSTRA CT\nThis paper introduces anovelwaytodetect metrical struc-\ntureinmusic. Weintroduce awaytocompute autocorre-\nlation such thatthedistrib ution ofenergyinphase space is\npreserv edinamatrix. Theresulting autocorrelatio nphase\nmatrix isusefulforseveraltasks involving metrical struc-\nture. First wecanusethematrix toenhance standard auto-\ncorrelation bycalculating theShannon entrop yateach lag.\nThis approach yields impro vedresults forautocorrelation-\nbased tempo induction. Second ,wecanefﬁciently search\nthematrix forcombinations oflagsthatsuggest particular\nmetrical hiera rchies. This approach yields agood model\nforpredic ting themeter ofapiece ofmusic. Finally we\ncanusethephaseinformation inthematrix toalign acan-\ndidate meter with music, making itpossible toperform\nbeat induction with anautocorrelation-based model. We\npresent results forseveralmeter prediction andtempo in-\nduction datasets, demonstrating thattheapproach iscom-\npetiti vewithmodels designed speciﬁcally forthese tasks.\nWealso prese ntpreliminary beat induction results ona\nsmall setofartiﬁcial patterns.\nKeywords: Meter prediction, tempo induction, beat in-\nduction, autocorrelation, entropy\n1Introduction\nInthispaper weintroduce anautocorrela tion phase ma-\ntrix, atwo-dime nsional structure (computed from MIDI\nordigital audio) thatprovides thenecessar yinformation\nforestimating thelags andphases ofthemusic’ smetrical\nhierarch y.Weusethismatrix asthecore data struc tureto\nestimate themeter ofapiece (meter prediction), toesti-\nmate thetempo ofapiece (tempo induction) andtoalign\nthepiece ofmusic with thepredicted metrical structure\n(beat induction).\nWewillprovide algorithm details andexperimental re-\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forproﬁt orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation ontheﬁrstpage.\nc\r2005 Queen Mary ,University ofLondonsults formeter prediction andtempo induction. Wewill\nalsopresent some details conc erning thealignment ofthe\nmetrical structure with apiece ofmusic. Wewill also\npresent alignment results forasmall dataset ofartiﬁcial\npatterns. Howeverthedetails ofcomputing thisalignment\nonline (forbeat induction) arethetopic ofanother paper .\nThestructure ofthispaper isasfollows.InSection 2\nwewilldiscuss other approaches toﬁnding mete randbeat\ninmusic. InSection3wewilldescribe ourmodel consist-\ningofthecreat ionofanautocorrelation matrix, computa-\ntionoftheentrop yforeach laginthismatrix, theselection\nofametrical hierarch yandthealignment ofthehierarch y\nwith music. Finally inSection 4wepresent simulation\nresults.\nDue tospace constraints we haveomitted de-\ntails for aligning the autocorrelation phase matrix\nwith amusical signal soastoaid inbeat induc-\ntion. Alonger report containing these details is\navailable atwww.iro.umontreal.ca/˜eckdo ug/\npublications.html .\n2Meter andAutocorr elation\nMeter isthesense ofstrong andweak beats that arises\nfrom theinteraction among hierarchical levelsofse-\nquences having nested periodic components. Such ahi-\nerarch yisimplied inWesternmusic notation, where dif-\nferent levelsareindicated bykinds ofnotes (whole notes,\nhalf notes, quarter notes, etc.) andwhere bars establish\nmeasures ofanequal numb erofbeats (Handel, 1993). For\ninstance, mostcontemporary popsongs arebuiltonfour-\nbeat meters. Insuch songs, theﬁrst andthird beats are\nusually emphasized. Knowing themeter ofapiece ofmu-\nsichelps inpredicting other components ofmusical struc-\nturesuch asthelocati onofchord changes andrepetition\nboundaries (Cooper andMeyer,1960).\nAutocorrelation works bytransforming asignal from\nthetime domain into thefrequenc ydomain. Autocorre-\nlation provides ahigh-resolution picture oftherelati ve\nsalience ofdifferent periodicities, thus motivating itsuse\nintempo andmeter related music tasks .However,the\nautocorrelation transform discards allphaseinformation,\nmaking itimpossible toalign salient periodicities with\nthemusic. Thus autocorrela tion canbeused topredict,\nforexample, thatmusic hassomething thatrepeats every\n1000ms butitcannot saywhen therepetition takesplace\n504relati vetothestart ofthemusic. One primary goal ofour\nworkhere istocompute autocorrelation efﬁciently while\natthesame timepreservin gthephase information neces-\nsary toperform suchanalignment. Our solution isthe\nautocorrelation phase matri x.\nAutocorrelation iscertainly nottheonly waytoper-\nform meterprediction andrelated tasks liketempoinduc-\ntion. Adapti veoscillator models (LargeandKolen, 1994;\nEck, 2002) canbethought ofasatime-d omain correlate to\nautocorrelation based methods andhaveshownpromise,\nespecially incogni tivemodeling. Multi-agent systems\nsuch asthose byDixon (2001) havebeen applied with suc-\ncess. ashaveMonte -Carlo sampling (Cemgil andKappen,\n2003) andKalm anﬁltering methods (Cemgil etal.,2001).\nManyresearchers haveused autocorrelation formu-\nsicinformatio nretrie val.Due tospace constraints only\nashort listing isprovided here. Brown(1993) used au-\ntocorrelation toﬁnd meter inmusical scoresrepresented\nasnote onsetsweighted bytheir duration. Vosetal.\n(1994) proposed asimilar autocorrelation method. The\nprimary difference between their workandthatofBrown\nwastheiruseofmelodic interv alsincomputing accents.\nScheirer (1998) provided amodel ofbeat tracking that\ntreats audio ﬁles directly andperfo rmsrelati velywell over\nawide range ofmusical styles (41correct of60exam-\nples). Volk(2004) explored theinﬂuence ofinteractions\nbetween levelsinthemetricalhierarch yonmetrical ac-\ncenting. Toiviainen andEerola (2004) also investig ated\nanautocorrelation-based meter induction model. Their\nfocus wasontherelati veusefulness ofdurational accent\nandmelodic accent inpredicting meter .Klapur ietal.\n(2005) incorporate thesignalprocessing approaches of\nGoto (2001) andScheierer inamodel thatanalyzes the\nperiod andphase ofthree levelsofthemetrical hierarch y.\n3Model Details\n3.1 Preprocessing\nForMIDI ﬁles, theonsets canbetransformed intospikes\nwith amplitude proportional totheir midi note onset vol-\nume. Alternately MIDI ﬁles cansimply berendered as\naudio andwritten towaveﬁles. Stereo audio ﬁles arecon-\nverted tomono bytaking themean ofthetwochannels.\nThen ﬁles aredownsampled tosome rate near 1000Hz.\nTheactual rateiskeptvariable because itdepends onthe\noriginal sampling rate. ForCD-audio (44.1Khz), weused\nasampling rateof1050Hz allowing ustodownsample by\nafactor of42from theoriginal ﬁle. Best results were\nachie vedbycomputing asum-of-squares envelope over\nwindo wsofsize 42with 5pointsofoverlap. However\nformost audio sources asimple decimation andrectiﬁ-\ncation works aswell. The mode lwasnotverysensiti ve\ntochange sinsampling ratenortominor adjus tments in\ntheenvelope computation such assubstituting RMS (root\nmean square) forthesum ofsquares computation.\n3.2 Autocorr elation Phase Matrix\nAutocorrelation isaspecial case ofcross-correlation\nwhere x1==x2.There isastrong andsome what sur-\nprising linkbetween autocorrelation andtheFourier trans-form. Namely theautocorrelation Aofasignal X(having\nlength N)is:\nA(X)=ifft(jfft(X)j) (1)\nwhere fftisthe(fast)Fourier transform, ifftisthein-\nverse (fast)Fourier transform andjjisthecomple xmod-\nulus. One advantage ofautoc orrelation forourpurposes\nisthatitisdeﬁned overperiods rather than frequencies\n(note theapplication oftheIFFT inEquation 1),yielding\nbetter representation oflow-frequenc yinformatio nthan is\npossible with theFFT.\nAutocorrelation values forarandom signal should be\nroughly equal across lags. Spikesinanautocorrelation in-\ndicate temporal order inasignal, making itpossible touse\nautocorrelation toﬁndtheperiodsatwhich high correla-\ntionexists inasignal. Asamusic example, consider the\nautocorrelation foraChaChaCha from theISMIR 2004\nTempo Induction contest isshown(Figure 1).Thepeaks\noftheautocorrelation align withthetempo andinteger\nmultiples ofthetempo.\n0 500 1000 1500 2000 2500 3000 3500 4000300350400\nTarget tempo = 483.9 ms (124.0 BPM)\nlag (msec)autocorrelationAlbums−Cafe_Paradiso−08.wav\nFigure 1:Autocorrelation ofaChaChaCha from theISMIR\n2004 Tempo Induction contest (Albums-Cafe Paradiso-08.w av).\nThedotted vertical lines mark theactual tempo ofthesong (484\nmsec, 124bpm) andharmonics ofthetempo.\nUnfortunately autocorrelation hasbeen showninprac-\nticetonotworkwell formanykinds ofmusic.Forexam-\nplewhen asignal lacks strong onset energy,asitmight\nforvoice orsmoothly changing musical instru ments like\nstrings, theautocorrelation tends tobeﬂat. Seeforex-\nample asong from Manos Xatzidakis fromtheISMIR\n2004 Tempo Induction inFigure 2.Here thepeaks are\nlesssharp andarenotwell-al igned with thetargettempo.\nNote thatthey-axis scale ofthisgraph isidentical tothat\ninFigure 1.\n0 500 1000 1500 2000 2500 3000 3500 4000460480500\nTarget tempo = 563.0 ms (106.6 BPM)\nlag (msec)autocorrelation15−AudioTrack 15.wav\nFigure 2:Autocorrelation ofasong byManos Xatzidakis\nfrom theISMIR 2004 Tempo Induction contest (15-AudioT rack\n15.w av).Thedotted vertical lines mark theactual tempo ofthe\nsong (563 msec, 106.6 bpm) andharmonics ofthetempo.\nOne waytoaddress thisistoapply theautocorrelation\ntoanumber ofband-pass ﬁltered versions ofthesignal, as\ndiscussed inSecti on3.1. Inplace ofmulti-band process-\ningwecompute thedistribution ofautocorrel ation energy\ninphase space.This hasasharpening effect, allowing au-\ntocorrelation tobeapplied toawider range ofsignals than\nautocorrelation alone without extensi vepreprocessing.\n505The autocorrelation phase information forlaglisa\nvectorAl:\nAl=0\n@bN\u0000l\nlcX\ni=0xli+\u001exl(i+1)+\u001e1\nAl\u00001\n\u001e=0(2)\nWecompute anautocorrelation phase vectorAlfor\neach lagofinterest. Inourcase theminim umlagofin-\nterest was200ms andthemaximum lagofinterest was\n3999ms. Lags were sampled at1ms interv alsyielding\nL=3800 lags. Equation 2effectively“wraps” thesignal\nmodulo thelaglquestion, yielding vectors ofdiffering\nlengths (jAlj==l).Tosimplify later computations we\nnormalized thelength ofallvectors byresampling. This\nwasachievedbyﬁxing thenumber ofphasepoints forall\nlagsatK(K=50forallsimulations; largervalues were\ntried andyielded similar results butsigniﬁcantly smaller\nvalues resulted inaloss oftemporal resolution) andre-\nsampling thevariable length vectors tothisﬁxedlength.\nThis process yielded anautocorrelation phase matrix P\nwhere jPj=[L;K].\nToprovide asimple example, weusetheﬁrstpattern\nfrom thesetfound inPovelandEssens (1985). SeeSec-\ntion 4.4foradescription ofhowthese patterns arecon-\nstructed. Forthisexample wesetthebase inter-onset in-\ntervaltobe300ms. InFigure 3theautocor relation phase\nmatrix isshown. Ontheright, thesum ofthematrix is\nshown.Itisthestandard autocorrelation.\nFigure 3:Theautocorrelation phase matrix forPovel&Essen s\nPattern 1computed forlags 250ms through 500ms. Thephase\npoints areshowninterms ofrelati vephase (0;2\u0019).Black in-\ndicates lowvalue andwhite indicates high value. Since only\nrelati vevalues areimportant, theexact colormap isnotshown.\nOntheright, theautocorrelation isdisplayed; itwasrecovered\nbytaking therow-wise sum ofthematrix.\n3.3 Shannon Entr opy\nAsalready discussed, ispossible toimpro vesigniﬁcantly\nontheperformance ofautocorrelation bytaking advan-\ntage ofthedistrib ution ofenergyintheautocorrelation\nphase matrix. Theidea isthatmetrically-salient lags will\ntend tobehavemore “spik e-like”distrib ution than non-\nmetrical lags. Thus eveniftheautocorr elation isevenlydistrib uted bylag,thedistrib ution ofautocorr elation en-\nergyinphase space should notbesoevenly distrib uted.\nThere areatleast twopossible measures of“spikiness” in\nasignal, variance andentrop y.Wefocus here onentrop y,\nalthough experiments using variance yielded verysimilar\nresults.\nEntrop yistheamountof“diso rder” inasystem. Shan-\nnonentrop yH:\nH(X)=\u0000NX\ni=1X(i)log2[X(i)] (3)\nwhere Xisaprobability density .\nWecompute theentrop yforlaglintheautocorrelat ion\nphase matrix byasfollows:\nAsum=NX\ni=0Al(i) (4)\nHl=\u0000NX\ni=0Al(i)=Asumlog2[Al(i)=Asum](5)\nThis entro pyvalue, when multi plied intotheautocor -\nrelation, signi ﬁcantly impro vestempo induc tion. Forex-\nample, inFigure 4weshowtheautocorrelation along with\ntheautoc orrelation multiplied bytheentrop yforthesame\nManos Xatzidakis showininFigure 2.Onthebottom ob-\nservehowthedetrended (1-entrop y)information aligns\nwell with thetargetlagand itsmultiples. Detrending\nwasdone toremo vealinear trend thatfavorsshort lags.\n(Simulations revealed that performance isonly slightly\ndegraded whendetrending isomitte.) Most robustper-\nformance wasachie vedwhen autocorrelation andentrop y\nwere multiplied together .Thiswasdone byscaling both\ntheautocorrelation andtheentrop ytorange betw een0and\n1andthen multiplying them together .\n0 500 1000 1500 2000 2500 3000 3500 400000.51\nTarget tempo = 563.0 ms (106.6 BPM)\nlag (msec)1 − entropy15−AudioTrack 15.wav\nFigure 4:Entrop y-of-phase calculation forthesame Manos\nXatzidakis song showninFigure 2.The plot displays (1-en-\ntropy),scaledto[0;1]anddetrended .Observ ehowtheentrop y\nspikesalign well with thecorrect tempo lagof563ms andwith\nitsintegermultiples (shownasvertical dotted lines). Entrop y\ncompares favorably with therawautocorrelation ofthesame\nsong asshowninFigure 2.\n3.4 Metrical hierar chyselec tion\nWenowmoveawayfrom theautocorrelation phase ma-\ntrixforthemoment andaddress task ofselecting awin-\nning metrical hierarch y.Arough estimate ofmeter canbe\nhadbysimply summing hierarchical combina tions ofau-\ntocorrelation lags. Inplace ofstandard autocorrelation we\nusetheproduct ofautocorrelation and(1-entrop y)AEas\ndescribed above.Thelikelihood ofaduple meter Mduple\n506existing atlaglcanbeestimated using thefollowing sum:\nMduple\nl=AE(l)+AE(2l)+AE(4l)+AE(8l)(6)\nThelikelihood ofatriple meter isestimated using the\nfollowing sum:\nMtriple\nl=AE(l)+AE(3l)+AE(6l)+AE(12l)(7)\nOther candidate meters canbeconstructed. using sim-\nilarcombinations oflags. Awinning meter canbechosen\nbysampling allreasonable lags (e.g.200ms<=l<=\n2000ms)andcomparing theresul tingM\u0003\nlvalues. Pro-\nvided thatthesame number ofpointsareused forallcan-\ndidate meters, theseM\u0003\nlvalues canbecompared directly ,\nallowing forasingle winning meter tobeselected among\nallpossible lags andallpossible meters. Furthermore,\nthissearch isefﬁcient giventhateach lag/candidate meter\ncombination requires only afewadditions. Forthemeter\nprediction simulations inSectio n4thiswastheprocess\nused toselect themeter .\n3.5 Prediction oftempo\nOnce ametrical hierarch yischosen, there areseveralsim-\nplemethods forselecting awinning tempo from among\nthewinning lags. Oneoption istopick thelagclosest to\nacomfortable tapping rate,say600ms. Asecond better\noption istomultiply theautocorrelation lags byawin-\ndowsuch thatmore accent isplaced onlags near apre-\nferred tappi ngrate. Thewindo wcanbeapplied either be-\nfore orafter choosing thehiera rchy.Ifitisapplied be-\nfore selecting themetrical hierarch y,then theselection\nprocess isbiased towards lags inthetapping range. We\ntried both approaches; applying thewindo wbefore selec-\ntionyields better results, butonly marginally better (onthe\norder of1%better perform ance onthetempo prediction\ntasks described below).Toavoidadding more parameters\ntoourmodel wedidnotconstruct ourownwindo wing\nfunction. Instead weused thefunction (with nochanges\ntoparameters) described inParncutt (1994): aGaussian\nwindo wcentered at600ms andsymmetrical inlog-scale\nfrequenc y.\n4Simulations\nWehaverunthemodel onseveral datasets. Totest\ntempo induction weused theBallroom andSong Excerpts\ndatabases from theISMIR 2004 Tempo Induction con-\ntest. Fortesting theabilityofthemodel toperform me-\nterprediction weusedthetheEssen Europea nFolksong\ndatabase andtheFinni shFolkSong database. Wealsoin-\nclude preliminary simulations onalignment usingthe35\nartiﬁcial patterns from PovelandEssens (1985) aswell as\n4.1 ISMIR 2004 Tempo Induction\nWeused twodatasets from theISMIR 2004 Tempo In-\nduction contest (Gouyon etal.,2005). The ﬁrst dataset\nwastheBallroom dataset consisting of698wavﬁleseach\napproximately 30seconds induration encompassing eight\nmusical styles. SeeTable 1forabreakdo wnofsongstylesTable 1:Performance ofmodel bygenre ontheBallroom\ndataset. Seetextfordetails.\nStyle Count Acc. A Acc. BAcc. C\nChaChaCha 111 106 107 109\nJive 60 6 60 60\nQuickstep 82 0 77 80\nRumba 98 84 85 92\nSamba 86 78 79 83\nTango 86 81 82 83\nVienn.W altz 65 0 57 64\nWaltz 110 86 86 93\nGlobal 698 441 633 664\nTable2:Summary ofmodels ontheBallroom dataset. See\ntextfordetails.\nModel Acc. A Acc. BAcc. C\nAcorr Only 49% 77% 77%\nAcorr+Meter 58% 80% 85%\nAcorr+Entrop y41% 85% 85%\nFullModel 63% 91% 95%\nKlapuri 63% 91% 93%\nalong with theperformance ofourmodel onthedataset.\nInthetable, “Acc. A”isAccurac yAfrom thecontest:\nthenumber ofcorrect predictions within 4%ofthetar-\ngettempo .“Acc. B”isAccurac yBfrom thecontest. It\nalsotakesintoaccount misses duetopredicting thewrong\nlevelofthemetrical hierarch y.Thus answers aretreated\nascorrect iftheyarewithin 4%ofthetargettempomul-\ntiplied by2,3,1/2 or1/3. “Acc C.”isourownmeasure\nwhich alsotreats answers ascorrect iftheyarewithin 4%\nofthetargettempo multiplied by2/3or3/2. This gives\nusameasure ofmodel failure duetopredicting thewrong\nmeter .\nWecomputed several baseline models fortheball-\nroom dataset. These results areshownalong with\nourbest results and those ofthecontest winner ,Kla-\npuri etal.(2005), inTable 2. The “Acorr Only”\nmodel uses simple autocorrelation. The “Acorr+Meter”\nmodel incorporates thestrate gydescribed inthispaper\nforusing multiple hierarchically-related lags inpredic-\ntion. The“Acorr+Entrop y”uses autocorrelation plus en-\ntropyascomputed onthephase autocorrelation matrix\n(butnometer) .Thefull model could alsobecalled\n“Acorr+Entrop y+Meter” andistheonedescribed inthis\npaper .“Klapuri” showstheresultsforthecontest winner .\nTwothings areimportant tonote. First, itisclear that\nboth ofourtwomain ideas, meter reinforceme nt(“Me-\nter”) andentropycalculation (“Entrop y”)aidincomput-\ningtempo. Second, themodel seems toworkwell, return-\ningresults thatcompete with thecontest winner .\nWealso used the“Song Excerpts” datase tfrom the\nISMIR 2005 datas et.This dataset consisted of465songs\nofrough ly20sec duration spanning nine genres. Dueto\nspace constraints ,wedonotreport model performance on\nindividual genres.Intable Table 3theresults aresumma-\nrized inaformat identical toTable 2.\n507Table 3:Summary ofmodels ontheSong Excerpts\ndataset. Seetextfordetails .\nModel Acc. A Acc. BAcc. C\nAcorr Only 49% 64% 64%\nAcorr+Meter 50% 80% 85%\nAcorr+Entrop y53% 74% 74%\nFullModel 60% 79% 88%\nKlapuri 58% 91% 94%\nHere itcanbeseen thatourmodel performed slightly\nbetter than thewinning model onAccurac yAbutper-\nformed considerably worse onAccurac yB.Inourview,\nAccurac yBisamore important measure because itre-\nﬂects thatthemodel hascorrectly predict edthemetrical\nhierarch ybuthassimply failed toreport theappropriate\nlevelinthehierarch y.\n4.2 Essen Database\nWecomputed ourmodel onasubset oftheEssen collec-\ntion (Schaf frath, 1995) ofEuropean folk melodies. We\nselected allmelodies ineither duple (i.e.having2neighth\nnotes permeasure; e.g. 2/4and4/4) ortriple/compound\nmeter (i.ehaving 3neighth notes permeasure; e.g.3/4and\n6/8). This resulted inatotal of5507 melodies ofwhich\n57% (3121) were induple meterand43% (2386) were in\ntriple/compound mete r.Thetask wastopredict themeter\nofthepiece asbeing either duple ortriple/compound. This\nisexactly thesame dataset andtask studied inToiviainen\nandEerola (2004).\nOurresults were promising. Weclassiﬁed 90% ofthe\nexamples correctly (4935 of5507 correct). Our model\nperformed better onduplesthan triple/compounds, classi-\nfying 94% oftheduple examples correctly (2912 of3121\ncorrect) and85% ofthetriple/compound example scor-\nrectly (2023 of2386 correct).\nThese success rates aresimilar tothose inToiviainen\nandEerola (2004). Howeveritisdifﬁcult tocompar eour\napproaches because their dataanalysis technique (step-\nwise discriminant function analysis) does notcontrolfor\nin-sample versus out-of-sample errors. Functions are\ncombined using thetargetvalue (the meter) asadepen-\ndent variable. This issuitable forweighing therelati ve\npredicti vepowerofeach function butnotsuitable forpre-\ndicting howwell theensemble offunctions would perform\nonunseen data unless training andtesting setsorcross-\nvalidation isused. Ourapproach used nosupervised learn-\ning.\n4.3 Finnish FolkSongs Database\nWeperformed thesame meter prediction taskonasub-\nsetoftheFinnish Folksong database (Eerola andToivi-\nainen, 2004). This dataset wasalso treated byToivi-\nainen andEerola (2004)andtheselection criteriawere\nthesame. Forthisdataset weused 7139melodies of\nwhich 80% (5720) were induple meter and20% (1419)\nwere triple/compound meter .(FortheToiviainen et.al.\nstudy ,6861 melodies were used due toslightly morestringent selection criteria. Howevertheratio ofduples\ntotriple/compounds isalmost identical.) Note that the\ndatasets areseriously imbalanced: aclassiﬁer whichal-\nwaysguesses duple willhaveasuccess rateof80%. How-\nevergiventherelati vepopularity ofduple overtriple, this\nimbalance seems unavoidable.\nOurresults were promising. Weclassiﬁed 93% exam-\nples correctly (6635 of71239 correct) .Again,ourmodel\nperformed better onduples than triple/compounds, classi-\nfying 95% oftheduple examples correctly (5461 of5720\ncorrect) and83% ofthetriple/compound examples cor-\nrectly (1174 of1419 correct).\n4.4 Povel&Essens Patter ns\nTotest alignment (beat induction) weused asetof\nrhythms from Experiment 1ofPovelandEssens (1985).\nThese rhythms aregener ated bypermut ingtheinterv alse-\nquence 11111223andterminating itbytheinterv al4.\nThese length-16 patterns allcontain nine notes andseven\nrests.\nTheir model works byapplying asetofrules that\nforced theaccentuation of(a)singleton isolated events,\n(b)thesecond oftwoisolated events and(c)theﬁrstand\nlastofalonger group ofisolated events. Ofparticular im-\nportance isthattheyvalidated their model using asetof\npsychological experiments withhuman subjects.\nOur model predicted thecorrect downbeat (correct\nwith respect tothePovel&Essens model) 97% ofthetime\n(34of35patterns). The pattern where themodel failed\nwaspattern 27.Ourinterestinthisdataset lieslessinthe\nerror rateandmore inthefactthatwecanmakegood pre-\ndictions forthese patterns without resorting toperceptual\naccentuation rules.\n5Discussion\nThe model seems toperform basic meter categorization\nrelati velywell. Itperformed atcompetiti velevelson\nboth theEssenand theFinnish simulations. Further -\nmore itachie vedgood perform ance without riskofunder -\ngeneralizing duetooverﬁtting from supervised learning.\nOne area ofcurrent resea rchistoseehowwell themodel\ndoes ataligning (identifying thelocation ofdownbeats) in\ntheEssen andFinnish database s.\nAsevidenced bythePovel&Essens results, themodel\nhaspoten tialforperfor ming alignment ofaninduce dmet-\nrical hierarch ywith amusical sequenc e.Though we\nhavemanyother examples ofthisability performance, in-\ncluding some entertaining automatic drumming toMozart\ncompositions, wehaveyettoundertak eamethodical\nstudy ofthethelimitations ofourmodel onalignment.\nThis, andrelated tasks likeonline beat induction, arear-\neasofongoing research.\n6Conclusions\nThis pape rintroduces anovelwaytodetecting metrical\nstructure inamusic andtousemeterasanaidindetect-\ningtempo. Twomain ideaswere explored inthispaper .\nFirst wediscussed animpro vement tousing autocorrela-\n508tions formusical feature extraction viathecomputation of\nanautoco rrelation phase matrix. Wealso discussed com-\nputing theShannon entrop yforeach laginthismatrix as\nameans forsharpening thestandard autocorrela tion. Sec-\nondwediscus sedwaystousetheautocorrelation phase\nmatrix tocompute analignment ofametrical hierarch y\nwith music. Weapplied themodel tothetasks ofmeter\nprediction andtempo induction onlargedatasets. Wealso\nprovided preliminary results foraligning themetrical hier-\narchywith thepiece (downbeat induction). Though much\nofthisworkispreliminary ,webelie vetheresults inthis\npaper suggest thattheapproach warrants further investi-\ngation.\nACKNO WLEDGEMENTS\nWewould liketothank Fabien Gouyon, Petri Toiviainen\nand Tuomas Eerola formanyhelpful email correspon-\ndences.\nRefer ences\nJ.C.Brown.Deter mination ofmeter ofmusical scores by\nautocorrelation. Journal oftheAcoustical Society of\nAmerica ,94:953–1957, 1993.\nA.T.Cemgil and H.J.Kappen. Monte Carlo meth-\nods fortempo tracking and rhythm quantization.\nJournal ofArtiﬁcia lIntellig ence Resear ch,18:45–81,\n2003. URLhttp://carol.science.uva. nl/\n˜cemgil/papers/cemgil03a .pdf .\nA.T.Cemgil, H.J.Kappen, P.Desain, and H.Hon-\ning.Ontempo tracking: Tempogram representation and\nKalman ﬁltering. Journal ofNewMusic Resear ch,28:4:\n259–273, 2001. URLhttp://carol.science.\nuva.nl/˜cemgil/papers/ce mgil-tt.pdf .\nGrosv enor Cooper andLeonard B.Meyer.TheRhythmic\nStructur eofMusic .TheUniv.ofChicago Press, 1960.\nSimon E.Dixo n.Automatic extraction oftempo andbeat\nfrom expressi veperformances. Journal ofNewMusic\nResear ch,30(1):39–58, 2001. URLhttp://www.\noefai.at/˜simon/pub/2001 /jnmr.pdf .\nDouglas Eck. Finding downbeats with arelaxation oscil-\nlator.Psychol.Resear ch,66(1):18–25, 2002.\nT.Eerola andP.Toiviainen. Digital Archi veofFinnish\nFolktunes, 2004. [computer database]. University of\nJyvaskyla.http://www .jyu.ﬁ /musica/sks.\nMasataka Goto. Anaudio-based real-time beat track-\ningsystem formusic withorwithout drum-sounds.\nJournal ofNewMusic Resear ch,30(2):159–171,\n2001. URLhttp://staff.aist.go.jp/m.\ngoto/PAPER/JNMR2001goto. pdf.\nF.Gouyon, A.Klapuri, S.Dixon, M.Alonso, G.Tzane-\ntakis, C.Uhle, andP.Cano. Anexperi mental compar -\nison ofaudio tempo induction algorithms, 2005. Sub-\nmitted.\nStephen Handel. Listening: Anintroduction tothepercep-\ntionofauditory events .MIT Press, Cambridge, Mass.,\n1993.A.Klapuri, A.Eronen, andJ.Astola. Analysis oftheme-\nterofacoustic music alsignals. IEEE Trans.Speec hand\nAudio Processing ,2005. Toappear .\nEdwardW.LargeandJ.F.Kolen. Reson ance andtheper-\nception ofmusical meter .Connection Science ,6:177–\n208, 1994.\nR.Parncutt. Aperceptual model ofpulse salience and\nmetrical accent inmusical rhythms. Music Perception ,\n11:409–464, 1994.\nD.JPovelandPeter Essens. Perception oftemporal pat-\nterns. Music Perception ,2:411–440, 1985.\nH.Schaf frath. The Essen Folksong Collection inKern\nFormat, 1995. [computer database]. Center forCom-\nputer Assisted Research intheHumanitites.\nE.Scheirer .Tempo andbeat analysis ofacoustic musical\nsignals. Journal oftheAcoustical Society ofAmerica ,\n103(1):588–601, 1998. URLhttp://web.media.\nmit.edu/˜eds/beat.pdf .\nPetri Toiviainen andTuomas Eerola. The role ofaccent\nperiodicities inmeter induction: aclassiﬁcatin study .\nInS.D. Lipscomb, R.Ashle y,R.O. Gjerdingen, and\nP.Webster ,editors, TheProceedings oftheEighth In-\nternational Confer ence onMusic Perception andCog-\nnition (ICMPC8) ,Adelaide, Australia, 2004. Causal\nProductions.\nAnja Volk. Exploring theinteraction ofpulse layers re-\ngarding their inﬂuence onmetrical accents. InS.D. Lip-\nscomb, R.Ashle y,R.O. Gjerdingen, andP.Webster ,ed-\nitors, TheProceedings oftheEighth International Con-\nference onMusic Percepti onandCognition (ICMPC8) ,\nAdelaide, Australia, 2004. CausalProductions.\nP.G.Vos,A.vanDijk, andLSchomak er.Melodic cues\nformetre. Perception ,23:965–976, 1994.\n509"
    },
    {
        "title": "Inferring Efficient Hierarchical Taxonomies for MIR Tasks: Application to Musical Instruments.",
        "author": [
            "Slim Essid",
            "Gaël Richard",
            "Bertrand David 0002"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416268",
        "url": "https://doi.org/10.5281/zenodo.1416268",
        "ee": "https://zenodo.org/records/1416268/files/EssidRD05.pdf",
        "abstract": "A number of approaches for automatic audio classification are based on hierarchical taxonomies since it is acknowledged that improved performance can be thereby obtained. In this paper, we propose a new strategy to automatically acquire hierarchical taxonomies, using machine learning methods, which are expected to maximize the performance of subsequent classification. It is shown that the optimal hierarchical taxonomy of musical instruments (in the sense of inter-class distances) does not follow the traditional and more intuitive instrument classification into instrument families. Keywords: Hierarchical taxonomy, musical instrument, clustering, probabilistic distance. 1",
        "zenodo_id": 1416268,
        "dblp_key": "conf/ismir/EssidRD05",
        "keywords": [
            "automatic audio classification",
            "hierarchical taxonomies",
            "machine learning methods",
            "musical instruments",
            "probabilistic distance",
            "instrument families",
            "optimal hierarchical taxonomy",
            "clustering",
            "improved performance",
            "classification"
        ],
        "content": "INFERRING EFFICIENT HIERARCHICAL TAXONOMIES FOR MIR\nTASKS: APPLICA TION TOMUSICAL INSTR UMENTS\nSlim ESSID\nGET-T´el´ecom Paris, CNRS LTCI,\n37,rueDareau, 75014 Paris\nslim.essid@enst.frGa¨elRICHARD\nGET-T´el´ecom Paris, CNRS LTCI,\n37,rueDareau, 75014 Paris\ngael.richard@enst.frBertrand DAVID\nGET-T´el´ecom Paris, CNRS LTCI,\n37,rueDareau, 75014 Paris\nbertrand.david@enst.fr\nABSTRA CT\nAnumber ofapproaches forautomatic audio classi\u0002ca-\ntion arebased onhierarchical taxonomies since itisac-\nknowledged that impro vedperformance canbethereby\nobtained. Inthispaper ,wepropose anewstrate gyto\nautomatically acquire hierarchical taxonomies, using ma-\nchine learning methods, which areexpected tomaximize\ntheperformance ofsubsequent classi\u0002cation. Itisshown\nthattheoptimal hierarchical taxonomy ofmusical instru-\nments (inthesense ofinter-class distances) does notfol-\nlowthetraditional andmore intuiti veinstrument classi\u0002-\ncation intoinstrument families.\nKeywords: Hierarchical taxonomy ,musical instrument,\nclustering, probabilistic distance.\n1INTR ODUCTION\nRecently ,hierarchical taxonomies havebeen pro\u0002tably\nused foraudio classi\u0002cation tasks, especially musical in-\nstrument classi\u0002cation [1,2,3,4]andgenre classi\u0002cation\n[5,6,7].Inthe\u0002rst place, byrecurring tohierarchical\nclassi\u0002cation, itisdesired toachie vebetter classi\u0002cation\nperformance than theso-called \u0003at systems, wherein all\nclasses areputatthesame levelwithout anyarrangement.\nFurthermore, classi\u0002cation scalability isthereby obtained\ninthesense thatcoarse classi\u0002cation yielding top-le vel\n(more vague) labelling ofsome sound properties ismade\npossible.\nInmost studies, straightforw ardtaxonomies were con-\nsidered which were borro wed from other areas ofactiv-\nity.Typically ,taxonomies used ininstrument classi\u0002ca-\ntion[1,2,3]arehighly inspired byinstrument family divi-\nsions derivedfrom instrument physics and/or musicology\nbased categories, whereas taxonomies exploited inmusi-\ncalgenre classi\u0002cation essentially originate from themu-\nsicindustry .\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonSuch taxonomies present theadvantage ofbeing ha-\nbitual andintuiti ve,hence enabling acertain ease ofap-\nplication foranypotential end-user .Ontheother hand,\ntheysufferfrom twomajor drawbacks. First, onthebasis\nofintuition, ahigh number ofalternati vetaxonomies can\nbepotentially used leading toheterogeneous systems and\ncontradictory classi\u0002cations. Second, such taxonomies\narenotnecessarily meant tomaximize theaccurac yofthe\nclassi\u0002cation tasks.\nAttempts toaddress both issues were made inprevious\nwork. Pachet &Cazaly proposed guiding principles to\nbeused inbuilding amusic genre taxonomy [5].Theap-\nplication ofMulti-Dimensional Scaling (MDS) analysis to\nobserv edissimilarities among musical instruments [8,3]\ncanalso beconsidered asanimportant step towards \u0002nd-\ningnatural organizations among sound classes. Finally ,\nveryrecently ,ataxonomy ofmusical genres wasinduced\nbygrouping genres which were themost frequently con-\nfused byagivenclassi\u0002er [7].\nWepropose analgorithm toacquire automatic tax-\nonomies using unsupervised machine learning techniques\ninorder toobtain solutions which areexpected toyield\nthebest classi\u0002cation performance. Ourapproach makes\nuseofhierarchical clustering toproduce atreedata struc-\nturewherein nodes represent optimal groupings ofclasses\nwith respect toarobustprobabilistic distance criterion.\nWestart bydescribing ouralgorithm andtherelated\nmachine learning concepts. Subsequently ,wepresent ap-\nplications ofourmethod tothecase ofmusical instru-\nments. Finally ,wesuggest some conclusions.\n2ALGORITHM DESCRIPTION\n2.1 Overview\nWeaimtoobtain ahierarchical taxonomy ofsome musi-\ncaldescriptions which areassociated with targetclasses\n(forexample instruments, orchestrations orgenres, etc.).\nThese arematerialized bytheleafnodes ofthetaxonomy\ntreerepresentation. Tothisend, weorganize targetclasses\nusing ahierarchical clustering algorithm. This isknownto\nbeanoptimal andnatural wayofarranging thedata since\nthemost similar classes with respect tothechosen close-\nness criterion arethen putinthesame clusters.\nThus, thechoice ofthecloseness criterion iscritical.\nWeneed robustdistances enabling ustoreduce theeffect\nofnoisy features ontheclustering performance. Also, the\n324distances arerequired tobematched with thebehavior of\ntheclassi\u0002ers tobeused. Aconvenient androbustmeans\nformeasuring thecloseness orseparability ofdata classes\nistouseprobabilistic distance measures between them, i.e\ndistances between their probability distrib utions [9].This\nisaninteresting alternati vetoclassic Euclidean distance\nbetween feature vectors knowntobeinef\u0002cient forsound\nsource classi\u0002cation.\nAnother fundamental choice istheclass descriptors.\nAlargenumber ofuseful attrib utes canbeexamined and\nreduced using afeature selection algorithm toretain only\ntheattrib utesthatarerelevantforproper overall class dis-\ncrimination.\n2.2 Clustering thetargetclasses\nWewish togroup together anumber ofMclass probabil-\nitydensities piintoanumber ofMcclusters Ciwithin L\nlevelsofahierarchical taxonomy .Thus weneed appro-\npriate probabilistic distances. Manysuch distances can\nbeconsidered among which wechose theBhattacharryya\nanddivergence duetotheresulting simpli\u0002cation inthe\nfollowing computations. Thedivergence distance JDbe-\ntween twoprobability densities p1andp2isde\u0002ned as\nJD(p1;p2)=Z\nx[p1(x)\u0000p2(x)]logp1(x)\np2(x)dx:(1)\nTheBhattacharryya distance isde\u0002ned as\nJB(p1;p2)=\u0000log\u0012Z\nx[p1(x)p2(x)]1\n2dx\u0013\n:(2)\nWhile these distances admit analytical expressions\nwhene vertheclass probability densities areGaussian,\ncomputing such distances can beotherwise adif\u0002cult\nproblem since itrequires performing heavynumeric in-\ntegrations [10]. Infact,intheGaussian case, thedistances\ncanbeexpressed asfunctions ofthemeans andcovariance\nmatrices according to\nJD(p1;p2)=1\n2(\u00161\u0000\u00162)T(\u0006\u00001\n1+\u0006\u00001\n2)(\u00161\u0000\u00162)\n+1\n2tr(\u0006\u00001\n1\u00062+\u0006\u00001\n2\u00061\u00002ID);\nJB(p1;p2)=1\n8(\u00161\u0000\u00162)T[1\n2(\u00061+\u00062)]\u00001(\u00161\u0000\u00162)\n+1\n2logj1\n2(\u00061+\u00062)j\nj\u00061j1\n2j\u00062j1\n2;\nwhere (\u00161;\u00061)and(\u00162;\u00062)arethemean vectors and\nthecovariance matrices ofthemulti variate Gaussian den-\nsities describing respecti velyclass 1andclass 2inRD.\nNevertheless, itwould behighly sub-optimal, inourcase,\ntoassume thattheoriginal class observ ations followGaus-\nsiandistrib utions since wedealwith datawith anon-linear\nstructure. Fortunately ,ifthisdataismapped from theorig-\ninalspace toaReproducing Kernel Hilbert Space (RKHS)\n[11],itisreasonable toassume ittobeGaussian intheRKHS [10].Thus, arobustestimation oftheneeded prob-\nabilistic distances canbederivedusing analytical expres-\nsions provided thataproper estimation ofthemeans and\ncovariance matrices intheRKHS canbeobtained. The\nstrength ofsuch anapproach resides inthat there isno\nneed forknowing explicitly either thestructure oftheorig-\ninalprobability densities orthenonlinear mapping tobe\nused. Interested readers arereferred to[10] forfurther\ndetails.\nWethen useagglomerati vehierarchical clustering [9,\n12]toproduce ahierarch yofnested clusterings based\nonprobabilistic distances inRKHS. Thealgorithm starts\nwith asmanyclusters asoriginal dataobjects (M1\nc=Mat\niteration 1),measuring theproximities J(pi;pj)between\nallpairs ofclusters andgrouping together theclosest pairs\ninto newclusters toproduce Ml\ncnewones atiteration l,\nuntil allvectors lieinsingle cluster (atiteration M).\nAconvenient waytounderstand theresult ofsuch a\nprocedure istorepresent itasagraph (called dendr ogram)\nwhich depicts therelations andproximities between the\nobtained nested clusters (see\u0002gure 1foranexample).\nTherelevance ofthecluster treecanbeevaluated by\ncomputing thecophenetic correlation coef\u0002cient [9].The\ncloser thecophenetic coef\u0002cient to1,themore relevantly\nthecluster treere\u0003ects thestructure ofthedata.\nClustering isthen obtained bycutting thedendrogram\natacertain levelorcertain value ofthevertical axis. By\napplying different cuts tothedendrogram wecanobtain\ndifferent clusterings (having adifferent number ofclus-\nters). The levelsofthehierarchical taxonomy aretobe\ninduced from these alternati veclusterings insuch away\nthatthehigh levelsarededuced from coarse clustering\n(lownumber ofclusters) while thelowlevelsarededuced\nfrom \u0002ner clustering (higher number ofclusters).\n3TAXONOMIES OFMUSICAL\nINSTR UMENTS\nVarious taxonomies havebeen proposed formusical in-\nstrument classi\u0002cation onisolated notes roughly follow-\ningtheinstrument families organization [1,2,3].While\nsome declinations arecommon tothese studies, asfor\nexample theprimary division ofinstruments into sus-\ntained and pizzicati, other groupings arenotunani-\nmously shared, especially forthewind instruments.\nItisworth tonote that Peeters undertook aMulti-\nDimensional Scaling (MDS) analysis based onthesignal\nfeatures inorder toverify theconsistenc yoftheclass tree\nhehadassumed [3].This provided objecti vejusti\u0002cation\nofsome ofthechoices made butcould notbeused toinfer\nataxonomy .\nWehere present anapplication ofouralgorithm to\nproduce ahierarchical taxonomy ofmusical instruments.\nThis taxonomy istobeinduced from realworld musical\nphrases andisexpected toyield theorganization thatbest\nmatches theclassi\u0002cation tobeperformed subsequently .\n3.1 Featur eextraction andselection\nAwide selection ofmore than 300signal processing fea-\ntures isconsidered including some oftheMPEG-7 de-\n325scriptors. Since these features havebeen extensi velyde-\nscribed invarious previous workinthe\u0002eld ofMusic In-\nformation Retrie val(see [13] forexample), inthefollow-\ning, wemerely listtheattrib utes which weexamined in\nourstudy .\n\u000fTempor alfeatur esconsist ofautocorrelation coef-\n\u0002cients, features obtained from thestatistical mo-\nments, zero crossing rates, andamplitude modulation\nfeatures.\n\u000fCepstr alfeatur esaremel-frequenc ycepstral coef\u0002-\ncients aswell astheir \u0002rst andsecond time deriva-\ntives.\n\u000fSpectr alfeatur esinclude features obtained from the\nstatistical moments, MPEG-7 audio spectrum \u0003at-\nness, spectral irregularity ,spectral crest, spectral\nslope, spectral decrease, frequenc ycutof f,temporal\nvariation ofspectrum, andoctaveband signal inten-\nsities andtheir ratios providing acoarse description\noftheenergydistrib ution ofsound partials [14].\n\u000fPerceptual featur esarealso utilized, namely loud-\nness, sharpness andspread.\nInorder tofetch themost relevantfeatures foropti-\nmalclass discrimination, weuseasimple feature selection\nalgorithm, belonging tothefamily of\u0002lter algorithms,\nwhich isbased onFisher' sLinear Discriminant Algorithm\n(LDA)[12]. Thechosen method computes therelevance\nofeach candidate feature using theweights estimated by\ntheLDA.\n3.2 Experimental parameters\nNineteen instruments from allinstrument families arecon-\nsidered. Table 1sums upthestudied instruments giving\ntheir codes. Solo musical phrases played byeach ofthese\ninstruments were excerpted from commercial recordings.\nWehadatleast 4different sources (different album,dif-\nferent artist) andatleast 3minutes available foreach in-\nstrument.\nAllfeatures described abovewere extracted onaframe\nbasis. Unless otherwise speci\u0002ed, thedefaultframe length\nis32ms. Silence frames were detected andremo ved.\nInstrument Code Instrument Code\naltosax As oboe Ob\nbassoon Bo piano Pn\ndouble bass-pizzicato Bs tenor sax Ts\ndouble bass-bo wed Ba soprano sax Ss\nbass clarinet Cb tuba Tb\nBbclarinet Cl trombone Tm\ncello Co trumpet Tr\n\u0003ute Fl viola Va\nFrench horn Fh violin Vl\nclassical guitar Gt\nTable 1:Studied instruments andtheir codes.\nComputing theprobabilistic distances inRKHS (tobe\nused forclustering) requires processing theEigenV alue\nDecomposition ofnk\u0002nkGram matrices [11],withnk\nthenumber oftraining feature vectors ofclass Ck.Suchanoperation iscomputationally expensi ve(O(n3\nk))since\nnkisquite large.Hence, thetraining setswere divided\nintosmaller setsof1500 observ ations andthedesired dis-\ntances were obtained byaveraging thedistances approxi-\nmated using asmanyreduced setsaspossible. Tomeasure\nthese distances, oneneeds tochoose akernel function. We\nused theRadial Basis Function kernel.\n3.3 Results\nAtotal of40features were selected bytheLDAapproach\nfrom theoriginal 304candidates, namely:\n\u000fthe4\u0002rstmel-frequenc ycoef\u0002cients (excluding the\nzero-th coef\u0002cient);\n\u000fthespectral centroid andthespectral asymmetry;\n\u000fthe15-th amplitude MPEG-7 spectral \u0003atness coef\u0002-\ncient;\n\u000fthefrequenc ycutof f;\n\u000fOctaveBand Signal Intensity (OBSI) coef\u0002cients 1,\n2,3and6,aswell asOctaveBand Signal Intensity\nRatios (OBSIR) 1to6;\n\u000fspectral irregularity coef\u0002cient 5;\n\u000fthe4-th statistical moments measured both onthe\nsignal temporal waveform andamplitude envelope\nover960-ms windo ws;\n\u000fthezero crossing ratemeasured over32-ms windo ws\nand960-ms windo ws;\n\u000ftheAmplitude Modulation (AM) strength inthe\nrange 4-8Hz (tremolo) and theproduct ofAM\nstrength and AM frequenc yintheranges 4-8Hz\n(tremolo) and10-40 Hz(graininess);\n\u000frelati vespeci\u0002c loudness coef\u0002cients 1,2,5,16,18\nand21aswell asperceptual loudness andsharpness.\nBased onthese features, probabilistic distances in\nRKHS between each pairofconsidered classes were com-\nputed. Both thedivergence andBhattacharryya distances\nwere obtained andfedtotheagglomerati vehierarchical\nclustering (described insection 2.2). Ahigher cophenetic\ncoef\u0002cient wasobtained with theBhattacharryya distance\ncompared totheoneobtained with thedivergence. Hence,\nmore relevantclustering wasobtained with theformer .Its\nrelated dendrogram isdepicted in\u0002gure 1.This canbe\nalready considered asaprimary taxonomy .However,itis\nworthbeing processed soastogainconsistenc yandread-\nability .\nTheprocessing ofthetreeconsisted inapplying 4dif-\nferent cuts tothedendrogram, each cutinducing alevel\nofhierarch y.Thecuts were performed based onthecon-\nsistency coef\u0002cients [9]foreach dendrogram linksoasto\nignore themost inconsistent links. This resulted inthetree\ndepicted in\u0002gure 2.\nThe obtained taxonomy does notfollowtheorgani-\nzation ofinstruments intotraditional families. Infact,al-\nthough some unions ofthefound solution areintuiti ve(for\n326FhTmBoPnAsClSsFlTrObCbCoVlVaTsBsBaGtTb0.0050.010.0150.020.025\nFigure 1:Dendrogram obtained with theBhattacharryya\ndistance. Vertical axisrepresents cluster distances.\nCl AsTs Bs−Ba−Gt−Tb\nOb Ss\nBo Tr Tm Fh FlCb PnVa Vl CoCb−Co Vl−Va As−Cl Ss−Fl−Ob−Tr Tb Gt\nBs BaBs−Ba\nBo−Fh−Tm Fl−TrPn−Bo−Fh−TmPn−As−Ss−Bo−Cl−Fh−Fl−Ob−Tm−Tr Cb−Co−Vl−Va\nFigure 2:Obtained hierarchical taxonomy ofmusical in-\nstruments.\nexample, cello, violin andviola areputinthesame clus-\nter)manyothers may besurprising.\nAtthetoplevel,bowed double bass andpizzicato dou-\nblebass aregrouped together with guitar andtuba, indi-\ncating thatthesustained/non-sustained property hasnot\nbeen considered byouralgorithm asuseful fortheclassi-\n\u0002cation. Indeed, since thisproperty seems nottobecap-\ntured bytheselected features, itwillnotbeseen bythe\nclassi\u0002ers tobeused, hence itisnotoptimal totakeit\nintoaccount inthetaxonomy .Additionally ,thepresence\noftuba inthesame cluster implies thatfeatures related to\ninstrument register play animportant roleintheclassi\u0002-\ncation.\nMost wind instruments aregrouped together except\nthetuba, thebass clarinet- which isassociated with cello,\nviolin andviola- andtenor sax, which isleftalone. The\nfactthat ourtenor saxexcerpts areexclusi velyjazz ex-\ncerpts while forallother instruments thesounds originate\nfrom both jazz andclassic music might explain thisex-\nception. Finally ,also surprising isthatthepiano liesin\nthesame cluster asmost wind instruments.\nGoing downinthehierarch y,interesting clusters are\nfound. Alto saxisgrouped with Bbclarinet, \u0003ute with\ntrumpet, and bassoon with French horn and trombone.\nThese arrangements donotreally surprise usastheyre-\n\u0003ect theconfusions which wehaveoften noted inourpre-\nvious experiments oninstrument recognition using musi-\ncalphrases. Itappears thattheinstruments thatarefre-\nquently confused areputbythealgorithm inthesame\nclusters.\n4CONCLUSIONS\nInthispaper ,wehavesuggested atechnique forinferring\nautomatic taxonomies ofmusical descriptions expected to\nmaximize theperformance ofsubsequent classi\u0002cation.Our approach exploits robustprobabilistic distances and\nagglomerati vehierarchical clustering algorithms topro-\nduce class organizations inanunsupervised fashion.\nWehavetested thismethod intheconte xtofmusical\ninstrument classi\u0002cation using signal processing features\nautomatically selected from ahigh number ofstate-of-the-\nartfeatures. Theobtained arrangement ofinstruments is\nsubstantially different from usual taxonomies following\ninstrument families organization. This suggests thatthe\nlatter isprobably notanoptimal solution forautomatic\nclassi\u0002cation.\nFuture workwill consider hierarchical classi\u0002cation\nexperiments based ontheinduced taxonomies. Fur-\nthermore, wewill attempt toaddress thefeature selec-\ntion problem inparallel toclustering soastoproduce\ntaxonomy-conte xtdependent features.\nACKNO WLEDGEMENTS\nThis workwaspartly supported bytheMusicDisco ver\nproject oftheACI-Masse dedonn ´ees.\nREFERENCES\n[1]KeithDana Martin. Sound-Sour ceReco gnition :A\nTheory andComputational Model .PhD thesis, Mas-\nsachusets Institue ofTechnology ,jun1999.\n[2]Antti Eronen. Automatic musical instrument recog-\nnition. Master' sthesis, Tampere University ofTech-\nnology ,April 2001.\n[3]Geof froyPeeters. Automatic classi\u0002cation of\nlargemusical instrument databases using hierarchi-\ncalclassi\u0002ers with inertia ratio maximization. In\n115th AES convention ,NewYork, USA, October\n2003.\n[4]Slim Essid, GaelRichard, andBertrand David. In-\nstrument recognition inpolyphonic music. InIn-\nternational Confer ence onAcoustics, Speec h,and\nSignal Processing (ICASSP) ,Philadelphia, USA,\nMarch 2005.\n[5]F.Pachet andD.Cazaly .Ataxonomy ofmusical\ngenres. InContent-Based Multimedia Information\nAccess Confer ence (RIAO),Paris, France, April.\n[6]C.McKay andI.Fujinag a.Automatic genre classi\u0002-\ncation using largehigh-le velmusical feature sets. In\n5thInternational Confer ence onMusic Information\nRetrie val(ISMIR) ,Barcelona, Spain, October 2004.\n[7]TaoLiandMitsunori Ogihara. Music genre clas-\nsi\u0002cation with taxonomy .InInternational Confer -\nence onAcoustics, Speec h,and Signal Processing\n(ICASSP) ,Philadelphia, USA, March 2005.\n[8]Stephen McAdams, S.Winsber g,S.Donnadieu,\nG.DeSoete, andJ.Krimphof f.Perceptual scaling of\nsynthesized musical timbres: common dimensions,\nspeci\u0002cities andlatent subject classes. Psychologi-\ncalreserach,58:177192, 1995.\n327[9]Sergios Theodoridis and Konstantinos Koutroum-\nbas. Pattern recognition .Academic Press, 1998.\n[10] S.Zhou andR.Chellappa. From sample similarity to\nensemble similarity: probabilistic distance measures\ninreproducing kernel hilbert space. IEEE transac-\ntions onpattern analysis andmachine intellig ence.\ntobepublished.\n[11] B.Sholk opfandA.J.Smola. Learning with kernels .\nTheMIT Press, Cambridge, MA, 2002.\n[12] Richard Duda andP.E.Hart. Pattern Classi\u0002cation\nandScence Analysis .Wiley-Interscience. John Wi-\nley&Sons, 1973.\n[13] Geof froyPeeters. Alargesetofaudio features\nforsound description (similarity andclassi\u0002cation)\ninthecuidado project. Technical report, IRCAM,\n2004.\n[14] Slim Essid, GaelRichard, andBertrand David.Mu-\nsical instrument recognition based onclass pairwise\nfeature selection. In5thInternational Confer ence\nonMusic Information Retrie val(ISMIR) ,Barcelona,\nSpain, October 2004.\n328"
    },
    {
        "title": "Combining D2K and JGAP for Efficient Feature Weighting for Classification Tasks in Music Information Retrieval.",
        "author": [
            "Rebecca Fiebrink",
            "Cory McKay",
            "Ichiro Fujinaga"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415754",
        "url": "https://doi.org/10.5281/zenodo.1415754",
        "ee": "https://zenodo.org/records/1415754/files/FiebrinkMF05.pdf",
        "abstract": "Music classification continues to be an important component of music information retrieval research. An underutilized tool for improving the performance of classifiers is feature weighting. A major reason for its unpopularity, despite its benefits, is the potentially infinite calculation time it requires to achieve optimal results. Genetic algorithms offer potentially sub-optimal but reasonable solutions at much reduced calculation time, yet they are still quite costly. We investigate the advantages of implementing genetic algorithms in a parallel computing environment to make feature weighting an affordable instrument for researchers in MIR.",
        "zenodo_id": 1415754,
        "dblp_key": "conf/ismir/FiebrinkMF05",
        "keywords": [
            "Music classification",
            "Music information retrieval",
            "Feature weighting",
            "Genetic algorithms",
            "Parallel computing",
            "Research",
            "MIR",
            "Optimal results",
            "Costly",
            "Affordable"
        ],
        "content": "COMBINING D2K AND JGAP FOR EFFICIENT FEATURE WEIGHTING \nFOR CLASSIFICATION TASKS IN MUSIC INFORMATION RETRIEVAL\nRebecca Fiebrink Cory McKay Ichiro Fujinaga \n Music Technology \nMcGill University \nMontreal, Canada  \n{rebecca.fiebrink, cory.mckay}@mail.mcgill.ca, ich@music.mcgill.ca \n \n \nABSTRACT \nMusic classification cont inues t o be an important com-\nponent  of m usic inform ation ret rieval research. An un-\nderutilized  tool for improving the performance of classi-\nfiers is feature wei ghting. A m ajor reason for its unpopu-\nlarity, despite its b enefits, is th e potentially in finite cal-\nculation time it req uires to  achieve optimal resu lts. Ge-\nnetic algorithms offer pot entially sub-opt imal but rea-\nsonable solutions at m uch reduced calculation tim e, yet \nthey are still q uite co stly. W e investigate th e advantages \nof implementing genet ic algorithms in a paral lel com put-\ning envi ronm ent to make feat ure wei ghting an affordabl e \ninstrum ent for researchers in M IR. \n \nKeyw ords: Classification, Feat ure W eighting, Paral lel \nComputing, D2K.  \n1 INTRODUCTION \nClassification is a lively area of research within m usic \ninform ation ret rieval (MIR). Genre and com poser cl assi-\nfication system s, sim ilarity-based m usic recom menda-\ntion system s, and intelligen t interactive accom panim ent \nsystem s are just a few of the areas where these tech-\nniques are used. Unfort unately, the vari ety and t echni cal \nsophistication of pattern r ecognition techniques available \ncan make it difficult to choose t he best  approach t o appl y \nto a part icular probl em.  \nAn automated system  for optimally selectin g and \nfine-tuning classifiers for a given probl em coul d allow \nresearchers to devote their time and energies to tasks \nmore important than adapt ing and i mplementing cl assi-\nfication systems themselves. The Aut onom ous C lassifi-\ncatio n Engine (ACE) p roject at McGill University is \nsuch a to ol, and it is b uilt with  the particular needs of \nthe MIR community forem ost in m ind (McKay et al. \n2005). ACE experi mentally com pares a vari ety of di -\nmensionality red uction tech niques, classificatio n algo-\nrithms, and classifier en sembles in  order to find suitable \napproaches t o use for a gi ven user’s data set, feature set, and t axonom y. The i ncorporat ion of effi cient, paral lel \nfeatu re weig hting using genetic algorithms will contrib-\nute significan tly to  ACE’s p ower an d flex ibility; th is \npaper di scusses t he implementation and perform ance of \nthe feat ure wei ghting subsy stem. \n2 FEATURE WEIGHTING \nEven though many features m ay be avai lable to a classi-\nfier, it is not necessarily d esirab le to use all o f them. In \nfact, the size of a classifier’s  training set  must general ly \ngrow exponentially with the dimensionality of the fea-\nture space (Duda et al. 2001,  169–70). Relevant features \ncan be selected from  the set of available features by ex-\nperim entation, wherein a cla ssifier is trained and evalu-\nated using candi date feat ure subset s. An exhaust ive \nsearch  for the optimal su bset is o ften infeasib le, how-\never, because the num ber of potential subsets grows at a \nrate of Θ(2d), where d is the num ber of available features \n(Siedlecki and Skl ansky  1989). \nFeature weighting attem pts to further optim ize a clas-\nsification schem e by assi gning real -valued wei ghts to \nfeatures according to their relevancy (Punch et al. \n1993). Just  as in feature sel ection, candi date sets of fea-\nture wei ghts can be eval uated experi mentally. However, \nthe search space of candidate weight sets now grows \nwith Θ(nd), where n is th e (potentially in finite) n umber \nof allowable values for each  weight (Punch et al. 1993). \nOne might be t empted to perform  an anal ysis of the \nfinal choi ce of feat ure subset  or wei ghts to obtain new \ninsights about  a classification probl em, part icularly in \nMIR. However, t he perform ance of sel ection or wei ght-\ning schem es is dependent  on t he dat a set, classification \nprobl em, and cl assifier in quite com plex way s, and such \nanalysis is beyond t he scope of t his project . \n3 GA’S AND FEATURE WEIGHTING \nGenet ic algorithms (GA’s) (Hol land 1975) are an ap-\nproach to computation inspired by  biological evol ution, \nand t hey are useful  in optimization probl ems in which \nmaxima are h ard to find deterministically. A GA main-\ntains a population of individua ls that evolve according \nto speci fic rules of sel ection and t hrough operat ors such \nas crossover and m utation. The fitness of each individ-\nual in the envi ronm ent is eval uated, and sel ection ex-\nploits this inform ation in favori ng high-fi tness individu-\nals.  Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies  bear this  notice and the full \ncitation on the first page. \n© 2005 Queen Mary , University  of London To accom plish feature selection or weighting for d \nfeatures usi ng GA’s, i ndividuals are represent ed as \nchromosomes with d genes, where t he genes are binary-\n510   \n \nvalued for sel ection and real -valued for weighting. The \nfitness of each chrom osome is evaluated experim entally \nby training and testing a cl assifier usi ng t he given \nweights, where better cla ssifier perform ance yields \nhigher fi tness. Si edlecki and Skl ansky  (1989) found that \nGA’s coul d qui ckly find near-opt imal solutions t o fea-\nture sel ection for a k-NN cl assifier. Punch et  al. (1993) \nstudied GA feat ure wei ghting for k-NN cl assifiers and \nfound t hat feature wei ghting out perform ed feature selec-\ntion al one. They  also found t hat implementing the sys-\ntem in paral lel com pensat ed for t he long com putation \ntime needed to experim entally com pute chrom osome \nfitness. McKay (2004) has su ccessfully applied GA’s to \nfeature sel ection and wei ghting in MIR classification \nprobl ems. Addi tionally, Minaei-Bidgoli et al. (2004) \nhave recently shown that GA’s are also powerful tools \nfor featu re weig hting in multiple classifier system s. \n4 IMPLEMENTATION \nThis system  includes functionality for featu re weig hting \nusing GA’s, and i t allows for paral lel confi gurat ion to \nreduce com putation tim e. Of forem ost concern to the \ndesign of any  feature wei ghting system  are its efficiency \nand accuracy. However, seve ral other goals m ust be \nconsi dered in the desi gn of a system  targeted at the di-\nverse M IR com munity. The sy stem shoul d be portable \nto any operat ing system and hardware confi gurat ion. It s \ncore features, including pa rallelism , should be accessi-\nble by any music research  lab, whether it is outfitted  \nwith high-end shared-m emory clusters or si mply with a \nhandful  of workst ations. The sy stem shoul d be scal able \nto use m ore or fewer com puting nodes for i ts operat ions, \ndependi ng on vari able external dem ands on computing \nresources. Fi nally, the system shoul d be fl exible to al-\nlow cust omization (e.g., changes t o the GA behavi or). \n4.1 JGAP \nWe looked t o the open-source com munity for existing \nstable, robust  GA soft ware. JGAP (R otstan and M effert \n2005) is a GA package written in Java and distributed \nfreely. Its design is modular, it is well docum ented and \neasily extensible, and act ive devel oper and user com mu-\nnities promote continual improvements. Th ese ch aracter-\nistics support our goals of portability and flexibility. \n4.2 D2K \nThe Dat a-to-Knowl edge (D2K) m achine learni ng envi-\nronm ent was chosen as t he fram ework t o support  paral lel \nand di stributed operat ions. D2K i s “a vi sual program -\nming envi ronm ent that allows for rapi d prot otyping and \nalgorithm  developm ent” (Downie 2004). It is written in \nJava, so i t is port able. Furt herm ore, i t is the underl ying \nfoundat ion of t he new M 2K sy stem for M IR research, so \nmany of ACE’s u sers will lik ely alread y be using D2K. \nD2K p rograms (o r “itin eraries”) are d ynamically \nscalable in that a user can specify at runtim e that indi-\nvidual component s (or “modules”) of a D2K program  are to  run in parallel, as well as designate on which re-\nmote machines they shoul d run. There are no const raints \non the hardware or operat ing sy stems of these machines, \nso D2K’s parallelism  is accessi ble to any lab with two \nor m ore com puters connected over a LAN or the inter-\nnet. \n4.3 Implementation of the Feature Weighting System \nOur featu re weig hting system  combines a GA b uilt on \nJGAP wi th cust om D2K m odules. The GA uses chrom o-\nsomes of l ength d, where d is the num ber of pot ential \nfeatu res. Mu tation occurs with  a probability of 6.67%. \nParents and children are all evaluated for fitness, and the \ntop m individuals are preserved for t he next  generat ion, \nwhere m is the desi red popul ation si ze. (W e keep the \nJGAP d efault settin gs for parameters su ch as mutation \nrate and selection m ethod, as t he goal  is not to optimize \nGA weig hting but rather to  demonstrate its p otential.) \nThe al gorithm stops when t he best  individual’s fitness \ndoes not  improve over fi ve subsequent  generat ions. \nThe parallel strateg y used by the system  is “m aster-\nslave paral lelism” (Cantú-Paz 2000), al so called “micro-\ngrained paral lelism” by Punch et  al. (1993). In a master-\nslave GA, a si ngle popul ation is maintained on a “mas-\nter” node t hat handl es sel ection, crossover, and muta-\ntion. The m aster sends a port ion of t he popul ation to \neach “slave” node for parallel fitness evaluation. The \nmaster-slav e model is easy to  implement, and existing \ndesign guidelines for seri al single-popul ation m odels \ncan be di rectly appl ied (Cantú-Paz 2000). \nThe system  uses a D2 K itin erary to  perform fitness \nevaluation on a popul ation. An Input  module loads the \nentire popul ation and passes one chrom osome at a time \nto a Fi tness Eval uator module. The Fitness Evaluator \nassesses a chrom osome’s fitness via leave-one-out \ncross-val idation of a k-NN cl assifier on t he data. This \nmodule can cl one i tself and run in paral lel on any num-\nber of slave m achines, one clone at a tim e per processor. \nAt its termination, each clone passes its chrom osome \nwith its calculated fitness t o an Out put module, which \nreassem bles the popul ation. Upon termination of the \nitinerary, co ntrol retu rns to the JGAP GA fo r selectio n.  \nThis im plem entation allows slaves that are operating \nfastest at th e tim e of execution to take on more of the \ncomputational load, as they can be assi gned m ore mod-\nule clones. Thi s resul ts in higher t hroughput  than a sys-\ntem that naively places equal loads on all processors. \nThis is of utmost im portance to  small lab s, in which all \nnodes m ay doubl e as workst ations and/ or web servers, \nand node perform ance can vary  dram atically over the \ncourse of a day . \nThe current  system is quite flexible: popul ation si ze, \nGA behavi or, the num ber of feat ures, and t he allowabl e \nfeature weights can all be changed easily. Currently, the \nsystem  evaluates chrom osomes usi ng a k-NN cl assifier \nderived from  Weka and reads dat a from  Weka ARFF \nfiles (Witten and Frank 2000), but a m odular design \nallows use with any other classifier system . \n \n511   \n \n5 SYSTEM PERFORMANCE \nThree basi c tests were conduct ed to assess system per-\nform ance, and an addi tional two tests were run t o further \ndemonstrate its useful ness on probl ems for which ex-\nhaustive featu re selectio n is in feasib le. In  the first test, \nthe system perform ed feat ure wei ghting on a standard \n768-i nstance, 8-feat ure dat a set (Pima Indian Diabetes \nfrom  the UCI Repository, Blake and M erz 1998). Fi ve \ntrials were perform ed on each of three sm all populations, \nand k=1 nei ghbor was used. The m aster node used a \n2.8GHz Pentium 4 PC  with 1.5GB  RAM, and t he slaves \nused a 2.8GHz Pent ium 4 server with 1GB  RAM run-\nning Li nux and a 867M Hz, 512M B PowerPC  runni ng \nOS X. \nTable 1 shows t he popul ation size, mean number of \ngenerat ions t o convergence, m ean hours t o convergence, \nand the mean and st andard devi ation of t he percent  of \ninstances correctly classified  using leave-one-out  cross-\nvalidation. C lassification of t his data set without weight-\ning or sel ection yielded a success rat e of 69.8%. An ex-\nhaust ive search for t he opt imal feature subset  took 0.27 \nhours and resulted in a success rate of 70.8%. \nTable 1: Feature wei ghting, UCI Diabetes. \nP Avg. \nGen. Avg. \nTime (h) Avg. % \nCorrect SD % \nCorrect \n10 8.8 0.109 72.5 0.475 \n20 10.2 0.230 72.1 0.687 \n50 18.6 0.944 73.2 0.590 \nIn this sm all test, classi fication accuracy using fea-\nture wei ghting com pared favor ably to classification us-\ning optimal feat ure sel ection and cl assification wi thout \nselection or wei ghting. \nThe second test exam ined cl assification time for an-\nother standard data set (UCI Breast Can cer, Zwitter an d \nSoklic 1988), which cont ained 286 i nstances wi th 9 fea-\ntures. Its sm aller size facilitated  more ex tensive testin g \nwhere ten tests were run on each population size: five \ntests used three nodes, and fi ve used one (t he m aster). \nThe m aster ran on a 2.4GHz, 512M B RAM PC, and the \nslaves ran on t he Linux and M acintosh machines used in \nTest 1. k=17 nei ghbors was used. Tabl e 2 shows t he \npopul ation size, mean num ber of generat ions t o conver-\ngence, mean and st andard devi ation of t he percent  cor-\nrectly classified usi ng l eave-one-out  cross-val idation, \nand runni ng time for one and t hree processors. Exhaus-\ntive search for t he opt imal feature subset  took 0.08 hours \nand y ielded a cl assification rat e of 76.9%, and classifica-\ntion wi thout selection or wei ghting yielded 73.4%. \nTable 2: Feature weighting, UCI Breast Cancer. \nAvg. Time (h) P Avg. \nGen Avg. % \nCorrect SD % \nCorrect 1p 3p \n10 10.8 76.9 0.66 0.04 0.03 \n20 11.4 77.2 0.43 0.10 0.05 \n50 10.2 77.5 0.47 0.27 0.08 \n100 11.2 77.8 0.54 0.58 0.16 \n200 13.4 78.1 0.46 0.84 0.35 \n500 11.5 78.2 0.51 2.09 0.59 Test 2 shows t hat feature wei ghting can be superi or to \nexhaust ive feat ure sel ection on t his data set and com-\npares favorabl y even for sm all popul ations. Test  2 al so \nshows that classification accuracy tends to im prove with \npopul ation si ze, a fi nding support ed by studies on GA’s \nindicating that the great er di versi ty of large popul ations \ndeters prem ature convergence and t herefore t ends t o \nproduce higher-quality solutions (Cantú-Paz 2000). This \nfinding underscores t he need for paral lel systems that \ncan reduce t he long runt ime of feature weighting using \nlarge popul ations.  \nTest 2 dem onstrates a significant speedup resulting \nfrom parallel ex ecution. The total runtime for three \nnodes i s, on average, approxi mately one-t hird of t hat for \none node al one. Thi s suggest s that communication costs \nare very low in com parison to the cost of fitness evalua-\ntion. These results suggest that additional nodes would \nappreci ably further speed up com putation, and that labs \nwith only a few m achines can  still b enefit fro m parallel-\nism. \nThe third test ap plied featu re selectio n to the snare \ndrum  timbre recogni tion probl em present ed by Tindale \net al. (2004). The dat a set consi sted of 1260 i nstances, \ncreated by three players playing each of seven snare \ndrum  strokes t wenty times on t hree drum s. Tindale’s \nstudy included feat ures from  time-dom ain only and \ntime- and frequency-dom ain measurem ents, extracted \nfrom  the attack port ion onl y and from  512-sam ple win-\ndows over t he whol e signal. Each of t he four classifica-\ntion probl ems selected here uses a unique combination \nof these feature and si gnal types t o distinguish am ong \nthe seven st rokes. Feat ure sel ection was perform ed once \non each problem  using an initial population size of 50. \nk=1 nei ghbor was used. Tabl e 3 shows the classification \nprobl em, the number of avai lable feat ures, t he perform -\nance of Ti ndale’s best  classifier on t hat probl em, and t he \nperform ance of our classifier after feature selection. 10-\nfold cross-val idation is used for bot h our resul ts and \nTindale’s. An exhaustive search for optim al feature se-\nlection was also perform ed on the time-dom ain features; \nthis yielded 91.9% accuracy for the attack portion prob-\nlem and 92.9% accuracy for the 512-sam ple problem , \nand it took 0.7 hours to perform  each search. \nTable 3: Comparison of snare drum  timbre cl assi-\nfication wi th Tindale et al. 2004. \nProblem No. \nFeatures  Tindale et \nal. % Feature \nSelection % \nAll, attack 57 94.9 98.5 \nAll, 512 35 93.0 95.5 \nTime, attack 8 90.8 91.9 \nTime, 512 8 90.9 92.9 \nTest 3 dem onstrates improvem ent in all four probl ems \nover Ti ndale’s best  classifications of snare drum  timbre. \nThese i mprovem ents arose from  just one run of a feature \nselection GA with a relatively small initial population. \nAddi tionally, the GA’s found opt imal solutions for t he \ntwo 8-feat ure probl ems for whi ch it was possi ble to ex-\nhaustively search the selection space. Based on the find-\n \n512   \n \nings above, it is likely that the selection on t he other two \nprobl ems was near-opt imal, and t hat usi ng feat ure \nweighting could result in even greater accuracy. \nFinally, the system was run on t wo other cl assifica-\ntion probl ems for whi ch exhaust ive feat ure sel ection was \ninfeasible. The first was the UCI vehicle recognition \nprobl em, in which 846 i nstances wi th 18 feat ures were \nclassified into 4 categori es (Blake and M erz 1998). k=1 \nneighbor was used. C lassification wi thout selection \nyielded 69.5% accuracy. Using feature selection and an \ninitial population of 50, the system  achieved 75.5% ac-\ncuracy. \nThe second classification task, anot her exam ple from  \nMIR, was a beat-box sound recognition problem  using \n1192 recorded instances, each belonging to one of five \nclasses of hi ts (Sinyor et al. 2005). The data was col-\nlected from  six beat boxers, and 24 potential features \nwere extracted using spectral and tem poral measure-\nments. k=1 nei ghbor was used. C lassification without \nselection yielded 93.3% accuracy. Using feature selec-\ntion and an initial population of  50, the system  was able \nto reach 94.7% accuracy.  \n6 CONCLUSIONS \nWe have im plem ented efficient and accurate feature se-\nlection and weighting using JGAP and D2K i n the con-\ntext of M IR and t he AC E project . Test s show that k-NN \nclassification usi ng feat ure wei ghting out perform s un-\nweighted cl assification and can surpass classification \nusing exhaust ive feat ure sel ection. Addi tionally, feat ure \nselection al one out perform s the best  publ ished resul ts on \nsnare drum  timbre classifica tion. Furt herm ore, t he sys-\ntem’s parallel im plementation resu lts in significan t \nspeedup usi ng as few as t hree nodes. These resul ts sug-\ngest that this system  can be quite useful to MIR research, \nespecially when applied to large classification problem s \nfor which exhaustive featu re selectio n is in feasib le. \nFuture wo rk will in clude adding greater flex ibility, \nsuch as the ability to  automatically o ptimize GA p arame-\nters given const raints on t ime and com puting resources. \nPlans are also in place to us e the existing parallel fram e-\nwork t o effi ciently conduct  empirical comparisons of \nGA’s with other selection and wei ghting al gorithms on a \nvariety of cl assification probl ems. Furt her t esting, par-\nticularly o n MIR-related  problems, will co ntinue to elu-\ncidate the system ’s relativ e stren gths and limitatio ns. \nACKNOWLEDGEMENTS \nWe gratefully acknowledge support from  the McGill \nUniversity Max Stern Fellowshi p in Music, SSHR C, and \nthe McGill Alm a Mater Fund. W e also thank Adam  Tin-\ndale an d Ellio t Sinyor for sharing their data. \nREFERENCES \nBlake, C ., and C . Merz. 1998. “UC I Repository of \nmachine learning databases.” <http://www.ics.uci.edu/ \n~mlearn /MLRep ository.html> Un iversity o f Califo rnia, Irvine, Depart ment of Inform ation and C omputer \nSciences. Accessed 13 Apri l 2005. \nCantú-Paz, E. 2000. Efficient and accurate parallel \ngenet ic algorithms. Boston: Kluwer Academ ic. \nDowni e, J. 2004. Int ernational music inform ation \nretrieval systems eval uation l aborat ory (IM IRSEL):  \nIntroduci ng D2K and M 2K. Demo Handout  at the 2004 \nInternational Conference on Musi c Informat ion \nRetrieva l. \nDuda, R ., P. Hart , and D. St ork. 2001. Pattern  \nclassifica tion. New York:  John W iley & Sons, Inc. \nHolland, J. H. 1975. Adaptation in natural and artificial \nsystems . Ann Arbor: University o f Mich igan Press. \nMcKay, C. 2004. Aut omatic genre cl assification of \nMIDI recordings. M.A. Thesis . McGill Un iversity, \nCanada. \nMcKay, C., R. Fiebrink, D. M cEnni s, B. Li, and I. \nFujinaga. 2005. AC E: A fram ework for optimizing \nmusic classification. Proceedings of the International \nConference on Music Information Retrieval.  \nMinaei-Bidgoli, B., G. Kort emeyer, and W . Punch. \n2004. Optimizing classification ensem bles via a genet ic \nalgorithm for a web-based educat ional sy stem. \nProceedings of the International Workshop on \nSyntactical and Structural  Pattern Recognition and \nStatistica l Pattern  Reco gnition, 397–406. \nPunch, W ., E. Goodm an, M . Pei, L. Chia-Shun, P. \nHovl and, and R. Enbody . 1993. Furt her research on \nfeature sel ection and cl assification using genet ic \nalgorithms. Proceedings of the 5th International \nConference on Genet ic Algorithms, 557–64. \nRotstan, N., and K. M effert. 2005. JGAP:  The Java \ngenetic algorithms p ackage. <h ttp://jgap. \nsourceforge.net /> Accessed 13 Apri l 2005. \nSiedlecki, W., and J. Skl ansky . 1989. A not e on genet ic \nalgorithm s for large-scale feature selection. Pattern  \nRecogni tion Let ters 10 (5):  335–47. \nSinyor, E., C . McKay, R. Fiebrink, D. M cEnni s, and I. \nFujinaga. 2005. B eatbox cl assification usi ng AC E. \nProceedings of the Inter national Conference on Music \nInformation Retrieva l. \nTindale, A., A. Kapur, G. Tzanet akis, and I. Fujinaga. \n2004. Retrieval of percussi on gest ures usi ng t imbre \nclassification t echni ques. Proceedings of the \nInternational Conference on Musi c Informat ion \nRetrieva l. \nWitten, I., and E. Frank. 2000. Data mining: Pract ical \nmachi ne learning t ools and t echni ques w ith Java \nimplemen tations. San Franci sco: Morgan Kaufm ann. \nZwitter, M., and M. Soklic. 1988. This breast cancer \ndomain was obt ained from  the University Medical \nCentre, Institute of Oncol ogy, Ljubljana, Yugoslavia. \n \n513"
    },
    {
        "title": "Novelty Detection Based on Spectral Similarity of Songs.",
        "author": [
            "Arthur Flexer",
            "Elias Pampalk",
            "Gerhard Widmer"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416504",
        "url": "https://doi.org/10.5281/zenodo.1416504",
        "ee": "https://zenodo.org/records/1416504/files/FlexerPW05.pdf",
        "abstract": "We are introducing novelty detection, i.e. the automatic identification of new or unknown data not covered by the training data, to the field of music information retrieval. Two methods for novelty detection one based solely on the similarity information and one also utilizing genre label information are evaluated within the context of genre classification based on spectral similarity. Both are shown to perform equally well. Keywords: novelty detection, spectral similarity, genre classification 1",
        "zenodo_id": 1416504,
        "dblp_key": "conf/ismir/FlexerPW05",
        "keywords": [
            "Novelty detection",
            "Spectral similarity",
            "Genre classification",
            "Music information retrieval",
            "Mel Frequency Cepstrum Coefficients (MFCC)",
            "Gaussian Mixture Models (GMM)",
            "Music recommendation",
            "Playlist generation",
            "Music spaces",
            "Machine learning"
        ],
        "content": "NOVELTYDETECTIONBASEDONSPECTRALSIMILARITYOFSONGS\nArthur Flexer1,2, Elias Pampalk2, Gerhard Widmer2,3\n1Institute of Medical Cybernetics and Artiﬁcial Intelligen ce\nCenter for Brain Research, Medical University of Vienna\nFreyung 6/2, A-1010 Vienna, Austria\n2Austrian Research Institutefor Artiﬁcial Intelligence (O FAI)\nFreyung 6 /6, A-1010 Vienna, Austria\n3Department of Computational Perception\nJohannes Kepler University\nAltenberger Str. 69, A-4040 Linz, Austria\narthur@ai.univie.ac.at, elias@ofai.at, gerhard.widmer @jku.at\nABSTRACT\nWe are introducing novelty detection, i.e. the automatic\nidentiﬁcation of new or unknown data not covered by the\ntraining data, to the ﬁeld of music information retrieval.\nTwo methods for novelty detection - one based solely on\nthe similarity information and one also utilizing genre la-\nbelinformation-areevaluatedwithinthecontextofgenre\nclassiﬁcationbasedonspectralsimilarity. Bothareshown\nto perform equally well.\nKeywords: novelty detection, spectral similarity, genre\nclassiﬁcation\n1 INTRODUCTION\nNovelty detection is the identiﬁcation of new or unknown\ndata that a machine learning system is not aware of dur-\ning training (see [1] for a review). It is a fundamental\nrequirement for every good machine learning system to\nautomatically identify data from regions not covered by\nthe training data since in this case no reasonable decision\ncan be made. This paper is about introducing novelty de-\ntectiontotheﬁeldofmusicinformationretrievalwhereso\nfar the problem has been ignored.\nFor music information retrieval, the notion of central\nimportanceismusicalsimilarity. Propermodelingofsim-\nilarity enables automatic structuring and organization of\nlarge collections of digital music, and intelligent music\nretrieval in such structured “music spaces”. This can be\nutilized for numerous different applications: genre class i-\nﬁcation, play list generation, music recommendation, etc.\nWhat all these different systems lack so far is the abil-\nity to decide when a new piece of data is too dissimilar\nfor making a decision. Let us e.g. assume the following\nuser scenario: a user has on her hard drive a collection of\nsongsclassiﬁedintothethreegenres’hiphop’,’punk’and\n’death metal’;given anew song fromagenre not yet cov-\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrstpage.\nc/circlecopyrt2005 Queen Mary, University of LondonTable 1: Statistics of our data set\nArtists/Genre Tracks/Genre\nGenres Artists Tracks Min Max Min Max\n22 103 2522 3 6 45 259\nered by the collection (say, a ’reggae’ song), the system\nshould mark this song as ’novel’ therefore needing man-\nual processing instead of automatically and falsely classi -\nfying it into one of the three already existing genres (e.g.\n’hip hop’). Another example is the automatic exclusion\nof songs from play lists because they do not ﬁt the over-\nall ﬂavor of the majority of the list. Novelty detection\ncould also be utilized to recommend new types of music\ndifferent from a given collection if users are longing for a\nchange.\nWe will present two methods for novelty detection\nbased on spectral similarity of songs and evaluate them\nwithin a genre classiﬁcation context (see e.g. [2]). Spec-\ntral similarity is computed using Mel Frequency Cep-\nstrum Coefﬁcients (MFCC) and Gaussian Mixture Mod-\nels (GMM). After introducing the data base used in the\nstudy as well as the employed preprocessing (Sec. 2), we\nwill describe the methods of GMMs and novelty detec-\ntion (Sec. 3), present our experiments and results (Sec. 4)\nwhich is followed by discussion (Sec. 5) and conclusion\n(Sec. 6).\n2 DATA\nFor our experiments we used an in-house collection con-\ntaining S= 2522songsbelongingto G= 22genres. De-\ntails are given in Tables 1 and 2. The data set has mainly\nbeen organized according to genre/artist/album. Thus, all\npieces of the same artist (and album) are assigned to the\nsamegenre,whichisaquestionablebutcommonpractice.\nThegenresareuserdeﬁned,farfromperfectandtherefore\nquite a realistic setting: there are two different deﬁnitio ns\nof trance, there are overlaps, for example, jazz and jazz\nguitar, heavy metal and death metal etc.\nFrom the 22050Hz mono audio signals two minutes\nfrom the center of each song are used for further analy-\nsis. We divide the raw audio data into overlapping\nframesofshortdurationanduseMelFrequencyCepstrum\nCoefﬁcients (MFCC) to represent the spectrum of each\nframe. MFCCs are a perceptually meaningful and spec-\n260Table 2: List of genres for our data set\na cappella acid jazz blues\nbossa nova celtic death metal\ndrum and bass downtempo electronic\neuro-dance folk-rock german hip hop\nhard core rap heavy metal/thrash italian\njazz jazz guitar melodic metal\npunk reggae trance\ntrance2\ntrally smoothed representation of audio signals. MFCCs\nare now a standard technique for computation of spec-\ntral similarity in music analysis (see e.g. [3]). The frame\nsize for computation of MFCCs for our experiments was\n23.2ms(512 samples), with a hop-size of 11.6ms(256\nsamples)fortheoverlapofframes. Theaverageenergyof\neach frame’s spectrum was subtracted. We used the ﬁrst\n20 MFCCs for all our experiments.\n3 METHODS\n3.1 Computing spectral similarityof songs\nThefollowingapproachtomusicsimilaritybasedonspec-\ntral similarity pioneered by Logan and Salomon [4] and\nAucouturier and Pachet [5] is now seen as one of the\nstandard approaches in the ﬁeld of music information re-\ntrieval. For a given music collection of Ssongs, each be-\nlonging to one of Gmusic genres, it consists of the fol-\nlowing basic steps:\n•foreachsong,computeMFCCsforshortoverlapping\nframes as described in Sec. 2\n•train a Gaussian Mixture Model (GMM) for each of\nthe songs\n•compute a similarity matrix between all songs using\nthe likelihood of a song given a GMM\n•based on the genre information, do nearest neighbor\nclassiﬁcation using the similaritymatrix\nThe last step of genre classiﬁcation can be seen as a\nform of evaluation. Since usually no ground truth with\nrespect to music similarity exists, each song is labeled as\nbelonging to a music genre using e.g. music expert ad-\nvice. Good genre classiﬁcation results are taken to indi-\ncate good similarity measures. The winning entry to the\nISMIR 2004 genre classiﬁcation contest1by Elias Pam-\npalk followed basically the above described approach.\nA Gaussian Mixture Model (GMM) models the den-\nsityof the input data by a mixture model of the form\np(x) =M/summationdisplay\nm=1PmN[x,µm,Um] (1)\nwhere Pmis the mixture coefﬁcient for the m-th compo-\nnent, Nis the normal density and µmandUmare the\n1ISMIR 2004, 5th International Conference on Music\nInformation Retrieval, Spain, 2004; see\nhttp://ismir2004.ismir.net/ISMIR-Contest.htmlmean vector and covariance matrix of the m-th mixture.\nThe log-likelihood function is given by\nL(X) =1\nTT/summationdisplay\nt=1log(p(xt)) (2)\nfor a data set Xcontaining Tdata points. This function\nis maximized both with respect to the mixing coefﬁcients\nPmand with respect to the parameters of the Gaussian\nbasis functions using Expectation-Maximization (see e.g.\n[6]). For all our experiments we used M= 30compo-\nnents. To compute similarity between two songs Aand\nB, we sample 400points SAfrom model A and com-\npute the log-likelihood of these samples given model B\nusing Equ. 2 which gives L(SA|B). Reversing the roles\nofAandBwe get L(SB|A). Summing these two log-\nlikelihoods and subtracting the self-similarity for norma l-\nization yields the following similarity function (which is\nan approximation of the symmetrised Kullback-Leibler\ndivergence between the two models A and B):\nd(A,B ) =L(SA|B)+L(SB|A)−L(SA|A)−L(SB|B)\n(3)\n3.2 Algorithms for novelty detection\nRatio-reject : The ﬁrst reject rule is based on density in-\nformationaboutthetrainingdatacapturedinthesimilarit y\nmatrix. An indication of the local densities can be gained\nfrom comparing the distance between a test object Xand\nits nearest neighbor in the training set NNtr(X), and the\ndistance between this NNtr(X)and its nearest neighbor\nin the training set NNtr(NNtr(X))[7]. The object is re-\ngardedasnoveliftheﬁrstdistanceismuchlargerthanthe\nsecond distance. Using the following ratio\nρ(X) =/bardbld(X,NNtr(X))/bardbl\n/bardbld(NNtr(X),NNtr(NNtr(X)))/bardbl(4)\nwe reject Xif:\nρ(X)> E[ρ(Xtr)] +s∗std(ρ(Xtr))(5)\nwithE[ρ(Xtr)]being the mean of all quotients ρ(Xtr)\ninside the training set and std(ρ(Xtr))the corresponding\nstandard deviation (i.e. we assume that the ρ(Xtr)have a\nnormal distribution). Parameter scan be used to change\nthe probability threshold for rejection. Setting s= 3\nmeans that we reject a new object Xif its ratio ρ(X)is\nlarger then the mean ρwithin the training set plus three\ntimes the corresponding standard deviation. In this case\na new object is rejected because the probability of its dis-\ntance ratio ρ(X)is less than 1%when compared to the\ndistribution of ρ(Xtr). Setting s= 2rejects objects less\nprobable than 5%,s= 1less than 32%, etc.\nKnn-reject : It is possible to directly use nearest\nneighbor classiﬁcation to reject new data with higher risk\nof being misclassiﬁed [8]:\nrejectXifnot:\n261g(NN1tr(X)) =g(NN2tr(X)) =...=g(NNktr(X))\n(6)\nwithNNitr(X))being the ith nearest neighbor of Xin\nthe training set, g()a function which gives the genre in-\nformation for a song and i= 1,... ,k. A new object Xis\nrejectedifthe knearestneighborsdonotagreeonitsclas-\nsiﬁcation. Thisapproachwillworkfornoveltydetectionif\nnewobjects Xinducehighconfusionintheclassiﬁer. The\nhigher the value for kthe more objects will be rejected.\n4 RESULTS\nTo evaluate the two novelty detection approaches de-\nscribed in Sec. 3.2 we use the following approach\nshown as pseudo-code in Table 3. First we set aside\nall songs belonging to a genre gas novel songs\n([novel,data]=separate(alldata,g) ) which\nyields data sets novelanddata(all songs not be-\nlonging to genre g). Then we do a ten-fold cross-\nvalidation using dataandnovel: we randomly split\ndataintotrainandtestfold ([train,test]\n= split(data,c) ) with train always consist-\ning of 90%andtestof10%ofdata. We\ncompute the percentage of novelsongs which are\nrejected as being novel ( novel reject(g,c) =\nreject(novel) ) and do the same for the testsongs\n(testreject(g,c) = reject(test) ). Last we\ncompute the accuracy of the nearest neighbor classiﬁca-\ntion on testdata that has not been rejected as being\nnovel ( accuracy(g,c) = classify(test(not\ntestreject)) ). The evaluation procedure gives G×\nC(22×10)matricesof novel reject,testreject\nandaccuracy for each parameterization of the novelty\ndetection approaches.\nTable 3: Outline of Evaluation Procedure\nfor g = 1 : G\n[novel,data] = separate(alldata,g)\nfor c = 1 : 10\n[train,test] = split(data,c)\nnovel_reject(g,c) = reject(novel)\ntest_reject(g,c) = reject(test)\naccuracy(g,c) =\nclassify(test(not test_reject))\nend\nend\nThe results for novelty detection based on the Ratio-\nreject and the Knn-reject rule are given in Figs. 1 and\n2 as Receiver Operating Characteristic (ROC) curves\n[9]. To obtain an ROC curve the fraction of false pos-\nitives (object is not novel but it is rejected, in our case\ntestreject)isplottedversusthefractionoftruepos-\nitives (object is novel and correctly rejected, in our case\nnovel reject). An ROC curve shows the tradeoff be-\ntween how sensitive and how speciﬁc a method is. Any\nincrease in sensitivity will be accompanied by a decrease\nin speciﬁcity. If a method becomes more sensitive to-\nwards novel objects it will reject more of them but at thesame it will also become less speciﬁc and also falsely re-\nject more non-novel objects. Consequently, the closer a\ncurve follows the left-hand border and then the top bor-\nder of the ROC space, the more accurate the method is.\nThe closer the curve comes to the 45-degree diagonal of\nthe ROC space, the less accurate the method. We plot the\nmeantestreject versus the mean novel reject\nfor falling numbers of s(Ratio-reject) and growing num-\nbers of k(Knn-reject). In addition the mean accuracy\nfor each of the different values of sandkare depicted as\nseparatecurves. Allmeansarecomputedacrossall 22×10\ncorrespondingvalues. Theaccuracywithoutanyrejection\ndue to novelty detection is 70%.\n020406080100020406080100\ntest_rejectnovel_reject\ns=3s=2s=1s=0\nFigure 1: Ratio-reject ROC, mean testreject vs.\nnovel reject (circles,solidline)and accuracy (di-\namonds, broken line) for ’no rejection’, s=5,3,2,1,0.\nRatio-reject : The results for novelty detection based\nontheRatio-rejectrulearegiveninFig.1. Withtheprob-\nability threshold for rejection set to s= 2(rejection be-\ncausedataislessprobablethan 5%),theaccuracyrisesup\nto79%while 19%of thetestsongs are falsely rejected\nas being novel and therefore not classiﬁed at all and 42%\nof thenewsongs are being rejected correctly. If one is\nwilling to lower the threshold to s= 0(rejection because\ndata is less probable than 50%) the accuracy is at 92%\nwithalready 49%ofthetestsongsrejectederroneously\nand84%of thenewsongs rejected correctly.\nKnn-reject : The results for novelty detection based\non the Knn-reject rule are given in Fig. 2. If kis set to\n2 the accuracy rises up to 89%while 35%of thetest\nsongs are wrongly rejected as being novel and therefore\nnot classiﬁed at all and 65%of thenewsongs are being\nrejected correctly. With k= 3the accuracy values start\nto saturate at 95%with already 49%of thetestsongs\nrejected erroneously and 81%of thenewsongs rejected\ncorrectly.\n5 DISCUSSION\nWe have presented two approaches to novelty detection,\nwhere the ﬁrst (Ratio-reject) is based directly on the dis-\ntance matrix and does not, contrary to Knn-reject, need\nthe genre labels. When comparing the two ROC curves\ngiven inFigs.1and 2itcanbeseenthatbothapproaches\nwork approximately equally well. E.g. the performance\n26220406080100020406080100\ntest_rejectnovel_reject\nk=1k=2k=3\nFigure 2: Knn-reject ROC, mean testreject vs.\nnovel reject (circles, solid line) and accuracy\n(diamonds, broken line) for k=1 (no rejection) and\nk=2,3,4,5,6,7,8,9,10,20.\nof the Ratio-reject rule with s= 1resembles that of the\nKnn-rejectrulewith k= 2. Thesameholdsfor s= 0and\nk= 3. Also the increase in accuracy is comparable for\nboth methods. Depending on how much speciﬁcity one\nis willing to sacriﬁce, the accuracy can be increased from\n70%towellabove 90%. LookingatbothROCcurves,we\nwould like to state that they indicate quite fair accurate-\nness of both novelty detection methods.\nWhen judging genre classiﬁcation results, it is impor-\ntanttorememberthatthehumanerrorinclassifyingsome\nof the songs gives rise to a certain percentage of misclas-\nsiﬁcationalready. Inter-raterreliabilitybetweenanumb er\nofmusicexpertsisusuallyfarfromperfectforgenreclas-\nsiﬁcation. Given that the genres for our data set are user\nand not expert deﬁned and therefore even more problem-\natic (see Sec. 2), it is not surprising that there is a consid-\nerable decrease in speciﬁcity for both methods.\nOf course there is still room for improvement in nov-\nelty detection for music similarity. The two presented\nmethodsareaﬁrstattempttotackletheproblemandcould\nprobably be improved themselves. One could change the\nKnn-reject rule given in Equ. 6 by introducing a weight-\ning scheme which puts more emphasis on closer than on\ndistant neighbors. Then there is a whole range of al-\nternative methods which could be explored: probabilis-\ntic approaches (see e.g. [10]), Bayesian methods [11]\nand neural network based techniques (see [12] for an\noverview).\nFinally we would like to comment that whereas the\nKnn-rejectruleisboundtothegenreclassiﬁcationframe-\nwork, Ratio-reject is not. Knn-reject probably is the\nmethodofchoiceifclassiﬁcationisthemaininterest. Any\nalgorithm that is able to ﬁnd a range of nearest neighbors\ninadatabaseofsongscanbeusedtogetherwiththeKnn-\nreject rule. Ratio-reject on the other hand has an even\nwider applicability. It is a general method to detect novel\nsongsgivenasimilaritymatrixofsongs. Sinceitdoesnot\nneedgenreinformationitcouldbeusedforanythingfrom\nplay list generation and music recommendation to music\norganization and visualization.6 CONCLUSION\nWeintroducednoveltydetection,i.e.theautomaticidenti -\nﬁcation of new or unknown data not covered by the train-\ning data, to the ﬁeld of music information retrieval. We\npresentedtwodifferentmethodsfornoveltydetectionwith\nthe ﬁrst relying solely on the similarity information and\nthe second also utilizing genre label information. Both\nhave been shown to perform equally well in terms of sen-\nsitivity, speciﬁcity and accuracy within a genre classiﬁca -\ntion context. We also discussed the potential of novelty\ndetection to improve a wide range of music information\nretrieval applications.\nACKNOWLEDGEMENTS\nParts of the MA Toolbox [13] and the Netlab Toolbox2have\nbeen used for this work. This research was supported by the\nEU project FP6-507142 SIMAC3. The Austrian Research Insti-\ntute for Artiﬁcial Intelligence is supported by the Austrian Fed-\neralMinistryofEducation,ScienceandCultureandtheAustrian\nFederal MinistryforTransport, Innovation and Technology.\nREFERENCES\n[1] Markou M., Singh S.: Novelty detection: a review-part\n1: statistical approaches, Signal Processing, 83(12):2481-\n2497, 2003.\n[2] Aucouturier,J.-J.,PachetF.: ImprovingTimbreSimilarity:\nHowhighisthesky? JournalofNegativeResultsinSpeech\nand Audio Sciences, 1(1), 2004.\n[3] Logan B.: Mel Frequency Cepstral Coefﬁcients for Music\nModeling,Proc.oftheInternationalSymposiumonMusic\nInformation Retrieval (ISMIR’00),2000.\n[4] Logan B., Salomon A.: A music similarity function based\non signal analysis, IEEE International Conf. on Multime-\ndia and Expo, Tokio, Japan, 2001.\n[5] Aucouturier J.-J., Pachet F.: Music Similarity Measures:\nWhat’stheUse?,inProc.ofthe3rdIntern.Conf.onMusic\nInformation Retrieval (ISMIR’02),pp. 157-163, 2002.\n[6] Bishop C.M.: Neural Networks for Pattern Recognition,\nClarendon Press, Oxford, 1995.\n[7] TaxD.M.J.,DuinR.P.W.: Outlierdetectionusingclassiﬁer\ninstability, in Amin A. et al. (eds.), Advances in Pattern\nRecognition,Proc.JontIAPRInt.WorkshopSSPR’98and\nSPR’98,LectureNotesinComputerScience,Springer,pp.\n593-601, 1998.\n[8] Hellman M.E.: The nearest neighbour classiﬁcation with\na reject option, IEEE Transaction on Systems Science and\nCybernetics, Vol. 6, No. 3, pp.179-185, 1970.\n[9] Metz C.E.: Basic principles of ROC analysis, Semin Nucl\nMed, 8(4):283-98, 1978.\n[10] Bishop C.: Novelty detection and neural network valida-\ntion,ProceedingsoftheIEEConferenceonVisionandIm-\nage Signal Processing, pp.217-222, 1994.\n[11] MacKay D.J.C.: The evidence framework applied to clas-\nsiﬁcationnetworks,NeuralComputation,4:720-736,1992.\n[12] Markou M., Singh S.: Novelty detection: a review-part\n1: neural network based approaches, Signal Processing,\n83(12):2499 -2521, 2003.\n[13] Pampalk E.: A Matlab Toolbox to compute music simi-\nlarity from audio, in Proceedings of the 5th International\nConference on Music Information Retrieval (ISMIR’04),\nBarcelona, Spain, pp.254-257,2004.\n2http://www.ncrg.aston.ac.uk/netlab\n3http://www.semanticaudio.org\n263"
    },
    {
        "title": "Singer Identification Based on Accompaniment Sound Reduction and Reliable Frame Selection.",
        "author": [
            "Hiromasa Fujihara",
            "Tetsuro Kitahara",
            "Masataka Goto",
            "Kazunori Komatani",
            "Tetsuya Ogata",
            "Hiroshi G. Okuno"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418285",
        "url": "https://doi.org/10.5281/zenodo.1418285",
        "ee": "https://zenodo.org/records/1418285/files/FujiharaKGKOO05.pdf",
        "abstract": "This paper describes a method for automatic singer identification from polyphonic musical audio signals including sounds of various instruments. Because singing voices play an important role in musical pieces with a vocal part, the identification of singer names is useful for music information retrieval systems. The main problem in automatically identifying singers is the negative influences caused by accompaniment sounds. To solve this problem, we developed two methods, accompaniment sound reduction and reliable frame selection. The former method makes it possible to identify the singer of a singing voice after reducing accompaniment sounds. It first extracts harmonic components of the predominant melody from sound mixtures and then resynthesizes the melody by using a sinusoidal model driven by those components. The latter method then judges whether each frame of the obtained melody is reliable (i.e. little influenced by accompaniment sound) or not by using two Gaussian mixture models for vocal and non-vocal frames. It enables the singer identification using only reliable vocal portions of musical pieces. Experimental results with forty popular-music songs by ten singers showed that our method was able to reduce the influences of accompaniment sounds and achieved an accuracy of 95%, while the accuracy for a conventional method was 53%. Keywords: Singer identification, artist identification, melody extraction, singing detection, similarity-based MIR 1",
        "zenodo_id": 1418285,
        "dblp_key": "conf/ismir/FujiharaKGKOO05",
        "keywords": [
            "automatic singer identification",
            "polyphonic musical audio signals",
            "singing voices",
            "music information retrieval systems",
            "accompaniment sounds",
            "reliable frame selection",
            "melody extraction",
            "singing detection",
            "accuracy of 95%",
            "conventional method"
        ],
        "content": "SINGER IDENTIFICATION BASED ON ACCOMPANIMENT SOUND\nREDUCTION AND RELIABLE FRAME SELECTION\nHiromasa Fujihara,†Tetsuro Kitahara,†Masataka Goto,‡\nKazunori Komatani,†Tetsuya Ogata,†and Hiroshi G. Okuno†\n†Dept. of Intelligence Science and Technology‡National Institute of Advanced Industrial\nGraduate School of Informatics, Kyoto University Science and Technology (AIST)\nSakyo-ku, Kyoto 606-8501, Japan Tsukuba, Ibaraki 305-8568, Japan\n{fujihara, kitahara, komatani, ogata, okuno }@kuis.kyoto-u.ac.jp m.goto@aist.go.jp\nABSTRACT\nThis paper describes a method for automatic singer iden-\ntiﬁcation from polyphonic musical audio signals includ-\ning sounds of various instruments. Because singing voices\nplay an important role in musical pieces with a vocal part,the identiﬁcation of singer names is useful for music infor-mation retrieval systems. The main problem in automati-cally identifying singers is the negative inﬂuences caused\nby accompaniment sounds. To solve this problem, we\ndeveloped two methods, accompaniment sound reduction\nandreliable frame selection . The former method makes it\npossible to identify the singer of a singing voice after re-ducing accompaniment sounds. It ﬁrst extracts harmoniccomponents of the predominant melody from sound mix-tures and then resynthesizes the melody by using a si-nusoidal model driven by those components. The lattermethod then judges whether each frame of the obtainedmelody is reliable (i.e. little inﬂuenced by accompanimentsound) or not by using two Gaussian mixture models forvocal and non-vocal frames. It enables the singer identiﬁ-cation using only reliable vocal portions of musical pieces.Experimental results with forty popular-music songs byten singers showed that our method was able to reducethe inﬂuences of accompaniment sounds and achieved anaccuracy of 95%, while the accuracy for a conventionalmethod was 53%.\nKeywords: Singer identiﬁcation, artist identiﬁcation,\nmelody extraction, singing detection, similarity-based\nMIR\n1 INTRODUCTION\nSinging voice is known as the oldest musical instrument\nthat everyone has by nature and plays an important role inmany musical genres, especially in popular music. When\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-mercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londona song is heard, for example, most people use the vocal\npart by the lead singer’s voice as a primary cue for rec-ognizing (identifying) the song (name). Therefore, most\nmusic stores classify music according to singers’ names\n(often referred to as artist names) in addition to musical\ngenres.\nAs known from the above importance of the singing\nvoice, the description of singer names of songs is use-\nful for music information retrieval (MIR). When a userwants to ﬁnd songs sung by a certain singer, an MIR sys-tem can use the description of singer names (artist names).Furthermore, detailed descriptions of acoustical charac-teristics of singing voices can also play an important rolein MIR because they are useful for computing acoustical\nsimilarities between singers. Most previous MIR systems,\nhowever, assumed that the metadata including artist namesand song titles were available: if they were not availablefor some songs, those songs cannot be retrieved by sub-mitting a query of their artist names.\nTo achieve such singing-voice-based MIR and com-\npute artist similarities without requiring the metadata forevery song to be prepared, in this paper, we focus onthe problem of identifying singers for songs, automatic\nsinger identiﬁcation problem. This problem is difﬁcult\nbecause most singing voices are accompanied by othermusical instruments. It is therefore necessary to focuson the vocal part in polyphonic sound mixtures whileconsidering the negative inﬂuences from accompaniment\nsounds. In other words, feature vectors extracted from\nmusical audio signals are inﬂuenced by the sounds ofaccompanying instruments. Although speaker identiﬁ-cation problem for (non-music) speech signals has been\ndealt with by many studies in the ﬁeld of speech infor-\nmation processing, their results cannot be directly appliedto the singer identiﬁcation problem for singing voiceswith accompaniments because most existing speaker-\nidentiﬁcation techniques assume speech signals presented\nwithout other simultaneous sounds. On the other hand,Tsai et al. [1, 2] have pointed out this problem and have\ntried to solve it by using a statistically-based speaker-identiﬁcation method for speech signals in noisy environ-\nments [3]. On the assumption that singing voices and ac-\ncompaniment sounds are statistically independent, theyﬁrst estimated an accompaniment-only model from in-terlude sections and a vocal-plus-accompaniment model\n329from whole songs, and the estimated a vocal-only model\nby subtracting the accompaniment-only model from thevocal-plus-accompaniment model. However, this assump-\ntion is not always satisﬁed and the way of estimating\nthe accompaniment-only model has a problem: accom-\npaniments during vocal sections and performances (ac-companiments) during interlude sections can have dif-\nferent acoustical characteristics. In other previous stud-ies [4, 5, 6, 7], the accompaniment sound problem has notexplicitly been dealt with.\nTo solve this problem, we propose two methods: ac-\ncompaniment sound reduction andreliable frame selec-\ntion. Using the former method, we reduce the inﬂuence of\naccompaniment. We ﬁrst extracted the harmonic structure\nof the melody from audio signals, and then, resynthesize it\nusing a sinusoidal model. This method reduces the inﬂu-ence of accompaniment sounds. The latter method selectsframes that are reliable enough for classiﬁcation.\nThe rest of this paper is organized as follows. In the\nnext section, we describe our method for a singer identiﬁ-cation task. In Section 3, we describe the implementationof our system. In Section 4, we describe our experimentsand present the results. In Section 5, we draw conclusionsand point out future directions.\n2 SINGER IDENTIFICATION ROBUST\nTO ACCOMPANIMENT SOUNDS\nThis paper describes an automatic singer identiﬁcation\nsystem, which is the system for determining a singer’s\nname of given musical audio signals. The target data\nare real-world musical audio signals such as popular mu-sic CD recordings that contain singing voices of a singlesinger and accompaniment sounds.\nThe main difﬁculty in achieving automatic singer\nidentiﬁcation lies in the negative inﬂuences of accompa-\nniment sounds. Since a singing voice usually exists withaccompaniment sounds at the same time, acoustical fea-tures that are extracted from such a singing voice will bedependent on the accompaniment sounds. When featuresthat are commonly used in speaker identiﬁcation studies,such as cepstral coefﬁcients or linear prediction coefﬁ-cients (LPC), are extracted, in fact, those to be obtainedfrom musical audio signals will represent not solely thesinging voice but a mixture of the singing voice and theaccompaniment sounds. To achieve accurate singer iden-tiﬁcation, therefore, it is indispensable to cope with thisaccompaniment sound problem.\nOne possible solution to this problem may be to use\ndata inﬂuenced by accompaniment sounds for both train-ing and identiﬁcation. In fact, most of the previous stud-ies [4, 5, 6, 7] adopted this approach. However, it oftenfails because accompaniment sounds usually have differ-ent acoustical features from song to song. For example,the acoustical similarity for two musical pieces, the ac-companiments of which are on a piano solo and a fullband, respectively, will not become high enough, even ifthey are sung by the same singer.\nTo solve the problem, we developed two methods: ac-\nHarmonic Structure Extraction\nResynthesisAccompaniment Sound \nReduction\nOutput (Singer’s name)F0 Estimation\nVocal \nGMM\nNon-Vocal \nGMMReliable Frame \nSelectionFeature Extraction\nfor each frame\nlikelihood> η\nYesNo\nReject Accept\nSinger DeterminationthresholdInput (Audio signals)\nFigure 1: Method overview\ncompaniment sound reduction and reliable frame selec-\ntion. Figure 1 shows an overview of our methods.\n2.1 Accompaniment Sound Reduction\nOne of the best solutions to the accompaniment sound\ninﬂuence is to reduce the accompaniment sounds froma given audio signal. In order to achieve this, we usea melody resynthesis technique based on the harmonicstructure that consists of the following three parts:\n1. Estimating the fundamental frequency (F0) of the\nmelody using Goto’s PreFEst [8].\n2. Extracting the harmonic structure corresponding to\nthe melody.\n3. Resynthesizing the audio signal (waveform) corre-\nsponding to the melody using a sinusoidal synthesis.\nThus, we obtain the waveform corresponding only to themelody. Note that the melody’s waveform obtained withthis method contains instrument ( i.e., non-vocal) sounds\nin interlude sections as well as voices in singing sections,because the melody is just deﬁned as the most predomi-nant note in each frame [8]. It may therefore be considerednecessary to detect singing sections. In practice, however,\n330we can omit this detection, which is a difﬁcult problem,\nby using reliable frame selection described below.\n2.2 Reliable Frame Selection\nAnother solution to the accompaniment sound inﬂuence\nis to select frames that are less inﬂuenced by the accom-paniment sounds and to use only them for identiﬁcation.\nWe call this approach reliable frame selection. In orderto achieve this, we introduce two kinds of Gaussian mix-ture models (GMMs), a vocal GMM\nλVand a non-vocal\nGMM λN. The vocal GMM λVis trained on feature vec-\ntors extracted from singing sections, and the non-vocal\nGMM λNis trained on those extracted from interlude sec-\ntions. Given a feature vector x, the likelihoods for the\ntwo GMMs, p(x|λV)andp(x|λN), represent how the fea-\nture vector xis like a vocal or a (non-vocal) instrument,\nrespectively. If the feature vector xis less inﬂuenced by\naccompaniment sounds ( i.e., more reliable), p(x|λV)will\nbe higher and p(x|λN)will be lower. We therefore deter-\nmine whether the feature vector xis reliable or not based\non the following equation:\nlogp(x|λV)−logp(x|λN)reliable\n≥\n<\nnot-reliableη, (1)\nwhere ηis a threshold. In our experiments, we use 64-\nmixture GMMs. It is difﬁcult to decide a universal thresh-\nold for a variety of songs because we cannot select enough\nfeature vectors for classiﬁcation from a song which havefew reliable frames. We therefore determine the thresh-old dependent on songs so that\nα% of the whole frames\nin the song are selected as reliable frames. Note that most\nof the non-vocal frames are rejected in this selection step.\nThis means that we can avoid detecting singing sectionsby using this reliable frame selection.\n3 IMPLEMENTATION\nIn this section, we describe the implementation of our sys-\ntem. As described above, our system consists of the fol-\nlowing four phases: accompaniment sound reduction, fea-ture extraction, reliable frame selection and classiﬁcation.\n3.1 Pre-Processing\nGiven an audio signal, it is monauralized and down-\nsampled to 16 kHz. Then, the spectrogram is calculatedusing the short-time Fourier transform shifted by 10.0 ms(160 points) with a 2048-point (128.0 ms) Hamming win-dow.\n3.2 Accompaniment Sound Reduction\nUsing the method described in Section 2.1, we reduce ac-\ncompaniment sounds as follows:\n3.2.1 F0 Estimation\nWe use Goto’s PreFEst [8] for estimating the F0s of the\nmelody. PreFEst estimates the most predominant F0in frequency-range-limited sound mixtures. Since themelody line tends to have the most predominant harmonicstructure in middle- and high-frequency regions, we canestimate the F0s of the melody by applying PreFEst with\nreliable frequency-range limitation.\nWe will describe a summary of PreFEst below. Here-\nafter, xis the log-scale frequency denoted in units of\ncents (a musical-interval measurement), and (t)means\ntime. Given the power spectrum Ψ\n(t)\np(x), we ﬁrst apply\na band-pass ﬁlter (BPF) that is designed so that it cov-\ners most of the dominant harmonics of typical melody\nlines. The ﬁltered frequency components can be repre-\nsented as BPF(x)Ψ(t)\np(x), where BPF(x)is the BPF’s fre-\nquency response for the melody line. To enable the ap-\nplication of statistical methods, we represent each of thebandpass-ﬁltered frequency components as a probability\ndensity function (PDF), called an observed PDF, p(t)\nΨ(x):\np(t)\nΨ(x)=BPF(x)Ψ(t)\np(x)\n/integraltext∞\n−∞BPF(x)Ψ(t)\np(x)dx. (2)\nThen, we consider each observed PDF to have been gen-\nerated from a weighted-mixture model of the tone models\nof all the possible F0s, which is represented as follows:\np(x|θ(t))=/integraldisplayFh\nFlw(t)(F)p(x|F)dF (3)\nθ(t)={w(t)(F)|Fl≤F≤Fh}, (4)\nwhere p(x|F)is the PDF of the tone model for each F0,\nand Fh and Fl is deﬁned as lower and upper limits of the\npossible (allowable) F0 range, and w(t)(F)is the weight\nof a tone model that satisﬁes\n/integraldisplayFli\nFhiw(t)(F)dF=1. (5)\nTone model represents a typical harmonic structrue and in-\ndicates where the harmonics of the F0 tend to occur. Then,\nwe estimate w(t)(F)using EM algorithm and regard it as\nthe F0’s PDF. Finally, we obtain the most dominant F0s\nF(t)by the following equation:\nF(t)=argmax\nFw(t)(F) (6)\n3.2.2 Harmonic Structure Extraction\nBased on the estimated F0, we extract the power and the\nphase of fundamental frequency component and harmoniccomponents. For each component, we allow |r|cent error\nand extract the peak in the allowed area. The power A\nl, the\nphase θland frequency Floflth overtone (l=1,..., 20)\ncan be represented as\nFl=argmax\nF|S(F)|\n(lF·(1−2r\n1200)≤F≤lF·(1+2r\n1200)),(7)\nAl=|S(Fl)|, (8)\nθl=argS(Fl), (9)\n331 0  2000  4000  6000  8000linear power\nFrequency [Hz]\n(a) An original spectrum and its envelope.\n 0  2000  4000  6000  8000linear power\nFrequency [Hz]\n(b) An extracted spectrum and its envelope.\nFigure 2: Example of F0 estimation and harmonic struc-\nture extraction. Envelopes of the spectrums are calculated\nusing linear prediction (LP) analyses.\nwhere S(F)denotes spectrum, Fdenotes F0 estimated by\nthe PreFEst. In our experiments, we set rto 20.\nFigure 2 shows an example of F0 estimation and har-\nmonic structure extraction. Figure 2 (a) shows an original\nspectrum and its envelope and Figure 2 (b) shows an ex-tracted spectrum and its envelope. As seen in the ﬁgures,a spectral envelope of extracted spectrum precisely rep-\nresents formants of singing voice, compared with that of\noriginal spectrum.\n3.2.3 Resynthesis\nWe resynthesize the audio signals of the melody from\nthe extracted harmonic structure by using a sinusoidalmodel [9]. Resynthesized audio signals are expressed as\ns(t)=\nL\n∑\nl=1Alcos(ωlt+θl), (10)\nwhere Al,θl,Flrepresent the power, the phase and the\nfrequency of the lth overtone and tis time.\n3.3 Feature Extraction\nWe calculate feature vectors from the resynthesized audio\nsignals. It is known that the individual characteristics of\nspeech signals are expressed in their spectral envelopes.In the ﬁeld of speech recognition studies, in fact, variousmethods have been proposed [10] for calculating featurevectors concerning spectral envelopes. Here, we comparesome of them, which are commonly used in speech recog-nition studies.\n3.3.1 Mel-frequency Cepstral Coefﬁcients (MFCC)\nMFCCs [11, 12] are cepstral coefﬁcients calculated on a\nmel-frequency scale. Cepstral analysis is the method toseparate envelope of spectrum from ﬁne structure. Inorder to compute cepstral coefﬁcients [10], we take thelog-magnitude descrete cosine transform (DCT) from thepower spectrum. The envelops are represented in lowerorder of the cepstral coefﬁcients, while the ﬁne structures\nare in higher order. Mel-frequency is a logarithmic fre-\nquency scale ﬁtted to the characteristics of the human au-ditory sense. For the MFCC computations, mel-ﬁlterbankanalysis is applied ﬁrst. Then, we obtain the MFCC\nfrom the log-magnitude DCT. In this paper, we use 15\ndimensional MFCC, caluculated via 20 dimensional mel-ﬁlterbank analysis.\n3.3.2 Linear Prediction Coefﬁcients (LPC)\nLinear prediction (LP) analysis [13, 14] is a method for\nestimating the transfer function of vocal tract, assumingthat input audio signal contains only human voice. In theLP model, given a signal s(n), we predict the signal as a\nlinear combination of its previous samples. The predictedvalue s\nW(n)is given by\nsW(n)=p\n∑\ni=1αisW(n−i)+g(n), (11)\nwhere prepresents the order of the predictor, αis are de-\nﬁned as the linear prediction coefﬁcients (LPC), and g(n)\nrepresents the error in the model. The LPCs are deter-\nmined by minimizing the mean squared prediction error\nofg(n). We use 20th-order LPC in this paper.\n3.3.3 LP-derived Cepstral Coefﬁcients (LPCC)\nLPCCs [13] are cepstral coefﬁcients of a LPC spectrum.\nCepstral analysis on the LPC spectrum plays a role of or-thogonalization and is known to be effective in patternrecognition. The LPCCs c(n)is directly obtained from\nthe LPC with the following equation:\nc(n)=⎧\n⎪⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎪⎩log\nσ2(n=0)\nαn+n−1\n∑\nk=1/parenleftbigg\n1−k\nn/parenrightbigg\nc(k)αn−k(1≤n≤p)\nn−1\n∑\nk=1/parenleftbigg\n1−k\nn/parenrightbigg\nc(k)αn−k (n>p),\n(12)\nwhere σ2represents the power of the signal, αnrepresent\nthe LPCs, and prepresents an order of the LPC. We set\nthe order of the LPCC to 15 in this paper.\n3.3.4 Linear Prediction Mel Cepstral Coefﬁcients\n(LPMCC)\nLPMCCs are mel-cepstral coefﬁcients of LPC spectrum.\nIn addition to the role of orthogonalization, the LPMCCs\nare superior to the LPC in terms of suitability to the humanauditory sense, which is a beneﬁt of the mel-frequencyscale. We derive the LPMCC by computing the MFCCfrom the LPC spectrum because of simplicity of imple-mentation. We set the order of the LPMCC to 15 in thispaper.\n332Table 1: Training data for reliable frame selection.\nName Gender Piece Number\nShingo Katsuta M 027\nYoshinori Hatae M 037\nMasaki Kuehara M 032, 078\nHiroshi Sekiya M 048, 049, 051\nKatsuyuki Ozawa M 015, 041\nMasashi Hashimoto M 056, 057\nSatoshi Kumasaka M 047\nOriken M 006\nKonbu F 013\nEri Ichikawa F 020\nTomoko Nitta F 026\nKaburagi Akiko F 055\nYuzu Iijima F 060\nReiko Sato F 063\nTamako Matsuzaka F 070\nDonna Burke F 081, 089, 091, 093, 097\nTable 2: Songs used for evaluation. The numbers written\nin the table are piece numbers of RWC-MDB-P-2001.\nName Gender D1 D2 D3 D4\naKazuo Nishi M 012 029 036 043\nbHisayoshi Kazato M 004 011 019 024\ncKousuke Morimoto M 038 039 042 044\ndShinya Iguchi M 082 084 088 090\neJeff Manning M 085 087 095 098\nfHiromi Yoshii F 002 017 069 075\ngTomomi Ogata F 007 028 052 080\nhRin F 014 021 050 053\niMakiko Hattori F 065 067 068 077\njBetty F 086 092 094 096\n3.4 Reliable Frame Selection\nWe select frames that are reliable and inﬂuenced a little by\naccompaniment sounds based on the method described in\nSection 2.2.\n3.5 Singer Determination\nThe name of the singer is determined based on 64-mixture\nGMMs. Let X={xt|t=1,..., T}be a time series of fea-\nture vectors selected in the reliable frame selection phase,\nandλsbe the GMM for the singer s. Then, the name of\nthe singer is determined through the following equation:\ns=argmax\ni1\nTT\n∑\nt=1logp(xt|λi). (13)\n4 EXPERIMENTS\nIn this section, we describe the experiments that were con-ducted to evaluate our system.\n4.1 Effectiveness of the whole system\nTo conﬁrm the effectiveness of our methods, accompa-\nniment sound reduction and reliable frame selection, weconducted experiments on singer identiﬁcation under the\nfollowing four conditions:Table 3: Experimental results that show effectiveness of\nthe whole system, where “reduc.” and “selec.” mean ac-companiment sound reduction and reliable frame selec-tion, respectively.\n(i) (ii) (iii) (iv)\nbaseline reduc. only selec. only ours\na 1/4 2/4 2/4 4/4\nb 3/4 1/4 3/4 4/4\nc 2/4 2/4 3/4 4/4\nd 4/4 4/4 4/4 4/4\ne 1/4 0/4 0/4 4/4\nf 1/4 2/4 2/4 3/4\ng 0/4 2/4 0/4 3/4\nh 4/4 4/4 4/4 4/4\ni 4/4 4/4 3/4 4/4\nj 1/4 3/4 2/4 4/4\nTotal 53% 60% 58% 95%\n(i)without both the reduction and the selection (baseline),\n(ii)without the reduction, with the selection,\n(iii) with the reduction, without the selection, and\n(iv) with both the reduction and the selection (ours).\nWe used forty songs by ten different singers (ﬁve were\nmale and ﬁve were female), listed in Table 2, taken from\n“RWC Music Database: Popular” [15]. Using these data,we conducted the 4-fold cross validation, that is, we ﬁrstdivided the whole data into four groups, D\ni(i=1,2,3,4)\nin Table 2, and then repeated the following step four times:\neach time, we left out one of the four groups for training\nand used the omitted one for testing. As the training datafor the reliable frame selection, we used twenty-ﬁve songsof sixteen different singers listed in Table 1, also takenfrom “RWC Music Data: Popular”, which differ from the\nsingers used for evaluation. We set\nαto 15%, in reference\nto the experiment described in Section 4.2. As a feature\nvector, in response to the experiment described in Section\n4.3, we use the LPMCC with the reduction and the MFCC\nwithout. We adopt the MFCC for the experiment with-\nout the reduction, because, as described in Section 3.3.2,\nthe LPMCC that is based on LPC can be applied only to\nhuman voice.\nTable 3 shows results of the experiments. As seen\nin the table, accompaniment sound reduction and reliableframes selection improved the accuracy of singer identi-ﬁcation. When these two methods were used together, inparticular, the accuracy was signiﬁcantly improved: from53% to 95%.\nFigure 3 shows confusion matrices of the experiments.\nAs seen in the ﬁgure, confusions between male and femaledecreased by using the reduction method. It means that, inthe cases of (ii) and (iv), the reduction method reduced theinﬂuences of accompaniment sound, and the system couldcorrectly identify the genders. On the other hand, in thecases without the reduction method (Conditions (i) and(iii)), the inﬂuences of accompaniment sound preventedthe system from correctly identifying even the genders ofthe singers.\n333jihgfedcba\njihgfedcbajihgfedcba\njihgfedcbaclassification resultsinger’s labeljihgfedcba\njihgfedcbajihgfedcba\njihgfedcbaclassification resultsinger’s labeljihgfedcba\njihgfedcbajihgfedcba\njihgfedcbaclassification resultsinger’s labeljihgfedcba\njihgfedcbajihgfedcba\njihgfedcbasinger’s labelclassification result\n(i) baseline (ii) reduction only (iii) selection only (iv) both reduction and selection\nFigure 3: Confusion matrices. Center lines in each ﬁgure are boundaries between male and female. Note that confusion\nbetween male and female decreased by using the accompaniment sound reduction method.\n30405060708090100\n5 1 01 52 02 53 0\nα (%)Accuracy (%)\nreduction only\nreduction and selection\nFigure 4: Experimental results that show dependency of\naccuracy on α.α% of all frames was judged as reliable.\n4.2 Dependency of accuracy on α\nWe conducted experiments with setting αto various val-\nues to investigate the dependency of accuracies on α. Ex-\nperimental results shown in Figure 4 show that classiﬁca-\ntion accuracy was not affected by small changes of α.I t\nis also noticeable that a value of αthat gives the highest\naccuracy differed. The reason of this fact is the follow-\ning: accompaniment sound reduction method reduced the\ninﬂuences of accompaniment sounds and emphasized the\ndifferences between reliable and unreliable frames. Thus,if we raised\nαtoo much, the system selected many unreli-\nable frames and the system performance decreases. With-\nout the reduction method, on the other hand, reliability of\neach frame did not make much difference because of the\ninﬂuences of accompaniment sounds. Therefore, it wasproﬁtable to use many frames for classiﬁcation by setting\nαcomparatively higher. However, in this case, we could\nnot attain sufﬁcient classiﬁcation accuracy because of the\ninﬂuences of accompaniment sounds.\n4.3 Investigation of Accompaniment Sound\nInﬂuence and Comparison of Features\n4.3.1 Conditions\nThere are two purposes in the experiments here. The\nﬁrst one is to investigate the inﬂuence by accompanimentsounds. We investigated it by comparing our results toTable 4: Investigation of accompaniment sound inﬂuence\nand comparison of features. The feature name writtenin bold font is the one that gives the highest accuracy.\n“corr. F0s” and “est. F0s” mean using correct F0s and\nestimated F0s, respectively, and “reduc.” means accom-paniment sound reduction.\n(i) (ii) (iii) (iv) (v)\nUsing vocal-only Using mixed-down data\nw/o with with reduc. w/o\nreduc. reduc. corr. F0s est. F0s reduc.\nMFCC 98% 95% 78% 75% 75%\nLPC 83% 88% 50% 58% 48%\nLPCC 95% 98% 75 % 75% 63%\nLPMCC 98% 98% 88% 83% 68%\nones for vocal-only data. In addition, we compared esti-\nmating F0s of melodies and using given correct ones. Inour experiments, we virtually generated the correct F0s by\nestimating ones using vocal-only tracks. Although the es-\ntimates using vocal-only tracks were not completely cor-\nrect, its accuracy was sufﬁciently high comparing with\nestimating ones using mixed-down versions. The second\npurpose is to compare a variety of features. We used fourkinds of features described in Section 3.3 and comparedthese results. In the experiments here, we manually cut out60 seconds of singing regions for each song, because weomitted reliable frame selection method in order to inves-tigate the effectiveness of the only accompaniment soundreduction method.\n4.3.2 Results and Discussion\nTable 4 shows the results of the experiments. When we fo-\ncused on the differences between the cases (iv) and (v), theaccuracies for the LPC, the LPCC, and the LPMCC wereimproved by the reduction method, whereas that for theMFCC was not improved. This is because the LPC etc.assume that given signal contains only a single speech.For this assumption, the accuracies for the LPC etc. in thecase (v) were low because the inputs were a mixture ofsinging voices and accompaniment sounds. Because thecase (iv) dealt with signals obtained by extracting onlysinging voices, the accuracies were higher than the case(v). Because the MFCC is not based on such an assump-\n334tion, on the other hand, the accuracy for the MFCC in\nthe case (v) was high, but that in the case (iv) was same.Because the LPMCC models the sounding mechanism of\nhumans’ voices, it could be expected to achieve a high ac-\ncuracy if the assumption is satisﬁed. In fact, whereas the\naccuracy for the LPMCC was lower than that for MFCCin the case (v), that for the LPMCC was higher than that\nfor the MFCC in the case (iv). Whereas most previousstudies used the MFCC, we achieved to adopt more ro-bust features by reducing accompaniment sounds.\nWhen we compared the LPC, the LPCC, and the\nLPMCC, the accuracy for the LPCC was 17% higher thanthat for the LPC, and that for the LPMCC was 8% higherthan that for the LPCC. The reason why the accuracy for\nthe LPCC was higher than that for the LPC is that the\nLPCCs are orthogonal features unlike the LPC. The rea-son why the accuracy for the LPMCC was higher than thatfor the LPCC is mel-frequency cepstrum allow better sup-pression of insigniﬁcant spectral variation in the higherfrequency bands.\nThe accuracies for the cases (iii) and (iv) were com-\nparatively close. This was because partial misestimationof F0s was not critically connected to errors for singeridentiﬁcation since the names of singers were determinedbased on the mean of a time series of likelihoods.\nThe case (iv) in experiment described in Section 4.1\nwas superior to the case (iv) in this experiment, eventhough we manually fed singing regions into the system\nfor this experiment. This means that the selection method\nactually functioned not only as distinguishing vocal andnon-vocal frames but also as determining whether eachframe was reliable or not. The case (v) in this experiment,\nhowever, was inferior to the case (iii) in experiments de-\nscribed in 4.1. This result means that, to accurately selectreliable frames, it is indispensable to use both the reduc-tion method and the selection method together.\nTable 5 lists an excerpt of experimental results for\neach singer. As seen in the table, the reduction methodimproved accuracies particularly for the singer (g). Thiswas because the songs of the singer (g) have differentkinds of genres such as a piano ballad and R&B. These\nsongs are accompanied on different instruments and hence\nhave different acoustical characteristics of accompani-ment sounds. Whereas the system without the reductionmethod did not correctly identify the singer’s name forthese songs, that with our method did. This result shows\nthat the reduction method could reduce the inﬂuence of\nacoustical differences in accompaniment sounds. In con-trast, identiﬁcation errors for the singer (e) increased bythe reduction method. This is because the melodies’ F0s\nwere incorrectly estimated in some songs. We can also\nconﬁrm this by the fact that identiﬁcation errors did notincrease when we provided the correct F0s.\n5 CONCLUSION\nWe have described two methods that work in combination\nto automatically identify singers for music information re-trieval. To identify the singer names of musical pieces in-\ncluding sounds of various instruments, our method solvedTable 5: Accuracy for each singer, where “reduc.” means\naccompaniment sound reduction.\n(iii) (iv) (v)\nwith reduc., with reduc., w/o\ncorr. F0s est. F0s reduc.\nLPMCC LPMCC MFCC\na 3/4 3/4 3/4\nb 4/4 4/4 4/4\nc 4/4 4/4 3/4\nd 4/4 4/4 4/4\ne 3/4 2/4 3/4\nf 3/4 2/4 2/4\ng 2/4 2/4 0/4\nh 4/4 4/4 4/4\ni 4/4 4/4 4/4\nj 4/4 4/4 3/4\nTotal 88% 83% 75%\nthe problem of the accompaniment sound inﬂuences. In\nour experiments with forty songs by ten singers, we found\nthat our methods achieved identiﬁcation accuracy of 95%and conﬁrmed the robustness and effectiveness of thosemethods.\nThe main contributions of this paper can be summa-\nrized as follows:\n•We clariﬁed the problem of the accompaniment\nsound inﬂuence for singer identiﬁcation, which hasnot been dealt with except for only a few attempts,and provided two effective solutions, accompani-ment sound reduction and reliable frame selection.\n•The use of the accompaniment sound reductionmethod made it possible to reduce the negative in-ﬂuence of accompaniment sound by extracting andresynthesizing the harmonic structure of the predom-inant melody. Though similar methods have beenused to improve the noise robustness in the ﬁeld ofspeech recognition [16], this is the ﬁrst paper thatshows its effectiveness for singer identiﬁcation.\n•The reliable frame selection method made it possi-ble to select frames reliable enough for classiﬁca-tion. Although similar methods were used in pre-vious studies, they focused on distinguishing vocaland non-vocal frames; they did not consider the reli-ability of each frame. Note that our selection method\nrejects even unreliable vocal frames as well as non-\nvocal frames.\n•We showed an investigation of features for singeridentiﬁcation. While many features have been pro-posed in the ﬁeld of speech recognition [10], it hasnot been clear which feature was appropriate for\nsinger identiﬁcation. We compared various features\nin terms of the singer identiﬁcation, and found thatthe LPMCC was the most robust among them. Thisresult will contribute to the progression of singeridentiﬁcation research.\nIn the future, we plan to extend our method to cal-\nculate acoustical similarities between musical pieces in\n335terms of singers and apply it to music information retrieval\nbased on singing voice similarities.\nACKNOWLEDGEMENTS\nThis research was partially supported by the Ministryof Education, Culture, Sports, Science and Technol-\nogy (MEXT), Grant-in-Aid for Scientiﬁc Research (A),\nNo.15200015, and Informatics Research Center for De-velopment of Knowledge Society Infrastructure (COEprogram of MEXT, Japan). We thank everyone who hascontributed to building and distributing the RWC Mu-\nsic Database [15]. We also thank Kazuyoshi Yoshii and\nTakuya Yoshioka (Kyoto University) for their valuablediscussions.\nREFERENCES\n[1] Wei-Ho Tsai, Hsin-Min Wang, and Dwight Rodgers.\nAutomatic singer identiﬁcation of popular music\nrecordings via estimation and modeling of solo vo-\ncal signal. In Proceedings of European Confer-\nence on Speech Communication and Technology\n(Eurospeech2003) , 2003.\n[2] Wei-Ho Tsai and Hsin-Min Wang. Automatic de-\ntection and tracking of target singer in multi-singer\nmusic recordings. In Proceedings of the 2004 IEEE\nInternational Conference on Acoustics, Speech, andSignal Processing (ICASSP 2004) , pages 221–224,\n2004.\n[3] R. C. Rose, E. M. Hofstetter, and D. A. Reynolds.\nIntegrated models of signal and background with ap-plication to speaker identiﬁcation in noise. IEEE\nTransactions on Speech and Audio Processing ,\n2(2):245–257, 1994.\n[4] Brian Whitman, Gary Flake, and Steve Lawrence.\nArtist detection in music with Minnowmatch. InProceedings of the 2001 IEEE Workshop on Neu-ral Networks for Signal Processing , pages 559–568,\n2001.\n[5] Adam L. Berenzweig, Daniel P. W. Ellis, and Steve\nLawrence. Using voice segments to improve artistclassiﬁcation of music. In AES 22nd International\nConference on Virtual, Synthetic, and EntertainmentAudio , 2002.\n[6] Youngmoo Edmund Kim and Brian Whitman.\nSinger identiﬁcatin in popular music recordings us-ing voice coding features. In Proceedings of the 3rd\nInternational Conference on Music Information Re-\ntrieval (ISMIR2002) , pages 164–169, 2002.\n[7] Tong Zhang. Automatic singer identiﬁcation. In Pro-\nceedings of IEEE International Conference on Mul-timedia & Expo (ICME 2003) , 2003.\n[8] Masataka Goto. A real-time music-scene-\ndescription system: predominant-F0 estimationfor detecting melody and bass lines in real-world au-\ndio signals. Speech Communication , 43(4):311–329,\n2004.\n[9] James Anderson Moorer. Signal procesing aspects\nof computer music: A survey. Proceedings of the\nIEEE , 65(8):1108–1137, 1977.\n[10] Joseph Picone. Signal modeling techniques in\nspeech recognition. IEEE Proceedings , 81(9):1215–\n1247, 1993.\n[11] Steven B. Davis and Paul Mermelstein. Comparison\nof parametric representation for monosyllabic word\nrecognition. IEEE Transactions on Acoustic, Speech\nand Signal Processing , 28(4):357–366, 1980.\n[12] Beth Logan. Mel frequency cepstral coefﬁcients for\nmusic modelling. In Proceedings of the Interna-\ntional Symposium on Music Information Retrieval\n(ISMIR 2000) , pages 23–25, 2000.\n[13] Bishnu S. Atal. Effectiveness of linear prediction\ncharacteristics of the speech wave for automatic\nspeaker identiﬁcation and veriﬁcation. the Journal\nof the Acoustical Society of America , 55(6):1304–\n1312, 1974.\n[14] K. Shikano. Evaluation of LPC spectral matching\nmeasures for phonetic unit recognition. Technical\nReport CMU-CS-96-108, CMU, Computer ScienceDepartment, 1986.\n[15] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC music database:Popular, classical, and jazz music databases. In Pro-\nceedings of the 3rd International Conference on Mu-sic Information Retrieval (ISMIR 2002) , pages 287–\n288, October 2002.\n[16] Tomohiro Nakatani and Hiroshi G. Okuno. Har-\nmonic sound stream segregation using localizationand its application to speech stream segregation.Speech Communcations , 27:209–222, 1999.\n336"
    },
    {
        "title": "Pitch Track Target Deviation in Natural Singing.",
        "author": [
            "David Gerhard"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418115",
        "url": "https://doi.org/10.5281/zenodo.1418115",
        "ee": "https://zenodo.org/records/1418115/files/Gerhard05.pdf",
        "abstract": "Unlike fixed-pitch instruments such as the piano, human singing can stray from a target pitch by as much as a semitone while still being perceived as a single fixed note. This paper presents a study of the difference between target pitch and actualized pitch in natural singing. A set of 50 subjects singing the same melody and lyric is used to compare utterance styles. An algorithm for alignment of idealized template pitch tracks to measured frequency tracks is presented. Specific examples are discussed, and generalizations are made with respect to the types of deviations typical in human singing. Demographics, including the skill of the singer, are presented and discussed in the context of the pitch track deviation from the ideal. Keywords: Singing, melody alignment, ornamentation, pitch track, vibrato. 1",
        "zenodo_id": 1418115,
        "dblp_key": "conf/ismir/Gerhard05",
        "keywords": [
            "pitch deviation",
            "melody alignment",
            "ornamentation",
            "pitch track",
            "vibrato",
            "singing",
            "melody",
            "idealized template",
            "subject",
            "singer"
        ],
        "content": "PITCH TRACK TARGET DEVIATION IN NATURAL SINGING\nDavid Gerhard\nDepartment of Computer Science, Department of Music\nUniversity of Regina\nRegina, SK CANADA S4S 0A2\ndavid.gerhard@uregina.ca\nABSTRACT\nUnlike ﬁxed-pitch instruments such as the piano, human\nsinging can stray from a target pitch by as much as a\nsemitone while still being perceived as a single ﬁxed note.\nThis paper presents a study of the difference between tar-\nget pitch and actualized pitch in natural singing. A set\nof 50 subjects singing the same melody and lyric is used\nto compare utterance styles. An algorithm for alignment\nof idealized template pitch tracks to measured frequency\ntracks is presented. Speciﬁc examples are discussed, and\ngeneralizations are made with respect to the types of devi-\nations typical in human singing. Demographics, including\nthe skill of the singer, are presented and discussed in the\ncontext of the pitch track deviation from the ideal.\nKeywords: Singing, melody alignment, ornamentation,\npitch track, vibrato.\n1 INTRODUCTION\nMusical query systems are designed with the expectation\nthat the singer will know they are making a query, and\ntherefore consciously or subconsciously regularize their\nsinging, reducing the impact of ornamentation like vi-\nbrato andrubato which have the tendency to make melody\nrecognition very difﬁcult. This paper is concerned with\nso-called “natural” singing, where the singer is not specif-\nically attempting to develop a query. Even when these or-\nnamentations are so extreme that the target melody may be\nunrecognizable by automated methods, human perception\nis capable of regularizing the pitch and timing to identify\nthe melody.\nHuman perception of singing is very forgiving, con-\nsidering that the pitch track of an average non-expert\nsinger is far from the ideal sequence of pitches intended\nby the singer or heard buy the listener. Trained singers\nachieve target pitches much more rapidly and accurately\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonthan untrained singers, and the vibrato is more easily mod-\neled by an idealized sinusoid, however, the amplitude of\nsuch oscillation is greatly increased, sometimes to well\nbeyond a semitone. Rossignol et al. (1999) have shown\nhow to detect and model vibrato in the context of musical\npitch, but when the musician is not expert at controlling\nthe vibrato, the resulting waveform is not well-formed and\nwhile it still sounds vibrato-like, it no longer ﬁts with a\nreasonable vibrato model. Section 5.2 presents examples\nof utterances made by trained and untrained singers, with\nan examination of the pitch deviation for each.\nCurrent query-by-humming systems have several ad-\nvantages, perhaps subconscious, over the average human\nlistener in this regard. In the extreme, many systems are\ndesigned to operate on idealized pitch tracks, either by\nusing ﬁxed-pitch instruments or variable-pitched instru-\nments played by experts. Also, the users of these sys-\ntems often know that they are making a musical query,\nand so sometimes try to make their musical utterances\nas smooth, rhythmic and “correct” as possible. Amateur\nsingers, when just singing for fun or without intent, often\ngenerate less accurate pitch tracks, aiming high for a note\nand compensating later, or accidentally switching keys in\nthe middle of the song. Even with this “messy” input, as\nwith so many other cognitive tasks, the untrained listener\ncan easily and accurately identify familiar melodies.\nIt is the pitch track of the utterance which seems to\nhold the majority of melodic information (Weyde, 2004),\nand it is the human brain which seems to be able to ﬁlter\nout ornamentation, errors and state changes and “lock-in”\nto the intended or target set of pitches. This paper will ex-\namine some of these errors, and show some of the typical\ndeviations from the intended or target pitch sequence.\n2 A NATIONAL ANTHEM AND A\nRIVER CRUISE\nThe data used for this study was initially collected to study\nthe differences between speaking and singing (Gerhard,\n2002). 50 subjects were prompted to speak and sing var-\nious lyrics. The utterances thus extracted included inter-\nmediate vocalizations like poetry and rap music, as well\nas spoken phrases and sung lyrics. This study used two\npairs of utterances from each subject:\n1. Please sing the phrase “Row, row, row your boat,\ngently down the stream.”\n5142. Please speak the phrase “Row, row, row your boat,\ngently down the stream.”\n3. Please sing the phrase “O Canada, our home and na-\ntive land.”\n4. Please speak the phrase “O Canada, our home and\nnative land.”\nHaving each subject answer all of these prompts en-\nsures that all variables are controlled for except the differ-\nences between speaking and singing.\nThe subjects ranged in experience from self-confessed\nnovices with little or no musical background or pub-\nlic speaking experience, to professional radio voices and\ntrained opera singers. No pitches were given, and subjects\nsang in a number of keys. Some subjects sang in more\nthan one key, and some, unintentionally, sang in multiple\nkeys across a single melody. Most subjects were already\nfamiliar with the songs, and for those that were not famil-\niar, and example was sung for them. Most subjects sung\nthe “expected” tune, although one decided to “rap” our\nnational anthem. The samples were recorded with consis-\ntent equipment (speech-recognition microphone and dig-\nital recording software) at 44.1 kHz. This paper contains\nresults relating to Prompt 3, the ﬁrst line of the Canadian\nnational anthem, sung.\n3 FEATURE EXTRACTION\nThe signal processing analysis used for this study con-\ncentrated on the pitch track of the utterance. Other fea-\ntures, such mel-frequency cepstral coefﬁcients, were con-\nsidered, but the speciﬁc pitch value and change over time\nis of particular interest in this work, so a direct estimate of\nthe fundamental frequency of the signal was extracted and\ntracked over the course of the utterance. Not all human\nvocal utterance is periodic, however, so the segments of\nthe utterance with pitch (the voiced segments) must also\nbe identiﬁed.\n3.1 Frequency estimation\nThe YIN frequency estimator (de Cheveign ´e and Kawa-\nhara, 2002) was used to do the initial pitch extraction.\nSeveral readily available frequency estimation algorithms\nwere examined and evaluated including the algorithms\navailable in Colea (Loizou, 2003), and YIN was shown\nto ﬁt the purpose well. It responds well to human vocal\nsounds, and provides a measure of conﬁdence which can\nbe used as a detector of pitched segments. Each utterance\nwas analyzed using YIN and the conﬁdence measure was\nused to provide an initial segmentation.\n3.2 Segment detection: energy and zero-crossing\nrate\nThe measure of conﬁdence from the YIN pitch estima-\ntions was combined with a zero-crossing rate fricative de-\ntector and a thresholded RMS energy calculation to pro-\nduce the initial set of note boundaries. The zero-crossing\nrate is a good estimator of the spectral centroid, (Kedem,\n1986) and as such can be used to identify voiced segmentsof the utterance. The clip is divided into windows of 512\nsamples each, and the zero-crossing rate is measured for\neach window. If the zero-crossing rate is above a previ-\nously set threshold, the window was considered not to be\nvoiced, and the segment is split at that point.\nAs a further attempt to identify possible note bound-\naries, the overall energy of the signal was calculated at\neach frame (as in the zero-crossing rate) and when the\nenergy dropped below a pre-deﬁned threshold for a set\nperiod of time, the segment was split at that point. To\navoid noise at the threshold boundary, a hysteresis-like\nalgorithm was employed, with a pair of thresholds. The\nenergy would have to cross the lower threshold in the neg-\native direction to indicate the end of a segment, and cross\nthe higher threshold in the positive direction to indicate\nthe beginning of a new segment.\n3.3 Discussion\nOne difﬁculty with this procedure is that when notes\nchange without a vocal stop or fricative, the note bound-\nary is not identiﬁed. Other standard methods of detecting\nnote boundaries include pitch track discontinuities, spec-\ntral envelope discontinuities, and changes in ﬁlter-bank\nenergy levels. Unfortunately, none of these techniques are\nsuccessful all of the time with human singing. Succes-\nsive notes can produce identical features, especially if the\nsinger is singing a series of notes on a single syllable (as\nis the case when humming). Singers usually bend pitch\nfrom one note to the next rather than making a discrete\njump, especially if there is no breaking stop or fricative.\nIf the notes are far apart, a threshold can be set such that\nthe differential of the pitch track rising above this thresh-\nold indicates a note boundary. Singers with even moder-\nate levels of vibrato can easily exceed half a semitone, so\na threshold set high enough to avoid being triggered by\nvibrato may miss a valid semitone note transition.\n4 ALIGNMENT ALGORITHM\nBecause the target tune is known a priori in this case, the\nalignment algorithm simply ﬁnds the best match between\nthe extracted pitch track and the ideal, or target pitch track.\nThe sequence of steps in this alignment algorithm is:\n1. Identify note boundaries in the frequency estimate of\nthe utterance under consideration.\n2. Quantize to a single pitch for each segment.\n3. Convert absolute frequency estimates (Hz) to relative\nfrequency (cents).\n4. Align the segments to the target pitches of the known\nmelody\nBecause the target pitch sequence and rhythmic struc-\nture is known, a “best ﬁt” can be achieved. For this pro-\ncedure to be useful in a query-by-humming system, the\nﬁrst three steps are common to all matching tasks and\ncan therefore be performed once on the incoming signal.\nThis procedure works best, however, when the number of\npitched segments from the estimation and the target are\n515the same. The segmentation problem (breaking the signal\ninto pitched segments) is quite difﬁcult for natural human\nsinging. It should be noted at this point that the alignment\nalgorithm presented here was intended only to allow anal-\nysis of the deviation from target of human singing.\n4.1 Note boundary identiﬁcation\nAs indicated above, the ﬁrst estimate of the note bound-\naries is found using a combination of the conﬁdence mea-\nsure of YIN, the zero-crossing rate and the energy. This\nproduces reasonable results but occasionally leaves pitch\nsegments which should be separated into a series of notes.\nIf these segments are not separated, the pitch quantization\nwill be unsuccessful, since contributions from more than\none note will produce erroneous results.\nHaving a target melody gives the algorithm a target\nfor the number of pitch segments to expect. If the number\nof segments is signiﬁcantly smaller than that, some seg-\nmentation must be done. The slope of the pitch is used\nto identify the next reasonable segmentation site. A pair\nof parameters are used to ﬁnd this site: BoundaryLength\nand BoundaryThresh. The pitch slope must remain above\nBoundaryThresh for the duration BoundaryLength in or-\nder to identify a segmentation site. BoundaryLength is dy-\nnamically adjusted to account for different singing styles.\nThis method is quick and produces reasonable results\nfor study, but is not completely robust. SInging style inﬂu-\nences these results greatly, and as will be discussed in Sec-\ntion 5, singers tend to glissando or glide from one pitch to\nthe next, reducing the ability of the algorithm to ﬁnd a\nreasonable segmentation site. Frustratingly, this does not\nseem to affect human perception of the same melody—\npeople can recognize a melody whether or not the singer\nis gliding from one pitch to another or jumping as brieﬂy\nas possible . Lyrics help the recognition, but even without\nlyrics we humans can recognize a tune which deviates in\nsegment pitches as well as notes and rhythm.\nA procedure that has not been implemented in this sys-\ntem is re-combination. It would be useful to be able to join\ntwo segments which seem to be the same pitch or belong\nto the same note. the difﬁculty with this is that two seg-\nments with two similar pitches could equally be a single\nnote erroneously split or a repeated note. Note onset and\noffset characteristics have the potential to help solve this\nproblem.\n4.2 Pitch quantization\nOnce the pitch track has been split into the appropriate\nnumber of segments, each segment is assigned a pitch\nwhich represents the entire segment. There are a num-\nber of ways to assign the overall pitch of the segment, the\nsimplest of which is to calculate the mean pitch of the seg-\nment. With a segment containing idealized vibrato, the\nmean pitch will be at the center of the oscillation and cor-\nrespond well to the perceived pitch of the segment. Unfor-\ntunately, pitch track segments often depart from the ideal-\nized vibrato at the beginning or end of the segment, indi-\ncating a transition to another note. The median may be a\nmore appropriate measure in this case.4.3 Frequency conversion\nThe target melody is constructed in terms of the number\nof cents from the melodic root note of the key. In the\n“O Canada” melody, the root occurs at the third note in\nthe sequence. All other notes are indicated in cents from\nthe root, and so the ﬁrst note in the melody, a major third\nup from the root, is indicated at 400 cents. Of course, any\nnote can be used as the base for this representation, and in-\ndeed in melodies where the key root is not present, another\nnote will have to sufﬁce. Since the cent scale is a relative\nscale, the starting note is unimportant - all semitone inter-\nvals are 100 cents (assuming equal temperament). Equa-\ntion 1 shows the conversion from hertz to cents:\nCi= 1200 ×log2/parenleftbiggfi\nf0/parenrightbigg\n(1)\nwhere Ciis the relative pitch of the note in cents, fiis the\nfrequency of the note in hertz, and f0is the frequency in\nhertz of the base note.\nThe extracted melody is likewise converted to cents.\nSince we know in advance that the melody contains the\nroot note and that it is the lowest note in the melody, we\ncan use the lowest segment-quantized pitch as the “root”\nof the frequency track. Again, the choice of the base fre-\nquency is arbitrary, and could depend on any segment or\nan average of all segments, however, it is important to pick\na root note such that it is possible to align the candidate\ntrack with the target track.\n4.4 Segment alignment\nAt this point in the algorithm there are two distinct sets\nof pitch segments (candidate and target), and the task is\nto align them to the best ﬁt. The rhythmic ﬁt is approxi-\nmated ﬁrst by aligning the beginnings of the ﬁrst and last\nnotes. This was originally intended as an initial condition\nto an iterative rhythmic alignment process, but the rhyth-\nmic alignment was not implemented, primarily because\nthe initial alignment was found to be sufﬁcient for our pur-\nposes. This is likely because rhythmic target deviation is\ntypically very small compared to pitch target deviation.\nFuture implementations of this system would include an\nalgorithm to ﬁne-tune the beginnings and endings of the\ntarget melody, and to evaluate the relative rhythm error.\nAligning the beginning of the last segment gave better re-\nsults than aligning the end of the last segment because\nnote offset accuracy is much less important for melody\nidentiﬁcation than onset accuracy, and singers tend to cut\noff ﬁnal notes in a phrase earlier than other notes.\nAligning the segments by pitch consisted of calculat-\ning the ratio between each pair of segments, and averaging\nthese ratios across all segments, weighted by the segment\nlength. This calculated the scaling factor which would\nbring the measured pitch track into as close alignment as\npossible with the ideal pitch track. Because the tracks\nhave already been converted to cents, the pitch intervals\nare relative and so two identical melodies in different keys\ndiffer only by a scaling factor. In this case, the ratio is\nfound which best aligns the measured pitch to the ideal\npitch.\n5165 RESULTS\nIn this section a set of ﬁgures is presented showing exam-\nples of the pitch track phenomena observed in the course\nof this study. These ﬁgures are presented in pairs, with\nthe ﬁrst ﬁgure containing a complete melody track with\na highlighted pitch segment, and the second ﬁgure con-\ntaining an enlarged plot of the segment in question. Both\nplots are shown in relative pitch, with different scales. The\ncomplete melody lines are constructed so the root note of\nthe target melody line is at 0 cents. The individual seg-\nment plots are built with the target pitch at 0 cents, which\nclearly shows the deviation from the target in cents. Fig-\nures 1 and Figures 2 show an example of these plots. The\nsinger of this clip is subject 211, and the “g” refers to the\n7th prompt in the generalized list (corresponding to the\nsinging of “O Canada”)\n0 1000 2000 3000 4000 5000 6000−20002004006008001000\ntime (msec)relative pitch (cents)g211: all segments\npitch track mean pitch target pitch\nFigure 1: Pitch track with mean and target pitches.\n100 200 300 400 500−250−200−150−100−500\ntime (msec)relative pitch (cents)g211: segment #1\npitch track mean pitch target pitch\nFigure 2: Indicated segment from Figure 1.\nThis ﬁrst pair shows the success of the alignment algo-\nrithm as well as the deviation in pitch scale of the singer.\nIn general, the singer is “in tune,” and the melody track is\nrecognizable, but there are some things of note here with\nregard to the deviation from the target. First, the singer\nglides up to the highlighted note. This is of particular in-\nterest because, as indicated previously, no cueing pitches\nwere given. Subjects sang in whatever key they chose. If\nthe subjects were singing in arbitrary keys, why should\nsubjects glide up to the ﬁrst note they sing? Perhaps it\nis because they have a target pitch in their head, and as\nthey start singing they notice and correct the mis-tuning,\nuntil the target pitch is reached. Control systems work\nthis way as well—a target is chosen, and the system can\nonly respond in a ﬁnite amount of time, thereby approach-\ning the target over a period of time. Professional singers\navoid this initial glissando by holding a mental model ofthe note to be sung before vocalizing the note. Choir di-\nrectors instruct their singers to “Think the note before you\nsing the note.”\nThe next sections and ﬁgures provide a discussion of\nsome of the typical deviations from the target pitch that\nwere observed in the course of this research.\n5.1 Vibrato\nVibrato is a well-known phenomenon in musical analysis,\nwherein the frequency of a voice or instrument is mod-\nulated by a pseudo-sinusoidal waveform. Prame (1994)\nShowed that in singing, this modulation is usually around\n6.0 Hz. Typically, the formants which characterize the\nphoneme being sung do not change with the vibrato,\nwhich means that as the frequency partials oscillate in and\nout of the frequency peak of the formant, their amplitude\nincreases and decreases as well. Vibrato blurs the pitch\nrealization, making it more difﬁcult to determine the in-\ntended pitch target.\nMany novice and expert singers studied in this re-\nsearch used vibrato in their singing. It is theorized (al-\nthough as yet unsubstantiated) that individuals using a\nquery-by-humming system may, consciously or subcon-\nsciously, attempt to reduce the amount of vibrato in their\nsinging, and to ﬂatten their pitch tracks, to clarify and reg-\nularize their query. Vibrato has the perceptual effect of\ntightly coupling the partials of the note being sung, al-\nlowing it to be heard above other sounds. This is one of\nthe reasons that opera singers utilize higher-frequency and\nhigher-amplitude vibrato than popular singers, who use\nmicrophones and ampliﬁers to achieve the same purpose.\n0 1000 2000 3000 4000 5000−20002004006008001000\ntime (msec)relative pitch (cents)g226: all segments\npitch track mean pitch target pitch\nFigure 3: Mis-aligned pitch track with high-amplitude vi-\nbrato.\n50 100 150 200 250 300 350050100150\ntime (msec)relative pitch (cents)g226: segment #5\npitch track mean pitch target pitch\nFigure 4: Indicated segment from Figure 3.\n5170 1000 2000 3000 4000 5000 6000 7000−20002004006008001000\ntime (msec)relative pitch (cents)g238: all segments\npitch track mean pitch target pitchFigure 5: Pitch track with low-amplitude vibrato.\n100 200 300 400 500 600−60−40−20020\ntime (msec)relative pitch (cents)g238: segment #1\npitch track mean pitch target pitch\nFigure 6: Indicated segment from Figure 5.\nFigures 3 and 4 show an example of a situation where\nhigh-amplitude vibrato can interfere with the retrieval of\npitch target information. Subject 226 is a trained tenor\nsoloist, and produces vibrato which ranges almost two\nsemitones from lowest to highest pitch in the segment\nshown in Figure 4. Figure 3 shows how the vibrato has\nmake the investigation of the pitch track difﬁcult. It is\nclear that the alignment algorithm has failed in this case,\nand it is difﬁcult even to follow the extracted pitch track\nwith the eye. The general contour is there but without a\ntemplate to match to, identifying the melody of that utter-\nance using this or any other method would be quite dif-\nﬁcult. This is in contrast to Figure 1 where the discrete\nnotes and melody of the extracted pitch track are easy\nto follow visually. It is interesting to note here that the\nrecordings from Subject 226 are arguably the most per-\nceptually cohesive, and while many would consider 226\nthe “best” singing of the 50 singers in the set, it is one of\nthe most difﬁcult to track algorithmically.\nIn contrast, Figures 5 and 6 show an example of a low-\namplitude tight vibrato. The pitch track is the most visu-\nally consistent of the set, and the most easy to follow with\nthe eye. It would not be difﬁcult to design a system to\nextract the melody from this signal without any a priori\nknowledge of the ideal pitches or ideal rhythm—both are\nstrongly adhered to by the singer.\n5.2 Expertise of the Singer\nSubjects were asked to indicate their level of experience\nwith singing, with music, and with public speaking. Some\nsubjects claimed little or no experience, while others were\nexpert or professional. The differences in the pitch tracks\nbetween the novice and the expert is quite interesting.\nFigures 7 and 8 show the pitch track for subject 219,\n0 1000 2000 3000 4000 5000−20002004006008001000\ntime (msec)relative pitch (cents)g219: all segments\npitch track mean pitch target pitchFigure 7: Pitch track of a novice singer.\n100 200 300 400 500−50050\ntime (msec)relative pitch (cents)g219: segment #1\npitch track mean pitch target pitch\nFigure 8: Indicated segment from Figure 7.\n0 1000 2000 3000 4000 5000 6000−20002004006008001000\ntime (msec)relative pitch (cents)g216: all segments\npitch track mean pitch target pitch\nFigure 9: Pitch track of a trained singer.\n50 100 150 200 250 300 350 400 450−150−100−50050\ntime (msec)relative pitch (cents)g216: segment #3\npitch track mean pitch target pitch\nFigure 10: Indicated segment from Figure 9.\nan example of a novice singer, and Figures 9 and 10 show\nthe pitch track for subject 216, an example of a trained\nsinger and musician. When asked to describe their musi-\ncal, choral or spoken voice training or experience, subject\n219 indicated 2 years of theatrical speaking, and subject\n216 indicated 10 years of piano, one semester of a voice\nworkshop, and 5 years in choirs. Note particularly that al-\nthough the pitch rises and falls above the intended target,\nthe novice singer does not have a consistent vibrato and\n518appears to be attempting to hold the pitch constant, while\nthe experienced singer has a well-formed and intentional\nvibrato, closely resembling a sinusoid. It should also be\nnoted here, however, that the novice is singing “in tune”\njust as well as the experienced singer.\nThe experience level of the singers whose tracks ap-\npear in this paper are summarized in the following list:\n211 (age 59) 10 years trumpet, 3 years piano, 25 years\nchoirs\n216 (age 32) 10 years piano, 1 semester voice workshop,\n5 years choirs\n219 (age 24) 2 years theatrical voice\n226 (age 26) 4 years classical voice, 20 years choirs, solo\ntraining, 2 years theory\n238 (age 39) 6 years guitar, 5 years guitar, 1 year ukelele,\n2/3 year public speaking, 9 years informal speaking\nAnother interesting observation is that the “quality” of\nthe singing of subjects with experience but without spe-\nciﬁc vocal solo or opera training was uncorrelated with\nthe amount of musical or vocal experience they had. Most\nsingers in this range produced consistent pitch and tone\nwith relatively well-formed vibrato. Only the subjects\nwith very little or very much training exhibited excep-\ntional characteristics in their pitch track, and even then,\nthe single subject who claimed no experience whatsoever\nproduced a pitch track comparable to those claiming years\nof experience in choirs.\n5.3 Onset and offset\nLooking at Figure 9, it is clear that as the singer ascends\nthe major scale consisting of 2, 4, 5, 7, and 9 semitones\nabove the root, that the pitch descends slightly before the\npitch transition, and rises above the target pitch before set-\ntling into a vibrato oscillation. This is a common observa-\ntion across the data set. Singers rarely make a clean break\nbetween notes, and whether they glide up to or down to\nthe target pitch depends on the previous note, if there is\none. Observe in Figure 5, toward the end of the clip the\npitch glides down in a very short period of time, but re-\nmains continuous during that glide. This is unusual in the\ndata set—most singers produce pitch tracks more like that\nseen in Figure 1, where a complete break is made and the\npitch glides up to the target of the last note.\n6 CONCLUSIONS\nA singing human aims for a consistent note, and uses\nan oscillation around that note to solidify the perception\nin the ears of a listening human. Listening machines\nmust therefore take this intentional deviation from the tar-\nget into account in order to accurately transcribe human\nsinging. In many cases, pitch is one of the only available\nfeatures for transcribing human singing. Deviations from\nthe ideal are not universal, and models of vibrato in a song\nrecognition system must take these into account while be-\ning able to distinguish between vibrato pitch changes and\nnote transition pitch changes.The level of experience of the singer has an impact on\nthe type of deviation shown in the singing. Novice singers\nand singers beginning to study “professional” singing may\nhave erratic vibrato and unpredictable glide transitions,\nwhile experienced singers tend to have more regularized\nvibrato and note transitions. Knowledge of the singer in\nquestion would be very useful, and for that reason it may\nbe worthwhile to investigate ways of classifying human\nsinging with the intent of developing personalized devi-\nation models or a set of standardized deviation models\nbased on feature clustering.\nACKNOWLEDGEMENTS\nThis work is supported by the Natural Sciences and En-\ngineering Research Council of Canada and the Canadian\nFoundation for Innovation.\nREFERENCES\nA. de Cheveign ´e and H. Kawahara. YIN, a fundamental\nfrequency estimator for speech and music. Journal of\nthe Acoustical Society of America , 111(4), 2002.\nD. Gerhard. A human vocal utterance corpus for percep-\ntual and acoustic analysis of speech, singing and inter-\nmediate vocalizations. Journal of the Acoustical Soci-\nety of America , 112(5):2264, November 2002.\nB. Kedem. Spectral analysis and discrimination by zero-\ncrossings. Proceedings of the IEEE , 74(11):1477–1493,\nNovember 1986.\nP. Loizou. COLEA: A matlab software tool for speech\nanalysis. [Online] Retrieved March 18, 2003, from\nhttp://www.utdallas.edu/ ˜loizou/speech/colea.htm,\n2003.\nE. Prame. Measurements of the vibrato rate of ten singers.\nThe Journal of the Acoustical Society of America , 96\n(4):1979–1984, October 1994.\nS. Rossignol, P. Depalle, J. Soumagne, X. Rodet, and J.-\nL. Colette. Vibrato: Detection, estimation, extraction,\nmodiﬁcation. In Proceedings of the COST-G6 Work-\nshop on Digital Audio Effects (DAFx-99) , December 9–\n11 1999.\nT. Weyde. The inﬂuence of pitch on melodic segmenta-\ntion. In Proceedings of the 5th International Confer-\nence on Music Information Retrieval , pages 128–132,\nBarcelona, Spain, October 2004.\n519"
    },
    {
        "title": "Drum Track Transcription of Polyphonic Music Using Noise Subspace Projection.",
        "author": [
            "Olivier Gillet",
            "Gaël Richard"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415606",
        "url": "https://doi.org/10.5281/zenodo.1415606",
        "ee": "https://zenodo.org/records/1415606/files/GilletR05.pdf",
        "abstract": "This paper presents a novel drum transcription system for polyphonic music. The use of a band-wise harmonic/noise decomposition allows the suppression of the deterministic part of the signal, which is mainly contributed by nonrhythmic instruments. The transcription is then performed on the residual noise signal, which contains most of the rhythmic information. This signal is segmented, and the events associated to each onset are classified by support vector machines (SVM) with probabilistic outputs. The features used for classification are directly extracted from the sub-band signals. An additional pre-processing stage in which the instances are reclassified using a localized model was also tested. This transcription method is evaluated on ten test sequences, each of them being performed by two drummers and being available with different mixing settings. The whole system achieves precision and recall rates of 84% for the bass drum and snare drum detection tasks. Keywords: Drum transcription, Rhythm analysis, highresolution methods 1",
        "zenodo_id": 1415606,
        "dblp_key": "conf/ismir/GilletR05",
        "keywords": [
            "drum transcription system",
            "polyphonic music",
            "band-wise harmonic/noise decomposition",
            "suppression of deterministic part",
            "residual noise signal",
            "segmentation",
            "events classification",
            "support vector machines",
            "probabilistic outputs",
            "sub-band signals"
        ],
        "content": "DRUM TRACK TRANSCRIPTION OF POLYPHONIC MUSIC USING\nNOISE SUBSPACE PROJECTION\nOlivier Gillet and Ga ¨el Richard\nGET / T ´el´ecom Paris\nCNRS LTCI,\n37 rue Dareau\n75014 Paris, France\n[olivier.gillet, gael.richard]@enst.fr\nABSTRACT\nThis paper presents a novel drum transcription system for\npolyphonic music. The use of a band-wise harmonic/noise\ndecomposition allows the suppression of the deterministic\npart of the signal, which is mainly contributed by non-\nrhythmic instruments. The transcription is then performed\non the residual noise signal, which contains most of the\nrhythmic information. This signal is segmented, and the\nevents associated to each onset are classiﬁed by support\nvector machines (SVM) with probabilistic outputs. The\nfeatures used for classiﬁcation are directly extracted from\nthe sub-band signals. An additional pre-processing stage\nin which the instances are reclassiﬁed using a localized\nmodel was also tested. This transcription method is evalu-\nated on ten test sequences, each of them being performed\nby two drummers and being available with different mix-\ning settings. The whole system achieves precision and re-\ncall rates of 84% for the bass drum and snare drum detec-\ntion tasks.\nKeywords: Drum transcription, Rhythm analysis, high-\nresolution methods\n1 INTRODUCTION\nTraditionally, automatic music transcription and music re-\ntrieval systems essentially focus on the transcription of\npitched melodic instruments. However, rhythmic infor-\nmation proves to be very useful for many music informa-\ntion retrieval tasks, as rhythm plays a key part in modern\npopular music - especially dance music - and since people\nwithout musical training exhibit better skills at rhythm-\nrelated tasks (tapping, ”beatboxing”, identifying a tempo)\nthan at melody-related tasks (singing, humming). Au-\ntomatic transcription of drum tracks allows several spe-\nciﬁc applications, such as drum-controlled sound synthe-\nsis, rhythm-driven sound effects, musical genre identiﬁca-\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londontion for dance music, automatic DJing, as well as content-\nbased indexing.\nDifferent approaches have been proposed to solve this\nproblem. A ﬁrst possible approach, suggested in Gouyon\nand Herrera (2001), is to segment the signal into indi-\nvidual events, and to classify each event using machine\nlearning or statistical approaches. This method was ap-\nplied in Gillet and Richard (2004) to the transcription of\ndrum loops, and was subsequently integrated in a drum\nsequence retrieval system in which queries are formulated\nwith spoken onomatopoeia (Gillet and Richard, 2005a).\nSuch an approach is well suited for monophonic signals\n- that is to say, signals in which no other instrument\nthan the drum kit is played - and for rather large tax-\nonomies. However, in the context of polyphonic music\nsignals, these methods usually perform poorly, due to the\nfact that the spectral or cepstral features used for classi-\nﬁcation are largely modiﬁed by the addition of harmonic\ninstruments. An efﬁcient way of dealing with this prob-\nlem is to perform a ﬁrst recognition step with very generic\nmodels, and then to retrain an adapted model on a selected\nset of the recognized occurences (Sandvold et al., 2004).\nTemplate matching and adaptation is another possible\napproach which was introduced in Zils et al. (2002). Oc-\ncurences of a temporal ”seed” template are detected in\nthe input signal using a cross-correlation measure. An\nadapted template is built from these occurences, and the\nprocess is iterated. This technique was reﬁned by Yoshii\net al. (Yoshii et al., 2004), by performing the template-\nmatching in the time-frequency domain with a complex\nspectral distance. This technique showed very promising\nresults for the detection of bass drum and snare drum in\npolyphonic recordings, but requires the timbral character-\nistics of each drum instrument to be constant across the\nentire song.\nConcurrent approaches consider drum transcription as\na source separation problem. Source separation aims at\nextracting individual sound sources from music record-\nings, using information gathered from different sensors -\nfor example the two channels of a stereo recording (Barry\net al., 2004) or microphone arrays. An increasing num-\nber of works recently focused on single source audio sep-\naration (Vincent and Rodet, 2004). Source separation is\ntraditionally performed by means of statistical methods\nsuch as Independent Component Analysis, which opti-\nmizes an independence criterion on the separated source.\n92Once the different sources have been separated, and once\neach instrument of the drum kit has been identiﬁed among\nthe separated sources, the transcription problem is equiv-\nalent to a simple onset detection. However, it is not clear\nhow the separation should be performed. Prior knowledge\nabout the spectral characteristics, or statistical properties\nof the drum track have to be introduced. Using prior sub-\nspaces or dictionaries of spectral shapes is a possible ap-\nproach which was successfully followed by FitzGerald et\nal. (FitzGerald et al. (2003b), FitzGerald et al. (2003a)).\nAnother separation approach followed by Dittmar and\nUhle (Uhle et al. (2003), Uhle and Dittmar (2004)) re-\nquires the identiﬁcation of percussive components among\nthe separated sources.\nIn this paper, we extend our previous works on drum\ntranscription to the case of polyphonic music signals, by\nproposing a novel transcription system. This system is\nbased on a separation step followed by a more traditional\nmachine learning approach. The source separation step\naims at removing the contribution of the non-rhythmic in-\nstruments, in order to get as close as possible to a mono-\nphonic transcription problem, solved by support vector\nmachine classiﬁers. Contrary to traditional source sepa-\nration approaches that require the number of sources to be\nknown in advance, and the separated sources to be identi-\nﬁed and combined, the noise subspace projection consid-\ners the entire drum track as a single source. The paper is\norganized as follows: section 2 presents the overall prin-\nciple of the system. Section 3 and 4 respectively detail the\nsource separation and classiﬁcation stages. Model adap-\ntation is detailed in section 5, along with another possi-\nble post-processing module taking into account the time\nstructure of the drum patterns. Following a section 6 pre-\nsenting the evaluation results, section 7 suggests some\nconclusions.\n2 SYSTEM ARCHITECTURE\nThe goal of our system is to transcribe the drum track\nof polyphonic music signals. The information we aim\nat extracting from the signal is thus a sequence of\n(onset, instrument )pairs describing its drum track,\nwhere onset is the onset time, and instrument the drum\ninstrument, or the combination of instruments played at\nthis time. In the scope of this study, two instruments\nare used, the bass drum and the snare drum. While the\ndrum kits played in popular music include other percus-\nsion instruments (cymbals, hi-hats, tom-toms, cowbell...),\nthe bass drum and snare drum sequences are often sufﬁ-\ncient to characterize the typical drum patterns of different\nmusical genres. Moreover, all the existing query by voice\n(also known as ”query by beatboxing”) systems - Kapur\net al. (2004), Gillet and Richard (2005a), Nakano et al.\n(2004) - rely on such a labelling of the content.\nThe input signals can be either monophonic or stereo-\nphonic. In case of stereophonic signals, a simple pre-\nprocessing stage aims at building an optimal monophonic\nmix from the left and right channels, by maximizing an\nimpulsivity criterion. The next stage is the decomposition\nof the input signal x(t)into eight non-overlapping sub-\nbands xk(t),k= 1..8. The noise subspace projectionstage extracts the stochastic part of each sub-band signal\nek(t).\nThen, an onset detection algorithm identiﬁes the onset\ntimes from the sub-band signals ek(t). For each detected\nevent, a feature vector fis computed. Two parallel sup-\nport vector machines are ﬁnally classifying the onset in\none of the four possible categories (no event, bass drum,\nsnare drum, bass drum and snare drum mixture).\nAn optional model adaptation stage trains localized\nSVM classiﬁers from the transcription. Optional pre-\nprocessing stages can also use language-modeling, or re-\nlated methods, to incorporate higher-level information\nabout drum patterns.\nFigure 1: System architecture. Optional modules are\ndrawn in dotted lines.\nThe overall architecture of the system is summarized\nin ﬁgure 1.\n3 DRUM SIGNAL EXTRACTION\nThe drum signal extraction module used in this study\nshares common characteristics with the system described\nin Gillet and Richard (2005b), where it is used for a\nsource-separation task rather than for transcription. This\nsection summarizes its salient features.\n3.1 Pre-processing of stereo signals\nWhile a large amount of music collections (CD-audio\nquality or compressed music ﬁles) consist in stereo sig-\nnals, most of the algorithms used for music transcription,\nor for the extraction of high-level descriptors such as mu-\nsical genre or tempo, operate on mono signals. This can\nbe explained by the fact that automatic music transcrip-\ntion or description aims at extracting high-level informa-\n93tion which is preserved when the music signals is reduced\nto a single channel from a stereo pair. However, the extra\ninformation available in an additional channel can be used\nto build an enhanced signal which can be optimized for a\nspeciﬁc task.\nWe observed that in popular music signals, in a rather\nlarge number of cases, a monophonic mix with an en-\nhanced percussive content could be obtained by simply\nmixing the left and right channels of the recording with\nappropriate gains. This can be explained by the fact\nthat many popular music recordings use the so-called\n”panoramic” mix, in which each instrument is recorded\nas a single monophonic source that is mixed with two dif-\nferent gains on the left and right channels.\nThus, our approach consists in selecting a pair of gains\nfor each channels, in order to maximize an impulsiveness\ncriterion on the envelope of the remixed monophonic sig-\nnal.\nWe tested this approach on a collection of 55 signals of\npopular music. In 17 cases, a source (most of the time the\nbass) was removed in the mono signal x(t). In 3 cases,\nthe non-rhythmic instruments were barely audible in the\noptimal mono signal x(t).\n3.2 Extraction of the stochastic component\nThe principle of this stage is to use a band-wise har-\nmonic/noise decomposition to obtain the stochastic part of\nthe signal in different frequency bands. Since drums are\nmixed loudly in popular music, and since unpitched per-\ncussive sounds have a very strong stochastic component,\nit can be seen that the stochastic part of music signals is\nmostly contributed by the drum sounds. As an illustration,\nthe spectrograms in ﬁgure 2 show the similarity between\nthe stochastic part of a snare drum + guitar mixture, and\nthe isolated snare drum sound.\nFigure 2: Spectrograms of a snare drum and a guitar note\n(top). Stochastic and harmonic components of a snare\ndrum+guitar mixture (bottom).\nAn important aspect of this approach is that the esti-mation of the number of sources, as well as their identiﬁ-\ncation is not needed.\n3.2.1 Filter bank\nThe use of a ﬁlter bank is justiﬁed by two reasons. Firstly,\nthe noise subspace projection performs better on narrow-\nband signals, in which the noise can be considered as\nwhite. Moreover, a ﬁlter bank with an octave decompo-\nsition allows the tracking of a ﬁxed number of sinusoids\nper octave - a very suitable approach for mixtures of har-\nmonic signals. Secondly, a polyphase implementation of\nthe ﬁlter bank greatly reduces the computational cost of\nthe noise subspace projection, by allowing the signals in\neach sub-band to be downsampled.\nThe ﬁlter bank used in our system is an octave-\nband (dyadic) ﬁlter bank, with M= 8 voices - each\nfrequency band being one octave large. The sam-\npling rate of the input being equal to 44100 Hz, it re-\nsults in the following eight frequency bands (in Hz):\n[0,172], [172,345], [345,689], [689,1378], [1378,2756],\n[2756,5512], [5512,11025] and [11025, 22050]. The ﬁl-\nter was implemented using a 100th order FIR ﬁlter as a\nprototype.\n3.2.2 Noise subspace projection\nThe noise subspace projection stage is based on the Ex-\nponentionally Damped Sinusoidal (EDS) model (Badeau\net al., 2002). According to this model, the signal can be\ndecomposed in a harmonic part, modelled as a sum of si-\nnusoids with an exponential decay; and a noise part de-\nﬁned as the difference between the original signal and the\nharmonic part.\nWhile it is possible to estimate the sinusoids using\na classical Fourier analysis, this approach suffer from\nthe the resolution limit of the short-term Fourier trans-\nform. Subspace-based approaches, also known as high-\nresolution methods do not have such limitations, and are\ntherefore used in this study. A window of length Lis\nextracted from the original signal, deﬁning a signal vec-\ntorx. The L-dimensional space containing xis split in a\np= 2n-dimensional space containing the signal part, and\naL−p-dimensional space containing the noise; where nis\nthe number of exponentionally damped sinusoids tracked.\nThe noise vector, corresponding to the stochastic part of x\ncan be computed by directly projecting xon the noise sub-\nspace. An entire signal can be processed using an overlap-\nadd method.\nThe tracking of the signal subspace itself is achieved\nusing the classical EVD iterative algorithm (Badeau et al.,\n2002), with 46ms long windows, using a 3/4 overlap.\nThe number of sinusoids in each frequency band was\nmanually adjusted. Two sinusoids are used for x1(t)(low-\nest frequency band, in which only the bass is playing), ﬁve\nforx2(t), ten for x3(t)andx4(t); and eight for the other\nbands. Using an insufﬁcient number of sinusoids might\nleave harmonic components in the output signal; while\nusing too many sinusoids might remove all the signiﬁcant\ntimbral information from the input signal. The number of\nsinusoids in each band can also be automatically selected\nby appropriate methods (Badeau et al., 2005), at the cost\nof an increased computational burden.\n94The output of the noise subspace projection is thus 8\nsub-band noise signals ek(t). Because of the multirate\nimplementation of the ﬁlter bank, these signals need to be\nresynchronized in time, by upsamling them and by apply-\ning a synthesis ﬁlter.\n4 ONSET CLASSIFICATION\n4.1 Onset detection\nIn the case of drum transcription of polyphonic music, a\npeculiarity of the onset detection problem is that we are\nnot interested in detecting all the onsets - only the on-\nsets corresponding to drum events are of interest. A ﬁrst\npossible approach is to design the onset detection module\nin such a way that only onsets associated to drum instru-\nments are detected. Unfortunately, even after the noise\nsubspace projection, the residual noise signals still con-\ntain attacks or transients from pitched instruments. An-\nother approach is thus to handle the case of non-percussive\nevents later in the machine learning stage.\nMost of the onset detectors are based on sub-band de-\ncompositions (Klapuri, 1999). For this reason, it seems\nrelevant to directly use the sub-band noise signal to detect\nonsets. Each of these sub-band noise signals is half-wave\nrectiﬁed and low-pass ﬁltered, the resulting signal being\nnoted bk(t). While the ﬁrst order relative difference func-\ntiond\ndtlog(bk(t) +A)is often used to detected onsets, we\nobserved that simply using a derivative gave a higher ac-\ncuracy. Thus, onsets are found by peak-pickingd\ndtbk(t).\n4.2 Features extraction\nFor each onset localized at time t, we compute the follow-\ning features over a 100ms long window starting at t:\n•The energy in the ﬁrst 6 sub-bands. These features\ncan be directly computed from the decomposition.\n•The average of the 12 ﬁrst MFCC (without c0) across\nsuccessive frames. The MFCC are computed on the\nnoise signal/summationtext\nkˆek(t)\nThe inclusion of the ﬁrst MFCC coefﬁcient c0gave\nslightly worse results. The use of the 4 spectral moments\ndid not increase the accuracy either - it is very likely that\nthese features are highly sensitive to the noise subspace\nprojection.\nDifferent transformations were tested on this feature\nset. Performing a Principal Component Analysis on the\ndata set did not signiﬁcantly increase the performances;\nhowever, it could be seen that the ﬁrst 12 components\ncontributed in 96 % of the total variance. Performing the\nclassiﬁcation on these 12 ﬁrst principal components re-\nduced the computational cost of the learning / classiﬁca-\ntion steps, without any signiﬁcant accuracy loss.\n4.3 SVM classiﬁcation\nThe classiﬁcation problem presented in this work is\nslightly different from a more traditional ”segment and\nclassify” approach. Firstly, some of the onsets to clas-\nsify are not occurences of drum instruments, and mustbe recognized as such and discarded. Secondly, the\nsmall number of categories used in our studies is well\nsuited for a binary classiﬁcation approach. Thus, we\ndecided to train two classiﬁers, one of them detect-\ning the presence of bass drums, and the other detect-\ning the presence of snare drums. When the input on-\nset does not correspond to an occurence of a percus-\nsion instrument, the pair of classiﬁers will output the pair\n(non bass drum ,non snare drum ).\nThe classiﬁers used are Support Vector Machines\n(Vapnik, 1995), which are well suitable for binary clas-\nsiﬁcation problems, and show very interesting general-\nization properties. A general-purpose kernel (radial ba-\nsis function) was used. The implementation chosen was\nSV Mlight(Joachims, 1999).\nThe output of a SVM is classically an uncalibrated\nvalue - its sign being used for the decision, and its abso-\nlute value roughly expressing the distance to the decision\nboundary. A method to obtain posterior probabilities from\nthis uncalibrated value has been described in Platt (2000).\nThe output of the SVM f(x)is mapped to the interval\n]0,1[with a sigmoid function: p(x) =1\n1+eAf(x)+B. The\nparameters A, B are ﬁtted using maximum likelihood es-\ntimation on a subset of the training data. Typically, a large\nfraction of the training set is used to perform the SVM\nlearning, and the remaining part is used to estimate the\nparameters AandB.\nThe availability of posterior probabilities allow fur-\nther post-processing stages, such as those described in the\nnext section. Moreover, it is easier to adjust the decision\nthreshold with scaled, probabilistic values, than with an\nuncalibrated output. Such adjustements are necessary if\nthe users of the transcription system need to adapt the ra-\ntio of ”miss” and ”false alarm” errors to their own speciﬁc\napplications.\n5 POST-PROCESSING STAGES\n5.1 Adaptation\nWe decided to follow an approach similar to the one de-\nscribed in Sandvold et al. (2004). This approach con-\nsists in performing a ﬁrst recognition step using a gen-\neral model - in our case this general model consists in the\nSVM classiﬁers presented in the previous section, the pa-\nrameters of which have been learned on the whole training\nset. Then, the Nrecognized instances are ranked using\na likelihood measure, and a subset of them (containing\nkNexamples) from which the best recognition scores are\nachieved is selected. In our case, we used the probabilis-\ntic output of the SVM classiﬁer as a likelihood measure,\ninstead of manually ranking the recognized instances as it\nwas done in the work of Sandvold et al. A ”localized” or\nadapted model is subsequently learned on this small train-\ning set. The recognition is ﬁnally performed again on the\nwhole sequence, this time using the excerpt-speciﬁc, lo-\ncalized model.\nDifferent values have been tested for the value of k,\nthe best results being achieved with k= 0.4(40% of the\nrecognized instances are used to retrain the system).\n955.2 Periodic decisions\nDifferent language-modeling techniques have been pro-\nposed to incorporate high-level information into drum\ntranscription systems. Short-term models, such as n-\ngrams (Gillet and Richard, 2004) usually model the time-\ndependencies in acoustic features caused by overlapping\nstrokes. It also models simple stereotypical patterns, such\nas tom ﬁlls. In the context of our study, in which only\ntwo categories of instruments are used, such a model is\nnot particularly useful. In fact, the different n-sequences\nof bass-drum and snare drums are almost equiprobable in\nour database. A similar problem occured with periodic n-\ngrams (Paulus and Klapuri, 2003): as our database covers\ndifferent styles, the different sequences were also almost\nequiprobable.\nWe ﬁnally decided to follow a different approach,\nwhich requires no prior training, and is only based on\nthe repetitive nature of drum patterns. In order to clas-\nsify an event occuring at time t, we fuse the classiﬁcation\nresults for the events occuring at time t,t−Mandt+M,\nwhere M is the duration of a bar or pattern. M can be\nautomatically estimated from the audio signal (Klapuri,\n2003), or from the symbolic transcription obtained at the\nprevious stage (Meudic and St-James). To evaluate our\nmethod independently of pattern duration estimation er-\nrors, the pattern duration was manually annotated for each\nﬁle. Different operators were tested for the fusion, such as\nweighted means, products, median, and the Yager t-norm\n(For a review of different aggregation operators, see De-\ntyniecki (2001)).\n6 EVALUATION AND RESULTS\n6.1 Database\nIn order to avoid the tedious manual annotation of pre-\nexisting material, and to enable a wide range of exper-\niments, we recorded our own database. This database\nmakes use of ”training sessions”, also known as ”minus\none” CDs. Such CDs are used for the teaching of drum-\nming, and allow students to practice on the top of a music\naccompaniment from which the drum track has been re-\nmoved. We selected ten excerpts from two ”minus one”\nCDs. The excerpts are one minute long, cover various\nstyles (blues, twist, metal, funk, celtic...) and are mostly\nplayed by acoustic instruments (bass, electric guitar, sax,\naccordion...) with a few synthetic keyboards (FM electric\npiano, organ).\nTwo professional drummers were asked to play a\nrhythmic accompaniment on the top of the excerpts, which\nwere played through headphones. Each drummer brought\nhis own drum kit. Inter-sequence variability was intro-\nduced by the use of different kinds of sticks (includ-\ning bundled sticks) and by asking the drummers to ad-\njust their playing style according to the genre of each se-\nquence. Both drummers played in a rather nuanced style,\nwhich introduced intra-sequence variability - a character-\nistic not present in databases using synthetic or sampled\ndrum sounds. The performances were recorded with 8 mi-\ncrophones (A Beyer 88 for the bass drum, a Shure SM57\nfor the snare drum, a Schoeps CMC with cardiod capsulefor the hi-hat, two Shure SM58 for the highest tom-toms,\na Sennheiser 441 for the low tom and two Audio-Technica\nAT4040 for the overheads), ampliﬁed by 4 Behringer Ul-\ntragain Pro Mic2200 dual pre-ampliﬁers, on a Tascam\nMX2424 digital multitracker (8 channels were used). A\nstereo mix was generated from the 8 tracks, using pan-\nning, equalization, and compression. This stereo mix and\nthe original ”minus one” excerpts were ﬁnally mixed with\ndifferent relative levels. First of all, a reference mix was\nproduced, in which the drums and other instruments were\nwell-balanced. Then, two other mixes were produced, in\nwhich the drums were respectively ampliﬁed and attenu-\nated by 6dB. The stereo drums mix was also kept. This re-\nsults in 80 stereo different signals (10 excerpts ×2 drum-\nmers ×4 mixes).\nThe annotation was obtained semi-automatically, by a\nsimple onset detection algorithm on the bass drum / snare\ndrum tracks, the output of which was manually checked\nand corrected. The average number of events (on both the\nbass drum and snare drum tracks) per excerpt is 178.\n6.2 Evaluation metric\nThe correctness of the transcription was evaluated by pre-\ncision and recall measures. Let Ndbe the total number of\nevents detected by the system, Ncthe number of correct\nevents detected by the system; and Nthe actual number\nof events to be detected. Precision and recall are deﬁned\nas:\nprecision =Nc\nNd\nrecall =Nc\nN\nAs it is possible to adjust the decision rules to favor\nprecision or recall, we chose a decision rule in which\nthe two kinds of errors, ”false alarms” and ”misses” are\nroughly balanced. The f-measure, which is deﬁned as:\nF-measure =2·precision ·recall\nprecision +recall\nis another measure of the accuracy of the system,\nwhich is independent of the chosen precision/recall trade-\noff.\nIt is worth noting that a small deviation is allowed be-\ntween the actual onset and the detected onset: events are\nconsidered as correctly detected when they are detected\nwithin 50ms of the reference onset.\n6.3 Evaluation protocol\nExperiment 1: Robustness In order to test the accuracy\nand robustness of the transcription system under different\nkind of mixing conditions, we repeated for each of the 4\nmixes (drum only, balanced, attenuated drums, ampliﬁed\ndrums) the following procedure:\n•Train the SVM on the events detected from the 10\nexcerpts played by drummer A.\n•Evaluate the SVM on the 10 excerpts played by\ndrummer B.\n96•Repeat the process after having exchanged A and B.\nThe precision, recall and F-measure obtained for each\ndrummer and excerpt are averaged. It is worth to mention\nthat the stereo pre-processing stage was not used in this\nﬁrst experiment.\nMore generally, considering the available data, this\ntwo-folds protocol is the most adapted to show the gener-\nalization capabilities of the learning algorithms. However,\nit is necessary to keep in mind, while interpreting the re-\nsults of our experiments, that the training set is relatively\nsmall.\nExperiment 2: Performance of the stereo pre-\nprocessing The same experiment was repeated using a\nstereo pre-processing stage, and the results were com-\npared.\nExperiment 3: Post-processing Results for a ”base-\nline” system are obtained using a protocol similar to the\none used in experiment 1, except that only the balanced\nmix is used. Results are then computed with the model\nadaptation stage, and with the periodic decision stage.\n6.4 Results and discussion\nResults for the robustness experiment are given in table 1.\nThe best scores are achieved with recordings in which the\ndrums are mixed loudly, but acceptable results are also\nobtained when the drums are attenuated. With balanced\nmixes, which correspond to the situation encountered in\nreal world recordings, the performances of the system are\nroughly comparable to those given in Yoshii et al. (2004),\neven though direct comparison is not possible since a dif-\nferent dataset was used.\nTable 1: Results of the robustness experiment\nMix Recall Precision F-measure\nDrums -6dB 75.8% 71.1% 73.4%\nBalanced mix 83.9% 84.2% 84.0%\nDrums +6dB 87.4% 91.2% 89.2%\nDrums only 83.7% 92.7% 88.0%\nThe evaluation of the stereo pre-processing stage is\ngiven in table 2. It can be seen that this stage signiﬁcantly\nincreases the accuracy of the transcription when the other\ninstruments are mixed more loudly than the drums.\nTable 2: Impact of the stereo pre-processing stage\nMix Recall Precision F-measure\nDrums -6dB 76.2% 78.4% 77.3%\nBalanced mix 82.0% 88.5% 85.1%\nDrums +6dB 84.3% 90.8% 87.4%\nDrums only 83.7% 92.7% 88.0%\nThe different post-processing stages are compared in\ntable 3. It can be seen that none of the methods described\nin section 5 improve the recognition.Table 3: Impact of the post-processing stages\nMethod Recall Precision F-measure\nBaseline 83.9% 84.2% 84.0%\nAdaptation 78.1% 71.0% 74.3%\nPeriodic decision 87.2% 78.4% 82.6%\nDifferent reasons can explain the failure of the local-\nized models. First of all, the local models are trained using\nonly a subset of the detected onsets. This results in a very\nsmall training set. Increasing the fraction of recognized\ninstances used to train the local model does not help ei-\nther, since it becomes more and more likely that some of\nthese instances are indeed misclassiﬁed. Secondly, we no-\nticed that our features set, computed on the residual noise\nsignal, did not exhibit a lot of variability from one track to\nanother, contrary to the feature set used in Sandvold et al.\n(2004) which was computed on the original signal, rather\nthan on a residual noise signal. Finally, we noticed that\nthe selected instances were mostly loud or solo strokes,\nmost of them played off-beat. It means that the adapted\nmodel will specialize itself in identifying such strokes,\nand will become unable to identify strokes with differ-\nent timbral characteristics or dynamics appearing within\nthe same track. It seems that the use of localized models\nwould give best results with synthetic or sampled drum\ntracks, in which there is very little variation between the\ndifferent snare drum or bass drum sounds.\nThe use of the modiﬁed decision function taking into\naccount the periodicity of drum patterns does not increase\nthe classiﬁcation results either. However, a thorough anal-\nysis of the classiﬁcation errors shows that this method\nmodiﬁes the kind of errors made by the system. Clas-\nsiﬁcation errors on the steady, typical component of the\ndrum pattern are less frequent, while many recognition er-\nrors are localized in breaks or in variations at the end of\na pattern. It is not clear which one of these two classes\nof errors is more acceptable. Applications such as au-\ntomatic accompaniement generation, or score following\nwould probably require better classiﬁcation results of loud\nstrokes, played on strong beats. On the other hand, play-\ning style analysis probably requires a very accurate tran-\nscription of breaks, soli and variations. This suggests the\nuse of a problem-speciﬁc evaluation metric with a differ-\nent cost for classiﬁcation errors occuring on strong beats /\nloud strokes; and the rest of the sequence.\nFinally, the detailed results of the baseline system are\ngiven in table 4. Drummer 2 has a very nuanced style,\nwith a lot of variations in the dynamic of the strokes, while\nDrummer 1 has an energic, steady, style with fewer vari-\nations in the dynamic of strokes. It can be seen that the\nalgorithm performs better when trained on Drummer 2\nand evaluated on Drummer 1. Thus, for large scale ap-\nplications, our system will need to be trained on a larger\ndatabase containing multiple variations in timbre and dy-\nnamics. The two sequences on which the algorithm gives\nthe worst results, Groove 5/4 andCeltic are played by both\ndrummers with a lot of ghost notes - quiet beats which\nhelp the drummer in keeping the tempo more accurately.\nWhile we annotated ghost notes and included them in the\n97Table 4: Detailed results of the baseline system\nBass drum Snare drum\nSequence Drummer Rec. Prec. F-meas. Rec. Prec. F-meas.\nBlues 1 72.1% 96.1% 0.82 95.6% 100.0% 0.98\n286.1% 82.7% 0.84 87.8% 87.8% 0.88\nBlues rock 1 92.2% 92.2% 0.92 100.0% 100.0% 1.00\n289.5% 91.7% 0.91 82.2% 80.4% 0.81\nCeltic 1 75.4% 94.9% 0.84 77.8% 33.3% 0.47\n270.1% 87.8% 0.78 80.3% 68.1% 0.74\nFunk 1 79.8% 62.5% 0.70 87.8% 97.0% 0.92\n277.6% 90.6% 0.84 81.5% 91.7% 0.86\nJazz funk 1 94.7% 87.3% 0.91 97.9% 85.2% 0.91\n278.6% 94.2% 0.86 84.6% 76.7% 0.80\nGroove 5/4 1 85.2% 54.1% 0.66 82.8% 96.0% 0.89\n291.9% 77.5% 0.84 86.3% 62.9% 0.72\nMetal 1 90.3% 77.8% 0.84 83.3% 88.7% 0.86\n275.5% 72.1% 0.74 77.9% 75.9% 0.77\nRock 1 90.5% 77.9% 0.84 88.5% 97.7% 0.93\n274.1% 88.6% 0.81 88.0% 89.0% 0.88\nShufﬂe 1 74.4% 81.5% 0.78 85.7% 85.7% 0.86\n267.6% 93.1% 0.78 68.9% 81.6% 0.75\nTwist 1 97.6% 75.0% 0.85 91.9% 95.8% 0.94\n284.8% 98.1% 0.91 78.9% 98.1% 0.87\nevaluation of this work, it is not clear if errors on such\nstrokes are acceptable or not, and if they should be taken\ninto account.\n7 CONCLUSION AND FUTURE WORK\nThis paper proposed a novel drum transcription system\nfor polyphonic music and evaluated its performances, as\nwell as the effect of different pre-processing and post-\nprocessing stages. Promising results (83.9 % recall, 84.2\n% precision) were obtained on a test database under stan-\ndard recording conditions. However, the failure of the\ndifferent post-processing stages tested raises interesting\nquestions.\nFirstly, the different kind of errors produced with or\nwithout language modeling suggests that evaluation met-\nrics could take into account the position and importance\nof the misdetected events within the rhythmic patterns.\nThe different tasks and applications for which drum tran-\nscription is needed should be clearly identiﬁed, and task-\nspeciﬁc evaluation metrics should be devised for each of\nthem.\nSecondly, our results show that the use of localized\nmodels not taking into account all the information learned\nin the generic model is not the best way to perform adap-\ntation. Further works will focus on the use of incremental\nlearning methods for support vector machines, in which\nthe original generic model is updated or transformed,\nrather than discarded.\nIt is also planned to improve the noise subspace pro-\njection stage by automatically selecting the number of si-\nnusoids in each frequency band. Finally, our system will\nbe trained and tested on a larger corpus, which will in-\nclude labels for other categories of drum instruments suchas hi-hats or cymbals. Such a larger corpus could be cre-\nated by mixing pre-recorded drum loops, from which an\nannotation is available, with the ”minus one” sequences.\nOur present corpus could then be used as a testing set.\nACKNOWLEDGEMENTS\nThe authors would like to thank Frederic Rottier and\nBertrand Clouard who performed the drum sequences\nused in this study, as well as the sound engineer Michel\nDesnoues for the high quality recordings.\nThis work was partly supported by the MusicDiscover\nproject of the ACI Masse de donn ´ees.\nREFERENCES\nR. Badeau, R. Boyer, and B. David. Eds parametric mod-\neling and tracking of audio signals. In Proceedings of\n5th International Conference on Digital Audio Effects\n(DAFX’02) , September 2002.\nR. Badeau, B. David, and G. Richard. Selecting the mod-\neling order for the esprit high resolution method: an\nalternative approach. In Proceedings of the 2004 Inter-\nnational Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP’04) , May 2005.\nD. Barry, B. Lawlor, and E. Coyle. Sound source separa-\ntion: Azimuth discrimination and resynthesis. In Pro-\nceedings of the 7th International Conference on Digital\nAudio Effects (DAFX’04) , October 2004.\nM. Detyniecki. Numerical aggregation operators: State of\nthe art. In International Summer School on Aggregation\nOperators and their Applications , 2001.\n98D. FitzGerald, B. Lawlor, and E. Coyle. Drum transcrip-\ntion in the presence of pitched instuments using prior\nsubspace analysis. In Proceedings of the Irish Signals\nand Systems Conference (ISSC 2003) , July 2003a.\nD. FitzGerald, B. Lawlor, and E. Coyle. Prior subspace\nanalysis for drum transcription. In Proceedings of the\n114th AES Convention , March 2003b.\nO. Gillet and G. Richard. Automatic transcription of drum\nloops. In Proceedings of the 2004 International Con-\nference on Acoustics, Speech, and Signal Processing\n(ICASSP’04) , May 2004.\nO. Gillet and G. Richard. Drum loops retrieval from spo-\nken queries. In Journal of Intelligent Information Sys-\ntems, volume 24:2/3, pages 159–177. Springer Science,\n2005a.\nO. Gillet and G. Richard. Extraction and remixing of drum\ntracks from polyphonic music signals. In Proceedings\nof the IEEE Workshop on Applications of Signal Pro-\ncessing to Audio and Acoustics (WASPAA’05) , October\n2005b.\nF. Gouyon and P. Herrera. Exploration of techniques for\nautomatic labeling of audio drum tracks. In Proceed-\nings of MOSART: Workshop on Current Directions in\nComputer Music , November 2001.\nT. Joachims. Making large-scale svm learning practical.\nInAdvances in Kernel Methods - Support Vector Learn-\ning. MIT Press, 1999.\nA. Kapur, M. Benning, and G. Tzanetakis. Query by beat-\nboxing: Music information retrieval for the dj. In Pro-\nceedings of the 5th International Conference on Music\nInformation Retrieval (ISMIR 2004) , October 2004.\nA. Klapuri. Sound onset detection by applying psychoa-\ncoustic knowledge. In Proceedings of the 1999 Inter-\nnational Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP’99) , March 1999.\nA. Klapuri. musical meter estimation and music transcrip-\ntion. In Proceedings of the Cambridge Music Process-\ning Colloquium , March 2003.\nB. Meudic and E. St-James. Automatic extraction of ap-\nproximate repetitions in polyphonic midi ﬁles based on\nperceptive criteria. In Lecture notes in Computer sci-\nence, LNCS 2771 . Springer Verlag.\nT. Nakano, J. Ogata, M. Goto, and Y. Hiraga. A drum\npattern retrieval method by voice percussion. In Pro-\nceedings of the 5th International Conference on Music\nInformation Retrieval (ISMIR 2004) , October 2004.\nJ. Paulus and A. Klapuri. Conventional and periodic n-\ngrams in the transcription of drum sequences. In Pro-\nceedings of IEEE International Conference on Multi-\nmedia and Expo (ICME’03) , July 2003.\nJ. Platt. Probabilistic outputs for support vector ma-\nchines and comparison to regularized likelihood meth-\nods. In Advances in Large Margin Classiers , pages 61–\n74, 2000.\nV. Sandvold, F. Gouyon, and P. Herrera. Percussion clas-\nsiﬁcation in polyphonic audio recordings using local-\nized sound models. In Proceedings of the 5th Interna-tional Conference on Music Information Retrieval (IS-\nMIR 2004) , October 2004.\nC. Uhle and C. Dittmar. Further steps towards drum tran-\nscription of polyphonic music. In Proceedings of the\n116th AES convention , May 2004.\nC. Uhle, C. Dittmar, and T. Sporer. Extraction of drum\ntracks from polyphonic music using independent sub-\nspace analysis. In Proceedings of the 4th International\nSymposium on Independent Component Analysis and\nBlind Signal Separation (ICA2003) , April 2003.\nV. Vapnik. The Nature of Statistical Learning Theory .\nSpringer-Verlag, 1995.\nE. Vincent and X. Rodet. Underdetermined source sepa-\nration with structured source priors. In Proceedings of\nthe 5th Symposium on Independent Component Analy-\nsis and Blind Signal Separation (ICA2004) , April 2004.\nK. Yoshii, M. Goto, and H. G. Okuno. Automatic drum\nsound description for real-world music using template\nadaptation and matching methods. In Proceedings of\nthe 5th International Conference on Music Information\nRetrieval (ISMIR 2004) , October 2004.\nA. Zils, F. Pachet, O. Delerue, and F. Gouyon. Automatic\nextraction of drum tracks from polyphonic music sig-\nnals. In Proceedings of WEDELMUSIC2002 , Decem-\nber 2002.\n99"
    },
    {
        "title": "Musicream: New Music Playback Interface for Streaming, Sticking, Sorting, and Recalling Musical Pieces.",
        "author": [
            "Masataka Goto",
            "Takayuki Goto"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415842",
        "url": "https://doi.org/10.5281/zenodo.1415842",
        "ee": "https://zenodo.org/records/1415842/files/GotoG05.pdf",
        "abstract": "This paper describes a novel music playback interface, called Musicream, which lets a user unexpectedly come across various musical pieces similar to those liked by the user. With most previous “query-by-example” interfaces used for similarity-based searching, for the same query and music collection a user will always receive the same list of musical pieces ranked by their similarity and opportunities to encounter unfamiliar musical pieces in the collection are limited. Musicream facilitates active, flexible, and unexpected encounters with musical pieces by providing four functions: the music-disc streaming function which creates a flow of many musical-piece entities (discs) from a (huge) music collection, the similaritybased sticking function which allows a user to easily pick out and listen to similar pieces from the flow, the metaplaylist function which can generate a playlist of playlists (ordered lists of pieces) while editing them with a high degree of freedom, and the time-machine function which automatically records all Musicream activities and allows a user to visit and retrieve a past state as if using a time machine. In our experiments, these functions were used seamlessly to achieve active and creative querying and browsing of music collections, confirming the effectiveness of Musicream. Keywords: Music interface, Music player, Musiccollection browser, Query-by-example, Playlist. 1",
        "zenodo_id": 1415842,
        "dblp_key": "conf/ismir/GotoG05",
        "keywords": [
            "Musicream",
            "novel music playback interface",
            "unexpected encounters",
            "similar musical pieces",
            "query-by-example",
            "similarities",
            "music collection browsing",
            "active flexible",
            "time-machine function",
            "experiments"
        ],
        "content": "MUSICREAM: NEW MUSIC PLAYBACK INTERFACE FOR STREAMING,\nSTICKING, SORTING, AND RECALLING MUSICAL PIECES\nMasataka Goto Takayuki Goto\nNational Institute of Advanced Industrial Science and Technology (AIST).\nIT, AIST, 1-1-1 Umezono, Tsukuba, Ibaraki 305-8568, Japan\nm.goto@aist.go.jp\nABSTRACT\nThis paper describes a novel music playback interface,\ncalled Musicream , which lets a user unexpectedly come\nacross various musical pieces similar to those liked by the\nuser. With most previous “query-by-example” interfaces\nused for similarity-based searching, for the same query\nand music collection a user will always receive the samelist of musical pieces ranked by their similarity and op-\nportunities to encounter unfamiliar musical pieces in the\ncollection are limited. Musicream facilitates active, ﬂex-ible, and unexpected encounters with musical pieces by\nproviding four functions: the music-disc streaming func-\ntion which creates a ﬂow of many musical-piece enti-\nties (discs) from a (huge) music collection, the similarity-\nbased sticking function which allows a user to easily pick\nout and listen to similar pieces from the ﬂow, the meta-\nplaylist function which can generate a playlist of playlists\n(ordered lists of pieces) while editing them with a high\ndegree of freedom, and the time-machine function which\nautomatically records all Musicream activities and allows\na user to visit and retrieve a past state as if using a time\nmachine. In our experiments, these functions were used\nseamlessly to achieve active and creative querying and\nbrowsing of music collections, conﬁrming the effective-\nness of Musicream.\nKeywords: Music interface, Music player, Music-\ncollection browser, Query-by-example, Playlist.\n1 INTRODUCTION\nAlthough current music playback interfaces can satisfy\nuser desires like “I would like to hear this song” based\non the name of the song or the artist’s name, they have\nnot been very strong in functions that satisfy desires like\n“I want to hear something” or “I want to hear something\nmy way.” Such desires, though, will become more com-\nmon as part of the next-generation music-listening envi-ronment as ﬂat-rate, all-you-can-hear music subscription\nservices giving users unlimited on-line access to a mil-\nlion or more musical pieces become widespread. We willsoon be hearing users say things like: “If I like a song that\nI have somehow chosen from a huge music collection, I\nwant to be able to pick out, by myself, other songs sim-\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-mercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonilar in mood to that song one after another”, or “When I\nwant to hear music in an order that I prefer, I would liketo try out a playback order with the same high degree of\nfreedom I have in stacking compact discs (CDs) on my\ndesk and ordering them as I like”, or even “I would like to\nreproduce a song order that I once listened to in the past\n(such as on a day with good memories) and hear it now.”\nFunctions that can satisfy such desires have been lack-\ning in the past, and with the aim of rectifying this sit-uation, technologies to enable such functions on an in-\ndividual basis have been developed. However, thereare still no interfaces that provide a comprehensive in-\ntegrated music-listening environment in which functions\nsuch as these can be used in an easy and seamless man-\nner. For example, while many effective similarity mea-\nsures [1, 2, 3, 4, 5, 6, 7, 8] have been proposed to enable\nusers to listen to songs of similar mood, most previous\n“query-by-example” interfaces for searching similar mu-\nsic only provide functions for listing songs similar to a cer-\ntain song. No consideration has been given to an operation\nthat ﬁnds similar music in a way analogous to a listener\npulling one CD after another from a stack of CDs and lis-\ntening to them as much as one desires. As for the music\nplayback order, many music players already have func-\ntions enabling users to specify playlists (i.e., lists of mu-\nsical pieces for playback). The drawback here is that the\noperations provided by these functions are usually limited\nto changing the order of pieces and to inserting and delet-\ning pieces in existing playlists; i.e., the degree of freedom\nis low. In addition, the only way that a user can currently\nremember the order in which certain musical pieces were\nlistened to at some time in the past is to make a conscious\neffort to store (save) the playlist at that time for later re-trieval (loading).\nIn this paper, we propose a new music playback inter-\nface called Musicream (“music\n” + “stream\n ”) that enables\nall of the above music-listening formats to provide a user\nwith a high degree of freedom. Musicream applies the\nfollowing functions to create a novel music-listening en-\nvironment.\n1.Music-disc streaming function : lets musical-piece\nentities (discs) stream down one after another on a\nscreen for user perusal and selection.\n2.Similarity-based sticking function : attaches similar\nmusical pieces to a music disc selected by the userin a manner similar to how two magnets attract eachother.\n3.Meta-playlist function : enables the user to try out\nvarious playback orders by making it easy to rear-\nrange groups of musical pieces on the screen in a\nmanner similar to rearranging CDs on a (real) desk-\ntop.\n404Music-disc streaming\nfunction : Active and\nunexpected encounters with\ninteresting musical piecesSimilarity-based sticking\nfunction : Controllable and\nﬂexible “query-by-example”\nsimilarity searchMeta-playlist function :\nAdvanced playlist editing\nwith a high degree of\nfreedomTime-machine function :\nIntuitive browsing and\nperfect recalling of past\nmusic listening activities\nFigure 1: Four functions of Musicream (demonstration video clips are available at http://staff.aist.go.jp/m.goto/ISMIR2005/).\n4.Time-machine function : provides a means of moving\nfreely back in time as if using a time machine to re-\nturn to a point in the past where one was listening tomusic.\nThese functions make it possible, for example, to pick outa musical piece (disc) from among many streaming on the\nscreen and then attach other pieces similar in mood to that\nmusical piece, thereby creating a group of pieces. Further-more, by simply continuing this process, several groups\n(playlists) of musical pieces can be formed and placed in a\nblank area on the screen in a manner that decides the play-\nback order of those groups. In this way, users can enjoy\nmusic in a much more active manner. Moreover, all op-erations on Musicream are recorded automatically so that\nany past state can be returned to at any time. A user can\nreproduce what he or she was listening to at some pointin the past and then continue with other operations from\nwithin that state if so desired.\nThis paper is organized as follows. Section 2 intro-\nduces the functions provided by Musicream and Section 3describes how they are implemented. Section 4 presents\nthe results of experiments on the use of Musicream.\nSection 5 discusses related research and Section 6 sum-\nmarizes this paper’s contributions.\n2 MUSICREAM FUNCTIONS\nMusicream provides four novel functions (Figure 1) to\nsatisfy user desires like “I want to hear something” and\n“I want to hear something my way.” Instead of working asindependent functions, they operate in a cooperative and\ncomprehensive manner to enable new music-listening for-\nmats. Each of these functions is described below.\n2.1 Music-disc streaming function\nThis function presents the user with images of discs, each\ncorresponding to a musical piece in a music collection\n1.\nThese discs ﬂow from top to bottom on the screen one\nafter another, and the user may select a disc and listen to\nthat musical piece. This function is especially useful as a\nmeans of encountering various musical pieces by chance\n1In this paper, “music collection” means any set of musi-\ncal pieces that a user is able to listen to. This would include\ngroups of musical pieces stored on portable music players andpersonal computers as well as those on ﬂat-rate, unlimited mu-sic subscription services.as opposed to making a speciﬁc request as in “I want to\nhear thismusical piece.”\nThe encountering of previously unknown music by\nchance is not without precedent. People often listento music broadcast on radio and television or listen to\nsongs or musical pieces on hit charts or recommendedby friends. Various studies have been made on music-\nrecommendation or playlist-generation schemes [9, 10,\n11, 12, 13, 14, 15, 16] based on collaborative (social) orcontent-based ﬁltering. If a person were to listen to only\nmusical pieces that have been selected by other people or\nsystems, though, such encounters would tend to be pas-sive in nature. At the same time, it is difﬁcult to use a per-\nsonal music collection consisting of (tens of) thousands\nof musical pieces for the purpose of encountering music\nby chance. While music search methods based on bib-\nliographic (catalog) information such as titles and artistnames have been used, they have not considered chance\nencounters with musical pieces\n2. There are also methods\nbased on folder-based hierarchical classiﬁcation by music\ngenre, artist name, etc. that allow a user to search from one\nhierarchical layer to another to reﬁne the search. These\nmethods, however, narrow down searches by promoting\nhierarchical selections such as “jazz ⇒bebop,” which un-\nnecessarily decrease the possibility of unexpected but in-teresting encounters with musical pieces.\nOur music-disc streaming function enables ﬂexi-\nble music encounters not possible through broadcasting,recommendations, traditional searching, or hierarchical\nstructures. Figure 2 illustrates the screen of Musicream\nwith some basic terminology. Each of the three rectangu-lar boxes at the top represents a music-supply “tap” that\nreleases small discs one at a time, each disc correspond-\ning to a musical piece. These discs fall straight down at\nthe same speed (since a tap is fairly wide, released discs\nare spread out crosswise along a tap). The taps are de-\nsigned so that each releases musical pieces of a different\nmood, and the rate of release can be adjusted with the slid-\ners above each tap. A falling disc that reaches the bottom\nof the screen (the “ground” ) disappears. The user may\nremove a disc that appears interesting from these streams\nof falling discs and listen to that musical piece by drag-\n2Although some existing players (such as Apple iPod shuf-\nﬂe) support a random (shufﬂe) playback function, they do not\nallow a user to intentionally control the general nature of musicencounters, which Musicream does allow.\n405Artist\nTitleAdjustment sliderMusic-supply tap\nDisc\nExpanded disc\nMouse pointer\nGround\nFigure 2: Music-disc streaming function: Discs corre-\nsponding to musical pieces stream downwards from three\ncolored taps.\nging the disc using a mouse (or a stylus, touch tablet, etc.).\nRolling the mouse pointer over a small disc expands it and\ndisplays the title and name of the artist for the musicalpiece in question, as shown in the ﬁgure to the left.\nEach tap and disc is given a color that reﬂects the\nmood or feeling of a musical piece (each disc falls outof a tap with a similar color). In other words, similar-\nity in color is associated with similarity in musical pieces,\nwhich means that a user who likes the musical piece of aselected disc can choose other pieces based on the color\nof that disc. While using the music-disc streaming func-\ntion, a user often wants to perform that operation time af-ter time, and this is where the similarity-based sticking\nfunction described next comes in.\n2.2 Similarity-based sticking function\nWith this function, the user takes (picks up) a disc previ-\nously removed from the streaming discs and touches other\ndiscs that are still streaming. This operation has the ef-\nfect of selectively “sticking” (attaching) discs of similar\nmood to the original disc one after another. This function\nmay be viewed as a “query-by-example” search for musi-\ncal pieces, but it is not a search that simply presents a list\nof similar songs. In our similarity-based sticking function,\nthe user collects musical pieces from streaming pieces ac-\ncording to the user’s own choice. The important point hereis that musical pieces can be encountered while the user is\nperforming an operation similar to the way people pick up\nthings they like in real life.\nHere, the “ease of sticking” has been designed to de-\npend on the similarity between two musical pieces interms of mood\n3. As shown in Figure 3, two discs with\nhigh similarity will stick on ﬁrst contact whereas two discs\nwith low similarity will not stick unless brought into con-tact several times. In this way, the range of similarity\nof musical pieces to be added can be easily adjusted by\nmaneuvering the picked-up disc appropriately through thestreams of falling discs.\nSticking discs using this function forms an overlap-\nping series of discs as shown at the left of Figure 4. Thisis called “compact mode. ” Clicking the top disc of this se-\nries rearranges the discs into a non-overlapping, horizon-\n3Instead of mood, which is used in our current implementa-\ntion, any similarity based on content-based or collaborative ﬁl-\ntering can be used for this function.It is easy to stick discs\nwith high similarityIt is difficult to stick discs\nwith low similarity\nFigure 3: Similarity-based sticking function: The “ease\nof sticking” depends on the similarity in terms of mood.\nSimilarity in color is associated with similarity in musicalpieces.\nArtist\nTitleArtist\nTitleArtist\nTitle\nCompact mode Maintenance modeClick\nFigure 4: Two disc-series modes.\ntal series as shown at the right of Figure 4. This is called\n“maintenance mode. ” In this mode, moving the mouse\npointer over a disc expands that disc to make it the focus\nof that series (the discs to either side of the disc-in-focus\nbecome slightly larger). It is also possible to rearrangedisc order in the series or remove a disc from the series,\nas shown in Figure 5.\nIn maintenance mode, the disc components shown in\nFigure 6 appear, thus enabling the following functions.\n•Playback control\nClicking on the playback button on the disc starts\nplayback of that musical piece. During playback, an\nanimated ripple effect emanates from the disc. If the\npiece is played until its end, playback automatically\nchanges to the next underlying disc. Clicking again\non the button stops playback.\n•Playback position control\nThe playback position slider, whose function is the\nsame as that of playback position sliders provided by\nordinary music players, is placed along the periphery\n(circumference) of the disc. One complete loop in\nthe clockwise direction from the twelve o’clock po-\nsition corresponds to the length of that musical piece.\nArtist\nTitleArtist\nTitle\nArtist\nTitleArtist\nTitleArtist\nTitleArtist\nTitleArtist\nTitleArtist\nTitle\nArtist\nTitleArtist\nTitle\nArtist\nTitleArtist\nTitle\nArtist\nTitle\nArtist\nTitleArtist\nTitleArtist\nTitleArtist\nTitle\nArtist\nTitleChange focus Rearrange Remove\n1) Moving mouse\n     pointer\n2) Disc under pointer     is focused1) Dragging disc over\n   neighboring disc\n2) The discs switch     positions1) Dragging disc quickly\n2) The disc is removed     from the series\nFigure 5: Editing operations for a disc series (playlist).\n406Artist\nTitlePlayback position slider Playback pointer\nArtist's name\nMinimize button\nSort-mode buttonTitlePlayback button\nFigure 6: Disc components.\nArtist\nTitleArtist\nTitleArtist\nTitle\nArtist\nTitle\nArtist\nTitleDiscs are sorted\nbased on disc-in-focusOriginal order is preserved\nChange to faint display\nFigure 7: Sort mode: Using the disc-in-focus as a base,\nother discs in the series descend in order of mood similar-\nity.\nClicking on any position along the slider starts play-back from that position in the piece.\n•Minimize\nClicking on the minimize button at the center of the\ndisc switches to compact mode.\n•Sort\nClicking on the sort-mode button on the disc\nswitches to sort mode as shown in Figure 7. Using\nthe disc-in-focus as a base for sorting, a copy of the\nother discs in the series appears below that disc in de-\nscending order of mood similarity. These duplicated\ndiscs disappear upon exiting sort mode.\nIn Musicream, such a disc series is already a highly\nfunctional playlist (i.e., a list of musical pieces that spec-\niﬁes playback order). In existing music players, “zap-\nping” (changing from one musical piece to another in\na playlist as the urge arises) in music playback requires\nthe user to perform a two-step procedure. First, the user\nmust (double-)click the title of the desired selection in the\nplaylist, and second, the user must click on the desired\nplayback position on the playback position slider located\nelsewhere. In Musicream sort mode, in contrast, simply\nmoving the mouse pointer during music playback overother discs in the series automatically starts playing those\nmusical pieces (there is no need for a new playback-start\noperation). In addition, clicking on the playback positionslider on the periphery of a disc right after moving the fo-\ncus enables a series of operations — musical piece selec-\ntion without click and playback position speciﬁcation witha single click — to be performed simultaneously making\nfor the smoothest zapping ever seen.\nSimilarity-based sticking enables a user to pick out\ndiscs from disc streams as desired, create one playlist afteranother, and leave them on the Musicream screen. Facing\nthese multiple playlists might lead a user to think about\na playback order for the playlists. This capability is pro-\nvided by the meta-playlist function described next.\nPlayback proceeds from top\nto bottom (this has priority)Discs at the same height are played from left to right\nArtist\nTitleArtist\nTitle\nArtist\nTitleArtist\nTitleArtist\nTitleArtist\nTitleRipples emanate during playback\nPlayback bar falls\n             from above\nCompact mode isexcluded from playback21\n3\n4\n56\nNumber in      indicates playback order\nFigure 8: Playback rule of meta-playlist function: The\nplayback bar falls from above and plays playlists in orderof contact.\n2.3 Meta-playlist function\nSimilar to rearranging a group of CDs on a (real) desktop,\nthis function enables playlists (multiple series of discs) sit-\nuated on the screen to be rearranged and their order of\nplayback speciﬁed. This is achieved by treating the entireMusicream screen as a meta-playlist and playing back the\nplaylists on the screen in order from top to bottom.\nThe playback pointer for the entire screen used by this\nfunction is simply a straight horizontal line (the “play-back bar”) as shown in Figure 8. The handle at the left\nof the playback bar can be used to drag the bar as desired\n(i.e., upward or downward). Once dragged to a new posi-\ntion, the playback bar starts to drop (automatically mov-\ning to the bottom of the screen) and plays back the series\nof musical pieces in any maintenance-mode or sort-mode\nplaylist (disc series) it comes in contact with. A playlist in\ncompact mode is ignored in this playback.\nFor this function, only positional relationships in the\nscreen’s vertical direction affect playback order. Sincehorizontal relationships have no affect on playback order,\nthe user can make good use of it to achieve ﬂexible play-\nback control. For example, to consider the best playback\norder of several playlists through trial and error, the user\ncan arrange those playlists horizontally and then raise or\nlower each playlist slightly to change their vertical rela-\ntionship and hear the resulting playback order as the play-\nback bar falls. The user may also arrange playlists ac-cording to self-created rules such as placing lively music\non the left and peaceful music on the right. In addition,\nplaylists that the user does not presently wish to hear canbe placed in compact mode and simply left on the screen\n— there is no need to delete them to prevent them from\ninterfering with playback.\nBy allowing free insertion, removal, and rearrange-\nment of groups of musical pieces (playlists) to createmeta-playlists, the meta-playlist function enables playlistediting with the highest degree of freedom yet provided.\nIf this type of switching in units of groups were attempted\non an existing music player, the operations of selectingand inserting multiple musical pieces would have to be\nrepeated any number of times while remembering group\nboundaries. Musicream allows groups of musical pieces\nto be arranged on the screen as individual (small) playlists\nand allows the user to change their playback order while\n407Figure 9: Musicream screen snapshot with the auto-\nplayback switch on: Enabling the auto-playback switchand letting the playback bar fall into the switch activates\nauto-playback.\npreserving their positional relationship in the horizontal\ndirection. This makes intuitive trial-and-error processing\npossible while using positional memory. Playlist editingin this way is not just a method for changing playback or-\nder. It can also be viewed as a creative way of enjoying\nmusic with the user becoming actively involved\n4.\nTo facilitate the use of Musicream for playing back-\nground music, we also prepared an auto-playback modethat enables listening to musical pieces in succession with-\nout requiring user interaction. This mode is entered by\nturning on the auto-playback switch and letting the play-back slider fall into the switch as shown in Figure 9. When\nplayback of the current musical piece ends in this mode,\nMusicream starts to automatically select and play back amusical piece similar in mood to that piece from among\nthe streaming discs on the screen.\nAlthough existing music players include a random\n(shufﬂe) playback function, an abrupt change in mood caneasily occur from one song to the next, such as when a\nwild rock tune follows a romantic ballad. In the auto-playback mode of Musicream, the user is always listening\nto a new order of musical pieces while continuing to listen\nto pieces with a similar feel (a mode that allows listening\nin a format equivalent to conventional random playback is\nalso available).\nA user who encounters new music using the music-\ndisc streaming and similarity-based sticking functions,and who creates playlists by trial and error using the meta-\nplaylist function, is most likely to make the act of listen-\ning to music by Musicream an everyday activity. As a\nresult, musical pieces played on Musicream will be ap-\nproximately equivalent to that user’s listening experience.\nAccordingly, if a user would like to know what music he\nor she was listening to in the past, the user should be able\nto ﬁnd out by investigating what music has been played\non Musicream. The time-machine function described next\n4The importance of playlists can be understood from the way\nthat artists carefully determine song order in their albums and\nfrom the attention given to WWW sites that present personalplaylists (e.g., http://www.artofthemix.org/), for example.makes this possible.\n2.4 Time-machine function\nThis function records all operations performed by the user\non Musicream as well as all screen changes so that theuser can browse this record of the past and return to an\nenjoyable point in time whenever desired. This enables\nthe user to reproduce a past Musicream screen (i.e., recalla past listening state) and continue with operations from\nthat screen as if traveling back in time with a time ma-\nchine. It is also possible to copy and paste a playlist fromthe past screen onto the present screen.\nReproducing music playback order from the past in\nthis way is difﬁcult to achieve on existing music players.\nThis would require that playlists be intentionally saved for\nfuture reference, but this method would only be practical\nif the user knew exactly what playlists would be important\nin the future. Musicream has no functions for saving or\nloading individual playlists because these are not needed\nas long as the time-machine function is available. How-\never, Musicream allows the user to label the current point\nin time with a keyword or sentence that can then be used\nto recall that point later in time.\nWhen the button to use the time-machine function is\npushed, two time-travel sliders are displayed as shown inFigure 10.\n•Rewind slider\nMoving this slider takes Musicream back through\ntime in units of seconds relative to the present. The\nuser can browse the operation history as if he or\nshe was rewinding recorded (video-taped) screen im-ages.\n•Date/time slider\nMoving this slider speciﬁes a particular date and time\n(year, month, day, hour, minute) in the past. The\nslider is colored only for the intervals during which\nMusicream was operating to make it easier to ﬁnd\nand return to a date and time during which Musi-cream was being used.\nThe time-machine function makes it even easier to lis-\nten to music in the present after deciding on a playback or-\nder through trial and error. Because past operations can be\nrecovered at any time, the user need not hesitate to throwaway a musical piece picked out through the music-disc\nstreaming function when thinking there might be a better\npiece available. While it is difﬁcult to intentionally reen-\ncounter musical pieces from the past as they stream and\nfall from the taps in the present, it is easy to return to apast encounter and pick them up there.\nWe also provide two functions to support the recall-\ning of the past through the time-machine function. Evenif speciﬁc information, such as date/time, title, etc., cor-\nresponding to a past Musicream screen cannot be remem-\nbered, these functions enable searching to be done whilerecalling musical pieces listened to in the past and their\nscreens.\n•Playback history search\nStarting with a certain musical piece, this function\nsearches for past screens where this piece was lis-\ntened to.\n408Figure 10: Musicream screen snapshot of the time-\nmachine function: The time-travel sliders consist of arewind slider (left) for travel back into time in units of sec-\nonds relative to the present and a date/time slider (right)\nfor specifying particular dates and times to return to.\nThe results of playback history search and past similarity\nsearch are displayed when a disc is clicked.\n•Past similarity search\nFrom among all musical pieces listened to in the past,\nthis function searches for past screens where musical\npieces similar in mood to the starting musical piecewere listened to.\nThe results of both of these functions are simultaneously\ndisplayed by clicking a musical piece in the time machine\noperation as shown in Figure 10.\n3 IMPLEMENTATION\nImplementing Musicream requires preprocessing of each\nmusical piece in a music collection and implementation\nof the Musicream interface which is executed with soundﬁles of musical pieces in the MPEG Audio Layer 3 (MP3)\nformat and a music-catalog ﬁle including the results of\npreprocessing. Although our current implementation sup-ports only MP3 sound ﬁles stored on a local hard disk,\nMusicream can be easily applied to online musical pieces\nand any music subscription services.\n3.1 Preprocessing\nA music-catalog ﬁle in XML format is generated from\nMP3 sound ﬁles in a music collection. The ﬁle includes ti-\ntle, artist name, name of the MP3 sound ﬁle, feature vector\nfor computing music similarity, and associated disc colorfor each musical piece. The feature vector is extracted\nfrom each musical piece and the disc color (hue and satu-\nration) is assigned on the basis of the feature vector.\nAlthough we can use any feature vector designed for\ncomputing music similarity, in our current implementa-tion we use a 30-dimensional feature vector described\nin [1], which is obtained by automatically analyzing the\nmood of each musical piece and is conﬁrmed as effectivefor genre classiﬁcation. It consists of the mean and vari-\nance of local spectral features (centroid, rolloff, ﬂux, andzero-crossings) across the entire musical piece (eight di-\nmensions); average values of mel-frequency cepstral co-\nefﬁcients (MFCC) across the entire musical piece (tendimensions); portion of the musical piece occupied by\nlow-energy intervals (one dimension); pitch content fea-\ntures reﬂecting periodicity in pitch (ﬁve dimensions); andrhythmic content features reﬂecting periodicity in beat\n(six dimensions). The feature vectors for all sound ﬁles\nare extracted using Tzanetakis’s MARSYAS [17], a soft-ware framework for audio analysis and synthesis.\nThe disc color (hue and saturation) is determined from\nthe color circle whose circumference and radius corre-\nspond to hue and saturation, respectively. Each musi-\ncal piece is projected into the circle according to its fea-\nture vector. Principal component analysis (PCA) is used\nto reduce the dimensionality of feature vectors to a two-\ndimensional vector on a plane. The planar coordinates\nconsisting of the ﬁrst and second principal components\nare converted to polar coordinates, and then angle θis as-\nsigned to hue and distance rfrom the origin is assigned to\nsaturation.\n3.2 Implementation of the Musicream interface\nThe Musicream interface is implemented using Macro-\nmedia Flash MX Professional 2004. It takes the music-\ncatalog ﬁle as input and provides the four functions of\nMusicream.\nFirst, for the music-disc streaming function, the hue\nangleθis divided into three equal sections of 120 degrees\neach of which is assigned to one of the three music-supply\ntaps. Any one tap can release only those discs havinghue values in the range assigned to that tap. The number\nof taps can be changed by assigning angle θin different\nways. The interval of disc release can be adjusted by a\nslider control in a range from 3 to 10 seconds, which is\ndetermined so that discs do not overlap when streaming\ndiscs are expanded.\nNext, for the similarity-based sticking function, the\nease-of-sticking between two discs is determined on the\nbasis of the hue angle θof each disc. If the difference be-\ntween θof one disc and θof the other lies in the range\nfrom 0 to 30 degrees, the two discs will stick on the ﬁrst\ntry. For a difference of 30 to 80, 80 to 130, and 130 and\ngreater degrees, the two discs will stick on the second,\nthird, and fourth tries, respectively.\nFor playlist sort mode, in which discs in a series are\nsorted in order of similarity and hung from the base disc,the degree of similarity between two discs is deﬁned interms of the cosine angle (scalar product) between the 30-\ndimensional feature vectors of those discs.\nFinally, the time-machine function is achieved by con-\ntinuously recording at one-second intervals the snapshot\ninformation needed to reproduce past states (all discs on\nthe screen, each tap’s internal state, date and time, etc.).\nThough it depends on the number of discs on the screen,\na memory capacity of about 10 Mbytes is needed to save\nthis information in XML format for one hour of Musi-\ncream use (there is a lot of room here, however, for com-\npression).\n4094 EXPERIMENTAL RESULTS\nWe operated Musicream using a music collection that in-\ncluded all 315 musical pieces of the RWC Music Database\n[18]5and 1572 popular songs which appeared on Japanese\nhit charts from 2000 to 2004. We found that the proposed\ninterface functioned effectively and that the four Musi-\ncream functions working in combination provided an ac-\ntive music-listening experience that was unique. When\nﬁrst using Musicream, users typically tried the similarity-\nbased sticking function after ﬁnding a musical piece that\nthey liked in the stream of discs. After becoming more fa-\nmiliar with Musicream, though, instead of ﬁrst determin-\ning whether they liked each musical piece they picked out,users went directly to sticking many other musical pieces\nto the piece they picked out and then entered sort mode\nwhere they would listen to those pieces through “zapping”and then decided which ones they liked. This was because\nthey found that even if they did not like the ﬁrst piece se-\nlected, they could still unexpectedly come across one that\nthey liked through sticking. Furthermore, by listening to\na group of musical pieces obtained through sticking, users\ncould understand what kind of music they might like to\nhear at that time.\nTo further analyze the advantages of Musicream, we\nconducted a user study with 27 subjects (16 male, 11 fe-male) who were not familiar with the music collection\nused here (to this end, we used musical pieces of only\nthe RWC Music Database for this experiment). To eval-\nuate their subjective assessment of Musicream and have\nthe subjects gain a good command of Musicream, each\nsubject was asked to complete a subjective questionnaire\nafter freely using Musicream for ﬁve minutes, excluding\nthe time for receiving brief instructions. The question-\nnaire results indicated that more than 81.5% of the sub-\njects rated each of the four functions as interesting (in par-\nticular, 92.6% found the similarity-based sticking func-tion interesting), that 92.6% of the subjects thought the\nmusic-disc streaming function was an effective way to en-\ncounter unfamiliar musical pieces such as those availablethrough ﬂat-rate, all-you-can-hear music subscription ser-\nvices, and that 96.3% of the subjects wanted to use Mu-\nsicream in the future. We also found that Musicream was\neasy enough to use without long training.\nWe then had each subject use both Musicream and\nan ordinary music player with conventional playback but-tons and a playlist editor on the same three tasks — lis-\ntening through the “zapping” operation, editing playlists,and ﬁnding music — and complete a subjective question-\nnaire comparing their operation. The number of subjects\nwho rated the zapping operation on Musicream as moreconvenient was double that who found it less convenient,\nwhile the ease of the zapping operation was rated almost\nequally (Musicream zapping scored slightly better). Theresults also indicated that 96.3% of the subjects thought\nthat the meta-playlist function on Musicream was easy\nto use and convenient compared to conventional playlist\n5Note that Musicream can support operations using a much\ngreater number of musical pieces. It should be highly suitable\nfor all-you-can-hear music-listening environments, like ﬂat-ratemusic subscription services, as well as personal music collec-tions.editing when the playback order were rearranged several\ntimes in units of groups of musical pieces. For browsingunfamiliar musical pieces to ﬁnd a musical piece having a\ncertain mood (e.g., uplifting music), the results indicated\nthat the number of subjects who thought Musicream was\nmore convenient, made music easier to ﬁnd, and was more\nenjoyable to use than an ordinary music player was, re-\nspectively, 3.6, 3.8, and 5.75 times the number who ratedthe ordinary music player more highly.\nThese results showed that Musicream was an effective\nway, through its four convenient functions, to enable usersto browse music collections to ﬁnd unfamiliar but interest-\ning musical pieces.\n5 RELATED RESEARCH\nTo go beyond conventional interfaces based on biblio-\ngraphic information or query-by-example retrieval, sev-\neral interesting approaches for browsing music collectionshave been reported.\nFor example, Tzanetakis et al. [19] developed the\n“GenreSpace” interface for browsing music collectionsin a three-dimensional space into which musical pieces\nare projected according to their similarity; they also de-\nveloped the “GenreGram” tool for displaying, along withreal-time audio input, several up-and-down cylinders cor-\nresponding to different genres. Tzanetakis et al. [20] have\nalso developed other interfaces, such as “Sound Sliders”\nwhich provides continuous aural feedback of retrieved\npieces while a user moves sliders of music properties suchas tempo and beat strength.\nWith an emphasis on visualization, Pampalk et al.\n[21] reported an interface featuring self-organizing maps\n(SOMs) that projects musical pieces onto a plane. They\nused a metaphor of “islands” that represent self-organized\nclusters of similar pieces. Van Gulik et al. [22] reported an\n“artist map” interface with the focus on artists and small\ndevices. It enables users to explore and discover music\ncollections on small devices by projecting artists into a\ntwo-dimensional space. Artists are drawn as dots in the\nspace so that similar artists are placed close together on\nthe basis of a modiﬁed spring-embedder algorithm. Tor-\nrens et al. [23] reported visualization techniques where\nmusical pieces are placed in a circle, rectangle, or tree-\nmap by using metadata of sound ﬁles without analyzing\naudio signals.\nWhen visual information related to musical pieces is\navailable, a “collaging” technique proposed by Bainbridgeet al. [24] is also an effective way to provide leisurely,\nundirected interaction with a music collection. In Musi-cream, visual information like jacket covers can also be\nused effectively by displaying a jacket cover image on or\nnear each music disc when such information is available.\nAlthough most of the above approaches share the same\ngoal of enabling a non-speciﬁc music search to satisfyuser desires like “I want to hear something”, Musicreamis the ﬁrst interface that supports the four functions de-\nscribed in Section 2. In particular, the time-machine func-\ntion is unique and has great potential. While the con-\ncept of time-machine computing itself was proposed for a\ncomputer desktop [25], our research is the ﬁrst to discuss\n410time-machine computing for music. In Musicream, the\nability to reproduce all operations from the past producesan even greater time-travel effect. For example, when a\nuser returns to the past and sticks a series of similar musi-\ncal pieces that were not previously selected (thereby cre-\nating a new future), the musical pieces streaming on the\nscreen revert to those that were streaming at that time.\nThis makes it possible to unexpectedly encounter musicthat was popular at that point in the past.\n6 CONCLUSION\nWe have described a rich, integrated music playback in-terface called Musicream that enables seamless opera-\ntion of four novel functions: “music-disc streaming, ”\n“similarity-based sticking, ” “meta-playlist, ” and“time-\nmachine. ” The ﬁrst two functions satisfy the user desire\nexpressed as “I want to hear something” and the last two\nfunctions satisfy the desire expressed as “I want to hear\nsomething my way.” The main contribution of Musicream\nis to provide a novel music-listening environment that en-\nables a user to interact with a huge music collection inactive, ﬂexible, and creative ways, which go beyond tradi-\ntional techniques of music information retrieval.\nAlthough the basic concept of Musicream has great\npotential, we have not fully exploited it. In the future, forexample, we could provide various specially designed mu-\nsic taps such as a tap streaming musical pieces by a spe-\nciﬁc artist (e.g., an artist name can be typed on the tap), a\ntap streaming music on a hit chart at a certain time in the\npast or currently, and a tap streaming music selected by\na famous artist or celebrity. It is also interesting to share\nthe screen (i.e., the playlist) of Musicream with friends or\nanonymous users via the Internet. If they use the same\nﬂat-rate music subscription service, as well as seeing the\ntitles on the screen other users can freely listen to those\npieces. For the “zapping” operation, we found it useful to\ncolor chorus (“hook”) sections in the playback position\nslider (at the disc periphery) as done in SmartMusicK-\nIOSK [26]: these chorus sections can be automatically\ndetected through the RefraiD method [27].\nNote that the concept of Musicream is also indepen-\ndent of similarity measures and dimensionality reduc-\ntion techniques. We plan to use another similarity mea-\nsure based on content-based or collaborative ﬁltering andanother dimensionality reduction technique such as the\nself-organizing map (SOM). Since Musicream is comple-\nmentary to conventional music information retrieval tech-niques, their seamless integration should be the next goal.\nFuture work will also include applying Musicream as an\ninterface for commercial online music subscription ser-\nvices.\nREFERENCES\n[1] George Tzanetakis and Perry Cook. Musical genre classiﬁ-\ncation of audio signals. IEEE Trans. on Speech and Audio\nProc. , 10(5):293–302, 2002.\n[2] Jean-Julien Aucouturier and Francois Pachet. Music simi-\nlarity measures: What’s the use? In Proc. of ISMIR 2002 ,\npages 157–163, October 2002.\n[3] Jouni Paulus and Anssi Klapuri. Measuring the similarity\nof rhythmic patterns. In Proc. of ISMIR 2002 , pages 150–\n156, October 2002.[4] Jonathan Foote, Matthew Cooper, and Unjung Nam. Audio\nretrieval by rhythmic similarity. In Proc. of ISMIR 2002 ,\npages 265–266, October 2002.\n[5] Simon Dixon, Elias Pampalk, and Gerhard Widmer. Clas-\nsiﬁcation of dance music by periodicity patterns. In Proc.\nof ISMIR 2003 , pages 159–165, 2003.\n[6] Martin F. McKinney and Jeroen Breebaart. Features for\naudio and music classiﬁcation. In Proc. of ISMIR 2003 ,\npages 151–158, 2003.\n[7] Wei-Ho Tsai, Hsin-Min Wang, Dwight Rodgers, Shi-Sian\nCheng, and Hung-Ming Yu. Blind clustering of popular\nmusic recordings based on singer voice characteristics. InProc. of ISMIR 2003 , pages 167–173, 2003.\n[8] Elias Pampalk. A MATLAB toolbox to compute music\nsimilarity from audio. In Proc. of ISMIR 2004 , pages 254–\n257, 2004.\n[9] Upendra Shardanand and Pattie Maes. Social information\nﬁltering: Algorithms for automating “word of mouth”. InProc. of CHI ’95 , pages 210–217, 1995.\n[10] William W. Cohen and Wei Fan. Web-collaborative ﬁlter-\ning: Recommending music by crawling the web. In Proc.\nof WWW9 , 2000.\n[11] Alexandra Uitdenbogerd and Ron van Schyndel. A review\nof factors affecting music recommender success. In Proc.\nof ISMIR 2002 , pages 204–208, October 2002.\n[12] Masoud Alghoniemy and Ahmed H. Tewﬁk. A network\nﬂow model for playlist generation. In Proc. of ICME 2001 ,\n2001.\n[13] Steffen Pauws and Berry Eggen. PATS: Realization and\nuser evaluation of an automatic playlist generator. In Proc.\nof ISMIR 2002 , pages 222–230, October 2002.\n[14] Beth Logan. Content-based playlist generation: Ex-\nploratory experiments. In Proc. of ISMIR 2002 , pages 295–\n296, October 2002.\n[15] Jean-Julien Aucouturier and Francois Pachet. Scaling up\nmusic playlist generation. In Proc. of ICME 2002 , 2002.\n[16] Beth Logan. Music recommendation from song sets. In\nProc. of ISMIR 2004 , pages 425–428, 2004.\n[17] George Tzanetakis and Perry Cook. MARSYAS: A frame-\nwork for audio analysis. Organized Sound , 4(3):169–175,\n2000.\n[18] Masataka Goto. Development of the RWC music database.\nInProc. of ICA 2004 , pages I–553–556, 2004.\n[19] George Tzanetakis, Georg Essl, and Perry Cook. Auto-\nmatic musical genre classiﬁcation of audio signals. In\nProc. of ISMIR 2001 , pages 205–210, October 2001.\n[20] George Tzanetakis, Andreye Ermolinskyi, and Perry Cook.\nBeyond the query-by-example paradigm: New query inter-faces for music information retrieval. In Proc. of ICMC\n2002 , pages 177–183, September 2002.\n[21] Elias Pampalk, Simon Dixon, and Gerhard Widmer. Ex-\nploring music collections by browsing different views. In\nProc. of ISMIR 2003 , pages 201–208, 2003.\n[22] Rob van Gulik, Fabio Vignoli, and Huub van de Weter-\ning. Mapping music in the palm of your hand, explore and\ndiscover your collection. In Proc. of ISMIR 2004 , pages\n409–414, 2004.\n[23] Marc Torrens, Patrick Hertzog, and Josep-Lluis Arcos. Vi-\nsualizing and exploring personal music libraries. In\nProc.\nof ISMIR 2004 , pages 421–424, 2004.\n[24] David Bainbridge, Sally Jo Cunningham, and J. Stephen\nDownie. Visual collaging of music in a digital library. In\nProc. of ISMIR 2004 , pages 397–402, 2004.\n[25] Jun Rekimoto. Time-machine computing: A time-centric\napproach for the information environment. In Proc. of\nUIST ’99 , pages 45–54, 1999.\n[26] Masataka Goto. SmartMusicKIOSK: Music listening sta-\ntion with chorus-search function. In Proc. of UIST 2003 ,\npages 31–40, 2003.\n[27] Masataka Goto. A chorus-section detecting method for\nmusical audio signals. In Proc. of ICASSP 2003 , pages\nV–437–440, April 2003.\n411"
    },
    {
        "title": "Music Information Retrieval, Memory and Culture: Some Philosohpical Remarks.",
        "author": [
            "Cynthia M. Grund"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415626",
        "url": "https://doi.org/10.5281/zenodo.1415626",
        "ee": "https://zenodo.org/records/1415626/files/Grund05.pdf",
        "abstract": "The burgeoning field of Music Information Retrieval (MIR) raises issues which are of interest within traditional areas of discussion in philosophy of music and of philosophy of culture in general. The purpose of this paper is twofold: the first goal is to highlight and briefly discuss a selection of these issues, while the second is to make a case for increased mutual awareness of each other on the parts of MIR and of humanistic research. Many traditional debates within the latter receive infusions of new perspectives from MIR, while research within MIR could be fruitfully pointed in directions suggested by questions of interest within traditional research in the humanities, e.g. the relationship of individual memory to cultural memory, issues regarding crosscultural understanding and the importance of authenticity in artistic contexts. Keywords: Philosophy of music, culture, memory, ethics, authenticity 1 INTRODUCTION",
        "zenodo_id": 1415626,
        "dblp_key": "conf/ismir/Grund05",
        "keywords": [
            "Music Information Retrieval",
            "Philosophy of music",
            "Philosophy of culture",
            "Mutual awareness",
            "Humanistic research",
            "Traditional debates",
            "New perspectives",
            "Crosscultural understanding",
            "Artistic contexts",
            "Ethics"
        ],
        "content": "MUSIC INFORMATION RETRIEVAL, MEMORY AND CULTURE:  \nSOME PHILOSOPHICAL REMARKS\n Cynthia M. Grund  \n Institute of Philosophy, Education and the Study of Religions \nPhilosophy \nUniversity of Southern Denm ark \nOdense, Denm ark \ncmgrund@filos.sdu.dk   \nABSTRACT \nThe burgeoni ng fi eld of Music Inform ation Retrieval \n(MIR) raises issu es wh ich are of interest within tradi-\ntional areas of di scussi on in philosophy  of music and of \nphilosophy  of culture in general . The purpose of t his \npaper is twofold: the first goal  is to highlight and bri efly \ndiscuss a selection of t hese issues, whi le the second i s to \nmake a case for increased mutual awareness of each \nother on t he part s of M IR and of humanistic research. \nMany traditional debates with in the latter receive infu-\nsions of new perspect ives from  MIR, whi le research \nwithin MIR coul d be frui tfully pointed in directions sug-\ngested by questions of interest with in traditional research  \nin the humanities, e.g . the relatio nship of individual \nmemory to cultural m emory, issues regarding cross-\ncultural underst anding and t he importance of authentic-\nity in  artistic co ntexts.   \nKeyw ords: Philosophy  of m usic, cul ture, m emory, eth-\nics, au thenticity \n1 INTRODUCTION \n \nEarlier this sem ester (spri ng 2005) I i ncluded a sect ion \non music in formation retriev al in an invited lectu re on \nmusic and m eaning whi ch was part  of a semester-long \nseries of lectures on Sem iotics an d Mean ing at th e Insti-\ntute of Language and C ommunication at  the University \nof Sout hern Denm ark (SDU) at  Odense, sponsored by \nSemioNet , The Net work for St udies in Semiotics and \nMeaning (SDU), Odense.  I am  an Associate Professor \nof Philosophy at SDU in th e Institute of Philosophy, \nEducat ion and t he Study of R eligions.  I provi de this \nadmittedly anecdotal pream ble to underscore the solidly \nhumanities-b ased context. As p art of the illu strativ e ma-\nterial in the PowerPoi nt present ation, I used a segm ent of \nstream ing video from BBC W orld’s Click  Online, Au -\ngust 1, 2002 (Eddo, 2002)  (t hus al ready  nearl y three \nyears ol d) descri bing the tune-t agging mobile telephone \nservice called Shazam  that was t hen bei ng launched i n the UK. M y spring 2005 audi ence – as has been t he case \nwith virtually ev ery h umanities-o riented audience I have \nencount ered si nce I began addressi ng M IR-related issues \nin talks som e three y ears ago – had never heard of MIR. \nI suppl emented the video segm ent with an overvi ew of \nthe vari ous ki nds of work whi ch had been pursued in \nMIR since I was introduced to  the field at two cross-\ndisciplinary conferences at SDU i n 2002 sponsored by  \nNetværk for Tvæ rvidenskabel ige Studier af M usik og \nBetydning/The Network for C ross-Di sciplinary Studies \nof M usic and M eaning (NTM SB).1\nDuring the quest ion-and-answer sessi on after the con-\nclusion of the three-hour l ecture, t he quest ion from  the \nfloor which sparked most discussion related  to the BBC-\nclip on Shazam.  The issues raised nearly all had to do \nwith the consequences t his and related technol ogy (i.e. \nMIR) woul d have for our percept ion and use of human \nmemory with regard t o music and sonic phenom ena, and \nthe broader i mplications for hum an cul ture in general . \nThese issues – and related ones – are the issues to which \nI will n ow turn. \n2 THE ISSUES \nThe speci fic quest ion whi ch launched the discussi on \ndealt with what  the sort  of m usic mining represent ed by \nShazam woul d mean for us as i ndividuals:  \n \n(*) Does the ability to acce ss music inform ation in \nthis fashion som ehow im poverish the quality of our con-\nnectio ns with  music an d the connectio ns music facili-\ntates between an individual and her fel low hum an bei ngs \nby – additionally –  eroding our abilities to  concentrate, \nget inform ation from  our envi ronm ent into our m emories \nand contextualize it, so we can rem ember it?   \n \nAs will b ecome evident in what fo llows, m y consid-\nered position is that this is n ot the case, th at MIR in  its \nvarious aspects will o nly resu lt in – on balance – an im-\nprovem ent with regard t o music-rem embering, bot h on \nan individual and on a cul tural level. This quest ion, \nmoreover, provi des an el egant  way  to introduce and \ncomment on t he fol lowing rel ations between MIR, on \nthe one hand, and several  issues of rel evance t o philoso-\nphy and phi losophy  of hum an cul ture on t he other: Permission to make digital or hard  copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies  bear this  notice and the full\ncitation on the first page. \n© 2005 Queen Mary , University  of London  \n \n1 I am particular ly grateful to E sben Skovenbor g for drawing m y atten-\ntion to Shazam . My thanks also to M arc Leman and Declan Thomas \nMurphy for fine intr oductor y insights into M IR back in 2002- 2003.  \n8   \n \n                                                           - The advent of intramedial external interactivity \n(defined in what follows) with regard to musical \nmaterials occurs with music mining and related areas of MIR.  \n- In the long run, MIR can contribute to the discus-\nsion of the role and philosophical significance of memory with regard to questions of memory and of metaphysics in the debate about the ontology of musical works. MIR may well also be able to contribute to the growing field of research which regards a better understanding of memory as be-\ning a key subtext for understanding human cul-\nture.  \n- Continued striving within MIR research to find \nout how people categorize music and thus will be \nled to search for it yields insights as to how vari-ous cultures hear their own music and provides a portal through which non-members of a culture also can be party to these insights. \n- MIR can introduce new aspects into discussions \nof authenticity as a factor to be taken into account in philosophy of performance and reception. \n- New ethical issues arise with regard to MIR and \nsome traditional ones are put into new and chal-lenging perspectives. \nThere are a multitude of other issues which also could be brought to the fore, but in the interest of focus and brev-ity, these will suffice for the time being. \n2.1 Intramedial external interactivity \n2.1.1 The Distinction Between Internal Interactivity \nand External Interactivity \nQuerying by humming, singing, playing and the like has, of course, gone on throughout the history of humankind. In order to determine the name of the tune one had run-ning through ones head, it was necessary to be lucky enough to encounter another human being who hap-pened to recognize the tune which was reproduced and, in addition, who knew its name or could identify it some \nway. For lack of a better term, I have chosen to call this \ninternal interaction , signalling the fact that a tune run-\nning through the consciousness of one human agent was then reproduced by this agent in order to run it by an-other human agent, who, after listening to it, then could identify it. Key steps in this process involve the interme-diation of conscious, intentional human behavior.  \nContrast this now with what is going on in something \nlike the Shazam example. Here, “you can pull out your \nmobile phone, which you already carry with you every-\nwhere, and you can dial f our digits, wait fifteen sec-\nonds, and immediately found [sic] out the name of the artist, the album and the song” (Chris Barton in Eddo, 2002). In the querying situation at hand no person has to \nremember the melody in order to reproduce it for the \npurposes of querying, and no person has to recognize it in order to identify it. Here, external interaction  is an \nappropriate term, in that in the interaction involved in the querying – all except the action of initiating the query itself – takes place out side of any human con-\nsciousness.\n2\nThere are, of course, examples of mixed inter-\nnal/external interaction: standard query by humming in \nmusic mining involves the ability of a human agent to reproduce to some extent or other a tune, which then is identified by an algorithm. \nThis distinction becomes most interesting when \npaired with the following one: \n2.1.2 The Distinction Between Intermedial Reference \nand  Intramedial Reference \nWhen referring to music, particularly in order to access it in order to perform, intermediality  is what has charac-\nterized the referential relationships between played, sounding music and score, chord chart, or whatever. Granted, there has been limited intramedial reference on \nan internal mnemonic level when, for example a conduc-tor of an orchestra hums the portion she wants to hear \nagain in rehearsal or in the sort of pre-MIR query-by-\nhumming traditionally practiced among human agents.  \n2.1.3 The Upshot \nWhat I think is interesting to think about in the music-\nmining case as it is exemplified by Shazam is that here \nwe are crossing over into the realm of intramedial exter-\nnal interactivity . Music is used to find information about \nmusic, without the need for any of the music to be in any way represented or internally “heard” in anyone’s con-sciousness.  \nOn the face of it, this woul d seem to feed the fears \nwhich might be seen to be lurking behind the question labelled (*) in the foregoing. Some afterthought will reveal, however, that this potential for finding and iden-tifying music when we don’t have access to other hu-man beings who can help us will only contribute to en-hanced possibilities for us as human agents to get hold \nof accurate information. We will thus be better enabled \nto find out what we want to know in an increasingly complex musical/sonic environment in which we are exposed to a heretofore unheard plurality of music and musical styles, so that we can resolve our own queries \n \n2  An anonymous reviewer has remarked: “ ‘External’ is typically used \nto refer to person-to-person interactions,” referring to work by Hüls-\nmann as well as Moore and Kearsley. S/he writes: “. . . the author \nseems to use terminology in a manne r inconsistent with the literature, \nsince she uses the term “external in teraction” to refer to a ‘learner-\ncontent’ or ‘learner-resource media’ relationship.” To avoid any further misunderstandings among readers regarding my use of the term “exter-\nnal interaction,” I would like to point out that it is my own invention in \nthis context, and – in this example – it refers to the interaction taking \nplace between the mobile phone as the mediating presenter of sonic \ninformation to the database of digita lized music which is then subjected \nto an algorithm-based search for matc hes. At no time in this scenario \ndoes any human agent need to repr oduce, remember or in any other \nway relate consciously to the music, save to be spurred by it to hold up \nthe mobile phone and to subsequently check the information yielded by the phone. \nI would like to take this opportunity to thank all four anonymous \npeer-reviewers for suggestions regard ing conceptual clarification and \ncopy editing. \n \n9   \n \n                                                          and find our own personal “tag s.” Bo th the quantity and \nquality o f what we will b e able to remember musically is \nenhanced. \n2.2 Memory, metaphysics and culture \n2.2.1 Memory and Met aphysi cs \nIn his very recent Philosophy of  Musi c: An Introduct ion, \na book solidly based in traditional, hum anistic treatm ents \nof the topic, and whi ch can be recom mended bot h to \nthose wh o are fam iliar with  the issu es dealt with  in the \nphilosophy  of m usic and t hose who are new to the field, \nR.A. Sharpe writes:  \nYou cannot  reduce a work ei ther to the class of \nits p erformances o r to the possibility of a per-\nform ance. The work is not the sam e as a per-\nform ance. Let ’s begi n with a few truisms. A work \ncan exi st unperform ed, as l ong as t he music has \nbeen written  out and preserv ed in a lib rary o r a \nstudy. It might, as well, be rem embered accu-\nrately by somebody  even i f no not ated copy  ex-\nists. W hile it is rem embered, it exists. If the last \ncopy  is destroyed it cont inues t o exist as long as \nthe last person to remember it can  do so. After \nthat th e wo rk is lo st. (W hether it is rig ht to say \nthat it no longer exi sts is som ething we shal l con-\nsider short ly.) (Sharpe, 2004, p. 59) \nThe “som ething we shal l consi der short ly” is, of course \nwhet her or not a musical work m ight have som e sort of \netern al existen ce as Plato nic entities. It would be quite \nastonishing if MIR – or any thing el se, for t hat matter, \ncould end up act ually provi ding us wi th tools for resol v-\ning m etaphysical quandari es of t his latter sort . \nTo the extent, however, that  MIR can enlarge, refine \nor otherwi se give us new t wists on what  is involved i n \nremembering a piece of m usic, the foregoing quote \nabout the relationship of existence of a piece of m usic in \nthe sense of it being rem embered reveals at least one \npoint of rel evant cont act between M IR and philosophi z-\ning about  music. Al though t his concern from  the hu-\nmanities sid e of the fen ce, b y virtue of its v ery m eta-\nphysical nature, m ay not be the sort  of thing that causes \nthe vast m ajority o f research ers with in MIR to  lose \nsleep at night, there are issu es deal ing wi th memory and \nmusic which have i ntrigued and perpl exed t hose who \nphilosophi ze and theorize about  music whi ch are wel l-\nsuited to examination by means of the tools afforded \nwithin MIR.  For exam ple, Sharpe cont rasts the proc-\nesses of suppl ying missing inform ation in a spoken \nconversat ion wi th com pletion of a m usical phrase:  \nBut with  music it is th e syn tactic featu res alo ne \nthat enabl e us to suppl y what  is missing, for t here \nis no sem antic element. Different  notes and \nchords do not  approxi mate to nouns, adject ives, \npreposi tions or pronouns. If we at tempt a gl os-\nsary of music, we will tak e either wh ole phrases \nand whol e harm onic sequences or not es in rela-\ntion to the tonic and give them  a character of the \nsort I have descri bed a bove. Thus we m ay think of rising major sequence as opt imistic or a chro-\nmatic descendi ng passage as sad. B ut the syntac-\ntic elem ent that en ables m usic to  be followed is \ncrucial to  the way we v alue it. Mu sic th at is \nmerely a concatenation of agreeable sounds does \nnot have the sam e appeal. The very memorability \nof music co nnects with  this cap acity to  be fol-\nlowed (Sharpe, 2004, p. 94). \nSince th e ability o f a human agent to remember at least \nsome sonic, non-textual aspects of a piece of m usic is \ncrucial in m any search contexts – exceptions are, of \ncourse, t hose i nvolving intramedial external interactiv-\nity, on t he one hand, and t hose i nvolving search in terms \nof title an d the like, on the other – this is also  an area of \nrelev ance to MIR. Sin ce MIR is rep lete with  tools which \ncan compare huge assort ments of soni c dat a with which \nreact ions on t he part  of test subject s have been associ -\nated, it shoul d onl y be a m atter of t ime, creat ivity and \nexperi ment desi gn before M IR can provi de us with sub-\nstantial insights into how we rem ember musical m aterial \nand how m emory of m usical material is related to other \nsorts of m emory.  \n2.2.2 Memory and C ulture \nMIR-based research into features which m ake a piece \nof m usic more or l ess memorable for an individual \nshould, in turn, be able to  shed light on the m ultifaceted \nuse of m usic in a wi de variety of cultural contexts. \nThere is a growi ng am ount of research now bei ng done \non the ways in which many elements of hum an cul ture \nare i ncreasi ngly being underst ood as root ed in pre-\nliterate contexts as part of our constant struggle to re-\nmember a past which is always receding and eluding us, \nboth on an i ndividual and on a collective, cultural level.3 \nThe tools wh ich MIR b rings to the tab le with  regard to \nisolating and analyzing m usical features wi th regard t o \nthe matter o f memorability in  larg e co rpora of sonic \nmaterial within and acro ss cu ltures can  be utilized  to \nprovi de concret e, empirically-support ed insights into a \nlarge and ot herwi se unwi eldy area of i nquiry.  \n2.3 Cross-cul tural insights \nThe study of how vari ous groups and cul tures cat egori ze \nthe music in which they are interested (or not!) becom es \na cardi nal point in the devel opment of m ore sophi sti-\ncated tools for MIR. An i mportant side effect  of t his \nresearch i s increasi ng refi nement of our insights into \nhow di fferent  groups approach t heir music. 4\nThis is, o f course, in terestin g in the trad itional phi-\nlosophi cal debat e with regard t o “pure” m usic, where \nthe work of interpretatio n is n ot short-circu ited by the \npresence of a t ext. Aft er a fi ne di scussi on of how we \ntalk about  music, R.A .Sharpe – once agai n – wri tes: \n \n3 For fascinating insights into the wa y in which the need to remember \ncan provide an explanation for  behavi or for which it is other wise ver y \ndifficult to give an account,  see W hitehouse,  2000.  \n4 Debopam , Pappu,  and Pr abhakar , 2004,  is a fine exam ple of this.  \n \n10   \n \n                                                          In at tempting to show how expressi ve de-\nscriptions becom e attached to m usic I have made \nmuch of the fact that what we m ay call “concert \nmusic for instrum ents only” is com paratively re-\ncent and com paratively restricted to Western and \nIndian cultures. Music h as been and still larg ely \nis a m ixed-m edia affai r. . . . For m ost of its his-\ntory, m usic either set te sts or was an accom pani-\nment for dance and i t may well be that purel y in-\nstrumental music woul d not  have t he effect  it has \nif we were not  brought  up i n a cul ture where our \nfirst experi ence of music is likely to be t hrough \nsinging and bei ng sung t o and m oving in time to \nmusic. Perhaps t his is where we shoul d look for \nthe primitive basis fo r our expressiv e descrip tions \nof music (Sharpe, 2004, p. 107). \nThe enorm ous databases of di gitalized m usic, much of \nwhich consists o f music with  lyrics, are indeed, “mixed-\nmedia affai rs.” The work current ly being done by \nStephan B aumann, Ti m Polhe, Vem bu Shankar, Beth \nLogan, Dan Yang and Wonsook Lee5is of great in terest \nto any philosopher interest ed in how traditional con-\ncerns within m usic-and-m eaning studies can increas-\ningly be placed within em pirically-based contexts as \nMIR cont inues t o devel op. \n2.4 Authenticity \nQuest ions of aut henticity are oft en lurking beneat h the \nsurface in m atters of musical perform ance, be it with \nregard t o interpret ation of score, i nstrumentation or any  \nnumber of related  issues. As will b e remarked in the next \nsection, M IR raises som e new, i nteresting and not  al-\nways unprobl ematic issues i n this regard. There are, \nhowever, respect s in whi ch MIR and related technol o-\ngies, can give us access to realities which, by their very \nnature, only can be virtual. I will o nly name one example \nhere, but it is v ery illu strativ e: Th e CAHRISMA Project. \nCAHRISM A is the acrony m for Conservat ion of the \nAcoustical Heritag e by the Revival and Identification of \nthe Sinan's Mosques Acoust ics. A bri ef descri ption from  \nthe project hom epage reads:  “The C AHR ISMA project  \nis an European C ommission Fi fth Fram ework INCO – \nMED Program me. Thi s program me is directed towards \ncooperat ion bet ween EU-count ries and the Mediterra-\nnean count ries. . . . The m ain focus within the project  is \nto innovat e [sic] the concept of Hybrid Architectural \nHeritage bei ng a new way  of identification that covers \nacoust ic as well as visual features, the idea being that for \nspaces having acoustic im portance, the architectural \nheritage concept , consi dered i n conservat ion and rest ora-\ntion project s shoul d be upgraded to cover acoust ical as \nwell as visual aspects.”  \nThe resul ts are st unning (and can be heard on t he \nwebsite). Th anks to the sim ulated  auralizatio ns, the lis-\ntener is given the opport unity to hear, t o experience mu-\nsic from  diverse historical peri ods as i t sounded i n the \n                                                           \n5 See Baum ann, Polhe,  Shankar  (2004) , Logan ( 2004) , Shankar  & \nBaum ann, (2004)  and Yang &  Lee (2004) . venues i n whi ch it was perform ed as they were con-\nstructed and situated at that time, where the effects of \nlater renovat ion and t he distractions of irrelevant ambi-\nent noise such as that of m odern traffic are rem oved.  \n \n2.5 Some Ethical Issues \n \nMIR, h owever, can  also  raise p otentially tro ubling is-\nsues with  regard to authenticity. On ce ag ain, the fram-\ning for t hese rem arks is provi ded by  Sharpe:  \nWe owe it to  the composer to  play it [the music] \nin the way  he concei ved i t and i t norm ally sounds \nbetter that way. Ad mittedly, unlike moral co nsid-\nerations elsewhere, these vary with the stature of \nthe music. . . . Great works were conceived to be \nplayed in a cert ain way. Undoubt edly, within \nthese constraints there will b e more than one way \nof doi ng it. Bach m ight not have worri ed as to \nwhet her his keyboard music was pl ayed on t he \nharpsi chord, cl avichord or earl y piano. He m ight \nhave had preferences, but  any  of these instru-\nments woul d do. B ut I suspect  that ornam ents \nwoul d have m attered as i ntensely to him as they \ndid to Rameau, and we know Rameau cared, \ngiven hi s instructions on the matter (Sharpe, \n2004, pp. 82-83). \nOne can i magine a poi nt in the devel opment of M IR and \nrelated computer modeling techni ques, such as beat  ex-\ntraction whi ch allows for t he conduct ing of audio files6 \nat whi ch rank am ateurs can “reconduct ” perform ances \nby worl d renowned orchest ras whi ch were originally \nunder t he bat on of a m aster.  Thi s raises some interest-\ning issues of aut henticity and et hics whi ch, though per-\nhaps anticipated by the foregoi ng quot e and si milar con-\nsiderat ions within ongoi ng debat es about  perform ance \npractice within traditional musicology, introduces a su f-\nficient num ber of fresh consi derations to warrant  re-\nnewed di scussi on. \n3 . . . AND WHERE DO WE GO FROM \nHERE? \nI concl ude wi th som e rem arks whi ch I found m yself \nmaking repeat edly in vari ous unpubl ished fora before \nand duri ng ISM IR2004 i n Barcelona; this gives them a \nchance to go “on record,” and the rem arks in section 2 \nare somewhat m ore am plified than any thing that I had an \nopport unity to form ulate in these fora. \nIt is important to stress, t hat although I am  sort of a \ntoken phi losopher/ humanist with the cont ext of ISMIR, I \nam not advocat ing any  kind of “M IR for poets” sessi ons \nor topics. It  is my experi ence t hat when humanists – who \noften feel u nnecessarily in timidated by an IT-heavy field \nof research such as MIR – are finally lu red into lectu res \nand t alks on M IR, they are able to underst and what  is \ngoing on and are oft en stunned by  the implications they \nrealize th at MIR h as for, say, aesth etics and epistem ol-\n \n6 Again thanks to Declan Thom as Mu rphy for insights into this area \nand his work within it. \n \n11   \n \n                                                          ogy. Som e of these have been outlined in the preceding \nsection. Of course i t dem ands som e preparat ion on t he \npart of a speaker to presen t highly techni cal topics to \nmixed audiences, but I have seen on num erous occasions \nin Denm ark t hat there can be very  product ive cross-\nfertilizatio n when the “hard” and “soft” scien ces m eet. 7\nWhile humanists m ay have to overcome initial feel-\nings of intimidation when confront ed by  MIR, on t he \nother hand, I have heard from  some very techni cally-\nprofi cient MIR and com puter m usic researchers who \nhave gi ven t alks to mixed audiences in which humanists \nare wel l represent ed, that they have found the experi ence \nto be bot h frui tful and rewardi ng, i n that they have felt \nsomewhat isolated and alone  in the techni cal environ-\nments in which they com monly work and i n which their \nown specul ations of, say , an aest hetic nature, are derided \nand not  taken seri ously. \nTo sum up: The goal  of this paper has been t o indicate \nhow at least some of the advances i n MIR may be con-\ntextualized  with in the fram ework of a selectio n of ques-\ntions wi thin philosophy  of m usic and phi losophy  of cul -\nture. It is m y hope that not m uch m ore tim e will pass \nbefore we see increased awareness of – and interest in – \nthe trem endous m ultidisciplin ary implications of MIR \nand its potential for providing a potent interface between \nscien ce an d the humanities. \nREFERENCES \nBaumann, Stephan: Tim Polhe, Vembu Shankar. \n“Towards a So cio-Cultural Co mpatibility of MIR \nSystm es”, (2004) Proceedings of the 5th International \nConference on M usic Inform ation Retrieval, \nUniversitat Pom peu Fabra, Barcelona, Spain, \nOctober 10-14, 2004, 460-465. \nThe      CAHRISMA      Project \nhttp://www.dat.dtu.dk/cahrism a.htm\nEddo, Mark. (2002) ''Shazam!!'', BBC Prime Click \nOnline Interview August  1, 2002 wi th, among ot hers, \nChris B arton and Vi jay Solanki of Shazam , \nhttp://bbcworld.com/content/click online_archive_31\n_2002.asp? pagei d=666&co_pagei d=4)\nHülsmann, T. (2000) The Costs of Open Learning: A \nHandbook. Ol denburg, B ibliotheks-und \nInform ationssy stem  der Ca rl Ossietsk y Un iversität \nOldenburg. \nJMM: The Journal of Music and Meaning, \nwww.m usicandm eaning.net\nLogan, Beth. (2004) “Music Recommendation from \nSong Sets.” Proceedings of the 5th International \nConference on M usic Inform ation Retrieval, \nUniversitat Pom peu Fabra, Barcelona, Spain, \nOctober 10-14, 2004, 425-428. \n \n7 A for um for this sor t of dialog is JM M: The Jour nal of Music and \nMeaning, www.m usicandm eaning.net Moore, M. and Kearsley, G. (1996) Distance \nEducat ion: A Sy stems View. Wadswort h Publishing \nCompany, Belm ont, CA. \nMurphy, D. T., H. Andersen, and K. Jensen. (2003) \n“Conducting audio files via computer vision.” \nProceedings of the Gesture Workshop, Genova. \nSpringer   Verlag,   (Available   at \nhttp://www.di ku.dk/ ~decl an/pub/papers.ht ml. ) \nNTSMB: Netværk for Tværvidenskabelige Studier af \nMusik og B etydning/Network for C ross-Di sciplinary \nStudies of M usic and M eaning, www.ntm sb,dk, \nInstitute of Philosophy, Education, and the Study of \nReligions, Uni versity of Sout hern Denm ark, Odense \nDenm ark. See specifically program s for Nature, \nCulture and Musi cal Meani ng and Musi c, Logi c and \nTechnol ogy both hel d at SDU i n 2002. \nRoy, Debopam, Nagaraju Pappu, T.V. Prabhakar. \n(2004) “Parichaykram a – An Exploratory Interface \nof Indi an C lassical Music Using Experi ential \nFram ework”, Final Proceedings of Com puter Music \nModeling and R etrieval, International Symposium. \nCMMR 2004, Esbjerg, Denm ark, M ay 2004, Revised \nPapers, 359-370. \nSemioNet, The Network for Studies in Semiotics and \nMeaning, www.sem ionet.sdu.dk , In stitute of \nLanguage and C ommunication, University o f \nSouthern Denm ark (SDU). \nSharpe, R.A. (2004) Philosophy of Music: An \nIntroduct ion. Acum en, B ucks. \nVembu, Shankar, Stephan Baumann (2004) “A Self-\nOrgani zing M ap Based Knowl edge Discovery  for \nMusic Recom mendation Sy stems”, Fi nal \nProceedings of Com puter Music Modeling and \nRetrieval, International Symposium. CMMR 2004, \nEsbjerg, Denm ark, May 2004, R evised Papers, 119-\n129. \nWhitehouse, Harvey. (2000) Arguments and Icons: \nDivergent Mo des of Relig iosity. Oxford University \nPress, Oxford. \nYang, Dan; WonSook Lee. (2004) ''Disambiguating \nMusic Emotion Usi ng Soft ware Agent s'', \nProceedings of the 5th International Conference on \nMusic Information Retriev al, Un iversitat Po mpeu \nFabra, B arcelona, Spai n, Oct ober 10-14, 2004, 52-\n27. \n \n \n12"
    },
    {
        "title": "Visual Playlist Generation on the Artist Map.",
        "author": [
            "Rob van Gulik",
            "Fabio Vignoli"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415206",
        "url": "https://doi.org/10.5281/zenodo.1415206",
        "ee": "https://zenodo.org/records/1415206/files/GulikV05.pdf",
        "abstract": "This paper describes a visual playlist creation method based on a previously designed visualization technique for large music collections. The method gives users high-level control over the contents of a playlist as well as the progression of songs in it, while minimizing the interaction requirements. An interesting feature of the technique is that it creates playlists that are independent of the underlying music collection, making them highly portable. Future work includes an extensive user evaluation to compare the described method with alternative techniques and to measure its qualities, such as the perceived ease of use and perceived usefulness.",
        "zenodo_id": 1415206,
        "dblp_key": "conf/ismir/GulikV05",
        "keywords": [
            "visual playlist creation",
            "large music collections",
            "high-level control",
            "song progression",
            "minimizing interaction",
            "portable playlists",
            "user evaluation",
            "alternative techniques",
            "perceived ease of use",
            "perceived usefulness"
        ],
        "content": "VISUAL PLAYLIST GENERATION ON THE ARTIST MAP\nRob van Gulik Fabio Vignoli \nInstitute of Information and Computing  \nSciences, Utrecht University \nPadualaan 14, 3584 CH, Utrecht (NL) \nrob@cs.uu.nl Philips Research Laboratories \nProf. Holstlaan 4, 5656 AA \nEindhoven (NL) \nfabio.vignoli@philips.com  \nABSTRACT \nThis paper describes a visual playlist creation method \nbased on a previously designed visualization technique for large music collections. The method gives users \nhigh-level control over the contents of a playlist as well \nas the progression of songs in it, while minimizing the interaction requirements. An interesting feature of the technique is that it creates playlists that are independent of the underlying music collection, making them highly \nportable. Future work includes an extensive user evalua-tion to compare the described method with alternative techniques and to measure its qualities, such as the per-ceived ease of use and perceived usefulness.   \n \nKeywords: Playlist generation, visualization of music \ncollections, artist map.  \n1 INTRODUCTION \nPlaylists play an important role in the user experience of \ndealing with digital music collections, on portable play-ers as well as desktop machines [1]. Unfortunately, cre-\nating a playlist is not an easy task. Given the large num-\nber of songs in many digital music collections and the short time people are generally willing to spend making a selection, a tradeoff has to be made regarding the quality of the list and the time spent creating it. Often a \nrough selection is made offline, which is refined during the actual playing of the list. For large collections (i.e. more than 1000 songs), it is not uncommon that either a large part of the music is ignored, or the whole collec-tion is simply played at random.  \nWhat makes playlist creation hard is the granularity \nof item selection: selecting songs one by one can be \ndifficult, because you have to know and remember song titles and artist names. Also, having to select songs in this linear fashion takes a long time. It may be more convenient to select certain kinds  of music. \nOne of the main reasons people do not want to spend \nmuch time creating a playlist is that these lists go out of date. Especially on portable music players, digital mu-sic collections are highly dynamic. New popular songs may replace yesterday’s hits on an almost daily basis.  \nIn this paper we propose a visual playlist creation \nmethod that requires little interaction, while giving the user substantial high-level control over the selected songs. Moreover, the resulting playlists are independent \nof the music collection for which they were originally created. The paper is organized as follows: Section 2 gives an overview of the related work. Section 3 de-scribes the playlist creation interface, and in section 4 we discuss the implementation; the actual selection of songs based on the playlist path created by the user. Finally, section 5 presents our conclusions and some \ndirections for future research. \n2 RELATED WORK \nThe visual playlist creation method described in this \npaper relates to two fields of research; visualization of music collections, and automatic playlist generation. \n2.1 Visualization of music collections \nThe visualization of music collections has been re-\nsearched at many different sites. Pampalk et. al. created the Islands of Music  system in which pieces of music \nare organized by a self organizing map, based on per-\nceived sound similarities [2]. The result depicts clusters of similar music as islands on a geographical map. \nThe artist map , introduced in [3] by van Gulik et. al., \nalso provides an annotated overview of a music collec-tion in the form of a two dimensional map. It uses data \nfrom external sources (i.e. mood) in addition to deriva-tives of the raw audio data (i.e. tempo and timbre) as attributes to organize and cluster music. \nOther approaches to visualizing music collections \ninclude the audio browser-editor for large wall displays introduced by Tzanetakis and Cook [4], the disc, rec-tangle and tree-map visualizations by Torrens et. al. [5], and the use of a multidimensional scaling algo-rithm called FastMap  to visualize songs on the basis of \ntheir similarity, by Cano et. al. [6]. \nAll of these visualizations are aimed at providing the \nuser with an easy to understand overview of a large music collection, and most can also be used to browse \nthrough such collections. Torrens et. al. go a step fur-\nther, suggesting that the visual overview can also be used to edit or create playlists, by selecting and combin-ing regions of music of interest. \nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-vided that copies are not made or distributed for profit or com-mercial advantage and that copies bear this notice and the full citation on the first page. \n© 2005 Queen Mary, University of London \n520   \n \n 2.2 Automatic playlist generation \nThe automatic playlist generation methods we found \ndescribed in literature are based on one of the following \nideas. They select songs similar to one or a few seed songs that were picked by the user [7], they base the selection of songs on user constraints and preferences \n[8], or they combine these two methods; using con-straint satisfaction to find a seed song on which the playlist can be based [9]. \nThis paper introduces a new way of creating playlists \non the visualization of a music collection. With this method, users are able not only to select the type of mu-\nsic of interest (as in [5]), but also to control the overall flow of the playlist with minimal interaction. We expect that this intuitive control over music flow can help im-prove the resulting quality of automatic playlist genera-tion methods that currently often rely on a small num-ber of seed songs. From a different point of view, draw-ing playlist paths can be seen as an easy way for speci-fying musical constraints. These constraints can be fur-ther used to generate a playlist. \n3 VISUAL PLAYLIST CREATION \nWe use the artist map we described in [3] as a frame-\nwork to create a new way of making playlists in a fast \nand easy manner: by drawing paths and/or specifying \nregions of interest on top of the visualization. The artist \nmap aims at visualizing a music collection in such a way that: \n• A clear overview of an entire music collection \nor a subset thereof can be given; \n• Similarity between artists is used and clearly \ndepicted; \n• The attributes mood , genre , year and tempo  \nlabel important positions on the map in order to provide context; \n• Navigation of a, possibly unknown, music col-lection is supported by non-specific or fuzzy \ncriteria \nThe artist map can visualize (large) music collec-\ntions on a small screen and was designed to support non-specific music searches (without the need to specify artist or song title).  \nIn the map, artists are positioned based on similarity \nof their music and attribute data extracted from their songs or obtained from external sources. The attribute information is visualized in the form of magnets, which form an integral part of the user interface as well as the placement algorithm (a force-directed graph drawing algorithm, described in [3]). Figure 1 shows an exam-ple in which year and tempo magnets are used, and \nartists are colored on the tempo attribute. The interface also features zooming functionality, which enables the user to select a subset of the music collection for closer investigation, and a list browser that can be used for traditional searches or to inspect and refine a selection.  The interaction required by the user to create a play-\nlist on the artist map consists of the following tasks, \nwhich together form the path drawing phase :\n \n• Drawing paths and/or clicking individual \npoints of interest on the map \n• Specifying the number of requested songs (or \nthe requested length of the list in minutes) \n \n \nFigure 1 . This is an example of a year-tempo \nartist map. The annotated magnets show that clustering is based on year of release  along the \nhorizontal direction and tempo along the vertical \ndirection, and coloring is done on tempo. \n \nCombining this input with the information that is \navailable from the visualization, we end up with rea-\nsonable constraints for which a playlist can be gener-\nated. The data we use from the visualization are: \n \n• Currently used magnet types (year and tempo \nin the example shown in Figure 2) \n• Current zoom state (upbeat, happy in Figure 2) \n• Positions of magnets and artists \n \nIn the next section we explain how an actual playlist \nis constructed using this information. \n4 SONG SELECTION METHOD \nSelecting songs given the user input and the contextual information is a process that consists of two phases. First the drawn path – consisting of line segments and \npoints – has to be converted to a number of playlist \npoints, equal to the requested number of songs. Then, \nfor each playlist point, a s ong matching the context has \nto be chosen from the available collection. \n \n521   \n \n \n \nFigure 2 . This picture shows an example of \ndrawing playlists on an artist map after zooming \nin on 45 artists that mainly make happy or up-beat music. Clustering is based on year of re-\nlease and tempo, and coloring is done on year of \nrelease. The playlist starts with music from the \n80s and 90s of increasing tempo, after which it \nwill play 3 fast newer songs. All songs will be upbeat or happy, as the user zoomed in on these moods. \n4.1 Setting playlist points \nA complete playlist path may consist of several smaller \nparts, in the form of either a path or a point. Informally, the playlist path is the ordered collection of drawings a user has made during the path drawing phase – where a drawing is defined as the recording of mouse move-ments within the visualization area while the left mouse button is pressed. Each drawing made in this way de-fines a subpath, and each subpath dete rmines a part of \nthe songs in the generated pl aylist. Figures 3 and 4 each \nshow an example of a playlist path.   \nGiven the playlist path and the requested number of \nsongs n, we have to choose n positions on the path from \nwhich to play a song. These positions are called playlist \npoints . The distribution of playlist points over the sub-\npaths depends on the following priority list:  \n \n1. Begin and end points of the complete path \n2. Begin and end points of subpaths (if they exist) 3. Remaining points, which are distributed over \nthe sub-paths \n \n \nFigure 3 . A playlist path created on the artist \nmap which progresses through rock and alterna-\ntive music, followed by some Americana. \n \nThis means that playing a song from both the start \nand the end of the drawn path is most important. Next, \nwe would like to play at least some songs for each sub-\npath so we put a playlist point at the beginning and end \nfor each of them. For the division of the remaining points, if any, the length of a subpath is used as divid-ing measure: longer subpaths get more playlist points. We define the ‘length’ of a subpath that consists of only \na single point (an individually specified region, see Fig-ure 4) as the mean length of the other subpaths in the drawing. For actually selecting a single song of a cer-tain type, the list interface can be used. \n \n \nFigure 4. The playlist path shown in this picture \nstarts with fast songs from the 90s, followed by \nsome medium tempo songs from 2000+, and ends with a subpath through slow music.  \n4.2 Song selection \nFor each of the playlist points, a matching song has \nto be found. Song selection is based on the following \nconstraints: \n \n522   \n \n • a song may only occur once in the playlist \n• each selected song has to conform to the given \nzoom-state \n• the song selected at each point should reflect its \ncontext on the map \n \nConforming to the zoom-state means that for each \nmagnet type on which the user zoomed in, every se-\nlected song should match one of the elements corre-\nsponding to the zoom. For example, if the user zoomed \nin at fast, happy and upbeat, every song in the playlist should be fast, and every song should be either happy or upbeat. \nReflecting context on the map means that for each of \nthe active magnet types, every selected song should \nmatch the closest magnet (or match the range between the two closest magnets). Further, every selected song should preferably match the closest artist on the map. \nSometimes the requirements a bove cannot be com-\nbined; the closest artist may not have a song of the re-\nquired type. In this case, a song is played from the clos-est artist that does match the constraints. If there is none, we resort to the artists that are not currently on the map and try to find a matching song there. Only if there is no matching song on the device at all, a song will be played that only conforms to part of the criteria. This strategy implies that for a playlist point that lies right on top of an artist, a song of a different artist may be played. But since our method is concerned with cre-ating a playlist based on kinds of music, this is not a problem. If the user wants to include a specific artist, he can do so in the list-based interface.  \n5 CONCLUSIONS AND FUTURE WORK \nThe automatic playlist generation method presented in \nthis paper helps music listeners to create playlists in an easy way and in little time. Users only need to specify \nthe kinds of music they want to hear and visually indi-\ncate the progression they would like the music in the playlist to have. Furthermore, our method has the fol-lowing interesting characteristics: \n \n• It is easy to control of the kind of selected mu-\nsic even if you do not remember names well \n• playlist paths are independent of the underlying \nmusic collection \n• the interface was designed to be used on port-able music players \n \nWe plan to test the playlist creation method described \nin this paper extensively, by comparing the method with alternative techniques and evaluating the perceived \nquality of both the interface and the resulting playlist. \nOther possible future work includes:  \n \n• Instead of selecting songs individually, improve \nplaylist coherence by the constraint that two consecutive songs should preferably be similar \n • Add other interesting magnet types to the artist \nmap, based on e.g. length of songs, date last played or number of times played last week \n• Add the possibility to start with specific items selected in the list based interface, and generate paths with the selection as additional constraint \nAcknowledgements \nPart of this research has been funded by the Dutch \nBSIK/BRICKS Project. \nREFERENCES \n[1] Vignoli, F., “Digital Music Interaction concepts: a \nuser study,” Proceedings of the 5th Int. Conference on Music Information Retrieval, ISMIR 2004, 2004. \n[2] Pampalk, E., Rauber, A., and Merkl, D., “Content-\nbased Organisation and Visualization of Music \nArchives,” Proceedings of the 10th ACM int. conf. on Multimedia, MULT IMEDIA ’02, 2002. \n[3] van Gulik, R., Vignoli, F., and van de Wetering, H., \n\"Mapping music in the palm of your hand, explore \nand discover your collection,\" Proceedings of the 5th International Conference on Music Information Retrieval, ISMIR 2004, 2004. \n[4] Tzanetakis, G., and Cook, P., “MARSYA3D: A \nprototype audio browser-editor using a large scale \nimmersive visual and audio display,\" Pro ceedings of \nthe 7th International Conference on Auditory Display, ICAD 2001, Helsinki, Finland, 2001. \n[5] Torrens, M., Hertzog, P., and Arcos, J.-L., \n\"Visualizing and exploring personal music libraries,\" Proceedings of the 5th International Conference on Music Information Retrieval, ISMIR 2004, 2004. \n[6] Cano, P., Kaltenbrunner, M., Gouyon, F., and \nBatlle, E., “On the Use of FastMap for Audio Retrieval and Browsing,” Proceedings of the 3rd Int. Conference on Music Information Retrieval, ISMIR \n2002, 2002. \n[7] Logan, B., “Content-Based Playlist Generation: \nExploratory Experiments,” Proc. of the 3rd Int. Conf. on Music Information Retrieval, ISMIR 2002, 2002. \n[8] Aucouturier, J.-J., and Pachet, F., “Scaling up music \nplaylist generation,” Proc. of the 3rd IEEE Int. Conf. on Multimedia and E xpo, ICME 2002, 2002. \n[9] French, J., and Hauver, D., “Flycasting: On the Fly \nBroadcasting,” Proceedings of the 2nd DELOS Network of Excellence Workshop on Personalization and Recommender Systems in Digital Libraries, DELOS 2001, 2001. \n523"
    },
    {
        "title": "ATTA: Automatic Time-Span Tree Analyzer Based on Extended GTTM.",
        "author": [
            "Masatoshi Hamanaka",
            "Keiji Hirata 0001",
            "Satoshi Tojo"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415572",
        "url": "https://doi.org/10.5281/zenodo.1415572",
        "ee": "https://zenodo.org/records/1415572/files/HamanakaHT05.pdf",
        "abstract": "This paper describes a music analyzing system called the automatic time-span tree analyzer (ATTA), which we have developed. The ATTA derives a time-span tree that assigns a hierarchy of 'structural importance' to the notes of a piece of music based on the generative theory of tonal music (GTTM). Although the time-span tree has been applied with music summarization and collaborative music creation systems, these systems use time-span trees manually analyzed by experts in musicology. Previous systems based on GTTM cannot acquire a timespan tree without manual application of most of the rules, because GTTM does not resolve much of the ambiguity that exists with the application of the rules. To solve this problem, we propose a novel computational model of the GTTM that re-formalizes the rules with computer implementation. The main advantage of our approach is that we can introduce adjustable parameters, which enables us to assign priority to the rules. Our analyzer automatically acquires time-span trees by configuring the parameters that cover 26 rules out of 36 GTTM rules for constructing a time-span tree. Experimental results showed that after these parameters were tuned, our method outperformed a baseline performance. We hope to distribute the time-span tree as the content for various musical tasks, such as searching and arranging music.",
        "zenodo_id": 1415572,
        "dblp_key": "conf/ismir/HamanakaHT05",
        "keywords": [
            "music analyzing system",
            "automatic time-span tree analyzer",
            "generative theory of tonal music",
            "structural importance",
            "music summarization",
            "collaborative music creation",
            "time-span tree",
            "musicology",
            "novel computational model",
            "adjustable parameters"
        ],
        "content": "ATTA: AUTOMATIC TIME-SPAN TREE ANALYZER BASED ON \nEXTENDED GTTM\nMasatoshi Hamanaka Keiji Hirata Satoshi Tojo \nPRESTO, Japan Science and \nTechnology Agency \nA.I.S.T.  1-1-1 Um ezono, \nTsukuba, Ibaraki, Japan \nm.hamanaka@aist.go.jp  NTT Com munication Science \nLaboratories \n2-4, Hikaridai, Seikacho, Kei-\nhanna Science City, Kyoto, Japan\nhirata@brl.ntt.co.jp  Japan Advanced Institute of  \nScience and Technoloty \n1-1, Asahidai, Nom i,  \nIshikawa, Japan \ntojo@jaist.ac.jp  \nABSTRACT \nThis paper descri bes a m usic analyzing system  called the \nautom atic tim e-span tree an alyzer (ATTA), which we \nhave devel oped. The ATTA deri ves a t ime-span t ree that \nassigns a hierarchy  of 'structural importance' to the not es \nof a piece of m usic based on the generative theory of \ntonal music (GTTM ). Al though t he time-span tree has \nbeen applied with music sum marization and col labora-\ntive m usic creation system s, these system s use tim e-span \ntrees m anually analyzed by expert s in musicology. Pre-\nvious systems based on GTTM  cannot  acqui re a t ime-\nspan tree wi thout manual appl ication of m ost of the rules, \nbecause GTTM does not resolv e much of the am biguity \nthat ex ists with  the applicatio n of the rules. To solve this \nprobl em, we propose a novel  com putational model of the \nGTTM th at re-fo rmalizes th e rules with computer im-\nplementation. The main advant age of our approach i s \nthat we can introduce adjust able parameters, wh ich en-\nables us to assig n priority to  the rules. Ou r analyzer \nautomatically acq uires tim e-span trees by  confi guring \nthe param eters that cover 26 rul es out  of 36 GTTM  rules \nfor const ructing a t ime-span tree. Experi mental resul ts \nshowed that after these param eters were t uned, our \nmethod out perform ed a basel ine perform ance. We hope \nto distribute the tim e-span tree as the content for various \nmusical tasks, such as s earchi ng and arrangi ng m usic. \n \nKeyw ords: ATTA, Generat ive Theory  of Tonal  Music \n(GTTM ), time-span t ree, groupi ng st ructure, metrical \nstructure, m usical knowledge , knowledge acquisition.  \n1 INTRODUCTION \nWe propose a m ethod for implementing a music theory \ncalled Generative Theory  of Tonal M usic (GTTM ) [1]. \nIt is difficult for t hose who are not  musical expert s to \nmanipulate music, because com mercial m usic se-\nquence software today only operates on the surface \nstructure of m usic, such as the not es, rest s, and chords. Our goal is to  create a system  that will en able a musical \nnovice to m anipulate a piece of m usic, which is an am-\nbiguous and subjective m edia, according to the user' s in-\ntentions, by implementing the musical knowl edge of m u-\nsician s.  Our first step  was to  attem pt to implement the \nGTTM, which analyses the m eaning of a piece of m usic \nand interprets th e implicit in tentions of the composer. \n   Musical theory provi des us with the methodol ogies \nfor anal yzing and t ranscri bing m usical knowl edge, ex-\nperiences, an d skills fro m a musician 's way o f thinking. \nOur concern i s whet her or not  the concepts necessary for \nmusic analysis are sufficiently  externalized in musical \ntheory. We consider the GTTM to  be the most promis-\ning theory among the many that have been proposed \n[2–4], in  terms of  its ability to  formalize musical \nknowledge, because the GTTM captures the aspects of \nthe musical phenom ena based on t he Gest alt occurri ng \nin music an d is presented with  relativ ely rig id rules . \nThe tim e-span tree provides a sum marization of a \npiece of m usic, which can be used as the representation \nof a search, by analyzing the results from  the GTTM , re-\nsulting in a music retriev al system  [5]. It can also be used \nfor perform ance renderi ng [6-8]  and reproduci ng music \n[9]. These system s enable users to m anipulate m usic us-\ning a tim e-span tree, disregar ding the surface structure of \nthe m usic. However, the tim e-span trees in these system s \nneed t o be m anually analyzed by  expert s in musicology.  \n   The bi ggest  probl em with com puter implementation \nof the GTTM is th at musical theories, including GTTM, \nare am biguous, because m usic interpretation is tacit and \nsubjective. Beside that m ost of the m usical theories are \npresent ed for hum ans, wi thout taking into consi deration \ncomputer logic. At tempts have been m ade to implement \nseveral ru les of the GTTM, b ut when these rules conflict \nwe co uld not adequately reso lve the priority in multiple \nrules [10, 11]. On the other hand, t he com puter model of \nthe GTTM  [12] coul d produce a t ime-span t ree, but  it re-\nquired m anual appl ication of m ost of the rules. \n   To overcom e the ambiguity of the GTTM  rules, we \npropose an ext ended GTTM  that re-form alizes the \nrules and establishes an al gorithm for acqui ring a t ime-\nspan tree by numerical expressi ons. In t he expressi ons, \nwe in troduce ad justable parameters fo r co ntrolling \ntacitn ess, ambiguity, an d subjectiveness. The extended \nGTTM  now covers 26 rul es out  of 36 GTTM  rules for \nconstructing a tim e-span tree. W e implem ented a time-\nspan analyzer, called ATTA, based on the extended \nGTTM in Perl. User can acquire the tim e-span tree by \nusing ATTA vi a CGI appl ication on t he Internet. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies  bear this  notice and the full \ncitation on the first page. \n© 2005 Queen Mary , University  of London \n358   \n \n \n…\n＾3a 3a 6 6 3a,6This paper is organi zed i n the fol lowing way . We \npresent  the probl em of appl ying GTTM  rules in Sec-\ntion 2, propose ext ended GTTM  in Sect ion 3, descri be \nthe tim e-span analyzer and it’s exam ples in Section 4 \nand 5, and present  experi mental resul ts and concl usion \nin Sections 6 and 7. Last ly, we provi de in the appendi x \nall th e expressio ns to implement the GTTM analyzer. \n2 INTRODUCTION OF GTTM AND ITS \nPROBLEMS \nThe GTTM is com posed of four modules, each of which \nassigns a separat e structural descri ption to a listener’s un-\nderstanding of a piece of m usic. These four m odules out-\nput a groupi ng st ructure, a m etrical structure, a t ime-span \ntree, and a prol ongat ional tree, respect ively (Figure 1). \n…\n・・・・・・・・ ・・・・ ・・・・ ・・・・ ・・ ・・ ・ ・・・・・・Time-span tree\nMetrical structure\nGrouping st ructure \nFigure 1 . Groupi ng st ructure, M etrical structure, \nand Ti me-span t ree. 1  2  3 4  5  6  7  8 9 10 11  12 13  14\n＾＾＾＾ The groupi ng st ructure i s intended t o form alize the \nintuitive belief that tonal m usic is organized into groups \nthat are i n turn com posed of subgroups. These groups \nare graphically presented as several levels of arcs below \na music staff. The m etrical structure describes the \nrhythm ical hierarchy of th e piece by identifying the po-\nsition of strong beats at the levels of a quart er not e, hal f \nnote, a m easure, two m easures, four m easures, and so \non. Strong beats are illustrated as several levels of dots \nbelow the m usical staff. The time-span tree is a binary \ntree, which is a hierarchi cal structure descri bing the \nrelativ e stru ctural im portance of notes th at differen tiate \nthe essent ial parts of t he melody from  the ornam entation. \nFor exam ple the left-hand si de of Figure 2 depicts a \nsimple melody and its tree. Th e tim e-span (designated \nas <--->) is represented by a single note, called a head, \nwhich is designated here as “C 4”. \nAbstractin g InstantiatingheadC4 C4\n⊂− \n \n \n \n \n \nFigure 2 . Subsum ption rel ation of m elodies. \n There are two types of ru les in  GTTM, i.e., “wel l-\nformedness rules” and “preference rules”. W ell-\nformedness rules are the necessary conditions for the  \nassignm ent of a structure a nd the restrictions on the  \nstructures. W hen m ore than one structure satisfies the \nwell-form edness rules, the pr eference rules indicate the \nsuperi ority of one st ructure over anot her.  \nIn this sectio n, we sp ecify th e problems with the \nGTTM ru les in  term s of computer im plementation. 2.1 Ambiguous concepts defining preference rules \nThe GTTM  uses som e undefi ned words, causi ng am bi-\nguities in  the analysis. Fo r example, the GTTM h as rules \nfor sel ecting proper st ructures when discoveri ng similar \nmelodies (cal led paral lelism), but does not have the defi-\nnition of similarity itself.  \nTo solve this problem we attem pted to formalize th e cri-\nteria for deciding whether each rule is applicable or not. \n2.2 Conflict between preference rules \nThe confl ict between rul es often occurs when applying \nthe rules and results in ambiguities in  the analysis b e-\ncause there is no strict orde r for applying the preference \nrules. Figure 3 shows a si mple exam ple of t he confl ict \nbetween t he groupi ng preference rules (GPR ). GPR 3a \n(Register) is applied bet ween not es 3 and 4 and GPR 6 \n(Paral lelism) is applied between not es 4 and 5. A \nboundary  cannot  be percei ved at  both 3-4 and 4-5, be-\ncause GPR 1 (alternative form ) strongl y prefers that note \n4, by  itself, cannot  form  a group.  \nTo solve this problem we in troduced adjustable pa-\nrameters that enable us to c ontrol the strength of each rule. \n \n \n \nFigure 3 . Sim ple exam ple of conflict between rules. \n2.3 Few mentions to how to calculate hierarchical \nstructures \nThe GTTM  does not  define a val id procedure for acqui r-\ning the hierarchical structure. It is n ot realistic to  first \nmake every structure satisfy the well-form edness rules \nand then select the optim al structure. For exam ple, only \na ten not e score provi des 185794560 (= ! 9 ) kinds of \ntime-span trees.  　92×\nTo solve this probl em we devel oped an al gorithm for \nacquiring the hierarchical stru cture, t aking into consi d-\neration som e of the exam ples in the GTTM. \n2.4 Less precise explanation of feedback link \nThe GTTM  has some feedback l inks from  higher l evel \nstructures to lower level ones, e.g. GPR 7 (time-span and \nprolongational stability) prefers a grouping structure \nthat resul ts in a more stable time-span and/ or prol onga-\ntion reduct ions. However, no detailed descri ption and \nonly a few exam ples are given. \n3 EXTENDED GTTM \nTo overcom e the probl ems with com puter implementa-\ntion of t he GTTM , we propose a com putational model of \nthe GTTM  called the ext ended GTTM , whi ch covers 26 \nrules out of 36 GTTM  rules for const ructing time-span \ntree. The rem aining 4 rules are for feedback links and \nanother 6 rul es are for hom ophony . In the current  stage, \nwe rest rict the music structure t o monophony  to cor-\nrectly evaluate the perform ance of each rule. \n359   \n \n     In this sect ion we part icularize our proposed ext en-\nsion of t he GTTM  for com puter implementation. The \npolicies are equally applied to the three an alyses, wh ich \nare the groupi ng st ructure, m etrical structure, and Time-\nspan reduction analyses. \n3.1 Re-formal ization of rul es \nIn order to deal with  the preference rules on a computer, \nwe have expressed t he rul es into num erical styles. Nu-\nmeric d escrip tions of the rules allo w to quantitativ ely \ncombine the result of each  rule application. \nWe expressed t he degree of appl ication of t he rule as a \nnumerical funct ion Dirule which out put 1 (appl icable) or 0 \n(not applicable) if the rule is clearly ap plicable or not. For \nexam ple, GPR2b (Attack-Point ) states that a relatively \ngreater in terval of time between  attack  points initiates a \ngroupi ng boundary  that can be expressed as fol lows: \n⎩⎨⎧ > <=+ −\nlsee          ioi oii and ioi oii           Di i i i GPR2b\ni011 1  ,   (1) \ni      : transition of not e \nioi i  : inter onset intervals. \n \nA num erical funct ion Dirule outputs bet ween 1 (appl ica-\nble) and 0 (not  appl icable) if the rule is not clearly ap-\nplicable or not. For exam ple, time-span reduct ion pref-\nerence rule 3a (TSRPR3a) that prefers that a higher m e-\nlodic pitch is used as the head of a tim e-span can be \nexpressed as fol low: \njjiTSRPR3\ni pitch pitch D max=,                                 (2) \ni         : head \npitch i : pitch (not e num ber of M IDI). \n3.2 Refinement of ambiguous concepts \nAs descri bed above, t he GTTM  uses som e undefi ned \nconcept s that provi de am biguousness i n anal ysis. The \nconcepts are am biguous, with  no unique definition. For \nexam ple, the concept of a sim ilar m elody has a lot of \nplausible defi nitions [13] , but no best  one. \nHere, we attem pted to formalize concepts based on \nthe following two policies, wh ich we esteem . \n1) To define intuitio nally and co mprehensiv ely. \n2) Equipment adjustable parameters for control \nof the ambi guity. \n3.2.1 C oncept  for symmet ry \nGPR 5 is the rule for sy mmetry in a groupi ng structure. It \nprefers groupi ng anal yses that most closely approach t he \nideal subdi vision of groups i nto two part s of equal  length.  \n    We define the degree of sy mmetry DiGPR5 so that there \nis a preference t o subdi vide a group i nto two part s of equal  \nlength. Here, we use a norm al distribution with the stan-\ndard devi ation σ as the degree of sym metry, as follows.  \n \n         (3) \nwhere \n   start : start transition of group. \n   end  :  end t ransition of a group. The σ is an adjustable param eter for a user to control the \ndegree of sy mmetry. In Fi gure 4a i s the degree of symme-\ntry correspondi ng to groupi ng level a. If t he next  level \nboundary  is found in the middle of t he group by  appl ying \nall groupi ng rul es, the next  groupi ng level’s the degree of \nsymmetry will b e like the one shown in Figure 4b. \nstart end start grouping le vel astart end start grouping le vel b endstart\nDiGPR5\n[time]0\nDiGPR5\n[time]0（a）\n（b）…\n11\n \nFigure 4 . Exam ples of sy mmetry level. \n3.2.2 C oncept  for paral lelism where \nGPR6 , MPR1 , and TSRPR4  rules fo r parallelism  are as \nfollows.  \nGPR6:  Where two or m ore segm ents of the m usic can \nbe construed as p arallel, th ey preferab ly form parallel \nparts of groups. \nMPR1:  Where t wo or m ore groups or part s of groups \ncan be const rued as paral lel, they preferabl y form  paral -\nlel m etrical stru ctures. \nTSRPR4: If two of m ore time-spans can be const rued \nas motivically an d/or rhythmically p arallel, p referab ly \nassig n them parallel h eads. where \nWe form alized the concept  of paral lelism and defi ned \nthe degree of parallel in each  rule, because the target \nstructures of the rules are different.  \nIn GPR 6, we focused on t he paral lelism of the seg-\nments. W e introduced t he degree of paral lel for GPR 6 \nDiGPR6, whi ch indicates a hi gh val ue at the start and end \nof the paral lel part  (Figure 5). The degree of paral lel  \nDiGPR6 was calculated by searching all the segm ents \nthroughout  the score. The l ength of the segm ents is \nfrom  a beat to a half of the score by every beat.  \nGPR6 has three adjustable param eters for controlling \nthe degree of paral lel: Wr (priority to the sam e rhythm \ncompared with the sam e register in  parallel seg ments), \nWs (priority to one end of a paral lel segm ent com pared \nwith the start o f the parallel segment), and Wl (priority  \nto larg e parallel seg ments) (0≦Wr, Ws , Wl ≦1). B y \nusing these param eters, a user can easi ly find and con-\nfigure the parallel seg ment. \nDiGPR6\ni 0\n…\n ⎭\nFigure 5 . Exam ple of t he degree of paral lel. ⎬⎫2σ\n⎩⎨⎧⎟⎠⎞⎜⎝⎛∑−∑−=\n= =GPR522 expend\nstartjji\nstartjj i ioi ioi D\n \n360   \n \n In MPR1, we focused on t he paral lelism of beat  in \ngroups. W e introduced t he degree of paral lel for MPR1 Di \nkMPR1, whi ch is calculated by  searchi ng al l the groups.  \nMPR1 has t wo adjust able param eters for controlling the \ndegree of parallel: Wr (weight of pri ority of t he sam e \nrhythm compared with the same register in  parallel  \ngroups), and TMPR1 (threshol d that deci des whet her beat  i \nand beat  k are p arallel ( Di kMPR1= 1) or not  (Di kMPR1= 0)). \nIn TSRPR4 , we fo cused on the parallelism  of time-\nspans, which are generat ed by  groupi ng st ructure and \nmetrical structure. W e introduced t he degree of paral lel \nfor TSRPR4 Di kTSRPR4, which is calculated by searching \nall th e tim e-spans. TSRPR1  has no adjust able param e-\nters for cont rolling the degree of paral lel. \n3.3 Resolving the preference rule confliction by pri-\noritizing  rules \nWe introduced adjustable parameters, Srule, for controlling \nthe strengt h of the GTTM  rules. By using these param eters, \nwe can acqui re the local-level strengt h of bound-\nary/beat/head. For exam ple, as  a resul t of applying the lo-\ncal-level groupi ng rul es, we can acqui re low-level groupi ng \nboundari es as wei ghted sum mations on t he groupi ng rul es \nresults DiGPR and adjustable param eter SGPR as fo llows: \n⎟⎟\n⎠⎞\n⎜⎜\n⎝⎛× × = ∑ ∑\n=′′=  )6,3,3,3,3,2,2(  \n  )6,3,3,3,3,2,2(    max\ndcbabajj GPR j GPR\niidcbabajj GPR j GPR\ni i S D S D B.(4) \n3.4 T op-dow n algorithm for cal culating hierarchi cal \nstructures \nWe introduced the top-down process for acqui ring the \nstructures. The hierarchal st ructure i s const ructed by  \ncalculating the local strengt h and choosi ng the next  \nlevel stru cture. \n-   Acquisition of grouping structure \nThe groupi ng structure is const ructed in the following way . \n(1) First, consider the whole piece of m usic as a group.  \n(2) Then, cal culate local-level boundary  strengt hs and \ndetect low-level boundari es. \n(3) Next , select the strongest  boundary  and divide the \ngroup at  the boundary . \n(4) Fi nally, iterate (3) whi le the local boundari es are \nfound at  the group. \n-   Acquisition of metrical structure \nThe metrical stru cture is co nstructed in the following way. \n(1) First, co nsider all th e beats as a lo west (global) level \nmetrical structure.  \n(2) Then, calculate the local -level m etrical strength. \n(3) Next , select the strongest  metrical structure from  \npossi ble structures. \n(4) Finally , iterate (2) and (3) while the current struc-\ntures have m ore than one beat . \n-   Acquisition of time-span tree \nThe tim e-span tree is co nstructed in the following way. \n(1) First, consider all the notes as a head.  \n(2) Then, calculate the lo cal-level head strength. \n(3) Next, select the next le vel head from  each tim e-span. \n(4) Fin ally, iterate (2 ) and (3) while the time-span con-\ntains more than one head.  4 STRUCTURE OF ATTA \nWe implemented the ext ended GTTM  descri bed above \non the com puter that we call ATTA. Figure 6 is the over-\nview of the ATTA whi ch consi sts of a groupi ng st ruc-\nture analyzer, a m etrical st ructure analyzer, and a tim e-\nspan tree analyzer. ATTA has three distinctive features, \nan XM L-based dat a structure, i ts implemented in Perl , \nand has a Java-based GUI. \nMusicXM L\nLow-Level boundary\n[time]boun dary strengthDetection of \nlow-level bou ndar y\nDetection of\nhigh-level boundary\nGroupi ngXM LDivide by top downApplying GP R1, 2, 3, 6 \nAppl ying G PR1, 2, 3, 4, 5, 6 (           )Bi\nCalcula tion of low-\nlevel beat strength\nChoosin g nex t\nlevel structure\nMetricalXML[time]Dilow-level\n(strength of be at)Applying M PR1,2,3,4,5Curr ent \nstructure\nChoi ce of \nnext level \nstructures\nChoosin g with applying M PR101ˆ=m\n2ˆ=m\n3ˆ=m\n4ˆ=m\n5ˆ=m\nYesNoCont ains more than one  beat\nCalculation of \nhead strength\nChoosing nex t\nlevel structureDitime-span\n(strength of he ad)Applying TSR PR1,3,4,8,9Curren t \nstructure\nNext level \nstructure\nTime -spanXMLYesNoContains m ore than one  headYesNoCont ains more than one  boun daryGrouping Stru cture Groupi ng St ructure \nAnalyzer Anal yzer\nMetrical Structure Metri cal S tructure \nAnalyzer Anal yzer\nTimeTime--span tree span tree \nAnalyzer Anal yzer\n \nFigure 6 . Processi ng flow of ATTA. \n4.1 X ML based data structure \nWe use an XM L form at for al l the input and out put data \nstructures of the ATTA. E ach analyzer of the ATTA \nworks i ndependent ly but are i ntegrated by  the XML-\nbased dat a structure. \nAs a pri mary input form at, we chose MusicXM L [14] \nbecause it provides a com mon ‘interlingua’ for m usic no-\ntation, anal ysis, retrieval, and ot her appl ications. We de-\nsigned Groupi ngXM L, M etricalXML, and Ti me-\nspanXM L as t he export  form ats for our analyzer. The \nXML form at is extrem ely qualif ied to express the hierar-\n \n361   \n \nchical groupi ng st ructures, m etrical structures, and time-\nspan trees. Not e that note elements in Groupi ngXM L, Met-\nricalXML, and Ti me-spanXM L are connected to note ele-\nments in MusicXM L, using Xpoi nter [15]  and Xl ink [16] . \nWe expect that the distri bution of a M usicXM L or a \nSMF, together wi th a groupi ng structure, metrical struc-\nture, and tim e-span tree, is useful for various m usical \ntasks such as searchi ng and arrangi ng. \n4.2 Implementation in Perl \nWe implemented the ATTA in  Perl so that using CGI \nallows it to be used t hrough t he internet (avai lable at \nhttp://staff.aist.g o.jp/m.hamanaka/atta/). W e believe that \nthe exhibition of this kind of resource i s very  important \nfor the m usic researching co mmunity. ATTA is th e first \napplicatio n for automatically acq uiring time-span tree. \nWe hope to benchm ark t he ATTA t o other sy stems, \nwhich hereafter will b e constructed.  \n4.3 Java based GUI \nAlthough our analyzer implemented in Perl  has a si mple \nuser interface, we also deve loped a graphical user inter-\nface in Java called GTTM ed itor (Figure 7). The GTTM \neditor has two  modes, the automatic an alysis an d man-\nual-edit modes. The autom atic-analysis m ode analyzes \nusing our a nalyzer and displays the results. The structures \nchange dependi ng on t he confi gured param eters. The \nmanual-edit mode assi sts in editing the groupi ng st ructure, \nmetrical structure, and time-span tree. It can be used to \nedit the resu lts of the automatic-an alysis m ode. \n \nFigure 7 . GTTM  editor (aut omatic-anal ysis mode). \n5 EXAMPLES OF ANALYSIS USING ATTA \nWe provi de in the appendi x all the expressi ons to implement \nthe ATTA, so t hat they may be hel pful for those users who \nintend to devel op ot her sy stems. In t his section we expat i-\nate how t o acqui re the groupi ng structure by  using ATTA. \n5.1 Detection of low-level boundaries  \nFigure 8 is the resul t of appl ying the local-level groupi ng \nrules, such as GPR 1, 2a, 2b, 3a, 3d and 61. We calculate \nthe degree of low-level boundary  Bi as the weighted sum -\nmation on the local-level groupi ng rul es resul ts DiGPR and \nadjustable param eter SGPR j. The t hreshol d Tlow-level decides \n                                                           \n1 The GTTM define the GPR6 for  large-level gr ouping r ules. However , \nwe also include it for  low- level gr ouping r ules, as manual analy zing \nresults based on GT TM by musicology  exper ts. if there is a low-level boundary  or not . In this case, seven \npositions are over the threshol d and fi ve posi tions are ap-\nplied to GPR1 . Therefo re, we can  acquire fiv e low-lev el \nboundari es as shown wi th the arrows i n Figure 8. \nTlow-level i 01\niDiGPR1\niDiGPR2a\niDiGPR2b\niDiGPR3a\niDiGPR3bBi\niDiGPR3c\niDiGPR3d\niDiGPR6Low-Level boundaries\nadjustable \nparametersSGPR j∑\n \nFigure 8 . Det ection of l ow-level boundari es. \n5.2 Detection of high-level boundaries  \nThe hierarchi cal groupi ng st ructure is const ructed in the \ntop-down m ethod (Fi gure 9). Fi rst of all, consi der a \nwhol e score as a group and cal culate the degree of high-\nlevel boundary  Di high-level boundary. Then sel ect the strong-\nest boundary  for t he next  level groupi ng boundary  as \nshown with the upward arrows i n Figure 9. Fi nally iter-\nate whi le the group cont ains low-level boundari es. \n…Low-leve l boundar ies\nCalcu late this w ay the d egree of high-level bou ndary  iterativelyi 0ii\n00\nboundary level high\niD  −\nboundary level high\niD  −\nboundary level high\niD  −\n \nTime-s pan tree \nadjustable \nparameters  \nGrou ping structure \nMetrical S tructure \nFigure 9 . Construction of hi erarchi cal groupi ng structure . \n6 EXPERIMENTAL RESULTS \nWe evaluated the perform ance of the m usic analyzer \nusing an F-m easure, whi ch is given by  the weighted \nharm onic mean of Preci sion P and Recall R, \nRPRPFmeasure+××=2 .                                                     (5) \nThis eval uation requi red us t o prepare correct  data of \na groupi ng st ructure, m etrical structure, and time-span \ntree. We collected a hundred pieces of 8-bar length, \nmonophonic, classical  music pieces, and asked m usicol-\nogy experts to  manually an alyze th em faith fully with  \nregard t o the GTTM , usi ng the manual-edit mode of \nJava GUI t o assi st in editing the groupi ng st ructure, \n \n362   \n \nmetrical structure, and tim e-span tree. Three other ex-\nperts crosschecked these manually produced resul ts. \nTo evaluate the baseline perform ance of our system , \nwe used the following default parameters: S rules=0.5, \nTrules=0.5,   Ws,=0.5 Wr =0.5, Wl=0.5, and σ=0.05. \nIn the current stage, the parameters are confi gured by  \nhumans, because the optim al values of the param eters \ndepend on a piece of m usic. When a user changes the \nparam eters, the hierarchical st ructures change as a result \nof the new analysis.  \nIt took us an average of about 10 m inutes per piece to \nfind the plausible tuning for t he set of param eters (Tabl e 1). \nAs a result of configuring the param eters, each F-measure \nof our analyzer outperform ed the baseline (Table 2). \n7 CONCLUSION \nWe devel oped a music anal yzing sy stem called ATTA, \nwhich derives the tim e-span tree of the GTTM. The fol-\nlowing three p oints are th e main resu lts of this stu dy. \n-    Proposed extended GTTM \nWe propose an ext ended GTTM  for com puter im-\nplem entation. The difficulty  with  the computer im-\nplementation of GTTM  has been desi gnated, how-\never no radi cal solutions have been proposed [17] . \nWe re-form alized the rules using a numerical ex-\npressio n with  adjustable parameters, so  that it can \nseparat e the definition and am biguity from  the ana-\nlyzed m aterial. \n-    Implemented ATTA on computer \nWe implem ented an actual working system  to ac-\nquire the hierarchi cal groupi ng st ructure, metrical \nstructure, and tim e-span tree of music, based on the \nGTTM. Th e ATTA au tomatically acq uirers th e time-\nspan t ree by  confi guring t he param eters wi thout \nmanually anal yzing by  expert s in musicology. -    Constructed a set of correct data  \nWe made a set of one hundred correct  data, whi ch is \nthe great est database of analyzed results from  the \nGTTM to  date. W e plan to exhibit this database in \nthe near future. \n-    Evaluated the performance of ATTA \nOur experim ental results showed that, as a result of \nconfi guring the param eters, our m usic anal yzer out-\nperform ed the baseline F-m easure. The set of pa-\nrameters that was tu ned for a certain  family of music \npieces would possibly refl ect the com mon features \nof the family. Thus, the idealized  parameter set fo r a \nmusic family, if any, would expectedly analyze a \nnew piece correctly, priort to hum an analysis. \nWe plan to devel op furt her sy stems, usi ng time-span \ntrees and the results of the m usic analyzer, for other \nmusical tasks, such as searchi ng, harm onizing, voicing, \nand ad-lib to indicate the e ffectiveness of im plem enting \nthe GTTM  to provi de m usic knowl edge. \nREFERENCES \n[1] Lerdahl, F., and Jackendoff, R. A Generative Theory \nof Tonal  Music. MIT Press, C ambridge, 1983. \n[2] Cooper, G., and M eyer, L. B. The Rhythmic Structure \nof M usic. The Uni versity of C hicago Press, 1960. \n[3] Narm our, E. The Anal ysis and C ognition of Basic \nMelodic Structure. The Univ ersity of C hicago Press, \n1990. \n[4] Temperley, D. The Congnition of Basic Musical \nStructures. M IT press, C ambridge, 2001. \n[5] Hirata, K., an d Matsu da, S. In teractiv e Music \nSummarization based on Generat ive Theory  of \nTonal  Music. Journal  of New M usic Research, 32: 2, \nTable 2 .F-measure for our m ethod.\nGroupi ng Structure Anal yzer Metrical Structure Analyzer Time-Span Tree Analyzer  \n \nMelodies Baseline  \nperformance Our method \nwith Configur ed \nparameters Baseline  \nperformance Our method \nwith Configur ed \nparameters Baseline  \nperformance Our method \nwith Configur ed \nparameters \n1. Moments musicaux \n2. W iegenlied \n3. Traumerei \n4. An die Freude \n5. Barcarolle  0.18 \n0.76 \n0.60 \n0.12 \n0.04 \n: 0.56 \n1.00 \n0.87 \n0.73 \n0.54 \n: 0.95 \n0.83 \n0.76 \n0.95 \n0.72 \n: 1.00 \n0.85 \n1.00 \n1.00 \n0.79 \n: 0.71 \n0.54 \n0.50 \n0.22 \n0.24 \n: 0.84 \n0.69 \n0.63 \n0.48 \n0.60 \n: \nTotal (100 m elodies) 0.46 0.77 0.84 0.90 0.44 0.60  Parameters Descrip tion \nSGPR j The strength of each grouping preference rule. j= (2a, 2b, 3a, 3b, 3c, 3d, 4, 5, 6) \nσ The standar d deviation of a normal distr ibution for  GPR5.  \nWs The priority to one end of a par allel segm ent co mpared with the start of a parallel segm ent. \nWr The priority to the sam e rhythm com pared with the sam e register in parallel segm ents. \nWl The priority to large parallel segm ents. \nTGPR4 The value of the thr eshold that decides whethe r GPRs 2 and 3 ar e relatively  pronounced or  not. Grouping str ucture \nTlow-level The value of the thr eshold that decides whet her transition i is a low- level boundar y or not. \nSMPR j The strength of  each m etrical pref erence rule.  j= (1,2,3,4,5a, 5b, 5c, 5d, 5e ,10) \nWr The priority to the sam e rhythm com pared with the sam e register  in par allel gr oups.  Metrical structure \nTMPR j The value of  the threshold that decides whether or not each rule is applicable.  j =(4, 5a, 5b, 5c) \nTime-span tr ee STSRPR j The strength of  each tim e-span tree pref erence rule. j= (1, 3a, 3b, 4, 8, 9) \n Table 1 .Adjustable param eters.\n \n363   \n \n165-177, 2003. \n[6] Todd, N. A M odel of Expressi ve Ti ming in Tonal  \nMusic. Musical Percept ion, 3: 1, 33-58, 1985. \n[7] Widmer, G. ' 'Underst anding and Learni ng Musical \nExpression' ', Proceedings of  International Com puter \nMusic Conference, pp. 268-275, 1993. \n[8] Hirata, K., and Hi raga, R . ''Ha-Hi-Hun plays \nChopin’s Etude'', Working Not es of IJC AI-03 \nWorkshop on Methods for Aut omatic Music \nPerform ance and their Applications in a Public \nRenderi ng C ontest, pp. 72-73, 2003. \n[9] Hirata, K., and M atsuda, S. ' 'Annotated M usic for \nRetrieval, Reproduction, and Sharing' ', Proceedings \nof Int ernational Computer M usic Conference, pp. \n584-587, 2004. \n[10] Ida, K., Hirata, K., and Tojo, S. ' ' The Attem pt of \nthe Aut omatic Anal ysis of the Groupi ng Structure \nand M etrical St ructure based on GTTM '', \nProceedings of Interna tional Com puter Music \nConference, SIG Techni cal Report, 2001(42): 49-54, \n2001 (i n Japanese).  \n[11] Touyou, T., Hirata, K., To jo, S., and Sat oh, K. ' ' \nImprovem ent of Groupi ng R ule Appl ication i n \nImplem enting GTTM' ', Proceedings of International \nCom puter Music Conference, SIG Technical Report, \n2002(47): 121-126, 2002 (i n Japanese).  \n[12] Nord, T. A. Toward Theoretical Verification: \nDevel oping a C omputer M odel of Lerdahl  and \nJackendoff’s Generative Theory  of Tonal M usic. \nPh.D. Thesis, The Un iversity o f W isconsin, \nMadison, 1992. \n[13] Hewlett, W. B. ed . Melo dic Sim ilarity Co ncepts, \nProcedures, and Appl ications, C omputing in \nMusicology 11, The M IT press, C ambridge, 1998. \n[14] Recordare LLC. MusicXML 1.0 Tutorial. \nhttp://www.  recordare.com /xml/musicxm l-\ntutorial.pdf., 2004. \n[15] W3C. XM L Poi nter Language (XPoi nter). \nhttp://www.w3.org/ TR/xptr/, 2002. \n[16] W3C. XM L Li nking Language (XLi nk) Versi on \n1.0. ht tp://www.w3.org/ TR/xlink/, 2001. \n[17] Heikki, V. Lerdahl  and Jackendoff R evisited. \nhttp://www.cc.jy u.fi/~heival ko/articles/lehr_jack.htm . \nAppendix 1: Grouping Structure analyz er \nStep 1: Calculation of basic parameters \nSix basi c param eters for not e transition i are calculated \nfrom MusicXML: rest i (interval  between current  offset  \nand next  onset ), ioii (inter-onset intervals), regi i (pitch \nintervals), leni (subt raction of durat ion), dyn i (subt raction \nof dy namics), and arti (subt raction of rat io between dura-\ntion of perform ed not e and proper durat ion of t he not e). Step2: Application of GPR \nGPR1 (Alternative form) \n⎩⎨⎧ ≥ ≤=+\n   lse          0      B B           11 i 1-i   GPR1\neB B andDi i\ni,     (6) \n⎟⎠⎟⎞\n⎜⎜\n⎝⎛× × = ∑ ∑\n=′′=  )6,3,3,3,3,2,2(  \n  )6,3,3,3,3,2,2(    max\ndcbaba jj GPR j GPR\niidcbaba jj GPR j GPR\ni i S D S D Bwhere \n. \nGPR2a (Slur/Rest) \n⎩⎨⎧ > <=+ −\nlsee          rest rest and rest rest          Di i i i GPR2a\ni011 1\n               (7)\n \nGPR2b (Attack-Point) \n⎩⎨⎧ > <=+ −\nlsee          ioi oii and ioi oii           Di i i i GPR2b\ni011 1\n                      (8)\n \nGPR3a (Register) \n⎩⎨⎧ > <=+ −\nlsee          regi regi and  regi regi          Di i i i GPR3a\ni011 1\n            (9)\n \nGPR3b (Dy namics) \n⎩⎨⎧ = ≠ ==+ −\nlsee          dyn and 0 dyn and dyn           Di i i GPR3b\ni00 0 11 1\n    (10)\n \nGPR3c (Articulation) \n⎩⎨⎧ = ≠ ==+ −\nlse          e and arti  and arti artiDi i i\ni00 0 0           11 1 GPR3c  (11) \nGPR3d (Length) \n⎩⎨⎧ = ≠ ==+ −\nlse          00 len and 0 len and 0 len           11 1 GPR3d\neDi i i\ni   (12) \nGPR4 (Intensification) \n( )\n⎩⎨⎧ >=\n    lse         0  P,P, P,P,P max        1   4 arti\nidyn\niregist\niioi\nirest\ni  GPR4\neTDGPR\ni  (13) \n arti arti    P   , \nlse                              0        0 regi        regi regiP   , dyn dyn    P  ,     P  ,  rest rest    P\n1\n1iarti\ni1\n11\n1i regist\ni1\n1idyn\ni1\n1iioi\ni1\n1irest\ni\n∑∑∑∑ ∑ ∑\n+\n−=+\n−=+\n−=+\n−=+\n−=+\n−=\n=\n⎪⎩⎪⎨⎧\n⎟⎟\n⎠⎞\n⎜⎜\n⎝⎛>== = =\ni\nijji\nijji\nijji\nijji\nijji\nijj\neioi ioi\nGPR5 (Sy mme try) where \n. \n⎪⎭⎪⎬⎫\n⎪⎩⎪⎨⎧\n⎟⎠⎞⎜⎝⎛∑−∑−=\n= =22\nGPR52 2 exp σend\nstartjji\nstartjj i ioi ioi D ,         (14) \nstart  :   start transiton of a group. where \nend   :   end transition of a group. \nGPR6 (Parallelism ) \n()\n()∑∑\n⎪⎪⎪\n⎩⎪⎪⎪\n⎨⎧\n== ×+−×= ×= −×\n=\njr\nj ij i    send\nr j i sstart\nr j ij i s end\nr j ij i sstart\nr j i\nGPR6\ni\nm                                                   m    W G W   Gm                                   W G m                           W G\nD\ns 0t 1eb 1\n ,   (15) \nwhere \ndivision : duration of a quarter note. \nr : length of parallel segments base d on the division of a quarter  note. \nlW\nr\nr  r-jq  r-iqr  r-jq  r-iqlW\nr\nr  r-jq  r-iqr  r-jq  r-iqend\nj ilW\nr\nr  jq  iqr  jq  iqlW\nr\nr  jq  iqr  jq  iqstart\nj i\nr Wzy\nr Wyz\nGr Wzy\nr Wyz\nG\n+ ++ +\n×× +×−× =×× +×−× =\n1 11 1\n) 1() 1( \n() integer  Gausian               division ioi qi\nkk i :][\n1⎥⎦⎤\n⎢⎣⎡=∑\n= \n⎪⎪\n⎩⎪⎪\n⎨⎧\n≠ ≠ ≠ ≠≠ ≠ = == = ≠ ≠\n=\n+ ++ ++ +\n  lsee    sq q  andq q and q q  andq q    t  q q  andq q and q q  andqq     eq q  andqq and q q  andq q     \nm\n 1j j  1i i  1-j j  1-i i 1j j  1i i  1-j j  1-i i 1j j  1i i  1-j j  1-i i\nj ib\n \n∑\n⎩⎨⎧ +≤ ≤\n=\nji j j i\nr iq  \nlsee       0rq q and q q         \n         x1  \n \n364   \n \n()\n∑∑∑∑\n⎪⎩⎪⎨⎧−= ×−\n= = =\nkll\nggk\ngg j i\nr jq iq\nlsee      ioi ioi division qq      \ny\n01\n1 1 \n()\n∑∑∑∑\n⎪⎩⎪⎨⎧= −= ×−\n= = =\nklj il\nggk\ngg j i\nr jq iq\nlsee   regi regi  and  ioi ioi division qq   \nz\n01\n1 1\n \nStep3: Detection of  low-level boundaries  \nThe degree of t he low-level boundary   Dilow-level boundary \nis expressed as fol lows. \n⎪⎩⎪⎨⎧ = >=−\nelse          D  and T B          D  GPR\nilevel low\ni boundary level-low\ni01 11\n   (16) \n \nStep4: Detection of high-level boundaries \nA group that contains a l ocal boundary  detected itera-\ntively by the next  level boundary  i  is calcu lated as fo llows. ˆ\n                              (17) boundary level high\ni\niD i   argmaxˆ−=\n \n∑\n=− −× × =\n)6,5,4,33,3,3,2,2(     \ndcbaba jj GPR j GPR\niboundary level low\niboundary level high\ni S D D D  \nAppendix 2: Metrical Structure analyz er \nStep1: Calculation of basic parameters \nCalculating from  MusicXM L and Groupi ngXM L \nfive basic param eters of a not e form  beat  i: velo i (veloc-\nity), valu i (length of note), vol i (durat ion of dy namic), \nslur i (length of sl ur), and num i (pitch). µvelo, µvalu, µvol, \nµslur, and µnum are the average of the basic param eters. \n \nStep2: Application of MPR \nMPR1 (Parallelism ) \n()\n⎪⎩⎪⎨⎧>−×+×=\n                                              lsee             T WyzWxy             DMPR1\nr\nk ik i\nr\nk ik iMPR1\nk i\n01 1\n,         (18)\n \ni   : beginning of a group start\ni end   : ending of a group \n∑ ∑\n=′ ′′\n=′ ′′\n⎩⎨⎧\n=>+\n⎩⎨⎧\n=>=endk\nstartkk kkendi\nstartii ii\nk ivelo   velo    \nvelo   velo             x0 00 1\n0 00 1  \n∑\n=′+′+\n⎩⎨⎧ > >=endi\nstartiiiik i\nk i                                       lsee   velo   and   velo             y00 0 1  \n∑\n=′−′+ −′+ ′ ′\n⎩⎨⎧ = = >=endi\nstartiiiik 1-iik i 1-i i\nk i                                                                                   lsee     num num  and num num and velo            z00 1\n \nMPR2 (Strong beat early ) \n()()start end end MPR2\ni i ii i D −−=                (19) \nMPR3 (Event) \n⎩⎨⎧\n=>=0 00 1\nii MPR3\nivelo             velo             D\n                          (20)\n  \nMPR4 (Stre ss) \n⎪⎩⎪⎨⎧ ××>=\n                                  lsee             T 2 velo             DMPR\nvelo i  MPR4\ni014µ\n                    (21)\n \nMPR5a (Long Pitch-Event) \n⎪⎩⎪⎨⎧ ××>=\n                                    lsee             T 2 valu             Da MPR\nvalu i MPR5a\ni015µ\n                  (22)\n \nMPR5b (Long Duration of Dy namic) \n⎪⎩⎪⎨⎧ ××>=\n                                 lsee             T 2 vol             Db MPR\nvol i MPR5b\ni015µ\n           (23)\n \nMPR5c (Long Slur) \n⎪⎩⎪⎨⎧ ××>=\n                                  lsee             T 2    slur           Dc MPR\nslur i MPR5c\ni015µ                     (24) MPR5d (Repetition of an Articulation Pattern) \n⎪⎩⎪⎨⎧ = ==+\n                                        lsee             1 D  and  1 D             DMPR5a\niMPR5a\ni MPR5d\ni011\n             (25)\n \nMPR5e (Pitch Repetition) \n⎩⎨⎧\n≠==\n++\n1i i1i i MPR5e\ninum num             num num             D01\n                                   (26)\n \nStep3: Cal culation of L ow-level beat strength \nLow-l evel beat  strengt h is calculated by weighted \nsummation of DiMPR j (=1,2,3,4,5a,5b,5c,5d,5e). \n∑⎪⎩⎪⎨⎧\n== ×+=\nkMPR1\nk iMPR1\nk iMPR1\nkimetrical  level-low\niD                            D           S BB D\n0 01\n,   (27)\n \n∑\n=× =\n edcba jj MPR j MPR\ni i S D B\n)5,5,5,5,5,4,3,2( . where \nStep4: Acquisition of hierarchical metrical structure \nWhen the current structure contains m ore than one \nbeat, the next  level structure  m is calcu lated  as fo llows: ˆ\nwhere ()\n() ∑\n⎪\n⎩⎪\n⎨⎧\n= ×=\n=\n= iMPR10 metrical level-low\nimetrical level-low\ni\nm              else                                                  3 mod m-i    S D2 mod m-i                     D\n  argmax m\n010\nˆ\n)5,4,3,2,1((28)\n . \nAppendix 3: Time-span tree analyz er \nStep1: Calculation of basic parameters \nFour basi c param eters of t he current  head (abst ract-\ning not e or non-abst racting note) i are calculated: rest i \n(interval  between current  head’s offset  and next  head’s \nonset ), ioii (inter onset intervals of heads), dot i (num ber \nof metrical dots), and pitch i (pitch). \n+ \nStep2: Application of TSRPR \nTSRPR1 (M etrical P osition) \njjiTSRPR1\ni dot dot D max=\n                                                (29)\n \nTSRPR3a (Higher Melodic Pitch) where \njji i pitch pitch D maxTSRPR3a=                                        (30) \nTSRPR3b (Lower Bass Pitch) \njji i pitch pitch D max 1TSRPR3b−=                                    (31) \nTSRPR4 (Parallelism ) \n⎩⎨⎧ = = ==+ + −\n                                                           lsee       ioi ioi  ,ioi ioi  , ioi ioi       Dk 1i k i k 1-i TSRPR4\nk i011 1  (32) \n. TSRPR8 (Structural Beginning) \n⎪⎩⎪⎨⎧ ==\nelse              ii               Dstart\nTSRPR8\ni01                                           (33) \nTSRPR9 (Structural Ending) \n⎪⎩⎪⎨⎧ ==\nelse              ii               Dend\nTSRPR9\ni01                                             (34) \nStep3: Cal culation of head strength \nThe st rengt h of a head i s calculated by weighted \nsummation of DiTSRPR j (=1, 3a, 3b, 4, 8, 9). \n∑⎪⎩⎪⎨⎧\n== ×+=\nkTSRPR4\nk iTSRPR4\nk iTSRPR4\nkispan-time\niD                            D           S BB D\n0 01\n,                   (35) \n∑\n=× =\n)9,8,3,3,1(  \nba jj TSRPR j TSRPR\ni i S D B . where \nStep4: Acqui sition of next l evel heads \nWhen a tim e-span contains m ore than one head i and \nj, the next  level head  h  is calcu lated  as fo llows: ˆ\n⎪⎩⎪⎨⎧ ≥=\nlsee          jD D           ihspan- time\njspan- time\ni ˆ\n.                                   (36)\n \n \n365"
    },
    {
        "title": "Symbolic Representation of Musical Chords: A Proposed Syntax for Text Annotations.",
        "author": [
            "Christopher Harte",
            "Mark B. Sandler",
            "Samer A. Abdallah",
            "Emilia Gómez"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415114",
        "url": "https://doi.org/10.5281/zenodo.1415114",
        "ee": "https://zenodo.org/records/1415114/files/HarteSAG05.pdf",
        "abstract": "In this paper we propose a text represention for musical chord symbols that is simple and intuitive for musically trained individuals to write and understand, yet highly structured and unambiguous to parse with computer programs. When designing feature extraction algorithms, it is important to have a hand annotated test set providing a ground truth to compare results against. Hand labelling of chords in music files is a long and arduous task and there is no standard annotation methodology, which causes difficulties sharing with existing annotations. In this paper we address this problem by defining a rigid, contextindependent syntax for representing chord symbols in text, supported with a new database of annotations using this system. Keywords: Music, Chords, Harmony, Notation, Annotation 1",
        "zenodo_id": 1415114,
        "dblp_key": "conf/ismir/HarteSAG05",
        "keywords": [
            "text representation",
            "musical chord symbols",
            "musically trained individuals",
            "computer programs",
            "hand annotated test set",
            "ground truth",
            "rigid context-independent syntax",
            "new database of annotations",
            "music files",
            "standard annotation methodology"
        ],
        "content": "SYMBOLIC REPRESENT ATION OFMUSICAL CHORDS: APROPOSED\nSYNT AXFOR TEXT ANNO TATIONS\nChristopher Harte, Mark Sandler andSamer Abdallah\nCentre forDigital Music\nQueen Mary ,University ofLondon\nMile EndRoad, London, UK\nchristopher.harte@elec.qmul.ac.ukEmilia G´omez\nMusic Technology Group\nIUA,Universitat Pompeu Fabra\nBarcelona, Spain\nemilia.gomez@iua.upf.es\nABSTRA CT\nInthispaper wepropose atextrepresention formusical\nchord symbols thatissimple andintuiti veformusically\ntrained individuals towrite and understand, yethighly\nstructured andunambiguous toparse with computer pro-\ngrams.\nWhen designing feature extraction algorithms, itis\nimportant tohaveahand annotated testsetproviding a\nground truth tocompare results against. Hand labelling of\nchords inmusic \u0002les isalong andarduous task andthere\nisnostandard annotation methodology ,which causes dif-\n\u0002culties sharing with existing annotations. Inthis pa-\nperweaddress thisproblem byde\u0002ning arigid, conte xt-\nindependent syntax forrepresenting chord symbols in\ntext,supported with anewdatabase ofannotations using\nthissystem.\nKeywords: Music, Chords, Harmon y,Notation, Anno-\ntation\n1INTR ODUCTION\nWhen dealing with largedigital music collections, itbe-\ncomes necessary todevelop technology capable ofdealing\nwith these collections inameaningful way.Much effort\nwithin themusic information retrie valcommunity isde-\nvoted toautomatically describing thecontent ofMIDI and\naudio recordings.\nHarmon yisoneofthemain axesofmusic descrip-\ntion. Manyresearchers inthis\u0002eld trytoautomatically\ndescribe theharmonic content ofapiece ofmusic, see\nforinstance Fujishima (1999); Harte andSandler (2005);\nSheh andEllis (2003); Yoshioka etal.(2004). There is\nnostandard methodology forchord annotation. When de-\nsigning chord detection algorithms, thelack ofannotated\ndatabases makesevaluation andcomparison ofresults dif-\n\u0002cult. This problem isnotspeci\u0002c toharmonic analy-\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonsis,butisrelevantinmanyareas ofMIR related research.\nWeattempt toaddress thisproblem byproposing arigidly\nstructured general annotation system forchords. Such an\nannotation system willafford researchers theopportunity\ntoshare annotated \u0002les easily .Thesystem presented here\niseasy formusically trained individuals towrite andun-\nderstand, yetsimple andunambiguous toparse with com-\nputer programs.\nWealso provide areference database ofannotations\nforsongs from TheBeatles back catalogue using thisrep-\nresentation attheCentre forDigital Music website.The\nannotations arein.lab transcription \u0002leformat, com-\npatible with theopen source wave\u0002leeditor Wavesurfer\nSj¨olander andBesk ow(2000); Gouyon etal.(2004).\nInthebackground section wediscuss some oftheno-\ntation methods used indifferent parts ofthemusic com-\nmunity .Inclassical western harmon ynotation, astyle de-\nveloped forscore analysis, certain chord symbols depend\nupon themusical keyconte xtfortheir fullmeaning tobe\napparent. Incontrast, jazz andpopular music notations\naremore commonly used forperformance andaregen-\nerally more explicit intheir meaning toavoidbeing mis-\nread. Text\u0002leannotations areoften astraight translation\nofanindividual' spreferred musical notation tothenearest\ntextual equivalent. This lack ofstandardisation cancause\nmanyproblems when other people come toread andin-\nterpret theannotated symbols. Toaddress thisproblem,\nweintroduce ageneral logical model foramusical chord\ninSection 3.This isused inSection 4tode\u0002ne therules\nandsyntax forarepresentation forchords in\u0003attextwith a\nformalised description ofthesyntax giveninBackus-Naur\nForm(BNF) Ledg ardandMarcotty (1981). Section 5cov-\nerstheuseofourrepresentation inmaking anannotation\ndatabase using Wavesurfer andMatlab tools formanipu-\nlating thetranscription \u0002les itproduces.\n2BACKGR OUND\nWhen twoormore notes areplayed simultaneously ,a\nchord isproduced. InWestern tonal music, anymusical\nchord may berepresented with thefollowing information:\n\u000fTherootnote ofthechord; thenote upon which the\nchord isbuilt.\n\u000fItstype orquality ,de\u0002ned bythecomponent inter-\nvalsthatmakeupthechord relati vetotheroot.\n66\u000fItsinversion,de\u0002ned bythedegree ofthechord\nplayed asitsbass note.\nThese parameters remain consistent forallthedifferent\nwaysinwhich notes ofaparticular chord may beplayed,\norvoiced Taylor (1989).\n2.1 Styles ofNotation\nThere aremanystyles ofharmon yandchord notation in\nmusic. These conventions canvarynotonly across genres\nbutalsowithin them. Toillustrate some ofthevariation in\nchord notation methods, severalstyles areshownforthe\nshort excerpt ofmusic inFigure 1(a).\n2.1.1 FiguredBass\nThe\u0002rststyle, inFigure 1(b) istheBaroque FiguredBass .\nThis wasasystem of\u0002gures written underneath abass line\nindicating which interv alsshould beplayed abovethebass\nnote tocomplete thecorrect harmon yTaylor (1989).\n2.1.2 Classical Harmony Analysis\nInclassical Western harmon yanalysis, chord notation was\ndeveloped toshowthesequential aspects ofharmon yor\nharmonic progression rather than justtheparticular chord\norsonority atanygiveninstant Tagg(2003). Figure 1(c)\nshowsRoman numeral style notation. Chords arelabelled\naccording totheposition oftheir rootnote within thescale\nrelated tothecurrent keyTaylor (1989). Inversions are\nmark edwith `b'for\u0002rstinversion, `c'forsecond inversion\nandsoonifthechord hasfurther degrees. The notation\nshowninFigure 1(d) with letters denoting theroot notes\nofchords isalso common inclassical analysis. Inboth\ncases major chords areshownwith uppercase characters\nandminor chords inlowercase.\nInclassical notation, because chords arenotated inthe\nconte xtofagivenkey,certain properties areimplied rather\nthan explicitly mark ed.Forexample, inamajor key,the\nseventh degree ofthekeyscale isamajor seventh inter-\nval,soinmarking atonic major seventh chord `I7'with\nasuperscript 7,themajor seventh isimplied. However,a\ndominant seventh chord, byde\u0002nition, contains aminor\nseventh interv albutitisalso mark edwith asuperscript 7\n`V7'(see second baroftheexample inFigure 1(c)). In\ntheRoman numerals system itisclear that`V7'isadomi-\nnant chord butwhen using letters asshowninFigure 1(d)\nthiscanbecome asource ofambiguity .Theextract isin\nthekeyofCmajor sothe\u0002rstchord ismark edC7butthe\ndominant chord inthesecond barismark edG7.Ifthe\nkeyconte xtislostfrom thisnotation, which isapossibil-\nityifstoring these symbols inatext\u0002le, then there can\nbenosure wayoftelling which quality ofseventh chord\nthetranscriber intended without trying toinfer theconte xt\nfrom thechord progression.\n2.1.3 JazzandPopular Music\nInpopular music andjazz, therole ofchord symbols is\nmore tailored foruseinperformance, with jazzmusicians\ninparticular often playing atsight. Forthisreason chords\narenotated inamuch more explicit manner sothatmu-\nsicians need spend theminimum oftime andthought to\ncorrectly workoutwhat theyarerequired toplay.The\n7 7 6\n36\n46\n47 5\n4 3\nR\nI IVc ii VII c V I IVb7 7 7 7a)\nb)\nc)\nd)\ne)C d F/CC major:\nF/A B° /F G C C major:7 7 7 7\nCM7 Dm7 F/C F/A Fdim7 G7 Csus4 C\nf) C^7 D   7 F,/C F,/A F07 G7 C, Csus4--\nFigure 1:Ashort extract ofmusic inCmajor with differ-\nentharmon ynotations: a)Musical score b)Figured bass,\nc)Classical Roman numeral, d)Classical letter ,e)Typical\nPopular music guitar style, f)Typical jazznotation\nqualities ofchords aremark edexplicitly butthemarkings\nthatareused varywidely anditishard to\u0002ndtwopeople\nwho agree onapreferred style foreverychord type.\nThe \u0002rst chord oftheexample inFigure 1(a), aC\nmajor seventh, may bemark edasCM7, CMaj7, orC47\nCoker(1964) asseen inFigure 1(e) and1(f). Thesecond\nchord intheexample, aDminor seventh, may bemark ed\nDm7, Dmin7 andD\u00007.TheGseventh chord inthesec-\nondbarcanbemark edG7orG7orsometimes Gdom7,\nalthough thislastmarking isoften incorrectly applied in\ncases where theseventh chord does notactually function\nasadominant chord. Inversions aremost often denoted\nbyanoblique strok e(=)followed bythebass note tobe\nplayed. This canbeseen with theinverted Fmajor chords,\nF/CandF/A, attheendofthe\u0002rstbaroftheexample Tay-\nlor(1989).\nAmbiguity between chord symbols canoccur when\ntranslated to\u0003attextifthenotation convention used by\nthetranscriber isnotgiven.Forexample, ifanannotation\ncontains thesymbolA7,thiscould beaseventh chord in\njazz notation orinclassical notation ifinthekeyofD.\nHowever,itcould alsobeamajor seventh chord inclassi-\ncalnotation ifinthekeysofAorEmajor .Itistoavoid\nthiskind ofambiguity that wepropose theadoption of\nthechord symbol representation outlined inthefollowing\nsections.\n3AMODEL FOR MUSICAL CHORDS\nWenowde\u0002ne amodel torepresent chords unambigu-\nously andindependent ofkeyconte xt.Therootisde\u0002ned\nasanote element which hasanabsolute pitch class value.\nThelistofcomponent interv alsandthebass note arede-\n\u0002ned asdegrees,relati vetotheroot note. Adiagram of\nthismodel isshowninFigure 2.\nWede\u0002ne sevennatur alnote names (letters AtoG,\neqn. 1),which correspond tothewhite keysonapiano\n67Chord\nListnote\ndegreeroot\ncomponent sbass\ndegreedegree\nFigure 2:Model forchord de\u0002nition\nkeyboard. Wealso de\u0002ne thirteen intervals (numbers 1\nto13,eqn. 2),which correspond tothemajor diatonic in-\ntervals(i.e. theyareeither major orperfect) uptoone\noctaveplus asixth (showninFigure 3).Toallowcorrect\nspelling ofenharmonics wealso de\u0002ne twomodi\u0002er op-\nerators, sharp and\u0003at.Thus:\nnatur al=fAjBjCjDjEjFjGg (1)\ninterval =f1j2j3\u0001\u0001\u000111j12j13g (2)\nmodi\u0002er =sharp j\u0003at (3)\nNaturals andinterv alsmay beoperated onbythese modi-\n\u0002ers. Inthisway,notes anddegrees may bede\u0002ned as:\nnote =natur aljmodi\u0002er (note) (4)\ndegree=interval jmodi\u0002er (degree) (5)\nAnexample model ofachord isshowninFigure 4.The\nchord intheexample isaCminor seventh chord in\u0002rst\ninversion. Theroot ofthischord isaC.Thecomponent\ninterv alsareaminor third, aperfect \u0002fth andaminor sev-\nenth ([3,5,[7).Thebass note ofa\u0002rstinversion chord is\nits3rddegree, which inthisexample isanE[.\nThesharp and\u0003atmodi\u0002ers allowproper enharmonic\nspelling ofnotes andinterv als.This isimportant incases\nsuch asthediminished seventh chord (comprising themu-\nsical interv als[3,[5,[[7)which contains adiminished\nseventh interv al(amajor seventh interv al\u0003attened twice).\nAlthough this interv alistonally equivalent toamajor\nsixth, ithasadifferent musical function.\naaaaaaaaaaaaaaaaaaa3!!!!!!\n!!!!!!! !!!!\n!!!!!\n!!!!!!\n1 M2M3P4P5M6 M7oct8910 1112 13 Interval:\nStep: T T S T T T S T T S T T\nFigure 3:TheMajor diatonic interv alsupon middle C.`T'\ndenotes astepofatone between adjacent interv alsand`S'\nasemitone.\n4REPRESENTION OFCHORDS IN\nFLA TTEXT\nInthissection wedevelop ageneral system fornotating\nchords in\u0003attextthatisboth musically intuiti veand\u0003ex-Croot bass\nListcomponent s\nflat\n3flat\n7 5flat\n3Chord\nFigure 4:Example model ofa\u0002rst inversion Cminor -\nseventh chord\niblebutatthesame time rigidly structured. Thebasic syn-\ntaxofthenotation isoutlined inSection 4.1.Ashorthand\nsystem using avocabulary ofprede\u0002ned labels forcom-\nmon chords isintroduced inSection 4.2. Finally ,afor-\nmalised description ofthesyntax forthesystem isgiven\ninBackus-Naur FormLedg ardandMarcotty (1981) inTa-\nble1.\n4.1 Developing aSyntax forChord Notation\nItisimportant foruseintextannotation thatchord sym-\nbols beconte xtindependent. Using thechord model de-\nscribed inSection 3andaconte xtindependent approach\ntonotation, similar totheJazz style described inSection 2,\nwede\u0002ne thefollowing syntax forrepresenting achord in\n\u0003attext:\nroot:(degree1, degree2...) /bass\nTheroot note iswritten \u0002rstfollowed byacolon (:)sep-\narator .Acomma delimited listofthechord degrees is\nthen written, contained byparentheses. Finally ,anop-\ntional bass note may beadded attheendafter aforw ard\nslash character (/)ifitisdifferent totheroot. Thenatu-\nrals, interv alsandmodi\u0002ers arede\u0002ned inTable 1follow-\ningequations 1to3.The sharp and\u0003ataresigni\u0002ed by\nthehash symbol#andthelowercasebrespecti vely.\nTokeepthenotation musically intuiti ve,note modi-\n\u0002ers come after naturals soA[becomesAb.Degree mod-\ni\u0002ers come before interv alssoa\u0003attened seventh becomes\nb7.Anextrachord state denoted byasingle uppercaseN\nisalso added tosignify `nochord' tomark silence orun-\ntuned, possibly percussi vemusical material. Toresolv e\nthepossible ambiguity between anoteBanda\u0003atmodi-\n\u0002erbthenotation isnecessarily case sensiti ve.\nFollowing these rules, allchords may nowbede-\nscribed in\u0003attextinanunambiguous manner .Forex-\nample, using oursystem aCmajor chord becomes:\nC:(3,5)\nLikewise, aCminor chord becomes:\nC:(b3,5)\nAmore comple xchord such asaD]minor seventh chord\ninsecond inversion with anadded ninth would become:\nD#:(b3,5,b7,9)/5\n68Table 1:Syntax ofChord Notation inBackus-Naur Form\n<chord> ::=<note>\":\"<shorthand> [\"(\"<degree-list>\")\"][\"/\"<degree>]\nj<note>\":\"\"(\"<degree-list>\")\" [\"/\"<degree>]\nj<note>[\"/\"<degree>] j\"N\"\n<note> ::=<natural> j<note><modifier>\n<natural> ::=AjBjCjDjEjFjG\n<modifier> ::=bj#\n<degree-list> ::=[\"*\"]<degree> [\",\"<degree-list>]\n<degree> ::=<interval> j<modifier> <degree>\n<interval> ::=1j2j3j4j5j6j7j8j9j10j11j12j13\n<shorthand> ::=maj jmin jdim jaug jmaj7 jmin7 j7jdim7 jhdim7\njminmaj7 jmaj6 jmin6 j9jmaj9 jmin9 jsus4\nTable 2:Shorthand de\u0002nitions forcommon chords\nChord Type Shorthand Notation Components List\nTriadChords:\nMajor maj (3,5)\nMinor min (b3,5)\nDiminished dim (b3,b5)\nAugmented aug (3,#5)\nSeventh Chords:\nMajor Seventh maj7 (3,5,7)\nMinor Seventh min7 (b3,5,b7)\nSeventh 7 (3,5,b7)\nDiminished Seventh dim7 (b3,b5,bb7)\nHalf Diminished Seventhhdim7 (b3,b5,b7)\nMinor (Major Seventh)minmaj7 (b3,5,7)\nSixth Chords:\nMajor Sixth maj6 (3,5,6)\nMinor Sixth min6 (b3,5,6)\nExtended Chords:\nNinth 9 (3,5,b7,9)\nMajor Ninth maj9 (3,5,7,9)\nMinor Ninth min9 (b3,5,b7,9)\nSuspended Chords:\nSuspended 4th sus4 (4,5)\n69Figure 5:Partofthetranscription \u0002leforNoReply byTheBeatles displayed inWavesurfer\n4.2 Shorthand Notation\nOur chord representation isstraightforw ardandcapable\noffully describing anychord within Western tonal music.\nHowever,formanual annotation purposes, theinclusion\nofmore musically intuiti veshorthand labels forcommon\nchords isauseful extension. Aproposed vocabulary of\nshorthand labels isgiveninTable 2where each label is\nunderstood asapre-set listofdegrees. Inthisway,achord\nmay nowalsobede\u0002ned by:\nroot:shorthand(extra-degrees) /bass\nAcommon convention forlabelling thequality ofchords\nis:major `M', minor `m', augmented `+'anddiminished\n`o'.Wechoose more verbose labels, however,because it\nmakestypographic errors inannotations easier todetect.\nProvision forextra degrees inparentheses isleftsothat\nadditional interv alsmay beadded tocommon chords. To\nmaketheshorthand system more \u0003exible aspecial `omit\ndegree' symbol, anasterisk*,isalso added todenote a\nmissing interv alfrom ashorthand notated chord. Hence a\nCminor seventh chord could become:\nC:min7 \u0011C:(b3,5,b7)\nandaCminor seventh with anadded 11th degree butno\n5thdegree could bewritten:\nC:min7(*5,11) \u0011C:(b3,b7,11)\nTostay consistent with most chord notation styles, aroot\nnote onitsown(i.e. with noshorthand label orde\u0002ned\ndegrees) isassumed todenote amajor chord. Therefore a\nCmajor chord may bewritten simply as:\nC \u0011C:maj \u0011C:(3,5)\nLikewise, arootnote followed directly byaforw ardslash\nandabass note isassumed tobeamajor chord inanin-\nverted form. Forexample a\u0002rstinversion Amajor chord\ncould bewritten:\nA/3 \u0011A:maj/3 \u0011A:(3,5)/3\nAdded note chords should beexplicitly labelled asmajor\norminor toavoidconfusion. Therefore aCmajor with an\nadded fourth becomes:\nC:maj(4) \u0011C:(3,4,5)5ANNO TATED COLLECTION\nAdatabase ofannotations ofsongs from TheBeatles back\ncatalogue isavailable attheCentre forDigital Music web-\nsite1.Theannotations areintheform of.lab transcrip-\ntion\u0002les compatible with theWavesurfer audio editor(see\nFigure 5).The transcription \u0002les contain alistofchord\nsymbols, each with anassociated start time andendtime.\nAsetofMatlab tools isalsoavailable atthesame web\naddress formanipulating chord symbols andtranscription\n\u0002les toaidannotation.\n6CONCLUSIONS\nInthispaper wehavepresented ageneral chord represen-\ntation syntax foruseintextannotations. Therepresenta-\ntion isanextendible format thatwill afford much easier\nsharing ofannotations between researchers. Tothisend\nwehavemade adatabase ofannotations using thissyn-\ntaxavailable ontheinternet andadedicated wavesurfer\nplug-in forchord annotation isplanned forthefuture.\nOne area where annotations inthisstyle may prove\nparticularly useful isinproviding benchmark testsetsfor\nevents such astheISMIR contest. Itisalso possible\nthatthisstyle ofrepresentation could \u0002nduses elsewhere\ninmusic meta-data standards such asMPEG-7 andMu-\nsicXML.\nOurrepresentation isextensible. Additions tothelist\nofshorthand labels canbemade easily because each la-\nbelissimply amacro de\u0002nition ofasetofinterv als.Ex-\ntensions tothenotation such astheinclusion ofmicro-\ntonal interv alsisapossibility forthefuture. This kind\nofenhancement could easily beachie vedusing additional\nswitch characters todenote thepresence ofsuch informa-\ntion.\nACKNO WLEDGEMENTS\nThis research isfunded byEU-FP6-IST -507142 project\nSIMA C(acron ymforSemantic Interaction with Music\n1www .elec.qmul.ac.uk/digitalmusic/\n70Audio Contents). More information canbefound atthe\nproject website http://www .semanticaudio.or g.\nREFERENCES\nJ.Coker.Impr ovising Jazz.Simon andSchuster ,New\nYork,1964.\nT.Fujishima. Realtime chord recognition ofmusical\nsound: Asystem using common lispmusic. InPro-\nceedings ofICMC ,1999.\nF.Gouyon, N.Wack, andS.Dixon. Anopen source tool\nforsemi-automatic rhythmic annotation. InProceed-\nings ofDAFx,2004.\nC.Harte andM.Sandler .Automatic chord identi\u0002ca-\ntionusing aquantised chromagram. InProceedings of\nAES118th Convention, Barcelona ,2005.\nH.Ledg ardandM.Marcotty .The Programming Lan-\nguageLandscape .Science Research Associates, Inc.,\nChicago, 1981.\nA.Sheh andD.P.Ellis. Chord segmentation andrecog-\nnition using em-trained hidden mark ovmodels. InPro-\nceedings ofICMC ,2003.\nK.Sj¨olander andJ.Besk ow.Wavesurfer -anopen source\nspeech tool. InProceedings oftheInternational Con-\nference onSpok enLangua geProcessing ,2000.\nP.Tagg. Tagg'sharmon yhandout, available at\nwww .tagg.or g.Version 3,(accessed 21/04/05) ,2003.\nE.Taylor .TheABGuide toMusic Theory Part1.ABRSM\nPublishing Ltd, Portland Place, London, UK, 1989.\nT.Yoshioka, T.Kitahara, K.Komatani, T.Ogata, and\nH.G.Okuno. Automatic chord transcription with con-\ncurrent recognition ofchord symbols andboundaries.\nInProceedings ofProceedings ofISMIR ,2004.\n71"
    },
    {
        "title": "MUCOSA: A Music Content Semantic Annotator.",
        "author": [
            "Perfecto Herrera",
            "Òscar Celma",
            "Jordi Massaguer",
            "Pedro Cano",
            "Emilia Gómez",
            "Fabien Gouyon",
            "Markus Koppenberger"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415980",
        "url": "https://doi.org/10.5281/zenodo.1415980",
        "ee": "https://zenodo.org/records/1415980/files/HerreraCMCGGK05.pdf",
        "abstract": "MUCOSA (Music Content Semantic Annotator) is an environment for the annotation and generation of music metadata at different levels of abstraction. It is composed of three tiers: an annotation client that deals with microannotations (i.e. within-file annotations), a collection tagger, which deals with macro-annotations (i.e. acrossfiles annotations), and a collaborative annotation subsystem, which manages large-scale annotation tasks that can be shared among different research centres. The annotation client is an enhanced version of WaveSurfer, a speech annotation tool. The collection tagger includes tools for automatic generation of unary descriptors, invention of new descriptors, and propagation of descriptors across sub-collections or playlists. Finally, the collaborative annotation subsystem, based on Plone, makes possible to share the annotation chores and results between several research institutions. A collection of annotated songs is available, as a “starter pack” to all the individuals or institutions that are eager to join this initiative.",
        "zenodo_id": 1415980,
        "dblp_key": "conf/ismir/HerreraCMCGGK05",
        "keywords": [
            "MUCOSA",
            "Music Content Semantic Annotator",
            "environment",
            "annotation and generation",
            "different levels of abstraction",
            "annotation client",
            "collection tagger",
            "collaborative annotation subsystem",
            "WaveSurfer",
            "speech annotation tool"
        ],
        "content": "MUCOSA: A MUSIC CONTENT SEMANTIC ANNOTATOR\nPerfecto Herrera, Òscar Celma, Jordi Massaguer, Pedro Cano, Emilia Gómez,  \nFabien Gouyon, Markus Koppenberger, David García,  \nJosé-Pedro García, Nicolas Wack \nUniversitat Pompeu Fabra \nPg. Circumval·lació 8 \nBarcelona, 08003 Spain \npherrera, ocelma, jmassaguer, pcano, egomez,  \nfgouyon, koppi, dgarcia, jpgarcia, nwack@iua.upf.es \n \nABSTRACT \nMUCOSA (Music Content Semantic Annotator) is an \nenvironment for the annotation and generation of music metadata at different levels of abstraction. It is composed of three tiers: an annotation client that deals with micro-annotations (i.e. within-file annotations), a collection tagger, which deals with macro-annotations (i.e. across-files annotations), and a collaborative annotation subsys-\ntem, which manages large-scale annotation tasks that can \nbe shared among different research centres. The annota-tion client is an enhanced version of WaveSurfer, a speech annotation tool. The collection tagger includes \ntools for automatic generation of unary descriptors, in-vention of new descriptors,  and propagation of descrip-\ntors across sub-collections or playlists. Finally, the col-laborative annotation subsystem, based on Plone, makes possible to share the annotation chores and results be-tween several research institutions. A collection of anno-tated songs is available, as a “starter pack” to all the in-dividuals or institutions that are eager to join this initia-tive.   \nKeywords: Semantic descriptors, music tagging, audio \nannotations, audio music content processing, music da-\ntabases.  \n1 INTRODUCTION AND MOTIVATION \nThe growing amount of digital music is driving the need \nfor effective methods for indexing, searching, and re-trieving of music based on its content. While recent ad-vances in content analysis, f eature extraction, and classi-\nfication are improving the capabilities for effectively searching and filtering digital music content, the process of reliably and efficiently indexing multimedia data is \nstill a challenging issue. Manual indexing of music col-\nlections has been attempted in different moments and contexts, but most of the attempts have been tagging artists and songs in a global way (i.e. assigning tags to a whole song, artist or recording). Micro-annotations, on the other hand, are required in order to compute predic-tive models of certain musical features. Micro-annotations may provide solid ground-truths for training artificial systems to automatically compute features like beats, chords, instruments and structural units. The mod-els induced by these artificia l systems can be exploited, \nin a second phase, for acceler ating the annotation proc-\ness itself, which, on its turn, should help to improving the quality of the inductive models, and so on. Descrip-tors generated thanks to micro-annotations are also used as building blocks for computing models for automatic labelling of higher-level descriptors that can be then exploited in macro-annotations. \nBack in 1992, the visionary Marvin Minsky stated: \n“the most critical thing, in both music research and gen-eral AI research, is to learn how to build a common (…) database” [1]. It seems that , more than a decade later, \nwe are recognizing its value, and some useful reflec-tions and recommendations have been discussed in [2]. Apart from lacking of a clear methodology and theory for annotating, and from having to deal with an ill-posed problem, building an annotated database of mu-\nsic, specially in the case of micro-annotations, is very expensive and time-consuming. Motivated by that, we have devised an environment that alleviates the individ-ual cost of annotating songs and music collections by maximizing the synergies between different research groups.  \nIn this paper we present MUCOSA (MUsic COntent \nSemantic Annotator), an annotation environment to al-low authors to semi-automatically annotate music con-tent with semantic descriptions. MUCOSA is a three-tiered environment consisting on an annotation client, a collection tagger, and a collaborative annotation man-ager. The tools included in this environment explore or will explore a number of interesting capabilities as auto-\nmatic segmentation, summarization, automatic label \npropagation, and template annotation propagation to \nsimilar segments and files. MUCOSA also includes an administrative web interface for the management of descriptors, users, groups of annotators, and annotation tasks. One of its most interesting features for the Music Information Retrieval community is that it allows an incremental shared-cost-and-benefit approach to getting a universally available corpus of annotated audio music files (i.e. the more, the merrier ). We are taking advan-\ntage, in this respect, of Creative Commons’ licensing \nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full citation on the first page. \n© 2005 Queen Mary, University of London \n77   \n \n \nschemes1, that make possible to get and give “free-for \nresearch-purposes” access to moderate-size music col-\nlections in audio format.  \nIn the forthcoming sections, we first present some re-\nlated annotation systems, then we move on to the pres-entation of the three components of the proposed envi-ronment, that is, the MUCOSA annotation client, the \nMUCOSA collection tagger, and the Collaborative An-\nnotation subsystem.  \n2 BACKGROUND  \nThere have been several annotators available for the \nvideo world: VideoAnnEx [3], developed by IBM, is \none of the most famous, and it has operated as an inspi-rational tool for the MUCOSA environment. Between 2001 and 2003, a more-than-a-hundred community of annotators, from twenty research centres, amassed a total amount of 100 annotated hours of video using \nVideoAnnEx [4]; the annotated labels were served as a \nfoundation for several TREC-video retrieval systems [5]. IBM also released a a Multimodal Annotation Tool , \nwhich was derived from an earlier version of VideoAn-\nnEx including special features  such as audio signal \ngraphs and manual audio segmentation functions [6]. Other video annotators that deserve a mention are MovieToo l\n2, developed by Ricoh for interactively creat-\n                                                           \n1 http://creativecommons .org/about/licenses/ \n \n2 http://www.ricoh.co.jp/src/multimedia/MovieTool/ ing \n(directly using XML) video content descriptions con-\nforming to MPEG-7 syntax, or Vannotea [7], a proto-type for the real-time collaborative indexing, browsing, description, annotation and discussion of digital films \nand videos. \nIn the musical audio side, annotators have been a rare \nspecies. Let us mention Timeliner [8], the Acous-mograph\n3, MiXA [9], Marsyas [10] or the CLAM An-\nnotator [11]. The first one is integrated in the Indiana \nUniversity digital music library (Variations2), and is intended for pedagogical functions related to the struc-tural description of music files. The Acousmograph, Marsyas and the CLAM Annotator are focused on mi-cro-annotations of an audio file, but they do not incor-\nporate automatic or semi-automatic annotation capabili-\nties, or the functionalities required to share large annota-tion tasks among different teams of annotators. MiXA, on the other hand, is intended to help the annotation of scores by means of musicXML [12] descriptions. \nIn a different category, the MTG-DB [13], a database \nof audio material that offers functionalities for adding audio content, content browsing, adding metadata and dealing with taxonomies and algorithms, provides most of the infrastructure upon which we have built MUCOSA, which can be considered as a complemen-tary subsystem, specifically focused on music annota-tion under collaborative requirements.  \n                                                          \n \n3 http://www.ina.fr/grm/outils_dev/acousmographe/ Figure 1. A song annotated using Wavesurfer with different types of descriptors; values for \nsome of them have been automatically computed by means of plug-ins. \n78   \n \n \n3 THE MUCOSA ANNOTATION CLIENT \nThe MUCOSA client is in charge of: \n• computing descriptors for a given song,  \n• depicting descriptors as time-varying lines or as \nlabelled segments, \n• computing a fingerprint for a given song \n The core of MUCOSA is another annotation tool, \nWaveSurfer, developed at the Stockholm’s Centre for Speech Technology (KTH) [ 14]. WaveSurfer was origi-\nnally designed for tasks such as viewing, editing, and labeling of audio data, and it is built around a small core to which most functionality is added in the form of plug-ins. The tool was designed to work on most common platforms and with the aims that it should be easy to configure and extend. WaveSurfer is provided as open source, under the GPL license, with the explicit goal that the speech community jointly will improve and expand its scope and capabilities. The WaveSurfer tool is built using the Tcl/Tk [15] scripting language\n4, with scripts \nand dynamic link libraries wrapped into a single execu-\ntable. The tool consists of a simple core, combined with a novel plug-in architecture for all task-specific func-tionality. Wavesurfer also incorporates analysis and visualization of pitch, spectrogram and formants.  \nThe MUCOSA client exploits the WaveSurfer func-\ntionalities plus added features coming from specific \nplug-ins, in order to categorize the semantic content of each music file or their extracted segments and upload the description to the central database. \n                                                          \n \n4 http://www.tcl.tk/ There are four major operations in a MUCOSA client \nworking session: \n1. Music segmentation can be performed to cut up \nthe file into smaller units.  \n2. A pre-defined semantic lexicon is used in order to regulate the music content descriptions.  \n3. A human annotator labels the music segments \nwith its semantic labels. Automatic annotation-\nlearning components can be used to speed up the annotation task. These components are in-tegrated as WaveSurfer plug-ins.  \n4. The resulting descriptions of the annotation process are uploaded from the MUCOSA client to a central server but th ey can also be locally \noutputted in a structured format.  \nDescriptors that are currently  extracted or in the way \nto be extracted include: \n• Low-level descriptors such as spectral cen-troid, skewness, or Mel Cepstrum coeffi-cients; an MPEG-7 subset of audio low-level descriptors can be specifically com-puted thanks to the integration of the MPEG7AudioEnc library\n5 [16] \n• Rhythm descriptors such as tempo, beat or \nmetric.  \n• Tonality descriptors. \n• Instrumentation descriptors such as the oc-\ncurrence of percussive events. \n• Miscellaneous descriptors such as dance-ability, subjective energy, or dynamics com-\nplexity. \n                                                          \n \n5 http://www.ient.rwth-achen.d e/team/crysandt/software/ Figure 2. A screenshot of the descriptor creator that is included in the collection tagger \n79   \n \n \n• Section segments, which sometimes corre-\nspond to structural units like intro, verse or chorus. \n• Genre assignment probabilities. \nTime-varying real-valued descriptors (i.e. spectral \ncentroid, MFCCs, etc.) can be visualized as colour-coded functions (several of them can be stacked on the \nsame window). Labelled segments (according to beat or chord marks, presence of a given instrument, or struc-tural sections) are stacked below the waveform repre-sentation, according to the re quirements of the user gen-\nerating the annotations (see figure 1). Segments can be “extracted” and converted into independent soundfiles for additional processing. Label prediction utilizes dif-ferent statistical and machine learning techniques [17] to suggest labels for further segments or for other songs that have not been annotated yet. Predictions can also be propagated to other songs acco rding to user-specified \nfilters available in the MUCOSA collection tagger (see section 4). \nThere are two working modes, independent annota-\ntion and collaborative annotation. In the independent mode, where the audio is located in the user’s computer, the annotations are stored locally in WaveSurfer format \n(text). In case that the song has not been previously reg-istered in the database, a fingerprint is computed and centrally stored for furthe r recognition and for fast \ndownloading of its description to anybody requesting for it.  \nIn the collaborative mode the audio file is \ndownloaded from the BOCA server to the annotators’ \nmachine (see section 5) and, once it has been annotated using the WaveSurfer client, the annotations are up-loaded to the central server, which checks if the song has been previously annotated, updates the annotation management checklists, stores the data in the database, and communicates all the partic ipants in the project that \na song has been annotated.  \n4 THE MUCOSA COLLECTION \nTAGGER \nA second facet of MUCOSA is  collection tagging, which \ndeals with macro-descriptions or the assignment of \n“unary” descriptors (i.e. those that describe a song with a Figure 3. Screenshot of BOCA annotator’s homepage \n80   \n \n \nsingle value like, for example, tempo ). There are three \nstrategies whereby a collection can be tagged with cate-\ngorical (i.e. discrete) labels:  \n• Batch-assigning a given categorical value to a subset of the available songs (i.e. “wild” songs). This is done first by means of creating an M3U playlist (with Winamp or XMMS) that is named \nusing a given concept, then by importing the playlist. This operation automatically tags all the songs in the playlist with the concept given. \n• Creating a predictive model for a given con-cept\n6. The user provides examples for each one \nof the declared values  for the concept (by \nmeans of submission of playlists, as explained \nabove). With all these information, the system makes several calls to a remote Weka\n7 server \nand computes a predictive model based on the signal-based descriptors that have been ex-tracted for the songs (see figure 2). \n• Retrieving words that have been used to de-scribe that song or that artist and using them as descriptors. For a given artist, a web crawling \nsystem has gathered a series of words that are frequently associated with it. Wordnet\n8 is used \nto expand this set with synonyms and related \nwords. The most significant are offered as ac-ceptable tags for a given song. A “propagate” button makes possible the propagation of the label to other similar songs, or to other similar artists’ songs. Regrettably this functionality is \nnot properly integrated into the system yet. \n                                                          \n \n6This functionality has been implem ented in collaboration with OFAI \n7 http://www.cs.waikato.ac.nz/ml/weka \n8 http://wordnet.princeton.edu/  5 BOCA: THE MUCOSA \nCOLLABORATIVE ANNOTATION \nSUBSYSTEM \nMUCOSA allows collaborative annotation among multi-\nple users through the Internet (see Figure 3). Music that is being collaboratively annotated is stored in a central server and is downloaded acco rdingly to the requests of \nthe annotator. As these music titles have been issued under the Creative Commons licensing scheme, they can be distributed, jointly with their annotations, in a way \nthat is “free of legal concer ns”. The currently available \ncollection comes from Magnatune\n9 but other collections \nare currently prospected. \nA central server, called BOCA (Backbone Of Col-\nlaborative Annotation), takes care of: \n• assigning user IDs and passwords to access \nthe music and annotation files \n• storing the collaboration checklists  \n• storing the annotation sessions \n• storing the data files  \n• making possible the centralized manage-\nment of all that. \nBOCA has been developed using Plone10, an open \nsource content management system (CMS) that is built \non top of Zope, a Python-based open source application server. Plone can be easily extended to meet specific needs like those generated by MUCOSA. It is also easy to create new content types as those used for BOCA files, and manage the client annotator while displaying \nthe audio and its descriptors. Contrastingly to other \n                                                          \n \n9 http://www.magnatune.org  \n10 http://plone.org  Figure 4. BOCA control page for the annotation reviewer \n81   \n \n similar CMS, Plone offers faster and more flexible \nworkflow management building capabilities. \nFor collaborative annotation, there are three catego-\nries of user access to the BOCA Server: (1) annotation task leader, (2) annotator, and (3) annotation reviewer. The annotation task leader, by means of a web interface, sets up the project on the BOCA server, selects the \nsongs to be annotated, registers the annotators, and dis-\ntributes the annotation tasks and files among them. The annotators are the persons who perform the annotation task on the MUCOSA client and belong to different research institutions that join  forces to get the job done \nin a fraction of the total amount of time. The annotation reviewers are the quality checking agents of the system: they review all the annotated songs and mark or com-ment those that should be revised. Every annotated song is reviewed by one annotation reviewer and by the task leader, in order to ensure a quality standard. The re-viewing process is also intended as a mechanism for achieving convergence in case of conflicting annota-tions.  \nA project could consist of, for example, annotating \nthe beats in 100 songs from Magnatune, selected by uniform sampling of genres. Here the annotation task leader would select those songs and would distribute them across the available annotators. The annotators, after logging into the collaborative annotation system (a specific web page will request for the user ID and pass-word), would select the proper annotation task from the assignments they get from the BOCA server assign-ments page. The annotators can choose a file to anno-tate, can leave it temporally unfinished, and can see the existing annotations for that music file.   \nThe usual workflow goes as follows: 1. The Annotation task leader adds annotators and reviewers to the system, and selects the songs to be annotated. \n2. An annotator logs in, finds the songs to be an-notated in a \"to be annotated\" list, and selects one of them to be downloaded and annotated. \n3. When one song is annotated, it appears at the \"to be reviewed\" list. \n4. The reviewer checks the completeness and cor-rectness of the annotation, and accepts it if eve-rything is alright. \n5. The song appears at the \"to be published\" list. \n6. When the task leader decides, it publishes the \nannotation to the annotators enrolled in that task. \n In case of observing different annotation speeds, the \ntask leader may re-assign songs or issue warning mes-sages. When the 100 songs have been annotated, all the annotators are permitted to download the complete pro-ject (i.e. all the annotations) or some portions of it, de-\npending on the existing agreements. \nThere is an annotation availability page where one \ncan see the group task status list by the different groups and institutions supporting the project. Entries include the group or institution name, administrator's name, \ntheir allocated assignments, and the annotation status. \nThe BOCA server is currently under internal testing \nand for the moment we have gathered tempo, beats, percussion hits, structural sections, and key annotations for 100 songs from the Magnatune collection. This an-notation effort required more than 160 work hours of \nspecialized annotators (i.e. trained music students and \none musicologist for coordinating them and reviewing their annotations). We hope ot her Institutions join this \ninitiative and we all share the time and the outputs of a joint annotation effort. In order to make the collabora-tion even more attractive to them, the system provides a \nstarting pack of 10 annotated songs, to be downloaded after providing a minimal amount of input to the exist-ing collection. \n6 CONCLUSIONS \nWe have presented the Music Content Semantic Annota-\ntor (MUCOSA), a three-tired environment that has been \ndevised for annotating music in automatic, semi-automatic and totally manual modes. With MUCOSA, song files can be micro- and macro- annotated, and the descriptions may range from low-level to semantic la-bels. Some of the presented functionalities still require \nadditional testing and improvement; other interesting \nones are under consideration as, for instance, the possi-bility to play the audio file and a linked midi file, or showing the lyrics in the timeline.  One of the most interesting features included in the envi-ronment is the collaborative annotation whereby the chores of annotating a collection of songs can be shared between groups of researchers which, in the end, get the whole corpus of annotated music by a fraction of the effort required to do the task alone. MUCOSA can be accessed through the following link: http://www.semanticaudio.org/mucosa\n. \n7 ACKNOWLEDGMENTS \nThe research and developm ent reported here was par-\ntially funded by the EU-FP6-IST-507142 project \nSIMAC (Semantic Interaction with Music Audio Con-tents) project. The authors would like to thank Edgar Barroso, and the Audioclas and CLAM teams for their support to the project. \nREFERENCES \n[1] Minsky, M. and Laske, O. “A conversation with Marvin Minsky”. AI Magazine, 31-45, 1992. \n[2] Lessaffre, M., Leman, M., De Baets, B. and  Martens, J.-P. “Methodological considerations concerning manual annotation of musical audio in function of algorithm development”. Proceedings of the 4\nth International Conference \non Music Information Retrieval, Barcelona, 2004, 64-71. \n82   \n \n [3] Lin, C-Y., Tseng, B. L. and Smith, J. R. \n“VideoAnnEx: IBM MPEG-7 annotation tool for multimedia indexing and concept learning”. Proceedings of the IEEE Intl. Conf. on Multimedia and Expo (ICME), Baltimore, MD, July 2003. \n[4] Lin, C-Y., Tseng, B. L. and Smith, J. R. “Video \ncollaborative annotation forum: Establishing \nground-truth labels on large multimedia datasets. Proceedings of the NIST TREC Video 2003. \n[5] Amir, A., Berg,M., Chang, S.F., Iyengar, G., Lin, C.Y., Natsev, A. P., Neti, C., Nock, H., Naphade, M., Hsu, W.,Smith, J. R., Tseng, B., Wu, Y. and Zhang, D. “IBM research TRECVID-2003 Video Retrieval System”, Proceedings of the TRECVID 2003 Workshop. \n[6] Adams, W.H., Lin, C.-Y., Iyengar, G., Tseng, B. L. and Smith, J. R.. “IBM multimodal annotation tool”, IBM Alphaworks, August 2002. \n[7] Schroeter, R., Hunter, J. and Kosovc, D. “Vannotea - A collaborative video indexing. Annotation and discussion system for broadband networks”, K-CAP 2003 workshop on knowledge markup and semantic annotation, Florida, October 2003. \n[8] Notess, M. and Swann, M. B. “Timeliner: Building a Learning Tool into a Digital Music Library”, Proceedings of ED-MEDIA, Lugano, Switzerland, 2004. \n[9] Kaji, K. and Nagao, K. “MiXA: A Musical \nAnnotation System”. Proceedings of the 3\nrd \nInternational Semantic Web Conference, \nHiroshima, Japan, 2004. \n[10] Tzanetakis, G. and Cook, P. R. “Experiments in computer-assisted annotation of audio”, Proceedings of the ICAD, Atlanta, GE, 2000. \n[11] Amatriain, X., Massaguer,  J., García, D. and \nMosquera, I. “The CLAM Annotator: A cross-\nplatform audio descriptors editing tool”. Proceedings of the 6\nth International Conference \non Music Information Retrieval, London, UK, \n2005.  \n[12] Good, M. “MusicXML in practice: Issues in translation and analysis”. Proceedings of the First International Conference MAX 2002: Musical Application Using XML , Milan, Italy, \n2002. \n[13] Cano, P., Koppenberger, M., Ferradans, S., \nMartínez, A., Gouyon, V., Sandvold, V., Tarasov, V. and Wack, N. “MTG-DB: A repository for music audio processing”. Proceedings of the 4\nth Intl. Conf. on Web \nDelivering of Music, Barcelona, 2004. \n[14] Sjölande, K. and Beskow , J. “Wavesurfer - \nAn open source speech tool”, Proceedings of \nthe Intl. Conf. on Spoken Language Processing, \n2000, IV: 464-467. \n[15] Ousterhout, J.K., Tcl and the Tk Toolkit. Addison Wesley, 1994. \n[16] Crysand, H., Tummarello, G. and Piazza, F. \n“MPEG-7 encoding and processing: \nMPEG7AUDIOENC+MPEG7AUDIODB. 3\nrd \nMusicnetwork Open Workshop, Munich, 2004. \n[17] Gouyon, F., Wack, N. and Dixon, S. “An open \nsource tool for semi-automatic rhythmic annotation”. Proceedings of the 7\nth Intl. \nConference on Digital Audio Effects, Naples, 2004. \n \n \n \n     \n83"
    },
    {
        "title": "The Persian Music and the Santur Instrument.",
        "author": [
            "Peyman Heydarian",
            "Joshua D. Reiss"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415048",
        "url": "https://doi.org/10.5281/zenodo.1415048",
        "ee": "https://zenodo.org/records/1415048/files/HeydarianR05.pdf",
        "abstract": "Persian music has had a profound effect on various Eastern musical cultures, and also influenced Southern European and Northern African music. The Santur, a hammered dulcimer, is one of the most important instruments in Persia. In this paper, Persian music and the Santur instrument are explained and analysed. Techniques for fundamental frequency detection are applied to data acquired from the Santur and results are reported.",
        "zenodo_id": 1415048,
        "dblp_key": "conf/ismir/HeydarianR05",
        "keywords": [
            "Non-western Musicology",
            "Persian",
            "Iranian Music",
            "Santur",
            "Santoor",
            "Dulcimer",
            "Quartertone",
            "Interval",
            "Fundamental Frequency detection",
            "Pitch"
        ],
        "content": "THE PERSIAN MUSIC AND THE SANTUR INSTRUMENT\nPeyman Heydarian Joshua D. Reiss \nCentre For Digital Music  \nQueen Mary, University of London  \nMile End Road, London  E1 4NS, UK  \n{peyman.heydarian, josh.reiss}@elec.qmul.ac.uk \nABSTRACT \nPersian music has had a profound effect on various East-\nern musical cultures, and also influenced Southern Euro-\npean and Northern African music. The Santur, a ham-\nmered dulcimer, is one of the most important instruments \nin Persia. In this paper, Persian music and the Santur \ninstrument are explained and analysed. Techniques for \nfundamental frequency detection are applied to data ac-\nquired from the Santur and results are reported. \n \nKeywords: Non-western Musicology, Persian, Iranian \nMusic, Santur, Santoor, Dulcimer, Dastgàh, Quartertone, \nInterval, Fundamental Frequency detection, Pitch. \nBACKGROUND \nMost of the efforts in music processing are focused on \nWestern music while there are a variety of subtle points \nin the Eastern musical systems. This section provides the \nbackground material on Persian Music including a brief \nhistorical review and explanation of the musicological \nconcepts.  \n1.1 A Brief History of Persian Music \nThe origin of Persian music traces back to the earliest \nwritten histories. According to legend, king Jamshid is \ncredited with the Invention of music.  \nThe two Greek historians, Herodotus and Xenophon, \nmention the use of martial, ceremonial and ritual music \nin Iran during the Medes (900- 550 B.C.) and \nAchaemenids (559 – 331 B.C.) dynasties [1]. Remains of \nan Achaemenids Clay Horn have been found recently \nand are held in the museum of Pars in the city of Shiraz \n[2]. During the Achaemenids, Persian territory from \nChina to Egypt and Greece made a common cultural \nplatform. During the Hellenic era from 331-238 B.C. the \nMacedonians ruled over the former Persian Empire and \nthis cultural platform remained unchanged. The Mace-\ndonians were later defeated by Iranians and the dynasty \nof Parthians ruled from 238 B.C. to 224 A.D. In this era, \nsort of popular musicians emerged who could create the \npoem, compose the music and perform it at once.   \nThe peak period for ancient Persian music occurred \nduring the Sàsànids dynasty (224-652 A.D.). Bàrbod, the most brilliant musician of the court of Khosro Parviz, \ndevised a musical system [3,4], containing seven modal \nstructures which were called the Khosravani (Royal \nModes) as the days of a week, thirty derivative modes or \nmodulations which were called the Lahn corresponding \nto days of a month, and three hundred sixty melodies \ncalled the Dastàn for each day of the year. There were \nalso music hymns with which the Gàt'hà of the Avestà \n(the Zoroastrian holy book) were sung. During this pe-\nriod, music was so important that one of the kings, \nBahràm-e Gur (421-439 A.D.) invited 1200 musicians \nfrom India to contribute to the musical programmes in \nPersia [4]. \nThe Arab conquest in the 7th century A.D. spread the \nPersian Instrumental and vocal music throughout the \nIslamic empire. Two major schools of music were devel-\noped from the Persian music: One in Baghdad and the \nother in Cordoba which sprang into the North African \nand the Flamenco music [4]. Persian musicians and mu-\nsicologists brought their musical concepts and instru-\nments to the furthest points in the empire. An interesting \nevent is the migration of Master Zaryàb along with his \nsons and daughters from Persia to Spain to teach the in-\nstrumental, vocal and dance techniques.  He brought the \nPersian lute or Barbat (Ud) to the Spain which was then \nevolved to the present day Guitar [4]. Some of the fa-\nmous musicians and musicologists of Persia in the East-\nern Muslim Empire are: Farabi (A.D. 950), Ebne Sinà or \nAvicennà (A.D. 1037), Ràzi (A.D. 1209), Safioddin Or-\nmavi (A.D. 1294),Qotbeddin Shiràzi (A.D. 1310), and \nAbdol-Qàder Maràqi (d.circa 1460) [3, 4].   \nThis era was followed by the disastrous Mongol inva-\nsion in the 1200s, which brought the Persian Musical \nconcepts to the East [4].  \nThe new golden age of Persian civilisation began with \nthe Safavids dynasty (1499-1746 A.D.), during which the \nmusic chambers of the Àli Qàpu and the Chéhél sotun \npalace in the city of Esfehàn were made. In this and pre-\nvious eras, musicians were patronized by nobility. How-\never, from that time until the 1850s, Persian music lost \nits official significance and was frowned upon [3]. \nIn recent decades, Persian music has again found \nbroader dimensions. An urge to create new traditions and \nan interest in the unique musical structures has emerged. \nHowever, the national music of Iran may be represented \nby the tradition of the past tinged with 19th century per-\nformance practices.  \n1.2 The Persian Composition  \nPersian music is often dichotomised into urban (music of \nthe large cities) and ethnic music (music of the different \nethnic groups living in smaller cities, villages, and moun-\ntainous areas). The first uses more ornamentations and \nfree rhythms. Farhat describes the ethnic music with its Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full \ncitation on the first page. \n© 2005 Queen Mary, University of London \n524 \n \nsimple melodies and a rhythmic directness [3]. The ethnic \nmusic is more preserved and is closer to the ancient tradi-\ntion. They both follow the same intervals and modes (sec. \n1.3 and 1.4) and rely to a large extent on improvisation. \nThere are three instrumental forms and one vocal \nform in urban music. The instrumental forms are Pish-\ndaràmad, Chàhàrmezrab, and Reng. Pishdaràmad was \ninvented by a master of the tar, Darvish Khàn, and was \nintended as a prelude to the Daràmad which is the open-\ning section of a Dastgàh. It may be in duple, triple, or \nquadruple time, and draws its melody from some of the \nimportant Gusheh of the piece (sec. 1.4).  \nChàhàrmezrab is a solo piece with a fast tempo, and is \nusually based on the melody immediately preceding it. \nThe third instrumental form is the Reng, a simple dance \npiece which is usually played at the conclusion of the \nDastgàh. The vocal form is called the Tasnif. It has a \ndesign similar to the Pishdaràmad, and is usually placed \nright before the Reng. The Persian music is mainly uni-\nson, where the instruments in an ensemble play the me-\nlodic scheme and relies highly on the improvisation [3].  \n1.3 The Persian Intervals  \nIn analogy with the Western tempered music, Vaziri sug-\ngested a division of the octave to 24 equal Quartertones. \nHe defined the Sori ( \n  ) and Koron ( \n ) symbols to \nshow half-sharp and half-flat quartertones [6]. This sys-\ntem is widely used by musicians. Here we use “q” to \nshow the Koron, and “s” to show the Sori.  \nA preferred option was suggested by Farhat [3]: Con-\nsidering some intervals between a semitone and a whole \ntone, and an interval greater than a whole tone to the 12 \nsemitones in tempered Western music.   \nIn fact, only a few quartertones exist in each Persian \nScale and all the scales (Dastgàh) in Persian music (sec-\ntion 1.4) can be performed with 13 different notes, 7 of \nwhich are the diatonic notes, 3 are semitones and 3 are \nquartertones [2]. The 13 principal notes with which all \nthe Persian Dastgàh can be played on a Santur are: \n \nMi–Fa-\nFa- #Fa– Sol-#Sol-La-\nSi–Si -Do-\nDo-#Do-Re  \n \n1.4 The Persian Dastgàh system \nPersian music is based upon a set of 12 modes, called the \nDastgàh system: Shur, Abu' Atà, Bayàt-e Tork, Afshàri, \nDashti, Homàyun, Bayat-e Esfehàn, Segàh, Chahàrgàh, \nMàhur, RàstPanjgàh, and Navà [3, 4].  \nThere is a tonal centre or centre of pitch gravity for \neach Dastgàh, which is called the Shàhed. Each Dastgàh \nhas a number of derivatives, called the Gushé. Moving \nfrom a Dastgàh to a Gushé is the usual way for modula-\ntion in Persian music. Most of the time, it occurs with a \nchange in the Shàhed, but it may change the tuning too. \nSome of the Gushé are independent, but when called \nthrough another Dastgàh, will play the role of a Gushé. \nFor example the Delkash Gushé of Bayàt-e Esfehàn  is \nabsolutely a Shur from a fifth interval.  \nPerformance in each Dastgàh starts with an opening \nsection, which is called the Daràmad.  Then, modulations \nto other modes (Gushé) occur, during which the Shàhed \nnote gradually moves upward. Finally, a Cadential phrase called the Forud, brings the mode back to the ini-\ntial mode of the Dastgàh.  \nIn terms of the rhythm, the urban Persian music con-\nsists of either free-rhythmic pieces (Avàz) or rhythmic \nsongs, typically in 2/4, 4/4, or 6/8. Complex rhythms like \n5/8 and 7/8 are mostly used in the ethnic music. \n1.5 The Santur \n1.5.1 The history \nThe Santur is a trapezoidal string instrument, played by a \npair of delicate hammer sticks. This instrument origi-\nnated in Iran, and was later brought to India, China, \nThailand, Greece, Germany (and other countries), where \nit is called Santoor, Yang-jin, Khim, Santouri, Hackbrett \nrespectively. It is often referred to as a dulcimer in Eng-\nlish. The Santur is one of the most popular instruments in \nPersia. In a typical Persian ensemble, the Santur per-\nformer usually sits in the middle and assumes a leader-\nship position. \n1.5.2 The Santur structure \nThe pair of hammer sticks, or the Mezràb, are held be-\ntween the index and the middle fingers and are used to \nhit the strings. The Mezràb are usually coated by a piece \nof cotton or leather. The body of the Santur is made of \nwalnut and the Mezràb are made of either walnut or \nNarenge (a citrus wood). Figure 1 shows a Santur and its \nperipherals. \nFour strings are vibrated for each note. They are \npulled between the string holders (figure 2-a) and the \ntuning pegs (figure 2-b), and sat on a bridge between \nthese two ends (figure 2-c). The notes can be tuned by \nturning the tuning pegs, using a tuning key which is also \nused as a hammer to hit the tuning pegs. The bridges are \nmovable and can continuously change the pitch of a note \nby several whole steps. \n \n \n(1-a) \n  \n                   \n  \n  (1-b)     (1-c) \nFigure 1 a) Santur b) Sticks (Mezràb) c) Tuning Key \n \nThere are two sound holes on the soundboard (figure \n2-d). They serve to enhance the sound quality. Modern \nIranian Santurs most often consist of 9 bridges, although \n11 or 12-bridge Santurs can be found too. \n \n    \n  \n  \n                  (2-a)                 (2-b) \n \n525 \n \n     \n        \n  \n                   (2-c)        (2-d) \nFigure 2  a) Bridges (Kharak) b) Sound holes \n \nThere are three note regions on a Santur. The first oc-\ntave, or the yellow notes, are made of brass and are lo-\ncated on the right side of the Santur; the second Octave \nnotes, denoted white notes, are in the centre and made of \nstainless steel. The extension of the second octave notes pass over the bridges and terminate on the string holders in the left end. These are called behind-the-bridge white notes. An 11-bridge Santur has a tone Range from C3 \n(130.8 Hz) to F6 (1396.9 Hz). The fundamental frequen-cies (F0) can be calculated with reference to a known \ntone, f\n1 using  where d represents the \ndistance in quartertones of f2 from f1. ) 24 / (2 1 2df f× =\nThe F0 of a string is a function of its length l, string ‘s \npulling force F, string material constant μ and a constant \nK, as given by Eq. (1). \n2KFf\nlμ=  (1) \nThe resonance body of a Santur is hollow, but there \nare wooden columns that keep the instrument from dis-\nruption. They bear the for ce exerted by the strings over \nthe bridges on the upper surface of Santur.  \nAll Dastgàh may be played on a Santur in different keys. Figure 3 shows the tuning system for an 11-bridge San-tur. Abu’Atà, Dashti, Bayàt-e-Tork and Afshàri which \nare all derivatives of Shur, have the same tuning. \nDSP ANALYSIS \nUsing a computer with a sound card, and an ordinary \nmicrophone, samples of 16-bit precision at 44.1 kHz sampling rate where recorded. The samples were \nperformed by the first author on a Santur instrument. The \nanalysis in section 2.2 is done on a database consisting of 10 samples for each of the 13 Persian notes explained in \nsec. 1.3. It was recorded with the same conditions, but a sampling rate of 16 kHz [4].     \n1.6 Analysing an A4 note and a 2 octave arpeggio \nFigure 5 shows the logarithm of the Spectrum of an A4 \nnote (F0=440 Hz) [2]. They show the variation of harmonic content through time. The amplitude of a \nharmonic component may change due to the resonance \ncharacteristics of the strings, the instrument body and the room acoustics. \nFigure 6 shows the harmonic content of a 1024 point \nframe of the same signal. To bypass the transient, the \nanalysis window for figure 6, starts at sample no. 4500. \nThe F0 and the major harmonics can be seen. \n \nFigure 4  The tuning system on Santur1\n \n \n \nFigure 5  Spectrum of the note A4  \n \n \nFigure 6  Frequency domain representation of A4. \n \nIn Figure 7 the Spectrum of a two-octave A4 minor \narpeggio is shown. An array of the following notes was \nplayed: A3-C4-E4-A4-C5-E5-A5 F0 for the array of \n                                                           \n1 More information on the modes can be found in [3, 4]. \n526 \n \nHertz respectively [2]. A 1024 point window has been \nused. The change in the harmonic content can be seen. \n \n \n \nFigure 7 Spectrum of a two-octave A4 minor arpeggio. \n1.7 Fundamental Frequency Detection \nHere, the fundamental frequency is calculated using the \ncross correlation between the test samples and a set of \nreference patterns [2, 5]. Analysis of Santur signals \nshows that different notes have different timbres and even \nsamples of the same note vary in spectrum when played \nwith different dynamics and with different Mezràb [2]. \nTo provide a constant-shape pattern, a uniform reference \npattern was made for each note, with 1's at the position of \nthe fundamental frequency and the harmonics. If this pat-\ntern is cross correlated or convolved with the spectral \ntransform of the signal, a maximum occurs at the position \nof the fundamental frequency [5].  \nIn practice, the overtones are not exactly at the integer \nmultiples of a the F0. So a pattern with components hav-\ning a thickness t, was chosen to cope with the inhar-\nmonicities (figure 8) [2]. \n \n         Magnitude \n         Frequency (Hz) \nFigure 8 The pattern for E5 considering 4 harmonics \nwith thickness t. \n \nThe fundamental frequencies were calculated using \npatterns like that in figure 8, a sampling frequency of \nFs=8000 Hz, different lags, window widths of N=512, \nN=1024 and N=2048; number of harmonics, between \nn=1 to n=8 and thicknesses between t=1 to t=13, the \nmaximum recognition rate was 96.15% which occurred \nat a window width of, N=2048, thickness t=9, number of \nharmonics n=4 and lag, l=250. Figure 9 shows the rec-\nognition rate when as a function of the thickness when \nall the other variables are optimized. \nAdditional tests were done, using patterns with different \nshapes, component thicknesses, and number of harmonics \n[2]. The highest recognition rate was obtained using an \nincreasing width pattern as shown in figure 10, where the \nwidth of the ith component is i times the width of the first \ncomponent. Using this pattern, with the same conditions \nas the previous test, the recognition rate increases to \n96.92%. For the test samples taken at Fs=16 kHz this \noccurs when the pattern has n=6 harmonics and the \nthickness of the first component is t=5. Among the 130 \ntest notes, there were only 4 mistakes. One E4 was rec-\nognized as an E5 which is an octave error; one F#4 as Fs4; one Bq4 as B4 and one C5 as Cs5 which they are all \nproximity errors between the neighboring quartertones.  \n \n \nRecognition Rate \n \n \n \n            Thickness (t) \nFigure 9 Recognition rate versus thickness \n \nMagnitude \n \n           Frequency (Hz) \n \nFigure 10 Increasing-width pattern for E5 with \n7 harmonics. \nCONCLUSION \nThe Persian Intervals and Scales are similar to those of \nvarious cultures like Kurdish, Azeri, Guilaki, Baluchi, \nTurkish, Arabian, Greek, etc… So, any analysis on Per-\nsian music may be extended to a wide range of cultures. \nIn this paper, Persian music and the Santur instrument \nwere introduced and an algorithm for the calculation of \nfundamental frequency was implemented. Future work \nmay be on studying the different rhythmic patterns, and \ndetermining the scale, key or genre of a piece. Such work \nwill enable a more complete understanding of how music \nprocessing algorithms need to be adapted to different \nmusical styles and structures. \nREFERENCES \n[1] Khaleqi, R., Nazari be Musiqie, 2nd Edition, \n1983. \n[2] Heydarian, P., “Music Note Recognition for \nSantoor”, M.Sc. thesis, Tarbiat Modarres University, \nTehran, Iran, 2000, supervised by Dr. E. Kabir. \n[3] Farhat, H., The Dastgah Concept in Persian \nMusic, Cambridge University Press, 1990. \n[4] Miller, L.C., Music and Song in Persia: The Art \nof Avaz, 1999. \n[5] Brown, J.C., \"Musical Fundamental Frequency \nTracking using a pattern recognition method\", J. \nAcoustics Soc., 1992. \n[6] Vaziri, A. N., Dastur-e Tàr, Tehran, 1913. \n527"
    },
    {
        "title": "A Benchmark Dataset for Audio Classification and Clustering.",
        "author": [
            "Helge Homburg",
            "Ingo Mierswa",
            "Bülent Möller",
            "Katharina Morik",
            "Michael Wurst"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417065",
        "url": "https://doi.org/10.5281/zenodo.1417065",
        "ee": "https://zenodo.org/records/1417065/files/HomburgMMMW05.pdf",
        "abstract": "We present a freely available benchmark dataset for audio classification and clustering. This dataset consists of 10 seconds samples of 1886 songs obtained from the Garageband site. Beside the audio clips themselves, textual meta data is provided for the individual songs. The songs are classified into 9 genres. In addition to the genre information, our dataset also consists of 24 hierarchical cluster models created manually by a group of users. This enables a user centric evaluation of audio classification and clustering algorithms and gives researchers the opportunity to test the performance of their methods on heterogeneous data. We first give a motivation for assembling our benchmark dataset. Then we describe the dataset and its elements in more detail. Finally, we present some initial results using a set of audio features generated by a feature construction approach. Keywords: Benchmark Dataset, Audio Classification, Audio Clustering, Meta Learning 1 CHALLENGES IN LEARNING ON AUDIO DATA Information retrieval has started several efforts to automatic indexing [1] and retrieval (e.g., querying by humming [2]). Machine Learning has shown its benefits for text classification and ranked document retrieval with respect to user preferences [3]. It is straightforward to expect a similar benefit for the classification and personalized retrieval of music records. However, this area is still very challenging for several reasons. Unlike many other types of data used with Machine Learning, audio data consists of time series which are usually quite large. Given a sampling rate of 44100 Hz, a three minute music record has a length of about 8 · 106 values. Moreover, current approaches to time series indexing and similarity Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. c⃝2005 Queen Mary, University of London measures rely on a more or less fixed time scale [4]. The key problem for automatic audio processing based on Machine Learning is to obtain a fixed set of features from the wave forms [5, 6, 7, 8, 9]. Beside the problems connected with these characteristics of audio data, current applications lead to additional challenging problems. Firstly, different classification tasks ask for different feature sets. It is not very likely that a feature set delivering excellent performance on the separation of classical and popular music works well also for the separation of techno and hip hop music. Machine Learning methods should be able to adapt to different areas of the input space. This is usually referred to as local learning [10]. Secondly, for many kinds of audio data important additional information exist as title or artist. This information is often called meta data. Other useful information about songs could be the lyrics, ratings or comments provided by listeners. To integrate all this information for Machine Learning is a very challenging task usually referred to as Multi View Learning [11]. Finally, audio processing based on Machine Learning is often applied in user oriented applications, such as personal media organizers (e. g. iTunes). Such organizers typically help users to manage a collection of songs by automatically classifying songs, clusterings songs, searching for similar songs, etc. However, music is a highly personal issue, users often arrange their songs using very different viewpoints [12, 13, 14]. This leads to problems similar to the ones mentioned above: classification with respect to different viewpoints may ask for different representations. Think for example of a first user, who arranges songs according to mood and a second user arranging songs according to whether the singer is male or female (as shown in Figure 1). The second task may require a set of features that is completely different from the first one, even if the songs themselves are from the same genre. Furthermore the possible viewpoints are usually neither restricted nor anticipated when the system is designed. The Machine Learning methods must be flexible enough to handle any possible viewpoint and thus classification. Still another problem in end user applications is that datasets are of varying size. While some users only arrange very few items, others have large collections of songs. Methods have to provide both, a high accuracy for small datasets and efficiency for large datasets. 528 Figure 1: Two examples of user defined classification schemes. The currently most popular freely available dataset is the RWC Music Database [15]. It provides audio samples together with extensive meta data and is well suited for the evaluation of many kinds of audio processing tasks. Unfortunately, the size of this data set is relatively small and hence does not meet the requirements of many Machine Learning methods. The dataset does also not contain different user viewpoints. We consider these heterogeneous viewpoints a major challenge for many real-world retrieval tasks. The idea of our benchmark dataset is to provide a possibility to compare how different approaches and algorithms handle the described challenges. It reflects all of the above problems. It contains 1886 songs given as 10 s samples from 9 genres. Beside the audio data itself, meta data (band name, genre, etc.), user comments and partially even lyrics are available for each song. Also we provide 24 classification schemes created by our students using arbritrary personal viewpoints. This allows to test methods on very heterogeneous learning tasks, as could be expected in many real life user oriented scenarios. As the user classification schemes only cover parts of the songs, they also provide a way to test how well a given method can adapt to such local problems. Given audio and textual data, the dataset is especially well suited for Multi View Machine Learning. In the next section, the dataset is described in more detail. 2 THE DATASET The dataset1 consists of 1886 songs from the Garageband site. Garageband is a website that allows artists to upload their music and offer it for free download. Visitors of the site might download the audio clips, rate them or write comments. A group of students downloaded the songs to1www-ai.cs.uni-dortmund.de/audio.html Genre Number Blues 120 Electronic 113 Jazz 319 Pop 116 Rap/HipHop 300 Rock 504 Folk/Country 222 Alternative 145 Funk/Soul 47 total 1886 Table 1: Number of songs per genre. gether with some meta information. Then they created personal classification schemes on these songs described in section 2.4. The songs were taken from nine different genres: Pop, Rock, Folk/Country, Alternative, Jazz, Electronic, Blues, Rap/HipHop, Funk/Soul. The number of songs in each genre varies, Table 1 gives an overview.",
        "zenodo_id": 1417065,
        "dblp_key": "conf/ismir/HomburgMMMW05",
        "keywords": [
            "freely available benchmark dataset",
            "10 seconds samples of 1886 songs",
            "Garageband site",
            "9 genres",
            "24 hierarchical cluster models",
            "user centric evaluation",
            "heterogeneous data",
            "audio classification and clustering",
            "meta learning",
            "audio features generated by a feature construction approach"
        ],
        "content": "ABENCHMARK DATASET FOR AUDIO CLASSIFICA TION AND\nCLUSTERING\nHelge Homb urg,Ingo Mierswa, B¨ulent M¨oller,Katharina Morik andMichael Wurst\nUniversity ofDortmund, AIUnit\n44221 Dortmund, German y\nABSTRA CT\nWepresent afreely available benchmark dataset foraudio\nclassi\u0002cation andclustering. This dataset consists of10\nseconds samples of1886 songs obtained from theGarage-\nband site. Beside theaudio clips themselv es,textual meta\ndata isprovided fortheindividual songs. The songs are\nclassi\u0002ed into 9genres. Inaddition tothegenre infor -\nmation, ourdataset alsoconsists of24hierarchical cluster\nmodels created manually byagroup ofusers. This en-\nables auser centric evaluation ofaudio classi\u0002cation and\nclustering algorithms andgivesresearchers theopportu-\nnitytotesttheperformance oftheir methods onheteroge-\nneous data. We\u0002rstgiveamotivation forassembling our\nbenchmark dataset. Then wedescribe thedataset andits\nelements inmore detail. Finally ,wepresent some initial\nresults using asetofaudio features generated byafeature\nconstruction approach.\nKeywords: Benchmark Dataset, Audio Classi\u0002cation,\nAudio Clustering, Meta Learning\n1CHALLENGES INLEARNING ON\nAUDIO DATA\nInformation retrie valhasstarted several efforts toauto-\nmatic indexing [1]andretrie val(e.g., querying byhum-\nming [2]). Machine Learning hasshownitsbene\u0002ts for\ntextclassi\u0002cation andrankeddocument retrie valwith re-\nspect touser preferences [3]. Itisstraightforw ardtoex-\npect asimilar bene\u0002t fortheclassi\u0002cation andperson-\nalized retrie valofmusic records. However,thisarea is\nstill verychallenging forseveral reasons. Unlik emany\nother types ofdata used with Machine Learning, audio\ndata consists oftime series which areusually quite large.\nGivenasampling rateof44100 Hz,athree minute mu-\nsicrecord hasalength ofabout8\u0001106values. Moreo ver,\ncurrent approaches totime series indexing andsimilarity\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonmeasures relyonamore orless\u0002xedtime scale [4].The\nkeyproblem forautomatic audio processing based onMa-\nchine Learning istoobtain a\u0002xedsetoffeatures from the\nwaveforms [5,6,7,8,9].\nBeside theproblems connected with these character -\nistics ofaudio data, current applications lead toaddi-\ntional challenging problems. Firstly ,different classi\u0002ca-\ntiontasks askfordifferent feature sets. Itisnotverylikely\nthatafeature setdelivering excellent performance onthe\nseparation ofclassical andpopular music works well also\nfortheseparation oftechno andhiphopmusic. Machine\nLearning methods should beable toadapt todifferent ar-\neasoftheinput space. This isusually referred toaslocal\nlearning [10].\nSecondly ,formanykinds ofaudio data important ad-\nditional information existastitleorartist. This informa-\ntion isoften called meta data. Other useful information\nabout songs could bethelyrics, ratings orcomments pro-\nvided bylisteners. Tointegrate allthisinformation for\nMachine Learning isaverychallenging task usually re-\nferred toasMulti ViewLearning [11].\nFinally ,audio processing based onMachine Learning\nisoften applied inuser oriented applications, such asper-\nsonal media organizers (e.g.iTunes). Such organizers\ntypically help users tomanage acollection ofsongs byau-\ntomatically classifying songs, clusterings songs, searching\nforsimilar songs, etc.However,music isahighly personal\nissue, users often arrange their songs using verydifferent\nviewpoints [12,13,14].This leads toproblems similar to\ntheones mentioned above:classi\u0002cation with respect to\ndifferent viewpoints may askfordifferent representations.\nThink forexample ofa\u0002rstuser,who arranges songs ac-\ncording tomood andasecond user arranging songs ac-\ncording towhether thesinger ismale orfemale (asshown\ninFigure 1).Thesecond taskmay require asetoffeatures\nthatiscompletely different from the\u0002rstone, evenifthe\nsongs themselv esarefrom thesame genre. Furthermore\nthepossible viewpoints areusually neither restricted nor\nanticipated when thesystem isdesigned. The Machine\nLearning methods must be\u0003exible enough tohandle any\npossible viewpoint andthus classi\u0002cation. Still another\nproblem inenduser applications isthat datasets areof\nvarying size. While some users only arrange veryfew\nitems, others havelargecollections ofsongs. Methods\nhavetoprovide both, ahigh accurac yforsmall datasets\nandef\u0002cienc yforlargedatasets.\n528Figure 1:Twoexamples ofuser de\u0002ned classi\u0002cation schemes.\nThecurrently most popular freely available dataset is\ntheRWCMusic Database [15].Itprovides audio samples\ntogether with extensi vemeta data andiswell suited for\ntheevaluation ofmanykinds ofaudio processing tasks.\nUnfortunately ,thesizeofthisdata setisrelati velysmall\nandhence does notmeet therequirements ofmanyMa-\nchine Learning methods. Thedataset does alsonotcontain\ndifferent user viewpoints. Weconsider these heteroge-\nneous viewpoints amajor challenge formanyreal-w orld\nretrie valtasks.\nTheidea ofourbenchmark dataset istoprovide apos-\nsibility tocompare howdifferent approaches andalgo-\nrithms handle thedescribed challenges. Itre\u0003ects allof\ntheaboveproblems. Itcontains 1886 songs givenas10s\nsamples from 9genres. Beside theaudio data itself, meta\ndata(band name, genre, etc.), usercomments andpartially\nevenlyrics areavailable foreach song. Also weprovide\n24classi\u0002cation schemes created byourstudents using\narbritrary personal viewpoints. This allowstotestmeth-\nodsonveryheterogeneous learning tasks, ascould beex-\npected inmanyreallifeuser oriented scenarios. Asthe\nuser classi\u0002cation schemes only coverparts ofthesongs,\ntheyalso provide awaytotesthowwell agivenmethod\ncanadapt tosuch local problems. Givenaudio andtextual\ndata, thedataset isespecially well suited forMulti View\nMachine Learning. Inthenextsection, thedataset isde-\nscribed inmore detail.\n2THE DATASET\nThedataset1consists of1886 songs from theGarageband\nsite. Garageband isawebsite thatallowsartists toupload\ntheir music andofferitforfreedownload. Visitors ofthe\nsitemight download theaudio clips, rate them orwrite\ncomments. Agroup ofstudents downloaded thesongs to-\n1www-ai.cs.uni-dortmund.de/audio.htmlGenr e Number\nBlues 120\nElectronic 113\nJazz 319\nPop 116\nRap/HipHop 300\nRock 504\nFolk/Country 222\nAlternati ve 145\nFunk/Soul 47\ntotal 1886\nTable 1:Number ofsongs pergenre.\ngether with some meta information. Then theycreated\npersonal classi\u0002cation schemes onthese songs described\ninsection 2.4. Thesongs were takenfrom nine different\ngenres: Pop, Rock, Folk/Country ,Alternati ve,Jazz, Elec-\ntronic, Blues, Rap/HipHop, Funk/Soul. The number of\nsongs ineach genre varies, Table 1givesanovervie w.\n2.1 Audio Samples\nEach song isassociated with a10second audio sample\ndrawnfrom arandom position ofthecorresponding song.\nAudio samples areencoded using mp3 with asampling\nrateof44100 Hzandabitrate of128mbit/s.\n2.2 Meta Data\nThe meta data foreach song consists ofseveral parts.\nThese arethename andthelength ofthesong, information\nabout thegenre, theband orartists name, andcomments\ngivenbylisteners. Lyrics arepartially available.\n529prediction ntrue Blues Electr onic Jazz Pop HipHop Rock Folk/Country Alter nativeFunk/Soul\nBlues 18 4 26 6 16 23 1 6 0\nElectr onic 2 17 12 6 11 9 0 10 0\nJazz 29 42 171 37 39 64 0 34 0\nPop 4 3 14 15 5 15 0 10 0\nHipHop 10 21 25 15 187 21 0 10 0\nRock 55 19 58 31 38 358 1 60 0\nFolk/Country 0 0 0 0 0 0 213 0 37\nAlter native 2 7 13 6 4 14 1 15 0\nFunk/Soul 0 0 0 0 0 0 6 0 10\nTable 2:Theconfusion matrix fork-NN onthegenre data.\n2.3 Audio Featur es\nBased ontheapproach described in[7]atotal number of\n49features were extracted from each song. These audio\nfeatures arealso part ofthebenchmark dataset. The set\noffeatures covertemporal features, spectral features, and\nsome unusual features extracted inthethephase space of\ntheaudio data.\n2.4 User Classi\u0002cation Schemes\nAgroup ofusers created 24classi\u0002cation schemes without\nanyfurther speci\u0002cation. These schemes areofvarying\nsizeandcoverdifferent subsets ofthesongs. Theaspects\nused forclassi\u0002cation differconsiderably .Forexample,\nusers arranged thesongs according togenre, quality or\npreference, mood, time ofday,instruments, singer ,etc.\nTheclassi\u0002cation schemes aretreelikestructures inwhich\neverynode hasalabel. Songs areallowed tobeanywhere\ninthetree.\nWethink thatthese userde\u0002ned classi\u0002cation schemes\nofferachallenging problem toaudio classi\u0002cation and\nclustering, astheyareveryheterogeneous, mostly small\nandcoverdifferent subsets ofthesongs, thus require the\nability forlocal adaptations from thealgorithm.\n3INITIAL RESUL TS\nWeperformed some initial experiments onourdataset.\nAllexperiments were performed with theMachine Learn-\ningenvironment YALE[16]. YALEisavailable asopen-\nsource softw areunder theGNU Public License (GPL)2.\nThenextsections describe theperformance which canbe\nachie vedongenre classi\u0002cation andontheuser de\u0002ned\nclassi\u0002cation schemes.\n3.1 Classifying Global Genr es\nA\u0002rstlearning taskonourdataset isclassi\u0002cation accord-\ningtogenre. Thegenre information isgivenaspartofthe\nmeta data. Classi\u0002cation isdone onthe49features de-\nscribed insection 2.3. The learning schemes used were\nC4.5 decision trees,k-nearest neighbors with anadapti ve\ndistance metric, NaiveBayes, andarandom classi\u0002er as\nbaseline. Results were measured with a10-fold cross val-\nidation. Table 3showstheperformance foralllearning\nschemes.\n2http://yale.cs.uni-dortmund.de/Accuracy\nRandom 26.72\nC4.5 45.44\nNaiveBayes 43.69\nk-NN 53.23\nTable 3:Theaccurac yforthegenre classi\u0002cation.\nAccuracy\nRandom 44.07\nC4.5 49.52\nNaiveBayes 49.92\nk-NN 49.63\nTable 4:Theaveraged accurac yfortheuser tasks.\nTheconfusion table forthecomplete dataset forgenre\nclassi\u0002cation with Nearest Neighbor isshowninTable 2.\nThe ability ofthealgorithm toclassify audio clips de-\npends onthegenre. Forsome genres, asalternati ve,it\nisveryhard forthealgorithm to\u0002ndthecorrect classi\u0002ca-\ntion. However,wecanassume thatevenhuman judgment\nwould notcome toade\u0002nite agreement inthiscase. Small\ngenres, asFunk/Soul, lead topoor classi\u0002cation perfor -\nmance aswell. This canbeexplained bythesmall number\nofexamples inthese classes. Weplan further experiments\nusing multi aspect learning combining textual information\nandaudio information.\n3.2 Classifying User Schemes\nThe user de\u0002ned classi\u0002cation schemes arewell suited\nfordiverse evaluation tasks asaudio classi\u0002cation, audio\nclustering orsimilarity search. Inthissection wepresent\nresults onaudio classi\u0002cation. Ahierarchical classi\u0002ca-\ntion scheme canbeinterpreted asasetofnested classi-\n\u0002cation tasks (using everyinner node assplitting point).\nUsing only inner nodes having child nodes with atleast\ntenitems, weobtained 27\u0003atclassi\u0002cation problems. We\nused severallearners onthese problems andcalculated the\naverage performance. The results arepresented inTable\n4.Alllearners yield apoor result. This motivatesthehy-\npothesis thatathefeature setismore important than apar-\nticular learning scheme. Especially ,weexpect thatanop-\ntimal feature setishighly dependent onthegivenlearning\ntask. Theempirical evidence forthishypothesis isgiven\nin[17].Inasecond experiment weapplied thefeature\n530-0.6-0.4-0.200.20.40.6\n-0.4 -0.2 0 0.2 0.4\nFigure 2:Feature weights oftheuser classi\u0002cation after a\ndimensionality reduction.\nconstruction scheme described there inthiswork. This\nleads to27feature weight vectors describing theutility of\neach feature foreach ofthe27classi\u0002cation tasks. Tovi-\nsualize theresulting matrix, weperformed adimensional-\nityreduction based onsingular value decomposition. The\nresult isshowninFigure 2.Each point represents aclas-\nsi\u0002cation task. Tasks thatareclose toeach other emplo y\nsimilar feature weights. Although wecanseethatsome\ntasks resemble each other tosome extent, ingeneral dif-\nferent tasks require different features. This observ ation\nsupports ourthesis andgivesrisetoameta learning ap-\nproach [17].\nWestrongly belie vethatheterogeneity poses anim-\nportant challenge tofuture audio applications. Wehope\nthatourdataset isahumble contrib ution tothescienti\u0002c\nworkinthisdomain.\nACKNO WLEDGEMENTS\nWewould liketothank themembers ofthestudent project\nNemoz forcreating andproviding their personal classi\u0002-\ncation schemes.\nREFERENCES\n[1]F.KurthandM.Clausen. Full-te xtindexing ofvery-\nlargeaudio data bases. In110th Convention ofthe\nAudio Engineering Society ,2001.\n[2]A.Ghias, J.Logan,D.Chamberlin, andB.C.Smith.\nQuery byHumming: Musical Information Retrie val\ninanAudio Database. InProc.ofACMMultimedia ,\npages 231236, 1995.\n[3]T.Joachims. Learning toClassify Textusing Support\nVector Machines .Kluwer International Series inEn-\ngineering andComputer Science. Kluwer ,2002.\n[4]E.Keogh andP.Smyth. Anenhanced representation\noftime series which allowsfastclassi\u0002cation, clus-\ntering andrelevance feedback. InProcs. ofthe3rd\nConfer ence onKnowledg eDisco very inDatabases ,\npages 2430, 1997.[5]G.Guo andS.Z.Li.Content-Based Audio Classi-\n\u0002cation andRetrie valbySupport Vector Machines.\nIEEE Transaction onNeur alNetworks ,14(1):209\n215, 2003.\n[6]Z.Liu, Y.Wang, andT.Chen. Audio Feature Ex-\ntraction andAnalysis forScene Segmentation and\nClassi\u0002cation. Journal ofVLSI Signal Processing\nSystem ,1998.\n[7]I.Miersw aandK.Morik. Automatic feature extrac-\ntion forclassifying audio data. Machine Learning\nJournal ,58:127149, 2005.\n[8]G.Tzanetakis. Manipulation, Analysis andRetrie val\nSystems forAudio Signals .PhD thesis, Computer\nScience Department, Princeton University ,2002.\n[9]T.Zhang andC.Kuo.Content-based Classi\u0002cation\nandRetrie valofAudio. InSPIE' s43rdAnnual Meet-\ning-Confer ence onAdvanced Signal Processing Al-\ngorithms, Architectur es,andImplementations VIII,\n1998.\n[10] T.Hastie, R.Tibshirani, andJ.Friedman. TheEl-\nements ofStatistical Learning: Data Mining ,Infer -\nence,andPrediction .Springer series instatistics.\nSpringer ,2001.\n[11] A.Blum andT.Mitchell. Combining labeled and\nunlabeled data with co-training. InAnnual Con-\nference onComputational Learning Theory (COL T-\n98),1998.\n[12] S.Baumann, T.Pohle, andV.Shankar .Towards a\nsocio-cultural compatibility ofmirsystems. InProc.\nofthe5thInternational Confer ence onMusic Infor -\nmation Retrie val(ISMIR 2004) ,2004.\n[13] S.Jones andS.J.Cunningham. Organizing digi-\ntalmusic foruse: anexamination ofpersonal music\ncollections. InProc.ofthe5thInternational Confer -\nence onMusic Information Retrie val(ISMIR 2004) ,\n2004.\n[14] E.Pampalk, S.Dixon, andG.Widmer .Exploring\nmusic collections bybrowsing different views. In\nProc.ofthe4thInternational Confer ence onMusic\nInformation Retrie val(ISMIR 2003) ,2003.\n[15] M.Goto, H.Hashiguchi, T.Nishimura, andR.Oka.\nRwc music database: Popular ,classical, andjazz\nmusic databases. InProc.ofthe3rdInternational\nConfer ence onMusic Information Retrie val(ISMIR\n2002) ,pages 287288, 2002.\n[16] S.Fischer ,R.Klink enber g,I.Miersw a,andO.Rit-\nthoff.Yale: YetAnother Learning Environment \nTutorial. Technical Report CI-136/02, Collaborati ve\nResearch Center 531, University ofDortmund, Dort-\nmund, German y,2002.\n[17] I.Miersw aandM.Wurst. Ef\u0002cient case based fea-\nture construction forheterogeneous learning tasks.\nTechnical Report CI-194/05, Collaborati veResearch\nCenter 531, University ofDortmund, 2005.\n531"
    },
    {
        "title": "Lyrics Recognition from a Singing Voice Based on Finite State Automaton for Music Information Retrieval.",
        "author": [
            "Toru Hosoya",
            "Motoyuki Suzuki",
            "Akinori Ito",
            "Shozo Makino"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417855",
        "url": "https://doi.org/10.5281/zenodo.1417855",
        "ee": "https://zenodo.org/records/1417855/files/HosoyaSIM05.pdf",
        "abstract": "Recently, several music information retrieval (MIR) systems have been developed which retrieve musical pieces by the user’s singing voice. All of these systems use only the melody information for retrieval. Although the lyrics information is useful for retrieval, there have been few attempts to exploit lyrics in the user’s input. In order to develop a MIR system that uses lyrics and melody information, lyrics recognition is needed. Lyrics recognition from a singing voice is achieved by similar technology to that of speech recognition. The difference between lyrics recognition and general speech recognition is that the input lyrics are a part of the lyrics of songs in a database. To exploit linguistic constraints maximally, we described the recognition grammar using a finite state automaton (FSA) that accepts only lyrics in the database. In addition, we carried out a “singing voice adaptation” using a speaker adaptation technique. In our experimental results, about 86% retrieval accuracy was obtained. Keywords: MIR, lyrics recognition, FSA 1",
        "zenodo_id": 1417855,
        "dblp_key": "conf/ismir/HosoyaSIM05",
        "keywords": [
            "music information retrieval",
            "singing voice",
            "lyrics recognition",
            "speech recognition",
            "database",
            "linguistic constraints",
            "speaker adaptation",
            "finite state automaton",
            "singing voice adaptation",
            "retrieval accuracy"
        ],
        "content": "LYRICSRECOGNITION FROMA SINGING VOICEBASEDON FINITE\nSTATEAUTOMATONFOR MUSIC INFORMATIONRETRIEVAL\nToruHosoya, Motoyuki Suzuki, Akinori Ito and Shozo Makino\nGraduate School of Engineering, TohokuUniversity\n6-6-05, Aoba, Aramaki, Aoba-ku\nSendai, 980-8579 Japan\nthosoya,moto,aito,makino@makino.ecei.tohoku.ac.jp\nABSTRACT\nRecently, several music information retrieval (MIR) sys-\ntems have been developed which retrieve musical pieces\nby the user’s singing voice. All of these systems use only\nthe melody information for retrieval. Although the lyrics\ninformationis usefulfor retrieval,therehavebeen fewat-\ntempts to exploit lyrics in the user’s input. In order to\ndevelop a MIR system that uses lyrics and melody infor-\nmation, lyrics recognition is needed. Lyrics recognition\nfrom a singing voice is achieved by similar technology to\nthat of speech recognition. The difference between lyrics\nrecognition and general speech recognition is that the in-\nputlyricsareapartofthelyricsofsongsinadatabase. To\nexploit linguistic constraints maximally, we described the\nrecognitiongrammarusingaﬁnitestateautomaton(FSA)\nthat accepts only lyrics in the database. In addition, we\ncarried out a “singing voice adaptation” using a speaker\nadaptation technique. In our experimental results, about\n86%retrievalaccuracywasobtained.\nKeywords: MIR, lyrics recognition, FSA\n1 INTRODUCTION\nRecently, several music information retrieval (MIR) sys-\ntems that use the user’s singing voice as a retrieval\nkey have been researched, some examples are MiDiLiB\n(University of Bonn), MELDEX (Rodger J. McNab,\nLloyd A. Smith, David Bainbridge and Ian H. Witten,\n1997), Themeﬁnder (Stanford University), TuneServer\n(University of Karlsruhe), SuperMBox (J.S.Roger Jang,\nH.LeeandJ.Chen,2001),SoundCompass(NaokoKosugi,\nYuichi Nishihara, Tetsuo Sakata, Masashi Yamamoto and\nKazuhiko Kushima, 2000), MIRACLE (J.S.Roger Jang,\nJiang-Chun,Ming-YangKao, 2001), etc.\nThese systems use melody information in the user’s\nsinging voice. In these systems, the lyrics sung by the\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitationon the ﬁrst page.\nc/circlecopyrt2005 Queen Mary,Universityof Londonvoice are not taken into consideration. We are attempt-\ning to develop a MIR system that uses melody and lyrics\ninformation in the user’s singing voice. Figure 1 shows\nan outline of the MIR system using lyrics and melody\ninformation. First of all, lyrics and melody are acquired\nfrom the user’s song input. Then the system retrieves the\nmusical piece by using one of the recognized lyrics and\nmelody. Finally, the two results are integrated into the re-\ntrieval result. Because it performs the retrieval by using\nboththelyricsandmelody,theaccuracyofretrievalisex-\npected to be better than using only the melody.\nWe have been trying to research lyrics recognition\nfrom the user’s singing voice as a ﬁrst step towards the\nrealization of this system. The lyrics recognition tech-\nniqueusedinseveralconventionalworksissimplyalarge\nvocabularycontinuousspeechrecognition(LVCSR)tech-\nnique, based on an HMM acoustic model and a trigram\nlanguage model. Ozeki et al. performed lyrics recog-\nnition from the singing voice divided into phrases, and\nthe word correct rate was about 59% (Hironao Ozeki,\nTakayuki Kamata, Masataka Goto and Satoru Hayamizu,\n2003). Moreover, we performed lyrics recognition us-\nextraction\nRetrieval\nby lyricsRetrieval\nby melody\nIntegration\nRetrieval resultsMelody LyricsSinging voice\nrecognition\nFigure 1: Outline of the MIR system using lyrics and\nmelody\n532ing an LVCSR system, and the word correct rate was\nabout 61% (Toru Hosoya, Motoyuki Suzuki, Akinori Ito\nand Shozo Makino, 2004). These results are consider-\nably worse compared with the recognition performance\nforread speech.\nWhen a user sings a song as the means of retrieval, it\nis natural to assume that the sung lyrics are a part of the\ndatabase. This assumption means that the lyrics recog-\nnition for MIR can exploit stronger linguistic constraints\nthan general speech recognition. To achieve this, we used\na ﬁnite state automaton (FSA) that accepts any subse-\nquences of the lyrics in the database as a language model\nforlyrics recognition.\n2 LYRICSRECOGNITION BASEDON A\nFINITESTATEAUTOMATON\n2.1 Introduction\nA large vocabulary continuous speech recognition\n(LVCSR) system performs speech recognition using two\nkinds of models — an acoustic model, and a language\nmodel. An HMM (Hidden Markov Model) is the most\npopular acoustic model. An HMM models the acoustic\nfeature of phonemes. On the other hand, bigram or tri-\ngram models are often used as language models. A tri-\ngram model describes probabilities of three contiguous\nwords. Inotherwords,itonlyconsidersapartoftheinput\nword sequence. One reason why an LVCSR system uses\natrigrammodelisthatatrigrammodelhashighcoverage\noveran unknownset of speech inputs.\nThinking of a song input for music information re-\ntrieval, it seems reasonable to make the assumption that\nthe input song is a part of one of the songs in the song\ndatabase. This is a very strong constraint compared with\nordinary speech recognition. To introduce this constraint\ninto our lyrics recognition system, we used a ﬁnite state\nautomaton (FSA) that accepts only a part of the lyrics in\nthe database. By using this FSA as a language model for\nthe speech recognizer, the recognition results are assured\nto be a part of the lyrics in the database. This strategy is\nnotonly useful in improvingthe accuracyof lyricsrecog-\nnition, but also very helpful to the retrieval of a musical\npiece, because the musical piece is naturally determined\nbysimplyﬁndingthepartamongthedatabasethatstrictly\nmatchesthe recognizer outputs.\n2.2 An FSA forrecognition\nFigure 2 shows an example of the ﬁnite state automaton\nused for lyrics recognition. In Fig. 2, “ <s>” is the start\nsymbol, and “ </s>” is the end symbol. The rectangles\nin the ﬁgure stand for words and the arrows are possible\ntransitions. One row in Fig. 2 stands for the lyrics corre-\nspondingto one song.\nIn this FSA, transition from the start symbol to any\nword is allowed, but only two transitions from the word\nareallowed: thetransitiontothenextwordandthetransi-\ntion to the end symbol. As a result, this FSA only accepts\na part of the lyrics that starts from any word and ends at\nanywordin the lyrics.\nFigure 2: Automaton expressionof the grammar\nTable1: Experimental conditions\nRecognitionengine\nHTK\nTestdata\nSinging voiceof ﬁvewords\nAcousticmodel\nmonophone HMM\ntrained from read speech\nDatabase\nJapanese children’ssongs\n238 songs\nWhen lyrics are recognized using this FSA, the song\nnamecanbedeterminedaswellasthelyricsbysearching\nthe transition path of the automaton.\n2.3 Experiment\nThe lyrics recognition experiment was carried out using\nthe FSA as a recognition grammar. Table 1 shows the ex-\nperimental conditions. The test data were singing voice\nsamples, each of which consisted of ﬁve words. The\nsingers were ﬁve male university students. These song\ndata were generated from the whole song data by auto-\nmatically segmenting the song into words. It is thought\nthat people sing a few words when people use MIR sys-\ntems. Therefore, we decided on a test data length of\nﬁve words. Segmentation and the recognition were per-\nformed by HTK (Cambridge University Engineering De-\npartment). The acoustic model was a monophone HMM\ntrained from normal read speech. The recognition result\nis shownin Table2 and 3.\nTable 2 shows the result of word recognition rates\n(word correct rate and word accuracy) and error rates. In\nthetable,“trigram”denotestheresultsusingatrigramlan-\nguagemodeltrainedfromlyricsinthedatabase. Theword\ncorrectrate(Corr)andwordaccuracy(Acc)inTable2are\ncalculated as follows:\nCorr =N−D−S\nN\nAcc =N−D−S−I\nN\nwhere Nis the number of words in the correct lyrics, D\nis the number of deletion error words, Sis the number\nof substitution error words, and Iis the number of inser-\ntion error words. The recognition results of the proposed\nmethod outperformed the conventional trigram language\nmodel.\n533Table2: Wordrecognition/error rate[%]\nGrammar\nCorr\nAcc\nSub\nIns\nDel\nFSA\n75.9\n64.5\n19.9\n4.2\n11.4\ntrigram\n58.3\n48.2\n31.7\n10.0\n10.1\nTable3: Retrievalaccuracy[%]\nretrievalkey\ntop 1\ntop 5\ntop 10\nrecognition results\n76.0\n83.9\n83.9\ncorrect lyrics\n99.7\n100.0\n100.0\nTable 3 shows the results of retrieval accuracy up to\nthe top-10 candidates. Basically, the retrieval accuracy of\nthe top- Rcandidate is the probability of the correct result\nto be listed within the top- Rlist generated by the system.\nThe retrieval accuracy of the top- Rcandidate A(R)was\ncalculatedas follows:\nA(R) =1\nQQ/summationdisplay\ni=1Ti(R)\nTi(R) =\n\n0 r(i)> R\n1 r(i) +ni(r(i))−1≤R\nR−r(i) + 1\nni(r(i))otherwise\nwhere Qis the number of queries, r(i)is the rank of the\ncorrect song in i-th query, ni(x)is the number of songs\ninthe x-thplacein i-thqueryand Ti(R)istheprobability\nthatthecorrectsongappearsinthetop R-thcandidatesof\nthei-th query.\nIn Table 3, “recognition results” is the retrieval accu-\nracyusingrecognizedlyricsand“correctlyrics”isthere-\ntrieval accuracy using the correct lyrics. Note that the re-\ntrieval accuracy of the top result from the “correct lyrics”\nwas not 100% because several songs had the same part of\nlyricsconsisting of ﬁvewords.\nIn our results, about 84% retrieval accuracy was ob-\ntained by the proposed method. As the retrieval accuracy\nitself is not better than that of the query-by-humming-\nbased system (A. Ito, S.-P. Heo, Motoyuki Suzuki and\nShozo Makino, 2004), thisis a promising result.\n3 SINGING VOICEADAPTATION\nAs the acoustic model used in the last experiment was\ntrained from read speech, it may not properly model the\nsinging voice. To improve the acoustic model for model-\ning the singing voice, we tried to adapt the HMM to the\nsingingvoiceusing the speakeradaptation technology.\nSpeakeradaptationisamethodtocustomizeanacous-\ntic model for a speciﬁc user. The recognizer uses a small\namount of the speech of the user, and the acoustic model\nismodiﬁedsothattheprobabilityofgeneratingtheuser’s\nspeech becomes higher. In this paper, we exploited the\nspeaker adaptation method to modify the acoustic model\nfor the singing voice. As we do not want to adapt the\nacoustic model to a speciﬁc user, we used several user’s\nvoicedata for the adaptation.Table4: Wordrecognition/error rate[%] \nAdaptation\nCorr\nAcc\nSub\nIns\nDel\nbefore\n75.9\n64.5\n19.9\n4.2\n11.4\nafter\n83.2\n72.7\n13.8\n3.1\n10.5\nTable5: Retrievalaccuracy[%]\nAdaptation\ntop 1\ntop 5\ntop 10\nbefore\n76.0\n83.9\n83.9\nafter\n82.7\n88.5\n88.5\nIn the following experiment, the MLLR (Maximum\nLikelihoodLinearRegression)method(C.J.Leggetterand\nP.C.Woodland, 1995) was used as an adaptation algo-\nrithm. 127 choruses sung by 6 males were used as the\nadaptationdata. These6singersweredifferentfromthose\nwho sang the test data. Other experimental conditions\nwere the same as those shownin Table1.\nTable 4 shows the word recognition rates before and\nafter adaptation. These results show that the adaptation\ngave more than 7 points of improvement in the word cor-\nrect rate. Table 5 shows the retrieval accuracy results.\nThese results provethe effectivenessof the adaptation.\n4 IMPROVEMENTOF THE FSA:\nCONSIDERATIONOF JAPANESE\nPHRASE STRUCTURE\nTheFSAusedintheaboveexperimentsacceptsanyword\nsequences which are a sub-sequence of the lyrics in the\ndatabase. However, no user begins to sing from any word\nin the lyrics and ﬁnishes singing at any word. As the lan-\nguage of the texts in these experiments is Japanese, the\nconstraints of Japanese phrase structure can be exploited.\nAJapanesesentencecanbe regardedasasequenceof\n“bunsetsu”. A “bunsetsu” is a linguistic structure similar\nto a phrase in English. One “ bunsetsu” is composed of\none content word followed by zero or more particles or\nsufﬁxes. In Japanese, singing from a particle or a sufﬁx\nhardlyeveroccurs. Forexample,inthefollowingsentence\nFigure 3: Example of improvedgrammar\n534Table6: Wordrecognition/error rate[%]\nFSA\nCorr\nAcc\nSub\nIns\nDel\noriginal\n83.2\n72.7\n13.8\n3.1\n10.5\nimproved\n86.0\n77.4\n10.6\n3.4\n8.6\nTable7: Retrievalaccuracy[%]\nFSA\ntop 1\ntop 5\ntop 10\noriginal\n82.7\n88.5\n88.5\nimproved\n85.9\n91.3\n91.3\nbara ga\nsai ta\n···\nrose (subject)\nbloom (past)\n“bara ga” and “sai ta” are “bunsetsu”, and a user hardly\never begins to sing from “ ga” or “ta”. Therefore, we\nchangedthe FSA described in Section 2.2 so that:\n1. Omit all transitions from the start symbol “ <s>” to\nanyparticles or sufﬁxes\n2. Omit all transitions from the start or middle word of\na “bunsetsu” to the end symbol “ </s>”\nAnexampleof the improvedFSA is shownin Fig. 3.\nThe lyrics recognition experiment was carried out us-\ning the improved FSA. The adapted HMM described in\nSection 3 was used for the acoustic model, and the other\nexperimental conditions were the same as those shown in\nTable1. The results are shownin Table6 and 7.\nBothwordrecognitionratesandretrievalaccuracyim-\nprovedcomparedwiththatoftheoriginalFSA.Theword\ncorrectrateandtheretrievalaccuracyoftheﬁrstrankwere\nabout 86%. These results showed the effectiveness of the\nproposed constraints.\n5 CONCLUSION\nIn pursuit of a MIR system that uses both melody and\nlyrics information in the singing voice, we tried to rec-\nognize lyrics in the users’ singing voice. To exploit the\nconstraints of the input song maximally, we used an FSA\nthat accepts only a part of word sequences in the song\ndatabase. Then we performed “singing voice adaptation”\nusing MLLR speaker adaptation technology. Finally, we\ntried to introduce further linguistic constraints into the\nFSA. From the experimental results, the proposed meth-\nodsprovedtobeeffective. Asaresult,about86%retrieval\naccuracywasobtained.\nREFERENCES\nA. Ito, S.-P. Heo, Motoyuki Suzuki and Shozo Makino.\nComparisonofFeaturesforDP-matchingbasedQuery-\nby-hummingSystem. In Proc.ISMIR ,10 2004.\nCambridge University Engineering Department. Hidden\nMarkovModel Toolkit. http://htk.eng.cam.ac.uk/.\nC.J.Leggetter and P.C.Woodland. Maximum likelihood\nlinear regression for speaker adaptation of continuousdensity hidden Markov models. In Computer Speech\nand Language ,4 1995.\nHironao Ozeki, Takayuki Kamata, Masataka Goto and\nSatoruHayamizu. Theinﬂuenceofvocalpitchonlyrics\nrecognitionofsungmelodies. In Theproceedingsofthe\n2003autumnmeetingoftheacousticalsocietyofjapan ,\n9 2003.\nJ.S.RogerJang,H.LeeandJ.Chen. SuperMBox: AnEfﬁ-\ncient/Effective Content-based Music Retrieval System.\nInIn the ninth ACM Multimedia Conference(Demo pa-\nper),2001.\nJ.S.Roger Jang, Jiang-Chun, Ming-Yang Kao. MIRA-\nCLE:AMusicInformationRetrievalSystemwithClus-\ntered Computing Engines. In International Symposium\non Music Information Retrieval , 2001.\nNaoko Kosugi, Yuichi Nishihara, Tetsuo Sakata, Masashi\nYamamotoandKazuhikoKushima. APracticalQuery-\nBy-Humming System for a Large Music Database. In\nACMMultimedia 2000 , 2000.\nRodger J. McNab, Lloyd A. Smith, David Bainbridge\nand Ian H. Witten. The NewZealand Digital Library\nMELody inDEX. D-Lib Magazine,May 1997.\nStanford University. Themeﬁnder.\nhttp://www.themeﬁnder.org/.\nToru Hosoya, Motoyuki Suzuki, Akinori Ito and Shozo\nMakino. Song retrieval system using the lyrics recog-\nnized vocal. In The proceedings of the 2004 autumn\nmeeting of the acoustical society of japan ,9 2004.\nUniversity of Bonn. MidiLiB. http://www-mmdb.iai.uni-\nbonn.de/forschungprojekte/midilib/english/.\nUniversity of Karlsruhe. Tuneserver. http://name-this-\ntune.com/.\n535"
    },
    {
        "title": "A Bootstrap Method for Training an Accurate Audio Segmenter.",
        "author": [
            "Ning Hu",
            "Roger B. Dannenberg"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416200",
        "url": "https://doi.org/10.5281/zenodo.1416200",
        "ee": "https://zenodo.org/records/1416200/files/HuD05.pdf",
        "abstract": "Supervised learning can be used to create good systems for note segmentation in audio data. However, this requires a large set of labeled training examples, and handlabeling is quite difficult and time consuming. A bootstrap approach is introduced in which audio alignment techniques are first used to find the correspondence between a symbolic music representation (such as MIDI data) and an acoustic recording. This alignment provides an initial estimate of note boundaries which can be used to train a segmenter. Once trained, the segmenter can be used to refine the initial set of note boundaries and training can be repeated. This iterative training process eliminates the need for hand-segmented audio. Tests show that this training method can improve a segmenter initially trained on synthetic data. Keywords: Bootstrap, music audio segmentation, note onset detection, audio-to-score alignment. 1",
        "zenodo_id": 1416200,
        "dblp_key": "conf/ismir/HuD05",
        "keywords": [
            "Supervised learning",
            "note segmentation",
            "audio data",
            "large set of labeled training examples",
            "handlabeling",
            "time consuming",
            "bootstrap approach",
            "audio alignment techniques",
            "symbolic music representation",
            "acoustic recording"
        ],
        "content": "ABootstrap Method forTraining anAccurate Audio Segmenter\nNing HuandRoger B.Dannenber g\nComputer Science Department\nCarne gieMellon University\n5000 Forbes Ave\nPittsb urgh,PA15213\nfninghu,rbd g@cs.cmu.edu\nABSTRA CT\nSupervised learning canbeused tocreate good systems\nfornote segmentation inaudio data. However,thisre-\nquires alargesetoflabeled training examples, andhand-\nlabeling isquite dif\u0002cult andtime consuming. Abootstrap\napproach isintroduced inwhich audio alignment tech-\nniques are\u0002rst used to\u0002nd thecorrespondence between\nasymbolic music representation (such asMIDI data) and\nanacoustic recording. This alignment provides aninitial\nestimate ofnote boundaries which canbeused totrain a\nsegmenter .Once trained, thesegmenter canbeused to\nre\u0002ne theinitial setofnote boundaries andtraining can\nberepeated. This iterati vetraining process eliminates the\nneed forhand-se gmented audio. Testsshowthatthistrain-\ningmethod canimpro veasegmenter initially trained on\nsynthetic data.\nKeywords: Bootstrap, music audio segmentation, note\nonset detection, audio-to-score alignment.\n1INTR ODUCTION\nAudio Segmentation isoneofthemajor topics inMusic\nInformation Retrie val(MIR). ManyMIR applications and\nsystems areclosely related toaudio segmentation, espe-\ncially those thatdeal with acoustic signals. Audio seg-\nmentation issometimes theessential purpose oftheappli-\ncation, such asdividing acoustic recordings into singing\nsolo andaccompaniment parts. Alternati vely,audio seg-\nmentation canform animportant module inasystem, for\nexample, detecting note onsets inthesung queries for\nQuery-by-Humming systems.\nAcommon practice istoapply various machine learn-\ningtechniques totheaudio segmentation problem, and\nthere aremanysatisfying results. Some oftherepresen-\ntativemachine learning models used inthisarea arethe\nHidden Mark ovModel (HMM) (Raphael, 1999), Neural\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonNetw ork(Marolt etal.,2002), Support Vector Machine\n(SVM) (Luetal.,2001), Hierarchical Model (Kapanci\nandPfeffer,2004), etc. However,asinmanyother ma-\nchine learning applications, audio segmentation using ma-\nchine learning schemes inevitably faces aproblem: get-\nting training data isdif\u0002cult andtedious. Manually seg-\nmenting each note ina\u0002ve-minute piece ofmusic cantake\nseveralhours ofwork. Since thequantity andquality of\nthetraining data directly affects theperformance ofthe\nmachine learning model, manydesigners havenochoice\nbuttolabel some training data byhand.\nMeanwhile, theresearch ofaudio-to-score alignment\nhasbecome apopular MIR topic inrecent years. Link-\ningsignal andsymbolic representations ofmusic canen-\nable manyinteresting applications, such aspolyphonic\nmusic retrie val(Hu etal.,2003), real-time score follow-\ning(Raphael, 2004), andintelligent editors (Dannenber g\nandHu,2003).\nInasense, audio-to-score alignment andmusic au-\ndiosegmentation areclosely related. Both theoperations\nareperformed onacoustic features extracted from theau-\ndio,though alignment focuses onglobal correspondence\nwhile segmentation focuses onlocal changes. Givena\nprecise alignment between thesymbolic andcorrespond-\ningacoustic data, desired segments canbeeasily extracted\nfrom audio. Evenifalignment isnotthatprecise, itstill\nprovides valuable information tomusic audio segmenta-\ntion. Conversely ,givena(precise) segmentation, align-\nment becomes almost trivial. This relationship between\nalignment andsegmentation canbeexploited toimpro ve\nmusic segmentation.\nWepropose abootstrap method that uses automatic\nalignment information tohelp train thesegmenter .The\ntraining process consists oftwoparts. Oneisanalignment\nprocess that \u0002nds thetime correspondence between the\nsymbolic andacoustic representations ofamusic piece.\nThe other part isanaudio segmentation process thatex-\ntracts note fragments from theacoustic recording. Align-\nment isaccomplished bymatching sequences ofchro-\nmagram features using Dynamic TimeWarping (DTW).\nThesegmentation model isafeed-forw ardneural netw ork,\nwith severalfeatures extracted from audio astheinputs,\nand areal value between 0and 1astheoutput. The\nalignment results help totrain thesegmenter iterati vely.\nOurimplementation andevaluation showthatthistraining\nscheme isfeasible, andthatitcangreatly impro vetheper-\n223formance ofaudio segmentation without manually label-\ninganytraining data. Though weneed tonote thattheau-\ndiosegmentation process discussed inthispaper isaimed\natdetecting note onsets, thisbootstrap learning scheme\ncombined with automatic alignment canalso beused for\nother kinds ofaudio segmentation.\nThe initial purpose ofthis project istoaidthere-\nsearch ofcreating high-quality music synthesis bylisten-\ningtoacoustic examples. The synthesis approach com-\nbines aperformance model thatderivesappropriate ampli-\ntude andfrequenc ycontrol signals from amusical score\nwith aninstrument model thatgenerates sound with ap-\npropriate time-v arying spectrum. Inorder tolearn the\nproperties ofamplitude andfrequenc yenvelopes forthe\nperformance model, weneed tosegment individual notes\nfrom acoustic recordings andlinkthem tocorresponding\nscore fragments. This certainly requires aaudio-to-score\nalignment process. Wepreviously developed apolyphonic\naudio alignment system andeffectivelydeplo yeditinsev-\neralapplications (Hu etal.,2003) (Dannenber gandHu,\n2003). Butwefaceaparticular challenge when trying to\nusethealignment system inthiscase, mainly duetothe\nspecial requirement imposed bythenature ofinstrumen-\ntalsounds. Foranyindividual note generated byamusical\ninstrument, theattack partisperceptually veryimportant.\nFurthermore, attacks areusually veryshort. The attack\npartofatypical trumpet tone lasts only about 30millisec-\nonds (see Figure 1).Butduetolimits imposed bythe\nacoustic features used foralignment, thesizeoftheanal-\nysiswindo wsisusually 0.1to0.25 s,which isnotsmall\nenough fornote segmentation, especially theattack part,\nwhich canbeeasily overlook ed.Therefore, wemust pur-\nsueaccur ateaudio alignment with aresolution ofseveral\nmilliseconds. Because oursegmentation system isdevel-\noped formusic synthesis, wearemainly concerned with\nmonophonic audio, butwebelie vethatitshould notbe\ntoodif\u0002cult toextend thisworktodeal with polyphonic\nmusic.\nFigure 1:Atypical trumpet slurred note (amezzo forte\nC4from anascending scale ofslurred quarter notes), dis-\nplayed inwaveform along with theamplitude envelope .\nAttack, sustain anddecay parts areindicated inthe\u0002gure.\nThe audio-to-score alignment process isclosely re-\nlated tothat ofOrio and Schw arz(2001), who also\nuses dynamic time warping toalign polyphonic music to\nscores. While weusethechromagram (described inalater\nsection), theyuseameasure called Peak Structure Dis-tance, which isderivedfrom thespectrum ofaudio and\nfrom synthetic spectra computed from score data. An-\nother noteworthyaspect oftheir workisthat, since they\nalso intend touseitformusic synthesis (Schw arz,2004),\ntheyobtain accurate alignment using small (5.8ms)analy-\nsiswindo ws,andtheaverage error isabout 23ms(Soulez\netal.,2003), which makesitpossible todirectly gener -\natetraining data foraudio segmentation. However,this\nalso greatly affects theef\u0002cienc yofthealignment pro-\ncess. Theyreport thatevenwith optimization measures,\ntheir system isrunning 2hours for5minutes ofmusic,\nandoccup ying 400MB memory. Incontrast, oursystem\nuses largeranalysis windo wsandaligns 5minutes ofmu-\nsicinlessthan 5minutes. Although weuselargeranalysis\nwindo wsforalignment, weusesmall analysis windo ws\n(and different features) forsegmentation, andthisallows\nustoobtain high accurac y.\nInthefollowing sections, wedescribe oursystem in\ndetail. Weintroduce theaudio-to-score alignment process\ninSection 2,andthesegmentation model inSection 3.\nSection 4describes thebootstrap learning method inde-\ntail.Section 5evaluates thesystem andpresents some ex-\nperimental results. Weconclude andsummarize thispaper\ninthelastsection.\n2AUDIO-T O-SCORE ALIGNMENT\n2.1 The Chroma Repr esentation\nAswementioned above,thealignment isperformed on\ntwosequences offeatures extracted from both thesym-\nbolic andaudio data. Compared with severalother repre-\nsentations, thechroma representation isclearly awinner\nforthistask(Huetal.,2003).\nThus our\u0002rststepistoconvertaudio dataintodiscr ete\nchromagrams:sequences ofchroma vectors. Thechroma\nvector representation isa12-element vector ,where each\nelement represents thespectral energycorresponding to\nonepitch class (i.e. C,C#,D,D#,etc.). Tocompute a\nchroma vector from amagnitude spectrum, weassign each\nbinoftheFFT tothepitch class ofthenearest step inthe\nchromatic equal-tempered scale. Then, givenapitch class,\nweaverage themagnitude ofthecorresponding bins. This\nresults ina12-value chroma vector .Each chroma vector\ninthisworkrepresents 0.05 seconds ofaudio data (non-\noverlapping).\nThe symbolic data, i.e.MIDI \u0002le, isalso tobecon-\nverted into chromagrams. The traditional wayistosyn-\nthesize theMIDI data, andthen convertthesynthetic au-\ndiointochromagrams. However,wehavefound asimple\nalternati vethatdirectly maps from MIDI eventstochroma\nvectors (Huetal.,2003). Tocompute thechromagram di-\nrectly from MIDI data, we\u0002rstassociate each pitch class\nwith anindependent unitchroma vector -thechroma vec-\ntorwith only oneelement value as1andtherestas0;\nthen, where there ispolyphon yintheMIDI data, theunit\nchroma vectors aresimply multiplied bytheloudness fac-\ntors, added andnormalized.\nThedirect mapping scheme speeds upthesystem by\nskipping thesynthesis procedure, anditrarely sacri\u0002ces\nthealignment results. Infact, inmost cases wehave\ntried, theresults aregenerally better when using thisal-\n224ternati veapproach. Furthermore, itispositi velyneces-\nsary tobypass thesynthesis step forthisparticular ex-\nperiment. While rendering audio from symbolic data,\nthesynthesizer Timidity++ (Toivonen andIzumo, 1995\n2004) alwaysintroduces small variations intime. Buta\nlater procedure needs toestimate note onsets intheacous-\nticrecording bymapping from theMIDI \u0002lethrough the\nalignment path. And anyasynchronization between the\nsymbolic andsynthetic datacangreatly affectitsaccurac y.\n2.2 Matching MIDI toAudio\nAfter obtaining twosequences ofchroma vectors from au-\ndiorecording andMIDI data, weneed to\u0002nd thetime\ncorrespondence between thetwosequences such thatcor-\nresponding vectors aresimilar .Before comparing the\nchroma vectors, wemust \u0002rstnormalize thevectors, asob-\nviously theamplitude levelvaries throughout theacoustic\nrecordings andMIDI \u0002les. Weexperimented with differ-\nentnormalization methods, andnormalizing thevectors\ntohaveamean ofzero andavariance ofoneseems to\nbethebest one. Butthiscancause trouble when dealing\nwith silence .Thus, iftheaverage amplitude ofanaudio\nframe islowerthan aprede\u0002ned threshold, wede\u0002ne it\nasasilence frame, andassign each element ofthecor-\nresponding chroma vector in\u0002nite. Wethen calculate the\nEuclidean distance between thevectors. The distance is\nzero ifthere isperfect agreement. Figure 2showsasimi-\nlarity matrix where thehorizontal axisisatime indexinto\ntheacoustic recording, andthevertical axis isatime in-\ndexintotheMIDI data. Theintensity ofeach point isthe\ndistance between thecorresponding vectors, where black\nrepresents adistance ofzero.\n  MIDI (s)\n  Acoustic Recording (s)2 4 6 8 10 1224681012\nFigure 2:Similarity Matrix forthe\u0002rst part inthethird\nmovement ofEnglish Suite composed byR.Bernard\nFitzgerald. Theacoustic recording isthetrumpet perfor -\nmance bythesecond author .\nWeusetheDynamic TimeWarping (DTW) algorithmto\u0002nd theoptimal alignment. DTW computes apath in\nasimilarity matrix where therowscorrespond toonevec-\ntorsequence andcolumns correspond totheother .The\npath isasequence ofadjacent cells, andDTW \u0002nds the\npath with thesmallest sum ofdistances. ForDTW ,each\nmatrix cell(i,j)represents thesum ofdistances along the\nbest path from (0,0) to(i,j). Weusethecalculation pat-\ntern showninFigure 3foreach cell. The best path up\ntolocation (i,j)inthematrix (labeled Dinthe\u0002gure) de-\npends only ontheadjacent cells (A,B,andC)andthe\nweighted distance between thevectors corresponding to\nrowiandcolumn j.Note thatthehorizontal step from\nCandthevertical step from Ballowfortheskipping of\nsilence ineither sequence. Wealso weight thedistance\nvalue inthestep from cellAbyp\n2soasnottofavorthe\ndiagonal direction. This calculation pattern istheonewe\nfeelmore comfortable with, buttheresulting differences\nfrom various formulations ofDTW (HuandDannenber g,\n2002) areoften toosubtle toshowaclear difference. The\nDTW algorithm requires asingle pass through thematrix\ntocompute thecostofthebestpath. Then, abacktracking\nstepisused toidentify theactual path.\nThe time comple xity oftheautomatic alignment is\nO(mn),where mandnarerespecti velythelengths of\nthetwocompared feature sequences. Assuming theex-\npected optimal alignment path isalong thediagonal, we\ncanoptimize theprocess byrunning DTW onjustapart\nofthesimilarity matrix, which isbasically adiagonal band\nrepresenting theallowablerange ofmisalignment between\nthetwosequences. Then thetime comple xitycanbere-\nduced toO(max(m;n)).\nBi\ni-1C D\nAj j-1\nD=Mi;j=min(2\n4A\nB\nC3\n5+2\n4p\n2\n1\n13\n5\u0002dist(i;j))\nFigure 3:Calculation pattern forcell(i;j)\nAfter computing theoptimal path found byDTW ,we\ngetthetime points ofthose note onsets intheMIDI \u0002le\nandmap them totheacoustic recording according tothe\npath (seeFigure 4).\nThe analysis windo wused foralignment isWa=\n50ms,andasmaller windo wactually makesthealign-\nment worse because ofthewaychroma vectors arecom-\nputed. Thus thealignment result isreally notthataccu-\nrate,considering theresolution from alignment isonthe\nsame scale astheanalysis windo wsize. Nevertheless, the\nalignment path stillindicates roughly where thenote on-\nsetsshould beintheaudio. Infact,theestimation ofthe\nerror between theactual note onsets andtheones found\nbythepath issimilar toaGaussian distrib ution. Inother\nwords, thepossibility ofobserving anactual note onset\naround anestimated onegivenbythealignment isapprox-\n225  MIDI (s)\n  Acoustic Recording (s)2 4 6 8 10 1224681012\nFigure 4:The optimal alignment path isshowninwhite\noverthesimilarity matrix ofFigure 2;thelittle circles on\nthepath denote themapping ofnote onsets.\nimately aGaussian distrib ution. This isvaluable informa-\ntionthatcanhelp totrain thesegmenter .\n3NOTESEGMENT ATION\n3.1 Acoustic Featur es\nSeveralfeatures areextracted from theacoustic signals.\nThebasic ones arelisted below:\n\u000fLogarithmic energy,distinguishing silent frames\nfrom theaudio,\nLogEng=10log10Energy\nEnergy0,\nwhere Energy0=1.\n\u000fFundamental frequenc yF0.Fundamental frequenc y\nand harmonics arecomputed using theMcAulay-\nQuatieri Model (McAulay andQuatieri, 1986) pro-\nvided bytheSND ANpackage (Beauchamp, 1993).\n\u000fRelati vestrengths of\u0002rstthree harmonics\nRelAmp i=Ampl itude i\nAmpl itude overall,\nwhere idenotes which harmonic.\n\u000fRelati vefrequenc ydeviations of\u0002rstthree harmon-\nics\nRelDFri=fi\u0000i\u0002F0\nfi,\nwhere fiisthefrequenc yoftheithharmonic\n\u000fZero-crossing rate(ZCR), serving asanindicator of\nthenoisiness ofthesignal.\nFurthermore, thederivativesofthose features arealso\nincluded, asderivativesaregood indicators of\u0003uctuations\nintheaudio such asnote attacks orfricati ves.\nAllofthose features arecomputed using asliding non-\noverlapping analysis windo wWswith asizeof5.8ms.Iftheaudio tobeprocessed hasthesample rateof44.1 KHz,\neveryanalysis windo wcontains 256samples.\n3.2 Segmentation Model\nWeuseamulti-layer Neural Netw orkasthesegmentation\nmodel (see Figure 5).Itisafeed-forw ardnetw orkthatis\nessentially anon-linear function with a\u0002nite setofparam-\neters. Each neuron (perceptron) isaSigmoid unit, which\nisde\u0002ned asf(s)=1\n1+e\u0000s,where sistheinput ofthe\nneuron, andf(s)istheoutput. Theinput units accept those\nfeatures extracted from theacoustic signals. Theoutput is\nasingle realvalue ranging from 0to1,indicating thelike-\nlihood ofbeing asegmentation point forthecurrent audio\nframe. Inother words, theoutput isthemodel' sestimate\nofthecertainty ofanote onset. When using themodel to\nsegment theaudio \u0002le, anaudio frame isclassi\u0002ed asa\nnote onset ifoutput ofthesegmenter ismore than 0.5.\nOutput Input Layer 1 Layer 2\nFigure 5:Neural Netw orkforSegmentation\nNeural netw orks offerastandard approach forsuper -\nvised learning. Labeled data arerequired totrain anet-\nwork. Training isaccomplished byadjusting weights\nwithin thenetw orktominimize theexpected output error .\nWeuseaconventional back-propag ation learning method\ntotrain themodel.\nWeshould note thatthesegmentation model used in\nthisproject isatypical butrather simple one, anditsper-\nformance alone may notbethebest among other more\ncomplicated models. The emphasis ofthispaper isto\ndemonstrate thatthealignment information canhelp train\nthesegmenter andimpro veitsperformance, nothowwell\nthestandalone segmenter performs.\n4Bootstrap Lear ning\nAfter wegettheestimated note onsets from thealignment\npath found byDTW ,wecreate aprobability density func-\ntion (PDF) indicating thepossibility ofbeing anactual\nnote onset ateach time point intheacoustic recording.\nAsshowninFigure 6,thePDF isgenerated byoverlap-\nping asetofGaussian windo ws.Each windo wiscentered\nattheestimated note onsets givenbythealignment path,\nandhastwice thesizeofthealignment analysis windo w\n(2\u00020:05s=0:1s).Forthose points outside anyGaus-\n226sianwindo w,thevalueisassigned toasmall valueslightly\nbigger than 0(e.g.0:04).\nPDF\nTime (s)\nFigure 6:PDF generated from thealignment ofasnippet,\nwhich isaphrase ofthemusic content inFigure 2.\nThen werunthefollowing steps iterati velyuntil either\ntheweights intheneural netw orkconverge,orthevali-\ndation error reaches itsminimum soasnottoover\u0002t the\ndata.\n1.Execute segmentation process ontheacoustic audio.\n2.Multiply thesequence ofrealvaluesvoutput bythe\nsegmenter with thenote onset PDF.The result isa\nnewsequence ofvalues denoted asvnew.\n3.Foreach estimated note onset, \u0002ndatime point that\nhasthebiggest valuevnewwithin awindo wWp,and\nmark itastheadjusted note onset. The windo wis\nde\u0002ned asfollows:\nWp(i)=[max\u0010\nTi+Ti\u00001\n2;Ti\u0000Wa\u0011\n;\nmin\u0010\nTi+Ti+1\n2;Ti+Wa\u0011\n],\nwhere Tiistheestimated onset time oftheithnote in\ntheacoustic recording givenbyalignment, andWais\nthesizeoftheanalysis windo wforalignment.\n4.Usetheaudio frames tore-train theneural netw ork.\nTheadjusted note onset points arelabeled as1,and\ntherestarelabeled as0.Because thedataset isimbal-\nanced asthenumber ofpositi veexamples isfarless\nthan thenegativeones, weadjust thecostfunction to\nincrease thepenalty when falsenegativesoccur .\nAsthesegmentation model hasasmaller resolution\nthan thealignment model, thetrained segmenter cande-tectnote boundaries inaudio signals more precisely ,as\ndemonstrated inFigure 7.\n5EVALU ATIONS\nTheexperimental data istheEnglish Suite composed by\nR.Bernard Fitzgerald (Fitzgerald) forBbTrumpet. Itis\nasetof5English folk tunes artfully arranged into one\nwork. Each ofthe5movements isessentially amono-\nphonic melody ,andthewhole suite contains atotal of673\nnotes. Wehaveseveralformats ofthisparticular music\npiece, including theMIDI \u0002les created using adigital pi-\nano, therealacoustic recordings performed bythesecond\nauthor ,andsynthetic audio generated from theMIDI \u0002les.\nWerunsome experiments tocompare twosystems.\nOne isabaseline segmenter ,which ispre-trained using a\ndifferent MIDI \u0002leanditssynthetic data; theother isa\nsegmenter with thebootstrap method, which hasthesame\ninitial setup oftheneural netw orkasthatofthebaseline\nsegmenter ,butthealignment information isused tohelp\niterati velytrain thesegmenter .Werunthebaseline seg-\nmenter through alltheaudio \u0002les inthedata setandcom-\npare itsdetected note onsets with theactual ones. Forthe\nsegmenter with bootstrapping, weusecross-v alidation. In\neveryvalidation pass, 4MIDI \u0002les andthecorresponding\naudio \u0002les areused totrain thesegmenter ,andtheremain-\ningMIDI-audio \u0002les pairisused asthevalidataion setfor\nstopping thetraining iterations topreventdata over\u0002tting.\nThis process isrepeated sothatthedata ofall5move-\nments haveonce been used forvalidation, andtheerror\nmeasuring results onthevalidation setsarecombined to\nevaluate themodel performance.\nWecalculate several values tomeasure theperfor -\nmance ofthesystems. Miss rateisde\u0002ned astheratio of\nmissed ones among alltheactual note onsets anactual\nnote onset isdetermined tobeamissed one, when there is\nnodetected onset within thewindo wWparound it;spu-\nrious rateistheratio between spurious ones detected by\nthesystem andalltheactual note onsets spurious note\nonsets include those detected ones thatdonotcorrespond\ntoanyactual onset; average error andstandard deviation\n(STD) indicate theattrib uteofthedistance between each\nactual note onset anditscorresponding detected one, ifthe\nnote onset isneither missed orspurious.\nWe\u0002rstusethesynthetic audio from MIDI \u0002les asthe\ndata set,andtheexperimental results areshowninTable\n1.\nTable 1:Model Comparison onSynthetic Audio\nModel Miss Spurious Average STD\nRate Rate Error\nBaseline\nSegmenter 8.8% 10.3% 21ms 29ms\nSegmenter\nw/Bootstrap 0.0% 0.3% 10ms 14ms\nWealsotrythetwosegmenters ontheacoustic record-\nings. However,itisverydif\u0002cult totakeoverall measures,\naslabeling allthenote onsets inacoustic recordings istoo\ntime consuming. Wehavetorandomly pick asetof100\n227PDF\nAcoustic\nWaveformEstimated Note Onsets  from Alignment\nDetected Note Onsets by Segmenter w/ Bootstrap\nFigure 7:Note segmentation results onthesame music content asinFigure 6.Note thatthenote onsets detected bythe\nsegmenter with bootstrapping arenotexactly thesame astheones estimated from alignment. This isbest illustrated on\nthenote boundary around 1:8seconds.\nnote onsets throughout themusic piece (20ineach move-\nment), andmeasure their results manually .Theresults are\nshowninTable 2.\nTable 2:Model Comparison onReal Recordings\nModel Miss Spurious Average STD\nRate Rate Error\nBaseline\nSegmenter 15.0% 25.0% 35ms 48ms\nSegmenter\nw/Bootstrap 2.0% 4.0% 8ms 12ms\nAswecansee,thebaseline segmenter performs worse\nontherealrecordings than onthesynthetic data, which\nindicates there areindeed some differences between syn-\nthetic audio andrealrecordings thatcanaffecttheperfor -\nmance. Nevertheless, thesegmenter with bootstrapping\ncontinues toperform verywell onrecordings ofanacous-\nticinstrument.\n6CONCLUSIONS\nMusic segmentation isanimportant step inmanymusic\nprocessing tasks, including beat tracking, tempo analysis,\nmusic transcription, andmusic alignment. However,seg-\nmenting music atnote boundaries israther dif\u0002cult. Inreal\nrecordings, theendofonenote often overlaps thebegin-\nning ofthenextduetoresonance inacoustic instruments\nandreverberation intheperformance space. Evenhumans\nhavedif\u0002culty deciding exactly where note transitions oc-\ncur.One promising approach togood segmentation isma-\nchine learning. Withgood training data, supervised learn-\ningsystems frequently outperform those created inanad\nhocfashion. Unfortunately ,wedonothaveverygood\ntraining data formusic segmentation, andlabeling acous-\nticrecordings byhand isverydif\u0002cult andtime consum-\ning.\nOur workoffersasolution totheproblem ofobtain-\ninggood training data. Weusemusic alignment totell\nus(approximately) where to\u0002nd note boundaries. This\ninformation isused toimpro vethesegmentation, andthesegmentation canthen beused aslabeled training data to\nimpro vethesegmenter .This bootstrapping process isit-\nerated until itconverges.\nOur tests showthat segmentation can bedramati-\ncally impro vedusing thisapproach. Note thatwhile we\nusealignment tohelp train thesegmenter ,wetested the\ntrained segmenters without using alignment. Ofcourse,\nwhene verasymbolic score isavailable, evenmore ac-\ncurate segmentation should bepossible bycombining the\nsegmenter with thealignment results.\nMachine learning isespecially effectivewhen many\nfeatures must beconsidered. Infuture work, wehope\ntoimpro vefurther onsegmentation byconsidering many\nmore signal features. This willrequire more training data,\nbutourbootstrapping method should makethisfeasible.\nInsummary ,wehavedescribed asystem formusic\nsegmentation thatuses alignment toprovide aninitial set\noflabeled training data. Abootstrap method isused to\nimpro veboth thelabels andthesegmenter .Segmenters\ntrained inthismanner showimpro vedperformance over\nabaseline segmenter that haslittle training. Our boot-\nstrap approach canbegeneralized toincorporate addi-\ntional signal features andother supervised learning algo-\nrithms. This method isalready being used tosegment\nacoustic recordings foramusic synthesis application, and\nwebelie vemanyother applications canbene\u0002t from this\nnewapproach.\nACKNO WLEDGEMENTS\nThis project greatly bene\u0002ts from thehelpful discussions\nandsuggestions during theComputer Music group meet-\nings held atCarne gieMellon University .Wewould also\nliketothank Guanfeng Liforhisvaluable inputs andsup-\nport.\nRefer ences\nJames Beauchamp. Unix workstation softw areforanal-\nysis, graphics, modi\u0002cations, andsynthesis ofmusical\nsounds. InAudio Engineering Society Preprint ,number\n3479. Berlin, 1993.\nRoger B.Dannenber gand Ning Hu. Polyphonic au-\ndiomatching forscore following and intelligent au-\n228dioeditors. InProceedings ofthe2003 International\nComputer Music Confer ence,pages 2734, Singapore,\n2003.\nR.Bernard Fitzgerald. English suite. Transcribed forBb\nTrumpet (orCornet) andPiano.\nNing Huand Roger B.Dannenber g. Acomparison\nofmelodic database retrie valtechniques using sung\nqueries. InJCDL 2002: Proceedings oftheSecond\nACM/IEEE-CS Joint Confer ence onDigital Libraries ,\n2002.\nNing Hu,Roger B.Dannenber g,andGeor geTzanetakis.\nPolyphonic audio matching andalignment formusic re-\ntrieval.In2003 IEEE Workshop onApplications ofSig-\nnalProcessing toAudio andAcoustics ,pages 185188,\nNewYork,2003.\nEmir Kapanci andAviPfeffer.Ahierarchical approach\ntoonset detection. InProceedings ofthe2004 Inter -\nnational Computer Music Confer ence,pages 438441,\nOrlando, 2004.\nLieLu,Stan Z.Li,andHong Jiang Zhang. Content-based\naudio segmentation using support vector machines. In\nProceedings oftheIEEE International Confer ence on\nMultimedia and Expo (ICME 2001) ,pages 956959,\nTokyo,Japan, 2001.\nMatija Marolt, Alenka Kavcic, and Mark oPrivosnik.\nNeural netw orks fornote onset detection inpiano mu-\nsic.InProceedings ofthe2002 International Computer\nMusic Confer ence,2002.\nR.J. McAulay and Th.F.Quatieri. Speech analy-\nsis/synthesis based onasinusoidal representation. IEEE\nTransactions onAcoustics, Speec h,andSignal Process-\ning,34(4):744754, 1986.\nNicola Orio andDiemo Schw arz. Alignment ofmono-\nphonic andpolyphonic music toascore. InProceedings\nofthe2001 International Computer Music Confer ence,\npages 155158, 2001.\nChristopher Raphael. Automatic segmentation ofacous-\nticmusical signals using hidden mark ovmodel. IEEE\nTransactions onPattern Analysis andMachine Intelli-\ngence,21(4), 1999.\nChristopher Raphael. Ahybrid graphical model foralign-\ningpolyphonic audio with musical scores. InISMIR\n2004: Proceedings oftheFifthInternational Confer -\nence onMusic Information Retrie val,2004.\nDiemo Schw arz. Data-Driven Concatenative Sound Syn-\nthesis .PhD thesis, Universit Paris6-Pierre etMarie\nCurie, 2004.\nFerr´eolSoulez, XavierRodet, andDiemo Schw arz. Im-\nproving polyphonic and poly-instrumental music to\nscore alignment. InISMIR 2003: Proceedings ofthe\nFourth International Confer ence onMusic Information\nRetrie val,pages 143148, Baltimore, 2003.\nTuukka Toivonen and Masanao Izumo. Timidity++,\n19952004. anOpenSource MIDI toWAVEcon-\nverter/player .\n229"
    },
    {
        "title": "Mining Music Reviews: Promising Preliminary Results.",
        "author": [
            "Xiao Hu 0001",
            "J. Stephen Downie",
            "Kris West",
            "Andreas F. Ehmann"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417067",
        "url": "https://doi.org/10.5281/zenodo.1417067",
        "ee": "https://zenodo.org/records/1417067/files/HuDWE05.pdf",
        "abstract": "In this paper we present a system for the automatic mining of information from music reviews. We demonstrate a system which has the ability to automatically classify reviews according to the genre of the music reviewed and to predict the simple one-to-five star rating assigned to the music by the reviewer. This experiment is the first step in the development of a system to automatically mine arbitrary bodies of text, such as weblogs (blogs) for musically relevant information.",
        "zenodo_id": 1417067,
        "dblp_key": "conf/ismir/HuDWE05",
        "keywords": [
            "automatic mining",
            "music reviews",
            "genre classification",
            "review rating prediction",
            "system development",
            "weblogs",
            "musically relevant information",
            "experiment",
            "first step",
            "arbitrary bodies of text"
        ],
        "content": "MINING MUSIC REVIEWS: PROMISING PRELIMINARY RESULTS\nXiao Hu J. Stephen Downie Kris West Andreas Ehmann \nGSLIS \nUniversity of Illinois \nat Urbana-Champaign  \nxiaohu@uiuc.edu  GSLIS \nUniversity of Illinois \nat Urbana-Champaign  \njdownie@uiuc.edu  School of Computing \nSciences, University \nof East Anglia \nkw@cmp.uea.ac.uk  Electrical Engineering \nUniversity of Illinois \nat Urbana-Champaign \naehmann@uiuc.edu  \nABSTRACT \nIn this paper we present a system for the automatic min-\ning of information from music reviews. We demonstrate a system which has the ability to automatically classify reviews according to the genr e of the music reviewed \nand to predict the simple one-to-five star rating assigned to the music by the reviewer. This experiment is the first step in the development of a system to automatically \nmine arbitrary bodies of text, such as weblogs (blogs) \nfor musically relevant information.  \nKeywords: music reviews, data mining, text classifica-\ntion, genre, and rating  \n1 INTRODUCTION \nMusic information retrieval (MIR) and music digital \nlibrary (MDL) systems require both content-based and metadata-based music information. In networked envi-ronments, ever-increasing numbers of users are coming together to help each othe r with their music seeking \ntasks. The sharing of online music reviews is one such sharing mechanism that operates in the metadata domain. \nDeveloping tools that can help users of MIR/MDL sys-\ntems acquire and use the wealth of music information embedded in online reviews is the goal of this pilot pro-ject. \nOnline customer reviews repr esent a rich resource for \nexamining the ways users of music describe their music preferences and the possible impacts of those prefer-\nences. Online reviews can be surprisingly detailed, cov-ering not only the reviewers’ personal opinions but also important background and contextual information about the music and musicians under discussion. In addition, \nthere is a large amount of review data online as most major online music stores (e.g., amazon.com ) provide \ncustomer reviews. There are also non-retail websites devoted to customer reviews (e.g., epinions.com ). \nThese sources of user-generated information provide us with an exploratory starting point for uncovering new mechanisms for leveraging the collective knowledge of the music-listening public. In this work, we use customer reviews of music CDs \npublished on www.epinions.com , a website devoted \nto online customer reviews of products available on the Web. This site was chosen because it contains a very large collection of music re views organized into a com-\nprehensive and detailed genre classification taxonomy. There are 28 major classes including Classical, Rock, \nPop, Jazz, Blues, Internati onal World music, etc. Under \nmost classes in this taxonomy, there are subclasses cate-gorized by various criteria (e .g., style, composer, etc.). \nFor example, Classical music is divided according to the \nperiod in which the music was produced, including the \nRenaissance, Medieval, Classical, Baroque, Romantic and 20th Century periods. Each review is associated with both a genre and a numerical rating expressed as number of stars (from 1 to 5) , with higher ratings indi-\ncating more positive opinions. \n2 BACKGROUND AND MOTIVATION \n2.1 Music Information User Studies \nIn recent years, research on us er issues in music retrieval \nhas attracted growing attention. [1] employed qualitative \nethnographic methods to study music seeking behaviors \nin public libraries and music stores. Using a combination of interviews, focus groups and observations, they col-lected detailed data regarding user behaviors and the users’ underlying motivations  and goals. However, due \nto the time-consuming nature of such qualitative ethno-\ngraphic methods, the Cunningham et al. study could not scale up; only seven subjects were intensely interviewed. Another user study in MIR applied survey methods to reach a larger group of users [3]. However, because sur-vey methods have to use que stions general enough to be \nminimally appropriate for all respondents, it is possible to miss what is most appropriate to many respondents. Further, survey designs (i.e., the tools and administration of the tools) have to remain unchanged throughout the data collection process, and thus cannot collect informa-tion about newly emergent categories previously un-known to the researchers. We believe a close examina-tion of user-generated reviews provides an opportunity to obtain the benefits of traditional ethnographic meth-ods (i.e., a detailed understanding of user expression via their own words) combined with the generalization abili-ties of well-constructed surveys. Furthermore, the appli-cation of automatic data mining techniques to the data analysis of the reviews allows for economies of scale unparalleled by qualitative methods. \nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full citation on the first page. \n© 2005 Queen Mary, University of London \n536   \n \n 2.2 Automatic Music Reviews \nWhitman and Ellis [8] recen tly attempted to automati-\ncally generate textual reviews from music audio signals. \nFor that purpose, they used music reviews to learn the \nconnections between the per ceptual audio features of \nmusic and textual terms in reviews. Whitman and Ellis also acknowledged that human description is a far richer \nsource than marketing tags in terms of describing music \ncontent. Notwithstanding our mutual interest in music reviews, it is clear that our work is quite different from theirs: our work uses the full review text while Whitman and Ellis’ focused on indivi dual terms (i.e., nouns and \nadjectives) related to audio features. Moreover, since they used music reviews to establish the ground truth of their text descriptions of audio music features, they pre-ferred “clean” music reviews which were “consistently concise, short and informative”. In our work, we intend to develop systems based upon all aspects of music in-\nformation use and users, and thus we need “natural” reviews from end users, which include comments on the music as well as the context and reasons for those com-ments. \n3 EXPERIMENTAL SETUP \nIn this section we describe the experimental setup used \nto examine the automatic cla ssification of reviews.   \n3.1 Data Collection \nFor each of the 12 genre classes used in our experiments, \nwe crawled and downloaded CD reviews listed on the first 30 pages of the epinions.com  product list. Each \nreview contains a title, author’s rating, a summary (ex-\npressed as “Pros”, “Cons” and “The Bottom Line”) and full review content. Figure 1 shows an example of a re-view. To simplify the process only the full review text and the rating were extracted from these documents. The title and summary are good resources to be exploited and we will do so in future work. \n3.2 Classification Schemes \nIn this paper we attempt to identify: #1 the genre of the \nmusic being reviewed (Experiment #1); and #2 the rating assigned to the music by the reviewer (Experiments #2a, #2b and #2c). The same preprocessing and modeling techniques are used in both of these classification ex-\nperiments. The genre of the music is not used as feature for the prediction of the rating, nor is the rating used as a feature to predict the genre to  ensure that the models are \nentirely based on features that  can be extracted from the \ntext. We have tested the cl assification of reviews accord-\ning to rating as a five class problem: classification into the individual ratings (1 star , 2 stars ... 5 stars) and bi-\nnary classification problems: classification into negative and positive reviews (1 or 2 stars against 4 or 5 stars) and ad extremis  (1 star against 5 stars).  \n \n \nFigure 1 . An example of a review on epinions.com \n3.3 Dataset \nThe dataset used to investig ate the automatic classifica-\ntion of reviews according to genre (Experiment #1) was \ncomposed of: \nz 12 Classes (Rock, Pop, J azz, Blues, Gospel, etc.) \nz 150 examples per class \nz A minimum of 3 kilobytes of text per review \nz Total 1800 examples \nThe dataset used to investigate the automatic classifi-\ncation of reviews by user-a ssigned ratings (Experiment \n#2) was composed of: \nz 5 Classes (1 star, 2 stars ... 5 stars) \nz 200 examples per class (400 in the binary tests) \nz A minimum of 3 kilobytes of text per review \nz Total 1000 examples (800 in the binary tests) \n3.3.1 Data Preprocessing \nThe first step in processing documents input to the sys-\ntem was to remove any residual HTML tags. The next step was to break the text down into terms and to remove all punctuations. The Porter stemming algorithm [6] was used so that different forms of the same word (e.g., plu-\nrals) would be recognized as the same term. The list of \nterms in each document was co llected together to pro-\nduce a global term list containing the frequency of each term in each document. The entire dataset was then rep-\nresented as a sparse document-term matrix.  \nThe sparse matrix produced by this process was then \nrandomly divided into test and training matrices, with 80% of the data used to train a model and the remaining 20% held back to test the model’s accuracy. \n3.4 Modeling \nThe sparse training matrix is used to train a Naive \nBayesian text classification model. Naive Bayes is a well-known probabilistic classi fication technique. Varia-\n537   \n \n tions of the technique have been widely used in text \ncategorization [4], [7] and [9]. As studies on multinomial mixture models have reported improved performance over multi-Bernoulli ones [4], in this paper, we have used a Naive Bayesian classifier based on a multinomial mixture model where values in document vectors are term frequencies (TF). \nWe can calculate the probability P(C\nj|di) that a \ndocument, di, belongs to a category, Cj, by applying \nBayes theorem, which states that: \n() ( )( )\n()ij i\nj i jdPCdPCP d CP|* | =                         (1)  \nwhere P(C j) is the prior probability of class j, P(d i|Cj) \nis the conditional probability of document i given class j \nand P(d i) is the prior probability of document i. \nThe estimation of P(d i|Cj) is problematic because al-\nmost all novel documents are different from training documents. By making the assumption that each term in a document is generated independently of the other terms in the document given the class label, Naive Bayes simplifies the estimation of P(d\ni|Cj) to estimating \nthe conditional probability of a term given a class: \n() ( ) ()()∏\n== =V\ntdwc\nj t j n j iitCwP Cw wwP CdP\n1,\n2 1 | | ,...,, |\n(2) \nwhere w1 ,w2 ,...,w n are terms occurring in document  \ndi, V is the vocabulary of terms occurring in the training \ndocument set, and  c(w t ,di) is the frequency count of \nterm wt in document di.  \nIn practice, the accumulati on of probabilities from all \nthe terms occurring in a document must be performed in the log domain (to prevent underflow) and smoothing is necessary to prevent zero probabilities for infrequently occurring terms [4]. We used Laplacian smoothing, one of the most widely used smoothing methods, to smooth \nthe probabilities in the log domain. \n3.5 Implementation \nThe experiments detailed here were implemented in the \nData-to-Knowledge Toolkit (D2K), the Text-to-Knowledge framework (T2K) and the General Architec-ture for Text Engineering (GATE). NCSA gives a thor-ough introduction to text mining in D2K/T2K [5]. \n4 RESULTS \nThe results achieved in each of  the tasks, detailed in sec-\ntion 3.2, are given in Table 1 and the confusion matrices for genre classification (Experiment #1) and full ratings classification (Experiment #2a) are given in Figures 2 and 3, respectively.  \n \n \nFigure 2 . Genre classification confusion matrix \n \nFigure 3 . Rating classification confusion matrix \nThe Experiment #1 results shown in Table 1 along \nwith the confusion matrix in Figure 2 show that the \nclassification of music revi ews, according to the genre \nof music reviewed, can be reliably performed. At 78.9% \nthis performance is significantly better than the random baseline, which is 8.3%.  \nThe Experiment #2a results for the prediction of the \nTable 1. Music review classification results \n Experiment Accuracy Std Dev Classes Term list size Average length Std Dev \n#1 Genre 78.89% 4.11% 12 47,864 terms 1,547 words 784 words\n#2a Rating (1 star, 2 stars … 5 stars) 44. 25% 2.63% 5 35,600 terms 1,875 words 913 words\n#2b Rating (Good vs. Bad, 1/2 stars vs. 4/5 star s) 81.25% N/A 2 33,084 terms 2,032 words 912 words\n#2c Rating (Good vs. Bad, 1 star vs. 5 stars)  86.25% N/A 2 32,563 terms 1,842 words 956 words\nResults for Experiment #1 and #2a were calcu lated with 3 random cross-validation tests \nResults for Experiment #2b and #2c we re calculated with a single iteration \n538   \n \n rating that accompanies a review initially look quite \npoor. However, the confusion matrix in Figure 3 shows that confusion is most likely to occur with the neighbor-ing classes, i.e., a 2 star revi ew is most likely to be con-\nfused with a 1 or 3 star review. Therefore, despite a relatively low overall accuracy  value, the system dem-\nonstrates the ability to di stinguish a positive review \nfrom a negative review. This contention is further sup-\nported by the results of the binary rating prediction tests (Experiments #2b and #2c), which show that the accu-rate estimation of ratings is possible (Table 1). \n5 CONCLUSION \nWe have demonstrated a proof-of-concept system that \ncan successfully mine online music reviews, by applying a Naive Bayesian classifier, to predict both the genre of \nthe music reviewed and the rating assigned to it by the reviewer. Both experiments were highly successful in terms of classification accu racy and the logical place-\nment of confusion in the confusion matrix. The experi-\nmental results show that the mining of music reviews is \na promising line of research, from which many user-related music features could be discovered. \n6 FUTURE WORK \nUser-generated reviews can provide both users and re-\nsearchers with music-related metadata in great quantity and detail. This exploratory study has examined a possi-ble approach to exploiting this resource. More powerful automatic data mining techniques and ethnographic con-tent analysis should be applied to more fully exploit the rich data available in user reviews. We intend to build \nupon the promise of our preliminary results by investi-\ngating the following possible applications: the recogni-tion of reviews within an arbitrary body of text, such as weblogs (blogs), the separati on of reviews of different \nmedia such as book, movie and music reviews, and the automatic classification and indexing of those reviews. \nThe subjects of many opinions expressed in the re-\nviews are nouns or noun phrases (e.g., “lyrics”, “mel-ody”), while most opinion words are adjectives (e.g., “awesome”, “crappy”). It is natural to hypothesize that \nnouns and noun phrases are salient features in genre classification while adjectives are important in rating classification. Research should be conducted into this hypothesis to reveal which parts-of-speech are impor-\ntant for each type of classification. \nOpinion feature mining (OPF) [2] is another possible \nmethod of exploiting the info rmation available in user-\ngenerated music reviews. OPF could be used to discover \nwhat features music users fre quently mention when they \nwrite reviews about music CDs and to rank those fea-tures according to the frequenc y with which they appear \nin the reviews. Those same f eatures are likely to be im-portant in the selection of new music, and thus the iden-tification of those features is important for the design of MIR/MDL systems that bette r serve the music informa-\ntion needs of their users. \nACKNOWLEDGEMENT \nThe Andrew W. Mellon Foundation is thanked for their \nfinancial support. This project is also supported by the National Science Foundation (NSF) under Grant Nos. NSF IIS-0340597 and NSF IIS-0327371. We also thank the content providers and the MIR/MDL community for their support. We also thank the Automated Learning Group at the National Center for Supercomputing Appli-\ncations at the University of Illinois at Urbana-\nChampaign. \nREFERENCES \n[1] Cunningham, S. J., Reeves, N., and Britland, M. ''An \nethnographic study of music information seeking: Implications for the design of a music digital library'', Proceedings of the third Joint Conference on Digital \nLibraries, Houston, USA, 2003. \n[2] Hu, M. and Liu, B. ''Mining opinion features in customer reviews'', Proceed ings of the 19th National \nConference on Artificial Intelligence, San Jose, USA, \n2004. \n[3] Lee, J. and Downie, J. S. ''Survey of music information needs, uses, and seeking behaviours: Preliminary findings'', Proceeding of the Fifth International Conference on Music Information Retrieval (ISMIR), Barcelona, Spain, 2004. \n[4] McCallum, A. and Nigam, K. ''A comparison of event models for naive bayes text classification'', Proceedings of the AAAI8 Workshop on Learning for Text Categorization, Palo Alto, USA, 1998. \n[5] NCSA. Online tutorial: Text mining: Email classification. Webpage, April 2005. http://algdocs.ncsa.uiuc.edu/TU-20031101-1.pdf. \n[6] Porter, M.F. An Algorith m for Suffix Stripping. \nProgram, 14, 3 (1980), 130 - 137. \n[7] Sebastiani, F. Machine learning in automated text categorization. ACM Computing Surveys, 34, 1 \n(2002), 1- 47. \n[8] Whitman, B. and Ellis, D. ''Automatic record \nreviews'', Proceeding of the Fifth International \nConference on Music Information Retrieval (ISMIR), Barcelona, Spain, 2004. \n[9] Yang, Y. An evaluation of statistical approaches to text categorization. Journal of Information Retrieval, 1, 1-2 (1999), 69 - 90. \n \n539"
    },
    {
        "title": "What You See Is What You Get: on Visualizing Music.",
        "author": [
            "Eric J. Isaacson"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415992",
        "url": "https://doi.org/10.5281/zenodo.1415992",
        "ee": "https://zenodo.org/records/1415992/files/Isaacson05.pdf",
        "abstract": "Though music is fundamentally an aural phenomenon, we often communicate about music through visual means. The paper examines a number of visualization techniques developed for music, focusing especially on those developed for music analysis by specialists in the field, but also looking at some less successful approaches. It is hoped that, by presenting them in this way, those in the MIR community will develop a greater awareness of the kinds of musical problems music scholars are concerned with, and might lend a hand toward addressing them Keywords: visualization, analysis, harmony 1",
        "zenodo_id": 1415992,
        "dblp_key": "conf/ismir/Isaacson05",
        "keywords": [
            "visualization",
            "analysis",
            "harmony",
            "MIR community",
            "visual means",
            "communication",
            "scholars",
            "problems",
            "addressing",
            "awareness"
        ],
        "content": "WHAT YOU SEE IS WHAT YOU GET: ON VISUALIZING MUSIC\nEric Isaacson\nIndiana University School of Music\nDepartment of Music Theory\nBloomington, IN 47405 USA\nisaacso@indiana.edu\nABSTRACT\nThough music is fundamentally an aural phenomenon, we\noften communicate about music through visual means.\nThe paper examines a number of visualization techniques\ndeveloped for music, focusing especially on those devel-\noped for music analysis by specialists in the ﬁeld, but also\nlooking at some less successful approaches. It is hoped\nthat, by presenting them in this way, those in the MIR\ncommunity will develop a greater awareness of the kinds\nof musical problems music scholars are concerned with,\nand might lend a hand toward addressing them\nKeywords: visualization, analysis, harmony\n1 INTRODUCTION\nThough music is fundamentally an aural phenomenon,\nwe very often communicate about music through visual\nmeans. A musical picture converts the unidirectional time\nof a piece of music into a spatially represented dimension.\nThis allows us to view a musical work as if it were a phys-\nical object–we can examine it in any order, at any pace,\ncomparing temporally detached events with a simple ﬂit\nof the eye.\nUsed in conjunction with a music-theoretically sound\nconcept of musical structure, pictures can be effective\ntools for both discovering and conveying musical infor-\nmation. Different methods allow us to view snapshots of\na musical work taken from different vantage points. No\ntool can provide a complete picture of a work, however,\nso researchers must keep in mind what their goals are, and\nwhat tools are most appropriate to achieve them.\nThis has implications for Music Information Retrieval\n(MIR), because musical visualizations are often direct re-\nﬂections of an underlying musical representation, and the\nchoice of a representation impacts directly on what mu-\nsical features can be searched. It is important, therefore,\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonthat researchers consider carefully the underlying repre-\nsentation and its outwardly expressed visualization when\nimplementing music IR tasks. Visual representations have\ngreat explanatory power. It is also important, however,\nthat they be properly grounded in both musical knowledge\nand an understanding of human cognition. When poorly\ndesigned, or based on an incorrect understanding of the\nunderlying musical structure, however, they can mislead.\nThe purpose of this paper is to consider some ways\nin which musicians and scholars have proposed we pic-\nture music and what role they might play in MIR tasks.\nThe paper will focus on visualization approaches that in-\nvolve music analysis, whose aim is to explain or illumi-\nnate characteristics of a musical work. The techniques dis-\ncussed touch on several musical features, including pitch\nand rhythm, form, texture, and structural hierarchies. The\nfocus is on Western music–and primarily on art music,\nthough the principles generalize readily to popular mu-\nsics. Some visualization techniques will be found to be\nless effective than others. Some of the techniques are\nalready technology-based, others are carried out strictly\nmanually. It is hoped that, by presenting them in this way,\nthose in the MIR community will develop a greater aware-\nness of the kinds of musical problems music scholars are\nconcerned with, and might lend a hand toward addressing\nthem.\n2 COMMON MUSIC NOTATION\nAlthough not itself analytical, because so much music\nanalysis is derived from music notation in some way, it\nis worth considering ﬁrst what Western notation does and\ndoes not represent. So-called Common Music Notation\n(CMN) traces its origins to the Middle Ages, with a num-\nber of important reﬁnements taking place in the Renais-\nsance, and additional incremental modiﬁcations occurring\nsince then. It developed to more easily preserve the exten-\nsive plainchant repertoire of the Roman Catholic church,\nwhich was previously carried on through oral tradition.\nThe primary purpose of CMN was thus, and in fact still\nremains, to facilitate the performance of a musical com-\nposition, by serving as a guide to performers. In much the\nsame way that the written word allowed languages to be\ncodiﬁed and for literature to emerge, the development of\nmusic notation facilitated the emergence of the concept of\na musical work–an artefact that could be reliably passed\n389Figure 1: Score reduction of Beethoven, Symphony No. 5, Mvt. 1, measures 1-24.\nFigure 2: Timeplot showing relative lengths of notated measures in a recorded performance of Beethoven’s Symphony\nNo. 5, Mvt. 1, measures 1-24.\nalong to later generations.\nOur notation system embodies a number of metaphors\nthat both reﬂect and shape how we understand music.\nPrimary among these is the notion that pitches are dis-\ncrete objects that exist in spatial relationships: one note\nis “higher” and “longer” than another. These basic\nspatial metaphors lead to others: scale degrees are ar-\nranged in “steps”; melodies “ascend” and “descend”; we\nspeak of “big sounds,” “thick textures,” of motives being\n“stretched” or “compressed”; we speak of not just “voice\nleading,” but “smooth” voice leading; we speak of “soft”\ndynamics, “hard” attacks, “harsh” dissonances, melodic\n“shape,” and musical “form.” (See Hatten, 1995, and\nZbikowski, 2002, for recent discussions of metaphor in\nmusic.)\nThe musical score horizontally represents the tempo-\nral order of events left-to-right. The vertical dimension\nexpresses multiple variables, particularly in a complex or-\nchestral score. Within a part, pitch height is determined\nby the height of a notehead on the staff (relative to the\nclef). Staves are grouped together in order of instrumen-\ntal range, ﬁrst within instrument families (e.g., piccolo,\nﬂutes, alto ﬂute) and then between instrument families\n(ﬂutes, oboes, clarinets, bassoons). Finally, the families\nthemselves are grouped together (woodwinds, brass, per-\ncussion, strings). The ordering is designed to optimize\nreading by the conductor. (The musical score itself was a\nlater development; many vocal works in the Renaissance\nexisted only as individual part books–one for soprano, one\nfor alto, and so on.)\nMusic notation reﬂects many centuries of accumulated\nuser feedback and collective wisdom. Properly interpret-\ning the intention of a score is more than a matter of read-\ning the notation, however. In addition to understanding\nthe symbols, there are a host of performance conventions\nthat affect the interpretation of those symbols. Some of\nthese include the addition of improvised ornamentation,determining when notated repeat signs are to be followed,\nknowing when the seventh note of the scale should be\nraised a half step in Renaissance polyphony, what tempo\nto play, and when and how much to deviate from the\nstrictly metronomic tempos.\nAlthough CMN is a remarkably adaptable system, it\nis largely optimized for performance. It is therefore in-\nsufﬁcient by itself for music analysis. It doesn’t show us\nharmonic analysis, motivic relations, musical form, etc.\nTo illustrate this with a single example, consider the prob-\nlem of depicting musical time. Though music is generally\nassumed to have a preferred tempo and note values seem\nto be deﬁned in a strictly hierarchical manner in which el-\nements at one level are grouped into twos or threes at the\nnext level. In practice, however, most music is anything\nbut regular.\nFigure 1 shows a score reduction of the ﬁrst 24 mea-\nsures of Beethoven’s ﬁfth symphony. Each measure con-\nsists of two beats which, in the absence of other factors,\nwould be expected to be roughly the same length. Fig-\nure 2 shows the timing in a recording of this passage with\nPierre Monteux conducting the London Symphony. Each\nblock represents one measure of notated music. (The tim-\ning points were set by manually clicking a button on each\ndownbeat and then carefully checking the placement and\nadjusting as needed.) Of course, the fermatas in the mea-\nsures shown in darker gray would be expected to be longer\nthan the others, but note that the measures preceding these\nare also longer than those in the fourteen measures in the\nmiddle that are uninterrupted by fermatas.\nAn MIR system needs to decide whether to focus on\nthe “musical” time as notated in the score or on the real\ntime reﬂected in the performance. In the case real-time\ninformation is desired, then another type of visualization\nmight be more appropriate. Similar decisions relating to\nthe representation of pitch, timbre, and other musical fea-\ntures must be made.\n390Figure 3: Spectrogram of a recorded excerpt of W.A. Mozart, Requiem K. 626, “Confutatis,” as performed by the chorus\nand orchestra of the Gulbenkian Foundation of Lisbon, Michael Corboz, Conductor. (Cogan, 1984)\n3 SEEING SOUND\nThe value we place on the musical score notwithstanding,\nwe experience music primarily through sound, usually in\nthe form of an intricate combination of complex wave-\nforms representing (potentially) dozens or even hundreds\nof different sound sources. An acoustic signal can be rep-\nresented visually with a spectrogram, which graphs time\n(x) vs. pitch frequency (often on a logarithmic scale on\ntheyaxis). Cogan (1984) devotes a book to the analysis\nof a wide range of pieces based on spectrograms taken of\nperformances of them. Figure 3, taken from that book,\nrepresents a section from Mozart’s Requiem. Numbered\nbands along the y-axis represent octave regions. The im-\nage depicts clearly the dramatic musical contrast between\nConfutatis maledictis, ﬂammis acribus addictis (“When\nthe accursed have been confounded And given over to\nthe bitter ﬂames,” sung by male voices and accompanied\nby brass and low strings) and Voca me cum benedictis\n(“Call me with the blessed,” sung by female voices). Co-\ngan observes that the high partials present in the Confu-\ntatis sections are due in part to the frequently occurring\nhigh-pitched [i] vowel. The spectrogram is particularly\nuseful for conveying the broad sonorous contrasts cre-\nated by changes in orchestration, in musical texture, and\ndynamics–more so than traditional notation.\nAlthough music in audio form is comparatively ubiq-\nuitous and, for computer-based applications, is easier to\nobtain than music in symbolic form, it has many limita-\ntions as a representation for analysis. Humans have the\n(remarkable) ability to recognize individual components\nin a sound source, including identifying speciﬁc instru-\nments, or instrument families, as well as melodic lines and\nrhythmic patterns within each, and to translate that infor-\nmation into a mental symbolic form that is more reminis-\ncent of the musical score than of a spectrogram. As those\ncarrying out research in polyphonic transcription know, it\nis exceedingly difﬁcult to extract this information from an\naudio signal, and hardly more visible in a picture of that\nsignal. In fact, except when spectral (i.e., timbral) infor-\nmation is speciﬁcally the focus, the visual “noise” that the\novertone structures add to the image masks much of the\ninformation that is traditionally of interest in music anal-\nFigure 4: Brinkman and Mesiti (1991) graphic rendition\nof score of Schoenberg, Drei Klavierst ¨ucke, Op. 11, No.\n1, measures 1-11. Annotations added.\nysis.\n4 SPECTROGRAM ANALOGS\nIt is possible to derive some of the same beneﬁts of the\nmusical spectrogram, but without the messiness of the\ntimbral information using symbolic data, which can easily\ndepict just fundamental pitches. Figure 4 is a “part plot”\n(Brinkman and Mesiti, 1991) of the ﬁrst eleven measures\nof Schoenberg’s Piano Piece, Op. 11, No. 1. As in the\nspectrogram, time proceeds left to right, though now by\nnotated time, not in real performance time, while notated\npitch follows the y axis as in a spectrogram. Pitches that\nbelong to the same musical voice and are not interrupted\nby rests are connected with vertical lines, so that melodic\ngestures can be readily seen. The format makes it easy\nto see recurrences of the same motive (A) or a variation\nof the opening melodic gesture (B), as well as where the\nmoving melodic lines occur in relation to the sustained\naccompanying chords.\nThis type of visualization has a number of potential\nbeneﬁts relative to standard notation. In a chamber or or-\nchestral score, for instance, it ﬂattens the contents of the\nvarious staves into a single coordinate system. (A sym-\nphonic score can easily include 15 staves or more.) It also\neliminates the visual clutter of staves, barlines, as well as\nnote heads and associated stems, ﬂags, and beams, allow-\ning one to focus on basic melodic shape. The tradeoff\nis that speciﬁc pitch and rhythmic/metric information is\nmissing, as is the timbral information present in a spectro-\ngram.\n391Figure 5: Video excerpt from Music Animation Machine\n(Malinowski, 2005) realization of J. S. Bach, “In dulci ju-\nbilo” from Das Orgelb ¨uchlein . V-shaped line added.\nA related type of notation is used in several anima-\ntions produced by Malinowski (2005). In Figure 5, time\nand pitch are plotted on the same axes as in Brinkman\nand Mesitis graphic, with two differences: pitches are not\nconnected with vertical lines to show larger gestures, and\nthe input is a MIDI ﬁle, which means the graph is based\non performance data rather than notated durations. (Of\ncourse Brinkman and Mesiti’s graphs could also use per-\nformance timing, and Malinowski’s MIDI ﬁles could be\ngenerated with strictly quantized data, so this distinction\nis not particularly meaningful.)\nMalinowski uses color effectively to represent differ-\nent parts of the four-part musical texture: the chorale\nmelody being played in the ﬁrst and third parts are in\ndarker colors. I have added lines to show how the lower\npart imitates the upper in this excerpt. The faster-moving\naccompanying parts are depicted with a lighter color.\n(White is used for the currently sounded notes.)\nThis type of line graph can be generalized to show fea-\ntures other than pitch on the yaxis. Another graph by\nBrinkman and Mesiti (Figure 6) shows, for instance, the\ndynamic levels notated in the score of the ﬁrst 24 mea-\nsures of Bart ´ok’s fourth string quartet, ﬁrst for each of the\nfour parts, and then in composite. When only a solid line\nis shown, it indicates that the instrument is not playing\nat that time. Those familiar with the opening of Bart ´ok’s\nquartet will recognize the characteristic dynamic contour\nof the movements opening gestures, as well as the loud-\nthen-soft contour of the passage as a whole.\n5 MUSICAL FORM AND TONALITY\nIt is common to represent musical form in a graphical for-\nmat. The purpose of a form diagram is to show the re-\ncurrence of previous themes and the introduction of new\nones. Relatively simple music can be diagrammed quite\nminimally. For instance, a large number of American pop-\nFigure 6: Brinkman and Mesiti (1991) plot of Bart ´ok’s\nString Quartet no. 4, mvt. 1, measures 1-24, mapping\ntime ( x) vs. notated dynamic level ( y).\nFigure 8: Wattenberg (2005) diagram showing repeated\nmusical fragments in an unspeciﬁed Mazurka in F# minor\nby Chopin.\nular songs (“Autumn Leaves,” “Over the Rainbow”) is in\nthe form aaba or a close variant.\nUltimately, the complexity of a formal diagram is\nbased on the complexity of the music and the desired level\nof granularity. Figure 7, for instance, shows a diagram of\na typical minuet and trio movement from a piano sonata.\nThematic similarities are depicted with both similar text\nlabels and similar colors. The large ABA structure depicts\nthe overall arrangement, minuet, trio, and shortened min-\nuet repeat. Internal bubbles reveal the essential bipartite\ndivision of each of those sections, while the ||: a : ||: b a : ||\nstructure of the A and B sections are apparent at the lowest\nlevel. Derived from a recorded performance, the sections\nin this diagram are shown proportional to real, not musical\ntime. (Numbers below the diagram are measure numbers.)\nFigure 8 represents musical recurrence in a rather dif-\nferent way. Designed by digital artist Martin Wattenberg,\nthe diagram uses arches to connect repetitions of musi-\ncal material. Whereas Figure 7 shows thematic repeti-\n392Figure 7: Formal diagram of Beethoven, Piano Sonata, Op 2, No. 1, Mvt. 3, produced using Variations2 timeline tool (\nhttp://variations2.music.indiana.edu/ ). Sectional proportions based on performance by Richard Goode.\nFigure 9: Craig Sapp’s Tonal Landscape (type 1 plot)\nof J. S. Bach, Well-Tempered Clavier , Book 1, Prelude\nin C-sharp minor. From http://ccrma.stanford.\nedu/∼craig/keyscape/ .\ntions only at the sectional level, this one connects repeated\nevents wherever they ﬁrst occur. The thicker the band, the\nmore extensive the material that is repeated. For exam-\nple, two large immediately repeated sections are appar-\nent in the ﬁrst quarter and the central half of the example,\nwhereas a number of short elements from the end of that\nﬁrst repetition recur at the end of the piece (see the se-\nries of tall, thin arches spanning most of the length of the\nﬁgure).\nThough the diagram is visually appealing (even more\nso in the translucent pastel blues in the color version), it\nfails as an effective depiction of musical design in several\nrespects. First, if the ﬁrst section is repeated (as suggested\nby the ﬁrst solid grey arch), then why do the materials at\nthe end of the piece refer to the repetition and not to their\nﬁrst instance? This leads to a strangely non-hierarchical\nview of the piece that is surely at odds with its structure\n(compare the orderliness of the previous Beethoven exam-\nple). Also, because the height of an arch is related only to\nthe distance of the events it connects, it gives a sense of\nimportance to repetitions that are far apart in the music\nthat may or may not be justiﬁed musically. An MIR sys-\ntem that contains (or automatically generates) form dia-\ngrams of musical pieces should support the formal model,\nnot the latter.\nA different type of musical structure is depicted in a\n“tonal landscape” by Sapp (2001). Figure 9 depicts the\ntonal structure of the C-sharp minor prelude from Bach’s\nWell-Tempered Clavier , Book 1. Again, time proceeds\nalong the xaxis, but in this case, as one moves downward\nFigure 10: Craig Sapp’s Tonal Landscape (type 2\nplot) of Mozart’s Viennese Sonatina No. 1 in C,\nMvt. 1. From http://ccrma.stanford.edu/\n∼craig/keyscape .\nfrom the top of the ﬁgure, each row divides the piece into\nn+1segments of equal length (the ﬁrst row 1, the second\n2, and so on). Each segment is then assigned a color based\non an estimate of the overall key that is characteristic for\nthat segment. By the bottom of the graph, key estimate\nare being made for very short segments of music. The es-\ntimated keys are displayed using a color scheme that maps\neach note around the circle of ﬁfths to adjacent colors in\nthe rainbow (E = red . . . C = green . . . A = violet).\nA second type of graph (Figure 10) also depicts in-\ncreasingly local key estimates as one moves from top to\nbottom. Rather than using n+ldiscrete segments for\nlayer l, the ﬁgure uses a continually sliding window that\ngrows smaller as it moves toward the musical surface at\nthe bottom. This ﬁgure has been further modiﬁed by the\napplication logarithmic scaling that squashes the top of\nthe image, allowing the features near the bottom to extend\nmore visibly toward the upper part of the image.\nSapp’s plots are intriguing in that they depict the oc-\n393casional ambiguity of tonal orientation experienced as one\nlistens to a piece. Several details of his approach are prob-\nlematic, however. One is that it is highly doubtful that\nwe perceive tonality on as many levels as suggested by\nthe diagrams. More critically, the perception of tonality is\nbound up closely with the perception of form. In partic-\nular, phrases usually begin and end stably in a key. Only\ncertain kinds of musical events trigger ambiguity of key\ncenter, and this ambiguity exists with nowhere near the\nfrequency implied by the diagrams. Third, though the im-\nages supposedly convey a sense of key distance, the use\nof colors to characterize this distance is of little help visu-\nally, for we do not generally conceive of colors as being a\ncertain distance from each other.\n6 OTHER MUSIC STRUCTURE VIEWS\nA ﬁnal pair of analytical approaches relies on music no-\ntation, albeit in untraditional ways. These approaches\nrequire a considerably more sophisticated conception of\nmusical syntax to understand properly.\nThe theories of Heinrich Schenker (1969) are among\nthe most commonly used in the analysis of tonal music.\nSchenker posits a hierarchical view of music that resem-\nbles the construction of buildings: upon a basic founda-\ntion common to all music is built an inner framework, to\nwhich is added wall and ﬂoor members, then paint and\ncarpeting, and ﬁnally the furnishings. (This oversimpliﬁ-\ncation will sufﬁce for present purposes.) Figure 11 depicts\none layer of Schenker’s analysis of a chorale by Bach.\nAs is characteristic of Schenker’s analyses, emphasis is\ngiven to the counterpoint between the outermost voices of\nthe music. In the layer shown in the ﬁgure, many of the\ndecorative pitches from the musical surface (passing and\nneighboring tones, for instance), along with the inner parts\n(alto and tenor) have been removed. Pitches that belong\nto deeper structural levels are shown with open noteheads\nand beamed together. Stemless noteheads are least sig-\nniﬁcant structurally and are generally considered decora-\ntive at this level. Schenker’s complete analysis includes\ntwo further stages of reduction, plus one version that is\nmore elaborated than the one shown here. The last stage\nof reduction shows only the notes in open noteheads here–\nthe foundation of the piece. The more elaborated version\nsimply resembles the actual score, but with the structural\npitches from this layer shown.\nThe ﬁnal analytical approach to be discussed is that\nproposed by Lerdahl and Jackendoff (1981). Based on\ntheories of generative linguistics, Lerdahl and Jakendoff\nuse various means to describe metrical organization in a\npiece, as well as grouping and prolongation structures.\nEach of these is depicted in Figure 12. Dots directly be-\nlow the music are used to show relative metrical strength\nof each beat. Brackets below that depict a hierarchy of\ngroupings of musical events. The tree diagram above the\nscore depicts either prolongations from one structure to\nthe same or a related structure, or progressions from one\nstructure to a different one, and these are further charac-\nterized as tensing or relaxing. The events linked to the\nhighest-level branches are considered the most important\nstructurally in the passage. Each symbol is determinedby well-formedness and preference rules, derived in spirit\nfrom generative linguistics.\nThe theories both of Schenker and of Lerdahl and\nJackendoff are highly sophisticated and, because they rely\non artistic interpretations on the part of the analyst, their\napplication in explaining musical works seem yet to be\nbeyond the capabilities of automated retrieval systems.\n7 CONCLUSIONS\nMany of the visualization techniques described here can\nreveal interesting, musically relevant, and at times highly\nsophisticated information about a musical work, informa-\ntion that would be hard to depict in another way. The ques-\ntions that led to these visualization techniques have been\nposed by music analysts who think deeply about musi-\ncal structure and musical meaning. They represent only\na small sampling of the rich literature that awaits discov-\nery by those in the MIR community who might wish to\naddress similar questions.\nREFERENCES\nA. Brinkman and M. Mesiti. Graphic modeling of musical\nstructure. Computers in Music Research , 3:1–42, 1991.\nR. Cogan. New Images of Musical Sound . Harvard Uni-\nversity Press, Cambridge, Mass., 1984.\nR. S. Hatten. Metaphor in music. In E. Tarasti, editor,\nMusical Signiﬁcation: Essays in the Semiotic Theory\nand Analysis of Music , number 121 in Approaches to\nSemiotics, pages 373–391. Mouton de Gruyter, Berlin,\n1995.\nF. Lerdahl and R. Jackendoff. A Generative Theory of\nTonal Music . MIT Press, Cambridge, Mass., 1981.\nS. Malinowski. Music animation machine, 2005.\nURL http://www.well.com/user/smalin/\nmam.html .\nC. Sapp. Harmonic visualizations of tonal music. In Pro-\nceedings , pages 423–430, Havana, Cuba, 2001. Inter-\nnational Computer Music Conference.\nH. Schenker. Five Graphical Music Analyses . Dover Pub-\nlications, New York, 1969.\nM. Wattenberg. The shape of song, 2005. URL http:\n//turbulence.org/Works/song/ .\nL. Zbikowski. Conceptualizing Music . Oxford University\nPress, New York, 2002.\n394Figure 11: Third (foreground) layer from Heinrich Schenker’s analysis of J. S. Bach’s setting of “Ich bin’s, ich sollte\nb¨ussen.” (Schenker, 1969).\nFigure 12: Excerpt from Lerdahl and Jackendoff’s diagram of Mozart, Symphony No. 40 in G minor, measures 1-22.\n(Lerdahl and Jackendoff (1981), p. 259).\n395"
    },
    {
        "title": "Tonal Similarity from Audio Using a Template Based Attractor Model.",
        "author": [
            "Özgür Izmirli"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416688",
        "url": "https://doi.org/10.5281/zenodo.1416688",
        "ee": "https://zenodo.org/records/1416688/files/Izmirli05.pdf",
        "abstract": "A model that calculates similarity of tonal evolution among pieces in an audio database is presented. The model employs a template based key finding algorithm. This algorithm is used in a sliding window fashion to obtain a sequence of tonal center estimates that delineate the trajectory of tonal evolution in tonal space. A chroma based representation is used to capture tonality information. Templates are formed from instrument sounds weighted according to pitch distribution profiles. For each window in the input audio, the chroma based representation is interpreted with respect to the precalculated templates that serve as attractor points in tonal space. This leads to a discretization in both time and tonal space making the output representation compact. Local and global variations in tempo are accounted for using dynamic time warping that employs a special type of music theoretical distance measure. Evaluation is given in two stages. The first is evaluation of the key finding model to assess its performance in key finding for raw audio input. The second is based on cross validation testing for pieces that have multiple performances in the database to determine the success of recall by distance.",
        "zenodo_id": 1416688,
        "dblp_key": "conf/ismir/Izmirli05",
        "keywords": [
            "template",
            "key finding",
            "sliding window",
            "tonal center",
            "tonal evolution",
            "tonal space",
            "chroma based",
            "templates",
            "attractor points",
            "tempo"
        ],
        "content": "TONAL SIMILARITY FROM AUDIO USING A TEMPLATE BASED \nATTRACTOR MODEL\nÖzgür İzmirli \nCenter for Arts and Technology \nConnecticut College \n270 Mohegan Ave. \nNew London CT, USA \noizm@conncoll.edu  \nABSTRACT \nA m odel that calcu lates sim ilarity of tonal evolution \namong pieces in an audio database is presented. The \nmodel employs a template based key  finding algorithm. \nThis algorithm is used i n a sl iding wi ndow fashi on to \nobtain a sequence of tonal center estim ates that delineate \nthe trajectory of tonal evolu tion in tonal space. A chrom a \nbased representation is used to capture tonality in forma-\ntion. Templates are form ed from  instrument sounds \nweighted according to pitch distribution profiles. For \neach window in the input audio, the chrom a based repre-\nsentation is interpreted with  respect to the precalculated \ntemplates that serve as attractor points in tonal space. \nThis leads to a discretizatio n in both time an d tonal \nspace making the output repr esentation com pact. Local \nand global vari ations i n tempo are account ed for usi ng \ndynam ic time warping that em ploys a speci al type of \nmusic theoretical distance m easure. Eval uation is given \nin two stages. The fi rst is eval uation of t he key  finding \nmodel to assess its perform ance i n key finding for raw \naudio input. The second i s based on cross val idation \ntesting for pieces that have m ultiple perform ances in the \ndatabase to determ ine the success of recall by distance.  \n  \nKeyw ords: Tonal  similarity, key finding, dy namic time \nwarping, tonal space.  \n1 INTRODUCTION \n \nIn the field of MIR, the im portance of tim e series repre-\nsentations i s wel l recogni zed si nce listeners can only \nexperience m usic through tim e. Recently, in this field,  \nmany methods dealing wi th similarity of time seri es have \nbeen either revi sited and rei nterpret ed or new ap-\nproaches have been proposed. These m ethods focus on \nfactors such as efficiency of the represent ation, al go-\nrithm com plexity and processi ng load. Sel ection of rep-\nresentative features and a resulting efficient and effec-tive represent ation are i mportant factors in model design. \nThis paper, introduces a m ethod for si milarity calcula-\ntion of an aspect  of m usic cogni tion: tonal evolution. \nThe represent ation used for t onal evol ution is a sequence \nof sy mbols that enabl es appl ication of fast and efficient \nstring processi ng al gorithms. Thi s can be vi ewed i n con-\ntrast to other m ethods deal ing wi th similarity that gener-\nally use ri ch feat ures and consequent ly have higher \nprocessi ng demands. The m ethod present ed here fi rst \nfinds a sequence of position estim ates in tonal space and \nthen uses th e tim e series to  calcu late sim ilarity by warp -\ning one sequence ont o anot her. \nThis paper expl ores t he probl em of si milarity from  a \ntonality standpoint. The m ethod utilizes a tem plate \nbased key finding model to estimate the posi tion in to-\nnal space at regular intervals throughout a piece. The \nsequence of symbols represent ing the tonal evol ution is \nused in sim ilarity calculati ons across pieces in a data-\nbase. In m usic, this kind of si milarity is underst ood as a \nmore abst ract and hi gh-level similarity when com pared \nto similarity o f more direct musical attributes such as \nrhythm or m elody. Nevert heless, in the context of West-\nern tonal m usic the induction of  tonality is central to the \ninterpret ation of m usic. The com positional process ad-\ndresses the interplay b etween  the elem ents of time and \npitch inducing the sense of tonality. A tonal center can \nbe defi ned as t he most stable pitch in a fragm ent of mu-\nsic sometimes also referred  to as th e tonic. To nality is \nubiquitous and m ost listeners m usically trained or un-\ntrained can identify th e most stab le pitch while listen ing \nto tonal music. Furt herm ore, t his process i s cont inuous \nand remains in action throughout  the listening experi -\nence. As a m usical work unfolds, the stable pitch m ight \nchange as a resul t of the music modulating from  one key  \ninto another. In simple term s, the mode of the musical \nscale together and the tonic signify the key of a piece. \nThe m ain key can also be vi ewed as the global key. Mu-\nsical wo rks in the tonal trad ition generally start and end \nwith the global key, m oving through m ultiple other keys \nthroughout the piece. On the other hand, a localized key \nestim ate can be viewed as an  estim ate of the tonal center \ngiven onl y a fragm ent of a larger musical work. In this \npaper, t onal evol ution is represent ed by  a sequence of \nsymbols obt ained by  the appl ication of a l ocalized key  \nfinding m odel on adjacent fragm ents of m usic.  Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies  bear this  notice and the full \ncitation on the first page. \n© 2005 Queen Mary , University  of London To estim ate the key from  an audio recording, one \nmight look at the beginning or end of the piece and de-\nvelop heuri stics to arrive at  a deci sion. A more compli-\ncated  problem is th e calcu lation of tonality ev olution \nover t ime whi ch has been addressed i n a limited num ber \n540   \n \n of works. This comes closer  to harmonic analysis where \nchords or at least tonal regions need to be identified as \nthe piece unfolds. The evolution of the tonal center characterizes the piece in an abstract and general way. \nSeveral levels that would be useful to music information retrieval can be identified. First, key finding would group an entire database into 12 or 24 classes (or major-\nminor). Second, identification of modulations would \ngive more information about pieces and lead to a further \ndivision of the database based on a more detailed repre-sentation that contains a sequence of keys. Third, and most useful level, would extract information regarding tonal evolution so as to be used in applications ranging \nfrom functional analysis to segmentation into musical sections and even to transcription. \nThe methods dealing with similarity that work di-\nrectly from audio generally use features with many di-\nmensions such as the 12 dimensional chromagram or Mel-frequency Cepstral Coefficients. The method pre-sented here uses an additional step to further reduce the dimensionality of the representation prior to similarity calculation. \nThe organization of the remainder of the paper is as \nfollows: Section 2 outlines related work in key finding, feature extraction from audio, similarity and time align-ment. Section 3 explains the procedure for obtaining templates which represent attractor points in tonal space. Section 4 discusses the determination of localized key estimates which are found with respect to the tem-\nplates. The result of this stage is a sequence of symbols \nrepresenting a trajectory in tonal space. Section 5 ex-\nplains the alignment process that deals with tempo dif-ferences between the pieces being compared. An evaluation of the method is given in Section 6. \n2 RELATED WORK \nIn this section an outline of related work in several areas \nis given. A chroma based representation is a compact form of spectral representation obtained by a many-to-one mapping from the short-time spectrum of audio. Chroma based representations have been used in key finding (İzmirli 2005; G όmez and Herrera 2004; Pauws \n2004), discovering similarity  and repetition in audio \nrecordings (Bartsch and Wakefield, 2001) and chord \nsegmentation, recognition and alignment in audio (Sheh and Ellis, 2003). Fujishima (1999) originally proposed the Pitch Class Profile (PCP) for use in chord recogni-tion. This chroma based spectral representation is widely used because it eff ectively summarizes chroma \ninformation and harmonic structure in the spectrum us-ing a manageable number of dimensions. However, the mapping is not unique, octave information is ambiguous and fine spectral detail is lost as a result of this map-ping. \nMany approaches to extr acting tonal center informa-\ntion have been reported in the literature. Leman (1992) \nproposed a method inspired by cognition that uses an ear model front-end for determination of tonal context and tone centers. \nİzmirli and Bilgen (1996) reported on a model that has a pitch-class note recognition front-end \nfollowed by a stage that consists of leaky integrators to model recency effects and decay. In this model, as mu-sical events are encountered, leaky integrators are \ncharged according to respective strengths of pitch even\nts. Huron and Parncutt (1993) use a psychoacoustic \nmodel of pitch perception that employs echoic memory \nand pitch salience to model key perception. Chuan and Chew's model (2005) estimates pitch strength using peaks in the spectrum which ar e then used by the Spiral \nArray model to estimate key. \nPurwins, Blankertz and \nObermayer (2001) proposed a model for tonal center and modulation tracking which collapses the spectrum into constant Q (CQ) profiles and calculates distances using a fuzzy distance measure between the profiles and reference CQ sets. G όmez and Herrera (2004) presented \na comparison of cognition-inspired models based on \nKrumhansl's method and feature-based machine learn-\ning methods for key finding from polyphonic audio. One of the features they use is the Harmonic Pitch Class Profile which is a specialized version of PCP that uses the peaks in the spectrum. Pauws' model (2004) uses an auditory perception inspired front-end to compute a chromagram which is then us ed to compute the correla-\ntions with the Krumhansl and Kessler profiles (1982). Zhu, Kankanhalli and Gao (2005) first find the tuning frequency of the input, perform partial tracking, apply consonance filtering, obtain a pitch profile, and deter-mine the scale root and key separately.  \nSimilarity within a single audio recording has been \nsubject to much research. Finding thumbnails or repeat-\ning sections are of interest  for systems that perform \nautomatic summarization. Dannenberg and Hu (2002) describe and compare three methods that find repetition \nof segments within musical pieces. Cooper and Foote (2002) describe a method to determine the most repre-\nsentative segment in a piece by maximizing the average \nsegment similarity over the piece. Bartsch and Wake-field (2001) perform similarity analysis on chroma based representations of audio to identify chorus sec-tions. İzmirli (2002) uses spectra of diatonic collections, \nas references, to calculate tonal context vectors indicat-\ning relative strengths of tonal centers which in turn are used to calculate similarity of tonal evolution in frag-ments within and across audio recordings in a database.   \nWork related to processing of time series information \ngenerally deals with time alignment, segmentation and \nsequence recognition. Hu, Dannenberg and Tzanetakis (2003) describe a method to align polyphonic audio to symbolic score information. They use a chroma based representation and align the chroma vectors obtained from the query of the polyphonic input to those obtained \nfrom symbolic information. Work by Sheh and Ellis \n(2003) demonstrates chord recognition from music re-\ncordings. They use an HMM model for sequence recog-nition and report that PCP features are more effective than cepstral coefficients. Although the octave is usually divided into 12 they use a higher resolution PCP by dividing the octave into 24. Yoshioka et al. (2004) re-\n541   \n \nport on a system for chord recogni tion that simultane-\nously detects chords and chord boundari es in the input \naudio. Adam s et al. (2004) descri be dy namic alignment \nprocedures for vari ous t ime seri es represent ations of \nsung queri es. \n3 TEMPLATES \nA template based key  finding m odel is descri bed i n Iz-\nmirli (2005). Thi s model uses short  fragm ents from  the \nbeginnings of pol yphoni c audi o recordi ngs that contain \nclassical music including sy mphoni c, vocal , sol o and \nensem ble recordi ngs. The m odel has been found t o pro-\nduce 86% correct  labelling of t he key  using a database \nof 85 recordi ngs. In the mentioned work, vari ous spec-\ntral represent ations and profiles are com pared with one \nanother. The m odel operat es on the assum ption that a \npiece starts in the key that appears in its label designated \nby the composer. Given the viability o f the model, here, \nwe choose to utilize it in a sliding window fashion to \nestim ate the position in tonal space at a given tim e in the \npiece. The m odel that results  in the best perform ance \nwill be describ ed here. Th is will co nstitute the basis fo r \nthe estim ation of position in tonal space. In this paper \nhowever, t he model is used wi th a di fferent  param eter \nselection to make it suitable for t he current  purpose. \nPitch distribution profi les may be used t o represent  \ntonal hierarchi es in music. Krum hansl  (1990) suggest ed \nthat tonal hierarchi es for W estern t onal music coul d be \nrepresent ed by the probe t one profi les found experi men-\ntally in an earl ier study (Krum hansl and Kessl er, 1982). \nHer m ethod of key  finding is based on t he assum ption \nthat a p attern matching mechanism between  the tonal \nhierarchi es and t he distribution of pi tches i n a m usical \npiece model the way listeners arrive at a sense of key. \nMany key finding m odels rely on this assum ption and \nseveral  extensions have been proposed. In one such \nextension, besi de ot her addi tions, Tem perley (2001) has \nproposed a pitch distribution profile. W e utilize this \nprofile in  combination with  a diatonic profile as this \ncombination resu lts in  the best performance. Profiles are \nincorporated into the calcu lation of tem plates to  ap-\nproxi mate the distribution of pi tches in the spect rum and \nthe resul ting chrom a represent ation. The base profi le for \na reference key (A in this case) has 12 elem ents, repre-\nsents weights of individual chrom a values and i s used t o \nmodel pitch distribution for t hat key . Given t hat this \ndistribution is inva riant under transposition,  the profiles \nfor all other key s are obt ained by  rotating this base pro-\nfile.  \nTemplates are obt ained usi ng recordi ngs from  mono-\nphoni c instrument sounds. These sounds, for exam ple,  \ncould be piano sounds from  the McGill Master Sam ples  \nor from the University of Iowa M usical Instrum ent \nSamples. Tem plates represent a prototype spectrum  ac-\ncordi ng to a di stribution det ermined by the chosen pro-\nfile. The sounds are low pass filtered and then sam pled \nat 5512.5 Hz. The anal ysis is carri ed out  using 50% \noverl apping 2048-poi nt FFTs wi th a Hann wi ndow. \nAnal ysis frequency  range i s taken t o be from  50Hz t o 2000 Hz.  The spect rum of an individual monophoni c \nsound wi th index i , Xi, is com puted by averaging win-\ndows t hat have si gnificant energy  over the durat ion of \neach sound and then scaling the average spectrum  by its \nmean value. Here, i=0  refers to  the note A in  the lowest \noctave, i=1 refers t o Bb a sem itone hi gher et c. R is the \ntotal n umber of notes with in the instrument’s pitch \nrange used in the calcu lation of the tem plates.  \n \n \n \nChrom a Diatonic \nMajo r \nDMDiatonic \nMinor \nDmTemperley \nMajo r \nTMTemperley \nMinor \nTm\n0 1 1 5.0 5.0 \n1 0 0 2.0 2.0 \n2 1 1 3.5 3.5 \n3 0 1 2.0 4.5 \n4 1 0 4.5 2.0 \n5 1 1 4.0 4.0 \n6 0 0 2.0 2.0 \n7 1 1 4.5 4.5 \n8 0 1 2.0 3.5 \n9 1 0 3.5 2.0 \n10 0 0 1.5 1.5 \n11 1 1 4.0 4.0 \n \nTable 1 . Two profi les used i n this study: major \nand m inor profi les for Tem perley and di atonic. \n \nUsing the spectra obtained for each individual note, \ntemplates are calcu lated  by weig hted sums. A tem plate \nfor a certain mode and chrom a value is the sum  of X i \nweighted by the profi le element that has t he correspond-\ning chrom a value. A tem plate is calculated for each \nmode-chroma pair resulting in a total of 24 templates as \ngiven in equat ion (1). The fi rst 12 are m ajor, st arting \nfrom  reference chrom a ‘A’, and last 12 are m inor.   \n⎪⎪⎪⎪⎪\n⎩⎪⎪⎪⎪⎪\n⎨⎧\n≤≤⎥⎥\n⎦⎤\n⎢⎢\n⎣⎡\n+− Ψ≤≤⎥⎥\n⎦⎤\n⎢⎢\n⎣⎡\n+− Ψ\n=\n∑∑\n−\n=−\n=\n23 12)12 mod)24 ((11 0)12 mod)12 ((\n1\n01\n0\nn ifni PXn ifni PX\nC\nR\nimiR\niMi\nn  (1) \nXi denot es the averaged am plitude spect ra of t he sound \ncorrespondi ng to note i. Pe(k) is th e profile weig ht as \ngiven in Table 1, where e denot es the mode (M :major or \nm:minor) and k denot es the chrom a. In this work, the \nprofi le is given by  the product  of the diatonic and Tem -\nperley profi les: Pe(k)=D e(k)T e(k). Ψ is a funct ion that \nmaps the spect rum into chrom a bins. The m apping is \nperform ed by  dividing the anal ysis frequency  range i nto \n1/12th octave regions wi th respect  to the reference \nA=440 Hz. Each chrom a element in the template is \nfound by  a sum mation of t he wei ghted m agnitudes of the \nFFT bi ns over al l regi ons t hat have t he sam e chrom a \nvalue. \n \n542   \n \n4 ESTIMATION OF POSITION IN \nTONAL SPACE \nTemplates can be viewed as attractor or focal points in \ntonal space that represent th e ideal locations of tonal \ncenters. Once the profiles and scales are chosen and \ntemplates are fo rmed, they become part of the model to \nwhich incoming information is co mpared. Su mmary \nvectors are obt ained from  the raw audio input using the \nsame method t o obtain the templates with two excep-\ntions: the first is that each sum mary vector is obtained \nfrom  a wi ndow of fi xed durat ion (i nstead of the entire \nsound) as t he window i s slid through t he entire audio \ninput. Note that this window spans a m uch larger dura-\ntion com pared t o the FFT wi ndow. In t his work a wi n-\ndow size of 2.5 seconds has been utilized. This has been \ndetermined experi mentally to balance the averaging \nover time and sl uggishness of t he est imation. A l onger \nwindow covers m ore not es and t ends t o be m ore stable \nin the estim ation whereas a shorter window will make \nthe estimation m ore adapt ive. The wi ndow is noncausal  \nand has a t ime regi stration poi nt at the center. The hop \nsize is 35 percent  of wi ndow l ength. The second differ-\nence is that at each hop a new sum mary vector is calcu-\nlated  and compared to the tem plates as d escrib ed below.  \nFor each window the position in tonal space is esti-\nmated by computing correl ation coeffi cients between \nthe summary vector and all 2 4 spectral templates, and \npicking the one wi th the maximum value. The index of \nthe template with the maximum correlatio n is th en re-\ncorded for that time step. For the entire audio, a se-\nquence of indices which fal l in the range of 1-24 are \ncalculated and recorded. This  resul ts in a sequence rep-\nresent ed by  Sk=(s 1,k,s2,k,s3,k,...s N,k) where s n,k represents \nthe mode and chrom a value (tonic) estim ate for the n’th \nwindow i n audi o file k.  \n5 ALIGNMENT \nThe extracted sequence of indices S k, represents a sam -\npling of the tonal evolution in both tim e and tonal space. \nThe resul ting di scret e represent ation can be used t o effi-\nciently compare the similarity o f tonal evolution between  \npieces. Given two perform ances of the sam e piece, direct \nsimilarity co mpariso n using, for exam ple, the Euclidean \ndistance is not possi ble due t o tempo di fferences i n the \nperform ances that lead to misalig nment of the two  se-\nquences. For this reason, Euclidean distance and similar \ndistance m easures t hat do not  allow warpi ng of the \nsource sequence toward the target sequence fail to serve \nas vi able indicators of si milarity. W e therefo re use dy-\nnamic tim e warping to reduce the effects of local tempo \ndifferences between perform ances.     \nAs expl ained above, t he durat ion of anal ysis used to  \ndetermine elements of S i s on t he order of seconds. If \nthe model had been operat ing wi th a short er window, \nsay at the not e level, then groupi ng woul d have been \nnecessary , for exam ple, to convert  arpeggi os into chords \nor obt ain rom an num eral anal ysis from  fixed size time \nspans. At this level of anal ysis each sym bol is calculated from  a window that spans a sizeable duration which \nperform s the necessary averag ing. Therefore, we can \nuse a m ethod t hat assum es monotonic unfol ding of both \nsequences t o find an opt imal warpi ng pat h and a resul t-\ning di stance. \nThe sequence S consists of elem ents that represent \nmode and t onic inform ation. Thi s means t hat the dy-\nnamic time warpi ng al gorithm cannot  use a geom etric \ndistance m easure directly on the values them selves. As \nsuch, an absol ute value of t he difference coul d not  be \nused due to the m ore com plicated distance relationships \nbetween the indices. For exam ple, index 4 represents C \nmajor and index 5 represents Db m ajor (or C# m ajor). \nAlthough the num erical difference bet ween t hese t wo \nkeys is 1 the distance in tonal space should be one of the \nmaximal distances. Even using a sim ple circle-of-fifths \ndistance Db shoul d be 5 st eps away  while F woul d be 1 \nstep away from  C. To perform  the dynam ic tim e warp-\ning a distance m easure needs t o be defi ned t hat models \ndistances in tonal space.  Lerdahl’s regional space \n(2001) is a tonal space in which these distances can be \ncalculated. The regional space is created by com bining \nthe circle-of-fifths with the parallel and relative m ajor-\nminor cycle. Lerdahl  defines t his general ized t onal pitch \nspace to calculate distances  between chords when \npitches are either chosen from  a single diatonic collec-\ntion or from  a different  one as a resul t of a shi ft in the \ndiatonic set. In this work we approxi mate the tonal dis-\ntance between elem ents in the sequence S using Ler-\ndahl’s regi onal distance. Tabl e 2 shows t he distances in \ntabular form  as gi ven i n (Lerdahl , 2001). Readers are \nreferred to the original source for the geom etrical repre-\nsentation and history of tonal pitch space. \n \n \nRegion Distance Region Distance \n1 0 13 7 \n2 23 14 23 \n3 14 15 10 \n4 14 16 21 \n5 16 17 9 \n6 7 18 14 \n7 30 19 21 \n8 7 20 14 \n9 16 21 23 \n10 14 22 7 \n11 14 23 21 \n12 23 24 16 \n \nTable 2. Lerdahl ’s regi onal distances. R egions are \ngiven i n semitones wi th respect  to region 1.  \n \nGiven t wo sequences Sk and S m we fi nd the warpi ng \npath R=(r1, r2, r3,…r N) with N bei ng the length of t he \npath and r n=(i,j) holding the associ ation bet ween el e-\nment i in sequence S k and elem ent j in sequence S m. \nDynamic time warpi ng is implemented usi ng the recur-\n \n543   \n \nsion gi ven i n Equat ion 2. The convent ional path con-\nstraint that chooses bet ween a si ngle step of t he diago-\nnal, vertical o r horizontal m oves was in itially tested . \nThis led to many successi ve vert ical or hori zontal \nmoves in the optimal path when the sequences were \nuncorrel ated. Therefore, anot her pat h const raint was \nused t o prevent  two non-di agonal  moves t o occur i n \nsequence. \n⎪⎩⎪⎨⎧\n−+−−−+−−−−\n+=\n)j,1i(d)1j,2i(D)1j,i(d)2j,1i(D)1j,1i(D\nmin)j,i(d)j,i(D         (2) \nD is the global distance up to the point in the recur-\nsion, with D(1,1)=d(1,1) as the initial condition. d(i,j) is \nLerdahl’s distance as given in Table 2 between elem ent \ni in sequence S k and elem ent j in sequence S m. After the \ndynamic program ming algorithm  is run, the minimum \nglobal distance found is divided by  the length of the \ntrace-back path to elim inate the dependence on duration \nof the recordings. Although the lo cal constraint given in \nEquation 2 can be interprete d as a global constraint  that \nprevents som e points in th e grid from  being reached, \nthis constraint does not lead to any speed-up until ex-\nplicitly stated as a global constraint and im plem ented in \nthe recursion. We therefore use an Itakura parallelogram  \nto prevent extrem e warping and to attain speedup.    \n6 EVALUATION \nThe key  finding model descri bed in this paper scored \n86 percent correct labelling on a set of 85 general poly -\nphonic audio files containing short fragm ents of classi-\ncal m usic recordings. The fragm ents were taken from  \nthe beginnings of audio rec ordings that contained sy m-\nphonic, vocal, solo and ense mble music. A single win-\ndow of duration 7.5 seconds was used. The works in the \ndatabase collection were c hosen to approxim ately have \nuniform  distribution across th e 24 keys. The key  label-\nling in the titles of the pieces were used as ground truth. \nThe results showed that the output of the key  finding \nmodel was able to produce a good estim ate of the key  in \na fragm ent of audio and could be used as a front-end to \nhigher level processing. \nThis database was com posed of the beginnings of the \npieces and did not contai n perform ances of the sam e \npiece by different perform ers. Each file contained ap-\nproxim ately 1 m inute into the piece. Next, different per-\nform ances of 5 pieces that were already in the database \nwere recorded and added to th e collection. All 5 queries \nusing the new pieces returned the correct files as being \nmost sim ilar.  \nNext another database of 125 recordings of C hopin \nMazurkas by  Vladim ir Ashkenazy , Ignaz Friedm an, \nArthur Rubinstein, Vladim ir Sofronitsky  and Jean-M arc \nLuisada were used (som e historical recordings were \nvery noisy .) Only  12 of th ese recordings were unique \nand the remaining were all played by m ultiple pianists. \nThe sliding window key  finding m odel and the dynamic \ntime warping algorithm  were tested using this set. Cross \nvalidation was applied by using each piece as a query and testing it against the rest of the database. For this, a \nsimilarity  matrix, M, was constructed based on the \nminimum cost path found by  the dy namic tim e warping \nalgorithm  between all pairs of recordings. The diagonal \nelem ents were not calculate d and were assigned large \nvalues to prevent self m atches. The most similar tonal \nevolution for a piece with i ndex k, was given by the \nindex of the m inimum elem ent in row k in m atrix M . \nThe first m easure of perform ance was the retrieval \naccuracy of the m ost sim ilar item  for a piece. If in re-\nsponse to a query, the piece with the m inimum distance \nto that query  had the sam e nam e then it was considered \na successful retrieval. The m easure was calculated as the \nsum of all successful recalls divided by  the total num ber \nof pieces that had m ultiple versions. For the 125 pieces \nthis m easure y ielded 88.8%. The second measure \nlooked for a successful retrieval in the first two sim ilar \npieces. That is, two pieces with the sm allest distances to \nthe query were checked to see if any one of them  was a \nsuccessful retrieval. Again, only pieces that had m ulti-\nple versions were considered. This measure yielded \n100%. A third m easure was used to understand the \noverall perform ance by  calculating the average ratio of \nall successful recalls in the top 5 to the possible success-\nful recalls. For exam ple, if  a piece had 4 different ver-\nsions and only  3 were retrieved in the first 5 then the \nratio would be 3/4. The ratios for all pieces were aver-\naged. This y ielded 92.5%.    \n7 CONCLUSIONS \nThe m ethod presented here has been shown to pro-\nduce an efficient representati on of tonal evolution in the \nform  of a tim e series. The tim e series is a one-\ndimensional sequence of sy mbols representing tonal \ncenters in tonal space at discrete points in tim e. A slid-\ning window version of a ke y finding m odel has been \nshown to work in this context with encouraging results. \nThe alignm ent of the sequences obtained from  re-\ncordings in the database are perform ed using dynamic \ntime warping with a tonal space distance measure. To \ndemonstrate the effectiveness of this representation in \nfinding a piece that has the m ost sim ilar tonal evolution \nin a database, a similarity matrix is constructed. Finding \nthe most sim ilar N pieces is a m atter of finding the N \nsmallest distances in a row of the sim ilarity m atrix. The \nresulting perform ance indicates that the method shows \npotential for use in M IR applications. \nThe key finding m odel is not m eant to be em ployed \nas a m usic analy sis tool such as a chord recognizer. It \nshould be regarded as a tractable approach to tonal evo-\nlution m odeling and one that captures the essential gist \nof tonal structure of a m usical work as it unfolds over \ntime. The m odel em ploys a structural approach to tonal-\nity by operating solely  on p itch inform ation, disregard-\ning note order and only  indi rectly  using inform ation \nrelating to tim e structure of the input. For future work, \nincorporating tim e structure will be considered. Of par-\nticular interest is finding the longest com mon sequences \nwithin elem ents in the database. This will serve to per-\n \n544   \n \nform  intelligent segm entations and find repetitions. An-\nother direction will be to adapt the m ethod to use a rela-\ntive representation to account for transpositions. \n \nREFERENCES \nAdam s, N. H., B artsch, M .A., Shifrin, J. B. and  \nWakefield, G. H. (2004) Tim e Series Alignm ent For \nMusic Inform ation R etrieval. Proceedings of the Fifth \nInternational Conference on Musi c Informat ion \nRetrieva l, Barcelona, Spain. \nBartsch, M . A. and Wakefield, G. H. (2001) To Catch a \nChorus: Using C hrom a-base d R epresentations for \nAudio. Proceedings of the IEEE Workshop on \nAppl ications of  Signal Processi ng t o Audi o and \nAcoust ics, New Paltz, NY. \nChing-Hua, C . and Elaine , C. (2005) Poly phonic Audio \nKey-Finding Using the Spiral Array  CEG Algorithm . \nProceedings of the International Conference on \nMultimedia and Expo  (ICME), Am sterdam , Netherlands, \nJuly 6-8. \nCooper M . and Foote, J. (2002) Autom atic Music \nSummarization via Sim ilarity Analysis. Proceedings of \nthe Third International C onference on Musi c \nInformation Retrieva l, Paris: IR CAM. \nDannenberg, R.B. and Hu, N. (2002) Pattern Discovery  \nTechniques for M usic Audio. Proceedings of the Third \nInternational Conference on Musi c Informat ion \nRetrieva l, Paris: IR CAM. \nFujishim a, T. (1999) Realtim e chord recognition of \nmusical sound: A sy stem using com mon lisp music. \nProceedings of the Inter national Computer Music \nConference , Beijing, C hina, 464–467. \nGόmez, E. and Herrera, P. (2004) Estim ating the \nTonality of Polyphonic Audio Files Cognitive versus \nMachine Learning M odelling Strategies. Proceedings of \nthe Fifth International Conf erence on Music Information \nRetrieva l, Barcelona, Spain. \nHu, N., Dannenberg, R.B. and Tzanetakis, G. (2003) \nPolyphonic Audio M atching and Alignm ent for M usic \nRetrieval. Proceedings of the IEEE Workshop on \nAppl ications of  Signal Processi ng t o Audi o and \nAcoust ics, New Paltz, NY, USA. \nHuron, D. and Parncutt R . (1993) An Improved Model \nof Tonality Perception Incorpora ting Pitch Salience and \nEchoic M emory. Psychomusi cology, 12(2), 154-171. \nLeman, M . (1992) Tonal C ontext by  Pattern Integration \nOver Tim e. In D. Baggi  (Ed.), Readi ngs i n Comput er-\nGenerat ed Musi c, Los Altim os, CA: IEEE Com puter \nSociety  Press. pp. 117-137. \nLerdahl, F. (2001) Tonal  Pitch Space , New York: \nOxford University  Press. İzmirli, Ö. and Bilgen, S. (1996) A M odel for Tonal \nContext Time Course C alculation from  Acoustical Input. \nJournal  of New Musi c Research,  Vol.25, No. 3, pp.276-\n288. \nİzmirli, Ö. (2002) Determ ination of Tonal Sim ilarity \nBased on Spectral Diatonic B ases. in electronic \nproceedings in the Informatics Rep ort Series, University \nof Edinburgh, II International Conference on M usic and \nArtificial Intelligence (ICM AI02), Septem ber 12-14, \nEdinburgh, United Kingdom . \nİzmirli, Ö. (2005) Tem plate B ased Key  Finding From  \nAudio. Proceedings of the International Computer \nMusic Conference  (ICMC), Barcelona, Spain. \nKrum hansl, C . and Kessler, E. (1982) Tracing the \ndynamic changes in percei ved tonal organization in a \nspatial representation of m usical keys. Psychol ogical \nReview , 89, 334–368. \nKrum hansl, C . (1990) Cognitive Foundat ions of  Musi cal \nPitch . Oxford University  Press, New York. \nPauws, S. (2004) M usical Key  Extraction from  Audio. \nProceedings of the Fifth International Conference on \nMusic In formation Retrieva l, Barcelona, Spain. \nPurwins, H., Blankertz, B. and Oberm ayer, K. (2001) \nConstant Q Profiles for Track ing M odulations in Audio \nData. Proceedings of the Inter national Computer Music \nConference , Havana, C uba. \nSheh, A. and Ellis, D. P. W . (2003) Chord Segm entation \nand Recognition using EM -Trained Hidden M arkov \nModels. Proceedings of the Fourth International \nConference on Music Information Retrieval , Baltim ore, \nMaryland. \nTemperley , D. (2001) The Cognition of  Basi c Musi cal \nStructures, Cambridge, M A: MIT Press. \nYoshioka, T., Kitahara, T., Ko matani, K., Ogata, T. and \nOkuno, H. G. (2004) Autom atic Chord Transcription \nwith Concurrent Recogn ition of Chord Sym bols and \nBoundaries. Proceedings of the Fifth International \nConference on Music Information Retrieval , Barcelona, \nSpain. \nZhu, Y., Kankanhalli, M . S. and Gao, S. (2005) Music \nKey Detection for M usical Audio. Proceedings of the \n11th International Multimed ia Modelling Conference , \nMelbourne, Australia. \n \n \n545"
    },
    {
        "title": "Continuous HMM and Its Enhancement for Singing/Humming Query Retrieval.",
        "author": [
            "Jyh-Shing Roger Jang",
            "Chao-Ling Hsu",
            "Hong-Ru Lee"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414842",
        "url": "https://doi.org/10.5281/zenodo.1414842",
        "ee": "https://zenodo.org/records/1414842/files/JangHL05.pdf",
        "abstract": "The use of HMM (Hidden Markov Models) for speech recognition has been successful for various applications in the past decades. However, the use of continuous HMM (CHMM) for melody recognition via acoustic input (MRAI for short), or the so-called query by singing/humming, has seldom been reported, partly due to the difference in acoustic characteristics between speech and singing/humming inputs. This paper will derive the formula of CHMM training for frame-based MRAI. In particular, we shall propose enhancement to CHMM and demonstrate that with the enhancement scheme, CHMM can compare favourably with DTW in both efficiency and effectiveness.",
        "zenodo_id": 1414842,
        "dblp_key": "conf/ismir/JangHL05",
        "keywords": [
            "HMM",
            "speech recognition",
            "melody recognition",
            "acoustic input",
            "query by singing/humming",
            "CHMM",
            "formula derivation",
            "enhancement scheme",
            "DTW",
            "efficiency and effectiveness"
        ],
        "content": "CONTINUOUS HMM AND ITS ENHANCEMENT FOR \nSINGING/HUMMING QUERY RETRIEVAL\nJyh-Shing Roger Jang Chao-Ling Hsu Hong-Ru Lee \nMultimedia Information Retrieval Laboratory \nComputer Science Department, National Tsing Hua University \nHsinchu, Taiwan \n{jang,leon,khair}@wayne.cs.nthu.edu.tw \nABSTRACT \nThe use of HMM (Hidden Markov Models) for speech \nrecognition has been successful for various applications in the past decades. However, the use of continuous HMM (CHMM) for melody recognition via acoustic input (MRAI for short), or the so-called query by sing-ing/humming, has seldom been reported, partly due to the difference in acoustic ch aracteristics between speech \nand singing/humming i nputs. This paper will derive the \nformula of CHMM training for frame-based MRAI. In \nparticular, we shall propose enhancement to CHMM and demonstrate that with th e enhancement scheme, CHMM \ncan compare favourably with DTW in both efficiency and effectiveness.  \nKeywords: HMM, Continuous HMM, Query by Sing-\ning/Humming, Melody Recognition via Acoustic Input \n(MRAI), Speech Recognition.  \n1 INTRODUCTION \nIt is well known that HMM (Hidden Markov Models) is \na standard and successful a pproach to speech recognition \nfor the past few decades. In particular, for large-vocabulary speaker-indepe ndent speech recognition \ntasks, CHMM (Continuous H MM) has been exceedingly \nsuccessful in creating practi cal real-word applications. \nHowever, the use of HMM for melody recognition via \nacoustic input (MRAI) has not been extensively reported. \nIn fact, most previous approaches to MRAI using HMM employ note-based method, in which the user needs to hum in “ta” or “da” in order to facilitate note segmenta-tion. The note-specific information is then used to build the DHMM (discrete HMM) of a given song (Shifrin et al., 2002; Meek & Birmingham, 2001; Liu & Li 2003). Shih et al. (Shih et al., 2002) proposed the use of CHMM for humming transcription and note segmenta-tion, but it is not used for frame-based MRAI directly. Needless to say, the most obvious drawback of the note-based approach rested in the requirement of humming in “ta” or “da”, which could greatly limit the presentation style of the user. On the other hand, if we allow the user to sing the lyrics, then the note segmentation will be error-prone due to the conti nuous variations in acoustic \nintensity. As a result, we have proposed a frame-based approach using DTW(Jang et al., 2001; Jang & Lee, 2001; Jang et al., 2004) to ach ieve high accuracy while \nallowing the user to use an y presentation style that \nhe/she feels most comfortable with. \nWhile DTW is highly effective in retrieving relevant \nsongs, it is computation intensive. An alternative is to use mathematical analysis to design a hierarchical filter-ing method (Jang & Lee, 2001). In this paper, we look into another direction that uses a frame-based CHMM for achieving a better balance between accuracy and \nefficiency.  Specifically, we shall derive the evaluation \nand training procedures for CHMM. Moreover, we shall propose an enhancement scheme to be used with CHMM for achieving both effici ency and effectiveness \nin MRAI. Experimental results demonstrate that the proposed method compares favourably with DTW, and scale up nicely when the size of the database increases.  \nIn order to promote the research of MRAI, we have \nput the song dataset and the singing voice corpus on the first user’s homepage for public access. As far as we know, this is also the first singing voice corpus for MRAI evaluation. \n2 CHMM FOR SINGING/HUMMING \nQUERIES \nIn our CHMM-based singing query retrieval system, each song (or theme) in the database is represented by a CHMM. We will describe the CHMM structure used in our system. We will also pr opose two approaches to the \nparameter identification of the CHMM. \n2.1 CHMM Structure \nAn intuitive way to model the probabilistic character-\nistics of the pitch of each song (or theme) is by a left-\nright CHMM where each note is represented by a state. If there are consecutive notes of the same pitch level, then they are represented by  a joined state in a CHMM. \nThe following is the CHMM for the song “10 little Indi-ans”: \nPermission to make digital or hard  copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the fullcitation on the first page. \n© 2005 Queen Mary, University of London \n546   \n \n \n \nFigure 1 . CHMM for “10 Little Indians” \n \nIn general, each state in  the CHMM model is charac-\nterized by a d-dimensional Gaussian probability den-\nsity function (PDF): \n[ ],2/) ( ) ( exp\n)2(1),,(1µ µ\nπµ −Σ−−\nΣ=Σ−x x xgT\nd\nwhere x is a d-dimensional observation vector, µ \nand Σ are the mean vector and covariance matrix of the \nPDF, respectively. For simplicity, we assume the obser-\nvation is the pitch of the query input, hence x is a sca-\nlar and the original d-dimensional Gaussian PDF re-\nduces to the following well-known 1-dimensional ver-\nsion: \n,21exp\n21),,(2\n2\n\n\n\n\n\n\n−− =σµ\nσπσµxxg  \nwhere x is a scalar observation, µ and 2σ are the \nmean and variance of the PDF, respectively. For a given \nobservation x, the value of ),,(2\nj jxgσµ  for state j is \nreferred to as the “state probability” of observation x in \nstate j, denoted by ),(jxps. In our implementation, \nwe use “the sum of log probabilities” instead of “the \nproduct of probabilities” to avoid round-off errors. We \nalso define a “transition probability” between state i \nand j, denoted by ),(jipt. For our left-right model, \npossible next moves for observation at state i are only \nstate i (self transition) or state 1+i  (next transition). \nTherefore we have the following constraints on transi-\ntion probabilities: \n\n\n\n∀=++=+><=∀≤≤\ni iip iipnnpijorijif jipji jip\nt tttt\n,1)1,( ),(1),(1 ,0),(,,1),( 0\n \nIn the above constraints, n is the number of states and \n1),(=nnpt  indicates that the only possible move \nfrom the last state is self-transition to itself. \n2.2 CHMM Evaluation \nGiven a CHMM (with corresponding parameters for \nstate and transition probabilities) and a sequence of ob-\nservations (pitch vector), we need to evaluate the model \nby finding the likelihood of the observations generated \nby the model. This is usually accomplished by dynamic programming.  More specifically, for a given observed pitch vector []mx xx x ,...,2 1=  of length m, and a \nCHMM represented by n states, we need to construct \nan nm× table H, where each entry of the table is \ncomputed by the following recurrent equation: \n \n,),1( ln )1,1(),( ln ),1(max),( ln),(\n\n− +−−+−+=\nj jp w j iHjjp wj iHjxp jiH\ntti s\n \nwhere w is a weighting factor for the transition prob-\nability. In other words, ),(jiH denotes the maximum \ncumulated log likelihood of the event that the first i \nobservations []ix xx ,...,2 1  are generated by the first j \nstates of the CHMM model. The initial conditions for the \nabove recurrent equation are: \n\n>−∞==\n1 ),1()1,( ln)1,1(1\njfor j Hxp Hs \nAnd the overall maximum likelihood of the observations \n[]mx xx x ,...,2 1=  generated by this model is equal to \n),( max\n~1jmH\nn j=. Once the maximum likelihood is found, \nwe can back track to find th e optimum path that assign \neach pitch point to a state. \n \nTherefore for a given observed pitch sequence \n[]mx xx x ,...,2 1= , we can apply the above formula to \nfind the likelihood of each song’s model. The model \nwith the maximum likelihood is then picked as the an-\nswer to the singing/humming query. For simplicity, we have two assumptions in this study: 1. The singing/humming query is from the beginning \nof the intended song. In other words, the first state is the “start state” and the first pitch point must be-\nlong to this state. \n2. A user is required to keep singing for 8 seconds. \nTherefore any state could be the “end state” which holds the last pitch point. \nThe case of “match beginning” of assumption 1 can \nbe relaxed by setting every stat e to be a “start state” to \naccommodate “match anywhere”.  It is also straightfor-\nward to change the recurrent equation and the initial conditions to take this case of “match anywhere” into \nconsideration. \nFigure 2 demonstrates a typical example of model \nevaluation, where the model is the song “10 Little Indi-ans” and the observation is a pitch vector derived from an 8-second clip of a real user’s singing. \n547   \n \n \n \nFigure 2 . A typical example of model evaluation, where \nthe model is “10 Little Indians”. (a) The original ob-\nserved pitch and the induced matched pitch. (b) The H table shown as a 3D surface. (c) The H table (shown as an image) and the corresponding optimum path (shown as white dots.)   \nBefore model evaluation, we should take key transposi-tion into consideration since the user may not sing or hum in the same key level as that of the intended song in the database. This issue will be addressed in subsec-tion 4.2.  Obviously the retrieval results are greatly influenced by the parameters of the model. In the next subsection, we will explain how the optimum parameters can be de-rived.   \n2.3 CHMM Training \n \nBefore we can perform model evaluation, we need to \nidentify the optimum parameters for a CHMM. This can be achieved based on the availability of either music scores or clips of users’ inputs. We shall refer to these two methods as “score-based training” and “corpus-based training”. \n \nScore-based training \n Each song in the database is actually a monophonic \nsequence of music notes, where each note is represented by its pitch (in semitone) and duration (in second). For a given music score of this format, we can find the corre-sponding CHMM according to the following steps: 1. Basically, each state in  the CHMM represents a \nmusic note (or several consecutive notes of the same pitch level) in the song. The \nµ of each state \nis equal to the note’s pitch level in semitone. \n2. The 2σ of each state is set to 1 subjectively. \n3. Let k equal to the pitch points within this state. \nSince only the last pitch point of this state will tran-\nsit to the next state, th erefore the transition prob-ability )1,(+iipt is equal to k1. Hence ),(iipt  \nis set to )1,( 1+− iipt . For the last note, we have \n1),(=nnpt  and 0)1,(=+nnpt . For instance, \nin our implementation, sampling rate = 8000 Hz, \nframe size = 256, overlap = 0, then for a state rep-\nresenting a note of duration 0.5 second, the corre-\nsponding value of k is \n160 25680005.0 5.0 ≈−×=−×overlap size framerate sampling \nIn our system, the actual music scores are in MIDI \nformat and they could be polyphonic. However, for the \npurpose of singing/humming retrieval, we only consider \nthe melody or vocal track of a MIDI file for constructing the corresponding CHMM. \nThe above procedure for “score-based training” is \nquite intuitive. Only the value of 2σ is set in a subjec-\ntive manner; all the other parameters are set according to the song’s characteristics. However, it does not reflect the characteristics of reco rdings from users’ sing-\ning/humming. Therefore we have developed “corpus-based training” that can further tune the model to achieve better performance. \n \nCorpus-based training \n \nIn order to proceed with corpus-based training of a \nmodel, we assume there is at least one recording of the \ncorresponding song. The procedure of “corpus-based training” for a given model involves the following re-estimation procedures: 1. Use the result of “score-based training” as the initial \nparameters of the CHMM. \n2. For each recording of the song, perform model \nevaluation and keep the optimum alignment path. Also compute the overall likelihood as the sum of the maximum log probabilities of all recordings. \n3. For each state \nj in the model, collect all pitch \npoints (from all recordings) corresponding to this \nstate based on the optimum alignment path in the previous step. If we con catenate these pitch points \ninto a vector []qy yy ,...,2 1 (here we drop off the \nsubscript j for simplicity), then the optimum pa-\nrameters can be derived based on the principle of maximum likelihood estimate, as follows: \nq yq\nkk j /\n1\n\n\n=∑\n=µ , \n() () q y yq\nkT\nj k j k j /\n12\n−−=∑\n=µµ σ , \n∑\n==+r\nkkt\ndrjjp\n1)1,( , \n548   \n \n )1,( 1),( +−= jjp jjps t , \nwhere r is the number of recordings for this model, \nand kd the number of pitch points of recording k \nthat belongs to state j. \n4. Repeat step 2 to 3 until the overall likelihood does \nnot increase. \n \nIt should be noted that the above procedure is re-\npeated to find the optimum parameters of each CHMM \nof a song.  Moreover, the above procedure is based on \nEM (Expectation Maximization) and the overall likeli-hood is guaranteed to be m onotonically increasing until \nconverged after several iterations. \n3 OTHER APPROACHES TO MRAI \nAs we shall explain later, CHMM alone cannot guaran-\ntee good performance. Our error analysis indicates that we need to use other approaches jointly with CHMM to achieve better performance. Hence this section will in-troduce several approaches to MRAI, including LS (lin-ear scaling) and DTW (dynamic time warping). \n3.1 Linear scaling \nLS (linear scaling) is one of the most simple and \nstraightforward ways for comparing the input pitch with the song database. The basic idea is to compress or stretch the input pitch in a linear manner in order to match those in the song database. The following figure demonstrates the use of LS. \n \nFigure 3 . A typical example of linear scaling, \nwhere the input pitch vector is linearly scaled 5 \ntimes and the best match occurs when the input \npitch is stretched by 1.5 of its original length. \n3.2 Dynamic time warping \nDTW (dynamic time warping) is one of the most effec-\ntive methods for frame-based MRAI, as reported exten-sively in the literature. Howe ver, due to the space limit, \nwe do not give technical details of DTW. Interested readers can refer to the corresponding references (Jang et al., 2001; Jang & Lee, 2001; Jang et al., 2004; Zhu & Shasha, 2003; Hu & Dannenberg, 2002). 4 OTHER MRAI RELATED ISSUES \nIn the previous two sections, we have introduced 3 dif-\nferent approaches to compare the input pitch vector with those in the song database, including the newly proposed CHMM, and previously proposed LS and DTW. In this section, we shall discuss the specific ways we tackle several MRAI-related issues in our implementation, in-cluding pitch tracking, key transposition, and rest han-\ndling. \n4.1 Pitch tracking \nIn general, pitch tracking in singing or humming is \neasier than in speech signals due to the fact that the \npitch variations in singing/humming are usually not as \ndrastic. For simplicity, in this paper we should adopt the human-labelled pitch (which is a part of the singing corpus SQC) as the input to our melody recognition \nengine. By using the presumably correct pitch, we can concentrate on the error analysis of the proposed recog-nition approach, instead of worrying about the pitch tracking error. \n4.2 Key transposition \nTo deal with key transposition, most note-based ap-\nproaches apply the difference operator before invoking \nthe comparison procedure. However, for frame-based approach proposed in this paper, the difference operator amplifies noises and deteriorates the performance. Hence we adopt a heuristic binary-search-like method (Jang & Lee, 2001) to shift the entire input pitch vector to a suitable position that can generate the minimum DTW distance. \n4.3 Rest handling \nGeneral speaking, there are two ways to handle rests in \nboth the input pitch and the pitch of the database songs: \n1. Remove all the rests. 2. Replace the rests with its previous note’s \npitch level. \nBoth methods were used in our experiments and the \nresults are discussed in the following section. \n5 EXPERIMENTAL RESULTS \nThe section introduces the song database and the singing \nvoice corpus used in our experiments, together with the experimental settings and the corresponding results. \n5.1 Song database and singing voice corpus \nWe have collected a MIDI database of 38 children’s \nsongs and a corresponding singing voice corpus called \nSQC (Sung Query Corpus) for evaluating our MRAI \napproach. This corpus contains 1460 wave files (up to \nrecordings of the year 2004) of 8 seconds each, re-corded by 64 subjects (51 males and 13 females). Each subject was asked to record 20~30 songs (out of the 38 songs in the database) that he/she was most familiar \n549   \n \n with. All recordings in SQC are in Microsoft .wav for-\nmat, with a sampling rate of 8000 Hz, single channel (mono), bit resolution of 8 bits. For simplicity, all re-cordings are from beginning of the intended songs. SQC is available from the first author’s homepage at \nhttp://www.cs.nthu.edu.tw/~jang\n \nPlease follow the link of “MIR corpora” to download the SQC corpus and corresponding documents. \nIn order to perform a fair evaluation, we have divided \nthe corpus into two equal-size disjoint groups: A and B. For each song in the database, we tried to divide its re-cordings evenly into two groups. In our experiments, we \nused group A for training CHMM and group B for test, \nand vice versa for checking the consistency of these two groups. In the following subsections, we shall describe our experiments and the corresponding results. \n5.2 CHMM and its enhancement \nAfter detailed error analysis, we found that the opti-\nmum CHMM path can achieve the maximum cumulated \nlog likelihood, but the path is not exactly what we wanted. For instance, a state can hold as few as a single \npitch point, corresponding to a note duration of 1/32 second, which is not likely to occur in practical situa-tions. To fix this problem, we have tried several meth-ods and found the best one is to use LS to identify note boundaries and then use the note’s Gaussian PDF com-pute the cumulated likelihood. Note that the Gaussian PDFs are obtained from either score-based training (de-noted by CHMM1) or corpus-based training (denoted by CHMM2). \nFigure 4 shows the recognition rates with respect to \nthe computation time of each query with respect to 38 songs in the database. (Here the recognition rates are based on group B while group A was used as the train-ing data. Moreover, the rests are removed and the weighting factor of transition probabilities is set to 1.) \n \nFigure 4 . Top-1 recognition rates with respect to \nthe computation time of various methods. \nFrom the above figure, it is obvious that \nLS+CHMM2 has a higher recognition rate of 99.45% \n(with less computation time) than DTW (97.23%) and CHMM2 (93.21). LS (96.26%) also seems to be a de-cent competitor, however, it does not scale up well, as \nshown in section 5.3. \n5.3 Rest handling and weight adjustment \nAs mentioned earlier, we can either remove the rests or \nextend the rests with their previous notes’ pitch levels. For the weighting factor of transition probabilities, we employed a simple linear search to find its optimum value. The following bar chart shows the top-1 recogni-tion rates of LS+HMM2 with respect to different settings. \n \nFigure 5 . Top-1 recognition rates of LS+HMM2 \nwith respect to different settings. \nFrom the above bar chart, it seems that we can adopt \neither way to handle rests with only small difference in \nrecognition rates. Moreover, the optimum weight (= 2.8) does increase the recognition rates in 3 out of 4 cases. For the experiments in the following next subsection, \nwe used the optimum weight and the rests were all re-moved. \n5.4 Recognition rates w.r.t. database sizes \nIt is essential know how these methods scale up properly \nor not. To explore along this direction, we increased the song database by adding othe r real-world songs sequen-\ntially till its size is 5000. Figure 6 plots the top-1 recog-nition rates with respective to the sizes of the song data-base. From the plot, it is obvious that “LS+CHMM2” has the highest recognition rate curve and degrades gracefully as the size of th e database increases. CHMM2 \nalso degrades gracefully, though its initial recognition \nrates are not so high. On the other hand, DTW, LS, and LS+CHMM1 all have a high recognition rates when the database size is small, but degrade rapidly when the da-tabase size increases. We can conclude that both LS+CHMM2 and CHMM2 scale up nicely due to its model identification via corpus-based training. \n550   \n \n  \nFigure 6 . Top-1 recognition rates with respect to \nthe sizes of the song database. CHMM1 is the \nCHMM via score-based training; CHMM2 is the \nCHMM via corpus-based training. \n6 CONCLUSIONS AND FUTURE WORK \nIn this paper, we have proposed the use of CHMM and \nits enhancement scheme for singing/humming query for music retrieval. The proposed LS+CHMM method can achieve a higher recognition rate and less computation time when compared with th e state-of-the-art frame-\nbased MRAI approach of DTW. Moreover, the proposed method also scales up nicely when the size of the song \ndatabase increases. We have put the song database and \nthe singing voice corpus on the web for public access.  \nThere are a lot of potential di rections for future work. \nSome of the immediate ones include the use of “rest” states in CHMM for appropriately representing rests, a better way of incorporating key transposition into the evaluation and training procedures of CHMM, and the use of more features (such as pitch and delta pitch) in CHMM for achieving a better recognition rate. \nREFERENCES \nDannenberg, Roger B., Bi rmingham, William P., \nTzanetakis, George, Meek, Colin, Hu, Ning \nand Pardo, Bryan, “The MUSART Testbed \nfor Query-by-Humming Evaluation”, Interna-tional Symposium on Music Information Re-trieval 2003, Baltimore, Maryland, USA, Oc-tober 2003. \nHu, Ning and Dannenberg, Roger B.A, “Compari-\nson of Melodic Database Retrieval Tech-\nniques Using Sung Queries”, Proceedings of the 2nd ACM/IEEE-CS Joint Conference on Digital Libraries, Portland, Oregon, 2002. \nJang, J.-S. Roger, and Lee, Hong-Ru, \"Hierarchical \nFiltering Method for Content-\nbased Music \nRetrieval via Acoustic Input\", The 9th ACM Multimedia Conference, PP. 401-410, Ot-\ntawa, Ontario, Canada, September 2001. \nJang, J.-S. Roger, Chen, Jiang-Chun, and Kao, \nMing-Yang, \"MIRACLE: A Music Informa-\ntion Retrieval System with Clustered Computing Engines\", InternationalSymposium on Music Information Retrieval 2001, Indiana University, Bloomington, Indiana, USA, October 2001. \nJang, J.-S. Roger, Lee, Hong-Ru, Chen, Jiang-\nChuen, and Lin, Cheng-Yuan, \"Research and Development of an MIR Engine with Multi-modal Interface for Real-world Ap\nplications\", \nJournal of the American Society for Informa-\ntion Science and Technology, 2004. \nLiu, Baolong, and Li, Yadong, “Linear hidden \nMarkov model for music information retrieval based on humming”, Proceedings of IEEE In-\nternational Conference on Acoustics, Speech, \nand Signal Processing, vol. 5, pages 533-536, 2003. \nMeek, Colin, and Birmingham, William, “Jonny \nCan’t Sing: A Comprehensive Error Model for Sung Music Queries, ” International Sym-\nposium on Music Information Retrieval 2002, \nParis, France, October 2001. \nShifrin, Jonah, Pardo, Bryan, Meek, Colin, and \nBirmingham, Birmingham, “HMM- based \nmusical query retrieval,” Proceedings of the \n2nd ACM/IEEE-CS joint conference on Digi-tal libraries, pages 295–300, 2002. \nShih, Hsuan-Huei, Narayanan, Shrikanth S., and \nKuo, C.-C. Jay, \"An HMM-\nbased Approach \nto Humming Transcription\", IEEE Interna-tional Conference on Multimedia and Expo, vol. 1, pages 337-340, Aug. 2002. \nZhu, Yunyue and Shasha, Dennis, “Warping In-\ndexes with Envelope Transforms for Query by Humming”, Proceedings of the 2003 ACM \nSIGMOD International Conference on Man-\nagement of Data, San Diego, CA, June 2003.\n \n551"
    },
    {
        "title": "Rhythm-Based Segmentation of Popular Chinese Music.",
        "author": [
            "Kristoffer Jensen",
            "Jieping Xu",
            "Martin Zachariasen"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418117",
        "url": "https://doi.org/10.5281/zenodo.1418117",
        "ee": "https://zenodo.org/records/1418117/files/JensenXZ05.pdf",
        "abstract": "We present a new method to segment popular music based on rhythm. By computing a shortest path based on the self-similarity matrix calculated from a model of rhythm, segmenting boundaries are found along the diagonal of the matrix. The cost of a new segment is optimized by matching manual and automatic segment boundaries. We compile a small song database of 21 randomly selected popular Chinese songs which come from Chinese Mainland, Taiwan and Hong Kong. The segmenting results on the small corpus show that 78% manual segmentation points are detected and 74% automatic segmentation points are correct. Automatic segmentation achieved 100% correct detection for 5 songs. The results are very encouraging.",
        "zenodo_id": 1418117,
        "dblp_key": "conf/ismir/JensenXZ05",
        "keywords": [
            "segmentation",
            "popular music",
            "rhythm",
            "self-similarity matrix",
            "shortest path",
            "diagonal",
            "cost optimization",
            "manual and automatic boundaries",
            "small song database",
            "Chinese songs"
        ],
        "content": "RHYTHM-BASED SEGMENTATION OF POPULAR CHINESE MUSIC\nKristoffer Jensen Jieping Xu Martin Zachariasen \nDepartment of Medialogy \nUniversity of Aalborg Esbjerg \nNiels Bohrsvej 8 \n6700 Esbjerg, Denmark \nkrist@cs.aaue.dk School of Information \n Renmin University \nBeijing, China \njieping.xu@263.net Department of Computer Science \nUniversity of Copenhagen \nUniversitetsparken 1 \n2100 Copenhagen, Denmark \nmartinz@diku.dk \nABSTRACT \nWe present a new method to segment popular music \nbased on rhythm. By computing a shortest path based on \nthe self-similarity matrix calculated from a model of \nrhythm, segmenting boundaries are found along the di-\nagonal of the matrix. The cost of a new segment is opti-\nmized by matching manual and automatic segment \nboundaries. We compile a small song database of 21 \nrandomly selected popular Chinese songs which come \nfrom Chinese Mainland, Taiwan and Hong Kong. The \nsegmenting results on the small corpus show that 78% \nmanual segmentation points are detected and 74% auto-\nmatic segmentation points are correct. Automatic seg-\nmentation achieved 100% correct detection for 5 songs. \nThe results are very encouraging. \n \nKeywords:  Segmentation, Shortest path, Self-similarity, \nRhythm  \n1 INTRODUCTION \nSegmentation has a perceptual and subjective nature. \nManual segmentation can be due to different dimensions \nof music, such as rhythm or harmony. Measuring simi-\nlarity between rhythms is a fundamental problem in \ncomputational music theory. In this work, music seg-\nmentation is based solely on rhythm, which can be per-\nceived by a changing beat or different instrument modu-\nlation. While rhythm is indeed an important music fea-\nture, parts of other features, such as loudness and tim-\nbre, are also present in the rhythm information, because \nof the way it is calculated. However, harmony informa-\ntion is not. In Chinese music, rhythm is sometimes dif-\nferent compared to Western music. The beat is not \nclearly present in some music styles, and the rhythm \nboundaries depend mainly on the voice.   \n     Segmentation of music has many applications such \nas music information retrieval, copyright infringement \nresolution, fast music navigation and repetitive structure \nfinding.  \n     Music segmentation is a popular topic in research \ntoday. Several authors have presented segmentation and visualization of music using a self-similarity matrix    \n[1-3] with good results. Foote [2] used a measure of \nnovelty calculated from the self-similarity matrix. Jen-\nsen [1] optimized the processing cost by using a \nsmoothed novelty measure, calculated on a small square \non the diagonal of the self-similarity matrix. Other \nsegmentation approaches include information-theoretic \nmethods [5]. \n     Here, we present a method to compute segmentation \nboundaries using a shortest path algorithm. Our assump-\ntion is that the cost of a segmentation is the sum of the \nindividual costs of segments, and we show that with this \nassumption, the problem can be solved efficiently to \noptimality.  The method is applied to popular Chinese \nmusic. \n     In order to optimize the manual segmentation, it is \ndone using a particular Chinese notation system as sup-\nport. This system is briefly introduced here. \n2 FEATURE EXTRACTOR \nThe features used in the segmentation task should pin-\npoint the main characteristics that define the difference \nbetween two segments in a rhythmic song. While spec-\ntral characteristics generally capture the timbral evalua-\ntion well, a rhythmic feature has been chosen here. This \nis believed to encompass changes in instrumentation and \nrhythm, while not prioritizing singing and solo instru-\nments that are liable to have influence outside the strict \nborders of a segment. The feature extractor consists of \nthree steps, a note-onset detector, a rhythm feature, the \nrhythmogram, and a self-similarity measure of the \nrhythmogram. \n2.1 Note onset detection \nThe beat in music is often marked by transient sounds, \ne.g. note onsets of drums or other instrumental sounds. \nOnset positions may correspond to the position of a \nbeat, while some onsets fall off beat. The onset detec-\ntion is made using a feature estimated from the audio, \nwhich can subsequently be used for the segmentation \ntask. The note-onset detection aim is to give an estimate \nof the note onset of an instrument. This task is more \ndifficult in the presence of other music and musical in-\nstruments with soft attacks. Often note onset methods \nuse some sort of loudness, timbral, or similar feature. \nJensen [1] compared a large number of features using an \nannotated database of twelve songs, and found the per-\nceptual spectral flux (psf) to perform best. This is calcu-\nlated as Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full \ncitation on the first page. \n© 2005 Queen Mary, University of London \n374   \n \n \n()()!\"#$%&'='=(3/113/12/1)()(nknkNkkaafWnpsfb, (1) \nwhere n is the feature block index, Nb is the block size, \nand \nnka and fk are the magnitude and frequency of the bin \nk of the Short-Time Fourier Transform (STFT), obtained \nusing a Hanning window. The step size is 10 millisec-\nonds, and the block size is 46 milliseconds. W is the fre-\nquency weighting used to obtain a value closer to the \nhuman loudness contour, and the power function is used \nto simulate the intensity-loudness power law. The power \nfunction furthermore reduces the random amplitude \nvariations. These two steps are inspired from the PLP \nfront-end [5] used in speech recognition.  \n     Note onset detection systems are often used as a step \nin beat following. Goto and Muraoka [7] presented a \nbeat tracking system, where two features were extracted \nfrom the audio, based on the frequency band of the snare \nand bass drum.  Scheirer [8] took another approach, by \nusing a non-linear operation of the estimated energy of \nsix band-pass filters as features. The result was com-\nbined in a discrete frequency analysis to find the under-\nlying beat. As opposed to the approaches described so \nfar Laroche [9] built an offline system, using one feature, \nthe energy flux, cross-correlation and dynamic pro-\ngramming, to estimate the time-varying tempo.  \n2.2 Rhythm model \nThe PSF feature detects most of the manual note onsets \ncorrectly, but it still has many peaks that do not corre-\nspond to note onsets, and many note onsets do not have \na peak in the PSF. In order to obtain a more robust \nrhythm feature, the autocorrelation of the feature is now \ncalculated on overlapping blocks of 8 seconds, with half \na second step size (2 Hz feature sample rate).  \n!\"++=+#=ifsrfsrnfsrnjnijpsfjpsfirg/8/21/2)()()(\n (2) \nwhere fsr is the feature sample rate, and n is the block \nindex. Only the information between zero and two sec-\nonds is retained. The autocorrelation is normalized so \nthat autocorrelation at zero lag equals one. \nIf visualized with lag time on the y-axis, time position \non the x-axis, and the autocorrelation values visualized \nas colours, it gives a fast overview of the rhythmic evo-\nlution of a song (figure 1).  \n     This representation, called rhythmogram [1], pro-\nvides information about the rhythm and the evolution of \nthe rhythm in time. The autocorrelation has been cho-\nsen, instead of the FFT, for two reasons. First, it is be-\nlieved to be more in accordance with the human percep-\ntion of rhythm [10], and second, it is believed to be \nmore easily understood visually. \n2.3 Self-Similarity \nTo obtain a better representation of the similarity of \nsong segments, a measure of self-similarity is used.   \n \n \nFigure 1. Rhyhtmogram of ‘caoyuan’. \n     Several studies have used a measure of self-\nsimilarity, or recurrence plots as it was initially called \n[11], in automatic music analysis. Foote [1] used the dot \nproduct on MFCC sampled at a 100 Hz rate to visualize \nthe self-similarity of different music excerpt.  Later he \nintroduced a checkerboard kernel correlation as a nov-\nelty measure [2] that identifies notes with small time \nlag, and structure with larger lags with good success.  \nBartsch and Wakefield [3] used the chroma-based \nrepresentation (all FFT bins are put into one of 12 \nchromas) to calculate the cross-correlation and to \nidentify repeated segments, corresponding to the chorus, \nfor audio thumbnailing. Peeters [12] calculated the self-\nsimilarity from the FFT on the energy output of an \nauditory filterbank. Jensen [1] used filtering and \nsegment following in the scale-space domain, inspired \nfrom image segmentation, to permit large-scale \nsegmentation from small-scale self-similarity measures. \n \n \nFigure 2. Selfsimilarity matrix of ‘Ҥჰ-caoyuan’. Dark \nregions indicate very similar or dissimilar segments.  \n375   \n \n In this work, self similarity is calculated as the L2 \nnorm between the rhythmogram vectors l and k, \n()!=\"=fsrikllkirgirgA/812)()(\n (3) \n     In parts of the music where the rhythmic content is \nsimilar, the corresponding self similarity is close to zero. \nAn example of the self-similarity, for the song  “Ҥჰ-\ncaoyuan”, can be seen in figure 2. \n3 SEGMENTATION BY             \nSHORTEST PATH \nTo find a best possible segmentation, based on the self-\nsimilarity matrix calculated from a model of rhythm, we \nfirst present a model for segmentation. Then we show \nthat the problem can be solved optimally by computing \na shortest path in a directed acyclic graph. \n     Our segmentation model is as follows. We have a \nsequence 1, 2, ..., N of N blocks of music that should be \ndivided into a number of segments. Let c(i,j) be the cost \nof a segment from block i to block j, where 1 ≤ i ≤ j ≤ N. \nThe cost of a segment should be a measure of the self-\nsimilarity of the segment, such that segments with a \nhigh degree of self-similarity have a low cost.  We have \nchosen to use the following cost function for segments:  \n! c(i,j)=1j\"i+1Alkl=ik#k=ij#\n (4) \n     This cost function computes the sum of the average \nself-similarity of each block in the segment to all other \nblocks in the segment. While a normalization by the \nsquare of the segment length (j-i+1) would give the true \naverage, it is believed that this would severely impede \nthe influence of new segments with larger self similarity \nin a large segment, since the large values would be nor-\nmalized by a relatively large segment length.  \n     Let i1j1, i2j2, ..., iKjK be a segmentation into K seg-\nments, where i1=1, i2=j1+1, i3=j2+1, ..., jK=N. The total \ncost of this segmentation is the sum of segment costs \nplus an additional cost f(K), which depends on the num-\nber of segments: \n! E=f(K)+c(in,jn)n=1K\"\n (5) \n     If we set f(K)=αΚ, where α>0 is a fixed cost for a \nnew segment, the total segmentation cost becomes: \n! E=\"+c(in,jn)()n=1K#\n (6) \n     By increasing α  we decrease the number of resulting \nsegments. Choosing an appropriate value of α is dis-\ncussed in our section on experiments.  \n     In order to compute a best possible segmentation, we \nconstruct an edge-weighted directed graph G=(V,E). The set of nodes is V={1,2,...,N+1}. For each possible seg-\nment ij, where 1 ≤ i ≤ j ≤ N, we have an edge (i,j+1) in \nE. The weight of the edge (i,j+1) is α+c(i,j). A path in G \nfrom node 1 to node N+1 corresponds to a complete \nsegmentation, where each edge identifies the individual \nsegments. The weight of the path is equal to the total \ncost of the corresponding segmentation. Therefore, a \nshortest path (or path with minimum total weight) from \nnode 1 to node N+1 gives a segmentation with minimum \ntotal cost. Such a shortest path can be computed in time \nO(|V|+|E|)=O(N2), since G is acyclic and has |E|=O(N2) \nedges [13]. An illustration of the directed acyclic graph \nfor a short sequence is shown in figure 3. \n \n1234! + c(1,1)! + c(1,2)! + c(2,3)! + c(2,2)! + c(3,3)! + c(1,3)\n \nFigure 3. Directed acyclic graph for the problem of \nsegmenting N=3 segments. \n     Goodwin and Laroche [14] presented a similar ap-\nproach where dynamic programming was used to com-\npute an optimal segmentation. However, in their ap-\nproach the graph has N2 nodes. As a consequence, the \ncost structure was less flexible, and it did not involve a \nparameter similar to our new segments cost (α). \n4 EXPERIMENT \nTo test the segmentation algorithm, we have compiled a \nsmall song database including 21 popular Chinese songs \ncoming from Chinese Mainland, Taiwan and Hong-\nKong. The songs are randomly selected from Chinese \ntop500 popular songs and have a variety in tempo, genre \nand style, including pop, rock, lyrical and folk. The sam-\npling frequency of the songs is 44.1 kHz, stereo channel \nand 16 bit per sample.  \n4.1 Segmentation behaviour as a function of α \nThe complete segmentation system is now complete. It \nconsists of a feature extractor (the psf), a rhythm model \n(the rhythmogram), a selfsimilarity measure, and finally \nthe segmentation, based on a shortest path algorithm. \nThe new segment cost (α) of the segmentation algorithm \nis analyzed here. What we are interested in is mainly to \ninvestigate if the total cost of a segmentation (eq. 6) has \na local minimum. The total cost as a function of the new \nsegment cost α is shown in figure 4. The total cost for a \nvery small α is close to zero. As expected, the cost in-\ncreases monotonically with α. \n     Another interesting parameter is the total number of \nsegments. It is plausible that the segmentation system is \n376   \n \n to be used in a situation where a given number of seg-\nments is wanted. The number of segments as a function \nof new segment cost α is shown in figure 5. This num-\nber decreases with the segment cost, as expected. \n \n10-210-1100101102100101102103\nNew segment costTotal costCost for 21 Chinese songs\n \nFigure 4. Total cost as a function of segment cost (α). \n     The number of segments follows the same shape for \nall songs. Indeed, the ratio between the maximum num-\nber of segments to the minimum number of segments is \nclose to two for the main part of α and always below \nthree. Thus, for a given α, all songs could be expected to \nhave between, e.g. 8 and 16-24 segments. \n \n \n10-210-1100101102100101102103\nNew segment costTotal number of segments# segments for 21 Chinese songs\n \nFigure 5. Total number of segments as a function of new \nsegment cost (α). \n \n \n4.2 Manual Segmentation of Popular Chinese music \nIn our experiment, each song is segmented manually \nusing Audacity software to obtain a rhythm sequence \nsuch as ABCBC, where each letter identifies a similar \npart of the song (figure 6). A composer and an audio \nresearcher made the manual segmentation.  \n     Segmentation has a perceptual and subjective nature. \nManual segmentation can be due to the different dimen-sions of music, such as rhythm or harmony. In our ex-\nperiment, we limited the manual segmentation to be \nbased on rhythm changes mainly. Most segmentation \ndepends on different instrument and beat modulation, \nsometimes depends on pitch change (modulation) and \nrarely on the voice. We use the Chinese numbered musi-\ncal notation to assist in the manual segmentation. \n     Figure 6 shows part of the audio for the song of  “ѯ\nපભ -Bosimao” and figure 7 shows its Chinese num-\nbered musical notation. \n \nFigure 6. Manual segmentation using Audacity soft-\nware. \n     From figure 6, we found that this part contains 4 dif-\nferent rhythm segment. The A and B segments are prel-\nudes played with different instruments. c is a repetition \nof the C segmentation,  they are the same rhythm in beat \nbut there is a clear rhythm segmenting point. The dura-\ntion of c is longer than C. There is different in the end of \nsegmentation c, as it includes a transition with different \nvoice but the same beat. \n     In the Chinese numbered musical notation (figure 7), \nthe tonality and time measure is marked first. Then, the \nmelody notes are given with numbers from 1 to 7, with \naccompanying b/#, if applicable. The number 0 indicates \nsilence. One dot above/below the number indicates a \nraise or lowering of one octave. Each number corre-\nsponds to one beat of the bar. One or several lines under \nthe number divide the length of the note accordingly. To \nextend a note to more than one beat, a dash is noted at \nthe next time location. Most other aspects of the nota-\ntion, such as the bars, repetitions, and ties are notated as \nin the traditional Western notation. Using the notation to \nmake the manual segmentation is believed to have \nhelped to make a more objectively correct segmentation. \nThe database is used to evaluate the automatic segmenta-\ntion performance. \n4.3 Comparison  \nThe last step in the segmentation is to compare the man-\nual and the automatic segment boundaries for different        Total number of segments \n                      Total cost \n377   \n \n values of the new segment cost (α). To do this, the auto-\nmatic segmentations are calculated for increasing values \nof α; a low value induces many segments, while a high \nvalue gives few segments. The manual and automatic \nsegment boundaries are now matched, if they are closer \nthan a threshold (currently 5 seconds).  For each value of \nα, the relative ratio of matched manual and automatic \nboundaries (Mm and Ma, respectively) are found, and the \ndistance to the optimal result is minimized:  \n \n()()2211)(amMMd!+!=\"  (7) \n   \n \nFigure 7. The Chinese numbered musical notation. \n     Although 5 seconds may seem a large threshold, and \nobviously the matched ratio diminishes with this thresh-\nold, it is chosen because it results in an appropriate num-\nber of automatic segment boundaries. \n \n \nFigure 8. Manual and automatic ratio for Лሰܛ֥ֆ-\nyibeizidegudan. A star indicates minimum distance to \noptimum result.      An example of this distance for ’ Лሰܛ֥ֆ-\nyibeizidegudan’ is given in figure 8. The corresponding \nα-dependent automatic segment boundaries are shown in \nfigure 9.The ratio between matched and total number is \n8/8 (100%) for manual segmentation and 8/9 (89%) for \nthe automatic segmentation.  \n     The minimum distances for the optimum result for all \nsongs are obtained for α between 4.5 and 12.9 with an \naverage α=6.96, and one and three quartile values of 5.4 \nand 8.0. As a certain range of α generally gives the opti-\nmum results, approximately 10% of the α values for the \ndifferent songs overlap. \n4.4 Results and discussion \nThe comparison results of segmenting the 21 popular \nChinese songs are shown in Table 1. The results show \nthat most manual segmentation points can be detected \ncorrectly by automatic segmentation. The average ratio \nbetween matched and total numbers is 78% for manual \nsegmentation and 74% for automatic segmentation. The \naverage automatic ratio is 74%. These results are en-\ncouraging; in particular if the average ratios are calcu-\nlated for the mean α of all songs, in which case the man-\nual ratio is 67% and the automatic ratio is 65%. \n \n \nFigure 9. The automatic segment boundaries for increas-\ning segment cost (α), with manual (‘*’) and automatic \n(‘o’) segment boundaries. \n \n     An example of the self-similarity, with the automatic \nand manual segmentation boundaries marked, is shown \nin figure 10. It is clear that both the automatic and man-\nual segment boundaries are put at the beginning of a \nsquare with high similarity. Some of the automatic mark \nis not matched, but they do not show a important new \nsimilarity area either. \n     Another example is shown in figure 11. In this case, \nalthough many marks are matched, it seems that the \nmanual marks are done on some of the segments starts, \n378   \n \n but not on others, possible because these were just repe-\ntitions of the previous segment. \n \n \nFigure 10. Self-similarity matrix for \"ਆᆺל޲-\nLiangzihudie\", with manual ('*') and automatic marks \n('o'). \nIn two of the songs, “ޓιޓι୆ -henaihenaini” and “ਆ\nᆺל޲- liangzihuidie”, the beat is not marked at all. Since \nthe songs do not have a clear beat, the segmentation is \nmade mainly from the voice of the songs.  The manual / \nautomatic ratio is 75%/100% and 80%/89%. This shows \nthat the model can be used, even for music that does not \nhave a clear beat. \n \n \nFigure 11. Self-similarity matrix for \"౦ದ-qingren\", \nwith manual ('*') and automatic marks ('o'). \n \n     The automatic and manual segmentation boundaries \nare shown in figure 12. While it is clear that many of the \nsegmentation boundaries are matched, some are still orphelin, i.e. not matched to another boundary. This is \ndue both to the manual marking not being homogene-\nous, but some errors could also be caused by the feature \nand self-similarity calculation choices. \n \nTable 1. Segmentation results of 21 popular Chinese \nsongs \n \nSong Man/tot Auto/tot \nҤჰ -Caoyuan 6/8 (75%) 6/6 (100%) \nܜߧ୒ -Huiguniang 11/16 (69%) 11/20 (55%) \nᆘڛ- Zhengfu 7/8 (88%) 7/30 (23%) \nѯපભ-Bosimao 10/12 (83%) 10/17 (59%) \nᇏݓದ -Zhongguoren 7/11 (64%) 7/9 (78%) \nᆘڛ- Conquer 8/10 (80%) 8/12 (67%) \nުট -Houlai 7/9 (78%) 7/16 (44%) \n൛࿽ -Oath2 11/14 (79%) 11/11 (100%) \n౦ದ -Qingren 9/11 (82%) 9/17 (53%) \n໖љ -Wenbie 16/19 (84%) 16/26 (62%) \nޓιޓι୆ -henaihenaini 9/12 (75% 9/9 (100%) \nܛֆК϶౯ -Adayan6 11/14 (79%) 11/13 (85%) \n঺ඊιն૜ -laoshuaidami 9/10 (90%) 9/29 (31%) \nι౦ 36࠹-Aq36 16/18 (89%) 16/17 (94%) \n૓ྜਔ -Mengxingle 6/9 (67%) 6/11 (55%) \nЛሰܛ֥ֆ -yibeizidegudan 8/8 (100%) 8/10 (80%) \n૓ष൓ֹ֥ٚ-Mengkai 6/9 (67%) 6/6 (100%) \n໡ૌ֥ι-Ourlove 11/14 (79%) 11/12 (92%) \n୆ݺ- Nihao 11/13 (85%) 11/12 (92%) \nਆᆺל޲-Liangzihudie 8/10 (80%) 8/9 (89%) \n໡҂൞฿൐ -Wbsts 5/9 (56%) 5/7 (71%) \naverage 9/12 (78%) 9/14 (74%) \n \n5 CONCLUSIONS \nThis paper presents a feasible method to segment music. \nThis is done using a rhythm model, the rhythmogram \nand a shortest path segmentation algorithm based on the \nself similarity of the rhythmogram. The free parameter \nof the shortest path algorithm, the cost of a new segment, \nhas been found by minimizing the distance to a optimal \nsolution in matching manual and automatic segmentation \nboundaries. Experiments using popular Chinese music \nhave resulted in 67% manual segmentation ratio and \n65% automatic segmentation ratio based on a database \nwith 21 songs. This result is improved to 78%/75% with \nindividually optimized segment costs for each song. \nThere are 5 songs which automatic segmentation \nachieved 100% correct detection. This indicated that the \nrhythm-based segmentation is useful for Chinese music, \nbut probably also for much other popular music, because \nof the similarities to the Chinese music.  \n \n379   \n \n ACKNOWLEDGEMENT \nLaurent “Saxi” Georges has been most helpful in under-\nstanding the Chinese numbering system, helping with the \nmanual segmentation and in general music related dis-\ncussions. \nREFERENCES \n[1] Foote, J., Visualizing Music and Audio using Self-\nSimilarity. In Proceedings of ACM Multimedia ’99, \npp. 77-80, Orlando, Florida, 1999. \n[2] Foote, J., Automatic Audio Segmentation using a \nMeasure of Audio Novelty. In Proceedings of IEEE \nInternational Conference on Multimedia and Expo, \nvol. I, pp. 452-455, July 30, 2000. \n[3] Bartsch, M. A. and Wakefield, G.H., To Catch a \nChorus: Using Chroma-Based Representations for \nAudio Thumbnailing. in Proceedings of the \nWorkshop on Applications of Signal Processing to \nAudio and Acoustics, pp. 15-18, 2001. \n[4] Jensen K., A Causal Rhythm Grouping. Lecture \nNotes in Computer Science, Volume 3310, pp. 83-95. \n2005. \n[5] Dubnov, S., Assayag, G., El-Yaniv, R., Universal \nClassification Applied to Musical Sequences. Proc. \nof the International Computer Music Conference, \nAnn Arbour, Michigan, pp. 332-340 1998. \n[6] Hermansky H., Perceptual linear predictive (PLP) \nanalysis of speech, J. Acoust. Soc. Am., vol. 87, no. \n4, pp. 1738-1752, Apr. 1990. [7] Goto M., and Muraoka, Y., A real-time beat tracking \nsystem for audio signals. Proceedings of the \nInternational Computer Music Conference, pp. 171-\n174, 1995. \n[8] Scheirer, E., Tempo and Beat Analysis of Acoustic \nMusical Signals, Journal of the Acoustical Society of \nAmerica, Vol. 103, No. 1, pp. 588-601, 1998. \n[9] Laroche J., Efficient tempo and beat tracking in \naudio recordings, J. Audio Eng. Soc., 51(4), pp. 226-\n233, April 2003. \n[10]  Desain P., A (de)Composable theory of rhythm. \nMusic Perception, 9(4) pp. 439-454, 1992. \n[11]  Eckmann, J. P., Kamphorst, S. O., and Ruelle, D., \nRecurrence plots of dynamical systems,  Europhys. \nLett. 4, 973, pp. 973-977, 1987. \n[12]  Peeters, G., Deriving musical structures from signal \nanalysis for music audio summary generation: \nsequence and state approach. In Computer Music \nModeling and Retrieval (U. K. Wiil, editor). Lecture \nNotes in Computer Science, LNCS 2771, pp. 143-\n166, 2003. \n[13]  Cormen T. H., Stein C., Rivest R. L., Leiserson C. \nE., Introduction to Algorithms, Second Edition. The \nMIT Press and McGraw-Hill Book Company, 2001. \n[14]  Goodwin, M. M. and Laroche, J., Audio \nsegmentation by feature-space clustering using \nlinear discriminant analysis and dynamic \nprogramming. In Proceedings of the IEEE \nWorkshop on Applications of Signal Processing to \nAudio and Acoustics, pp. 131-134, 2003.\n Figure 12. Automatic (o) and manual (*) segment boundaries for the 21 Chinese songs. \n0123456-25-20-15-10-50\nTime (minutes)Song indexAutomatic (o) and manual (*) marks      Song index \n380"
    },
    {
        "title": "Harmonic-Temporal Clustering via Deterministic Annealing EM Algorithm for Audio Feature Extraction.",
        "author": [
            "Hirokazu Kameoka",
            "Takuya Nishimoto",
            "Shigeki Sagayama"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417629",
        "url": "https://doi.org/10.5281/zenodo.1417629",
        "ee": "https://zenodo.org/records/1417629/files/KameokaNS05.pdf",
        "abstract": "This paper proposes “harmonic-temporal structured clustering (HTC) method”, that allows simultaneous estimation of pitch, intensity, onset, duration, etc., of each underlying source in multi-stream audio signal, which we expect to be an effective feature extraction for MIR systems. STC decomposes the energy patterns diffused in timefrequency space, i.e., a time series of power spectrum, into distinct clusters such that each of them is originated from a single sound stream. It becomes clear that the problem is equivalent to geometrically approximating the observed time series of power spectrum by superimposed harmonictemporal structured models (HTMs), whose parameters are directly associated with the specific acoustic characteristics. The update equations in DA(Deterministic Annealing)EM algorithm for the optimal parameter convergence are derived by formulating the model with Gaussian kernel representation. The experiment showed promising results, and verified the potential of the proposed method. Keywords: audio feature extraction, multi-pitch estimation, harmonic-temporal structured clustering. 1",
        "zenodo_id": 1417629,
        "dblp_key": "conf/ismir/KameokaNS05",
        "keywords": [
            "Harmonic-temporal structured clustering",
            "Audio feature extraction",
            "Multi-pitch estimation",
            "Deterministic Annealing EM algorithm",
            "Time-frequency space",
            "Power spectrum",
            "MIR systems",
            "Gaussian kernel representation",
            "Source separation",
            "Acoustic characteristics"
        ],
        "content": "HARMONIC-TEMPORAL-STRUCTUREDCLUSTERING VIA\nDETERMINISTICANNEALING EM ALGORITHM FOR\nAUDIOFEATUREEXTRACTION\nHirokazuKameoka, TakuyaNishimoto, Shigeki Sagayama\nGraduate School of Information Science and Technology,The Universityof Tokyo\n7-3-1Hongo, Bunkyo-ku,Tokyo,113-8656, Japan\ne-mail: fkameoka,nishi,sagayama g@hil.t.u-tokyo.ac.jp\nABSTRACT\nThis paper proposes “harmonic-temporal structured clus-\ntering (HTC) method”, that allows simultaneous estima-\ntionofpitch,intensity,onset,duration,etc.,ofeachunder-\nlying source in multi-stream audio signal, which we ex-\npecttobeaneffectivefeatureextractionforMIRsystems.\nSTC decomposes the energy patterns diffused in time-\nfrequencyspace,i.e.,atimeseriesofpowerspectrum,into\ndistinct clusters such that each of them is originated from\na single sound stream. It becomes clear that the problem\nisequivalenttogeometricallyapproximatingtheobserved\ntimeseriesofpowerspectrumbysuperimposedharmonic-\ntemporal structured models (HTMs), whose parameters\nare directly associated with the speciﬁc acoustic charac-\nteristics. The update equations in DA(Deterministic An-\nnealing)EM algorithm for the optimal parameter conver-\ngencearederivedbyformulatingthemodelwithGaussian\nkernel representation. The experiment showed promising\nresults,and veriﬁedthe potential of the proposed method.\nKeywords: audiofeatureextraction,multi-pitchestima-\ntion,harmonic-temporal structured clustering.\n1 INTRODUCTION\nAutomatic audio feature extraction of music signals has\nbeen taken as one of the most important topics in recent\nmusic processing area, towards developing music infor-\nmation retrieval (MIR) systems. This paper describes a\nnew approach of extracting audio features, e.g., pitch, on-\nset, duration, intensity, timbre and so forth of underlying\nnote events, simultaneously from input multi-stream mu-\nsic signal, based upon bottom-up deterministic model pa-\nrameteroptimization methodology.\nDevelopingreliablemulti-pitchanalysisalgorithmfor\naccurately obtaining these features is of primary impor-\ntance. Contrary to this requirement, the standard level\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitationon the ﬁrst page.\nc°2005Queen Mary,Universityof Londonof the numerous conventional methods for multi-pitch\nanalysis has been considered to be far from a practi-\ncal use. However, the recent pioneering ideas, e.g.,\ngraphical model based (Kashino et al., 1995), ﬁlter-\nbank based (Klapuri et al., 2000), Kalman ﬁltering based\n(Nishi et al., 1996; Abe and Ando, 2000), multi-agent\nbased (Nakatani, 2002) and parametric signal and spec-\ntrum modelings based approaches (Feder and Weinstein,\n1988;Chazanetal.,1993;GodsillandDavy,2002;Goto,\n2004;Kameokaetal.,2005)broughtremarkableprogress.\nWhile multi-pitch analysis is, in general, a typical ill-\nposed problem of extracting necessary information lying\nbeneath an ambiguous observation, most of these meth-\nods made the problem solvable basically by dealing with\nfrequency and time dimensions separately: ﬁrst extract\ninstantaneous pitch likelihoods of concurrent sources at\neach short-time segment and then interpolate/extrapolate\nthem to build up most likely overall continuous tempo-\nral pitch structures of multiple audio streams. In auditory\nscene analysis (ASA), these two processes in human au-\nditionaregenerallycalled‘segregation’and‘integration’,\nrespectively.\nIncontrasttothecommonstrategybasedonsequential\nintegrationofinstantaneouspitchlikelihoodsextractedvia\nsegregation process, whose performance depends criti-\ncallyonhowpreciselythesegregationprocessworks,this\npaper aims to offer yet another framework based on si-\nmultaneousestimationofgeometricstructuresinbothfre-\nquencyandtimedirectionsofpowerspectraofunderlying\nsound sources.\n2 GENERAL FORMULATION\nConsidertimeseriesofobservedpowerspectrum W(x; t),\nwhere xandtare log-frequency and time, whose domain\nof deﬁnition is\nD=fx; t2Rj­0·x·­1; T0·t·T1g:(1)\nThe problem we are dealing with is to decompose this\nobserved pattern into Knumber of sequential spectral\nstreams, i.e., clusters, such that each is originated from\na single distinct source activation. This problem is, obvi-\nously, an unsupervised categorization of the energy den-\nsityW(x; t)at each coordinate (x; t), and is hardly a\nstraightforwardissue.\n115The observed energy density W(x; t)at each coordi-\nnate(x; t)isnotalwayscompletelyoriginatedfromasin-\ngle source but rather superposed by energy patterns gen-\neratedfromdifferentsourcesthatarelocatedclosetoeach\nothersin xtplane,makingittotallyambiguous. Thus,we\nshall assume that each energy density W(x; t)has fuzzy\nmembership,i.e.,themembershipdegree m(k;x; t),ink-\nth cluster. Approximately assuming that observed power\nspectral densities are the sum of actual power densities of\nunderlyingsources,whichisnotpreciselytruebutaccept-\nable in expectation sense (where phase differences take\nuniformlyrandom values), m(k;x; t)satisﬁes\nX\n8km(k;x; t) =1;8k; 0·m(k;x; t)·1:(2)\nTherefore, m(k;x; t)W(x; t)denotes the decomposed\nspectral density of the k-th source, i.e., k-th cluster. Let\nus deﬁne qk(x; t;£)as a function modeling latent dis-\ntinct spectral stream density of k-th active source (in mu-\nsic, corresponds to a single note event), governed by pa-\nrameter vector £, where the class of the sources men-\ntioned here, in general, includes not only harmonic sig-\nnals but even white or pink noises or any others, as far\nas those properties can be well modeled in qk(x; t;£)\nwith a mathematical representation. Now the function\nqk(x; t;£)is what we are to estimate and ‘goodness’ of\nthepartitionedcluster m(k;x; t)W(x; t)canbemeasured\nbyaquasi-distanceof m(k;x; t)W(x; t)andqk(x; t;£):\nZZ\nDm(k;x; t)W(x; t)|\n{z\n}\ndensity of cluster klogm(k;x; t)W(x; t)\nqk(x; t;£)dxdt(3)\nThough deﬁning some other forms for the quasi-distance\nis certainly possible such like L2norm, the intention\nof giving this speciﬁc form shown above will become\nclear in the following descriptions. It is obvious that as\nqk(x; t;£)andm(k;x; t)W(x; t)become closer, Eq. 3\napproacheszero. Henceaglobalcostfunctionoftheclus-\nteringto minimize w.r.t. £is givenas\nJ=X\n8kZZ\nDm(k;x; t)W(x;t)logm(k;x; t)W(x;t)\nqk(x; t;£)dxdt(4)\nsubjectedto\nZZ\nDW(x; t)dxdt=X\n8kZZ\nDqk(x; t;£)dxdt=W(5)\n(tolet Jbenon-negative,cf.,Jensen’sinequality)whereit\ncanbe further rewrittenas\nJ= -I(£) -¸ÃX\n8km(k;x; t) -1!\n+X\n8kZZ\nDm(k;x; t)W(x; t)logm(k;x; t)W(x; t)dxdt\nI(£)´X\n8kZZ\nDm(k;x; t)W(x; t)logqk(x; t;£)dxdt(6)\nwhere ¸is a Lagrange multiplier. Although minimizing\nJw.r.t. both £andm(k;x; t)rarely has an analytic so-\nlution, it can be monotonically decreased by alternatelyoptimizing £andm(k;x; t),similartothebasiciterative\nclustering algorithm such as the k-means algorithm. Par-\ntial derivativeof the integrandin Jw.r.t. m(k;x; t)is\nW(x; t)µ\n1+logm(k;x; t)\nqk(x; t;£)¶\n-¸ (7)\nsuch that setting it zero gives\nm(k;x; t) =qk(x; t;£)expµ¸\nW(x; t)-1¶\n:(8)\nFrom Eqs. 2 and 8, we get\n¸=W(x; t)Ã\n1-logX\n8kqk(x; t;£)!\n(9)\nsuch that substituting Eq. 9 in Eq. 8, we ﬁnally have the\noptimal membership degreeunder ﬁxed £,givenas\nbm(k;x; t) =qk(x; t;£)\nX\n8kqk(x; t;£): (10)\nSubstitutingEq. 10in4,itbecomesexactlythesameform\nas the KL(Kullback-Leibler) divergencebetween W(x; t)\nand the sum of qk(x; t;£)for all k, i.e.,\nJmk=/x62mk=ZZ\nDW(x; t)logW(x; t)\nX\n8kqk(x; t;£)dxdt(11)\nsuch that this clustering can also be understood as a\nmodel-based geometric optimal approximation. Another\ninterestinginterpretationofthisresultisthatbyregarding\nkas missing data and replacing qk(x; t;£)with com-\nplete data pdf p(k; x; tj£), it proves the convergence of\nEM(Expectation-Maximization) algorithm from another\nviewpointwithoutapplyinganyprobabilitylaws. Thecor-\nrespondence to the EM algorithm becomes much clearer\nby comparing Eqs. 6 and 10 with Qfunction, givenby\nQ(£;e£) =\nX\n8kZZ\nDmissing data pdfz\n}|\n{\np(kjx; t;£)observedpdfz\n}|\n{\nW(x; t)logcomplete data pdfz\n}|\n{\np(k; x; tje£)dxdt\np(kjx; t;£) =p(k; x; tj£)\np(x; tj£)=p(k; x; tj£)\nX\n8kp(k; x; tj£)\nwhere k; x; t 2­(probabilistic variable)and\nZZ\nDW(x; t)dxdt=1;X\n8kZZ\nDp(k; x; tj£)dxdt=1:\nUnder ﬁxed membership degree m(k;x; t), on the other\nhand, parameter £canbe updated by\nb£=argmin\n£J=argmax\n£I(£)\n³\n=argmax\n/x66£Q(£;e£)´\n(12)\naccording to the speciﬁc form of qk(x; t;£), which will\nbe formulated in the next section. We call the methodol-\nogy for multi-pitch analysis based on this general formu-\nlation ‘spectro-temporal structured clustering (STC)’.\n116Log-frequencyTimeEq. 15\n\u0000 \u0001\u0003\u0002\u0005\u0004\u0006\b\u0007Power density\nFigure1: k-thharmonic-temporal structured model (HTM) qk(x; t;£)(Eq. 17)\npower density\nµ\n\u0000+log2µ\n\u0000 log-freq. +log3µ\n\u0000\nFigure2: Cuttingplaneof qk(x; t;£)attime t(Eq. 15)\nφ\n\u0000\nφ\n\u0000φ\n\u0000φ\n\u0000φ\n\u0000φ\n\u0000φ\n\u0000φ\n\u0000 timepower density\nFigure3: Powerenvelopefunction Ukn(t)(Eq. 19)\n3 HTC FORMULATION\n3.1 MODEL REPRESENTATION\nIn this section, a mathematical form of qk(x; t;£)is de-\nscribed. Now let us focus only on harmonic signal, that\nhas pitch or fundamental frequency ( F0), through the rest\nof this paper, given that music audio feature extraction is\nindeed what we are practically aiming for. Let us call the\nmodelparticularlylimitedtoharmonicsignals‘harmonic-\ntemporal structured model (HTM)’. Suppose the funda-\nmentallog-frequencytrajectoryduringasinglesourceac-\ntivationis expressedwith a polynomial\n¹k(t) =¹k0+¹k1t+¹k2t2+¢¢¢ (13)\n(imagine vibrato or glissando), a cutting plane of\nqk(x; t;£)atparticulartime tshallformapureharmonic\nstructureoffundamentallog-frequency ¹k(t)(seeFig. 2).\nFrequency and power of each partial in harmonic\nstructure yield continuous curves along time. Given fun-\ndamental log-frequency trajectory ¹k(t)ink-th HTM,\nfrequency trajectory of the n-th partial is ¹k(t) +logn.\nNow if each partial distribution is approximated by a\nGaussian function, which is a quite convincing modeling\nespecially when spectra are obtained by Gabor wavelet\ntransform,andsupposepowerenvelopecurveof n-thpar-\ntial is denoted by Ukn(t)(presumed to be a function that\nisnormalizable since qk(x; t;£)hasto satisfy Eq. 5),\n8k;8n;Z/x31\n-/x31Ukn(t)dt=1; (14)\nthe power density of the n-th partial in k-th HTM is ex-\npressedas a multiplication:\nUkn(t)£vkn\np\n2¼¾ke-(x-¹k(t)-logn)2\n2¾k2\n|\n{z\n}\nweightedGaussian centered at x=¹k(t)+logn¡\nn=1;¢¢¢; N¢\n(15)\nwhere ¾kdenotes the width of every partial distributionandvknis the relativepowerof n-thpartial, that satisﬁes\n8k;X\n8nvkn=1: (16)\nTherefore, the power density of k-th HTM, i.e.,\nqk(x; t;£),as a whole (see Fig. 1) becomes\nqk(x; t;£) =wkX\n8nvknUkn(t)\np\n2¼¾ke-(x-¹k(t)-logn)2\n2¾k2(17)\nwhere wkindicates the intensity of the k-th source.\nFurther, superposition of Knumber of spectral streams,\ni.e., overall density of the model for given observation\nW(x; t),shall be expressedas a sum of HTMs,\nL(x; t;£) =X\n8kqk(x; t;£)¡\nk=1;¢¢¢; K¢\n(18)\nSince developing general algorithm for music audio\nfeature extraction that appropriately works even if any in-\nstruments are used is a completely ‘blind’ problem, it is\nperhaps wise not to limit the class of the power envelope\nfunction Ukn(t)to a model valid only for a particular\nphysical sound production mechanism. Thus, the general\nmodeling of Ukn(t)is one of the core parts of this work.\nUkn(t)is supposed to be continuous, non-negative,\nconverging to zero at both ends of the time axis, adapt-\nable to any observed curves and elastic in time direction.\nFurthermore, to accomplish Eq. 12 and to satisfy Eq. 14,\nit should be differentiable and inﬁnite integrable. Find-\ning non-linear function satisfying these requirements at\nthe same time is hardly simple, however, we came up to\nformulating it with an original Gaussian kernel function,\nwhich is givenas\nUkn(t)=Y-1X\ny=0ukny\np\n2¼Áknexpµ\n-(t-¿k-yÁkn)2\n2Ákn2¶\n(19)\nwhere ¿kis the center of the forefront Gaussian, that\nshould be treated as an onset time estimate, uknyis the\n117Table1: The feature parameters that can possibly be useful for MIR systems \ndenotation\nphysicalmeanings\n/x16/x6b/x28/x74/x29\npitchtrajectory during /x6b-th source activation(0-order polynomial wouldbe a reasonable wayto use)\n/x77/x6b\nintensityof/x6b-thactivesource\n/x76/x6b/x6e\nrelativeenergyof /x6e-th partial stream (perhaps useful as a timbre feature)\n/x75/x6b/x6e/x79\ndecisiveelement characterizing the shape of powerenvelopecurveof /x6e-th partial stream\n/x1c/x6b\nonsettime of/x6b-thsource activation\n/x59/x1e/x6b\ndurationof/x6b-th source activation\nweight parameter multiplied to each kernel, allowing the\nfunctiontobeadaptabletovariousshapes(when Ákn!0\nandY! 1, this function becomes principally trans-\nformableto ﬁt anynon-negativefunctions), that satisﬁes\n8k;8n;X\n8yukny=1 (20)\nThe remarkable originality in this function is that the Y\nnumberofGaussiankernelsarecenteredwiththeequalin-\ntervaloftheircommonstandarddeviationparameter Ákn\n(see Fig. 3). It may be quite unfamiliar to ﬁnd the stan-\ndard deviation parameter Áknalso within the numerator\ninside the exponential of Gaussian. This speciﬁc feature\nmakes Ukn(t)a linear elastic function allowing various\ndurationsofnoteeventsandneverletseachkernelbeiso-\nlated, so that Ukn(t)is always ensured to be a smooth\nenvelope.\nThe parameters of the HTM to estimate, that can be\nessentially useful as acoustic features available for MIR\nsystems,arelistedinTable3.1. Onemayrealizethatmost\noftheparametersinHTMdirectlyreﬂectdecisivefeatures\ncharacterizingmusic performances.\n3.2 KERNEL SUBCLUSTERING\nAs the HTM is speciﬁed as a kernel function represen-\ntation, solving Eq. 12 can be mathematically simpliﬁed\nby further breaking down each cluster into fn; yg-labeled\nsubclusters,associated with the kernelfunctions.\nqk(x; t;£)can be simply broken down into a sum of\nfk; n; y g-labeled kerneldensity Skny(x; t;£),\nqk(x; t;£) =wkX\n8n±\nvkn\np\n2¼¾ke-(x-¹k(t)-logn)2\n2¾2\nk\n|\n{z\n}\n=Hkn(x;t)ÃX\n8yukny\np\n2¼Ákne-(t-¿k-yÁkn)2\n2Á2\nkn\n|\n{z\n}\n=Ekny(t)!²\n=X\n8nX\n8ywkHkn(x; t)Ekny(t)|\n{z\n}\n=Skny(x;t;£)(21)\nIntroducing m(n; y;k; x; t ), membership degree of the\nk-th partitioned cluster m(k;x; t)W(x; t)in the fn; yg-\nlabeledsubcluster,that satisﬁes\n8k;X\n8nX\n8ym(n; y;k; x; t ) =1; 0·m(n; y;k; x; t )·1;we havethe inequality\nJk´ZZ\nDm(k;x; t)W(x; t)logm(k;x; t)W(x; t)\nX\n8n;8ySkny(x; t;£)dxdt\n·eJk´X\n8n;8yZZ\nDm(k;x; t)m(n; y;k; x; t )W(x; t)\nlogm(k;x; t)m(n; y;k; x; t )W(x; t)\nSkny(x; t;£)dxdt(22)\nwhere the equality holds when\nm(n; y;k; x; t ) =Skny(x; t;£)\nX\n8nX\n8ySkny(x; t;£)(23)\n(the proof will be omitted since it can be easily proved\nby following the same derivation as in section 2). Us-\ning Eq. 23 as a subcluster membership degree, one can\nmakeeJ=P\n8keJkequivalent to the global cost function\nJ=P\n8kJkand minimizing eJobviously offers absolutely\nbetterprospectforyieldingthesolutiontoEq. 12thandi-\nrectly solving Eq. 12. Accordingly, the parameter update\nequation shall be derivedby\nb£=argmin\n£J\n,argmax\n£X\n8kX\n8n;8yZZ\nD´m(k;n;y ;x;t)z\n}|\n{\nm(k;x; t)m(n; y;k; x; t )W(x; t)\nlogSkny(x; t;£)dxdt(24)\n4 INTERPRETATIONAS MAP\n4.1 PRIOR DISTRIBUTIONASSUMPTION\nSuppose W(x; t)is an observed pdf and L(x; t;£) =P\n8kqk(x; t;£)is a parameter conditional pdf (i.e.,\nmodellikelihooddensity)inEq. 11,onecanalsointerpret\nour ultimate objective as being equivalent to maximizing\nexpectationofthelog-likelihood(whatisgenerallycalled\nas maximum likelihoodproblem), i.e.,\nb£ML=argmin\n£Jmk=/x62wk\n,argmax\n£*\nlogL(x; t;£)+\nW(x;t)(25)\nw.r.t. £, where h¢i­refers to as an expectation. Inter-\npreting in this way, it is natural to expand our problem to\nMAP(Maximum A Posteriori ) estimation by introducing\n118prior distributions p(£)on the parameters, so that from\nthe Bayes theorem, optimal parameters under prior con-\nstraints to estimate could be found by maximizing the ex-\npectationofthelogarithmicposteriorprobability,givenby\nb£MAP=argmax\n£*\nlogL(x; t;£) +logp(£)+\nW(x;t)(26)\nPrior distribution assumption often plays a big role in\ncontributing to prevent model estimates from overﬁtting.\nFor example, we do not of course wish for a model of\noctave errored pitch estimate that corresponds to subhar-\nmonics of the true pitch, where in this kind of situation,\nthe model tends to be estimated as an ‘abnormal’ timbre\n(such that all partial components except particular ones\nare zero). This can be, however, avoided by assuming\nprior distribution on vknso as to prevent the model from\n‘abnormal’ timbre estimates. For another example, we do\nnot indeed want several models to build a power enve-\nlope curve that is supposed to be originated from a single\nsource activation (otherwise it would be estimated as sev-\neral onset times). This situation could also be reduced by\nassumingprior distributionon ukny.\nWe apply the prior distribution proposed by Goto\n(2004),which is explicitlygivenby:\n8\n>>><\n>>>:p(vk)´1\nZvexpµ\n-dvX\n8n¯vnlog¯vn\nvkn¶\np(ukn)´1\nZuexpµ\n-duX\n8y¯uylog¯uy\nukny¶(27)\nX\n8n¯vn=1;X\n8y¯uy=1 (28)\nwhere¯rnand¯cyare the most preferred ‘expected’ val-\nues of vknandukny,dranddcare contribution de-\ngrees of the priors and ZrandZcare normalization fac-\ntors. Both p(rk)andp(ckn)take maximum value when\nvkn=¯rnandukny =¯cyfor all nandy. When\ndranddcare zero, p(rk)andp(ckn)become uniform\n(noninformative-prior)distributions. Theadvantageofus-\ning this particular form is in a considerable simpliﬁcation\nofcalculatingLagrangemultipliersinmaximizingEq. 24\nwithout affecting its substance. Note that Dirichlet distri-\nbutionis also practically applicable.\nGiven that °w,°k\nrand°kn\ncare Lagrange multipliers\nforwk,vknandukny,thuswhatwearetosolvetoderive\ntheupdate equation of £underprior constraint is\nb£MAP=argmax\n£X\n8kÃµX\n8nX\n8yZZ\nDm(k; n; y ;x; t)\nW(x; t)logSkny(x; t;£)dxdt¶\n-dvX\n8n¯vnlog¯vn\nvkn-duX\n8nX\n8y¯uylog¯uy\nukny\n-°(k)\nvµX\n8nvkn-1¶\n-X\n8n°(kn)\nuµX\n8yukny-1¶!\n-°wµX\n8kwk-1¶\n(29)4.2 DAEMALGORITHM (Uedaand Nakano,1998)\nOneofthecrucialproblemsinanytraditionaliterativepa-\nrameterestmationalgorithmsisthat,themoremodelsbe-\ncome complex, the more likely they cause the estimates\ntobetrappedintolocalminima/maxima. Therehavebeen\nmany efforts for such difﬁculty over decades in wide re-\nsearch area. For instance, Deterministic Annealing EM\n(DAEM)algorithmproposedbyUedaandNakano(1998)\nis known to be one of the effective approaches offering\nstable convergenceto global maximum/minimum.\nSo far we have shown that the iterative procedure\nof updating m(k; n; y ;x; t) =m(k;x; t)m(n; y;k; x; t )\nand£guarantees the convergence of £to a stationary\npoint. From Eqs. 10 and 23, the subcluster membership\nm(k; n; y ;x; t)should be updated to\nbm(k; n; y ;x; t) =bm(k;x; t)m(n; y;k; x; t )\n=Skny(x; t;£)\nX\n8kX\n8nX\n8ySkny(x; t;£)(30)\nwhen£iscompletelyﬁxedandthen £shouldbeupdated\nusing Eq. 29. Although this deterministic procedure is\nexpected to give appropriate convergence of £when the\ninitial point is chosen to be close to the global minimum,\nbut may often fail if it is not. Ueda and Nakano (1998)\nconsideredthatsuchcommonprobleminEMalgorithmis\nmainly due to the fact that the membership degree (miss-\ning data posterior density function) given by Eq. 30 is\noften unreliable at early stage of the iteration. They re-\nformulated EM algorithm to improve its drawback, from\nthe viewpoint of the statistical mechanics analogy. In\nplace of bm(k; n; y ;x; t), they gave the membership de-\ngreebf(k; n; y ;x; t)parameterized by ¯as\nbf(k; n; y ;x; t; ¯ ) =Skny(x; t;£)¯\nX\n8kX\n8nX\n8ySkny(x; t;£)¯:(31)\nSince in general cases, initial points are of course not al-\nways near the global solution, every cluster should share\nW(x; t)almost evenly by setting ¯¼0(where it con-\ntributes to smoothing J, that is perhaps often multimodal,\ni.e., ‘spiky’) in the early stage of the iteration and as the\niteration proceeds, Eq. 31 should approach the original\none (Eq. 30) by setting ¯=1to accomplish the primary\nobjective. To achieve this, they newly added a ¯-loop in\naddition to the traditional EM loop. DAEM-based HTM\noptimization is implemented as following:\n————————————————————\n1. Set ¯Ã¯min(0 < ¯min< 1)\n2. Set £(0),iÃ0\n3. Iterate EM-steps until con-\nvergence:\nE-step: Substitute £(i)to\nEq.31\nM-step: £(i+1)ÃEq.29\nSet iÃi+1.\n1194. Increase ¯.\n5. If ¯ < 1, repeat from step 3;\notherwise stop.\n————————————————————\n5 PARAMETERUPDATEEQUATIONS\nFor the purpose of reducing the dimension of the feature\nto extract, let us roughly assume that all pitch trajectories\nareparallel to the time axis (0-order polynomial), i.e.,\n¹k(t)¼¹k0 (32)\nand each partial stream in a HTM has similar power en-\nvelope(onlyasinglepowerenvelopefunctionisassumed\nin a single HTM so that the index ninUkn(t)shall be\nexcluded). Since our objective here is not to strictly ana-\nlyze precise music expressions, these assumptions would\nnot be fatal ﬂaws in practical situation. From Eq. 21, log-\narithmickerneldensity log Skny(x; t;£)isgivenby\nlogSkny(x; t;£) =logwkvknuky\n2¼¾kÁk\n-(x-¹k0-logn)2\n2¾2\nk-(t-¿k-yÁk)2\n2Á2\nk(33)\nsothatsolvingEq. 29,theupdateequationofeachparam-\neterat M-step is derivedas follows:\n`kny(x; t;¯)´bf(k; n; y ;x; t; ¯ )W(x; t)\nbw(i+1)\nk=X\n8n;8yZZ\nD`kny(x; t;¯)dxdt\nb¹(i+1)\nk0=X\n8n;8yZZ\nD(x-logn)`kny(x; t;¯)dxdt\nbw(i+1)\nk\nb¿(i+1)\nk=X\n8n;8yZZ\nD(t-yÁ(i)\nk)`kny(x; t;¯)dxdt\nbw(i+1)\nk\nbv(i+1)\nkn =dv¯vn+X\n8yZZ\nD`kny(x; t;¯)dxdt\ndv+bw(i+1)\nk\nbu(i+1)\nky=du¯un+X\n8nZZ\nD`kny(x; t;¯)dxdt\ndu+bw(i+1)\nk\nbÁ(i+1)\nk=-¤k+µ\n¤2\nk+4X\n8yZ\n¨ky(t)2(t-¿k)2dt¶1\n2\n2bw(i+1)\nk 0\nBBB@¤k=X\n8n;8yZZ\nDy(t-¿k)`kny(x; t;¯)dxdt\n¨ky(t) =X\n8nZ\n`kny(x; t;¯)dx1\nCCCA\nb¾(i+1)\nk =\n0\nBBB@X\n8n;8yZZ\nD(x-¹(i)\nk0-logn)2`kny(x; t;¯)dxdt\nbw(i+1)\nk1\nCCCA1\n2Table4: Experimental Conditions \nfrequency\nSamplingrate\n16kHz\nanalysis\nframeshift\n16ms\nfrequencyresolution\n12:0cent\nfrequencyrange\n60–3000Hz\nHTC\ninitial# of HTMs\n20\n#of partials: N\n6\n#of kernelsin Uk(t):Y\n10\n¯min\n0:5\n¯rn\n0:6547 £n-2\ndr; dc\n0:04\ntimerange of analyzing segment\n80frames ( 1:28s)\n#of analyzing segments\n21(total time: 24s)\nPreFEst-core\npitchresolution\n20cent\n(Goto,2004)\n#of partials\n8\n#of tone models\n200\nstandarddeviationof Gaussian\n3:0\n¯rn\n0:6547 £n-2\n¯d(prior contributionfactor)\n3:0\n6 EXPERIMENTALEVALUATION\n6.1 CONDITIONS\nTo verify the potential performance of the proposed\nmethod as an audio feature extraction application, we\ntesteditonasetofrealperformancemusicdataexcerpted\nfrom RWC music database (see table 2 for the list of\nthe experimental data). Time series of power spectrum\nwas analyzed using Gabor wavelet transform with frame\nshift of 16ms for input digital signals of 16kHz sam-\npling rate. The lower bound of the frequency range and\nthe frequency resolution were 60Hz and 12cent, respec-\ntively. The initial parameters of (¹k0; ¿kjk=1;¢¢¢; K)\nfor DAEM algorithm were automatically determined by\npicking 20largestpeaksintheobservedspectraltimepat-\ntern of 80consecutive frames. After the parameters con-\nverged, the total number of note events were estimated\nby intensity thresholding, i.e., every HTM whose wkes-\ntimate becomes smaller than the threshold was truncated.\nSee table 4 for the detailed conditions.\n6.2 CALCULATINGACCURACY\nUsing the supplementary hand-labeled reference MIDI\ndata, associated with each test data, the comprehensive\naccuracy of pitch, onset time and duration estimates was\nautomatically calculated by the followingprocedure.\n1. Truncate HTMs with intensity thresholding on wk\nestimate.\n2. Quantize pitch estimate ¹k0, onset time estimate ¿k\nand duration estimate YÁkto the closest note, frame\nand number of frames in each remaining HTM and\nthen create a framewise binary series each for 128\nnumberofnotes,where 1and0indicate‘noteactiva-\ntion’ and ‘silence’ at each frame, respectively.\n3. Convert the hand-labeled reference MIDI data to a\nreferenceframewisebinary serieseach for 128num-\nber of notes similarly where 1and0indicate ‘note\nactivation’and ‘silence’.\n4. Add up the accumulated costs, computed by Dy-\nnamicProgramming(DP)matchingbetweenthetwo\nbinary series, of 128notes. Since the onset and off-\nset times of respective note events in the real perfor-\nmance data and the reference MIDI data are not per-\n120Table2: List of The Experimental Data Excerpted from RWCMusic Database \nSymbol\nTitle(Genre)\nCatalog number\nComposer/Player\nInstruments\n#of frames\ndata(1)\nCrescentSerenade (Jazz)\nRWC-MDB-J-2001No.9\nS.Yamamoto\nGuitar\n4427\ndata(2)\nForTwo(Jazz)\nRWC-MDB-J-2001No.7\nH.Chubachi\nGuitar\n6555\ndata(3)\nJive(Jazz)\nRWC-MDB-J-2001No.1\nM.Nakamura\nPiano\n5179\ndata(4)\nLoungeAway(Jazz)\nRWC-MDB-J-2001No.8\nS.Yamamoto\nGuitar\n9583\ndata(5)\nForTwo(Jazz)\nRWC-MDB-J-2001No.2\nM.Nakamura\nPiano\n9091\ndata(6)\nJive(Jazz)\nRWC-MDB-J-2001No.6\nH.Chubachi\nGuitar\n3690\ndata(7)\nThreeGimnopedies no. 1 (Classic)\nRWC-MDB-C-2001No.35\nE.Satie\nPiano\n6571\ndata(8)\nNocturneno.2, op.9-2(Classic)\nRWC-MDB-C-2001No.30\nF.F.Chopin\nPiano\n7258\nTable 3: Accuracy results of PreFEst-core (Goto, 2004) and HTC. Columns (A) \u0018(J) and (K) \u0018(R) show the accuracies with different thresholds for PreFEst-core and\nHTC,respectively: (A) 2:0£108,(B)2:5£108,(C)5:0£108,(D)7:5£108,(E)10£108,(F)15£108,(G)17:5£108,(H)20£108,(I)25£108,(J)27:5£108,\n(K)7:5£109, (L)1:0£1010, (M)2:0£1010, (N)3:0£1010,(O)4:0£1010, (P)5:0£1010, (Q)6:0£1010,(R)7:0£1010.\nAccuracy(%)\nPreFEst-coreGoto(2004)\nHTC\n(A)\n(B)\n(C)\n(D)\n(E)\n(F)\n(G)\n(H)\n(I)\n(J)\n(K)\n(L)\n(M)\n(N)\n(O)\n(P)\n(Q)\n(R)\ndata(1)\n56.6\n62.49\n75.9\n81.6\n83.3\n84.6\n83.0\n81.5\n78.4\n75.8\n69.5\n74.8\n83.9\n84.8\n88.2\n88.8\n88.7\n85.1\ndata(2)\n68.7\n69.6\n66.3\n59.0\n53.7\n36.3\n32.4\n30.3\n26.8\n26.5\n84.3\n88.2\n90.6\n82.5\n75.7\n72.3\n67.9\n61.9\ndata(3)\n-20.8\n-7.3\n31.7\n47.8\n56.9\n65.1\n69.5\n71.9\n75.5\n71.8\n68.8\n70.0\n77.6\n80.0\n80.2\n77.4\n73.3\n73.4\ndata(4)\n55.1\n56.8\n60.7\n63.3\n63.1\n63.6\n64.1\n62.3\n60.6\n60.2\n82.6\n83.0\n83.8\n82.4\n82.8\n82.0\n81.5\n76.5\ndata(5)\n50.7\n53.2\n61.0\n60.0\n58.8\n59.3\n57.6\n58.0\n57.5\n49.7\n76.3\n79.3\n79.4\n81.7\n77.6\n76.2\n76.5\n72.8\ndata(6)\n-7.2\n6.6\n37.9\n51.1\n57.7\n65.9\n65.6\n66.7\n66.3\n65.7\n77.5\n79.6\n81.7\n82.7\n84.4\n82.3\n81.4\n80.7\ndata(7)\n51.6\n54.1\n62.7\n52.4\n47.0\n45.9\n42.7\n41.1\n42.2\n42.7\n72.1\n69.9\n70.3\n68.3\n66.9\n63.1\n61.5\n62.0\ndata(8)\n20.8\n22.9\n36.6\n42.5\n38.5\n39.1\n38.8\n37.7\n32.7\n30.6\n73.7\n75.9\n75.6\n72.2\n67.6\n61.1\n48.9\n46.7\nfectlyaligned,timewarpingtechniquewassomehow\nrequired.\n5. The accumulated cost divided by the total number\nof frames of note activation in 128sets of reference\nbinary series gives the error rate. The accuracy rate\nissimply givenby subtracting the error rate from 1.\nAccuracy (%) =A- (accumulatedcostz\n}|\n{\nIns+Del)\nA£100\nA:totalframe # of ‘note activation’\nIns:#of insertion errors\nDel :#of deletion errors\nNotethatthiscalculationcountsasubstitutionerrorasdu-\nplicated errors (one deletion and one insertion errors), so\nthatthe accuracycan possibly takenegativevalues.\n6.3 RESULTS\nWe chose1‘PreFEst-core’(Goto, 2004) for a comparison,\nas it is recently accepted as one of the most success-\nful methods developed for multi-pitch analysis. Since\nPreFEst-core extracts the most dominant pitch trajectory\nfrom multi-pitch signals and does not include a speciﬁc\nprocedure of estimating the number of sources, we in-\ncludedintensitythresholdingsimilarlyforpitchcandidate\ntruncation. A typical example of the estimated binary se-\nries extracted via step 2 mentioned in 6.2 on particular\ntest data is shown in Fig.5 together with the hand-labeled\nreference MIDI data. The optimized model and the cor-\nresponding observed spectral time pattern are shown with\n3Dand grayscale displays in Fig.4.\n1Note that we have only implemented the ‘PreFEst-core’,\ni.e., a framewise pitch likelihood estimation, for the evaluation\nand not included the ‘PreFEst-back-end’, i.e., multi-agent based\npitchtracking algorithm.\n4045505560657075\n0 200 400 600 800 1000 1200 1400note #\n# of frames (shift:16ms)\nFigure 5: Estimates of ¹k0; ¿k; YÁk(top) and the refer-\nence MIDI data displayed in piano-roll form (bottom).\nIn thresholding, there is a trade-off between the num-\nberofinsertionanddeletionerrorsaccordingtothethresh-\nold degree. Therefore, to properly validate the perfor-\nmance of the proposed method, we considered that the\nmaximal accuracy among all the thresholds that were\ntested, which might show the limit of a potential capabil-\nity,shouldbeacriterionforcomparison. Accuracyresults\nof PreFEst-core and HTC with different degrees of trun-\ncation thresholding are shown in table 3. The number in\nbold-facedtypeisthebestaccuracyineachdataamongall\nthe thresholds, which we are only concerned with. Com-\nparing these accuracies between PreFEst-core and HTC,\nHTC signiﬁcantly outperforms PreFEst-core, from which\nits potential is clearly veriﬁed.\n121(a)Observedspectraltime pattern\n(c)spectrogramof the observation\n(b)Theoptimized HTMs on (a)\n(d)Agrayscaledisplay of the optimized model\nFigure4: 3D and grayscale diplays of the givenspectrum and the parameter-optimizedmodel\n7 CONCLUSION\nWe established a new framework for multi-pitch analysis\nbased upon two dimensional geometric modeling and es-\ntimation of the distinct spectral streams localized in time-\nfrequency‘scene’,andinvestigatedpossibilitiesforanap-\nplicationof audio feature extractionavailablefor MIR.\nThe method described in this paper still has many in-\nterestingissuestoconsider,e.g.,estimationofthenumber\nof note events without relying on heuristic thresholding,\nfurther precise modeling by introducing higher order co-\nefﬁcientsinpitchtrajectorypolynomialandinharmonicity\nfactorparameterandothersforsoundsegregationornoise\nreductionapplications.\nREFERENCES\nM. Abe and S. Ando. Auditory scene analysis based on\ntime-frequency integration of shared fm and am (ii):\nOptimum time-domain integration and stream sound\nreconstruction. IEICE Trans. , J83-D-II(2):468–477,\n2000. in Japanese.\nD. Chazan, Y. Stettiner, and D. Malah. Optimal multi-\npitch estimation using the em algorithm for co-channel\nspeech separation. In Proc. ICASSP’93 , volume 2,\npages728–731, 1993.\nM.FederandE.Weinstein.Parameterestimationofsuper-\nimposed signals using the em algorithm. IEEE Trans.\nAcoustics,Speech,andSignalProcessing ,ASSP-36(4):\n477–489,1988.S. Godsill and M. Davy. Baysian harmonic models\nfor musical pitch estimation and analysis. In Proc.\nICASSP2002 , volume2, pages 1769–1772, 2002.\nM. Goto. A real-time music-scene-description system:\nPredominant-f0 estimation for detecting melody and\nbass lines in real-world audio signals. ISCA Journal ,\n43(4):311–329, 2004.\nH. Kameoka, T. Nishimoto, and S. Sagayama. Minimum\nbic estimation of harmonic kernel regression model for\nmulti-pitch analysis. IEEE Trans. Speech and Audio\nProcessing , 2005. submitted.\nK. Kashino, K. Nakadai, and H. Tanaka. Organization\nof hierarchical perceptual sounds: Music scene analy-\nsis with autonomous processing modules and a quan-\ntitive information integration mechanism. In Proc. IJ-\nCAI,volume1, pages 158–164, 1995.\nA. Klapuri, T. Virtanen, and J. Holm. Robust multipitch\nestimation for the analysis and manipulation of poly-\nphonic musical signals. In Proc. COST-G6 Conference\non Digital AudioEffects , pages 233–236, 2000.\nT.Nakatani. Computationalauditorysceneanalysisbased\non residue-driven architecture and its application to\nmixed speech recognition. Ph.D. thesis, Kyoto Univer-\nsity, 2002.\nK. Nishi, S. Ando, and S. Aida. Optimum harmonics\ntracking ﬁlter for auditory scene analysis. In Proc.\nICASSP’96 ,pages 573–576, 1996.\nN. Ueda and R. Nakano. Deterministic annealing em al-\ngorithm. NeuralNetworks ,11(2):271–282, 1998.\n122"
    },
    {
        "title": "New Music Interfaces for Rhythm-Based Retrieval.",
        "author": [
            "Ajay Kapur",
            "Richard I. McWalter",
            "George Tzanetakis"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418313",
        "url": "https://doi.org/10.5281/zenodo.1418313",
        "ee": "https://zenodo.org/records/1418313/files/KapurMT05.pdf",
        "abstract": "In the majority of existing work in music information retrieval (MIR) the user interacts with the system using standard desktop components such as the keyboard, mouse or sometimes microphone input. It is our belief that moving away from the desktop to more physically tangible ways of interacting can lead to novel ways of thinking about MIR. In this paper, we report on our work in utilizing new non-standard interfaces for MIR purposes. One of the most important but frequently neglected ways of characterizing and retrieving music is through rhythmic information. We concentrate on rhythmic information both as user input and as means for retrieval. Algorithms and experiments for rhythm-based information retrieval of music, drum loops and indian tabla thekas are described. This work targets expert users such as DJs and musicians which tend to be more curious about new technologies and therefore can serve as catalysts for accelerating the adoption of MIR techniques. In addition, we describe how the proposed rhythm-based interfaces can assist in the annotation and preservation of perfomance practice. Keywords: user interfaces, rhythm analysis, controllers, live performance 1",
        "zenodo_id": 1418313,
        "dblp_key": "conf/ismir/KapurMT05",
        "keywords": [
            "user interfaces",
            "rhythm analysis",
            "controllers",
            "live performance",
            "keyboard",
            "mouse",
            "microphone",
            "drum loops",
            "Indian tabla",
            "performance practice"
        ],
        "content": "NEW MUSIC INTERFACES FOR RHYTHM-BASED RETRIEVAL\nAjay Kapur\nUniversity of Victoria\n3800 Finnerty Rd.\nVictoria BC, Canada\najay@ece.uvic.caRichard I. McWalter\nUniversity of Victoria\n3800 Finnerty Rd.\nVictoria BC, Canada\nrmcwalte@ece.uvic.caGeorge Tzanetakis\nUniversity of Victoria\n3800 Finnerty Rd.\nVictoria BC, Canada\ngtzan@cs.uvic.ca\nABSTRACT\nIn the majority of existing work in music information re-\ntrieval(MIR)theuserinteractswiththesystemusingstan-\ndard desktop components such as the keyboard, mouse or\nsometimes microphone input. It is our belief that moving\naway from the desktop to more physically tangible ways\nof interacting can lead to novel ways of thinking about\nMIR. In this paper, we report on our work in utilizing\nnew non-standard interfaces for MIR purposes. One of\nthemostimportantbutfrequentlyneglectedwaysofchar-\nacterizing and retrieving music is through rhythmic infor-\nmation. We concentrate on rhythmic information both as\nuser input and as means for retrieval. Algorithms and ex-\nperiments for rhythm-based information retrieval of mu-\nsic,drumloopsandindiantablathekasaredescribed. This\nworktargetsexpertuserssuchasDJsandmusicianswhich\ntendtobemorecuriousaboutnewtechnologiesandthere-\nforecanserveascatalystsforacceleratingtheadoptionof\nMIR techniques. In addition, we describe how the pro-\nposedrhythm-basedinterfacescanassistintheannotation\nand preservation of perfomance practice.\nKeywords: user interfaces, rhythm analysis, con-\ntrollers, live performance\n1 INTRODUCTION\nMusical instruments are fascinating artifacts. For thou-\nsands of years, humans have used all sorts of different\nmaterials including wood, horse hairs, animal hides, and\nbones to manufacture a wide variety of musical instru-\nments played in many different ways. The strong cou-\npling of the musicians’ gestures with their instruments\nwas taken for granted for most of music history until the\ninvention of recording technology which made it possible\nto listen to music without the presence of performers and\ntheir instruments.\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of LondonMusic Information Retrieval (MIR) has the potential\nof revolutionizing the way music is produced, archived\nand consumed. However, most of existing MIR systems\nare prototype systems developed in academia or research\nlabs that have not yet been empbraced by the public. One\nof the possible reasons is that so far most of existing sys-\ntemshavefocusedontwotypesofusers: 1)usersthathave\nknowledge of western common music notation such as\nmusicologists and music librarians or 2) “average users”\nwho are not necessarily musically-trained. This bias is\nalsoreﬂectedintheselectionofproblemstypicallyinvolv-\ningcollectionsofeitherwesternclassicalmusicorpopular\nmusic.\nInthispaper,weexploretheuseofnon-standardinter-\naction devices for MIR. These devices are inspired by ex-\nisting musical instruments and are used for both retrieval\nand browsing. They attempt to mimic and possibly lever-\nage the tangible interaction of performers with their in-\nstruments. Therefore the target users are musicians and\nDJswhichinourexperiencetendtobemorecuriousabout\nnew technologies and therefore can serve as catalysts for\naccelerating the adoption of MIR techniques.\nAlthough the main ideas we propose are applicable to\nany type of instrument-inspired interaction and music re-\ntrieval tasks the focus of this paper is the use of rhyth-\nmic information both as user input and as means for re-\ntrieval. Rhythmisfundamentalinunderstandingmusicof\nany type and provided us with a common thread behind\nthis work. The described interfaces are inspired by ex-\nisting musical instruments and have their origins in com-\nputermusicperformance. Theyutilizeavarietyofsensors\nto extract information from the user. This information is\nsubsequentlyusedforbrowsingandretrievalpurposes. In\naddition these sensor-enhanced instruments can be used\nto archive performance-related information which is typi-\ncallylostinaudioandsymbolicrepresentations. Theinte-\ngration of the interfaces with MIR algorithms into a pro-\ntotype system is described and experimental results using\ncollections of drum loops, tabla thekas and music pieces\nof different genres are presented. Some reprsentative sce-\nnarios are provided to illustrate how the proposed inter-\nfaces can be used. It is our hope that these interfaces will\nextendMIRbeyondthestandarddesktop/keyboard/mouse\ninteraction into new contexts such as practicing musical\ninstruments, live performanceand the dance hall.\n1302 RELATED WORK\nTherearetwomainareasofrelatedwork: 1)non-standard\ntangible user interfaces for information retrieval and 2)\nrhythm analysis and retrieval systems.\nUsingsensor-baseduserinterfacesforinformationre-\ntrieval is a new and emerging ﬁeld of study. The bricks\nproject (Fitzmaurice et al., 1995) at MIT is an early ex-\nample of a graspable user interface being used to control\nvirtual objects, such as objects in a drawing program.\nOne of the most inspiring interfaces for our work is\nmusicBottles , a tangible interface designed by Hiroshi\nIshiiandhisteamattheMITMediaLab. Inthiswork,bot-\ntlescanbeopenedandclosedtoexploreamusicdatabase\nof classical, jazz, and techno music (Ishii, 2004). Ishii\nelegantly describes in his paper his mother’s expertise in\n”everyday interaction with her familiar physical environ-\nment - opening a bottle of soy sauce in the kitchen.” His\nteamthusbuiltasystemthattookadvantageofthisexper-\ntise, so that his mother could open a bottle and hear birds\nsinging to know that tomorrow would be a sunny, beau-\ntiful day, rather than having to use a mouse, keyboard to\ncheck the system online. This work combines a tangible\ninterface with ideas from information retrieval. Similarly\nin our work we try to use existing interaction metaphors\nfor music information retrieval tasks.\nThe use of sensors to gather gestural data from a mu-\nsician has been used as an aid in the creation of real-time\ncomputer music performance. Examples of such systems\ninclude: the Hypercello (Machover, 1992), and the digi-\ntizedjapanesedrumAobachi(YoungandFujinaga,2004).\nAlso there has been some initial research on using in-\nterfaces with music information retrieval for live perfor-\nmanceonstage. AudioPad(Pattenetal.,2002)isaninter-\nfacedesignedatMITMediaLab,whichcombinestheex-\npressive character of multidimensional tracking with the\nmodularity of a knob-based interface. This is accom-\nplished by using embedded LC tags inside a puck-like in-\nterface which is tracked in two dimensions on a tabletop.\nIt is used to control parameters of audio playback, act-\ning as a new interface for the modern disk jockey. Block\nJam (Newton-Dunn et al., 2003) is an interface designed\nby Sony Research which controls audio playback with\nthe use of 25 blocks. Each block has a visual display\nand a button-like input for event driven control to con-\ntrol functionality. Sensors within the blocks allow for\ngesture-basedmanipulationoftheaudioﬁles. Researchers\nfrom Sony CSL Paris proposed SongSampler (Aucou-\nturier et al., 2004), a system which samples a song and\nthenusesaMIDIinstrumenttoperformthesamplesfrom\nthe original sound ﬁle.\nThere has also been some work in using rhythmic in-\nformation for MIR. The use of BeatBoxing , the art of vo-\ncal percussion, as a query mechanism for music informa-\ntionretrievalwasproposedbyKapuretal.(2004a). Asys-\ntem which classiﬁed and automatically identiﬁed individ-\nual beat boxing sounds, mapping them to corresponding\ndrumsampleswasdeveloped. Asimilarconceptwaspro-\nposed by Nakano et al. (2004) in which the team created\na system for voice percussion recognition for drum pat-\ntern retrieval. Their approach used onomatopoeia as the\ninternal representation of drum sounds which allowed fora larger variation of vocal input with an impressive iden-\ntiﬁcation rate. Gillet and Richard (2003) explore the use\nofthevoiceasaquerymechanisminthedifferentcontext\nofIndiantablamusic. Asystemfor Query-by-Rhythm was\nintroducedbyChenandChen(1998). Rhythmisstoredas\nstrings turning song retrieval into a string matching prob-\nlem. TheauthorsproposeanL-treedatastructureforefﬁ-\ncient matching. The similarity of rhythmic patterns using\na dynamic programming approach is explored by Paulus\nand Klapuri (2002). A system for the automatic descrip-\ntion of drum sounds using a template adaption method is\nYoshii et al. (2004).\n3 INTERFACES\nThe musical interfaces described in this paper are tan-\ngible devices used to communicate musical information\nthrough gestural interaction. They enable a more rich\nand musical interaction than the standard keyboard and\nmouse. These interfaces use modern sensor technology\nsuch as force sensing resistors, accelerometers, infrared\ndetectors,andpiezoelectricsensorstomeasurevariousas-\npects of the human-instrument interaction. Data is col-\nlected by an onboard microprocessor which converts the\nsensordataintoadigitalprotocolforcommunicatingwith\nthe computer used for MIR. Currently, the MIDI message\nprotocol is utilized.\n3.1 Mercurial STC1000\nThe Mercurial STC1000 (shown in Figure 1 (A))1uses\na network of ﬁberoptic sensors to detect pressure as posi-\ntion on a two-dimensional plane. It has been designed by\nthe Mercurial Innovations Group. This device is a singe\ntouchcontrollerthatdirectlyoutputsMIDImessages. The\nmappingtoMIDIcanbecontrolledbytheuser. Theactive\npad area is 125mm X 100mm (5 X4 inches).\n3.2 Radio Drum\nThe Radio Drum/Baton, shown in Figure 1 (B), is one\nof the oldest electronic music controllers (Mathews and\nSchloss, 1989). Built by Bob Boie and improved by Max\nMathews, it has undergone a great deal of improvement\nin accuracy of tracking, while the user interaction has re-\nmainedrelativelyconstant. Thedrumgenerates6separate\nanalog signals that represent the current x, y, z position\nof each stick. The radio tracking is based on measuring\nthe electrical capacitance between the coil at the end of\neachstickandthearrayofreceivingantennasonthedrum\n(one for each corner). The analog signals are converted\nto MIDI messages by a microprocessor. The sensing sur-\nface measures approximately 375mm X 600mm (15 X 24\ninches).\n3.3 ETabla\nThe traditional tabla are a pair of hand drums used to\naccompany North Indian vocal and instrumental music.\nThe Electronic Tabla Controller (ETabla) (Kapur et al.,\n1http://www.thinkmig.com/stc1000.html\n131Figure 1: (A) Mercurial STC 1000 and (B) Radio Drum\nFigure 2: (A) ETabla (B) ESitar (C) EDholak\n2004b) (shown in Figure 2 (A)) is a custom built con-\ntroller that uses force sensing resistors to detect different\nstrokes, strike velocity and position. Though an acousti-\ncally quiet instrument, it adheres to traditional technique\nand form. The ETabla is designed to allow performers to\nleverage their skill to control digital information.\n3.4 EDholak\nThe Electronic Dholak Controller (EDholak) (Kapur\net al., 2004b) is another custom built controller using\nforcesensingresistorsandpiezoelectricsensorstocapture\nrhythmicgestures. TheDholakisanIndianfolkdrumper-\nformed by two players. Inspired by the collaborative na-\ntureofthetraditionaldrum,theEDholak(showninFigure\n2(C))isatwoplayerelectronicdrum,whereonemusician\nprovides the rhythmic impulses while the other provides\nthetempoandcontrolsthesoundproductionoftheinstru-\nment using a sensor-enhanced spoon. This interface, in\naddition to sending sensor data, produces actual sound in\nthe same way as it’s traditional counterpart.\n3.5 ESitar\nThe Electronic Sitar Controller (ESitar) (Kapur et al.,\n2004b) is a hyperinstrument designed using a variety of\nsensor techniques. The Sitar is a 19-stringed, pumpkin\nshelled, traditional North Indian instrument. The ESitar\n(shown in Figure 2 (B)) modiﬁes the acoustic sound of\nthe performance using the gesture data deduced from the\nsensors,aswellasservingasareal-timetranscriptiontool\nthat can be used as a pedagogical device. The ESitar ob-\ntains rhythmic information from the performer by a force\nsensing resistor placed under the right thumb, deducing\nstroke direction and frequency.\nFigure 3: “Boom-chick” onsetdetection\n4 RHYTHM ANALYSIS AND\nRETRIEVAL\nThe proposed interfaces generate essentially symbolic\ndata. However our goal is to utilize this information to\nbrowse and retrieve from collections of audio signals.\nTherefore a rhythmic analysis front-end is used to con-\nvertaudiosignalstomorestructuredsymbolicrepresenta-\ntions. This front-end is based on decomposing the signal\ninto different frequency bands using a Discrete Wavelet\nTransform (DWT) similarly to the method described by\nTzanetakis and Cook (2002) for the calculation of Beat\nHistograms . Theenvelopeofeachbandisthencalculated\nbyusingfullwaverectiﬁcation,lowpassﬁlteringandnor-\nmalization. In order to detect tempo and beat strength the\nBeat Histogram approach is utilized.\nIn order to extract more detailed information we per-\nform what we term “boom-chick” analysis. The idea is\nto detect the onset of low frequency events typically cor-\nresponding to bass drum hits and high frequency events\ntypically corresponding to snare drum hits. This is ac-\ncomplishedbyonsetdetectionusingadaptivethresholding\nand peak picking on the amplitude envelope of two of the\nfrequency bands of the wavelet transform (approximately\n300Hz-600Hzforthe“boom”and2.7KHz-5.5KHzforthe\n“chick”). Figure 3showshowadrumloopcanbedecom-\nposed into “boom” and “chick” bands and corresponding\ndetected onsets. Even though more sophisticated algo-\nrithms for tempo extraction and drum pattern detection,\nsuch as the ones mentioned in the related work section,\nhave been proposed, the above approach worked quite\nwell and provide us with the necessary infrastructure for\nexperimenting with the new interfaces.\nThe onset sequences of “boom-chick” events can be\nconverted into a string representation for retrieval pur-\nposes. Onceonsettimesarefound,thefollwoingtworep-\nresentations are created:\nChick array --C---C-----C-C----\nBoom array: B---B-B---B---B---B\n132The next step is to combine the two representations\ninto one. If a Bass and Snare occur at the same time, then\nT represents B+C:\nCombined Array: B-C-BCB---T-C-B---B\nThis composite representation array is then used to\ncreate a string combining the type of onset with dura-\ntions between each event. There are six types of transi-\ntions that are labeled with durations: BC, CB, TC, CT,\nBT. Durations are relative (similar to common music no-\ntation). They are calculated by picking the most common\ninter-onset interval (IOI) using clustering and expressing\nallotherIOI’safterquantizationasratiostothemostcom-\nmon one. Typically the most common IOI corresponds to\neighthnotesorquarternotes. Thisrepresentationisinvari-\nant to tempo. The quantized IOIs form essentially a dic-\ntionaryofpossiblerhythmicdurations(anhierarchyof5-6\ndurations is typically adaquate). In order to represent the\n“boom-chick” events these durations are combined with\nthe 6 possible transitions to form an alphabet. For exam-\nple a full beat string can berepresented as:\n{BC2,CB2,BC1,CB1,BT4,TC2,CB2,BB4}\nIn this string representation each triplet essentially\ncorrespondstoonecharacterinthealphabet. Thesestrings\ncan then be compared using standard approximate string\nmatching algorithms such as Dynamic Programming. Al-\nthoughstraightforward,thisapproachworksquitewellfor\nmusic with strong beats which is the focus of this work.\n5 SCENARIOS\nIn this section, we illustrate using scenarios some of the\nways the proposed interfaces can be used for MIR. These\nscenarios have been implemented as proof-of-concept\nprototypesandarerepresentativeofwhatispossibleusing\nour system. They also demonstrate the interplay between\nannotation, retrieval and browsing that is made possible\nby non-standard MIR interfaces. Initial reception by mu-\nsicians and DJs of our prototypes has been encouraging.\n5.1 Tapping Tempo-based Retrieval\nOne of the most basic reactions to music is tapping. Any\nof the proposed drum interfaces can be used to generate a\ntempo-based query. Even in this simple fundamental in-\nteraction, being able to tap the rhythm using a stick or a\nﬁnger on a surface is preferable to clicking a mouse but-\nton or keyboard key. The query tempo can directly be\nmeasuredandcomparedtoadatabaseoftempo-annotated\nmusic pieces or drum loops. The annotation can be per-\nformedmanuallyorautomaticallyusingaudioanalysisal-\ngorithms. Moreover, tempo annotation can also be done\nusingthesameprocessasthequeryspeciﬁcation. Tempo-\nbasedretrievalisalsousefulfortheDJ,savingtimebynot\nhaving to thumb through boxes of vinyl, or scroll through\nhundreds of mp3s for a song that is at a particular tempo.\nTapping the tempo into an interface is more convenient\nbecause the DJ can be listening to a particular song and\njust tap to ﬁnd all ﬁles that match, rather than having to\nmanually beat match.5.2 “Boom-Chick” Rhythmic Retrieval\nA slightly more complex way of using rhythm-based in-\nformation and the proposed interfaces is what we term\n“boom-chick” retrieval. In this approach rhythmic in-\nformation is abstracted into a sequence of low fre-\nquency (bass-drum like) events and medium-high fre-\nquency (snare-drum like) events. Although simple, this\nrepresentationcapturesthebasicnatureofmanyrhythmic\npatterns especially in dance music.\nThe audio signals (drum loops, music pieces) in the\ndatabase are analyzed into “boom-chick” events using\nthe rhythm-based analysis algorithms described in sec-\ntion 4. The symbolic sensor data is easier to convert\ninto a “boom-chick” representation. Using the EDho-\nlak interface is ideal for this application. The ﬁrst mu-\nsician taps out a query beat. One piezo sensor represents\na ”Boom” low event, while a separate piezo sensor rep-\nresents a ”Chick” high event. The query is matched with\nthe patterns in the database and the most similar results\narereturned. Thesecondmusiciancanthenusethedigital\nspoontobrowseandselectthedesiredrhythmataparticu-\nlartempo. Alsotimestretchingtechniquessuchasresam-\npling and phasevocoding can also be used for changing\nthe tempo of the returned resultusing the spoon.\n5.3 Rhythm-based Browsing\nThis scenario focuses on browsing a collection of drum\nsamplesormusicalpiecesratherthanretrieval. TheRadio\nDrum or the STC-1000 can be used. One axis of the sur-\nfaceismappedtotempoandtheotheraxiscanbemapped\nto some other attribute. We have experimented with au-\ntomatically extracted beat strength, genre and dance style\nfor the second axis. The pressure (STC-1000) or the stick\nheight is used for volume control. With this system a DJ\ncanﬁndsongsataparticulartempoandstylejustbyplac-\ning the ﬁnger or stick at the appropriate location. If there\narenoﬁlesordrumloopsattheappropriatetempothesys-\ntemlooksfortheclosestmatchandthenusestimestretch-\ning techniques such as overlap-add or phasevocoding to\nadjust the piece to the desired tempo. Sound is constantly\nplayingprovidingdirectfeedbacktotheuser. Thismethod\nprovidesatangibleexploratorywayoflisteningtocollec-\ntions of music rather than the tedious playlist-play button\nmodel of existing music players.\n5.4 Sensor-based Tabla Theka Retrieval\nA more advanced scenario is to use the ETabla controller\nto play a tabla pattern and retrieve a recorded sample, po-\ntentially played by a professional musician. This can be\nusedduringliveperformanceandforpedagogicalapplica-\ntions. There are 8 distrinct tabla basic strokes detected by\nthe ETabla. The database is populated either by symbolic\npatternsusingtheETablaorbyaudiorecordingsanalyzed\nsimilarly to Tindale et al. (2005). This approach can also\nbe utilized to form queries if an ETabla is not available.\nOne of our goals is to be able to automatically determine\nthe type of thekabeing played. Thekaliterally means cy-\ncle and we consider 4 types: TinTaal (16 beats/cycle),\nJhapTaal (10 b/c), Rupak (7 b/c),and Dadra (6 b/c).\n1335.5 Perforamance Annotation\nOne of the frequent challenges in developing audio anal-\nysis algorithms is ground-truth annotation. In tasks such\nas beat tracking, annotation is typically performed by lis-\ntening to the recorded audio signal. An interesting possi-\nbility enabled by sensor-enhanced interfaces is to directly\nprovide the annotation while the music is being recorded.\nThis is also important for preserving performance-related\ninformation which is typically lost in symbolic and audio\nrepresentations. Finally, even when listening to music af-\nter the fact these interfaces can facilitate annotation. For\nexample it is much easier for a tabla player to annotate\na particular thekaby simply playing along with it on the\nETabla rather than having to use the mouse or keyboard.\n5.6 Automatic Tabla Accompaniment Generation\nfor the ESitar\nTheclosestscenariotoaninteractiveperformancesystem\nis based on the ESitar controller. In the playing of the\nsitar rhythm information is conveyed by the direction and\nfrequency of the stroke. The thumb force sensor on the\nESitar controller is used to capture this rhythmic infor-\nmationcreatingaquery. Thequeryisthenmatchedintoa\ndatabaseinordertotoprovideanautomatically-generated\ntabla accompaniment. The accompaniment is generated\nbymatchingtherhythmicinformationintoadatabasecon-\ntaining variations of different thekas. We hope to use this\nprototype system in the future as a key component of live\nhuman-computer music performances.\n6 EXPERIMENTS\nIn order for the rhythm-based matching using dynamic\nprogramming the detected “boom-chick” onsest must be\naccurate. Anumberofexperimentswereconductedtode-\ntermine the accuracy of the ”Boom-Chick” detection al-\ngorthmdescribedinsection 4. Thedataconsistsofaudio\ntracks with strong beats which is the focus of this work.\n6.1 Data Collection\nThree sample data sets were collected and utilized. They\nconsist of techno beats, tabla thekasand music clips.\nThe techno beats and tabla thekaswere recorded using\nDigiDesignDigi002ProToolsatasamplingrateof44100\nHz. The techno beats were gathered from Dr. Rex in Pro-\npellerheads Reason. Four styles (Dub, House, Rhythm &\nBlues, Drum & Bass) were recorded (10 each) at a tempo\nof 120 BPM. The tabla beats were recorded with a pair\nof AKG C1000s to obtain stereo separation of the differ-\nent drums. Ten of each of four ”thekas” (meaning beats\npercycle)wererecorded(TinTaalTheka(16),JhaapTaal\nTheka(10),RupakTheka(7),DadraTheka(6)). Themu-\nsic clips were downsampled to 22050 Hz and consist of\njazz,funk,pop/rockanddancemusicwithstrongrhythm.\nA large collections of similar composition was used for\ndeveloping the prototype systems used in the scenarios.\nFigure 4: Tabla theka experimental results\n6.2 Experimental results\nThe evaluation of the system was performed by compara-\ntive testing between the actual and detected beats by two\ndrummers. Afterlisteningtoeachtrack,falsepositiveand\nfalsenegativedrumhitsweredetectedseperatelyforeach\ntype (“boom” and “chick”). False positives are the set of\ninstances in which a drum hit was detected but did not\nactually occur in the original recording. False negatives\nare the set of instances where a drum hit occurs in the\noriginalrecordingbutisnotdetectedautomaticallybythe\nsystem. In order to determine consistency in annotation,\nﬁve random samples from each dataset were analyzed by\nboth drummers. The results were found to be consistent.\nThe results are summarized using the standard preci-\nsion and recall measures. Precision measures the effec-\ntiveness of the algorithm by dividing the number of cor-\nrectly detected hits (true positives) by the total number of\ndetected hits (true positives + false positives). Recall rep-\nresentstheaccuracyofthealgorithmbydividingthenum-\nber of correctly detected hits (true positives) by the total\nnumber of actual hits in the original recording (false neg-\natives + true positive). Recall can be improved by low-\nering precision and vice versa. A common way to com-\nbine these two measures is the so called F-measure de-\nﬁned as (P is precision, R is recall and higher values of\nthe F-measure indicate betterretrieval performance):\nF=2∗P∗R\nP+R(1)\nIn our ﬁrst experiment, the accuracy of our algorithm\nontheReasondrumloopswastested. Asseeninﬁgure 5\nHouse beats have almost 99%F-measure accuracy. This\nis explained by the fact that house beats generally have a\nsimplebasspatternofonehitoneachdownbeat. Forbass\ndrum detection the hardest style was Rhythm & Blues.\nThis can be explained by the largest number of bass hits,\nwhich were often located close to each other. The snare\ndrum detection worked well indepenedently of style. One\nproblem we noticed was that some bass hits would be de-\ntected as snare hits as well.\n134Figure 5: Beat loop experimental results\nFigure 6: Overall results\nThe second experiment was conducted on the Tabla\nrecordings. This time, instead of detecting bass and snare\nhits, ”ga” stroke ( the lowest frequency hit on the bayan\ndrum) and ”na” and ”ta” strokes (high frequency hits on\nthedahinadrum)(Kapuretal.,2004b)aredetected. From\nﬁgure 4 it can be seen that the “ga” stroke was detected\nwith high accuracy comparedto the dahina strokes.\nThe ﬁnal experiment consisted of the analysis of 15\nmusic tracks separated into 3 subgenres; Dance, Funk,\nand Other. The Dance music results were fairly incon-\nsistent,witharangefrom 40to100%forrecallandpreci-\nsion. The bass drum hits were overall more accurate than\nthose found for the snare hits. The Funk music was more\nconsistent, though held the same overall accuracy as the\nDance music. The ﬁnal category, Other, which consisted\nof Rock, Pop and Jazz tracks was more dependent on the\nindividaul track than the genre. If a pronounced bass and\nsnare drum was present, the algorithm was quite success-\nfulindetection. Theaccuracyoftheseresultswassigniﬁ-\ncantly less than those found in Funk and Jazz. As seen in\nﬁgure 6 the algorithm did not work as well on the these\nmusicﬁlesasitdidonthebeatsandtabladatasets. Thisis\nduetotheinterferenceofvoices,guitars,saxophones,etc.Table 1: “Chick” hit detection results\nCategory Recall Precision F-measure\nRnb 0.844 0.878 0.861\nDnb 0.843 0.891 0.866\nDub 0.865 0.799 0.831\nHse 0.975 0.811 0.886\nAverage 0.882 0.845 0.861\nDadra 0.567 1.000 0.723\nRupak 0.662 1.000 0.797\nJhaptaal 0.713 1.000 0.833\nTintaal 0.671 0.981 0.727\nAverage 0.653 0.995 0.787\nVarious 0.699 0.554 0.618\nDance 0.833 0.650 0.730\nFunk 0.804 0.621 0.701\nAverage 0.779 0.609 0.683\nTable 2: “Boom” hit detection results\nCategory Recall Precision F-measure\nRnb 0.791 0.956 0.866\nDnb 0.910 0.914 0.912\nDub 0.846 0.964 0.901\nHse 0.967 0.994 0.980\nAverage 0.879 0.957 0.915\nDadra 0.933 0.972 0.952\nRupak 1.000 0.763 0.865\nJhaptaal 0.947 0.981 0.963\nTintaal 0.843 0.965 0.900\nAverage 0.931 0.920 0.920\nVarious 0.745 0.803 0.773\nDance 0.823 0.864 0.743\nFunk 0.863 0.820 0.841\nAverage 0.810 0.829 0.819\n7 SYSTEM INTEGRATION\nInordertointegratetheinterfaceswiththemusicretrieval\nalgorithms and tasks we developed IntelliTrance an ap-\nplication written using the MARSYAS2which is a soft-\nware framework for prototyping audio analysis and syn-\nthesis applications. The graphical user interface is writ-\nten using the Qttoolkit3.IntelliTrance is based on an\ninterface metaphor of a DJ console as shown in Figure\n7. It introduces a new level of DJ control and function-\nality for the digital musician. Based on the standard 2-\nturntable design, this software driven system operates on\ntheprinciplesofmusicinformationretrieval. Theuserhas\nthe ability to analyze a library of audio ﬁles and retrieve\nany sound via an array of preset MIDI interfaces includ-\ning the ones described in this paper. The functionality of\nIntelliTrance can be found in the main console window.\nThe console offers 2 decks, consisting of 4 independently\ncontrolledchannelswithloadfunction,volume,muteand\nsolo. Each deck has a master fader for volume control,\ncrossfadertocontrolthetheiramplituderelationship,and\n2http://marsyas.sourceforge.net\n3http://www.trolltech.com/products/qt\n135Figure 7: IntelliTrance Graphical User Interface\na main fader for the audio output. The MIR portion of\neach track allows for the retrieval of audio ﬁles for pre-\nview. Once the desired audio ﬁle is found from the re-\ntrieval, it can be sent to the track for looped playback.\nIntelliTrance offers save functionality to store the audio\ntracks and levels of the current session and load the same\nsettings at a later date.\n8 CONCLUSIONS AND FUTURE WORK\nThe experimental results show an overall high accuracy\nfortheanalysisofaudiosamplesforselectedtracksusing\nthe “Boom-Chick” algorithm. IntelliTrance has a strong\nfocus on music with a pronounced beat therefore these\nexperimental results demonstrate the potential of our ap-\nproach. Theproposedinterfacesenablenewwaysofinter-\naction with music retrieval systems that leverage existing\nmusic experience. It is our hope that such MIR interfaces\nwill be commonplace in thefuture.\nThere are various directions for future work. Integrat-\ning more interfaces into our system such as BeatBoxing\n(Kapur et al., 2004a) is an immediate goal. In addition,\nwe are building a custom controller for user interaction\nwith theIntelliTrance GUI. Another interesting possibil-\nityistheintegrationofalibraryofdigitalaudioeffectsand\nsynthesis tools into the system, to allow more expressive\ncontrol for musicians. We are also working on expanding\nour system to allow it to be used in media art installa-\ntions where sensor-based environmental data can inform\nretrieval of audio and video.\nACKNOWLEDGEMENTS\nWewouldliketothankthestudentoftheCSC484Music\nInformation Retrieval course at University of Victoria for\ntheir ideas, support and criticism during the development\nof this project. A special thanks to Stuart Bray for his\nhelp with QT GUI design. Thanks to Peter Driessen and\nAndrew Schloss for their support.REFERENCES\nJ. J. Aucouturier, F. Pachet, and P. Hanappe. From sound\nsamplingtosongsampling.In Proc.Int.Conf.onMusic\nInformationRetrieval(ISMIR) ,Barcelona,Spain,2004.\nJ. Chen and A. Chen. Query by rhythm: An approach for\nsong retrieval in music databases. In Proc. Int. Work-\nshop on Research Issues in Data Engineering , 1998.\nG. W. Fitzmaurice, H. Ishii, and W. Buxton. Bricks: Lay-\ning the foundations for graspable user interfacesb. In\nProc. Human Factors in Computer Systems , 1995.\nO. Gillet and G. Richard. Automatic labelling of tabla\nsymbols. In Proc. Int. Conf. on Music Information Re-\ntrieval (ISMIR) , Baltimore, USA, 2003.\nH. Ishii. Bottles: A transparent interface as a tribute to\nmark weiser. IEICE Transactions on Information and\nSystems, E87-D(6), 2004.\nA. Kapur, M. Benning, and G. Tzanetakis. Query-by-\nbeatboxing: Music information retrieval for the dj. In\nProc. Inter. Conf. on Music Information Retrieval (IS-\nMIR), Barcelona, Spain, 2004a.\nA. Kapur, P. Davidson, P. Cook, P. Driessen, and\nA. Schloss. Digitizing north indian performance. In\nProc. Inter. Computer Music Conf. (ICMC) , Miami,\nFlorida, 2004b.\nT.Machover. Hyperinstruments: Aprogressreport. Tech-\nnical report, MIT, 1992.\nM. Mathews and W. A. Schloss. The radio drum as a\nsynthesizer controller. In Proc. Inter. Computer Music\nConf. (ICMC) , 1989.\nT. Nakano, J. Ogata, M. Goto, and Y. Hiraga. A drum\npattern retrieval method by voice percussion. In Proc.\nInter.Conf.onMusicInformationRetrieval ,Barcelona,\nSpain, 2004.\nH. Newton-Dunn, H. Nakono, and J. Gibson. Block jam:\nAtangibleinterfaceforinteractivemusic. In Proc.New\nInterfaces for Musical Expression(NIME) , 2003.\nJ. Patten, B. Recht, and H. Ishii. Audiopad: A tag-based\ninterface for musical performance. In Proc. New Inter-\nfaces for Musical Expression (NIME) , 2002.\nJ. Paulus and A. Klapuri. Measuring the similarity of\nrhythmic patterns. In Proc. Inter. Conf. on Music In-\nformation Retrieval (ISMIR) , Paris, France, 2002.\nA. R. Tindale, A. Kapur, W. A. Schloss, and G. Tzane-\ntakis. Indirect acquisition of percussion gestures using\ntimbre recognition. In Proc. Conf. on Interdisciplinary\nMusicology (CIM) , Montreal, Canada, 2005.\nG. Tzanetakis and P. Cook. Musical Genre Classiﬁcation\nof Audio Signals. IEEE Trans. on Speech and Audio\nProcessing , 10(5), July 2002.\nK. Yoshii, M. Goto, and H. Okuno. Automatic drum\nsound description for real world music using template\nadaption and matching methods. In Proc. Int. Conf. on\nMusic Information Retrieval(ISMIR) , 2004.\nD. Young and I. Fujinaga. Aobachi: A new interface for\njapanese drumming. In Proc. New Interfaces for Musi-\ncal Expression (NIME) , Hamamatsu, Japan, 2004.\n136"
    },
    {
        "title": "Content-Based Music Information Retrieval in Wireless Ad-Hoc Networks.",
        "author": [
            "Ioannis Karydis",
            "Alexandros Nanopoulos",
            "Apostolos N. Papadopoulos",
            "Dimitrios Katsaros 0001",
            "Yannis Manolopoulos"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417665",
        "url": "https://doi.org/10.5281/zenodo.1417665",
        "ee": "https://zenodo.org/records/1417665/files/KarydisNPKM05.pdf",
        "abstract": "This paper, introduces the application of Content-Based Music Information Retrieval (CBMIR) in wireless ad-hoc networks. We investigate for the first time the challenges posed by the wireless medium and recognise the factors that require optimisation. We propose novel techniques, which attain a significant reduction in both response times and traffic, compared to naive approaches. Extensive experimental results illustrate the appropriateness and efficiency of the proposed method in this bandwidth-starving and volatile, due to mobility, environment. Keywords: music information retrieval, content-based similarity, wireless ad-hoc, P2P. 1",
        "zenodo_id": 1417665,
        "dblp_key": "conf/ismir/KarydisNPKM05",
        "keywords": [
            "Content-Based Music Information Retrieval",
            "wireless ad-hoc networks",
            "challenges posed by the wireless medium",
            "optimisation of factors",
            "novel techniques",
            "significant reduction in response times",
            "traffic",
            "bandwidth-starving environment",
            "volatility due to mobility",
            "extensive experimental results"
        ],
        "content": "Content-Based Music Information Retrieval in Wireless Ad-hoc Networks\nIoannis Karydis Alexandros Nanopoulos\nDimitrios Katsaros Yannis Manolopoulos\nAristotle University, Thessaloniki 54124, Greece\nkarydis,alex,apostol,dimitris,manolopo@delab.csd.auth.grApostolos Papadopoulos\nABSTRACT\nThis paper, introduces the application of Content-Based\nMusic Information Retrieval (CBMIR) in wireless ad-hoc\nnetworks. We investigate for the ﬁrst time the challenges\nposed by the wireless medium and recognise the factors\nthat require optimisation. We propose novel techniques,\nwhich attain a signiﬁcant reduction in both response times\nand trafﬁc, compared to naive approaches. Extensive ex-perimental results illustrate the appropriateness and efﬁ-\nciency of the proposed method in this bandwidth-starving\nand volatile, due to mobility, environment.\nKeywords: music information retrieval, content-based\nsimilarity, wireless ad-hoc, P2P.\n1 INTRODUCTION\n1.1 An emerging way of music distribution\nImagine jogging or resting in London’s Hyde Park while\nlistening to music through your enhanced pocket-sized ul-\ntralight device. A device that, apart from the ability to\nplay pre-stored music like Apple’s iPod in a area which\nis not covered by wireless local area networks, can also\nsearch for and acquire music songs from other people’s\nsimilar musical prodigies. This is feasible, as the device isequipped with wireless connectivity and can participate in\nan ad-hoc network formed with devices being in its close\nproximity.\nBeing already at the end of an era for the traditional\nmusic distribution (Premkumar, 2003), the developmentof technologies like MP3 (and supporting applicationsfor their distribution, e.g., Apple’s iTunes ,MS iMusic\nonline music services) and the penetration of the World\nWide Web, have reshaped the market model and changed\nconsumer’s buying behavior. The maturing distributed\nﬁle sharing technology, implemented by peer-to-peer net-\nworks, enables the dissemination of musical content in\ndigital forms, permitting customers an ubiquitous reach\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-mercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/AD2005 Queen Mary, University of Londonto stored music ﬁles.\nThe widespread penetration of the wireless networks,\ne.g., wireless LANs, GPRS, UMTS, creates brand new\nopportunities for music delivery, e.g., pioneering applica-\ntions (Roccetti et al., 2005) supporting the distribution of\nMP3-based songs to 3G UMTS devices. Such applica-\ntions are supported by the existence of a central server,\nwhich receives requests for and delivers audio ﬁles to themobile clients. These single-hop infrastructure wireless\nnetworks are not the only possible alternative for mu-\nsic delivery. The emergence of wireless ad-hoc networkswill probably give rise to scenario like the one previously\nmentioned. The salient characteristics of these networks,\ni.e., dynamic topology, bandwidth-constrained commu-\nnication links and energy-constraint operation, introduce\nsigniﬁcant design challenges.\nLimited research work dealing with the issue of de-\nlivering streaming media (audio and video) in wirelessmobile ad-hoc networks (Baochun and Wang, 2003) does\nexist. Though, there are no prior efforts that deal with\ncontent-based music information retrieval over ad-hoc\nnetworks, where mobile clients place queries (through\nhumming or small samples) that search for music pieces\ncontaining parts similar to the query excerpt.\n1.2 Requirements posed by the wireless medium\nThe focal point of this work is the development of meth-\nods for searching by content in wireless ad-hoc networks,where the querier receives music excerpts matching to a\nposed query. Note that for the legal issues concerning the\ntransferring and reproduction of pieces found, CBMIR ap-plications in wireless networks, can adopt the solutions\nproposed in the context of online music distribution over\nwired P2P networks (Kalker et al., 2004).\nDespite the relationship of the searching procedure\nwith the latest approaches for CBMIR in wired P2P net-works (see Section 2.1), the wireless medium poses new\nand challenging requirements, which call for new solu-\ntions. These requirements are summarised as follows:\n1. CBMIR methods for wired P2P networks do not con-\nsider the continuous alteration of the network topol-\nogy, which is inherent in wireless ad-hoc networks.\nOne impact of this mobility is that selective propaga-\ntion of the query among peers, e.g., by using data in-\ndexing like DHT (Tzanetakis et al., 2004) or caching\n137past queries (Kalogeraki et al. (2002) for text doc-\numents and Karydis et al. (2005b) for music), is notfeasible. Additionally, the recall of the searching pro-\ncedure is affected by the possibility of unsuccessful\nrouting of the query, as well as the answers, over the\nchanging network topology.\n2. The need to reduce trafﬁc by replacing the origi-\nnal query with a newly developed representation that\nutilises novel, appropriate transcoding schemes. Al-\nthough trafﬁc concerns CBMIR in wired P2P net-\nworks too, the requirement of trafﬁc reduction ismuch more compelling in wireless ad-hoc networks\ndue to node constraints in processing power and au-\ntonomy.\n3. In CBMIR over wired P2P networks, should a match-\ning music excerpt is found, it can immediately be\nreturned to the querying node, since the querier is\ndirectly accessible (through its IP address). In con-\ntrast, in wireless ad-hoc networks the answers to the\nquery have to be propagated back to the querier via\nthe network (the querier is not directly accessible).\nThis requirement further burdens trafﬁc.\nAlthough some aspects of the aforementioned issues\nare being considered by algorithms proposed for the prob-lem of routing in wireless ad-hoc networks, they consider\nneither the peculiarities of searching for CBMIR purposes\nnor the size of the transferred data, since music data are\nconsiderably larger than routing packets.\n1.3 Contribution and paper organisation\nTo address the requirements posed by the wireless\nmedium, we propose the following techniques:\n1. To fulﬁll the ﬁrst requirement, we perform breadth-\nﬁrst searching over the wireless ad-hoc network us-ing knowledge about neighbouring peers (obtained\nby probing neighbourhood at speciﬁc time points).\nThis approach can cope with mobility, maintain in-creased ﬁnal recall, and constraint the drawbacks of\nﬂooding, e.g., excessive trafﬁc due to multiple broad-\ncastings (explained in Section 2.3).\n2. The second requirement is addressed by a technique\nthat uses a concise, feature-based representation of\nthe query with reducing length. The reducing-length\nrepresentation (a.k.a transcoding) that we propose\ndrastically degrades trafﬁc, while reducing the com-\nputation performed at each peer as well.\n3. For the additional trafﬁc produced by the third re-\nquirement we propose: (i) to constraint the number\nof peers involved for the propagation of the answers,\nby exploiting any peers that were involved during the\npropagation of the query, (ii) to allow such peers to\nprune the propagation of answers, based on a prop-erty of the previously described representation.\nTo our best knowledge this work is the ﬁrst to exam-\nine the issue of CBMIR in ad-hoc wireless networks. Thecontributions are: the introduction of the problem and theidentiﬁcation of the resulting requirements, a novel al-\ngorithm that combines the aforesaid techniques and ad-\ndresses the posed requirements, and extensive experimen-\ntal results, which illustrate the efﬁciency of the proposed\nalgorithm.The rest of the paper is organised as follows. Sec-\ntion 2 describes background and related work. Section 3\nprovides a complete account of the proposed method as\nwell as two baseline algorithms. Subsequently, Section 4\npresents and discusses the experimentation and results ob-\ntained. Finally, the paper is concluded in Section 5.\n2 BACKGROUND & RELATED WORK\n2.1 CBMIR in P2P networks\nResearch related to the application of CBMIR in wired\nP2P networks is recent. One of the ﬁrst attempts (Wang\net al., 2002) presents four P2P models for CBMIR,\nwhich include centralised, decentralised and hybrid cat-egories. In another research based on a hybrid conﬁgura-\ntion (Tzanetakis et al., 2004), the authors propose a DHT-\nbased system utilising both manually speciﬁed attributes(artist, album, title, etc.) and extracted features in order to\ndescribe the musical content of a piece. Yang (2003), pro-\nposed the utilisation of the feature selection and extractionprocess for CBMIR in a decentralised unstructured P2P\nsystem. Finally, Karydis et al. (2005b) investigated CB-\nMIR in P2P networks under the time-warping distance.\nThis work deals with a wireless ad-hoc network,\nwhere two nodes can communicate only if in close prox-\nimity (in-range). In this type of network peers partici-\npate randomly and for short term, and when they do, they\nchange frequently their location. These factors cause ex-\nisting approaches, e.g., indexing, to become inapplicable.\n2.2 Features for CBMIR\nThe selection of appropriate features is very important in\nmusic information retrieval. Meaningful features help in\nthe effective representation of the objects and enable the\nuse of indexing schemes for efﬁcient query processing. In\nthis work, we do not concentrate on devising new features.\nInstead, we are interested in a methodology for the search-\ning procedure. Thus, we apply a feature extraction processbased on the wavelet transform. Wavelet transforms pro-\nvide a simple but yet efﬁcient representation of audio by\ntaking into consideration both non-uniform frequency res-olution and impulsive characteristics (C. Roads and Poli,\n1997). In detail, we consider the Haar wavelet transfor-\nmation, as it is easy to compute incrementally, and for itscapability concerning the capture of time dependant prop-\nerties of data and overall multiresolution representation of\nsignals (Kin-Pong Chan and Yu, 2003).\n2.3 Information discovery in mobile ad-hoc\nnetworks\nA wireless mobile ad-hoc network (MANET) is a collec-\ntion of wireless mobile hosts forming a temporary net-\nwork without the aid of any centralised administration or\nstandard support services regularly available on the widearea network to which the hosts may normally be con-\nnected. When a source node desires to send a message to\nsome destination node and does not already have a valid\nroute to that node, it initiates a path discovery process to\nlocate the destination. It broadcasts a route request to its\n138neighbours, which then forward the request to their neigh-\nbours and so on, until the destination or an intermediatenode with a route to the destination is located. Nodes are\nidentiﬁed by their IP address and maintain a broadcast ID,\nwhich is incremented after every route request they initi-\nate. The broadcast ID together with the node’s IP address,\nuniquely identify a route request. In the same manner, the\ntransmitted data requests can be identiﬁed.\nThere is no prior relevant work on performing content-\nbased information retrieval in MANETs, though thereis a wealth of routing algorithms. For wireless ad-hoc networks there have been proposed various rout-\ning/discovery protocols, which roughly fall into the fol-\nlowing categories: a) table-driven routing protocols, b)\nsource-initiated on-demand routing protocols, and c) hy-\nbrid routing protocols. Apart from the former, the re-\nmaining two families of information (node) discovery pro-\ntocols rely on some form of broadcasting .Flooding is\nthe simplest broadcasting approach, where every node in\nthe network forwards the packet exactly once. Flood-\ning, though, generates too many redundant transmissions,\ncausing the broadcast storm problem (Ni et al., 1999).\nVarious algorithms have been proposed to address this\nproblem (Lou and Wu, 2004). They can be classiﬁedas follows: a) probabilistic approaches (counter-based,\ndistance-based, location-based), and b) deterministic ap-\nproaches (global, quasi-global, quasi-local, local). The\ndeterministic approaches provide full coverage of the net-\nwork for a broadcast operation, by selecting only a subset\nof nodes to forward the broadcast packet ( forward nodes ),\nand the remaining nodes are adjacent to the nodes that for-ward the packet. The selection of nodes is done by exploit-\ning “state” information, i.e., network topology and broad-cast state (e.g., next selected node to forward the packet,\nrecently visited nodes and their neighbour sets).\n3 SEARCHING ALGORITHMS FOR\nCBMIR IN MANETs\n3.1 Outline\nThe searching process is initiated at the querying peer,\naiming at detecting sequences in other peers containing\nexcerpts whose distance (deﬁned in Section 3.2) from the\nquery sequence /C9is less than a user-deﬁned threshold /AF.\nThe length of detected excerpts is equal to the length of /C9.\nDue to the described requirements of the wireless\nframework, the examined problem is formulated as fol-lows: (i) Peers that have qualifying sequences have to be\nreached in a way that addresses their mobility and min-\nimises the trafﬁc. Because of their relative positions andof the preferred tolerance to trafﬁc (see below), all such\nnodes may not be possible to reach. (ii) At each reached\npeer, the qualifying sequences have to be detected by de-taining the peers as little as possible. (iii) Each qualifying\nsequence has to reach the querier in a way that reduces\ntrafﬁc. Notice that the answers may have to be routedback to the querier by following paths different from those\nthrough which the peers with qualifying sequences were\nreached, since intermediate peers may have changed their\nposition, and therefore may be out of range. Due to this\neffect, every detected answer may not be possible to reachthe querier. An example is illustrated in Figure 1.\nP1P2\nP3\nP4Fwd phase\nP1P2\nP3P4Bwd phase\n(a) (b) \nFigure 1: The querier is peer /C8/BD. During the forward\nphase (a), /CAis received by peers /C8/BEand /C8/BF. During the\nbackward phase (b), answers can be directly returned by/C8/BE(still in range of /C8/BD). Due to relative movement, /C8/BFis,\nnow, out of range. Thus its answers are routed through /C8/BG\n(which was previously out the range of /C8/BD).\nTo address trafﬁc minimisation, /C9is transformed to a\nrepresentation form, denoted as /CA, through which qual-\nifying sequences are detected. The impact of the se-\nlection of /CAwill be explained in the following. Ev-\nidently, /CAmust present no false-negatives. How-\never, its particular implementation determines whetherfalse-positives may be produced or if they will be\ndeﬁnitely avoided. Based on all the aforemen-\ntioned issues, an abstract schema to describe the en-\ntire searching process consists of the following steps:\n1. User poses a query /C9.\n2. /C9is transformed to a representation form /CA.\n3. /CAis broadcast to all peers in range.\n4. Qualifying sequences (true- and false- positives)\ndetected at each peer comprise an answer-set.\n5. Each answer-set is broadcast back to the querier.\n6. Resolution of false-positives (possible places are: at\npeers that provide answers, at the querier, at\nintermediate peers).\n7. Return of actual matches to the user/application.\nIn the following, the seven aforementioned steps are\nsummarised according to four resulting events:query ini-tialisation (involves steps 1, 2, 3), reception of/CA(involves\nsteps 4, 5), reception of an answer-set (involves steps 5, 6)and answer-set reaching the querier (involves step 7).\nDuring the propagation of/CAresulting from step 3, by\nappropriately tagging it with an ID (see Section 2.3), peersthat have already received it will perform no further ac-\ntion, in order to avoid duplicate effort. Additionally, the\npropagation of/CAto the neighbouring peers is controlled\nby a parameter called maxhop , which is a counter that is\ndecreased at each receiving peer. Its initial value, at the\nquerier, is equal to /C5 /CP/DC/C0 /D3/D4 . This value corresponds to\nthe preferred tolerance to trafﬁc. The propagation of an-\nswer sets (resulting from step 5) is handled analogously,\nby employing again a maxhop parameter (the same initial\nvalue, MaxHop , is used).\nEvidently, the searching process consists of two\nphases: (i) a forward phase, during which /CAis propa-\ngated and (ii) a backward phase, during which answers are\nrouted back to the querier. The two phases are interleaved,\nsince during the propagation of /CAby some peers, other\npeers are returning answers to the querier. In general, thevolume of information transferred during the backward\n139phase is larger than that of the forward phase.\nIn the sequel, we ﬁrst present some details on the ac-\nceleration of similarity searching within each peer by us-\ning indexing. Next, we describe two algorithms (provided\nfor comparison purposes), which follow simplistic deci-\nsions. Finally, the proposed method is studied.\n3.2 Indexing within peers\nIn each peer, the original audio sequences are transformed\nto a number of multidimensional points by applying a slid-\ning window to the audio data and by applying the DiscreteWavelet Transform (DWT) to each window. Therefore,\neach audio sequence produces a set of multi-dimensional\npoints in the feature space. The dimensionality of thetransformed space depends on the number of DWT coefﬁ-\ncients that will be used for the representation. By keeping\nthe ﬁrst few DWT coefﬁcients the size of the original au-\ndio sequence is reduced signiﬁcantly.\nChan and Fu (1999) proved that no false dismissals\nare introduced when using this transformation technique.\nHowever, false-positives are a possibility and thus require\nresolution. They also proved the property of preservation\nfor the Euclidian distance. Although this distance measure\nis simple, it is known to have several advantages (Keogh\nand Kasetty, 2002). Nevertheless, the proposed methodol-\nogy does not decisively depend on the particular features\nand distance measure, which are used herein following\nsimplicity as well as computation efﬁciency reasons.\nTo achieve efﬁcient retrieval of sequences contain-\ning parts similar to the query sequence/C9, the trans-\nformed audio sequences are organised by means of an in-\ndexing scheme. However, the direct indexing of multi-\ndimensional points in feature space, would lead to large\nstorage consumption, since each audio sequence can gen-\nerate a large number of multi-dimensional points. To over-\ncome this problem we perform a grouping of points from\nconsecutive windows, which are also close enough, with\nMinimum Bounding Rectangles (MBRs). MBRs are or-\nganised by means of an R\n/A3-tree (Beckmann et al., 1990)\nor any other multi-dimensional access method. An anal-\nogous type of indexing for audio sequences for the pur-\npose of similarity searching has been described by Kary-\ndis et al. (2005a).\n3.3 Algorithm based on minimal query\nrepresentation\nDue to the previously described indexing scheme, it is\nevident that the minimal representation /CAof the query\nsequence /C9consists of a number of DWT coefﬁcients,\nwhich allow index probing in the examined peers. Thenumber of coefﬁcients used is pre-speciﬁed, since all in-dexes have the same dimensionality. Opting for low traf-\nﬁc during the forward phase, an algorithm, denoted as CQ\n(querying by Coefﬁcients, resolution at Querier), selects\nfor/CAthis minimal representation.\nThe steps of CQ are summarised according to the ac-\ntions performed for each occurring event (see Section 3.1),as follows:\nQuery initialisation: The querier sets/CAas the querycoefﬁcients and propagates (broadcasts) it to all its\nneighbours.\nReception of /CA:Upon the reception of /CA, each peer /C8\nprobes its indexes and produces a list of results.\nSince /CAconsists only of DWT coefﬁcients, this step\nproduces both true and false-positives. The answer-set (i.e., all found positives from this node) is prop-agated back to the querier, by broadcasting it to all\nneighbours of/C8(backward phase). Accordingly,\nshould there be available maxhop, /CAis conveyed to\nall /C8’s neighbouring peers (forward phase).\nReception of an answer-set: Each peer /C8, that is not\nthe querier, receiving an answer-set, continues thepropagation (backward phase) to all its neighbouring\npeers, while/C5/CP /DC /C0 /D3 /D4 has not been reached.\nAn answer-set reaches the querier: When an answer-\nset reaches the querier, then the result sequences areresolved against the query sequence and all true pos-\nitives are presented to the user.\nBy choosing/CAto contain only the DWT coefﬁcients,\nCQ is expected to produce very low trafﬁc for the forward\nphase. However, by using only the DWT coefﬁcients, it\nmay result to a large number of false-positives. This is\ndue to the fact that the coefﬁcients may not be able to per-\nform effective pruning, especially for query sequences of\nlarger length and for larger values of /AF. Hence, the trafﬁc\nduring the backward phase is going to be signiﬁcantly af-\nfected, since answer-sets will be large in number and size\n(recall that the backward phase is much more demand-\ning). This is further aggravated, since CQ does not use\nany devisable policy for the routing of answer-sets (it per-\nforms plain propagation by broadcasting to all neighbour-\ning peers). Accordingly, it is expected that the trafﬁc of\nthe backward phase will be prohibitive for CQ.\n3.4 Algorithm based on maximal query\nrepresentation\nConsidering the antipodal of the previous case, one can\nopt for eliminating the burden to the backward phase\ncaused by false-positives. This can be achieved by choos-\ning /CAequal to the maximum possible representation, that\nis, the entire query sequence /C9. In order to be able to\nperform index probing (i.e., to avoid sequential searching\nat each peer), the DWT coefﬁcients are also included in/CA, however its size is dominated by the size of the query\nsequence. The advantage of this choice is that at each\npeer, after some matches have been found by index prob-\ning, these results can be immediately tested against the\nquery itself (reﬁnement). Thus, no false-positives will be\nincluded in the answer-sets. The resulting algorithm is\ndenoted as QL(full Query, Local resolution at peers). As\npreviously, the steps of QL can be summarised according\nto the actions performed for each occurring event:\nQuery initialisation: The querier assigns to /CAthe en-\ntire query sequence (plus the query coefﬁcients) and\npropagates (broadcasts) it to all its neighbours.\nReception of /CA:Upon the reception of /CA, each peer /C8\nprobes its indexes, resolves the false-positives, and\nproduces a list of results (only true-positives). The\n140answer-set is propagated back to the querier, by\nbroadcasting it to all the neighbours of /C8(back-\nward phase). Accordingly should there be avail-\nable /D1/CP/DC/CW/D3/D4 , /CAis conveyed to all /C8’s neighbouring\npeers (forward phase).\nReception of an answer-set: Each peer /C8, that is not the\nquerier, receiving an answer-set, continues to propa-gate it (backward phase) to all its neighbouring peers\nas long as there is available maxhop.\nAn answer-set reaches the querier: When an answer-\nset reaches the querier, then the result are immedi-\nately presented to the user.\nIn contrast to CQ, QL manages to decrease the exces-\nsive trafﬁc of the backward phase. However, this comesat the cost of increasing the trafﬁc of the forward phase,\nsince the entire query is propagated. Although the back-ward phase is more crucial, the forward phase requires\noptimisation too, especially in the case of large query se-\nquences. Moreover, like CQ, QL does not use any de-\nvisable policy for the routing of answer-sets (performs\npropagation by broadcasting to all neighbouring peers).Thus, the improvement of backward trafﬁc is expected to\nbe moderate.\n3.5 Proposed algorithm\nFrom the description of CQ and QL algorithms, it is clear\nthat they refer to the two extremes with respect to the size\nof the query representation/CA. The result of each decision\nis that the backward and the forward trafﬁc is affected, re-\nspectively. We devise a hybrid representation scheme aim-ing towards high performance, which is based on a sam-\nple of the query, plus the DWT coefﬁcients (commonly\nused by all algorithms). A sample of the query does notburden forward trafﬁc as much as in the case where/CA\nis equal to the entire query sequence, while at the sametime, it can manage to eliminate a signiﬁcant number offalse-positives. A sample discards a false positive when\nthe distance between it and the corresponding elements in\nthe examined sequence is greater than/AF. Evidently, the\nlength of the sample presents a trade-off: a large sample\ncan prune more false positives, but incurs higher trafﬁc.\nThe sample used for the representation initially takes\nan adequate value (see the section with experimental re-\nsults for its tuning) and this size is monotonically reducedduring the propagation of/CAin the forward phase. By this\ntranscoding scheme we attain: (i) to keep forward trafﬁclow, (ii) the reduction in the processing time at each peer(the cost of eliminating false drops at each peer depends\non the size of the representation), (iii) we can still develop\na policy for pruning during the backward phase (to be ex-plained next), thus to reduce backward trafﬁc.\nThe reduction of/CA’s size can be achieved in many\nways. The reduction of the initial value is based on asigmoid function. Due to the shape of the function, the\nimmediate neighbourhood of the querier, which can pro-\nvide results faster, receives a larger/CA, whereas the burden\nposed on peers that are far is appreciably smaller.\nAs mentioned, the reduced representation size does\nnot inhibit the development of a policy to optimise back-\nward trafﬁc, compared to the plain propagation of answer-\nsets to all neighbours, which is used by QL and CQ. Theoptimisation is as follows. During the forward phase, each\npeer that receives /CA, additionally receives the ID of all\npeers in the path that was followed from the querier to it.\nIn the example depicted in Figure 2a, for peer /C8/BGthis path\nconsists of the IDs of peers /C8/BD\n/BN/C8/BE, and /C8/BF. These IDs\ncan be maintained along with /CAwith minimal cost (only\nsome bytes). When any of such peers that were involved\nduring the forward phase starts propagating answer-setsfor the backward phase, it has to select which of its cur-\nrent neighbouring peers will propagate these answer-set.\nP1P2P4Fwd phase\n(a) (b)Bwd phase\nP3\nP5P1\nP2P4P3\nP5\nFigure 2: Example of relative locations of peers in forward\nand backward phase.\nThe simplistic decision followed by QL and CQ, is to\npropagate to all neighbours. In contrast, we exploit anypeers that were in the path up to this peer during the for-\nward phase, which are still in range (i.e., they are cur-\nrently neighbours). Therefore, if at least a factor, denoted\nasneighbour factor (NF), of the current neighbours was in\nthe path, we select them as the only ones to propagate the\nanswer-set. If their number does not sufﬁce, then we se-\nlect randomly some of the current neighbours (not from\nthe path) in order to achieve at least NFneighbours to\npropagate the answer-set. In contrast, if their number is\nlarger than NF, then they are all selected. For instance, in\nthe occasion depicted in Figure 2b (which is after some\npeers have relocated compared to Figure 2a), and by as-\nsuming that NFis equal to two,/C8/BGchooses /C8/BFfrom the\npath, whereas /C8/BHis chosen randomly.\nThe emphasis on exploiting peers that appear in the\npath of the forward phase is based on that: (i) they may\nbe more promising to reach the querier, and (ii) they may\nhave received a representation of the initial query with\nlarger length than that received by the current peer. Inthe latter case, we can reduce backward trafﬁc by em-\nploying the selected neighbours for potential pruning of\nfalse-positives. Now it becomes easy to see why we chosea reduced-size representation, since it is possible to test\nagainst a sample with larger size and reduce the backward\ntrafﬁc. Notice that the aforementioned conditions are notalways met due to mobility, thus this pruning may not al-\nways be possible.\nFor the nodes that were selected at random in order\nto fulﬁll the NFparameter, we still provide to them the\nknowledge of the path of the peer that initiated the prop-\nagation of the answer-set (for the previous example,/C8/BH,\nwhich is selected by /C8/BG, will know the path from /C8/BDto/C8/BG). This way, due to mobility, it is possible for such\nnodes during the backward phase to ﬁnd neighbours that\nappear in the carried path (again in the same example, /C8/BH\nﬁnds /C8/BEthat was in the path). Therefore, the impact of\n141such randomly selected peers on the proposed policy may\nbe kept at a moderate level.\nThe algorithm that combines all the described tech-\nniques is denoted as ST (querying by Sample, probable\nresolution at peers due to query Transcoding), and is sum-\nmarised as follows:\nQuery initialisation: The querier sets /CAequal to a sam-\nple with an initial size (parameter) plus the query co-\nefﬁcients, and broadcasts it to all its neighbours.\nReception of /CA:Upon the reception of /CA, each peer /C8\nprobes its indexes, resolves as many false-positives\nas possible based on the received query sample of /CA,\nand produces a list of results. The answer-set is prop-agated back to the querier, by following the described\npolicy for the backward phase. Accordingly, should\nthere be available/D1/CP/DC/CW/D3/D4 , /CA’s size is reduced, and\nthe reduced /CAis conveyed to all /C8’s neighbouring\npeers (forward phase).\nReception of an answer-set: When a peer receives a re-\nply, we check if it can resolve any false-positives.This is true should it had received (if any) a rep-\nresentation that was larger than the one that the se-\nquences in the answer set were examined previously(i.e., at the sending peer). After any possible pruning,\nas long as there is available/D1/CP/DC/CW/D3/D4 , the answer-set\nis routed backwards following the proposed policy.\nAn answer-set reaches the querier: When an answer-\nset reaches the querier, initially any remaining false-positives requiring resolution are checked, and then\nthe results are presented to the user.\n4 PERFORMANCE EVALUATION\n4.1 Simulation conﬁguration\nThe performance of the algorithms was compared through\nsimulation. The settings of the simulation were as fol-\nlows. The P2P network had 100 nodes. We used 300\nreal acoustic sequences, which correspond to various popsongs. The average duration was about 5 minutes. To ac-\ncount for the fact that songs (especially the popular ones)\nare common in several nodes, we replicated each sequence\nto a number of peers (default value equals to four).\nRegarding the simulation of mobility, we used the\nGSTD simulator (Theodoridis et al., 1999), which con-siders points moving freely in a 2-D area. We used a\nsquared area with size equal to 4 Km/BE, whereas the trans-\nmission/reception range of each peer was set to 500 m ra-\ndius. Different degrees of velocity were selected for the\nmoving peers, adjusted by parameters of the GSTD, but\ndue to lack of space we present results only for the av-\nerage walking speed of a human (5 Km/h). Regarding\nST, the default value for NFwas 10% of the number of\nneighbours at each peer, whereas the default initial sample\nsize was 10% of the query sequence’s size. For all algo-\nrithms, the default value of /AFwas 0.3, and the default size\n(in number of elements) of the query sequence was about\n64 K. Additionally, the number of DWT coefﬁcients re-\ntained was set to eight. Finally, the default MaxHop was\nset to 5. When parameter values are not speciﬁed, we as-\nsume the default values.The evaluation metrics are the average trafﬁc (mea-\nsured in MB) that each query incurs and the time the ﬁrst\nand last result were discovered (the time of the ﬁrst result\nis an useful measure, since users may terminate searching\nearly). The results on time reﬂect the perceived latencyrequired for the response to the querier. In contrast, total\ntrafﬁc reﬂects the load posed to the network in order to\nprovide responses. Thus, the two factors require separateconsideration. We have to notice that in all presented re-\nsults, all algorithms ﬁnd approximately the same number\nof result, i.e., they have about the same average recall (dif-ference only in few decimal positions, due to the random-\nness in mobility), which guarantees a fair comparison.\nWe have experimented with the impact of Maxhop,\nquery range and size, the degree of replication of music\npieces, the number of peers in the network, the transmis-sion/reception range, the sensitivity against the shape of\nthe sigmoid function (used by ST), NFand initial sample\nfactor.\n4.2 Experimental results\nIn our ﬁrst experiment, illustrated in Figure 3a, we exam-\nined the time of the ﬁrst and last results against MaxHop\n(the time of ﬁrst result is depicted as a fraction of the total\ntime for all results, while vertical axis is logarithmic). In-crease in available MaxHop produces longer times, since\nmore peers are examined. Though, beyond a value (in\nthis case, ﬁve) few additional results are found, thus the\nincrease diminishes. In all cases, the increase is far more\nsteep for CQ. ST clearly outperforms the other algorithms.\nThis result can be further clariﬁed by the results on\ntrafﬁc, which are depicted in Figure 3b (the addition offorward and backward trafﬁc gives the total trafﬁc). As ex-\npected, CQ produces the lower forward trafﬁc in all cases,\nQL the highest, whereas the forward trafﬁc of ST is close\nenough to that of CQ (the vertical axis is logarithmic).\nWith regard to the backward trafﬁc, as anticipated, CQ re-\nsults to prohibitive values. (For large MaxHop, CQ did not\nmanage to ﬁnish in reasonable time, and the correspond-\ning results are omitted.) QL considerably improves back-\nward trafﬁc, though ST performs signiﬁcantly better (the\nvertical axis is logarithmic), due to the proposed routing\npolicy for the backward phase. From this result it becomes\nobvious that, although the backward phase is in generalmore demanding for all algorithms, due to the reduction\nof backward trafﬁc attained by ST, the requirement of op-\ntimising the forward phase, is fair.\nNext, we examine the impact of query range. Fig-\nure 3c shows the time of the ﬁrst and last result againstdifferent ranges (vertical axis is logarithmic). As ex-pected, the performance of CQ is by far worse than the\nothers. (For higher/AFvalues, CQ could not manage to ﬁn-\nish, thus its results for these values are omitted.) Again,\nST presents the best performance. Analogous conclusions\ncan be drawn for the trafﬁc (see Figure 3d). As of this\npoint on, we do not examine CQ any further, since it in-\ncessantly presented the worse performance.\nWe move on to examine the size of the query (mea-\nsured by the number of elements in the query sequence).\nThe results are given in Figure 3e, which presents the time\nof the ﬁrst and last result against varying query size. The\n142110100\nQL CQ ST QL CQ ST QL CQ ST QL CQ ST QL CQ ST QL CQ ST\n345678\nMaxHopTime (s)Last result\nFirst result\n(a)0.00010.0010.010.11101001000\nQL CQ ST QL CQ ST QL CQ ST QL CQ ST QL CQ ST QL CQ ST\n345678\nMaxHopTraffic (MB)Backward traffic\nForward traffic\n(b)\n1101001000\nQL CQ ST QL CQ ST QL CQ ST QL CQ ST\n0.1 0.3 0.5 1\nRange of queryTime (s)Last result\nFirst result\n(c)0.0010.010.1110100100010000\nQL CQ ST QL CQ ST QL CQ ST QL CQ ST\n0.1 0.3 0.5 1\nRange of queryTraffic (MB)Backward\nForward\n(d)\n024681012\nQL ST QL ST QL ST\n32 K 64 K 128 K\nQuery sizeTime (s)Last result\nFirst result\n(e)0246810121416\nQL ST QL ST QL ST32 K 64 K 128 K\nQuery sizeTraffic (MB)Backward\nForward\n(f)\n012345678\n0.1 0.2 0.25 0.4 0.6 0.8 1\nNFTraffic (MB)Backward\nForward\n(g)2.22.32.42.52.62.72.8\n0.01 0.05 0.1 0.2 0.3\nInitial Sample FactorTraffic (MB)Forward\nBackward\n(h)\nFigure 3: Experiments.\n143increase in query size bears an increase in response time\nfor both QL and ST. The reason for that is the increasedprocessing required for the determination of matching ex-\ncerpts of longer queries, and the increase in transmission\ntimes and trafﬁc due to the propagation of larger repre-\nsentations. The latter is illustrated in Figure 3f. For\nlarger queries the technique used by ST in the forward\nphase pays-off signiﬁcantly. Moreover, the policy for thebackward phase manages to keep the corresponding trafﬁc\nquite lower than that of QL.\nFinally, we tested the sensitivity of ST against the NF\nparameter and the size of the initial sample. The resultsfor the trafﬁc for varying NFare illustrated in Figure 3g.\nWhen NFis high, the effectiveness of the policy for the\nbackward phase is limited, since most peers are selected at\nrandom by this policy. Thus, the resulting backward traf-\nﬁc is high (forward trafﬁc is not affected). Notice that, for\nthe examined range of NFvalues, the reduction in trafﬁc\ndoes affect the number of found matches (the difference\nbetween the results for the extreme values of NFare only\nin the order of decimal values). Conclusively, small NF\nvalues are sufﬁcient. The resulting trafﬁc of ST for vary-ing initial-sample sizes are depicted in Figure 3h (for bet-\nter illustration, forward trafﬁc is depicted on the top). In\nthis case, backward trafﬁc is unaffected. As expected, for-\nward trafﬁc increases with increasing sample size. Again,\nfor the examined range of values, the reduction in trafﬁc\nis not combined with a decrease in the number of found\nmatches. Hence, small initial sample size sufﬁces.\n5 CONCLUSIONS\nIn this paper, we introduce the application of CBMIR ap-\nplication in wireless ad-hoc networks. We recognise the\nnew challenges posed by this type of networks. To ad-\ndress them, we propose a novel algorithm, which is based\non a twofold optimisation: (i) the use of query represen-\ntation with reducing length, (ii) a selective policy for the\nrouting of answers, which performs additional pruning oftrafﬁc. The combination of these factors attains signif-\nicant reduction in both response times and trafﬁc. This\nis veriﬁed through extensive experimental results, which\nillustrate the superiority of the proposed method against\ntwo baseline algorithms. Concluding, we have to mentionthat the examined context does not depend on the speciﬁc\nfeatures and distance measure, since it can be used in com-\nbination of several other ones, as long as they allow for a\nreducing-length representation.\nACKNOWLEDGEMENTS\nThis research is supported by the /C0/C8 /BT/C3/A3/BX/C1/CC/C7/A6 and/A5/CH/A2/BT/A0/C7/C8 /BT/A6 /C1/C1national programs and funded by the/BX/A5/BX/BT/BX/C3 framework.\nReferences\nL. Baochun and K. H. Wang. NonStop: Continuous multi-\nmedia streaming in wireless ad hoc networks with node\nmobility. IEEE Journal on Selected Areas in Commu-\nnications , 21(10):1627–1641, 2003.\nN. Beckmann, H. P. Kriegel, and B. Seeger. The R\n/A3-tree:An efﬁcient and robust method for points and rectan-\ngles. In Proc. ACM SIGMOD Conf. , pages 322–331,\n1990.\nA. Piccialli C. Roads, S. Pope and G. De Poli, editors.\nMusical Signal Processing . Royal Swets & Zeitlinger,\n1997.\nK. Chan and A. W.-C. Fu. Efﬁcient time series match-\ning by wavelets. In Proc. ICDE Conf. , pages 126–133,\n1999.\nT. Kalker, D. H. J. Epema, P. H. Hartel, R. L. Lagendijk,\nand M. van Steen. Music2share - copyright-compliant\nmusic sharing in p2p systems. Proceedings of the IEEE ,\npages 961–970, 2004.\nV . Kalogeraki, D. Gunopulos, and D. Zeinalipour-Yazti. A\nlocal search mechanism for peer-to-peer networks. InProc. CIKM Conf. , pages 300–307, 2002.\nI. Karydis, A. Nanopoulos, A. Papadopoulos, and\nY . Manolopoulos. Audio indexing for efﬁcient musicinformation retrieval. In Proc. MMM Conf. , pages 22–\n29, 2005a.\nI. Karydis, A. Nanopoulos, A. N. Papadopoulos, and\nY . Manolopoulos. Music retrieval in p2p networks un-\nder the warping distance. In Proc. ICEIS Conf. , 2005b.\nE. Keogh and S. Kasetty. On the need for time series data\nmining benchmarks: A survey and empirical demon-stration. In Proc. ACM SIGKDD Conf. , pages 102–111,\n2002.\nA. W.-C. Fu Kin-Pong Chan and C. Yu. Haar wavelets\nfor efﬁcient similarity search of time-series: With and\nwithout time warping. IEEE Transactions on Knowl-\nedge and Data Engineering , 15(3), 2003.\nW. Lou and J. Wu. Broadcasting in ad hoc networks us-\ning neighbor designating. In I. Maghoub and M. Ilyas,editors, Handbook of Mobile Computing . CRC Press,\n2004.\nS. Y . Ni, Y . C. Tseng, Y . S. Chen, and J.P. Sheu. The\nbroadcast storm problem in a mobile ad hoc networks.\nInProc. ACM/IEEE MOBICOM , pages 151–162, 1999.\nG. P. Premkumar. Alternative distribution strategies for\ndigital music. Communications of the ACM , 46(9):89–\n95, 2003.\nM. Roccetti, P. Salomoni, V . Ghini, and S. Ferretti. Bring-\ning the wireless Internet to UMTS devices: A casestudy with music distribution. Multimedia Tools and\nApplications , 25(2):217–251, 2005.\nY . Theodoridis, J.R.O. Silva, and M.A. Nascimento. On\nthe generation of spatiotemporal datasets. In Proc. SSD\nSymp. , 1999.\nG. Tzanetakis, J. Gao, and P. Steenkiste. A scalable peer-\nto-peer system for music information retrieval. Com-\nputer Music Journal , 28(2):24–33, 2004.\nC. Wang, J. Li, and S. Shi. A kind of content-based music\ninformation retrieval method in a peer-to-peer environ-\nment. In Proc. ISMIR Symp. , pages 178–186, 2002.\nC. Yang. Peer-to-peer architecture for content-based mu-\nsic retrieval on acoustic data. In Proc. WWW Conf. ,\npages 376–383, 2003.\n144"
    },
    {
        "title": "VOISE: Learning to Segregate Voices in Explicit and Implicit Polyphony.",
        "author": [
            "Phillip B. Kirlin",
            "Paul E. Utgoff"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417225",
        "url": "https://doi.org/10.5281/zenodo.1417225",
        "ee": "https://zenodo.org/records/1417225/files/KirlinU05.pdf",
        "abstract": "Finding multiple occurrences of themes and patterns in music can be hampered due to polyphonic textures. This is caused by the complexity of music that weaves multiple independent lines of music together. We present and demonstrate a system, VoiSe, that is capable of isolating individual voices in both explicit and implicit polyphonic music. VoiSe is designed to work on a symbolic representation of a music score, and consists of two components: a same-voice predicate implemented as a learned decision tree, and a hard-coded voice numbering algorithm. Keywords: voice segregation, explicit polyphony, implicit polyphony, machine learning, theme finding. 1",
        "zenodo_id": 1417225,
        "dblp_key": "conf/ismir/KirlinU05",
        "keywords": [
            "multiple occurrences",
            "themes and patterns",
            "polyphonic textures",
            "complexity",
            "multiple independent lines",
            "music",
            "symbolic representation",
            "voice segregation",
            "explicit polyphony",
            "implicit polyphony"
        ],
        "content": "VOISE:LEARNINGTOSEGREGATEVOICES\nINEXPLICITANDIMPLICITPOLYPHONY\nPhillipB.KirlinandPaulE.Utgoff\nDepartmentofComputerScience\nUniversityofMassachusettsAmherst\nAmherst,MA01003\n{pkirlin,utgoff}@cs.umass.edu\nABSTRACT\nFindingmultipleoccurrencesofthemesandpatternsin\nmusiccanbehamperedduetopolyphonictextures.This\niscausedbythecomplexityofmusicthatweavesmulti-\npleindependentlinesofmusictogether. Wepresentand\ndemonstrateasystem,VoiSe,thatiscapableofisolating\nindividualvoicesinbothexplicitandimplicitpolyphonic\nmusic.VoiSeisdesignedtoworkonasymbolicrepresen-\ntationofamusicscore,andconsistsoftwocomponents:\nasame-voicepredicateimplementedasalearneddecision\ntree,andahard-codedvoicenumberingalgorithm.\nKeywords:voicesegregation,explicitpolyphony,im-\nplicitpolyphony,machinelearning,themeﬁnding.\n1 INTRODUCTION\nMusicians,musicologists,musictheoreticians,andmu-\nsiclibrarians(broadlyconstrued)continuetodemonstrat e\nstronginterestinautomatedproceduresforanalyzingand\norganizingmusicinitsvariousforms. Fororganization\nofmusicpiecesbycontent,someamountofanalysisis\nrequiredinordertoprovideabasisforcomparisons.Mu-\nsicpiecescanbeclustered,distinguished,orindexedby\noneormorecriteriaofinterest.Onesuchcriterionisthe\ndegreeofsimilarityofthemesormotives.\n2 THEMEFINDING\nWeconsiderathemetobeapatternofnotesthatispromi-\nnenttoahumanlistener,andthatisrepeatedoftenenough\ntobecomerecognizable,andevenanticipated. Atheme\nmaybealteredorsystematicallyvariedinitsrepetitions,\ntypicallywithoutsacriﬁcingitsrecognizability.Anexpe -\nriencedlistenerwillhearthestatementsofthetheme(or\nthemes)relativelyeasily,andatrainedmusicianwillbe\nabletoisolatetheminaprintedscore.Withthemesinear,\nPermissiontomakedigitalorhardcopiesofallorpartofthis\nworkforpersonalorclassroomuseisgrantedwithoutfeepro-\nvidedthatcopiesarenotmadeordistributedforproﬁtorcom-\nmercialadvantageandthatcopiesbearthisnoticeandthefull\ncitationontheﬁrstpage.\nc/circlecopyrt2005QueenMary,UniversityofLondononegainsinsightintothemusicathand.Thiscanbeuse-\nfulforcomparingagiventhemeagainstacorpusofknown\nthemes,identifyingapieceoritsquotation,organizing\nmusicalworksbythemes,identifyingthemusicgenre,\ncatalogingtechniquesofvariation,oranynumberofother\npurposes.Ageneraltechniqueforautomaticallyidentify-\ningthemeswouldbeofuse,andseveralapproacheshave\nbeendevised(SmithandMedina,2001;MeekandBirm-\ningham,2001;Lartillot,2003).\n3 VOICESEGREGATION\nAnimportantaspectofanalyzingmusicforthemeidenti-\nﬁcationistosegregate(separate)thestreamofnotesofthe\nmusicpieceintodistinctvoices.Theterm voiceisgeneric\nforasingerorinstrumentalistwhosoundsasinglenote\natatime. Itisrelativelyuncommonforastatementofa\nthemetobedistributedinsegmentsacrossmultiplevoices.\nThevoicesegregationproblemistopartitiontheset\nofnotescomprisingapieceofmusicintoasmanyblocks\nastherearevoices.Allofthenoteswithinablockofthe\npartitionbelongtotheonecorrespondingvoice,andno\nnotesthatbelongtothisvoicewillbefoundinanyother\nblock.\nItisimportanttodistinguishvoicesegregationfor\nthemeﬁndingandanalysisfromthesimilarproblemthat\narisesinautomatedmusictranscription. Ratherthanfo-\ncusontheaestheticproblemoflayingoutvoicesonstaves\n(KilianandHoos,2002;Cambouropoulos,2000),wecon-\ncentrateonthemusic-theoreticissueofseparatingdistin ct\nlinesofmusic,withoutregardtovisualproperties.\nOurfocusisonhowtoautomatetheprocessofvoice\nsegregation. Weassumethatsegregationoccurspriorto\nanalysisofmonophonicstringsofnotestoﬁndthemes,but\nofcoursehavingidentiﬁedathemecanservetohelpthe\nsegregationprocess.Indeed,humananalystsappeartobe\nsomewhatopportunistic,identifyingfragmentsofstruc-\ntureinthesetofnotesofthemusicpiece,andusingsuch\nfragmentstoguidesubsequentanalysis. Weapproach\nvoicesegregationmoresimply,assumingthatitcanbe\ndonewellevenwhendisregardinginformationfromother\navenuesofanalysis.\nThereareatleasttwobroadcategoriesofpolyphony\nforvoicesegregation.Intheﬁrstcase, explicitpolyphony,\ntherearemultiplenotessoundingatoneormoreinstants.\nUndertheassumptionthatasinglevoicecannotsoundtwo\n552 \n/noteheads.s2/clefs.G/accidentals.M243/noteheads.s1/noteheads.s1/dots.dot /noteheads.s2/noteheads.s2/flags.u3\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s1/noteheads.s1/noteheads.s1/dots.dot /noteheads.s2/noteheads.s2\n/accidentals.2/flags.u3/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot\n/noteheads.s1/noteheads.s1/noteheads.s2/flags.u3\n/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/scripts.tenuto\n/noteheads.s2/accidentals.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/scripts.tenuto\n/noteheads.s2/flags.d3/noteheads.s2/noteheads.s2\nFigure1:Measures1–4,ShowingExplicitPolyphony \n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.2\n/noteheads.s2/accidentals.2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.0\n/noteheads.s2/accidentals.2\n/accidentals.0/clefs.G/accidentals.M243\n/noteheads.s2/noteheads.s2\n/accidentals.0/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.0\n/noteheads.s2/accidentals.M2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/accidentals.2/scripts.tenuto/noteheads.s2/scripts.tenuto/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s235\n/clefs.G/accidentals.M2\nFigure2:Measures33–36,ShowingImplicitPolyphony\nnotessimultaneously,onecaninfersafelythattwoexplici t\nvoicesareatwork.Inthesecondcase, implicitpolyphony,\nthereisatmostonenotesoundingatanyinstant.Never-\ntheless,throughleapstodifferentregisters,arpeggiati on,\nandotherartisticdevices,theillusionofmultiplevoices\ncanbeachieved.Properanalysiswillattributethesenotes\ntodifferentimplicitvoices,eventhoughthenotesdonot\noverlapintime. Weaddressbothexplicitandimplicit\npolyphony.\nConsiderJ.S.Bach’sCiacconaforunaccompaniedvi-\nolinfromhisDMinorPartita.Aciacconaisbuiltatopa\nthemeinthebassthatisrepeatedandhighlyvaried.The\nlabelciacconacanbeoftremendousaidtothelistener\nbecauseitforetellstheexplicitvoiceinwhichthetheme\nwillusuallyappear,thatthethemewillberepeatedre-\nlentlessly,andthatonecanexpectmyriadvariationsand\nelaborations.\nObservetheﬁrstfourmeasures,asshowninFigure1.\nThefourvoicesarereadilyapparent,withthesopranoen-\nteringonthedownbeatoftheﬁrstfullmeasure.Nowcom-\nparemeasures33–36,showninFigure2.Here,thetheme\nisheardinthebass,whichoccupiesitsownregisterinthe\nimplicitpolyphony.Thethemehasbeenvariedsothatit\nformsadescendingchromaticline.Theﬁrstmeasureofa\nthirdexample,measure89,appearsinFigure3.Starting\natthispoint,Bachwritesaprogressionofchordsalong\nwiththeannotationtoarpeggiate. Thecomposershows\nthatheconceivesexplicitpolyphony,andthathecallson\ntheperformertorenderthenotesinimplicitpolyphonyto\nproducethedesiredauralillusion.\nItisimportanttodistinguishbetweenrepresentations\nusedinthetaskofvoicesegregation.Therehasbeenwork\nonisolatinglinesofmusicinlow-levelaudiostreams,\nsuchasBaumann(2001),andalsostudiesusinghigher\nlevelsymbolicrepresentations,includingthosebyHuron\n(1991),Gjerdingen(1994),andTemperley(2001).How-\never,wehavenotlocatedanythathaveharnessedauto-\nmatedlearningtechniques.\n4 LEARNINGTOSEGREGATEVOICES\nTheVoiSesystemiscapableoftakingasinputarepre-\nsentationofamusicscoreandsegregatingthenotesof\nthemusicintodistinctvoices,representingseparatemusi -\ncallinesinthepiece.VoiSeincludestwocomponents:a \n/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/clefs.G/accidentals.M243\n/clefs.G/accidentals.M243/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\n/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s2\narpeggio/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\nFigure3:Measure89,ShowingImplicitPolyphony\npredicatethatdetermineswhetherornottwonotesarein\nthesamevoice,andaseparatealgorithmthatperformsthe\nvoicesegregation.\n4.1 TheSame-VoicePredicate\nThesame-voice(note 1, note2)predicateexaminesvarious\nfeaturesofanorderedpairofnotestodeterminewhether\nornotthesetwonotesbelongtothesamevoice. Were-\nstrictourpredicatetonotepairssatisfyingtwocondition s.\nFirst,thenotesmustbedistinct,andsecond,theonsetof\ntheﬁrstnotemustoccuratorbeforetheonsetofthesec-\nond.Whilethereisnoguaranteethattheresultingpredi-\ncatewillbetransitive,a“perfect”predicatewouldbe.\nHumansuseavarietyoftechniquesinsegregating\nvoices. Asigniﬁcantfeatureinvoicesegregationisthe\n“vertical”(pitch)distancebetweentwonotes;thisideais\nsupportedbythePitchProximityPrincipleasdescribed\nbyHuron(2001).Weincludeﬁvefeaturesthatcalculate\nvariousmeasurementsofthisdistance.Theseare:\n•HalfSteps,thenumberofhalfstepsseparatingtwo\npitches;\n•DiscreteInterval, a representation of the interval\nsize,suchas“M3”foramajorthirdor“P4”fora\nperfectfourth;\n•IntervalCompoundSize ,apurelynumericmeasure-\nmentofthediatonicinterval,suchas3or4;\n•IntervalBaseSize,ameasurementsimilarto Inter-\nvalCompoundSize butonethatdisregardsinterven-\ningoctavesbetweenpitches;and\n•IntervalUpOrDown ,abooleanfeaturethatreports\nwhethertheintervalbetweentwoorderednotesisas-\ncendingordescending.\nRhythmalsoplaysalargepartinvoicesegregation.Fre-\nquentlyapassagewillcontainarepeatedrhythmwhere\nnotesthatfallonspeciﬁcbeatsarealwaysinthesame\nvoice,whilenotesthatfallonotherbeatsarealwaysina\ndifferentvoice.VoiSeconsiderstenrhythmicfeatures:\n•BeatDifPart,thetotalnumberofbeatsseparating\ntwonotes;\n•BeatDifMeasure, the number of beats separating\ntwonotes,relativetoasinglemeasure;\n•BeatDifMeasureAbs ,theabsolutevalueof BeatDif-\nMeasure;\n•BeatDifMeasMod ,the(fractional)numberofbeats\nseparatingtwonotes,relativetoasinglebeat;\n•Overlapping,whethertwonotesoverlapintimeat\nall;thatis,whetherthesecondnotestartsbeforethe\nﬁrstoneends;\n553•InDifferentMeasures , whichreportswhethertwo\nnotesareindifferentmeasures;\n•Note1MeasBeatandNote2MeasBeat,whichgive\nthenumericbeatonwhicheachnotefallswithina\nmeasure;\n•Note1MeasBeatMod andNote2MeasBeatMod ,\nwhicharesimilartotheirnon-modcounterparts,but\ncalculatebeatplacementrelativetothesinglebeat.\nTheissueofbeatdifferences“relative”toameasureor\nbeatwarrantssomeexplanation. Whilethetotalnumber\nofbeatsseparatingtwonotesiscertainlyausefulfeature,\nwecanalsocalculatethisdistancebyleavingoutmulti-\nplesofacertainnumberofbeats.Forexample,ifwedis-\nregardthemeasureinwhicheachnoteoccurs,weobtaina\nnotionofbeatdistancecalculatedasthoughthetwonotes\nappearedinthesamemeasure.Forexample,incommon\ntime,anotethatfallsonbeatoneofmeasure16andanote\nthatfallsonbeatthreeofmeasure17haveanabsolutebeat\ndistancebetweenthemofsix. However,theirbeatdis-\ntancerelativetoonemeasureisonlytwo,becauseifthe\ntwomeasureswere“overlayed”oneupontheother,there\nwouldbeonlytwobeatsseparatingthenotesinquestion.\nTakingthisideaonestepfurther,the BeatDifMeas-\nModfeaturelistedaboveiscalculatedrelativetoasingle\nbeat. Forexample,incommontime,anotethatfallson\nbeatoneofmeasure18andanotethatfallsonbeat1.5\n(oneeighthnotefromthestartofthemeasure)ofmeasure\n19haveanabsolutebeatdistanceof4.5,howevertheir\nbeatdistancerelativetowholebeatsisonly0.5.\nOften when comparing two notes to determine\nwhethertheyresideinthesamevoiceornot,thetaskbe-\ncomesmorechallengingasthedistancebetweenthenotes\nincreases. Thisisespeciallytrueinfreetextureswhere\nvoicesmayappearandvanishthroughoutthepiece. Due\ntothisinherentambiguity,weimposealeft-alignedwin-\ndowthatrestrictsthepredicatetotrainonlyonnotepairs\nwherethenotesinthepairareseparatedbylessthana\ncertainmusical“distance.”\n4.2 TheVoice-NumberingAlgorithm\nThis same-voicepredicateisnotofmuchuseonitsown.\nWecanreachourultimategoalofvoicesegregationby\nproducingavoicenumbering: amappingfromnotesto\nuniqueidentiﬁers,suchasintegers.Aperfectvoicenum-\nberingindicatesthatnotesmappedtothesameintegerare\ninthesamevoice,andconversely,thatnotesmappedto\ndifferentintegersareindifferentvoices.Wecaninclude\nthepreviouslylearnedpredicateinavoicenumberingal-\ngorithm,whichoperatesunderrestrictionsthatnotepairs\nthataredeterminedtobeindifferentvoicesbemappedto\ndistinctintegers,andconversely,thatnotepairsthatare\nfoundtobeinthesamevoicebemappedtothesamein-\nteger.Withoutaperfectpredicate,however,itispossible\nthattheserequirementswillnotbecompletelymet.\nInrestrictedmusicaltextures,suchaschoralesorother\nvocalmusic,therearefrequentlyamaximumoffourex-\nplicitvoicesthroughouttheentirepiece.Inouralgorithm ,\nassign-voice-numbers ,showninFigure4,wedonotplace\nalimitonthenumberofvoicesthatmayoccurthroughoutfunction assign-voice-numbers (P,W,E)\n{Pisthelearnedsame-voicepredicate,Wisthewin-\ndowusedtotrainP,Eisthemusicexcerptwhose\nvoiceswearenumbering,fisthemappingweare\ndeﬁningfromnotestointegers. }\nnextvoice←0\nforeachnoten∈E,inorderofnoteonset, do\niff(n)isundeﬁnedthen\ndeﬁnef(n):=nextvoice\nnextvoice←nextvoice +1\nelse\nlookahead,inorderofnoteonset,forthenextnote\nm∈EthatsatisﬁesP(n,m)and(n,m)∈W.\nifmultiplenotesarefoundwhoseonsetscoincide\nthen\nchoosemtobethenotethatisthesmallestnum-\nberofhalf-stepsawayfrom n.\ndeﬁnef(m):=f(n).\nreturnf\nFigure4:TheVoiceNumberingAlgorithm\nthecourseofapiece.Ourreasoningisthatwhenthere-\nstrictionsofhumansingingandvocalrangesareremoved,\nawidervarietyoftexturesispossible.Forexample,voices\nmayseemtoappearinthemiddleofapieceandthenvan-\nish,onlytoreappearlater. Ifthisoccursoften,itcanbe\ndifﬁculttodeterminewhethertwovoicesseparatedbya\nsizablebreakintimeactuallybelongtothesamevoice.\n5 EXPERIMENTS\nInthefollowingfourexperiments,wetrainedthe same-\nvoicepredicateonamusicalexcerpt,thenranthe assign-\nvoice-numbersalgorithmwiththenewly-learnedpredi-\ncateonadifferentexcerpt. Ineachoftheﬁrstthreeex-\nperiments,thetrainingandtestingexcerptswereofsimi-\nlartextureandstyle.Forthefourth,boththetrainingand\ntestingselectionswereformedastheunionoftherespec-\ntiveexcerptsfromtheﬁrstthreeexperiments.\n5.1 DataGatheringandPredicateTraining\nToobtaintrainingdataforthe same-voicepredicate,we\nemployed the Ciaccona by J.S. Bach. We excerpted\nanumberofpassagesofvariouslengths,textures,and\nstyles,andproducedavoicenumberingbyhandwhich\nweusedasgroundtruth.\nWeusedtwotypesofwindowtotrainthepredicate:\nawindowbasedonamaximumnumberofbeats,anda\nwindowbasedonamaximumnumberofnotes.Awindow\nbasedonanumberofbeatsallowsthepredicatetotrain\nonlyonnotepairsforwhichtheendoftheﬁrstnoteis\nnomorethanacertainnumberofbeatsawayfromthe\nonsetofthesecond. Thisallows,forexample,training\nonallnotepairsseparatedbyaquarter-orhalfnote-sized\ndistance.\nIncontrast,usingawindowbasedonthenumberof\ninterveningnotesrestrictsthelearningalgorithmtosee-\ninginstanceswherethenumberofnotesbetweenthepair\nofnotesinquestionisnomorethanaﬁxedinteger k.This\ntypeofwindowallows,forexample,trainingononlyad-\njacentpairsofnotes,whileignoringthenumberofbeats\n554thatoccurbetweenthem.\nAfterasetoftraininginstanceswasgeneratedfrom\neachgivenmusicselection,weusedtheITIAlgorithm\n(Utgoffetal.,1997)tolearnthe same-voicepredicateas\nadecisiontree. Onepredicatedwaslearnedperwindow\ntypeandsizeused.\n5.2 EvaluationFunctions\nWeevaluatethe same-voicepredicateusingtwodistinct\nmethods. First,wecalculatetheaccuracyofthelearned\npredicatethroughten-foldcrossvalidation.However,be-\ncausewewouldliketorunthe assign-voice-numbers algo-\nrithmoncompletelyseparatedata,wecannotbesurethat\nthedistributionofinstancesobtainedfromthenewdata\nwillmatchthedistributionfromthetrainingdata.There-\nfore,whenwetestthelearnedpredicateonacompletely\nnewmusicexcerpt,wepresentthesenewaccuraciesas\nwell.Eachpredicateistestedonlyonnotepairsthatsat-\nisfythesamewindowingconditionsonwhichthepredi-\ncatewastrained.\nEvaluationofthe assign-voice-numbers algorithmis\nlessstraightforward. Thereisnoimmediatelyapparent\nproceduretoevaluateamappingofnotestointegers,as\ntheintegersthemselvesaremeaningless; itistherela-\ntionshipsamongthenotesthataremappedtosameor\ndifferentintegersthatmustbeevaluated. Furthermore,\nana¨ıveevaluationmethodmayseverelypenalizeasin-\nglemistakeinanumberedvoice. Forexample,ifthe\nassign-voice-numbers algorithmpredictsasetofnotes\nS={n1,n2,...,nk}tobeallinthesamevoice,thenitis\ntemptingtocompareallofthenotesin Spairwisetode-\nterminewhethereachnoteisactuallyinthesameground-\ntruthvoiceaseachothernote. However,considerwhat\nhappenswhentheﬁrsthalfofthenotesin Saretruly\ninvoiceA,andthesecondhalfaretrulyinadifferent\nvoiceB.Withthisﬁrstattemptatevaluatingthispredicted\nvoice,only50%ofthepairwisecomparisonswouldbe\ntrue,wheninfacttherewouldreallyonlybeonemistake\nmadeinthevoicenumbering. Ifwejust“break”these-\nriesintotwovoicesatthehalfwaypoint,thensuddenly\ntheevaluationjumpsbackupto100%.Clearlywemust\nconstructourevaluationfunctionmorecarefully.\nTherefore,weprovidetwomeasurementsofaccuracy\nforthe assign-voice-numbers algorithm:soundnessand\ncompleteness. Thesemeasuresaresimilarto fragment\nconsistencyandvoiceconsistencyasdeﬁnedinChewand\nWu(2004),andalsotothevariousmeasuresof purityin\nclusteringalgorithms.\nTodescribethesequantities,letusassumethatwe\ntrainthe same-voicepredicatePonmusicalexcerpt E1\nandapplyPtonumberthevoicesinexcerpt E2. The\nassign-voice-numbers algorithmproducesamapping, f,\nfromnotesinE2tointegers,representingthepredicted\nvoicenumbering.Becausewealsohaveground-truthdata\nforE2,wehaveanothermapping, g,fromnotesinE2to\nintegers,thatrepresentsthetruevoicenumbering.\nSoundnessisdesignedtogiveanindicationofwhether\nornotnotepairsthatthe assign-voice-numbers algorithm\nplacedinthesamevoiceare,inreality,inthesamevoice.\nSoundnessiscalculatedbyﬁnding, foreachpredictedTable1:AccuracyofLearnedPredicateforExperiment1\nWindowtypeand\nsizeC-VAccuracyNewData\nAccuracy\nbeats,eighthnote 78.33±13.7291.61\nbeats,quarternote 79.38±7.2591.50\nbeats,halfnote 74.09±8.8589.71\nbeats,dottedhalfnote 76.90±4.0091.48\nnotes,onenote 85.71±13.4792.50\nnotes,twonotes 82.73±13.8593.18\nnotes,threenotes 81.67±10.2491.55\nnotes,fournotes 79.23±14.0791.72\nTable2:VoiceNumberingAccuracyforExperiment1\nWindowtypeandsize SoundnessCompleteness\nbeats,eighthnote 84.6271.88\nbeats,quarternote 82.7678.13\nbeats,halfnote 83.3378.13\nbeats,dottedhalfnote 86.2175.00\nnotes,onenote 89.4753.13\nnotes,twonotes 88.0068.75\nnotes,threenotes 84.6271.88\nnotes,fournotes 84.6271.88\nvoiceV,thepercentageofadjacentnotes n1,n2∈Vthat\nalsosatisfyg(n1) =g(n2). Inotherwords,wecalculate\nthefractionofadjacentnotepairsthatwerepredictedcor-\nrectthataretrulycorrect.\nCompleteness,ontheotherhand,givesanindication\nofhowwellthe assign-voice-numbers algorithmplaced\nnotestogetherthatshouldbelonginthesamevoice.This\nvalueiscalculatedbyﬁnding,foreachground-truthvoice\nV,thepercentageofadjacentnotes n1,n2∈Vthatalsosat-\nisfyf(n1) =f(n2).Wecalculatethefractionofadjacent\nnotepairsthatshouldhavebeenplacedinthesamevoice\nthatactuallywere.\n5.3 Experiment1\nInExperiment1,wetrainedthe same-voicepredicateon\nmeasures1–8oftheCiaccona.Thisexcerptpresentsthe\nmelodyandharmonyinexplicitpolyphonicstyle.Wethen\ntestedthe same-voicepredicateonmeasures9–12,which\naresimilarinstyleto1–4,butmoreornamented.\nTheresultsofthisexperimentappearinTable1.The\nC-Vtrainingaccuraciesarenoticeablylowerthanthenew\ndataaccuracies, likelyduetotheadditionalﬁguration\npresentinmeasures9–12. Theseornamentsfrequently\noccurinasinglevoiceatatime,whiletheothervoices\nremainconstant.Itisthereforeusuallyeasytodetermine\nthatallthenotesofeachornamentbelonginthesame\nvoice,therebyraisingthenewdataaccuracyonmeasures\n9–12.\nThe soundness and completeness values from the\nassign-voice-numbers algorithm,showninTable2,sug-\ngestlittleconnectionbetweenwindowsizeandsoundness\norcompletenessinthisexplicitpolyphonictexture.This\ncouldbeduetothefactthattherearefewrepeatedtonal\norrhythmicpatternsforthedecisiontreeinducertorec-\nognize.\n555Table3:AccuracyofLearnedPredicateforExperiment2\nWindowtypeandsize C-VAccuracyNewData\nAccuracy\nbeats,eighthnote 95.00±8.0571.74\nbeats,quarternote 97.50±5.2772.89\nbeats,halfnote 93.08±9.2184.75\nbeats,dottedhalfnote 94.12±6.7982.18\nnotes,onenote 100.00±0.0087.23\nnotes,twonotes 100.00±0.0072.04\nnotes,threenotes 97.50±5.2769.56\nnotes,fournotes 98.00±4.2271.98\nTable4:VoiceNumberingAccuracyforExperiment2\nWindowtypeandsize SoundnessCompleteness\nbeats,eighthnote 66.6740.48\nbeats,quarternote 62.0745.24\nbeats,halfnote 66.6757.14\nbeats,dottedhalfnote 51.4342.86\nnotes,onenote 81.8242.86\nnotes,twonotes 66.6740.48\nnotes,threenotes 66.6740.48\nnotes,fournotes 64.2942.86\n5.4 Experiment2\nInournextexperiment,wetrainedanew same-voicepred-\nicateonmeasures33–36oftheCiaccona. Thispassage\nisoneofthemorestarkexamplesofimplicitpolyphony\nintheCiaccona, asthesequenceofeighthnotesand\nthelargeleapspresentindicatethepresenceofmultiple\nvoices.Wethenappliedthe same-voicepredicatetoclas-\nsifynotepairsinmeasures37–40,apassagethathasa\ntexturethatisnotsimilar,butnotidenticaltothetrainin g\ndataofmeasures33–36. Thecontoursofbothexcerpts\naresimilar;however,thetrainingmeasuresconsistmostly\nofeighthnotes,whilethetestingmeasuresincludeonly\nsixteenthnotes. Additionally,eventhoughthenoteval-\nuesaremostlyidenticalthrougheachexcerpt,thevoice\nseparationislessapparentinmeasures37–40thanin33–\n36.Table3reﬂectsthis,astheC-Vaccuraciesaremuch\nhigherthanthenewdataaccuracies.\nBecauseofthedissimilaritiesbetweenourtrainingand\ntestingexamplesinthisexperiment,theresultsforthe\nassign-voice-numbers algorithm,showninTable4,were\nnotashighasfromtheﬁrstexperiment.\n5.5 Experiment3\nInExperiment3,wetrainedanew same-voicepredicate\nonmeasures89–92oftheCiaccona,apassageconsisting\nentirelyofrapidarpeggiationofchords. Wethentested\nsame-voiceonanexcerptofsimilartexture,namelythe\nnextfourmeasuresoftheCiaccona. Asalways,each\npredicatewastestedusinganappropriatewindow;there-\nsultsareshowninTable5,andtheresultsof assign-voice-\nnumbersappearinTable6.\nThedatashowthatthepredicate’saccuracydeclines\nslightlyaswindowsizeisincreased,whilethestandard\ndeviationfallsaswell—illustratingabias-variancetrad e-\noff.Inthisarpeggiatedtexture,ﬁndingtherhythmicpat-Table5:AccuracyofLearnedPredicateforExperiment3\nWindowtypeandsize C-VAccuracyNewData\nAccuracy\nbeats,eighthnote 95.11±3.0297.85\nbeats,quarternote 93.54±3.2598.17\nbeats,halfnote 91.22±2.2195.67\nbeats,dottedhalfnote 89.09±1.6790.89\nnotes,onenote 97.00±4.83100.00\nnotes,twonotes 96.84±4.4499.47\nnotes,threenotes 96.55±3.2599.65\nnotes,fournotes 95.00±3.3998.40\nTable6:VoiceNumberingAccuracyforExperiment3\nWindowtypeandsize SoundnessCompleteness\nbeats,eighthnote 97.7091.40\nbeats,quarternote 96.6793.55\nbeats,halfnote 100.0094.62\nbeats,dottedhalfnote 96.5990.32\nnotes,onenote N/A10.00\nnotes,twonotes 100.0049.46\nnotes,threenotes 100.0049.46\nnotes,fournotes 97.7894.62\nternbecomeseasierasmoreinstancesareseen.\n5.6 Experiment4\nForourﬁnalexperiment,wecombinedallthreeprevious\ntrainingsetsandusedtheaggregatedatatotrainanew\nsame-voicepredicate.Becauseweexpectedthatmusical\ntextureplaysalargepartindeterminingvoicesegregation ,\nandwedidnothaveanyfeaturestodeterminetexturedi-\nrectly,wedidnotknowwhataccuraciestoexpect. We\ntestedourlearnedpredicateonallthreetestingsetsin-\ndividuallyandalsoontheirunion,mirroringthetraining\nprocedure.\nWhilethetraining(C-V)accuraciesinTable7ob-\ntainedarelowerthantherespectiveaccuraciesinExper-\niments2and3,theyarehigherthaninExperiment1.\nThismaybeduetothefactthatExperiment3’straining\nsetcontainsmorenotesthantheothertwo,andtherefore\nExperiment3’sinstancesoutnumbertheotherexamples’,\ncausingthemtoexertthegreatestinﬂuenceinthelearning\nalgorithm.\nThetestingdataaccuraciesindicatethattrainingon\na combination of three different musical textures did\nnothinderlearningthe same-voicepredicate. Whenwe\ncomparedtheindividualaccuraciesforeachofthethree\ntestingsetsevaluatedbytheaggregatepredicateagainst\nthecorrespondingaccuraciesevaluatedbytheseparately-\ntrainedpredicates,wediscoveredtherewaslittledegra-\ndationatall. Inafewcases,theindividualaccuracies\nfortheaggregatepredicate rose,likelybecauseeachtest-\ningsetsharescharacteristicsoftheothertrainingsetsth at\nindividually-trainedpredicatescannotcapture.\nThelevelsofaccuracyattainedbytheglobally-trained\npredicateimplythatitisfeasibletocombinemultiple\nstylesandtextureswhentraining,asnotmuchaccuracy\n1Noadjacentnoteseverclassiﬁedinsamevoice\n556Table7:AccuracyofLearnedPredicateforExperiment4\nWindowtypeandsize C-VAccuracyNewData\nAccuracy\nbeats,eighthnote 90.47±4.2093.70\nbeats,quarternote 90.19±3.2492.50\nbeats,halfnote 87.70±1.5590.88\nbeats,dottedhalfnote 86.68±2.1585.20\nnotes,onenote 94.74±4.9694.59\nnotes,twonotes 90.86±5.3590.10\nnotes,threenotes 92.98±4.1491.28\nnotes,fournotes 91.33±3.1292.85\nTable8:VoiceNumberingAccuracyforExperiment4\nWindowtypeandsize SoundnessCompleteness\nbeats,eighthnote 93.6779.04\nbeats,quarternote 89.5877.25\nbeats,halfnote 88.5978.44\nbeats,dottedhalfnote 88.4477.25\nnotes,onenote 85.3720.96\nnotes,twonotes 81.4856.89\nnotes,threenotes 89.0053.89\nnotes,fournotes 93.1080.84\nonunseendataissacriﬁced. Whilethesoundnessand\ncompletenesslevelsshowninTable8arenotashighas\nintwooftheearlierexperiments,theystillsuggestthat\nlittledegradationoccurswhentrainingonmixedtextures;\nthefeaturesetisapparentlyrobustenoughtodistinguish\nstyles.\n6 DISCUSSIONANDFUTUREWORK\nWhilethepreviousfourexperimentsillustratethattrainin g\napredicatetosegregatevoicesisapromisingapproach,\ntherearemethodsbywhichVoiSecouldbeimproved.\nItisimportanttorecallthatahighlyaccuratelearned\nsame-voicepredicate does not necessarily imply that\nassign-voice-numbers will produce a highly sound or\ncompletevoicenumbering.Becauseofthedifferentwin-\ndowingtechniques, andthefactthatthe assign-voice-\nnumbersalgorithmmustbreaktieswhenmultiplenotes\nsatisfytheselectioncriteria,ahighly-accuratepredica te\nmayleadtoasoundbutnon-completevoicenumbering,\nasseeninExperiment3.\nOnemayalsoask,whenmeasuringaccuracyforthe\nassign-voice-numbers algorithm,whythesoundnesslev-\nelsareconsistentlyhigherthanthecompletenesslevels.\nThisphenomenonoccursbecauseasthenumberofvoices\ninatrainingexcerptincreases,thenumberofnotepairs\nindifferentvoicesincreasesfasterthanthenumberof\npairsinthesamevoice.Therefore,the same-voicepred-\nicatelearnstoerronthesideofnotclassifyingnote\npairsasbeinginthesamevoice. Recallthatthe assign-\nvoice-numbersalgorithmworksbylookingaheadintime,\nwithinanappropriatewindow,forthenextnotethatisin\nthesamevoiceastheoneitiscurrentlyexamining. Ifa\nground-truthvoicedropsoutforadurationgreaterthan\nthesizeofthewindowthat assign-voice-numbers isus-\ning,thenthereisnowaythatthealgorithmcancorrectly\nplacenotesonoppositessidesofthegapinthesamevoice.Whenthisoccurs, assign-voice-numbers endsupdividing\naground-truthvoiceintosections,whichbydeﬁnitionwill\nlowerthecompletenessmeasurement.\nVoiSeisalsolimitedinthefactthatitisalloweda\nsinglepassthroughthedatatomakesame-voicedetermi-\nnationsandproduceavoicenumbering. Whilethistac-\nticcertainlyworksuptoapointinourexperiments,we\nsuspectthathumanscansolvethisproblemmoreaccu-\nratelybecausetheymaymakemultiplepassesoverthe\nmusic. Thatis,ifonedeterminesthatagroupofnotes\ndeﬁnitelyisorisnotinthesamevoice,onecanusethis\nfactatotherplacesinthepiecewhereasimilaritytothe\nformergroupisdiscovered.Thissuggestsaniterativeap-\nproach,inwhichVoiSe’spredictionscanbereﬁnedover\ntime.\nREFERENCES\nU.Baumann.Aprocedureforidentiﬁcationandsegrega-\ntionofmultipleauditoryobjects. InS.Greenbergand\nM.Slaney,editors, ComputationalModelsofAuditory\nFunction,pages268–280.IOSPress,Berkeley,2001.\nE.Cambouropoulos. FromMIDItotraditionalmusical\nnotation.InProceedingsoftheAAAIWorkshoponAr-\ntiﬁcialIntelligenceandMusic:TowardsFormalModels\nforComposition,PerformanceandAnalysis ,2000.\nE.ChewandX.Wu.Separatingvoicesinpolyphonicmu-\nsic:Acontigmappingapproach. In ComputerMusic\nModelingandRetrieval:SecondInternationalSympo-\nsium,pages1–20,2004.\nR.O.Gjerdingen. Apparentmotioninmusic? Music\nPerception,11(4):335–370,1994.\nD.Huron.Tonalconsonanceversustonalfusioninpoly-\nphonicsonorities. MusicPerception, 9(2):135–154,\n1991.\nD.Huron.Toneandvoice:Aderivationofvoice-leading\nfromperceptualprinciples. MusicPerception,19(1):\n1–64,2001.\nJ.KilianandH.H.Hoos. Voiceseparation—alocal\noptimisationapproach. In ProceedingsoftheThird\nAnnualInternationalSymposiumonMusicInformation\nRetrieval,pages39–46,2002.\nO.Lartillot. Discoveringmusicalpatternsthroughper-\nceptiveheuristics. In ProceedingsoftheFourthAn-\nnualInternationalSymposiumonMusicInformation\nRetrieval,pages89–96,2003.\nC.MeekandW.P.Birmingham. Thematicextractor. In\nProceedingsoftheSecondAnnualInternationalSym-\nposiumonMusicInformationRetrieval , pages119–\n128,2001.\nL.SmithandR.Medina. Discoveringthemesbyexact\npatternmatching. In ProceedingsoftheSecondAn-\nnualInternationalSymposiumonMusicInformation\nRetrieval,pages31–32,2001.\nD.Temperley.TheCognitionofBasicMusicalStructures .\nTheMITPress,Cambridge,Massachusetts,2001.\nP.E.Utgoff,N.C.Berkman,andJ.A.Clouse. Decision\ntreeinductionbasedonefﬁcienttreerestructuring. Ma-\nchineLearning,29(1):5–44,1997.\n557"
    },
    {
        "title": "Instrument Identification in Polyphonic Music: Feature Weighting with Mixed Sounds, Pitch-Dependent Timbre Modeling, and Use of Musical Context.",
        "author": [
            "Tetsuro Kitahara",
            "Masataka Goto",
            "Kazunori Komatani",
            "Tetsuya Ogata",
            "Hiroshi G. Okuno"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415724",
        "url": "https://doi.org/10.5281/zenodo.1415724",
        "ee": "https://zenodo.org/records/1415724/files/KitaharaGKOO05.pdf",
        "abstract": "This paper addresses the problem of identifying musical instruments in polyphonic music. Musical instrument identification (MII) is an improtant task in music information retrieval because MII results make it possible to automatically retrieving certain types of music (e.g., piano sonata, string quartet). Only a few studies, however, have dealt with MII in polyphonic music. In MII in polyphonic music, there are three issues: feature variations caused by sound mixtures, the pitch dependency of timbres, and the use of musical context. For the first issue, templates of feature vectors representing timbres are extracted from not only isolated sounds but also sound mixtures. Because some features are not robust in the mixtures, features are weighted according to their robustness by using linear discriminant analysis. For the second issue, we use an F0-dependent multivariate normal distribution, which approximates the pitch dependency as a function of fundamental frequency. For the third issue, when the instrument of each note is identified, the a priori probablity of the note is calculated from the a posteriori probabilities of temporally neighboring notes. Experimental results showed that recognition rates were improved from 60.8% to 85.8% for trio music and from 65.5% to 91.1% for duo music. Keywords: Musical instrument identification, mixedsound template, F0-dependent multivariate normal distribution, musical context, MPEG-7 1",
        "zenodo_id": 1415724,
        "dblp_key": "conf/ismir/KitaharaGKOO05",
        "keywords": [
            "musical instrument identification",
            "polyphonic music",
            "feature variations",
            "pitch dependency",
            "musical context",
            "F0-dependent multivariate normal distribution",
            "MPEG-7",
            "temporally neighboring notes",
            "recognition rates",
            "experimental results"
        ],
        "content": "INSTRUMENT IDENTIFICATION IN POLYPHONIC MUSIC:\nFEATURE WEIGHTING WITH MIXED SOUNDS, PITCH-DEPENDENT\nTIMBRE MODELING, AND USE OF MUSICAL CONTEXT\nTetsuro Kitahara,†Masataka Goto,‡Kazunori Komatani,†Tetsuya Ogata†and Hiroshi G. Okuno†\n†Dept. of Intelligence Science and Technology‡National Institute of Advanced Industrial\nGraduate School of Informatics, Kyoto University Science and Technology (AIST)\nSakyo-ku, Kyoto 606-8501, Japan Tsukuba, Ibaraki 305-8568, Japan\n{kitahara, komatani, ogata, okuno }@kuis.kyoto-u.ac.jp m.goto@aist.go.jp\nABSTRACT\nThis paper addresses the problem of identifying musi-\ncal instruments in polyphonic music. Musical instrumentidentiﬁcation (MII) is an improtant task in music informa-tion retrieval because MII results make it possible to au-tomatically retrieving certain types of music ( e.g., piano\nsonata, string quartet). Only a few studies, however, have\ndealt with MII in polyphonic music. In MII in polyphonicmusic, there are three issues: feature variations causedby sound mixtures, the pitch dependency of timbres, andthe use of musical context. For the ﬁrst issue, templatesof feature vectors representing timbres are extracted fromnot only isolated sounds but also sound mixtures. Be-cause some features are not robust in the mixtures, fea-tures are weighted according to their robustness by using\nlinear discriminant analysis. For the second issue, we use\nanF0-dependent multivariate normal distribution , which\napproximates the pitch dependency as a function of funda-mental frequency. For the third issue, when the instrumentof each note is identiﬁed, the a priori probablity of the noteis calculated from the a posteriori probabilities of tempo-rally neighboring notes. Experimental results showed thatrecognition rates were improved from 60.8% to 85.8% fortrio music and from 65.5% to 91.1% for duo music.\nKeywords: Musical instrument identiﬁcation, mixed-\nsound template, F0-dependent multivariate normal distri-\nbution, musical context, MPEG-7\n1 INTRODUCTION\nThe increasing quantity of musical audio signals available\nin electric music distribution services and personal mu-sic storage has made users spend a longer time on ﬁnding\nmusical pieces that they want. Efﬁcient music information\nretrieval (MIR) technologies are indispensable to shortenthe time to ﬁnd musical pieces. In particular, automaticdescription of musical content in a universal framework isexpected to become one of the most important key tech-nologies for achieving sophisticated MIR. In fact, the ISOrecently established a new standard called MPEG-7 [1],\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonwhich provides a universal framework for describing mul-\ntimedia content.\nThe names of musical instruments play an impor-\ntant role as music descriptors because musical pieces aresometimes characterized by what instruments are used. In\nfact, the names of some music genres are based on in-strument names, such as “piano sonata” and “string quar-tet.” In addition, when a user wants to search for certaintypes of musical pieces, such as piano solos or string quar-tets, a retrieval system can use the description of musicalinstrument names. Therefore, musical instrument iden-tiﬁcation (MII), which aims at determining what instru-ments are used in musical pieces, has been studied in re-cent years [2, 3, 4, 5, 6, 7, 8].\nIdentifying instruments in polyphonic music is more\ndifﬁcult than in monophonic music. In fact, most meth-\nods of identifying monophonic sounds [3, 4, 7, 9] of-ten fail in dealing with polyphonic music. For example,our previous method [9], which identiﬁed an instrumentby calculating the similarities between a feature vectorof a given isolated sound and prestored feature vectorsof instrument-labeled sounds (called training data ), had\ndifﬁculty dealing with polyphonic music because featuresextracted from simultaneously played instruments weredifferent from those extracted from monophonic sounds.\nTo achieve highly accurate MII in polyphonic music,\nit is essential to resolve three issues: feature variationscaused by sound mixtures, the pitch dependency of tim-bres, and the use of musical context. These issues, how-\never, have not been fully dealt with in existing studies.\nSome techniques such as time-domain waveform templatematching [5], feature adaptation [6] and the missing fea-ture theory [2] have been proposed to address the ﬁrst is-sue, but no attempts have been made to construct a tem-plate from polyphonic music although this is expected tocontribute to improving MII. To address the second issue,most existing studies have used multiple templates cov-ering the entire pitch range for each instrument, but theyhave not dealt with effective modeling of the pitch depen-dency of timbres. To address the third issue, Kashino et\nal.[5] introduced music stream networks and proposed a\ntechnique of propagating the a posteriori probabilities ofmusical notes in a network to one another based on theBayesian network. To apply musical context to identi-ﬁcation frameworks not based on the Bayesian network,however, we need an alternative solution.\nIn this paper, to address the ﬁrst issue, we construct a\nfeature vector template (i.e., a set of training data) from\npolyphonic sound mixtures. Because features tend to vary\n558in similar ways during both training and identiﬁcation,\nthis method can improve MII. Furthermore, because therobustness of features to the mixtures can be analyzedthrough their variances in the mixture-based templates,features are weighted according to their robustness by\nusing linear discriminant analysis. To address the sec-ond issue, we use a pitch-dependent timbre model, calledF0-dependent multivariate normal distribution , which we\nproposed in our previous paper [9] for isolated sounds.This model represents the pitch dependency of each fea-ture of instrument timbres as a function of fundamentalfrequency (F0) and is expected to remain effective forpolyphonic mixtures. To address the third issue, we cal-culate the a priori probability of each note from the a pos-teriori probabilities of its temporally neighboring notes.This method aims at avoiding musically unnatural errors\nby considering temporal continuity of melodies; for ex-ample, if the identiﬁed instrument names of a successivenote sequence are all “ﬂute” except for one “clarinet,” thisexception can be considered an error and corrected.\nThe rest of this paper is organized as follows: Sec-\ntion 2 discusses the three issues involved in applying MIIto polyphonic music. Sections 3, 4 and 5 describe our so-lutions to the three issues. Section 6 reports the results ofour experiments and Section 7 concludes the paper.\n2 INSTRUMENT IDENTIFICATION IN\nPOLYPHONIC MUSIC\nThe aim of our study is to identify, when an audio sig-\nnal of polyphonic music is given, which musical instru-ments are being used to play this musical piece. In a typ-ical framework, an MII system ﬁrst detects musical notesand then identiﬁes the name of the instrument for each\nnote, because the musical audio signals handled here con-tain many musical notes, which are often simultaneouslyplayed. Once a musical note is detected, a feature vectorxconcerning the note is extracted. Then, the a posteriori\nprobability given by p(ω\ni|x)=p(x|ωi)p(ωi)/p(x)is cal-\nculated, where ωidenotes an instrument ID, p(x|ωi)and\np(ωi)are a probability density function (PDF) and the a\npriori probability of the instrument ωi. Finally, the instru-\nment maximizing p(ωi|x)is determined as an MII result.\nAs previously mentioned, dealing with polyphonic\nmusic is much more difﬁcult than with monophonic mu-\nsic. The main issues in dealing with polyphonic music canbe summarized as follows:\nIssue 1 Feature variations caused by sound mixtures\nThe main difﬁculty of dealing with polyphonic music\nlies in the fact that it is impossible to extract acous-tical features of each instrument without blurring be-cause of the overlapping of frequency components. If aclear sound for each instrument could be obtained withsound separation technology, the identiﬁcation of poly-phonic music might result in identifying monophonicsounds. In practice, however, it is very difﬁcult to sep-arate a mixture of sounds without distortion occuring.\nTo achieve highly accurate identiﬁcation, it is neces-sary to deal with feature variations caused by the over-lapping of frequency components.\nIssue 2 Pitch dependency of timbres\nThe pitch dependency of timbres also makes MII dif-ﬁcult. In contrast to other sound sources includinghuman voices, musical instruments have wide pitch\nFigure 1: Example of musically unnatural errors. This example\nis an excerpt from results of identifying each note individually in\na piece of trio music. The marked notes are musically unnaturalerrors, which can be avoided by using musical context. PF,VN,\nCLandFLrepresent piano, violin, clarinet and ﬂute.\nranges. For example, the pitch range of pianos cov-\ners over seven octaves. Such a wide pitch range makestimbres quite different from pitch to pitch. It is there-fore necessary to deal with this pitch dependency toattain accurate MII.\nIssue 3 Musical context\nWhen identifying the instrument playing a musical\nnote, a system should take identiﬁcation results of tem-porally neighboring notes into consideration due to thetime continuity of melodies. Individually identifyingthe instrument of each note sometimes causes musi-cally unnatural errors as can be seen in Figure 1 ( e.g.,\nonly one clarinet note in a melody played on a ﬂute).To avoid such musically unnatural errors, it is impor-tant to exploit musical context.\nWe resolve these issues with the following approaches:\nSolution 1 Mixed-sound template\nWe construct a feature vector template from poly-\nphonic sound mixtures (called a mixed-sound tem-\nplate ). It would be effective, not only because it is\nobtained from features that have already been affectedby other instruments playing simultaneously, but alsobecause it facilitates to weight features based on theirrobustness by applying linear discriminant analysis\n(LDA), which maximizes the ratio of the between-classcovariance to the with-in class covariance; features thatvary because of the overlapping of featurency compo-nents have high variances within the class (instrument),which are given low weights by LDA.\nSolution 2 Pitch-dependent timbre model\nWe use a pitch-dependent timbre model, called an\nF0-dependent multivariate normal distribution [9], to\nsolve the pitch dependency problem. It approximates\nthe pitch dependency of features representing the tim-\nbres of musical instruments as a function of F0. Al-though our previous experiments [9] showed the ef-fectiveness of this method for solo musical sounds, wehave not yet conﬁrmed it for polyphonic music.\nSolution 3 Musical-context-based a priori probabilities\nOur key idea in using musical context is to apply,when calculating the a postriori probability given byp(ω\ni|xk)=p(xk|ωi)p(ωi)/p(x)of a musical note nk,\nthe a posteriori probabilities of temporally neighboring\nnotes to the a priori probabilities.\n559Figure 2: Overview of the process of constructing a mixed-\nsound template. HSE and FE represent harmonic structure ex-\ntraction and feature extraction, respectively.\n3 MIXED-SOUND TEMPLATE\nAs previously described, we construct a feature vector\ntemplate, called a mixed-sound template , from polyphonic\nsound mixtures. Figure 2 has an overview of the processof constructing the mixed-sound template. The sound of\neach note used in making the template has been before-\nhand labeled with the instrument name, the pitch, the on-set time, and the duration. By using these labels, we ex-tract the harmonic structure corresponding to each notefrom the power spectrum. We then extract acoustic fea-tures from the harmonic structure. We thus obtain a set ofmany feature vectors extracted from sound mixtures.\nThe main issue in constructing the mixed-sound tem-\nplate is to design an appropriate subset of sound mixturesbecause there are an inﬁnite number of possible combi-nations of musical sounds. These combinations mainlyconsist of note combinations , related to which harmonic\ncomponents could be affected by other sounds, and instru-\nment combinations , related to how much each frequency\ncomponent is affected (the amount depends on the instru-\nment). In particular, the former causes a serious problembecause musical instruments have wide pitch ranges.\nTo solve this problem, we focus on the fact that not all\ncombinations usually appear in music. Because the com-plete set of combinations contains many disharmoniousones ( e.g., simultaneously playing three notes of C4, C#4\nand D4) that are rarely used in music, it is not necessary tocover all possible combinations. To obtain only combina-tions that actually appear in music, the template is madefrom ( i.e., is trained on) sound mixtures performed based\non the scores of musical pieces. Note that this training\ncould be done on musical pieces that are not used for iden-tiﬁcation. Because our method considers only relative-pitch relationship for feature extraction, once a note com-\nbination has been trained, other transposed note combina-\ntions are not generally necessary. We can thus expect thata sufﬁcient number of sound combinations can be trainedon a small set of musical pieces.\n4 F0-DEPENDENT MULTIV ARIATE\nNORMAL DISTRIBUTION\nThe key idea behind our method is to approximate the\npitch dependency of each feature representing the tim-bres of musical instrument sounds as a function of F0. AnF0-dependent multivariate normal distribution [9] has twoparameters: an F0-dependent mean function and an F0-\nnormalized covariance . The former represents the pitch\ndependency of features and the latter represents the non-\npitch dependency. The reason why the mean of a distribu-tion of tone features is approximated as a function of F0is that tone features at different pitches have different po-sitions (means) of distributions in the feature space. Ap-\nproximating the mean of the distribution as a function ofF0 makes it possible to model how the features will varyaccording to the pitch with a small set of parameters.\n4.1 Parameters of F0-dependent multivariate\nnormal distribution\nThe following two parameters of the F0-dependent multi-\nvariate normal distribution N\nF0(µi(f),Σi)are estimated\nfor each instrument ωi.\n•F0-dependent mean function µi(f)\nFor each element of the feature vector, the pitch de-\npendency of the distribution is approximated as a func-tion (cubic polynomial) of F0 using the least square\nmethod.\n•F0-normalized covariance Σ\ni\nThe F0-normalized covariance is calculated with the\nfollowing equation:\nΣi=1\nni/summationdisplay\nx∈χi(x−µi(fx))(x−µi(fx))/prime,\nwhere χiis the set of the training data of the instrument\nωiandniis the total number. fxdenotes the F0 of the\nfeature vector x.\n4.2 Bayes decision rule for F0-dependent\nmultivariate normal distribution\nOnce the parameters of the F0-dependent multivariate nor-\nmal distribution have been estimated, the Bayes decision\nrule is applied to identify the name of the instrument. TheBayes decision rule for the F0-dependent mutltivariatenormal distribution is given by the following equation [9]:\nˆω=a r g m a x\nωi/braceleftBig\n−1\n2D2\nM(x,µi(f))\n−1\n2log|Σi|+l o gp(ωi)/bracerightBig\n,\nwhere D2\nMis the squared Mahalanobis distance deﬁned by\nD2\nM(x,µi(f)) = ( x−µi(f))/primeΣ−1\ni(x−µi(f)).\nThe a priori probability p(ωi)is determined based on the\na posteriori probabilities of temporally neighboring notes\nas described in the next section.\n5 USING MUSICAL CONTEXT\nAs previously mentioned, the key idea behind using mu-\nsical context is to apply, when calculating the a posterioriprobability given by p(ω\ni|xk)=p(xk|ωi)p(ωi)/p(x)of\na musical note nk, a posteriori probabilities of temporally\nneighboring notes to the a priori probability p(ωi)of the\nnotenk(Figure 3). To achieve this calculation, we have\nto resolve the following two issues:\nIssue 1 How to ﬁnd notes that are played on the same\ninstrument as the note nkfrom neighboring notes.\nBecause various instruments as well as that for the note\nnkare played at the same time, an identiﬁcation system\nhas to ﬁnd notes that are played on the same instrumentas the note n\nkfrom notes on the various instruments.\nThis is not easy because it is mutually dependent onmusical instrument identiﬁcation.\nIssue 2 How to calculate a posteriori probabilities of\nneighboring notes.\nCalculating the a posteriori probabilities of temporally\n560Figure 3: Basic idea for using musical con-\ntext. To calculate the posteriori probability of\nthe note nk, the a posteriori probabilities of\ntemporally neighboring notes of nkare used.Figure 4: An example of judgment of whether notes are played on the same instrument\nor not. The tuple “ (a, b) ” in the ﬁgure represents sh(nk)=aand sl(nk)=b.\nneighboring notes also suffers from the mutual depen-\ndency problem, that is, p(ωi|xk)is calculated from\np(ωi|xk−1)etc., while p(ωi|xk−1)is calculated from\np(ωi|xk)etc.\nWe resolve these issues as follows:\nSolution 1 Use of musical role consistency\nTo solve Issue 1 , we exploit musical role consistency ,\nwhich is musical heuristics that means each instrument\nhas a single musical role ( e.g., a principal melody or\nbass line) from the beginning to the end of a musi-cal piece. Kashino et al. [5] also used musical role\nconsistency to generate music streams. They designedtwo kinds of musical roles: the highest and lowestnotes (usually corresponding to the principal melodyand bass lines). This method, however, had prob-lems in that it could cause ambiguity when appliedto a piece where four or more instruments are beingplayed simultaneously and that it could mistakenly de-\ntermine, when a principal melody was temporarily ab-sent, the highest note to be a principal melody. To helpsolve these problems, in this paper, we deﬁne a mu-sical role as being based on how many simultanouslyplayed notes there are in the higher or lower pitchrange. Let s\nh(nk)andsl(nk)be the maximum num-\nber of simultaneously played notes in the higher and\nlower pitch ranges when the note nkis being played,\nrespectively. Then, the two notes, nkandnj, are con-\nsidered to be played on the same instrument if and onlyifs\nh(nk)=sh(nj)andsl(nk)=sl(nj)(Figure 4).\nSolution 2 Two-pass calculation\nTo solve Issue 2 , we pre-calculate a posteriori proba-\nbilities without musical context. After this calculation,we calculate them again using the a posteriori proba-\nbilities of temporally neighboring notes.\n[1st pass] Pre-calculation of a posteriori probabilities\nFor each note n\nk, the a posteriori probability p(ωi|xk)\nis calculated by considering the a priori probability p(ωi)\nto be a constant, because the a priori probability, which\ndepends on the a posteriori probabilities of temporallyneighboring notes, cannot be determined in this step.\n[2nd pass] Re-calculation of a posteriori probabilities\nThis pass consists of three steps:(1)Finding notes played on same instrument\nNotes that satisfy {n\nj|sh(nk)=sh(nj)∩sl(nk)=\nsl(nj)}are extracted from temporally neighboring\nnotes of nk. This extraction is performed from the\nnearest notes to the farthest notes, and stops when c\nnotes are extracted. Let Nbe the set of the extracted\nnotes.\n(2)Calculating a priori probability\nThe a priori probability of the note nkis calculated\nbased on the a posteriori probabilities of the notes ex-\ntracted in the previous step. Let Znkbe a random\nvariable that represents the instrument for the note nk.\nThen, the probability p(Znk=ωi)that the instrument\nfor the note nkwill be ωiis the a priori probability to\nbe calculated and can be expanded as follows:\np(Znk=ωi)\n=p(Znk=ωi/vextendsingle/vextendsingle∀nj∈N:Znj=ωi)\n×/productdisplay\nnj∈Np(Znj=ωi)\nThe ﬁrst factor of the right side of this equation repre-\nsents the probability that the note nkwill be played on\nthe instrument ωiwhen all the extracted neighboring\nnotes of nkare played on ωi. Although this can be ac-\nquired through statistical analysis, we use 1−(1/2)2c\nfor simplicity. This is based on the heuristics that, asmore notes are used to represent a context, the contextinformation is more reliable. The a posteriori proba-bility calculated in the ﬁrst pass is used to calculate\np(Z\nnj=ωi).\n(3)Updating a posteriori probability\nThe a posteriori probability is re-calculated using the apriori probability calculated in the previous step.\n6 IMPLEMENTATION\n6.1 Overview\nFigure 5 has an overview of our MII system. Given an\naudio signal of polyphonic music, the system ﬁrst calcu-\nlates a spectrogram using the short-time Fourier transform(STFT) and then obtains the pitch, the onset time and the\nduration of each note. Because our focus was solely onevaluating the performance of MII by itself, we manuallyfed correct note data into the system. Then, it identiﬁesthe instrument of each note through four steps of feature\n561Figure 5: Overview of our musical instrument identiﬁca-\ntion system.\nextraction, dimensionality reduction, a posteriori proba-\nbility calculation, and instrument determination.\n6.2 Short-time Fourier transform\nThe spectrogram of the given audio signal is calculated by\nSTFT shifted by 10 ms (441 points at 44.1 kHz sampling)\nwith an 8192-point Hamming window.\n6.3 Harmonic structure extraction\nThe harmonic structure of each note is extracted according\nto the manually fed note data. Spectral peaks correspond-ing to the ﬁrst 10 harmonics are extracted from the onset\ntime to the offset time. Then, the frequency of the spectralpeaks are normalized so that the temporal mean of F0 is 1.\nThen, the harmonic structure is trimmed because train-\ning and identiﬁcation need notes with a ﬁxed-length du-ration. Because a template with a long duration is morestable and robust than a template with a short one, it isbetter to trim a note as long as possible. We therefore pre-pare three templates with different durations (300, 450,\nand 600 ms), and the longest within the actual duration of\neach note is automatically selected and used for trainingand identiﬁcation. Notes shorter than 300 ms are excludedfrom identiﬁcation.\n6.4 Feature extraction\nFeatures that are useful for identiﬁcation are extracted\nfrom the harmonic structure of each note. From a featureset that we previously proposed [9], we selected 43 fea-tures (for the 600-ms template), summarized in Table 1,that we expected to be robust with respect to sound mix-tures. We use 37 and 31 feautres for the 450- and 300-Table 1: Overview of 43 features\nSpectral features\n1\n Spectral centroid\n2\n Relative power of fundamental component\n3\n–\n10\n Relative cumulative power from fundamental to\ni-th components ( i=2 ,3,··· ,9)\n11\n Relative power in odd and even components\n12\n–\n20\n Number of components whose duration is p%\nlonger than the longest duration\n(p=1 0 ,20,··· ,90)\nTemporal features\n21\n Gradient of straight line approximating power\nenvelope\n22\n–\n30\n Average differential of power envelope duringt-sec interval from onset time\n(t=0 .15,0.20,0.25,··· ,0.55[s])\n31\n–\n39\n Ratio of power at t-sec after onset time\nModulation features\n40\n,\n41\n Amplitude and Frequency of AM\n42\n,\n43\n Amplitude and Frequency of FM\nms template, respectively, because some features are ex-\ncluded due to limitations with note durations.\n6.5 Dimensionality reduction\nThe dimensionality of the 43-, 37- or 31-dimensional fea-\nture space is reduced through two successive processingsteps: it is ﬁrst reduced to 20 dimensions by applying PCAwith a proportion value of 99%, and then further reducedby applying LDA. The feature space is ﬁnally reduced to a3-dimensional space when we deal with four instruments.Because LDA is a dimensionality reduction technique thatmaximizes the ratio of the between-class covariance to thewithin-class covariance, it enables us to set high weightsfor robust features with respect to sound mixtures.\n6.6 A posteriori probability calculation\nFor each note n\nk, the a posteriori probability p(ωi|xk)\nis calculated. This is based on the two-pass method, de-\nscribed in Section 5, with the F0-dependent multivariatenormal distribution, described in Section 4.\n6.7 Instrument Determination\nThe instrument maximizing the a posteriori probability\np(ω\ni|xk)is determined as the result for the note nk.\n7 EXPERIMENTS\n7.1 Data for experiments\nWe used two kinds of musical audio data: isolated notes\nof solo instruments, and polyphonic music. The dataon the isolated notes were excerpted from\nRWC-MDB-I-\n2001 [10], and were used to create the audio data for the\npolyphonic music, as well as to obtain the solo-sound tem-plate. Details on the solo-instrument audio data are listedin Table 2. All data were sampled at 44.1 kHz with 16 bits.\nBoth trio and duo music were used as polyphonic mu-\nsic, and their audio data were generated by mixing the\naudio data listed in Table 2 according to standard MIDIﬁles (SMFs) on a computer. The SMFs we used in theexperiments were three pieces taken from\nRWC-MDB-C-\n2001 (Piece Nos. 13, 16 and 17) [11]. We chose three or\n562Table 2: Audio data on solo instruments\nInstr.\n Name\n Pitch\n Vari-\n Dynam-\n Articu-\n #o f\nNo.\n Range\n ation\n ics\n lation\n data\n01\n Piano ( PF)\n A0–C8\n 1, 2, 3\n Forte,\n 792\n15\n Violin ( VN)\n G3–E7\n /prime/prime\n Mezzo\n Normal\n 576\n31\n Clarinet ( CL)\nD3–F6\n /prime/prime\n &\n only\n 360\n33\n Flute ( FL)\n C4–C7\n 1, 2\n Piano\n 221\nTable 3: Experimental results\n(a) (b) (c) (d) (e)\nMS Templ.\n ——√√√\nF0-dpt.\n —√—√√\nContext\n —√√—√\nPF\n 83.4%\n 88.6%\n 95.1%\n 83.8%\n 91.6%\nTrio VN\n 70.8% 86.9%\n 83.8%\n 74.6% 86.8%\nNo.13 CL\n 45.0% 34.8% 78.4% 77.3% 85.5%\nFL\n 57.8% 60.1% 81.2%\n 77.2% 81.8%\nPF\n 86.7%\n 95.6%\n 97.4%\n 91.2%\n 97.7%\nTrio VN\n 60.7% 73.4% 84.5%\n 60.9% 82.9%\nNo.16 CL\n 52.9% 29.0% 89.1%\n 78.0% 89.8%\nFL\n 50.1% 65.7% 79.0% 71.9% 80.9%\nPF\n 80.0% 87.5%\n 87.5%\n 83.9%\n 91.1%\nTrio VN\n 56.5% 71.0% 71.0% 66.8% 84.8%\nNo.17 CL\n 33.8% 19.4% 74.5% 66.0% 78.6%\nFL\n 52.3% 51.1% 77.0% 75.0% 78.6%\nAverage\n 60.8% 63.6% 83.2% 75.6% 85.8%\nPF\n 92.3%\n 96.9%\n 98.8%\n 91.4%\n 97.4%\nDuo VN\n 71.1% 86.6%\n 85.1%\n 71.1% 90.2%\nNo.13 CL\n 58.5% 52.1% 93.6%\n 83.0%\n 93.6%\nFL\n 56.4% 54.5% 86.1%\n 76.2% 91.1%\nPF\n 93.6%\n 98.6%\n 99.0%\n 95.3%\n 98.7%\nDuo VN\n 64.0% 74.0% 86.0%\n 58.9% 78.4%\nNo.16 CL\n 63.4% 37.3% 95.4%\n 83.7%\n 94.1%\nFL\n 47.5% 61.7% 83.0%\n 69.5% 86.5%\nPF\n 89.0%\n 94.9%\n 94.5%\n 92.3%\n 96.7%\nDuo VN\n 60.3% 80.3%\n 74.7% 69.1% 91.9%\nNo.17 CL\n 41.8% 30.7% 92.8%\n 74.5% 91.5%\nFL\n 48.5% 50.9% 77.8% 74.9% 82.6%\nAverage\n 65.5% 68.2% 88.9% 78.3% 91.1%\nMS. Templ.: Mixed-sound template.\nSingle- and double-underlined numbers denote recognition rates\nof more than 80% and 90%, respectively.\ntwo simultaneous voices from each piece to generate trio\nor duo music from these SMFs. To avoid using the sameaudio data for training and testing, we used\n011PFNOM ,\n151VNNOM ,311CLNOM , and 331FLNOM for the test data\nand the rest in Table 2 for the training data.\n7.2 Experimental results\nTable 3 lists results of experiments conducted with the\nleave-one-out cross-validation method. Using a mixed-sound template, an F0-dependent multivariate normal dis-tribution and musical context, we improved the recogni-tion rate of MII from 60.8% to 85.8% for trio music andfrom 65.5% to 91.1% for duo music, on average. The ob-servations of the results can be summarized as follows:\n•Effectiveness of the mixed-sound templateWhen we compared the case (e) with the case (b), therecognition rate was improved by more than 20% onaverage. In particular, those for\nCLandFLwere signif-\nicantly improved: from 20–65% to 78–94%.\n•Effectiveness of pitch-dependent model\nF0-dependent multivariate normal distribution im-proved the recognition rates, on average, from 83.2%to 85.8% for trio music and from 88.9% to 91.1% for\nduo music. This improvement, however, only occurredwhen the mixed-sound template was used.\n•Effectiveness of musical contextUsing musical context also improved the recognitionrates, on average, from 75.6% to 85.8% for trio musicand from 78.3% to 91.1% for duo music. This was be-cause, in the musical pieces used in our experiments,pitches rarely cross among the melodies of simultane-ous voices.\n8 CONCLUSION\nWe have described three methods that work in combi-\nnation to automatically generate the description of mu-sical instrument names for music information retrieval.To identify the name of the musical instrument perform-ing each note in polyphonic sound mixtures of musical\npieces, our methods solve three problems: feature varia-tions caused by sound mixtures, the pitch dependency oftimbres, and the use of musical context. In our experi-ments with three musical pieces including four musicalinstruments, we found that our methods achieved recogni-tion rates of 85.8% for trio music and 91.1% for duo music\non average and conﬁrmed the robustness and effectiveness\nof those methods. Future work will include to integrateour methods with a musical note estimation method be-cause the pitch and onset time of each note are manuallygiven in our experiments to reveal the performance of in-\nstrument identiﬁcatin.\nAcknowledgements: This research was partially supported\nby the Ministry of Education, Culture, Sports, Science and\nTechnology (MEXT), Grant-in-Aid for Scientiﬁc Research (A),\nNo.15200015, and Informatics Research Center for Develop-ment of Knowledge Society Infrastructure (COE program ofMEXT, Japan). We thank everyone who has contributed to build-\ning and distributing the RWC Music Database [10, 11].\nReferences\n[1] B. S. Manjunath, P. Salembier, and T. Sikora. Introduction\nof MPEG-7 . John Wiley & Sons Ltd., 2002.\n[2] J. Eggink and G. J. Brown. Application of missing fea-\nture approach to the recognition of musical instruments in\npolyphonic audio. Proc. ISMIR , 2003.\n[3] A. Eronen and A. Klapuri. Musical instrument recogni-\ntion using cepstral coefﬁcients and temporal features. Proc.\nICASSP , pages 735–756, 2000.\n[4] I. Fujinaga and K. MacMillan. Realtime recognition of or-\nchestral instruments. Proc. ICMC , pages 141–143, 2000.\n[5] K. Kashino and H. Murase. A sound source identiﬁcation\nsystem for ensemble music based on template adaptation\nand music stream extraction. Speech Communication , 27:\n337–349, 1999.\n[6] T. Kinoshita, S. Sakai, and H. Tanaka. Musical sound\nsource identiﬁcation based on frequency component adap-\ntation. Proc. IJCAI CASA Workshop , pages 18–24, 1999.\n[7] K. D. Martin. Sound-Source Recognition: A Theory and\nComputational Model . PhD thesis, MIT, 1999.\n[8] E. Vincent and X. Rodet. Instrument identiﬁcation in solo\nand ensemble music using independent subspace analysis.\nProc. ISMIR , pages 576–581, 2004.\n[9] T. Kitahara, M. Goto, and H. G. Okuno. Musical in-\nstrument identiﬁcation based on F0-dependent multivariatenormal distribution. Proc. ICASSP , volume V , pages 421–\n424, 2003.\n[10] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka. RWC\nmusic database: Music genre database and musical instru-ment sound database. Proc. ISMIR , pages 229–230, 2003.\n[11] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Popular, classical, and jazz musicdatabases. Proc. ISMIR , pages 287–288, 2002.\n563"
    },
    {
        "title": "Multiple Lyrics Alignment: Automatic Retrieval of Song Lyrics.",
        "author": [
            "Peter Knees",
            "Markus Schedl",
            "Gerhard Widmer"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415140",
        "url": "https://doi.org/10.5281/zenodo.1415140",
        "ee": "https://zenodo.org/records/1415140/files/KneesSW05.pdf",
        "abstract": "We present an approach to automatically retrieve and extract lyrics of arbitrary songs from the Internet. It is intended to provide easy and convenient access to lyrics for users, as well as a basis for further research based on lyrics, e.g. semantic analysis. Due to the fact that many lyrics found on the web suffer from individual errors like typos, we make use of multiple versions from different sources to eliminate mistakes. This is accomplished by Multiple Sequence Alignment. The different sites are aligned and examined for matching sequences of words, finding those parts on the pages that are likely to contain the lyrics. This provides a means to find the most probable version of lyrics, i.e. a version with highest consensus among different sources. Keywords: Lyrics, Web Mining, Multiple Sequence Alignment. 1",
        "zenodo_id": 1415140,
        "dblp_key": "conf/ismir/KneesSW05",
        "keywords": [
            "lyrics",
            "Internet",
            "automatic retrieval",
            "arbitrary songs",
            "web mining",
            "Multiple Sequence Alignment",
            "typos",
            "semantic analysis",
            "sites",
            "matching sequences"
        ],
        "content": "MULTIPLE LYRICS ALIGNMENT:\nAUTOMATIC RETRIEVAL OF SONG LYRICS\nPeter Knees1\npeter.knees@jku.atMarkus Schedl1,2\nmarkus.schedl@jku.at\n1Department of Computational Perception\nJohannes Kepler University Linz\nA-4040 Linz, Austria\n2Austrian Research Institute for Artiﬁcial Intelligence (O FAI)\nA-1010 Vienna, AustriaGerhard Widmer1,2\ngerhard.widmer@jku.at\nABSTRACT\nWe present an approach to automatically retrieve and ex-\ntract lyrics of arbitrary songs from the Internet. It is in-\ntended to provide easy and convenient access to lyrics\nfor users, as well as a basis for further research based\non lyrics, e.g. semantic analysis. Due to the fact that\nmanylyricsfoundonthewebsufferfromindividualerrors\nlike typos, we make use of multiple versions from differ-\nent sources to eliminate mistakes. This is accomplished\nby Multiple Sequence Alignment. The different sites are\naligned and examined for matching sequences of words,\nﬁnding those parts on the pages that are likely to contain\nthe lyrics. This provides a means to ﬁnd the most proba-\nbleversionoflyrics,i.e. aversionwithhighestconsensus\namong different sources.\nKeywords: Lyrics, Web Mining, Multiple Sequence\nAlignment.\n1 INTRODUCTION\nPeople like to sing their favorite songs or at least like to\nknow what they are hearing. Because it is often hard to\nunderstand all of the words in a song, it is convenient to\nhave them in a written form. The vast number of online\nlyrics portals is a response to this. However, even though\nthe portals have large numbers of available lyrics in their\ndatabases, none is complete. Another fact is that very\nfrequently the lyrics differ among the portals, e.g. due\nto simple typos, different words, or different annotation\nstyles. As a consequence, a user may be forced some-\ntimes to examine different sources and to ﬁgure out the\n“correct” lyrics. A simpliﬁed way to do so consists of\nmeta-searching lyrics portals (and the rest of the web) by\nsimply using a standard search engine like Google. Al-\nthoughthisprovidesafastwaytoﬁndlyrics,anefforthas\nto be made to obtain and compare them.\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrstpage.\nc/circlecopyrt2005 Queen Mary, University of LondonIn this paper, we suggest a technique to automatically\nextractthelyricsofarbitrarysongsfromtheInternet. Fur -\nthermore, the method presented provides a means to ﬁnd\nthe most probable version of lyrics, i.e. a version with\nhighest consensus among different sources. Besides en-\nabling users to easily retrieve lyrics, our approach can\nalso serve as a basis for further applications like seman-\nticanalysis or automatic karaoke annotation.\n2 RELATED WORK\nTo the best of our knowledge, no previous work on au-\ntomatic lyrics extraction has been published yet. Nev-\nertheless, work concerning the exploitation of semantics\nof lyrics in Music Information Retrieval exists. Scott\nand Matwin (1998) use two sets of more than 400 folk\nsongs for text categorization experiments. Extending the\ntraditional bag-of-words approach by integrating Word-\nNet hypernyms, classiﬁcation accuracy can be improved.\nIn Baumann and Kl ¨uter (2002) ontology-based document\nretrievalisappliedtocharacterizelyrics. Usingthisrep re-\nsentation, similarities between songs based on the corre-\nspondinglyricsareevaluated. InLoganetal.(2004)about\n16 000lyrics are gathered and used to determine artist\nsimilarity. To analyze and compare the semantic content,\nProbabilistic Latent Semantic Analysis is applied.\nFromanabstractpointofviewthisworkisalsorelated\nto the ﬁeld of automatic text summarization, e.g. Radev\netal.(2002). Sincethecentralideaistoextractaninteres t-\ning section from many similar documents and to remove\nunnecessaryparts,thegoalofthisworkcouldalsobefor-\nmulated as a multi-document summarization task. How-\never, current off-the-shelf approaches are not applicable\ndirectly to this speciﬁc task.\nApart from scientiﬁc research on semantic lyrics\nanalysis, supportive and convenient applications that as-\nsist users in the retrieval of lyrics are freely available on\ntheInternet. Themostnotableamongthemis EvilLyrics1.\nEvilLyricsiscapableofcooperatingwiththemostpopular\nmusic players and searches for lyrics as a song is played.\nFor given artist and track name, the result of a Google\nquery is examined for known lyrics providers. For each\nknown lyrics page a ﬁlter consisting of characteristic se-\nquences to determine start and end of the lyrics on the\npage has been written. The user is then presented with\n1http://www.evillabs.sk/evillyrics/\n564the bare lyrics and can choose between multiple versions\nfrom different web pages. This is a handy utility which\nreceived a lot of positive feedback from users, indicating\nthat automatic retrieval of lyrics is a desired feature for\nmany people. Although results are displayed quickly and\nmost are useful, this approach suffers from many draw-\nbacks. The ﬁrst is that the ﬁlters to extract the content are\nsimply based on key sequences for each page. Presum-\nably,mostchangestothestructureofthepagesrequirethe\nﬁltertobeadaptedmanually. Furthermore,theretrievalis\nlimited to a set of known lyrics portals, making it infeasi-\nble to exploit pages focusing, for example, only on lyrics\nfrom one particular artist, like ofﬁcial artist pages or fan\npages. Finally, ﬁnding the best version of the lyrics is left\nto the user.\nIncontrasttothis,ourapproachautomaticallyextracts\nlyrics without knowledge about the structure of particular\nweb pages. It is capable of using information from all\nkinds of web pages. Multiple sites are processed and ex-\namined for matching sequences of words, ﬁnding those\nparts on the pages that supposably contain the lyrics.\n3 ANALYSIS OF LYRICS ON THE WEB\nInthissection,wewanttogiveanoverviewofvariousan-\nnotation characteristics in lyrics. We do not claim the fol-\nlowing listing to be complete; it is only intended to point\nout some of the difﬁculties that occur when comparing\nlyrics frommultiple sources.\n•Different spellings of words: Beside unintended ty-\npos, words can have different morphologic appear-\nances. For example, the slang term causeis often\nfound to be written as ’cause,’coz,cuz, or even cor-\nrectly asbecause. Although the semantic content is\nthe same, these variations can not be handled with\nsimple string comparison. Similar observations can\nbe made for numbers (e.g. 40’svs.forties) and cen-\nsored words ( f**k).\n•Differences in the semantic content : These result\nfrom the simple fact that not all people contributing\nto lyrics pages understand the same words. Many\nperspicuous examples of misheard lyrics can be\nfound on the web2. For this reason, most of the\nlyrics portals offer the possibility to rate the quality\nof lyrics or to submit corrections.\n•Different versions of songs: Querying the web with\nthenameofthetrackmayresultinavarietyoflyrics\nwhich are all “correct”, but highly inconsistent, e.g.\ndue to cleaned versions or changes over time. Also\ntranslations of lyrics can be found frequently.\n•Annotation of background voices, spoken text, and\nsounds: For songs where phrases are repeated, for\nexample by background singers, in some lyrics ver-\nsions these repetitions are written out. Sometimes\neven comments ( yeah I know , etc.) or sounds\nfrom the background ( *scratches* ,etc.) are explic-\nitly noted.\n2http://www.amiright.com/misheard/•Annotation of chorus, verses, and performing artist :\nSome authors prefer to give meta-information about\nthe structure of the song and explicitly annotate the\ntype of the subsequent section, e.g. chorus,verse 1,\nhook,pre-chorus etc. In duets, the artist to sing the\nnext part is often noted (e.g. [Snoop - singing] ).\n•Referencesandabbreviations ofrepetitions : Tokeep\nthe lyrics compact, to minimize redundancies and\nto avoid unnecessary typing effort, lyrics are rarely\nfoundtobewrittencompletely. Rather,referencesto\nearlier sections ( repeat chorus ) and instructions for\nmultiplerepetitions( x4)areused,whichareverydif-\nﬁcult to handle for a machine, since they can occur\nin variable form ( x2,(4x),repeat two times , or even\nchorus x1.5 ).\nBeside these differences in content, also trivial devia-\ntions like upper-/lowercase inconsistencies, or bracketi ng\nof words have tobe handled.\n4 METHODOLOGY\nOur approach consists in three main steps: gathering the\nlyricsfromtheInternet,aligningthelyricstoﬁndthecor-\nrespondingparts,andproducinganoutputstringbasedon\nthealignment. Themainideabehindtheproposedmethod\nis to identify and extract the lyrics by ﬁnding large seg-\nments of text that are common to web pages returned by\nGoogle when queried with a song title. The consistent\nparts among these pages are considered to be the lyrics.\nThe inconsistent rest is usually irrelevant page speciﬁc\ncontent like advertisements, site navigation elements, or\ncopyright information.\n4.1 Gathering the lyrics\nTheﬁrststepwehavetoperformistoobtaindifferentweb\npages containing the lyrics. To retrieve pages we send\nqueries of the form “artist name” “track name” lyrics to\nGoogle. From the retrieved pages we remove all HTML\ntags, as well as all links, and convert them to lower case,\nso only the plain content is used for lyrics extraction. We\nassume most of the resulting pages to contain lyrics or\nat least to contain a link to a page containing the lyrics.\nSince the decision on which pages contain the lyrics is a\nnon-trivial task, we examined two approaches to exclude\npages without lyrics.\n4.1.1 Page selection\nThe ﬁrst page selection approach simply collects pages\ncontaining the artist’s name, the name of the track and\nthe word lyricsin their title (in the following kwit, for\nkeywords-in-title). Result pages that do not comply with\nthese requirements are scanned for hyperlinks that con-\ntainthenameofthetrack. Theﬁrstlinkedpagehavingall\nkeywords in the title is then used instead. We examine up\nto 50 results to ﬁnd at most ten pages fulﬁlling the kwit\nconditions.\nThe second approach (in the following denoted by tf-\ncorr, for term frequency correlation) tries to ﬁnd pages\ncontaining lyrics independently of the page title and is\n565thus intended to be the more general page selection\nmethod. To this end, the ﬁrst ten accessible pages are re-\ntrieved. Eachpageisthentransformedintoavectorrepre-\nsentationbycountingtheoccurrencesofallwords(except\nfor stopwords) appearing on the page ( term frequency ).\nUsing these vectors, for all pages pairwise correlation is\ncalculated,assuminghighcorrelationforpagescontainin g\nlarge portions of similar text. To avoid including a page\ncontaining only the hyperlink to the actual lyrics page, all\npages (starting with the lowest in the Google ranking) are\nscanned for links containing the track title. If the average\ncorrelation for a linked page with the other pages in the\nset is higher than for the original page, the linked page is\nconsidered to contain the lyrics and is taken instead.\n4.1.2 Page preprocessing\nFirst experiences with the alignment algorithm (see be-\nlow) showed that frequently the matching of multiple\nsources works well at the beginning of songs and increas-\ningly gets confused after the second verse. This is caused\nby the fact that some pages simply refer to the ﬁrst cho-\nrus (e.g. repeat chorus ) while others rewrite the chorus\neachtime. Also,differentannotationsofrepetitivephras es\nraise a problem here (see Section 3 for a more detailed\ndiscussion). For this reason, we decided to perform a\npreprocessing step of the pages before actually trying to\nmerge them. To this end we make use of the structure\nin the annotation, if available. The ﬁrst thing to do is to\nﬁnd a paragraph that is preceded by a line starting with\nthe words chorusorrefrain. Subsequently, all other oc-\ncurrences of lines starting with (repeat) chorus orrefrain\nare replaced by this paragraph. After that, we perform an\nexpansion step. Repeating phrases are inserted as indi-\ncated. Therefore, we search for occurences of the letter\nxin combination with a digit (e.g. 2x,x4) and for the\npatternrepeat /angbracketleftdigit /angbracketrighttimes. Insertion is applied for lines\n(having e.g. x4at the end of the line), as well as for para-\ngraphs (x4at beginning or end of a paragraph). This im-\nproves performance of the following alignment step sig-\nniﬁcantly. Furthermore, it is also advantageous to have\nthe complete written form of a song, instead of an abbre-\nviated version, since further applications, e.g. semantic\nanalysis, may proﬁt from using the actual content rather\nthan using meta-notation.\n4.2 Aligning the lyrics\nThe problem with most pages is that they do not solely\ncontain lyrics. In most cases the lyrics are surrounded\nby advertisements, informations about the page, links\nto other lyrics, links to ringtones, or notices concerning\ncopyright. In average, about 43% of the content on pages\nisirrelevant. Toﬁndtheactuallyrics,weperformMultiple\nSequence Alignment (MSA). This technique is borrowed\nfrom Bioinformatics, where it is used to align DNA and\nprotein sequences. For our purposes, we can use it to ﬁnd\nnearly optimal matching word sequences over all lyrics\npages. Hence,MSAallowstodiscovertheconsistentparts\ninthepages(thelyrics),aswellastheinconsistent(their -\nrelevant text fragments from the sites).\nFor alignment, we transform the web pages into se-quences of words. To simplify the alignment, the words\nare reduced to a basic morphological version, i.e. all\nnon-lettersandnon-digitslikeparentheses,brackets,cu rly\nbrackets, dots, semicolons etc. are removed and special\ncharacters, for example characters with acute or dieresis,\naresubstitutedbytheirbasicequivalent(e.g. ´a,`a,˜a,ˆa,and\n¨a are replaced by a).\n4.2.1 Needleman-Wunsch algorithm\nTo align two sequences, we use the Needleman-Wunsch\nalgorithm (Needleman and Wunsch, 1970). It is based on\ndynamic programming and returns the globally optimal\nalignment of two strings for a given scoring scheme. For\nour case, we decided to reward matching pairs of words\nwith a high score (i.e. 10) and to penalize the insertion of\ngaps with a low value (i.e. −1). For mismatching words,\nwe deﬁned a score of 0. Using this conﬁguration, we ex-\npectthealgorithmtoﬁndlargecoherentsegmentswithout\ngapinsertion. Fornon-matchingtermswithinsuchalarge\nsegment,wewantthevaryingtermstobealignedtogether\nrather than gaps in both alignments which might lead to\nboth terms being output in some cases.\nThe algorithm itself uses a two-dimensional array\nMATof size (n+ 1) ×(m+ 1), where nis the length\n(the number of words) of the ﬁrst sequence Aandmthe\nlength of the second sequence B. The extra column and\nthe extra row in the array are necessary to enable gaps at\nthe beginning of each sequence. Every entry MAT i,jis\ninterpretedastheoptimalscoreofthepartialalignmentof\na1,...,a iandb1,...,b j. Thus, MAT n,mcontainsthescore\nfor the optimal alignment of AandB.\nEntries of MATare computed recursively. To calcu-\nlateMAT i,jtheoptimalchoiceforthenextalignmentstep\nis made by examining the three following cases (in the\ngiven order):\n1. Alignment of aiandbj. This is equivalent to a di-\nagonal step to the lower right in the array. Thus, the\nscoreatposition MAT i,jisdeterminedasthesumof\nMAT i−1,j−1andthescoregainedthroughalignment\nofaiandbj.\n2. Alignment of aiwith a gap. This is equivalent to\na step down. The score at position MAT i,jis then\ndetermined as the sum of MAT i−1,jand the penalty\nfor gap insertion.\n3. Alignment of bjwith a gap. This is equivalent to a\nsteptotheright. Thescoreatposition MAT i,jisthen\ndetermined as the sum of MAT i,j−1and the penalty\nfor gap insertion.\nConsidering these three options, the one to achieve the\nhighest score is chosen. Substituting the values we chose\nfor our lyrics alignment task, this gives us\nMAT i,j=max\n\nMAT i−1,j−1+d(ai,bj)\nMAT i−1,j −1\nMAT i,j−1 −1(1)\nwhere\nd(x,y) =/braceleftBigg\n10,if x=y,\n0,otherwise.(2)\n566After computation of all array entries, a traceback\nphaseisnecessarytodeterminetheactualalignmentfrom\nthescoringmatrix. Startingfrom MAT n,manddepending\non the origin of the score in the entries, the alignment is\nbuilt backwards until MAT 0,0is reached.\n4.2.2 Multiple Sequence Alignment\nTheprincipleoftheNeedleman-Wunschalgorithmisthe-\noretically easily extendable to more than two sequences.\nHowever, for two sequences this algorithm already uses\nO(n·m)in space and time. For every additional se-\nquence the effort is further multiplied by the length of the\nsequence and is thus not practicable. To circumvent this\nproblem, we decided to implement a hierarchical align-\nment, as proposed e.g. in Corpet (1988).\nIn the following, we will use the term rowto denote\nasequencewithinacompletealignmentandtheterm col-\numnto denote the list of words that have been aligned\ntogether on a position in the alignment (cf. Figure 1).\nFurthermore, the term lengthwill be used to refer to the\nnumber of words in a row (i.e. the number of columns in\nan alignment), while the term depthrefers to the number\nof sequences in a column. For example, the alignment in\nFigure 1 has depth four.\nNext, we will describe the hierarchical multiple se-\nquence alignment. For all pairs of sequences, pairwise\nalignmentusingtheNeedleman-Wunschalgorithmisper-\nformed. The pair achieving the highest score is aligned.\nThis step is performed again on the remaining sequences,\nuntil all are aligned (in case of an odd number of se-\nquences, the last remains unaligned). The principle of\npairwise alignment with respect to the highest score is\nthen again applied to the group of aligned sequences, un-\ntil only one alignment remains. For being able to perform\npairwisealignmentonpre-alignedsequencesconsistingof\nmultiple rows, we have to adapt the Needleman-Wunsch\nalgorithm. The basic idea is that words that already have\nbeenaligned,remainaligned. Thus,thealgorithmmustbe\ncapableofcomparingcolumnsofwords,insteadofsingle\nwords. We achieved this by a simple modiﬁcation of the\nscoring scheme.\nMAT′\ni,j=max\n\nMAT′\ni−1,j−1+dcol(a′\ni,b′\nj)\nMAT′\ni−1,j −1\nMAT′\ni,j−1 −1(3)\nwhere a′\niis theithcolumn in alignment A′with depth k,\nb′\njthejthcolumn in alignment B′with depth l, and\ndcol(x,y) =k/summationdisplay\ni=1l/summationdisplay\nj=1d(xi,yj), (4)\nwithxidenotingthe ithwordincolumn x,andyjthejth\nword in column y.\n4.3 Producing the result\nGiven the multiple lyrics alignment, we are able to pro-\nduce a string containing the lyrics. To this end we exam-\nineeveryalignedcolumnandﬁndthemostfrequentwordwin it. If the column’s most frequent word is a gap, the\ncolumn is skipped. Additionally, a threshold parameter t\nis introduced to determine if wis accepted. If the ratio\nof occurrences of win the column and the depth of the\nalignment is below t, then the column is skipped too.\nBefore producing the actual output, a preliminary\nalignment using a threshold tof 0.3 is performed. This\nalignment is used to eliminate pages not containing the\nlyrics at all. For this purpose, a temporary output is gen-\nerated as described above, and the agreement of each row\nin the alignment with the temporary output is computed.\nThis is accomplished by examining every column which\ncontributed to the result and checking for each row if the\nwordinthiscolumnmatchestheresult. Theagreementof\neach sequence is then computed as the fraction of match-\ning words and the total length of the result. Sequences\nwith agreement below 0.33are removed. After this elim-\nination step the alignment is performed on the remaining\nsequences. In Figure 1 a sample section from one such\nalignment is depicted. It can be seen that different pages\nprovide different lyrics. Despite the variations in the dif -\nferent sequences, our approach extracts the correct ver-\nsion. For ﬁnal output generation, the original words are\nreused instead of the basic morphologic words.Therefore\nthe most frequent version among the agreeing words is\nchosen.\n4.3.1 Smoothing\nWhen looking at the produced results, frequently, words\nnot part of the lyrics can be found at the beginning and at\nthe end. This is one of the negative effects of good align-\ning, since the algorithm strives to ﬁnd as many matching\nwords as possible. Therefore, results are often preceded\nby repetitions of the artist name, the track name, and the\nwordlyrics, since these tend to occur on different pages\nabove the lyrics. After the lyrics occurrences of words\nlikecopyright orlegalcan be observed. To remove such\nwords we perform a smoothing on the output sequence.\nWordsatthebeginningandendtendtostandaloneinlarge\nsequences of gaps, since their matching is rather coinci-\ndental. To exploit this ﬁnding, we analyze the coherence\nof the produced sequence. For every word in the output\nall rows agreeing with the output on this word are con-\nsulted. The number of agreements of the ﬁve preceding\nwords and of the ﬁve subsequent words with the output\nis summed up. If this sum is below 35% of the number\nof totally examined words, the word is removed from the\noutput. Althoughthisprocedureremovesunrelatedwords\nin most cases, it is also at risk to remove words at the\nend or the beginning of coherent sequences, especially if\nagreement among the different pages is low.\n5 EVALUATION\nEvaluation of this approach is a non-trivial task, since\nlyrics can have countless different appearances, which\nare nonetheless all correct (see Section 3). In fact, the\nonly way to decide if the generated result is correct, is to\nask humans (in the optimal case the authors themselves).\nSincethiswouldbeinfeasible,wehavetobecontentwith\na simpler measurement. Therefore, we decided to sim-\n567…it‘sshowtime -fordryclimesandbedlamisdreaming ofrainwhenthehills…\n…itsshowtimefordryclimesandbedlamisdreaming ofrainwhenthehills…\n…it‘sshowtime -fordryclimesandbedlamisdreaming ofrainwhenthehills…\n…it‘sshowtime -fordraglinesandbedlamisdreamin’ ofrainwhenthehills…\n…it‘sshowtime -fordryclimesandbedlamisdreaming ofrainwhenthehills…\nFigure1: Sectionfromalignmentofsong“LosAngelesisburn ing”by“BadReligion”. Thefourrowsonthetopareword\nsequences extracted fromthe web, the row at the bottom is the result obtained with any threshold value tbelow 0.75.\n0.7 0.75 0.8 0.85 0.9 0.95 10.650.70.750.80.850.90.951\nPrecisionRecall\nkwit (smoothed)\nkwit\ntfcorr (smoothed)\ntfcorr\nFigure2: Precision-Recall-Plotforbothpageselectionap -\nproaches(smoothedandunsmoothed). Theleftmostpoint\nof each curve is obtained with threshold t=0.0, the right-\nmost with t=1.0.\nply compare our results to an “ofﬁcial” reference – lyrics\nprinted in the booklets of CDs. The drawback of this\nmethod is that we have to expect the lyrics to be exactly\nlike in the booklets, which also includes annotations of\nsections, references to sections and abbreviations. Also\ntypos found in the booklet lyrics are transferred one-to-\none to our reference set. Thus, experimental results have\nto be interpreted with these constraints in mind. Since\ncompilationofsuchatestsetinvolvestheexhaustingtasks\nof manual veriﬁcation and typing, we use a small set lim-\nited to 258 songs from various genres3.\nTo compare the multiple lyrics alignment result (mla\nresult) and the reference lyrics, we simply reused the\nNeedleman-Wunschalgorithmforoptimalpairwisealign-\nment. In contrast to the lyrics alignment, this time we\nwere not interested in alignment of mismatching words.\nTherefore, instead of adapting the scoring scheme, we\ntreat mismatching words as insertion of a gap in both se-\nquences. Havingtheoptimalalignmentbetweenretrieved\nandreferencelyricswecanderivethecharacteristicinfor -\nmation retrieval metrics precision andrecall. Since preci-\n3the listof used lyricsis available online at\nhttp://www.cp.jku.at/people/knees/publications/258lyrics.htmlTable 1: Absolute numbers of lyrics (from total 258) in\nspeciﬁed recall intervals for values of t. Results were\nachieved using the unsmoothed kwit method.\nthreshold t\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nRec=1.00 6161616161615342332214\n1.00>Rec≥0.98 9595959595908269543429\n0.98>Rec≥0.95 4646464646495254654328\n0.95>Rec≥0.90 2020202020213042365150\n0.90>Rec≥0.80 3030303030302632405163\n0.80>Rec≥0.65 5555551213173034\n0.65>Rec>0.00 00000125122639\nRec=0.00 11111111111\nsionisameasureofhowwelltheapproachperformsinnot\naddingnonrelevantwordstotheresult,inourcase,thisis\nindicatedbygapsinthealignedreferencelyricssequence.\nRecall is a measure of how well the approach performs in\nincludingtherelevantwords. Thiscanbederivedfromthe\nnumberofgapsinthealignedmlaresultsequence. Hence,\nwe deﬁne them as follows, using lento denote the length\nof the alignment.\nPrec = 1−|gaps ref|\nlen(5)\nRec= 1−|gaps mla|\nlen(6)\nTo uncover the impact of the threshold ton precision\nandrecall,weconductedsystematicexperiments,varying\ntfrom0.0(nolowerboundformostfrequentwordincol-\numn to be chosen) to 1.0 (accept exclusively words that\nappearinallofthealignedsequences)instepsof0.1. For\neach of the page selection approaches (kwit and tfcorr)\nand both unsmoothed and smoothed outputs, we calcu-\nlated the average precision and recall over all 258 lyrics\nfor all values of t. Results are visualized in a Precision-\nRecall-Plot in Figure 2.\nObviously, thekwit page selectionapproach performs\nsigniﬁcantly better than tfcorr. For values of tranging\nfrom 0.0 to 0.5 the average recall reaches about 0.96, for\n0.6 it is still around 0.95 using kwit. For higher values of\ntrecallfurtherdecreases. Sinceretrievingcompletelyric s\nis much more important than presenting no unnecessary\nwords, in this task, recall is the crucial measure. Trying\nto keep a high recall value, while getting as much preci-\nsion as possible, a value of around 0.5 or 0.6 for tseems\nreasonable. Furthermore,wecanstatethat,atleastforthe\n568kwit approach with values of tbelow 0.8, smoothing in-\ncreasesprecisionwithoutnoticeablelossinrecall. Infac t,\nthecurvewiththesmoothedkwitresultisbasicallyaright\nshifted version of the unsmoothed kwit curve.\nTo give further evidence for the high performance\nof the approach, Table 5 shows the absolute numbers of\nlyricsinspeciﬁedrecallintervals. Forvaluesof tfrom0.0\nto 0.5 the number of lyrics with a recall higher or equal\nthan 0.95 is above or equal to 200 (from a total of 258),\nfort=0.6 it is still 187. For all values of t, there is one\npiece(DavidHasselhoff-Torero-TeQuiero )withrecallof\n0, since no useful pages have been found for it.\n6 CONCLUSIONS\nWe have presented an approach to automatically extract\nlyrics for arbitrary songs from web pages. Evaluation\nof the extracted lyrics on a set of 258 “ofﬁcial” lyrics\ntakenfromCDbookletsyieldsamedianrecallabove98%.\nUsing a hierarchical Multiple Sequence Alignment tech-\nnique, we can ﬁnd large coherent sequences in the pages\nwith reasonable effort. Based on the alignment, the most\nprobable sequence is determined by voting on each word.\nIn contrast to existent applications, our method does not\nrequire explicit knowledge about the structure of the sites\nand is thus able to incorporate a broader variety of pages.\nHowever, this approach also entails some disadvan-\ntages. As the results for the tfcorr page selection method\nshow, the approach would strongly beneﬁt from higher\nrobustness against unrelated or hardly alignable pages.\nHardly alignable means pages that actually contain the\nlyrics, but also other information, i.e. guitar tabs. This\nraisesaproblem,sincethelinesalternatelycontainchord s\nand lyrics. Improvements could be achieved by applying\nmethods from the ﬁeld of co-derivative document identi-\nﬁcation, e.g. Bernstein and Zobel (2004).\nAlso the fact that important words are possibly omit-\nted, poses a problem. Since the approach is intended\nto provide users with convenient access, returning com-\nplete results is mandatory to gain high acceptance among\nusers. This can not be achieved with a simple local vot-\ning strategy as the one proposed here, making thus more\nsophisticatedwordselectionapproachesnecessarytopro-\nduce output with higher consistency. Furthermore, some\nof the omissions made are caused by improperly aligned\nsequences, suggesting that improvements could also be\nachieved by involving more complex MSA approaches.\nTaking another look at the ﬁeld of Bioinformatics reveals\nthat strategies have been evolved incrementally in the last\nyears, leading to more powerful, but also more elaborate\nalignmenttechniques. Proposalsforfurtherenhancements\ncan be found e.g. in Thompson et al. (1994), which cov-\ners interesting approaches like sequence weighting and\nposition-speciﬁc gap penalties.\nFor future improvements also better multilingual sup-\nport is desirable. In principle, the proposed technique is\none-to-one applicable for lyrics in languages other than\nEnglish,ashasbeenprovedforthreelyricsbytheGerman\nHip Hop group Absolute Beginner , since it is completely\nfree of language speciﬁc constraints. However, it could\nbe valuable to perform searches using Google with key-words other than lyrics(e.g.letrasorsongtext) to yield\nmore localized results. The proper choice for the addi-\ntionalkeywordcouldthenbeaccomplishedbydetermina-\ntion of frequent languages in the results.\nTo give consideration to the user, also the appearance\nof the output must be adapted. At the moment, output\nconsistsofasequenceofwordswithoutanylinebreaking\norpunctuation. Toremedy this,analignment oftheresult\nwiththehighestmatchingsequencecouldbeperformedto\nﬁnd probable positions for insertions. Thus, the underly-\ning technique could be further obscured from the user by\npresenting results looking like manually produced lyrics.\nACKNOWLEDGEMENTS\nThis research is supported by the Austrian Fonds zur\nF¨orderung der Wissenschaftlichen Forschung (FWF) un-\nder project numbers L112-N04 and Y99-START, and by\nthe EU 6th FP project SIMAC (project number 507142).\nThe Austrian Research Institute for Artiﬁcial Intelligenc e\nis supported by the Austrian Federal Ministry for Edu-\ncation, Science, and Culture and by the Austrian Federal\nMinistryfor Transport, Innovation, and Technology.\nREFERENCES\nS. Baumann and A. Kl ¨uter. Super-Convenience for Non-\nMusicians: Querying mp3 and the Semantic Web. In\nProc. of the 3rd International Conference on Music In-\nformation Retrieval (ISMIR) ,Paris, France, 2002.\nY. Bernstein and J. Zobel. A Scalable System for Identi-\nfying Co-Derivative Documents. In A. Apostolico and\nM.Melucci,editors, Proc.oftheStringProcessingand\nInformation Retrieval Symposium (SPIRE) , pages 55–\n67, Padova, Italy, 2004. Springer LNCS 3246.\nF. Corpet. Multiple Sequence Alignment with Hierarchi-\ncalClustering. NucleicAcidsResearch ,16(22):10881–\n10890, 1988.\nB.Logan,A.Kositsky,andP.Moreno. SemanticAnalysis\nof Song Lyrics. Technical Report HPL-2004-66, HP\nLaboratories Cambridge, 2004.\nS. B. Needleman and C. D. Wunsch. A General Method\nApplicable to the Search for Similarities in the Amino\nAcid Sequence of Two Proteins Journal of Molecular\nBiology, 48(3):443–453, 1970.\nD. R. Radev, A. Winkel, and M. Topper. Multi Doc-\nument Centroid-based Text Summarization. In ACL\n2002, Philadelphia, PA, July 2002.\nS.ScottandS.Matwin.TextClassiﬁcationusingWordNet\nHypernyms. InS.Harabagiu,editor, UseofWordNetin\nNatural Language Processing Systems: Proceedings of\nthe Conference , pages 38–44. Association for Compu-\ntational Linguistics, Somerset, New Jersey, 1998.\nJ. D. Thompson, D. G. Higgins, and T. J. Gibson.\nCLUSTAL W: Improving the Sensitivity of Progres-\nsive Multiple Sequence Alignment through Sequence\nWeighting, Position-SpeciﬁcGapPenaltiesandWeight\nMatrix Choice. Nucleic Acids Research , 22(22):4673–\n4680, 1994.\n569"
    },
    {
        "title": "Geospatial Location of Music and Sound Files for Music Information Retrieval.",
        "author": [
            "Ian Knopke"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417765",
        "url": "https://doi.org/10.5281/zenodo.1417765",
        "ee": "https://zenodo.org/records/1417765/files/Knopke05.pdf",
        "abstract": "A relatively new avenue of Web-based information retrieval research, intended to semantically improve information extraction, is the idea of using geographical information to accurately locate resources. This paper introduces a technique for locating sound and music files geographically. It uses information extracted from the Web relating to audio resources and combines it with geospatial location data to provide new information about audio usage in various countries. The results presented here illustrate the enormous potential for MIR to use the vast amount of audio materials on the Web within a physical and geographical context. Statistics of audio usage around the world are provided, as well as examples of other applications of these techniques. Keywords: ISMIR, AROOOGA, geospatial, mapping, World Wide Web, Web crawler, GIS, semantic, CIA, McGill, music 1",
        "zenodo_id": 1417765,
        "dblp_key": "conf/ismir/Knopke05",
        "keywords": [
            "geographical",
            "information retrieval",
            "semantic",
            "location",
            "resources",
            "Web-based",
            "research",
            "audio",
            "files",
            "technique"
        ],
        "content": "Geospatial Location ofMusic andSound Files forMusic Information Retrie val\nIanKnopk e\nMcGill Music Technology\nMontr ´eal,Qu´ebec, Canada\nian.knopke@mail.mcgill.ca\nABSTRA CT\nArelati velynewavenue ofWeb-based information re-\ntrievalresearch, intended tosemantically impro veinfor -\nmation extraction, istheidea ofusing geographical infor -\nmation toaccurately locate resources. This paper intro-\nduces atechnique forlocating sound andmusic \u0002les ge-\nographically .Ituses information extracted from theWeb\nrelating toaudio resources andcombines itwith geospa-\ntiallocation data toprovide newinformation about audio\nusage invarious countries. Theresults presented here il-\nlustrate theenormous potential forMIR tousethevast\namount ofaudio materials ontheWebwithin aphysical\nandgeographical conte xt.Statistics ofaudio usage around\ntheworld areprovided, aswell asexamples ofother ap-\nplications ofthese techniques.\nKeywords: ISMIR, AROOOGA, geospatial, mapping,\nWorld WideWeb,Webcrawler,GIS, semantic, CIA,\nMcGill, music\n1INTR ODUCTION\nThe World WideWebisthelargest data repository on\nEarth, constituting billions ofpages oftextual informa-\ntion, aswell asproviding access tomanyother types of\nresources, including multimedia. Manyofthecurrently\navailable search techniques areinsuf \u0002cient foranswer -\ningspeci\u0002c queries about these resources. Searches for\ntextual information may workwell under manycircum-\nstances, butcurrent techniques often failtotakeinto ac-\ncount thespeci\u0002c nature ofother media, ordonotmeet\ntheneeds ofmore discerning users.\nArelati velynewavenue ofWeb-based information re-\ntrievalresearch, intended tosemantically impro veinfor -\nmation extraction, istheidea ofusing geographical infor -\nmation toaccurately locate resources. While theWebis\nusually considered tobeavirtual space, without dis-\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondontance ordimension, itisrapidly becoming possible tode-\ntermine theexact physical location ofInternet resources\nintherealworld. This hasmanypotential possibilities for\ninformation retrie val.Forinstance, search engines could\nbeimpro vedbyranking local user resources forcommon\nqueries (stores, movietheaters, etc.) ahead ofmore distant\nlocations. Geospatial data mining techniques could also\nproveuseful tomark etanalysts intargeting newproducts\nforpotential mark ets(Buyuk okkten etal.,1999). Other\nuses, such asspatial Webbrowsing, verifying user loca-\ntions, orevenpreventing credit card fraud havebeen pro-\nposed (MaxMind, 2005).\nThis paper introduces atechnique forlocating sound\nandmusic \u0002les geographically .Ituses information ex-\ntracted from theWebrelating toaudio resources andcom-\nbines itwith geospatial location data toprovide statistics\nabout audio usage invarious countries around theworld.\nTheresults presented here illustrate theenormous poten-\ntialforMIR tomakeuseofthevastamount ofaudio ma-\nterials ontheWebwithin aphysical andgeographical con-\ntext.\nTheavailability ofsuch information canbeexploited\ninanumber ofways:\nGeospatial Music WebBrowsing Thepossibility ofap-\nplying this information tomapping systems ises-\npecially promising, andcould form thebasis fora\nsound \u0002lebrowsing system, especially ifused incon-\njunction with other MIR concerns such asgenre in-\nformation ormelodic detection techniques, andcom-\nbined with aWeb-based dynamic mapping system\nsuch asthatrecently introduced byGoogle (2005).\nMusic Mark eting Accurate information about thenature\nofsound andmusic inaparticular geographical lo-\ncation could form animportant resource forthecom-\nmercial music sector aswell, forplanning music mar-\nketing strate gies, orevenintheselection oftouring\nlocations formusical ensembles.\nBand width Optimization User downloads ofmusic re-\nquire considerable Internet bandwidth resources. Ge-\nographical information could beused tooptimize\nbandwidth anddownload times bypre-selecting sites\nthataregeographically closer tousers, especially in\nthecase ofsimilar orduplicated content. This could\nhaveimportant rami\u0002cations forWeb-based digital\n29library systems orInternet sound repositories thatex-\nchange audio \u0002les.\nMusicological Resear chObtaining information about\ncurrent music listening trends canbeextremely dif-\n\u0002cult, andisoften limited tostatistics reported by\nindustry sources. Forsome locations around the\nworld, such information may notbeavailable atall.\nThe technique presented here provides analternate\nmethod ofindependently gathering andverifying this\ntype ofinformation.\n2GEOSP ATIAL MAPPING\nBuilding ageospatial database ofdomains and IPad-\ndresses isaccomplished through theuseofmultiple data\nsources thatmay inthemselv esbeincomplete, butcanbe\ncombined toform amore consistent whole. Sources can\nbecategorized intotwomain types: available information\nabout host machines, orfrom Webpage content stored on\nspeci\u0002c servers.\nHost machine information, when present, canbeob-\ntained directly from theWhois database fordomain name\nregistrations. Routing information canalso beused, such\nasmonitoring thelasthopprovided bythetraceroute\nutility .Some information isavailable from Domain Name\nSystem (DNS) servers,although thisconsidered tobeless\naccurate geographically .\nOther information canbegathered byexamining Web\npage content onspeci\u0002c servers.Information such asad-\ndresses, postal codes, place names and eventelephone\nnumbers canprovide valuable clues astothereal loca-\ntionofahost machine. McCurle y(2001) provides agood\novervie wofgeospatial database-creation techniques.\nMuch geospatial Webresearch isundertak eninthe\nconte xtofsemantic Websearching (Hiramatsu andRe-\nitsma, 2004; Jones etal.,2002). Egenhofer (2002) notes\nthat, forthecurrent Webtobecome atrueinformation re-\nsource, itrequires better semantic search techniques and\nproposed anewresearch agenda forthecreation ofaSe-\nmantic Geospatial Webthat coordinates existing search\nengine query information with geospatial data. (Tomko,\n2002) hasprovided anassessment anduser study ofthe\nsuitability oftheWebforproviding information forcom-\nmon navigational tasks.\n3THE AROOOGA MIR SYSTEM\nAROOOGA (Articulated Resource for Obsequious\nOpinionated Observ ations intoGathered Audio) isaMu-\nsicInformation Retrie valsystem designed tolocate and\nanalyze audio resources ontheWorldWideWeb(Knopk e,\n2004a, 2005). Unlik eother Internet-based sound distri-\nbution systems such astheKazaa P2P program orAp-\nple'siTunes, AROOOGA only gathers information about\nsound andmusic \u0002les that areavailable onpublic Web\npages. These pages must beeasily accessible; pages\nbehind sign-up forms, passw ordprotection, orpay-to-\ndownload schemes arenotgenerally retrie vable byauto-\nmatic means. Asthese \u0002les aremade freely available to\nanyone with anInternet connection, these resources aregenerally notencumbered bythekind ofcommercial re-\nstrictions thatmay arise with other systems.\nThe central component ofAROOOGA isahigh-\ncapacity ,scalable, distrib uted Webcrawling system. The\nsecret toAROOOGA 'sspeed isasetofretrievermodules\ndistrib uted across manycomputers, making itpossible to\nretrie vemultiple resources inparallel. This iscoordinated\nbyacentral managing component knownasthecrawl\nmana gerthat controls thelistofURLs toberetrie ved,\naswell ashandling alldata storage. Inter-machine com-\nmunication ishandled byacustomized message-passing\nprotocol overEthernet connections.\nAROOOGA begins acrawlbyworking from aseed\nlist,usually oflessthan ahundred URLs. After retrie val,\neach Webpage encountered ismined fortwotypes ofin-\nformation: links toother Webpages oraudio \u0002les, and\nWebpage textrelating tolinkedsound \u0002les. Links toother\npages arethen added toaqueue forlater retrie vals,allow-\ningthecrawling process tocontinue. Link edsound \u0002les\narealsoretrie vedandanalyzed. Information extracted for\neach audio \u0002lefallsinto oneofthree categories: exter-\nnalmetadata gathered from thelinking Webpage, inter-\nnalmetadata stored within thesound \u0002lesuch assampling\nrateorkeywords, andinformation gathered from analy-\nsisoftheactual encoded audio. Ineffect, theexternal\nmetadata acts asakind ofannotation system forthere-\ntrievedsound \u0002les (Knopk e,2004b), andtheassociation\nofthethree types ofinformation hasbeen showntopro-\nvide better indexing than canbeachie vedfrom mining\nWebpages oraudio \u0002les alone (Knopk e,2005). Forau-\ndioextraction, AROOOGA currently uses theMARSY AS\n(Tzanetakis andCook, 2000) system, primarily formu-\nsic/speech determination andtoobtain thegenre ofmusic\n\u0002les (Tzanetakis etal.,2000).\nUnlik eother general-purpose Webcrawling orsearch\nengine systems such asGoogle orYahoo, information ex-\ntraction isdone atthetime ofanalysis andtheresults are\nsenttothecrawlmanager forpermanent storage, andlater\nuseinresolving queries. Only theresults ofthethree\ntypes ofanalysis arestored, andallother textual informa-\ntion onretrie vedWebpages isconsidered irrele vantand\nignored. This allowsthesystem tofocus solely onmu-\nsicandsound resources inorder toanswer speci\u0002c audio-\nrelated queries, forming thecore ofasearch engine. The\nsystem isprogrammed inPerl, with audio analysis com-\nponents inCandC++ forgreater ef\u0002cienc y.\nAudio data ontheWebisquite sparse ascompared to\nthenumber ofWebpages. Themajority ofsound \u0002les on\ntheWebtend tobeMP3 types, with smaller numbers of\nWAVEandAIFF \u0002les occurring regularly .Using statis-\ntical data takenfrom severalsample crawls, itispossible\ntoextrapolate some information about thenumber andna-\nture ofsound resources ontheWeb.While itisdif\u0002cult\ntoestimate theexact size oftheWeb,byusing aplausi-\nblesizeof9billion pages wecanarriveatthefollowing\nvalues, showninTable 1.\nItisclear thatthere isanexceptional amount ofsound\nandmusic materials available ontheWeb,although much\nofthisisdif\u0002cult tosearch forwith anyaccurac y.\n30Table 1:Predicted Values foraWebof9Billion Pages\nType %ofTotal Number\nWebpages 100.00 9,000,000,000\nPages with audio links 0.26 23,400,000\nSound \u0002les 2.39 215,100,000\nWAVE 31.99 68,810,490\nAIFF 1.63 3,506,000\nMPEG-1 66.33 142,675,830\nOTHER 0.06 129,060\n4METHODOLOGY\nAsample crawlwasundertak enofapproximately 600,000\nWebpages. The seed listwaschosen tore\u0003ect typical\npages auser might encounter within amore narro wrange\nofaudio-related interests, andcanbeseen asaform of\nseed pre-selection orfocused crawling (Chakrabarti etal.,\n1999; Ester etal.,2001). Starting URLs ofWebsites were\nused that speci\u0002cally contained multiple links tosound\n\u0002les, manyofthem.com domains ofmusic companies\norportals. Duplicate sound \u0002les were remo vedduring the\ncrawling process. Some examples ofsites used areshown\ninTable 2.\nTable 2:Partial URL seed list\nwww .loopmasters.com\nwww .cobwebaudio.co.uk\nwww .pro-music.or g\nmusic.do wnload.com\nwww .soundcentral.com\nwww .analoguesamples.com\nwww .multi-edit.com\nwww .musicstuf f.de\nfree-music.com/fma2000\nwww .propellerheads.se\nThe country oforigin where thesound \u0002leresides\nwasdetermined using ageospatial database from Max-\nMind corporation, claimed tohavea97% accurac yratefor\ncountry determination (MaxMind, 2005). These were also\ncorrelated against additional information takenfrom the\nCIA WorldFactbook (CIA, 2004). Map plotting wasdone\nusing theGeneric Mapping Toolkit (Wessel andSmith,\n2005). Allresults were calculated using custom Perlcode,\nandgeospatial database access wasprovided through the\nGeo::IP PerlModule (Mather, 2005).\n5RESUL TS\nGeneral statistics forthecrawlareshowninTable 3.At-\ntempted downloads refers tothetotal number ofrequests\nforWebpages. Some pages inanycrawlwill beunre-\ntrievable, usually duetobrokenlinks orsecurity features,\nandarelisted asunsuccessful downloads. Bydividing the\nnumber ofretrie vedsound \u0002les bythenumber ofsuccess-\nfulpage downloads, anaverage ofthenumber ofsound\n\u0002leresources perpage canbecalculated (0.07).\nThe actual country oforigin foreach audio \u0002lewas\nderivedbyquerying each URL against thegeospatialTable 3:General crawlstatistics\nPageLinks collected 85,764,415\nAttempted downloads 612,311\nSuccessful downloads 532,849\nUnsuccessful downloads 79,462\nSuccess/Unsucc. % 87.02%\nUnique sound \u0002les 37,353\nSound /Page 0.07\ndatabase. Table 4givethetopgeospatial statistics listed\nbycountry .Approximately three quarters ofallthesound\n\u0002les retrie ved(71.86%) arelocated intheUnited States,\nregardless ofdomain suf\u0002x. Italy (10.35%), German y\n(4.11%) andGreat Britain (3.72%) were also found to\ncontain signi\u0002cant sound \u0002lerepositories.\nTable 4:Topstatistics bycountry\nCountry Count %\nUSA\n 26842 71.86%\nItaly\n 3866 10.35%\nGerman y\n 1537 4.11%\nGreat Britain\n 1390 3.72%\nJapan\n 378 1.01%\nCanada\n 335 0.90%\nSouth Korea\n 253 0.68%\nFrance\n 241 0.65%\nSpain\n 199 0.53%\nSweden\n 159 0.43%\nBelgium\n 100 0.27%\nAustralia\n 94 0.25%\nAustria\n 92 0.25%\nNetherlands\n 85 0.23%\nDenmark\n 68 0.18%\nSwitzerland\n 52 0.14%\nCzech Republic\n 48 0.13%\nRussia\n 35 0.09%\nSouth Africa\n 34 0.09%\nFinland\n 31 0.08%\nPoland\n 26 0.07%\nNorw ay\n 19 0.05%\nOther 37 0.10%\nUnkno wn 1432 3.83%\nTotal 37353 100%\nThis information canalso begrouped together into\nlargergeographical regions. Table 5givesthese results\nforeach continent, correlated bythethree main \u0002letypes\nencountered: WAVE, AIFF ,andMP3. Byfarthemost\ncommon type ofsound \u0002les areMP3 \u0002les, at81.58%,\nwith theexception oftheAsian continent, where WAVE\n\u0002les \u0002gure more predominantly .This may indicate apos-\nsible preference foroperating system andhardw areinthat\nregion.\nAccess togeospatial information naturally suggests\nthepossibility ofdisplaying such statistics using ageo-\ngraphical mapping system. Figure 1showsanexample of\n31Table 5:Sound \u0002letypes sorted bycontinent\nContinent WAVE% AIFF % MP3 %\nAfrica 0.00 0.00 100.00\nAsia 89.22 0.16 10.62\nAustralia 1.06 1.06 97.87\nEurope 4.83 0.92 94.26\nNAmerica 17.03 3.50 79.45\nSAmerica 0.00 0.00 100.00\nAll 15.55 2.86 81.58\nsuch adistrib ution fortheEuropean region, anddemon-\nstrates howsuch information might beused incombina-\ntionwith aWeb-based graphical interf acefor\u0002leretrie val.\nFigure 1:European sound \u0002ledistrib ution bypercentage\nITA 49.16DEU 19.54 GBR 17.68\nFRA 3.06\nESP 2.53SWE 2.02\nBEL 1.27\nAUT 1.17NLD 1.08DNK 0.86\nCHE 0.66CZE 0.61POL 0.33 IRL 0.01\nKnowing thegeographic location ofa\u0002lemakesit\npossible tocorrelate thisinformation with other statistics,\nsuch asthesizeofthepopulation orthearea ofthecoun-\ntry,andcould beused asakind ofdata normalization.\nForinstance, acountry with asmaller population would\nbeexpected tohavefewersound andInternet resources\navailable. Table 6and7givessuch \u0002gures forthepopula-\ntionandarea respecti vely.\n6FUTURE WORK\nThese results areconsidered some what preliminary ,and\nneed tobeapplied tolargercrawlsoftheWeb.Currently\nthisislimited bytheavailable search hardw are.Also, the\ngeospatial database used forthisstudy canonly identify\nthecountry within which thehost serverresides. More\naccurate databases, with resolution tothelevelofcities or\nevenstreet address, areavailable onasubscription basis.\nTheuseofimpro vedresources would allowforadditional\npossibilities, such asaccurate geographical distance mea-\nsurements between serverlocations orcould beused in\nconjunction with other semantic search orsocial netw ork-\ningtechniques (Hiramatsu andReitsma, 2004).Table 6:Density ofsound \u0002les perperson\nCountry Files Pop.(106)Density( 10\u00006)\nUSA 26842 278.06 96.53\nItaly 3866 57.68 67.03\nGreat Britain 1390 59.65 23.30\nGerman y 1537 83.03 18.51\nSweden 159 8.88 17.92\nDenmark 68 5.35 12.70\nAustria 92 8.15 11.29\nCanada 335 31.59 10.60\nBelgium 100 10.26 9.75\nSwitzerland 52 7.28 7.14\nTable 7:Density ofsound \u0002les persquare Km\nCountry Files Area(106)Density( 10\u00006)\nItaly 3866 0.30 12834.05\nGreat Britain 1390 0.24 5677.64\nGerman y 1537 0.36 4305.07\nBelgium 100 0.03 3277.61\nUSA 26842 9.63 2787.59\nSouth Korea 253 0.10 2569.05\nNetherlands 85 0.04 2046.61\nDenmark 68 0.04 1577.95\nJapan 378 0.38 1000.44\nFrance 241 0.55 440.56\nThis information isalso intended toform thebasis\nforaspatially-a waremusic browsing system oftheWeb,\nmaking itpossible topoint-and-click onspeci\u0002c geo-\ngraphical maps andobtain alistofresources bycountry\norcity.Even\u0002ner resolutions should bepossible.\n7CONCLUSIONS\nThe useofgeospatial information incombination with\nsound and music information ontheWorld WideWeb\nshowsmuch promise, andhasthepotential toproveuseful\nformanydifferent information retrie valsituations. Witha\nsuf\u0002ciently largepool ofdata, theability togeographically\nlocate audio resources canbeused toderiveinformation\nthatistypically dif\u0002cult toobtain byother means. This\ndata canalso beused inconjunction with other statistical\ninformation toprovide newmeasurements ofsound and\nmusic andcould form animportant newresource forse-\nmantic Websearch techniques andMIR research ingen-\neral.\n8REFERENCES\nRefer ences\nO.Buyuk okkten, Ji.Cho, andN.Shivakumar H.Garcia-\nMolina, L.Gravano. Exploiting geographical location\ninformation ofweb pages. InProceedings ofWorkshop\nonWebDatabases ,pages 916. ACMPress, 1999.\nS.Chakrabarti, M.Berg,andB.Dom. Focused crawling:\n32Anewapproach totopic-speci\u0002c web resource disco v-\nery.Computer Networks ,31(116):162340, 1999.\nCIA. Worldfactbook, 2004.http://www.cia.gov/\ncia/download.html .\nM.Egenhofer .Towardthesemantic geospatial web.In\nACM-GIS .GIS, 2002.\nM.Ester ,M.Gross, and H.P.Kriegel. Focused\nweb crawling: Ageneric frame workforspecifying\ntheuser interest andforadapti vecrawling strate gies.\nInTwenty-Se venth International Confer ence onVery\nLargeDatabases ,2001.\nGoogle. Maps, 2005.http://maps.google.com/ .\nK.Hiramatsu and F.Reitsma. Georeferencing the\nsemantic web: ontology based markup ofgeo-\ngraphically referenced information. InJoint Eu-\nroSDR/Eur oGeo graphics workshop onOntolo gies and\nSchema Translation Services ,2004.\nC.Jones, R.Purves,A.Ruas, M.Sanderson, M.Sester ,\nM.vanKreveld,andR.Weibel. Spatial information re-\ntrievalandgeographical ontologies: Anovervie wofthe\nspirit project. InProceedings ofthe25th Annual Inter -\nnational ACMSIGIR Confer ence onResear chandDe-\nvelopment inInformation Retrie val,pages 3878, 2002.\nI.Knopk e.AROOOGA: Anaudio search engine forthe\nWorld WideWeb.InProceedings oftheInternational\nComputer Music Confer ence,pages 2903, November\n2004a.\nI.Knopk e.Sound, music and textual associations for\ntheWorld WideWeb. InProceedings oftheInter -\nnational Symposium onMusic Information Retrie val,\npages 4848, October 2004b.\nI.Knopk e.Building aSearchEngine forMusic andAudio\nontheWorld WideWeb.PhD thesis, McGill University ,\n2005.\nT.Mather . Geo::ip perl module, 2005.\nhttp://search.cpan.org/tjmather/\nGeo-IP-1.25/ .\nMaxMind. Geoip free country database, 2005.\nhttp://www.maxmind.com/download/\ngeoip/database/ .\nK.McCurle y.Geospatial mapping andnavigation ofthe\nweb.InWWW '01: Proceedings ofthetenth inter-\nnational confer ence onWorld WideWeb,pages 2219.\nACMPress, 2001.\nM.Tomko.Case study -assessing spatial distrib ution of\nweb resources fornavigation services. In4thInterna-\ntional Workshop onWebandWireless Geographical In-\nformation Systems ,2002.\nG.Tzanetakis andP.Cook. MARSY AS:Aframe workfor\naudio analysis. Organized Sound ,4(3):16975, 2000.\nG.Tzanetakis, G.Essl, andP.Cook. Automatic musical\ngenre classi\u0002cation ofaudio signals. InInternational\nSymposium onMusic Information Retrie val.n.p,2000.\nP.Wessel andW.Smith. Generic mapping tools, 2005.\nhttp://gmt.soest.hawaii.edu/ .\n33"
    },
    {
        "title": "Syncplayer An Advanced System for Multimodal Music Access.",
        "author": [
            "Frank Kurth",
            "Meinard Müller",
            "David Damm",
            "Christian Fremerey",
            "Andreas Ribbrock",
            "Michael Clausen"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416496",
        "url": "https://doi.org/10.5281/zenodo.1416496",
        "ee": "https://zenodo.org/records/1416496/files/KurthMDFRC05.pdf",
        "abstract": "In this paper, we present the SyncPlayer system for multimodal presentation of high quality audio and associated music-related data. Using the SyncPlayer client interface, a user may play back an audio recording that is locally available on his computer. The recording is then identified by the SyncPlayer server, a process which is performed entirely content-based. Subsequently, the server delivers music-related data like scores or lyrics to the client, which are then displayed synchronously with audio playback using a multimodal visualization plug-in. In addition to visualization, the system provides functionality for contentbased music retrieval and semi-manual content annotation. To the best of our knowledge, our system is moreover the first to systematically exploit automatically generated synchronization data for content-based symbolic browsing in high quality audio recordings. SyncPlayer has already proved to be a valuable tool for evaluating algorithms in MIR research on a larger scale. In this paper, we describe the technical background of the SyncPlayer framework in detail. We also give an overview of the underlying MIR techniques of audio matching, music synchronization, and text-based retrieval that are incorporated in the current version of the system. Keywords: MIR systems and infrastructure, multimodal interfaces and music access, synchronization 1",
        "zenodo_id": 1416496,
        "dblp_key": "conf/ismir/KurthMDFRC05",
        "keywords": [
            "Multimodal presentation",
            "Audio recording playback",
            "Content-based identification",
            "Music-related data delivery",
            "Synchronization with audio",
            "Visualizing music data",
            "Content-based music retrieval",
            "Semi-manual content annotation",
            "Automatic synchronization data",
            "Evaluation of MIR algorithms"
        ],
        "content": "SYNCPLAYER — AN ADV ANCED SYSTEM FOR MULTIMODAL MUSIC\nACCESS\nFrank Kurth, Meinard M ¨uller, David Damm, Christian Fremerey, Andreas Ribbrock, Mi chael Clausen\nUniversit ¨at Bonn\nInstitut f ¨ur Informatik III\nR¨omerstr. 164, D-53117 Bonn, Germany\n{frank,meinard,damm,fremerey,ribbrock,clausen }@cs.uni-bonn.de\nABSTRACT\nIn this paper, we present the SyncPlayer system for mul-\ntimodal presentation of high quality audio and associated\nmusic-related data. Using the SyncPlayer client interface ,\na user may play back an audio recording that is locally\navailable on his computer. The recording is then identiﬁed\nby the SyncPlayer server, a process which is performed\nentirely content-based. Subsequently, the server deliver s\nmusic-related data like scores or lyrics to the client, whic h\nare then displayed synchronously with audio playback us-\ning a multimodal visualization plug-in. In addition to vi-\nsualization, the system provides functionality for conten t-\nbased music retrieval and semi-manual content annota-\ntion. To the best of our knowledge, our system is moreover\nthe ﬁrst to systematically exploit automatically generate d\nsynchronization data for content-based symbolic brows-\ning in high quality audio recordings. SyncPlayer has al-\nready proved to be a valuable tool for evaluating algo-\nrithms in MIR research on a larger scale. In this paper,\nwe describe the technical background of the SyncPlayer\nframework in detail. We also give an overview of the un-\nderlying MIR techniques of audio matching, music syn-\nchronization, and text-based retrieval that are incorpora ted\nin the current version of the system.\nKeywords: MIR systems and infrastructure, multi-\nmodal interfaces and music access, synchronization\n1 INTRODUCTION\nClassical MIR techniques typically work with a sin-\ngle type of music representation. Examples include\nmost query-by-example scenarios like audio identiﬁcation\n(where a short query signal is to be identiﬁed w.r.t. a large\ndatabase of audio signals), melodic queries (where a note\nsequence is to be matched to a melody database), poly-\nphonic search (where an excerpt of a score is searched in\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londona score database), or text-based search (where a sequence\nof query terms is searched in a database of textual meta-\ndata). As a consequence of such approaches, all steps in\nthe typical MIR chain of query formulation, content-based\nretrieval, and display of query results are based on the\nsame type of data representation. However, it is evident\nthat there is no single data representation that is equally\nsuitable or even a natural choice to be used for all of the\nlatter tasks. As an example, consider a user querying a\nmelody database. While it may be the most natural op-\ntion to just hum a melody resulting in a waveform-based\nquery, most successful algorithms for melody-based re-\ntrieval work in the domain of symbolic music. On the\nother hand, retrieval results are most naturally presented\nby playing back an actual recording of the piece of music\ncontaining the melody, while a musical score or apiano-\nrollwill frequently be the most appropriate form of visu-\nally displaying a query result.\nUp to now, research on integrating different types of\nmusic representation within MIR scenarios has mainly fo-\ncused on the aspects of query formulation and the actual\nretrieval algorithms: considering query formulation, man y\napproaches like existing query-by-humming techniques\n(for example Pauws (2002)) use signal processing tech-\nniques to convert acoustic data to the symbolic domain in\na preprocessing step. Concerning retrieval, methods have\nbeen proposed to query music in a cross-domain fashion.\nFor example, Pickens et al. (2002) present a system for\nquerying polyphonic audio recordings in a database of\npolyphonic music given in the symbolic domain.\nIn contrast to this, there still is a signiﬁcant lack of\nMIR systems for simultaneously presenting (which in-\ncludes display, visualization, playback, and browsing)\nmusic and content-related data in an adequate multimodal\nform. Generally, such data is presented based on a sin-\ngle type of music data only. As an example, consider the\ndisplay of query results in content-based audio retrieval:\nresults are presented using acoustic playback along with\na visualized waveform (presentation based on waveform\ndata only). In MIDI-based retrieval, query results are typ-\nically presented as synthesized MIDI along with a MIDI-\nbased piano-roll.\nA particular lack of appropriate systems for content-\npresentation is found in several areas of MIR research .\nFor example, there are only few user-friendly interfaces\nfor analyzing results of the multitude of proposed algo-\n381rithms for automatic music annotation such as score-to-\naudio synchronization (see below).\nAs a ﬁrst step towards a proper multimodal presenta-\ntion of music and content-related data, we recently pro-\nposed a prototypic client-server based system for syn-\nchronized playback and display of acoustic recordings\ntogether with content-related metadata (see Kurth et al.\n(2004)). The system’s capabilities were demonstrated in a\nkaraoke-like scenario where the displayed data consists of\nlyrics information. In contrast to other systems such as a\nsimilar system by Philocode LLC1, which is available as a\nplug-in to various audio players (e.g., WinAmp or iTunes),\nSyncPlayer identiﬁes a selected musical recording and a\nplayback position therein entirely content-based. Hence i t\ndoes not rely on the availability of further metadata such\nas ID3 tags.\nIn the present paper, we describe how the SyncPlayer\nframework has been redesigned to constitute an inte-\ngrated MIR system incorporating functionality for mul-\ntimodal display of symbolic data, content-based querying\nand semi-manual content annotation. To achieve this, we\nsuitably adapt and combine recent techniques from audio\nidentiﬁcation (Clausen and Kurth (2004)), music synchro-\nnization (M ¨uller et al. (2004)), and text retrieval. To the\nbest of our knowledge, our system is the ﬁrst to system-\natically exploit automatically generated synchronization\ndata for content-based symbolic browsing in high quality\naudio recordings.\nThe main contributions of our system are summarized\nas follows:\n•We propose an integrated system for symbolic\n(including textual) music searching and browsing\nwith acoustic audio playback based on high quality\nrecordings of the selected pieces of music.\n•Our novel MultiVis (Multimodal Visualization) plug-\nin offers functionality for output as well as a piano-\nroll-like interface for visualization and MIDI-based\nbrowsing . This interface turns out to be a valuable\ntool for larger scale evaluation of research results in\nthe domain of music synchronization.\n•The newly developed query engine allows for textual\nsearch in a lyrics database, a ranked representation\nof query results, and playback of the exact positions\nof all matches. Our query engine is generic in that\nit is extensible to musical content-based search (for\nexample, melody-based queries).\n•An extended version of our Sync File Maker plug-in\nallows for semi-manual annotation of lyrics (or gen-\neral textual) metadata to audio recordings. This func-\ntionality is supported by means of signal processing\ntechniques for “slow-motion playback” of the under-\nlying audio recording. By offering additional func-\ntionality for feature extraction, the plug-in can be\nused to extend and maintain the underlying audio\ndatabase.\n•The SyncPlayer framework is based on a ﬂexible\n1http://www.philocode.com/minilyrics/\nFigure 1: Overview of the SyncPlayer system architecture\nclient-server architecture , which makes it suitable\nfor arbitrarily distributed environments.\nOur paper is organized as follows. In the next section\nwe describe the SyncPlayer system, consider its modes of\noperation, and give a detailed account on the constitut-\ning software modules and their interaction. In the sub-\nsequent sections we summarize the audio identiﬁcation\nand music synchronization techniques that are used in\nthe SyncPlayer framework. Audio identiﬁcation is con-\nsidered in Section 3, where we give a suitable model-\ning for the type of identiﬁcation problem relevant to the\nSyncPlayer scenario. Moreover, we sketch a novel fast\nindex-based search algorithm, which is capable of deal-\ning with very large data collections. Section 4 brieﬂy dis-\ncusses algorithms for music synchronization used in our\nsystem. We furthermore describe a new version of our an-\nnotation module for semi-manual text-to-PCM synchro-\nnization. In Section 5 we consider content-based music\nqueries. We propose a novel query module which, in the\ncurrent version of our system, allows for text-based query-\ning and browsing. In the concluding Section 6, we give an\noverview of our ongoing work and propose some prob-\nlems for future research.\n2 SYNCPLAYER SYSTEM\nIn this section we describe the SyncPlayer system in de-\ntail. After giving an overview of the basic operation mode,\nwe describe the architecture of the software system. We\ngive a detailed account on the client-server based frame-\nwork and the plug-in modules used for playback, visual-\nization, annotation, and querying. We also describe an\nadministration tool for creating and updating the audio in-\ndex, which is used for the audio identiﬁcation task.\n382Figure 2: SyncPlayer client interface (left) together with two instances of the MultiVis plug-in, one displaying lyric s\nmetadata, the other showing piano-roll data of an audio reco rding.\n2.1 Basic Mode of Operation\nUpon starting the SyncPlayer client application, the user\nselects an audio ﬁle from his local music collection, and\nacoustic playback starts automatically. Currently, MP3\nand WA V audio are supported. At any time before or\nduring playback, the user may launch any of the sup-\nplied plug-in modules. The different plug-ins offer func-\ntionality for visualization, semi-automatic annotation, and\nquerying. The regular operation mode uses the Multi-\nVis plug-in, which provides functionality for visualizing\naudio-related information synchronously to acoustic play-\nback. Currently, two types of visualization are supported,\none displaying textual information (e.g., lyrics or com-\nmentaries) in a karaoke-like fashion, the other giving a\npiano-roll representation of the musical notes occurring i n\nthe audio recording. The current playback position is in-\ndicated in both types of visualization by highlighting cor-\nresponding text and note positions, respectively. Avail-\nable types of visualization (lyrics or piano-roll) are dis-\nplayed in the track selection box of the MultiVis plug-in\nand may be subsequently chosen by the user. Depend-\ning on the musical contents and existing metadata for the\nselected audio recording, different types of visualizatio ns\n(e.g., lyrics only, piano-roll only, both lyrics and piano-\nroll, multiple lyrics tracks) may be available. Note that an\narbitrary number of MultiVis plug-ins may be launched\nsimultaneously, each operating in an independently se-\nlectable visualization mode. Figure 2 shows the Sync-\nPlayer client application and two instances of the Multi-\nVis plug-in for an audio recording with available lyrics\nand piano-roll metadata. During playback, the piano-roll\nmoves, while a locator is positioned on the current notes.\nAll notes are highlighted at the point of their onset. Past\nand future notes are displayed in different colors.\n2.2 General System Architecture\nThe SyncPlayer framework consists of several software\ncomponents, which are summarized as follows:•The user operates the client application , which per-\nforms acoustic playback and plug-in operation (visu-\nalization, annotation, querying).\n•A remote computer system runs the server applica-\ntion, which is used for identifying the audio record-\nings the user selects. Furthermore, the server system\nis used for retrieving metadata related to the identi-\nﬁed recordings such as lyrics or musical notes, which\nare then used by the client-side visualization plug-\nins.\n•A server-side administration system is used to create\nand maintain an index used for audio identiﬁcation\nas well as for the various types of metadata.\nThe framework uses different types of music-related data:\n•Alocal collection of audio recordings , which are se-\nlectable for playback.\n•A (possibly large) database of audio recordings\nstored on a server. This collection is used in a pre-\nprocessing step for creating an audio index.\n•This audio index is permanently accessible by the\nserver and is used for fast audio identiﬁcation\n•The global metadata (e.g., title, artist, album, etc.)\nfor the pieces contained in the audio database are\nstored in a metadata table , which is accessible by the\nserver application.\n•The content-related metadata (lyrics, piano-roll data,\netc.) for the database items are stored in multi-track\nSync Files (one Sync File for each audio recording; a\nSync File may contain multiple tracks, each contain-\ning different metadata). There are two Sync File for-\nmats: Binary Sync Files (BSFs) are generated during\nthe synchronization step (cf. Section 4). They store\ncontent-related metadata along with synchronization\ninformation (i.e., data relating actual time positions\n383to chunks of metadata) as well as audio feature data\nused for creating the audio index. Textual (XML-\nbased) Sync Files contain content-related metadata\nalong with synchronization information in a human-\nreadable format.\nFigure 1 gives an overview of the SyncPlayer system\narchitecture. In the following subsections, interaction o f\nthe system components is illustrated in more detail.\n2.3 Client System\nWhen the user selects an audio recording and launches the\nMultiVis plug-in, SyncPlayer automatically tries to iden-\ntify both the audio recording and the current playback po-\nsition within the audio recording. The identiﬁcation is\nperformed content-based, i.e., based on the PCM wave-\nform, making it independent of the availability of meta-\ndata (such as ID3 tags) or speciﬁc data formats. More-\nover, the identiﬁcation method as described in Section 3\nis robust against signal distortions, lossy compression, a nd\ncropping. For audio identiﬁcation, the client system ﬁrst\nperforms a feature extraction step. The features are then\ntransmitted to the server system, which performs the ac-\ntual identiﬁcation. Note that the feature-based approach\nsaves both bandwidth and helps preventing legal prob-\nlems, which could result if parts of original audio record-\nings were transmitted from client to server. Upon success-\nful identiﬁcation, the client system is enabled to retrieve\ncontent-related metadata from the server.\nMost of the client system is implemented in Java. As\nfeature extraction generally is a time-critical task, it ha s\nbeen implemented in a C++-based module. This module\nis then accessed using the Java Native Interface (JNI) tech-\nnology. Client-server communication is performed us-\ning Java’s Remote Method Invocation (RMI) framework.\nWithin this framework, the client communicates with the\nserver using an interface of predeﬁned functions. As a par-\nticular beneﬁt of RMI, network communication is almost\ntransparent. Hence, client and server may be located at\nvirtually any point of the network (even on the same com-\nputer system) without requiring major changes in Sync-\nPlayer’s system conﬁguration.\n2.4 Server System\nThe SyncPlayer server consists of a scheduler component\nand a module for audio identiﬁcation (the audentify\nserver), which is also accessed using the Java RMI frame-\nwork. When receiving a request for audio identiﬁca-\ntion, the scheduler directs the request to the audentify\nserver, which in turn performs audio identiﬁcation by\nquerying the audio index using fast search algorithms as\ndescribed in Section 3. Similar to feature extraction, the\nactual index search is implemented in C++. Upon suc-\ncessful identiﬁcation, audentify returns a unique ﬁle\nID, which is subsequently used by the scheduler to re-\ntrieve the content-related data. For this, the scheduler ﬁr st\nretrieves global metadata from the metadata table stored\nin a MySQL database. In particular, the metadata table\ncontains references to existing Sync Files.2.5 Plug-In Modules\nSubsequently, any of the running plug-ins may request the\nglobal as well as content-related metadata stored in the re-\ntrieved Sync Files. Global metadata consists of the num-\nber of different tracks stored in a particular Sync File as\nwell as content identiﬁcators (currently, lyrics and MIDI\nare supported). Access to the Sync Files is possible by re-\nquesting the server to deliver all metadata available for\na certain time interval [s,e]. In the current version of\nour system, time stamps may be speciﬁed in milliseconds.\nThe server returns a sorted list of metadata events consist-\ning of pairs (t,d)where t,s≤t≤e,is a time stamp\nanddis some context-based metadata related to the time\nstamp t. Note that although such metadata might as well\nbe given in sample precision, technical reasons such as\ndifferent sampling rates and different feature extraction\nmethods suggest to use milliseconds as a common basis\nfor specifying time stamps.\nCurrently, three types of plug-ins are supported: in ad-\ndition to the MultiVis plug-in, the Sync File Maker plug-in\ncan be used for semi-manual text-to-audio synchroniza-\ntion, feature extraction, and Sync File creation (see Sec-\ntion 4 for details). Furthermore, the Lyrics Seeker plug-in\nprovides text-based search in lyrics data (see Section 5).\n2.6 The Index Admin Tool\nThe Index Admin Tool is a server-side Java application\nthat is used to organize existing Binary Sync Files, meta-\ndata and audio recordings. It can be used to deﬁne groups\ncontaining arbitrary subsets of Binary Sync Files and gen-\nerate both textual Sync Files as well as audio indexes from\nthose groups. For audio identiﬁcation, the audentify\nserver uses the particular audio index that has been con-\nﬁgured in the Index Admin Tool. Exported textual Sync\nFiles are used by the SyncPlayer server to deliver content-\nrelated metadata to the client application.\n3 AUDIO IDENTIFICATION\nWhen the user starts playback of a speciﬁc audio record-\ning and launches a visualization plug-in, the recording has\nto be identiﬁed before the retrieval of content-related dat a\nis possible. In order to avoid noticeable delays in display-\ning these data, identiﬁcation has to be perfomed very efﬁ-\nciently. Furthermore, as audio recordings are available in\ndifferent qualities or may have been edited prior to play-\nback, identiﬁcation should be robust against basic signal\nprocessing operations such as resampling, lossy compres-\nsion, cropping, or equalization.\nIn the last ﬁve years, several powerful audio identiﬁca-\ntion methods have been proposed such as described by Al-\nlamanche et al. (2001); Wang (2003); Clausen and Kurth\n(2004). We refer to the survey paper Cano et al. (2002)\nfor a more detailed overview of existing methodologies.\nIn this section we ﬁrst describe how we extend the\nidentiﬁcation technique of Clausen and Kurth (2004) to\nbe suitable for the SyncPlayer framework. Then, we look\nat the more general scenario of audio matching and out-\nline how future developments might allow identiﬁcation\nof a musical work regardless of the speciﬁc interpretation.\n384Figure 3: V oting matrix used for fast fault tolerant audio\nidentiﬁcation. The prominent peaks indicate two matches.\n3.1 Identiﬁcation Algorithm\nAssume that a database of Naudio recordings is repre-\nsented as a sequence D= (x1,... ,x N)of ﬁnite energy\nsignals xi,1≤i≤N. From those signals, a compact\naudio index is constructed in a preprocessing step. For\nthis, each signal is processed by a feature extractor F,\nwhich transforms a signal xito a feature document F[xi].\nThe feature document F[xi]is composed of features [t,j],\neach consisting of a feature class jand a time stamp t, in-\ndicating where the feature occurs within the signal. For\nwhat follows, we assume that Fﬁrst transforms a sig-\nnalxiusing a short time Fourier transform (STFT), re-\nsulting in a 2D time-frequency representation ˆxi. Let\n(f1,... ,f K)denote a sequence of Kpitch classes. Then,\nfor each local maximum of |ˆxi|at time position tand\npitchfj, a feature [t,j]is added to F[xi]. Applying fea-\nture extraction to all audio signals, we obtain a collection\nF[D] = (F[x1],... ,F [xN])of feature signals. Note that\nthe latter construction just sketches the idea of our event-\ndriven approach to feature extraction. For details, we refe r\nto Clausen and Kurth (2004).\nThe audio index is then constructed by suitably adapt-\ning inverted ﬁle indexing. In particular, for each feature\nclassj, we create an inverted ﬁle\nHF[D](j) :={(t,i)|[t,j]∈F[xi]}\nconsisting of all pairs (t,i)such that a feature of class\njoccurs at position twithin the ith audio signal, i.e.,\n[t,j]∈F[xi]. In this setting, audio identiﬁcation is a sim-\nple task: consider a short audio signal q, e.g., an excerpt\nof a recording that the user selects for playback. Applying\nfeature extraction, we obtain a feature query F[q]. Then\none easily shows that an intersection\nHF[D](F[q]) :=/intersectiondisplay\n[t,j]∈F[q]HF[D](j)−t (1)\nof suitably adjusted inverted ﬁles\nHF[D](j)−t:={(τ−t,i)|(τ,j)∈HF[D](j)}\nreturns a set HF[D](F[q])of pairs (t,i), each correspond-\ning to an occurence of the t-shifted query features F[q]\nwithin the ith document. It turns out that for suitably sized\nqueries qof only a few seconds of length, one may assume\nthat each of those feature-based matches (t,i)identiﬁes qas a speciﬁc segment of the audio signal xi. For details,\nwe again refer to Clausen and Kurth (2004).\nTo make audio identiﬁcation robust to signal distor-\ntions, it is appropriate to allow a certain percentage pof\nfeature mismatches. In terms of the query method pre-\nsented in Eq. (1) this means we have to determine all\npairs (τ,i)contained in (100 −p)percent of the inter-\nsected lists HF[D](j)−t. Instead of using a previously\nproposed dynamic programming approach (Clausen and\nKurth (2004)), which may be too inefﬁcient for solving\nthis time-critical task for very large data sets, we propose\na novel fast search technique based on a variant of geo-\nmetric hashing, see Wolfson and Rigoutsos (1997). The\nmethod is outlined as follows: assume Tis the maximum\nﬁrst coordinate of an element occuring in any of the lists\nHF[D](j)−t. Further assume that all elements of those\nlists are positive (in practice, both conditions may easily\nbe enforced in a preprocessing step). We then construct\nan integer array Mof dimension T×Nthat will be used\nin the subsequent voting scheme. In a second step, for\neach [t,j]∈F[q]we process the list HF[D](j)−t. For\neach (τ,i)contained in that list, the entry (τ,i)of matrix\nMis increased by one. After this step, each of those en-\ntriesM[τ,i]contains the amount of features of a t-shifted\nversion of F[q]matching document F[xi]. Fig. 3 shows\nan example of the voting matrix Mwith two prominent\npeaks indicating two matches.\nTo account for slight time-distortions of query qand\noriginal signal xi, adjacent entries of Mmay be pooled in\na postprocessing step. This essentially amounts to calcu-\nlating sums M′[τ,i] :=/summationtextλ\nℓ=−λM[τ+ℓ,i]for all posi-\ntionsτ. After this, all entries of M′exceeding a certain\nthreshold are identiﬁed as matches. Note that since inter-\nnal memory for Mmay be allocated in advance and no\nfurther postprocessing is necessary, this procedure is ver y\nefﬁcient. To achieve a further speed-up, Mmay be or-\nganized in a hierarchical manner: at a coarse level, one\nidentiﬁes (for example rectangular) regions of Mcontain-\ning only few votes, which are then rejected at an early\nstage. As detailed by Ribbrock (2005, to appear), the lat-\nter approach amounts in a considerable gain in efﬁciency\nas compared to previous approaches like Wang (2003).\n3.2 Towards Audio Matching\nIn the SyncPlayer scenario, audio identiﬁcation in the\nabove sense appears to be quite restrictive, particularly i n\nthe case of classical music. Because of the multitude of\ndifferent available performances (and recordings) of one\nand the same piece of music, it is unrealistic to assume\nthat a user owns the same version of a piece of music as\nis stored in the server’s database. Hence, it will be of\ngreat interest to identify a piece of music regardless of\nthe speciﬁc performance. We will call this problem audio\nmatching . M¨uller et al. (2005) describe an approach to au-\ndio matching that works well for a broad class of Western\nmusic. The approach is based on using rough harmonic\nprogressions, which is often sufﬁcient for characterizing a\npiece of music. Although the latter approach to matching\nis still to be sped up by suitable indexing mechanisms, it\nshould be feasible to integrate audio matching into future\n385Score\n\nAudio\nMIDI\nFigure 4: Synchronization of music data in the score (top),\naudio (center), and MIDI (bottom) data formats represent-\ning the same piece of music (ﬁrst four measures of Etude\nno. 2, op. 100, F. Burgm ¨uller).\nversions of the SyncPlayer framework to achieve indepen-\ndence of the actual performance of a piece of music.\n4 MUSIC SYNCHRONIZATION\nIn our SyncPlayer system, synchronous display of\ncontent-related musical data relies on appropriate meth-\nods for generating corresponding synchronization data in\na preprocessing step, which is then stored in the previ-\nously described Sync Files. This amounts to linking the\nrespective data (like lyrics or score data) to the actual\nmusical recordings. In the last three years, research has\nfocused on developing ﬁrst approaches for automatically\nperforming these so called synchronization tasks. Impor-\ntant examples include the synchronization of music data in\nthe audio, MIDI and score domain as proposed by Soulez\net al. (2003); Hu et al. (2003); Turetsky and Ellis (2003);\nM¨uller et al. (2004); Raphael (2004). In this section, we\nﬁrst give a brief introduction to music synchronization and\nthen summarize our method for automatically synchro-\nnizing score-to-audio data. Finally, we describe the Sync\nFile Maker plug-in, which was developed for semi-manual\nsynchronization of audio recordings to lyrics data.\n4.1 Score-to-Audio Synchronization\nIn score-to-audio synchronization, we consider the score\nof a particular piece of music as well as an audio record-\ning of the same piece. Then the synchronization problem\nmay be stated as follows: given a musical onset time in the\nscore representation, determine the corresponding physi-\ncal onset time within the audio representation. Synchro-\nnization of score and audio data is illustrated in the upper\npart of Fig. 4. Note that score and audio data differ fun-\ndamentally in their respective structure and content. On\nthe one hand, the score consists of note parameters such\nas pitches, note onsets, or note durations, leaving a lot\nof space for various interpretations concerning, e.g., the\ntempo, dynamics, or multi-note executions such as trills.\nOn the other hand, the waveform-based audio recording\nof a particular performance encodes all the information\nneeded to reproduce the acoustic realization—note param-eters, however, are not given explicitly.\nTo synchronize score and audio data, our approach\ndescribed in M ¨uller et al. (2004) proceeds in two sta-\nges: in a ﬁrst step, semantically meaningful descriptors\nare extracted from the score and audio data streams mak-\ning them locally comparable. In short, the audio signal\nis analyzed using an IIR subband ﬁlterbank consisting of\none ﬁlter for each musical pitch. Subsequent processs-\ning results in one onset signal for each of the subbands,\nwhere note onsets show up as prominent peaks. Finally, a\npeak picking step is used to obtain a score-like representa-\ntion of the audio signal, which consists of one note event\n(p,t,s )for each note candidate of strength sdetected in\nsubband pat onset time t. It is straightforward to convert\nthe score data to a similar format. In a second step, the\ntwo data streams are synchronized by minimizing a suit-\nable cost functional using an alignment technique based\non dynamic time warping (DTW). Here, the score data\nis used as a kind of ground truth in that only those note\nevents extracted from the audio signal are considered that\nhave an explicit counterpart in the score data stream.\nSymbolic data is frequently stored in the widely used\nsemi-symbolic MIDI format, which, in contrast to score\nformats is capable of accurately representing onset posi-\ntions of a particular performance. Fortunately, the latter\nsynchronization techniques may be adapted to the prob-\nlem of MIDI-to-audio synchronization. The bottom part\nof Fig. 4 illustrates this type of synchronization.\nThe score material considered in M ¨uller et al. (2004)\nwas converted to the MIDI format and then synchronized\nto actual piano performances. To make these results avail-\nable to our SyncPlayer, we exported the synchronization\ndata to SyncFiles and indexed the audio recordings as de-\nscribed in Section 3.\n4.2 Text-to-Audio Synchronization\nCurrently, text-to-audio synchronization at the lyrics le vel\nstill requires manual interaction. To assist this process,\nour framework provides the Sync File Maker plug-in. Ba-\nsically, the plug-in allows the user to load a text ﬁle con-\ntaining lyrics data for a selected audio recording. During\nplayback, the user may then specify lyrics positions by\npressing keys on his keyboard at the point of their acous-\ntic output. As this process is frequently very difﬁcult due\nto playback speed and rhythmic complexity of a particular\naudio recording, we extended the plug-in by integrating\na time-scaling functionality. Using a sliding bar, the user\nmay specify an adequate playback speed. The plug-in then\noutputs a slower version of the original audio recording\nhaving the same perceptible pitch (i.e., we use to time-\nscaling to suppress pitching effects). Our time-scaling\nmethod is an adapted real-time version of the WSOLA-\nmethod by Verhelst and Roelands (1993), which basically\nconsists of the follwing steps: the original signal is cut\ninto overlapping parts using a sliding window of a time-\nvarying step-size of s′\n1∈[s1−δ:s1+δ]samples. The\nresulting signal parts are then used to construct a new sig-\nnal. To achieve time scaling, adjacent parts are overlapped\nusing a new ﬁxed step size s2. In order to obtain an s-\ntimes as slow version of the original signal, we choose\n386s2:=s·s1. To suppress audible artifacts, in each step a\nlocal autocorrelation between the current signal part and\nits local neighborhood within the original signal is used\nto determine the latter “best ﬁtting” s′\n1, i.e., we choose s′\n1\nto maximize autocorrelation. Using fast convolution, we\ndesigned an algorithm for performing this step faster than\nin real time (Java-based on an 1.6 GHz Intel P4 platform).\nMethods for automatically synchronizing music to\nlyrics data constitute an upcoming ﬁeld of research. A re-\ncent approach by Wang et al. (2004) presents ﬁrst results\nfor line-level based text-to-audio synchronization.\n5 QUERY ENGINE\nMany types of content-based retrieval like full-text-,\nmelody-, or score-based retrieval (see Clausen and Kurth\n(2004)) allow for an exact localization of a given query\nwithin a matching document. In the SyncPlayer scenario,\nit is then possible to provide the user with an adequate\npresentation of the queries’ occurence in the retrieved doc -\nument. In particular, we may use SyncPlayer’s MultiVis\nmodule to display any of the matches while synchronously\nplaying back a high-quality audio recording of the match-\ning positions.\nAs it is quite natural for many non-expert users to for-\nmulate queries by specifying fragments of the lyrics oc-\ncuring in a song, we integrated a prototypical text-based\nquery engine into the SyncPlayer framework.\nOur technique for lyrics retrieval is term-based and\nalso uses inverted ﬁles as an index structure. In a pre-\nprocessing step, we generate the inverted ﬁles from our\nSync Files S:= (S1,... ,S N): basically, for each term t,\nthe inverted ﬁle HS(t)contains all pairs (p,i)such that t\noccurs as pth lyrics term within Sync File Si. Using in-\nverted ﬁles, query processing may be performed using the\nsame type of intersection as in Eq. (1): a textual query\nq:= (t1,... ,t k), which is a sequence of words (terms),\nis then used to calculate the set of all matches\nHS(q) :=/intersectiondisplayk\nj=1HS(tj)−j. (2)\nThis yields all positions of occurrences of the query q\nwithin the lyrics stored in the Sync Files. To account for\ntyping errors, we preprocess each query term tjand deter-\nmine the set Tjof all terms in our inverted ﬁle dictionary2\nthat have a close edit distance to ti. Then, instead of only\nconsidering the exact spelling tjby using HS(tj)in (2),\nwe consider the union ∪t∈TjHS(t)of occurrences of all\nterms close to t. To account for word errors like inserted\nor omitted words, we preprocess all word positions oc-\ncurring in (2) by a suitable quantization. This amounts\nto replacing jby⌊j/Q⌋and each element (p,i)of an in-\nverted ﬁle by (⌊pj/Q ⌋,i)for a suitably chosen integer Q\nprior to calculating the intersection ( Q= 5 was used in\nour tests). The latter yields a coarse approximation of the\nset of all matching positions, which may then be reﬁned\nin a postprocessing step. There, we compare qwith the\nexact order of occurrence of all query terms and their ac-\ntual proximity at the matching position, a process which\nis very similar to Google’s proximity-based ranking.\n2i.e., the set of all terms with an existing inverted ﬁleFigure 5 shows the Lyrics Seeker plug-in for textual\nqueries (right). A query string can be entered at the top,\na list of ranked query results is displayed below. Lyrics\npositions of matched query terms are highlighted. Upon\nselecting a matching position, the MultiVis plug-in is\nlaunched (left) and playback starts at the position of the\nmatch. Note that also in this scenario legal reasons require\nthat a user has access to the actual audio recordings, i.e.,\nthe recordings have to be stored on the computer running\nthe SyncPlayer client.\nTo conclude this section we remark that it should be\npossible to improve lyrics-based retrieval signiﬁcantly b y\nusing some elaborate text retrieval techniques such as\nstemming, analysis of phrases or thesauri. As a particular\nadaptation to lyrics-type data, it should moreover be ben-\neﬁcial to consider some more Google-like ranking strate-\ngies such as term occurrence at prominent places (such as\nsong title, chorus or hook line).\n6 CONCLUSIONS AND FUTURE WORK\nThe proposed SyncPlayer framework constitutes a pow-\nerful tool for accessing music-related contents in that it\ncombines different modalities like acoustic, graphical an d\ntextual representations in a synchronous fashion. This be-\ncomes possible by suitably integrating elaborate methods\nfrom music- and text-based retrieval with signal process-\ning techniques. We would like to stress that the current\nversion only constitutes the next step towards a framework\nfor integrated querying, display, and annotation of musi-\ncal content. There are various interesting directions for\nextending the proposed SyncPlayer framework as well as\nthe plug-in modules in future work:\n•Extension of the query engine to support melody- or\ngeneral polyphonic score-based queries. For this we\nplan to integrate our own retrieval algorithms in the\nrespective ﬁelds, see Clausen and Kurth (2004).\n•Adaptation of audio matching techniques devel-\noped by M ¨uller et al. (2005) to allow for version-\nindependent audio identiﬁcation,\n•Extension and integration of more elaborate algo-\nrithms for automatic synchronization of music and\nlyrics to audio recordings.\n•Improvement of the Lyrics Seeker plug-in to incor-\nporate advanced ranking mechanisms and concepts\nfor fault tolerant queries.\n•Support text-based browsing functionality with syn-\nchronous playback.\nFor using SyncPlayer in commercial or at least\n“rights-critical” scenario, it would moreover be beneﬁcia l\nto incorporate some kind of digital rights management.\nThus it could be possible to allow for streaming audio\nrecordings in a client-server setting. This is of particu-\nlar interest in retrieval scenarios, where a user generally\nwants to listen to his query results.\n387Figure 5: Lyrics Seeker plug-in: text-based query (right, t op) and retrieval results (right, bottom). Left: SyncPlaye r client\n(top) and MultiVis plug-in (bottom), which is launched upon selection of a query result.\nSOFTWARE AND DEMO\nThe client version of our SyncPlayer is available from our\nwebpage3. Note that for legal reasons we are generally un-\nable to provide audio recordings for download. However,\nto test full functionality of the MultiVis plug-in, the web\npage offers some of our own recordings for free download.\nREFERENCES\nE. Allamanche, J. Herre, B. Fr ¨oba, and M. Cremer. Au-\ndioID: Towards Content-Based Identiﬁcation of Audio\nMaterial. In Proc. 110th AES Convention, Amsterdam,\nNL, 2001.\nP. Cano, E. Battle, T. Kalker, and J. Haitsma. A Review\nof Audio Fingerprinting. In Proc. 5th IEEE Workshop\non MMSP , St. Thomas, Virgin Islands, USA , 2002.\nM. Clausen and F. Kurth. A Uniﬁed Approach to Content-\nBased and Fault Tolerant Music Recognition. IEEE\nTransactions on Multimedia , 6(5), Oct. 2004.\nN. Hu, R. Dannenberg, and G. Tzanetakis. Polyphonic\naudio matching and alignment for music retrieval. In\nProc. IEEE WASPAA, New Paltz, NY , October 2003.\nF. Kurth, M. M ¨uller, A. Ribbrock, T. R ¨oder, D. Damm,\nand C. Fremerey. A Prototypical Service for Real-Time\nAccess to Local Context-Based Music Information. In\nISMIR, Barcelona, Spain , 2004.\nM. M ¨uller, F. Kurth, and M. Clausen. Audio Matching via\nChroma-based Statistical Features. In ISMIR, London,\nGB (submitted) , 2005.\nM. M ¨uller, F. Kurth, and T. R ¨oder. Towards an Efﬁcient\nAlgorithm for Automatic Score-to-Audio Synchroniza-\ntion. In ISMIR, Barcelona, Spain , 2004.\nS. Pauws. CubyHum: a fully operational query by hum-\nming system. In ISMIR, Paris , 2002.\n3http://www-mmdb.iai.uni-bonn.de/projects/syncplayer/J. Pickens, J. P. Bello, G. Monti, T. Crawford, M. Dovey,\nM. Sandler, and D. Byrd. Polyphonic Score Retrieval\nUsing Polyphonic Audio. In ISMIR, Paris , 2002.\nC. Raphael. A hybrid graphical model for aligning poly-\nphonic audio with musical scores. In ISMIR , Barcelona,\nOctober 2004.\nA. Ribbrock. Schnelle Algorithmen zur Konstellation-\nssuche in Multimediadaten . PhD thesis, Department of\nComputer Science, University of Bonn, 2005, to appear.\nF. Soulez, X. Rodet, and D. Schwarz. Improving poly-\nphonic and poly-instrumental music to score alignment.\nInISMIR, Baltimore , 2003.\nR. J. Turetsky and D. P. Ellis. Force-Aligning MIDI Syn-\ntheses for Polyphonic Music Transcription Generation.\nInISMIR, Baltimore, USA , 2003.\nW. Verhelst and M. Roelands. An overlap-add technique\nbased on waveform similarity (WSOLA) for high qual-\nity time-scale modiﬁcation of speech. In Proc. ICASSP ,\nvolume 2, pages 554–557, 1993.\nA. Wang. An Industrial Strength Audio Search Algorithm.\nInISMIR, Baltimore , 2003.\nY . Wang, M.-Y . Kan, T. L. Nwe, A. Shenoy, and J. Yin.\nLyricAlly: Automatic Synchronization of Acoustic\nMusical Signals and Textual Lyrics. In MULTIMEDIA\n’04: Proceedings of the 12th annual ACM international\nconference on Multimedia , pages 212–219, New York,\nNY , USA, 2004. ACM Press.\nH. Wolfson and I. Rigoutsos. Geometric Hashing: An\nOverview. IEEE Computational Science and Engineer-\ning, 4(4):10–21, 1997.\n388"
    },
    {
        "title": "Annotating Musical Scores in ENP.",
        "author": [
            "Mika Kuuskankare",
            "Mikael Laurson"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417805",
        "url": "https://doi.org/10.5281/zenodo.1417805",
        "ee": "https://zenodo.org/records/1417805/files/KuuskankareL05.pdf",
        "abstract": "The focus of this paper is on ENP-expressions that can be used for annotating ENP scores with user definable information. ENP is a music notation program written in Lisp and CLOS with a special focus on compositional and music analytical applications. We present number of built-in expressions suitable for visualizing, for example, music analytical information as a part of music notation. A Lisp and CLOS based system for creating user-definable annotation information is also presented along with some sample algorithms. Finally, our system for automatically analyzing and annotating an ENP score is illustrated through several examples including some dealing with music information retrieval. Keywords: Music representation, annotating, symbolic notation. 1 OVERVIEW Expressive Notation Package (ENP, [1, 2]) is a music notation program that is designed for displaying scores using the common Western music notation. ENP has been used in several research projects ranging from computer aided composition to controlling virtual instruments. A special focus has been given to compositional and computerassisted music analysis applications. ENP has a graphical user interface that allows musical objects to be edited directly with the mouse. It supports two fundamental notational styles, i.e., mensural and non-mensural notation, and a number of special notational styles such as time notation, frame notation, etc. Representation of musical units must offer ways of making annotations, giving names, making comments, adding images or diagrams, providing links to informative resources on the web, etc [3]. ENP provides a collection of standard and non-standard notational attributes Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. c⃝2005 Queen Mary, University of London (e.g, articulations) called ENP-expressions. Furthermore, it offers a set of attributes that can be used to represent analytical information or other user-defined annotations as a part of a musical texture. In addition to their traditional use, ENP-expressions can be used in wide range of applications: (1) display music theoretical analysis information, e.g, annotate motives, harmonic progressions, etc; (2) visualize specialized analytical information, such as Schenker graphs, or pitch-class set theoretical information; (3) attach arbitrary textual annotations, names, or comments to objects; (4) dynamically inspect and visualize data contained by notational objects, i.e., duration, velocity, start-time, etc.; (5) add instructions to tutorials, documentation, or presentations, etc. All ENP-Expressions can access the data contained by the notational objects they are associated with. This allows to design dynamic expressions that can automatically display relevant information about themselves and their musical context. It is also possible to use a scripting language called ENP Script [4] as an algorithmic complement to the manual approach where the user inserts ENPexpressions by hand. This is useful when building, for example, computer-assisted music analysis applications or automatically annotating a musical score. In the following, we present and discuss the annotation possibilities of ENP in more detail. We start with a brief introduction of ENP-Expressions and the notational output of ENP. Section 3 is the main part of this study. Here we present a wide range of annotation devices along with some example scores. Section 4 gives a brief look at the possibilities of automatic music information retrieval and annotation. Section 5 presents some conclusions and ideas for future work. 2 ENP-EXPRESSIONS IN BRIEF Every expression is attached to some musical object or to a group of objects. Currently these objects can be either notes or chords (see [5] for a description of the object hierarchy in ENP). All the expressions are aware of their musical context and can automatically adjust their position and graphical appearance accordingly. New expressions can be created through a textual interface using Lisp and CLOS or by using a set of specialized editors inside ENP. Figure 1 gives a concise overview of the notational possibilities of ENP in a modern context. The example is 72 written in non-mensural notation (time notation) and contains various expressions, special note heads and playing styles. Figure 1: An example of the notational output of ENP. 3 ANNOTATIONS OVERVIEW",
        "zenodo_id": 1417805,
        "dblp_key": "conf/ismir/KuuskankareL05",
        "keywords": [
            "ENP-expressions",
            "music notation program",
            "Lisp and CLOS",
            "user definable information",
            "visualizing music analytical information",
            "Lisp and CLOS based system",
            "creating user-definable annotation information",
            "automatically analyzing and annotating an ENP score",
            "music representation",
            "annotating symbolic notation"
        ],
        "content": "ANNOTATING MUSICAL SCORES IN ENP\nMika Kuuskankare\nDepartment of Doctoral Studies\nin Musical Performance andResearch\nSibelius Academy\nFinland\nmkuuskan@siba.fiMikael Laurson\nCentre for Music and Technology\nSibelius Academy\nFinland\nlaurson@siba.fi\nABSTRACT\nThefocusofthispaperisonENP-expressionsthatcanbe\nused for annotating ENP scores with user deﬁnable infor-\nmation. ENP is a music notation program written in Lisp\nandCLOSwithaspecialfocusoncompositionalandmu-\nsic analytical applications. We present number of built-in\nexpressions suitable for visualizing, for example, music\nanalytical information as a part of music notation. A Lisp\nandCLOSbasedsystemforcreatinguser-deﬁnableanno-\ntationinformationisalsopresentedalongwithsomesam-\nple algorithms. Finally, our system for automatically ana-\nlyzing and annotating an ENP score is illustrated through\nseveral examples including some dealing with music in-\nformation retrieval.\nKeywords: Music representation, annotating, symbolic\nnotation.\n1 OVERVIEW\nExpressive Notation Package (ENP, [1, 2]) is a music no-\ntationprogramthatisdesignedfordisplayingscoresusing\nthe common Western music notation. ENP has been used\nin several research projects ranging from computer aided\ncomposition to controlling virtual instruments. A spe-\ncial focus has been given to compositional and computer-\nassisted music analysis applications.\nENP has a graphical user interface that allows musi-\ncal objects to be edited directly with the mouse. It sup-\nportstwofundamentalnotationalstyles,i.e.,mensuraland\nnon-mensuralnotation,andanumberofspecialnotational\nstyles such as time notation, frame notation, etc.\nRepresentation of musical units must offer ways of\nmaking annotations, giving names, making comments,\nadding images or diagrams, providing links to informa-\ntive resources on the web, etc [3]. ENP provides a col-\nlection of standard and non-standard notational attributes\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of London(e.g, articulations) called ENP-expressions. Furthermore,\nit offers a set of attributes that can be used to represent\nanalytical information or other user-deﬁned annotations\nas a part of a musical texture. In addition to their tra-\nditional use, ENP-expressions can be used in wide range\nof applications: (1) display music theoretical analysis in-\nformation, e.g, annotate motives, harmonic progressions,\netc; (2) visualize specialized analytical information, such\nasSchenkergraphs,orpitch-classsettheoreticalinforma-\ntion; (3) attach arbitrary textual annotations, names, or\ncomments to objects; (4) dynamically inspect and visu-\nalize data contained by notational objects, i.e., duration,\nvelocity, start-time, etc.; (5) add instructions to tutorials,\ndocumentation, or presentations, etc.\nAll ENP-Expressions can access the data contained\nby the notational objects they are associated with. This\nallows to design dynamic expressions that can automat-\nically display relevant information about themselves and\ntheir musical context. It is also possible to use a scripting\nlanguagecalledENPScript[4]asanalgorithmiccomple-\nment to the manual approach where the user inserts ENP-\nexpressionsbyhand. Thisisusefulwhenbuilding,forex-\nample, computer-assisted music analysis applications or\nautomatically annotating a musical score.\nIn the following, we present and discuss the annota-\ntion possibilities of ENP in more detail. We start with a\nbrief introduction of ENP-Expressions and the notational\noutput of ENP. Section 3 is the main part of this study.\nHerewepresentawiderangeofannotationdevicesalong\nwith some example scores. Section 4 gives a brief look at\nthe possibilities of automatic music information retrieval\nand annotation. Section 5 presents some conclusions and\nideas for future work.\n2 ENP-EXPRESSIONS IN BRIEF\nEvery expression is attached to some musical object or to\na group of objects. Currently these objects can be either\nnotesorchords(see[5]foradescriptionoftheobjecthier-\narchy in ENP). All the expressions are aware of their mu-\nsical context and can automatically adjust their position\nand graphical appearance accordingly. New expressions\ncan be created through a textual interface using Lisp and\nCLOS or by using a set of specialized editors inside ENP.\nFigure 1 gives a concise overview of the notational\npossibilities of ENP in a modern context. The example is\n72writteninnon-mensuralnotation(timenotation)andcon-\ntains various expressions, special note heads and playing\nstyles.\nFigure 1: An example of thenotational output of ENP.\n3 ANNOTATIONS OVERVIEW\n3.1 Groups\nGroupsare multi-purpose expressions that can be used to\nmark continuous passages in music. The visual appear-\nance of the groups can be adjusted to suit the current ap-\nplication. Groups can also contain user deﬁnable textual\ninformation. The next example (Figure 2) shows one use\nfor groups to annotate motives in a score. The groups, in\nthis case, are represented asbrackets with labels.\nFigure 2: Motivic analysis information inserted in the\nscore with the help of groups (J.S. Bach: Invention no.\n1).\n3.2 Text Expressions\nTextual information can be attached to both chords and\nnotes. The expressions can be either static or dynamic.\nThese expressions typically contain some written instruc-\ntions or annotations made by the user. Dynamic ones, in\nturn, can adjust the visual appearance and printed infor-\nmation by analyzing their musical context.\n3.2.1 Static Text Expressions\nNext, we give two examples of static text expressions. In\nFigure3theuserhasinsertedtextexpressionstothenotes\nand entered the note names accordingly. This kind of be-\nhavior is acceptable when the data contained by the notes\nis not expected to change (e.g., through transposing). In\nthis case the expressions show their association with the\nnotes by drawing a dotted line connecting the expression\nto the corresponding note.\nFigure 3: Some static text expressions inserted in a score.\nTheassociationbetweentheexpressionandthenotational\nobject is displayed by a dottedline\nAnother example of a static expression is the window\nexpression . This device is useful for giving focus to an\nabject at any position on the screen. These expressions\napply only to the current window so they are suitable for\ninserting markings only to one static page at a time, as in\ncase of tutorials or presentations.\nFigure 4: Window expressions can be used to mark any\nposition in the score.\n3.2.2 Dynamic Text Expressions\nDynamic expressions can adjust their visual representa-\ntion (e.g, position, shape, color, text) automatically. This\nensures that the score is always up-to-date. This is also\nuseful when dealing with analytical problems: the analy-\nsisinformationremainscorrectevenifthemusicisedited.\nFor this purpose we introduce a special expression called\nannotation . Dynamic expressions are also useful to dis-\nplay relevant information accessed from the objects they\nare attached to. Almost any attribute of the notational ob-\njectscanbemadevisibleinthisway. Theattributecanbe\nselected from an automatically generated list that is spe-\ncialized for both chords and notes. The current set in-\ncludes, for example, midi,start-time ,diatone,duration,\nvelocity,channel, etc.\nFigure5givesoneexampleofthebehaviorofdynamic\nannotation. The data displayed is read directly from the\nobjects. In this case the data shown is the value found in\nthemidislot of the note.\nFigure 5: Dynamic expressions can adjust their visual ap-\npearance automatically.\nThe user can also write additional methods for dis-\nplaying the result of some user-deﬁnable code with the\nhelp of annotation expressions. ENP provides an add-to-\nannotation-library macro that can be used to add user de-\nﬁnable annotations to a special library. These annotations\nare immediately usable and can be inserted into the score\n73through context sensitive menus. The parameters of add-\nto-annotation-libraryare: (1)thetypeoftheobjecttheex-\npressionisassociatedwith,(2)thenameoftheexpression,\nand (3) the function that constructs the displayed data.\nBelow we give two relatively simple examples of dy-\nnamictextannotationscreatedwiththehelpoftheadd-to-\nannotation-library macro.\nFirst, we deﬁne a new annotation that can be attached\nto notes (1). In (2) we deﬁne the name of the annotation\n(note-name )andin(3)wegivethefunctionthatconstructs\nthetextualinformationdependingonthemidivalueofthe\nnote. Figure 6 gives the corresponding score.\n(add-to-annotation-library\n:note ;(1)\nnote-name ;(2)\n#’(lambda(note) ;(3)\n(format () \"note name: ˜a\"\n(midi-to-note-name (midi note)))))\nFigure 6: A dynamic text annotation, displaying the note\nname, created with the help of the add-to-annotation-\nlibrary macro.\nAs can be seen in Figure 7, the needed user-interface\ncomponents (context sensitive menus) are also added au-\ntomatically. The annotations can be inserted in the score\nas any built-in ENP-expression (see the ”User methods”\nmenu in Figure 7). The two menus marked as ”Methods”\nand”Slots”containalistofavailableslotsandpredeﬁned\nmethods applicable to the object in question.\nFigure 7: Menus for user deﬁnable annotations are added\nautomatically by the system.\nThesecondexampledealswithdisplayingsomepitch-\nclass set theoretical analysis information. Here we use a\ndynamic text expression to mark the pitch-classes above\nthe individual notes in a twelve-tone row. In Figure 8 we\nhave two twelve-tone rows. The former one is the row\nused in Alban Berg’s Lyrische Suite in original form, and\nthe latter one, in turn, is itstransposed version.\nThe expression deﬁnition is given below. There are\nonly few changes when compared to the previous exam-\nple. The most important one is that in this case we take\nthe modulo 12 of the midi value contained by the note, as\ncan be seen in (1).\n(add-to-annotation-library\n:note\npc-name\n#’(lambda(note)\n(format () \"˜a\"\n(mod (midi note) 12)))) ;(1)\nFigure 8: The twelve-tone row of Alban Berg’s Lyrische\nSuite (above) and its transposed version (below). Note,\nthat the pitch classes shown by the dynamic annotations\n(pc-name) are displayed correctly in the transposed row\nform.\n3.3 Score Expressions\nScore-expressions can be used to visualize discontinuous\ninformation in the score (Figure 9). This is a convenient\nway to display information that is scattered across differ-\nent parts. Arbitrary vertical relations can be made visible\nforanalyticalpurposes. Theshapeofthescoreexpression\ncan also be selected. The next example illustrates the use\nofthebox-shapedscore-expressiontodrawafocusaround\ntwochordsinthescore. Thiskindofanexpressionisuse-\nful for revealing groups containing several entries.\nFigure 9: Focus drawn around two chords using a\nbox-shaped score-expression (Arnold Schoenberg: Sechs\nKleine Klavierst ¨ucke, op. 19).\nA line-shaped score-expression is best suited to indi-\ncate a relation between two notational objects as can be\nseenintheFigure10. Heresomemusicanalyticinforma-\ntion (cross relation) is inserted in the score. In addition to\nthe line connecting the two notational objects the expres-\nsion can contain some user deﬁnable text.\nFigure 10: Voice-leading analysis information displayed\nusing the line-shaped score-expression.\n3.4 Pitch-class Clock Face\nThe Pitch-class clock is a highly specialized expression\nthatcanbeusedtovisualizesettheoreticalinformationas\napartofthemusicaltexture. Itisadynamicgraphicalob-\nject that displays the pitch-class set of a selected chord as\na pitch-class clock face. The pitch classes that are present\nin the set are highlighted (Figure 11).\n74Figure11: Apitch-classclockfaceisaspecializedexpres-\nsionforvisualizingpitch-classsettheoreticalinformation.\n3.5 Highlighting\nHighlighting is a visualization device that can be used\nfor demonstration purposes. Relevant information in the\nscorecanbehighlightedtofocustheattentiontoaspeciﬁc\ndetailinthescore(Figure12). Theareasoutsidethehigh-\nlighted area are dimmed but still clearly visible, thus pre-\nservingtheviewtothewholemusicalcontextatthesame\ntime. The highlighted area can be moved and re-sized by\nthe user.\nFigure 12: A highlighted area created in the bassoon part\nof Rite of Spring by Igor Stravinsky.\n4 AUTOMATICALLY ANNOTATING A\nSCORE\nIn this section we brieﬂy describe how to automatically\nanalyze and annotate an ENP score by using a built-in\nscriptinglanguage,ENPscript. Ascriptisusuallyusedto\nautomate complex or advanced tasks within the program.\nInamusicnotationprogramitisusefultoallowfrequently\nrepeatedoperations(e.g.,applyinganarticulationpattern)\nto be automated by using scripts.\nThe scripting examples in this section are explained\nonlybrieﬂy. Itisnotourintentiontoexplainthesyntaxof\nENP Script but to illustrate the potential of the system in\ngeneral.\nIn our ﬁrst example we examine the vertical aspect of\na piece of music by performing a simple harmonic analy-\nsis.1Theresultoftheanalysis,thenameofthepitch-class\nset of each prevailing harmony, is printed on the bottom\nof the score. The naming of the pitch-class sets follows\nthe conventions introduced by Allen Forte [6]. Figure\n13 gives a score with the set theoretical analysis informa-\ntion. Thescriptusedtogeneratetheinformationisshown\nabovethescore. Thiskindofuseofascriptresemblesthe\ncontainer-iterator idea found, for example, in CPNView\n[7, 8]. Here we iterate through every harmonic situation\n1Here we consider the term harmony to be a result of one,\ntwo or more notes being sounded simultaneously. Usually the\nsmallest possible harmony contains at least two notes. For sim-\nplicity, monophonic situations are regarded as special cases.in the score (see the :harmony keyword in (1) in the ex-\nample script). A special Lisp function add-expression (2)\nisusedtoinsertintothescoreagroupexpressiondisplay-\ning the analysis information. The expression is attached\nto the lowest note of the chord (3). In (4) the built-in set-\ntheoretical function sc-nameis used to identify the pitch-\nclass set name of the currentharmony.\nAscanbeseeninFigure13therearethreepitch-class\nsetnamesbelowtheﬁrstnoteinthesecondmeasure(E/flat).\nThis is due to the fact that there are three different har-\nmonies that include the note in question. The set-class\nnames are, from the bottom up: (1) 1-1, which is the re-\nsult of the E/flatsounding alone, (2) 2-5, which is the result\nof the E/flatand two-line B/flatin the right hand staff sounding\ntogether, and (3) 2-6, resulting from the low E/flatand the\none-line A in the right hand.\n(*?1 :harmony ;(1)\n(?if\n(when (complete-chord? ?csv)\n(add-expression ;(2)\n’group (give-bass-item ?csv) ;(3)\n:info (sc-name (m ?1)))))) ;(4)\nFigure 13: Vertical pitch-class set theoretical analysis in-\nformation displayed in the score with the help of ENP\nScript and groups (Arnold Schoenberg: Suite f ¨ur Klavier,\nop. 25).\nThe second example deals with horizontal aspects of\nmusic. The script in this case is used to calculate and dis-\nplay the interval chain of a melodic line. The script is\ngivenbelowandtheresultingscorecanbeeseeninFigure\n14. In this case we are interested in the relation between\ntwo consecutive notes (see (1) in the script). An analysis-\ntext-group expression is used in this case (2). It is posi-\ntioned above the associated objects (see Figure 14). The\nexpression is also displaced horizontally (3) so that it is\npositioned between the notes. The printed information is\ncalculated by subtracting twoconsecutive pitches (4).\n(*?1 ?2 ;(1)\n(?if\n(add-expression ’analysis-text-group ?1 ;(2)\n:user-position-X-correction ;(3)\n(- (/ (width ?1) 2.0) 1.0)\n:info (- (m ?2) (m ?1))))) ;(4)\nFigure 14: The interval chain of the melodic line dis-\nplayed above the staff (Anton Webern: Zwei Lieder, op.\n8).\nFinally, we givean example dealing with n-grams. N-\ngrams have been widely used in text retrieval, where the\n75sequenceofsymbolsisdividedintooverlappingconstant-\nlength sub-sequences [9]. The use of n-grams in MIR has\nalso been considered by several authors (see for example\n[10]).\nLet us examine the script in more detail. It is divided\ninto two parts. The ﬁrst part (A) calculates di-grams and\nthe second part (B) calculates tri-grams. The Lisp func-\ntion, add-expression, is again used to insert the analysis\ninformation into the notes (2). This time the information\nis attached to multiple notes (see the di-gram information\nandthecorrespondingbracketsinFigure15). Alsoamore\nsubtlebracketstyleisutilized(3). In(4)and(5)theinter-\nval information is calculated and formatted (the speciﬁcs\nof the Lisp’s format function are not in the scope of this\npaper. See[11]fordetailedinformation). Thesecondpart\nof the script (B) is otherwise identical, when compared to\ntheﬁrstone,exceptthatwenowconsiderfourconsecutive\nnotes(6)formingthreeintervals. Theprintedinformation\nis again calculated in (7-9).\nA.\n(*?1 ?2 ?3 ;(1)\n(?if\n(add-expression ’group ?1 ?2 ?3 ;(2)\n:kind :bracket-at-end ;(3)\n:info (format () \"˜3,@d |˜3,@d\" ;(4)\n(- (m ?2) (m ?1)) ;(5)\n(- (m ?3) (m ?2))))))\nB.\n(*?1 ?2 ?3 ?4 ;(6)\n(?if\n(add-expression ’group ?1 ?2 ?3 ?4\n:kind :bracket-at-end\n:info (format () \"˜3,@d |˜3,@d |˜3,@d\"\n(- (m ?2) (m ?1)) ;(7)\n(- (m ?3) (m ?2)) ;(8)\n(- (m ?4) (m ?3)))))) ;(9)\nFigure 15: Di- and tri-grams displayed as a part of a mu-\nsical texture (J.S. Bach: Musikalisches Opfer).\n5 CONCLUSIONS\nA comprehensive set of different annotation devices in\nENP were discussed. Also a scheme to automatically an-\nalyze a score and to visualize analytical data was intro-\nduced.\nCurrentlytheENP-expressionscanbeattachedonlyto\nnotes or chords. The present scheme should be extended\nto apply all the objects in the score. This way it would\nbe possible to attach annotations to parts, voices, mea-\nsures,barlines,clefs,etc. Itispossible,however,toinsert\ncommentstomeasures,forexample,buttheseannotations\nhavedifferentstatusthantheregularENP-expressions. By\naddressing this problem it would also make the ENP ob-\nject representation schememore uniﬁed and coherent.Displaying MIR results in a score is suitable only for\ncertain types of data. To bring ENP closer to practical\nMIR applications it would be beneﬁcial to integrate a sta-\ntisticalpackage,suchasR[12],intotheenvironment(Ris\na language and environment for statistical computing and\ngraphics). This would allow to represent, for example,\npitch histograms and other similar information retrieved\nfrom an ENP score.\nACKNOWLEDGEMENTS\nThe work of Mikael Laurson has been supported by the\nAcademy of Finland (SA 105557).\nREFERENCES\n[1] Mika Kuuskankare and Mikael Laurson. ENP2.0\nA Music Notation Program Implemented in Com-\nmon Lisp and OpenGL. In Proceedings of Interna-\ntional Computer Music Conference , pages 463–466,\nGothenburg, Sweden, September 2002.\n[2] MikaKuuskankare andMikaelLaurson. Expressive\nNotation Package - an Overview. In International\nSymposium on Music Information Retrieval , 2004.\n[3] TillmanWeyde. CaseStudy: RepresentationofMu-\nsical Structure for Music Software.\n[4] Mika Kuuskankare and Mikael Laurson. Intelligent\nScripting in ENP using PWConstraints. In Proceed-\nings of International Computer Music Conference ,\npages 684–687, 2004.\n[5] MikaKuuskankareandMikaelLaurson. RecentDe-\nvelopments in ENP-score-notation. In Sound and\nMusic Computing ’04 , October 2004.\n[6] AllenForte. TheStructureofAtonalMusic. Journal\nof Music Theory , 1973.\n[7] Donncha ´O Maid´ın. Common Practice Notation\nView: a Score Representation for the Construction\nof Algorithms. Proceedings of International Com-\nputer Music Conference , pages 248–251, 1999.\n[8] Donncha ´O Maid´ın and Margaret Cahill. Score Pro-\ncessingforMIR. InternationalSymposiumonMusic\nInformation Retrieval , pages 59–64, 2001.\n[9] Shyamala Doraisamy. Polyphonic Music Retrieval:\nThe N-gram Approach . PhD thesis, University of\nLondon, 2004.\n[10] J.S. Downie. Evaluatinga SimpleApproach toMu-\nsic Information Retrieval: Conceiving Melodic N-\ngrams as Text . PhD thesis, University of Western\nOntario, 1999.\n[11] GuyL.Steele. CommonLISP:TheLanguage . Dig-\nital Press, 2nd edition, 1990.\n[12] Francisco Cribari-Neto and Spyros G. Zarkos. R:\nYet another econometric programming environ-\nment.Journal of Applied Econometrics , 14:319–\n329, 1999.\n76"
    },
    {
        "title": "Preservation Digitization of David Edelberg&apos;s Handel LP Collection: A Pilot Project.",
        "author": [
            "Catherine Lai",
            "Beinan Li",
            "Ichiro Fujinaga"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416616",
        "url": "https://doi.org/10.5281/zenodo.1416616",
        "ee": "https://zenodo.org/records/1416616/files/LaiLF05.pdf",
        "abstract": "This paper describes the digitization process for building an online collection of LPs and the procedure for creating the ground-truth data essential for developing an automated metadata and content capturing system.",
        "zenodo_id": 1416616,
        "dblp_key": "conf/ismir/LaiLF05",
        "keywords": [
            "digitization",
            "online collection",
            "LPs",
            "automated metadata",
            "content capturing",
            "system",
            "ground-truth data",
            "procedure",
            "essential",
            "describes"
        ],
        "content": "PRESERVATION DIGITIZATION OF DAVID EDELBERG’S \nHANDEL LP COLLECTION: A PILOT PROJECT\nCatherine Lai Beinan Li Ichiro Fujinaga \n \n \n \nlai@music.mcgill.ca Music Technology, F aculty of Music \nMcGill University \nMontreal, Canada \nbeinan.li@mail.mcgill.ca  \n \n \nich@music.mcgill.ca \nABSTRACT \nThis paper describes the digitization process for build-\ning an online collection of LPs and the procedure for \ncreating the ground-truth data essential for developing \nan automated metadata and content capturing system.  \n \nKeywords: Digitization, Preservation, Analogue \nSound Recordings, Use and Access, Digital Library \nCollections.  \n1 INTRODUCTION \nLong-playing phonograph records (LPs) were one of \nthe major analogue recording formats distributed \ncommercially throughout most of the twentieth cen-\ntury. Although most of these historic sound recordings \nhave long shelf lives, compelling reasons have led to a \nshift toward digital preservation.  \nTo assure preventative preservation and facilitate \nnew forms of access to this very important cultural \nheritage, a large digitization effort is required. An \nefficient and economical workflow management sys-\ntem is essential to carry out the steps in the digitiza-\ntion process. This digitization process is time-\nconsuming and expensive since many steps involved \nin the digital conversion, such as metadata extraction, \nrequire much human intervention and a high-level \nmusical and bibliographic knowledge.  \nIt is essential to minimize human intervention so as \nto reduce the cost of digitizing very large numbers of \nLPs. One way of achieving this is to integrate sophis-\nticated pattern recognition systems to automatically \ngenerate text and metadata from the captured images. \nAnother time-consuming task, if performed by a \ndedicated human digitization operator, is separating \nthe music tracks that are on each side of audio discs. \nA plausible approach to automating track separation \nis to use digital signal classification techniques.  \nApproximately thirty LPs from David Edelberg’s \nHandel collection were digitized as a pilot study. The \nLPs, housed in McGill University’s Marvin Duchow Music Library, are one of the largest collections of \nanalogue recordings of Handel’s music. Much of the \neffort at this initial stage of the project was devoted to \ndigital benchmarking for conversion and access and \nto creating ground-truth data that can be used to train \nand test content analysis systems, thereby automating \nthe digitization process.  \n2 BACKGROUND \nDigital library projects focusing on audio preservation \nare still in the development stage. The Loeb Music \nLibrary Audio Preservation Studio of Harvard Univer-\nsity is currently examining the methodologies and \ntechnologies needed to access sound recordings and \nother digital objects [1]. The Digital Audio-Visual \nPreservation Prototyping Project of the Library of \nCongress (LC) is investigating approaches for refor-\nmatting recorded sound and moving image collec-\ntions, with a focus on metadata [2]. The University of \nCalifornia at Santa Barbara is conducting a pilot pro-\nject on cylinder preservation and digitization [3]. \nOther related research projects on sound recordings \ninclude Indiana’s Variations2 project [4], the digitiza-\ntion of 78rpm recordings at the Frontera Archives [5], \nand the Digital Audio project at the National Library \nof Canada [6].  \nThe digital preservation of the Edelberg Handel \ncollection is unique for several reasons. It deals with \na large collection of LPs, involves digitization of both \naudio and visual components (album covers and liner \nnotes), and involves benchmarking for conversion \nand access. It implements an integrated database with \nsearchable full text, images of album covers and re-\ncord labels, and audio files of LPs. Furthermore, it \ndevelops automated content capture systems to reduce \nthe cost of digital conversion whenever possible.  \n3 PREPARATION OF THE QUALITY \nCONTROL ENVIRONMENT  \nQuality control (QC) is an essential and integral com-\nponent in various stages of digitization. The quality of \ndigital reproduction rests to a significant degree on the \nQC instruments and software [7]. The Handel digitiza-\ntion project uses state-of-the-art digitization equip-\nment and software tools to reformat and reproduce \nanalogue sound recordings. The multimedia digitiza-\ntion workstation consists of professional models of a \nrecord cleaning machine, turntable, and large-format \nflatbed scanner; a phono-preamplifier and A/D audio Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full \ncitation on the first page. \n© 2005 Queen Mary, University of London \n570   \n \n converter; and a powerful workstation with dual moni-\ntors for close visual inspection of image quality. Other \nQC features relating to equipment installation, con-\nfiguration, and development include colour manage-\nment, which ensures colour consistency from image \ncapture through display, and monitor calibra-\ntion/optimization, which ensures onscreen accuracy by \nsetting white and black point, gamma, and colour bal-\nance.  \n4 COPYRIGHT AND RIGHTS \nMANAGEMENT  \nCopyright of sound recordings usually belongs to the \nrecord labels that issued the recordings, and it is the \nsingle most important legal issue to consider when \nplanning a digitization project. However, since LPs \nare often the aggregate creation of several parties, \nindividuals such as photographers, translators of lyr-\nics, designers of artwork on album covers, and others \nalso have varied rights to the use of the sound record-\nings. Since the dates of LP release were not widely \nindicated on LP labels or album covers before the in-\nclusion of phonogram dates beginning in 1976 [8] and \nrenewed or extended copyrights due to re-release of \nalbum records can occur, legal clearances of LPs for \ndigitization require complex rights management \nchecks via various sources such as the WorldCat \nOCLC Online Union Catalogue, the Bielefelder Cata-\nlogue, the Diapason Catalogue, the Schwann Cata-\nlogue, and the Gramophone Catalogue. \n5 METADATA SCHEMA \n5.1 Need for a Metadata Schema \nTraditional standards for cataloguing sound recordings \nexist [9, 10, 11] but are inadequate for digitized repre-\nsentations of LPs as cultural heritage objects. The \nstandards are generally limited to bibliographic de-\nscription of relatively few elements [12]. Information \nabout artwork or photographs in the album packaging, \nfor example, is usually not included. \nA metadata schema was designed to better facili-\ntate the management and use of resources from the \nintellectual content to the research of rights and per-\nmissions. This schema was created to the finest level \nof granularity possible in order to meet user needs \nand provide various functionalities in the context of a \ndigital library.  \n5.2 Existing Metadata Standards \nVarious efforts in the archival communities, digital \nlibrary research groups, and museums have estab-\nlished recommended metadata element sets for multi-\nmedia such as digital still images. A few well-known \nmetadata standards designed for visual collections are \nthe Categories for the Description of Art (CDWA) \n[13], the Visual Resources Association’s core catego-ries (VRA Core 3.0) [14], Harvard’s data dictionary of \nadministrative metadata for digital still images [15], \nand the National Information Standards Organiza-\ntion’s Data Dictionary of technical metadata for digi-\ntal still images (ISO Z39.87) [16].  \nOther recognized authorities have contributed and \nexpanded the utility of metadata element sets to digi-\ntal library service models. The Making of the Amer-\nica II Testbed Project of the California Digital Li-\nbrary (CDL) worked extensively to identify and de-\nfine the structural and administrative metadata ele-\nments that are crucial in the development of digital \nlibrary services and tools [17]. LC also developed a \nset of core metadata elements, including administra-\ntive, structural, and descriptive metadata, for the LC \nDigital Repository Development [18].  \nMetadata standards for describing sound record-\nings, in comparison to digital still images, are rela-\ntively few. MARC21 has been used for bibliographic \ndescription of sound recordings in major libraries, but \nthe traditional cataloguing practices take a forced \none-schema-fits-all approach. Some metadata \nschemes (e.g., the core metadata elements of LC) \nhave a very small number of elements pertaining to \ncharacteristics of sound recordings: audio bits per \nsample, audio channel configuration, audio channel \ninformation, and audio sampling frequency. MPEG-7 \n[19] defines elements for description of audio and \nvideo content. However, none of these existing stan-\ndards have characteristics tailored to the structural \ncomplexity that is necessary for the full description of \nsound recordings, such as information at the individ-\nual track or song level (e.g., recording session date, \nperformers, recording engineers, recording equipment \nused, etc.). \n5.3 Metadata Schema Design for Sound Recordings \nA comprehensive metadata schema for describing LPs \nhas been designed to facilitate resource discovery, \nrights management and access control, as well as ad-\nministration and preservation in the networked envi-\nronment. The schema includes five types of metadata: \ndescription metadata to enable discovery and identifi-\ncation of resources; administration metadata to sup-\nport management of resources; structure metadata to \ndescribe font and layout characteristics of texts; legal \nrights metadata to protect intellectual property rights; \nand technical information metadata to record the cap-\nture process and technical characteristics of digital \nobjects. The metadata type and categorization designs \ndescribed above are based on informed decisions be-\nlieved to best serve the delivery, organization, and \nmanagement of networked information relating to \nsound recordings. The new metadata schema provides \nfor complete auditory, pictorial, and textual content \nanalysis. Characteristics from Dublin Core, MARC21, \nMODS, METS, TEI, metadata schemes dedicated for \nvisual collection and audio as introduced above, and \nothers were partially incorporated into its design. The \ncurrent schema contains more than 120 fields. \n571   \n \n 5.4 Examples of Metadata Elements \nMetadata is created at different levels to facilitate the \nmanagement of the wide variety of components (e.g., \ntracks, discs, performers, recording sessions, etc.) and \ncombinations of these components that comprise \nsound recordings. Metadata belongs to one or more of \nthe following hierarchical classes: Collection, Album, \nImage, Disc, and Track. Examples of metadata ele-\nments at different levels are: \n• Collection level: summary, subject, scope and con-\ntent. \n• Album level: title, varying form of title, language \nof title, series statement, label name, form of musi-\ncal composition, acknowledgement, handling in-\nstructions.  \n• Image level: description, date of photo, photogra-\npher. \n• Disc level: audio disc size, number of tracks, play-\ning speed, matrix numbers, duration, playback \nchannel. \n• Track level: title, duration, date of work, perform-\ners, date of recording, recording location, recording \nequipment used. \n6 CONTENT MANAGEMENT  \n6.1 Web Data-entry Form \nA web data-entry form written in PHP was imple-\nmented for the encoding of LP data and metadata us-\ning the metadata schema. The metadata entry system \nenforced quality control, using check boxes and option \nbuttons whenever possible to reduce typing errors. \nThe form also incorporated dynamic features, allow-\ning multiple entries of one metadata element (e.g., \ntracks). For data-entry fields that stay unchanged \nthroughout one entire digitization session (e.g., scan-\nning equipment), the form provided auto-fill options \nto populate the repeating data values. Moreover, the \nform employed error checking to validate data before \nsubmission to a relational database. The data-entry \nform also provided easy-to-update features to modify \nexisting records stored in the database. \n \n6.2 Database Design and Maintenance \nA relational database in MySQL was designed and \nimplemented to hold the metadata and the content of \nthe digitized material. The database model reflected \nclass hierarchies presented in the metadata schema \ndesigned for sound recordings. \nData verification and database tuning were con-\nducted iteratively throughout the digitization process \nfor quality assurance and performance considerations. \nThis guaranteed data accuracy and helped to attain \ntransparency in database design. 7 DIGITIZATION \nThe digitization process began with cleaning each disc \nand digitizing audio at 24bit/96kHz using an audio-\nphile-quality turntable and a cartridge. This was fol-\nlowed by scanning all images, including the album \ncovers, audio discs (for labels and matrix numbers), \nand any accompanying materials at 24bit/1200dpi. \nThe process finished with metadata entry and text \nconversion. \n7.1 Audio Capture \nAn LP must be as clean as possible to achieve opti-\nmum audio quality; therefore, each side of the records \nwas thoroughly vacuum-cleaned before each audio \ndigitization to remove any dirt or surface particles. \nThe cleaning and digitization of one side of an audio \ndisc took approximately 30 minutes.  \n7.2 Image Scanning \nImage digitization of LPs included scanning the album \ncovers, audio discs, and all accompanying material. \nThe LPs were in the standard size, i.e., 12 inches on \neach side and 12 inches in diameter for the covers and \ndiscs, respectively. To ensure consistency and im-\nprove the exchange of images across a wide range of \ndisplay equipment, a small colour separation guide \n(Kodak No. Q-13) composed of a set of standard col-\nour patches (primaries, white, and black) was always \nplaced in the same position relative to the scanned \nobjects. Scanning an image at 24bit/1200dpi took ap-\nproximately 13 minutes and saving the file to disk \ntook an additional 12 minutes.  \n7.3 Metadata Extraction and Text Conversion \nMetadata extraction and text conversion were two of \nthe most expensive steps of digitization. This included \nmeasuring the physical positions and size of the visual \ncontents of the album and any accompanying material. \nHigh-level musical and bibliographic knowledge was \nrequired to extract and enter metadata using the web \ndata-entry form, and text conversion of program notes \non album covers or any accompanying material re-\nquired typing columns of text that took at least 20 \nminutes per column of approximately 670 words.  \nAn average of six hours was needed to process a \nphonograph album using manual entry in this initial \nexperiment. Although taking the physical measure-\nment was extremely time consuming, it is estimated \nthat even without this requirement, the process would \nstill take about three hours per phonograph album. \n8 CREATION OF DIGITAL MASTERS \nAND DERIVATIVES \nAlthough there are several standards governing the \ncreation and use of digital images and audio, such as \nCDL’s digital image format standards [20] and the \nIASA Guide [21], there is no uniform approach that \nsuits all circumstances [7]. To ensure fitness in differ-\n572   \n \n ent situations for the creation of digital images and \naudio for various purposes (i.e., archiving, accessing, \nor browsing), different technical specifications were \nexamined and developed for the digitization of the \nDavid Edelberg Handel LPs.  \n8.1 Requirements and Evaluation of Digital Images \nTwo representative items in distinctive form were \nassessed to realize a scanning template for the creation \nand use of digital images: an audio disc (made of vi-\nnyl) and its album cover (made of cardboard). Re-\nquirement definitions were developed to set the \nevaluation criteria for the scanned images, and these \nincluded attention to the legibility of the smallest text, \npreservation of colour appearance, and speed of deliv-\nery.  \nThe strategy pursued in the creation of the digital \nmasters was matched to the technology available at \nhand. Michael Ester has argued for the creation of \nrich digital masters to safeguard the long-term value \nof images and the investment in acquiring them [22]. \nThe focus of the archival copy was therefore on fidel-\nity in addition to legibility. To exploit the best re-\nsources (e.g., storage space and computing power) \nand utilize the current state-of-the-art scanning tech-\nnology (e.g., scanning resolution and depth), master \nfiles were created at 24bit/1200dpi. For the file for-\nmat of the preservation copy, two lossless compres-\nsion formats, TIFF and PNG, were examined. Al-\nthough TIFF has been the preferred format for long-\nterm retention in the digital library community, some \ngroups, such as the Technical Advisory Service for \nImages, favour PNG and other formats such as SPIFF \nbecause they are open formats, offer good metadata, \nand use better lossless compression [7]. A compari-\nson of the file size among non-compression TIFF, \nlossless compression TIFF (i.e., TIFF LZW) and \nlossless compression PNG was made. The signifi-\ncance in file size reduction (e.g., 863MB vs 400MB \nvs 337MB) informed the decision to save digital mas-\nters in the PNG lossless compression format.  \nAccess dictates other evaluation criteria. Three \ntypes of access files were automatically generated: a \nprint file (300dpi), a web display file (96/120dpi), and \na thumbnail file (72dpi). The requirements for the \naccess file are to meet legibility of the smallest text \nwhen displayed online, preserve colour appearance, \nand meet a reasonably fast speed of delivery. A good \naccess image conveys the most desired information \ngiven constraints such as the speed of delivery and \nuser tolerance. Two different specifications were used \nfor the creation of access files due to the inherent \ndifferences in the nature of the two types of material. \nFor the presentation of album covers, it was deter-\nmined that the smallest text was becoming illegible \non a standard computer screen at resolutions less than \n24bit/96dpi. A resolution at 96 dpi was therefore cho-\nsen for the access files of album cover images while \n120dpi was chosen for the label of the audio disc \nwhich often contained very small texts. Two lossy compression formats, JPEG and GIF, \nwere also examined. JPEG was preferred over GIF \nbecause album covers often contain art that consists \nof many colours, and GIF is usually recommended for \ncompressing graphics that have large areas of the \nsame colour. \nThe specification for the thumbnail files was based \non another set of evaluation criteria. Since visibly rich \ndata is unlikely to be perceived in this browsing for-\nmat, the emphasis was placed on ease of identifica-\ntion rather than detailed viewing. The technical speci-\nfication of the thumbnail files for scanned album im-\nages was determined to be at 8bit/72 dpi. The same \nspecification was used for the thumbnail files of the \naudio disc. The GIF format was chosen over the \nJPEG format for thumbnail files because of its gener-\nally smaller size.  \nSpecial consideration was given to the creation of \ndigital masters and derivatives of scanned audio \ndiscs. Specifically, each audio disc, as image, was \nscanned and saved in its entirety during archiving. \nHowever, for better display purposes, the access and \nthumbnail files of audio discs included only the label \nof the disc, which was cropped from the original \nscanned images.  \n8.2 Batch-processing of Image Derivatives \nThe process of creating different versions of images \nfrom master files was accomplished automatically \nusing the open-source, cross-platform software Ima-\ngeMagic. Using UNIX shell scripting, the colour sepa-\nration guide that was included in the original scan was \ncropped and different versions of derivatives were \ncreated. The time to convert a 1200dpi TIFF image of \napproximately 860 MB to PNG format was about 35–\n50 minutes, and the time for JPEG format was about \n30 minutes on a 2.8GHz Pentium 4 computer running \nRedHat Linux 9.0. \n8.3 Requirements and Evaluation of Digital Audio \nThere are very few guidelines for audio digitization \nrequirements and the creation of audio derivatives. \nThe audio-visual prototyping project of LC is one of \nthe few research projects that has put forward recom-\nmended technical specifications for the master and \nderivative service files [2]. IASA is another research \ngroup that has published guidelines on the production \nand preservation of digital audio [21].  \nFor the audio digitization of the Handel LPs, pres-\nervation masters files were captured at 24bit/96kHz in \nmono or stereo, depending upon the characteristic of \nthe LP, and saved in the industry standard AIFF for-\nmat. Access or derivative service files were created in \nvarious levels of fidelity in different formats: WAV \nfiles of 16bit/44.1kHz, MP3 files of 192kbps and \n112kbps, and Ogg Vorbis files of quality 5. WAV \nfiles were created to offer high-quality audio that can \nbe easily used on both Macs and PCs. Access files in \nMP3 format were created in higher and lower fideli-\nties to provide good-quality audio with smaller file \n573   \n \n sizes. Ogg derivative files were created because the \nformat is completely free, open and unpatented. This \nformat is also currently gaining popularity and is in-\ncluded in popular media players under both Windows \nand Mac OS X. Furthermore, Ogg quality 5 is the \nofficial setting recommended for representing music \nof CD quality [23]. \n8.4 Batch-processing of Audio Derivatives \nThe process of creating different versions of audio \nderivatives was accomplished automatically using the \nopen-source, cross-platform software sndfile-convert, \nSoX, and LAME. The first step in audio derivative \ncreation was to convert the original 24bit/96kHz AIFF \nfiles to 16bit/96kHz AIFF files, which took approxi-\nmately 50 seconds for one side of an LP disc (about \n25 minutes). The second step was to convert the audio \nsampling rate to 44.1kHz, which took approximately \n12 minutes. Derivative files of lower fidelities were \nsubsequently created (i.e., Ogg quality 5 and MP3 of \n112kbps and 192kbps). SoX was used to derive Ogg \nVorbis files from WAV files and LAME was used for \nWAV to MP3 conversion. The time needed to convert \nthe specified Ogg file from a 16bit/44.1kHz WAV file \nof approximately 25 minutes of music was about 4 \nminutes, and the time used to convert 112kbps and \n192kbps MP3 files was about 3 and 5 minutes, respec-\ntively. \n9 WEB DELIVERY \nA web site written in PHP was designed, developed, \nand maintained to facilitate easy information retrieval \nof the digital collection of David Edelberg’s Handel \nLPs. This site provides online access to the intellectual \ncontent and reproductions of both images and audio. \nAll the metadata associated with an LP, links to sepa-\nrated and continuous audio tracks, and scanned images \nof album covers and any accompanying material are \ndisplayed for each record. Multimedia files in differ-\nent formats and resolutions are offered to meet diverse \nuser needs due to variations in computer platform and \nconnection bandwidth. \n10 CHALLENGES \nThe structural complexity of music and LPs imposes \nmany challenges to developing digital collections of \nphonograph recordings. One of the challenges in \nbuilding a digital collection is to define metadata for \nsound recordings and determine the level of detail to \nmaintain the various kinds of metadata [7]. Defining \nthe level of granularity for the metadata is important \nand challenging because the success of digital preser-\nvation efforts rests to a significant degree on the scope \nand completeness of the metadata recorded.  \nBecause Handel LPs are available in different lan-\nguages in addition to English, and they also contain \npieces by other composers, another challenge in \nbuilding the digital collection is to develop and adopt a name authority control. Such control would allow \nmanagement of variations in spelling (e.g., names of \ncomposers, performers, producers) and musical work \ntitles. Similarly, a systematic vocabulary control is \nnecessary for proper semantic description of image \ncontent of artwork on album covers for effective \nsearch based on image description keywords.  \nOther challenges in developing a digital collection \ninclude evaluating the effectiveness of the overall \nworkflow management system and testing the usabil-\nity of the initial model of the web delivery system. \nThe evaluation of the digital derivatives, for example, \nwas entirely based on visual perception. A standard-\nized evaluation guideline based on concrete empirical \nevidence is necessary to determine the best practices.  \nLastly, the complex rights management for differ-\nent elements of LPs, including photographs, artwork, \ntrademarks, music, music arrangements, lyrics, etc. \nremains a complicated task.  \n11 FUTURE WO RK \nAn immediate task for this project is to automatically \nseparate music tracks using digital signal classification \ntechniques. An open-source software to remove pops, \nclicks, and other noise from LP recordings is also un-\nder development. Another challenging future task is to \nautomate the metadata and content extraction. This \ngoal can be achieved by using the ground-truth data \nalready captured in this project and further developing \nthe structured document analysis application, Gamera \n[24]. Finally, software and tools to automatically link \nbibliographic records from existing MARC records \nduring metadata entry will be developed, thus further \nreducing the cost of metadata extraction.  \n12 CONCLUSION AND SIGNIFICANCE \nDue to the enormous quantity of existing recordings \nand the time required to properly and faithfully digit-\nize them, an efficient and economical workflow man-\nagement system for digital conversion is necessary. \nThis project is part of a larger research plan to develop \nframeworks and tools for creating distributed digital \nmusic archives and libraries. By developing digital \ncollections of analogue holdings such as David Edel-\nberg’s Handel LPs, libraries protect their special hold-\nings as part of a cultural necessity and meet the pres-\nervation goal by limiting physical access to the origi-\nnal sound recordings. With full information capture \nusing advanced digital technology, moreover, libraries \nincrease the serviceability of their underutilized col-\nlection with improved access points. This allows new \nresearch and educational uses of valuable but rare \nholdings, delivers information content directly without \nhuman intervention, and facilitates simultaneous use \nby potentially competing users across traditional \nboundaries. By applying and using emerging post-\nprocessing techniques and tools, libraries can offer \nunprecedented services, presenting audio and visual \n574   \n \n content in optimized digital quality that is otherwise \nimpossible to achieve with the original sources. \nACKNOWLEDGEMENT \nThis research is funded in part by the “Richard M. \nTomlinson Digital Library Innovation and Access \nAward,” David Edelberg Foundation, CIRMMT, and \nthe FQRSC. The authors would like to thank Rebecca \nFiebrink and Cory McKay for proofreading. \nREFERENCES \n[1] Eda Kuhn Loeb Music Library. Music from the \narchive. Retrieved July 23, 2005 from \nhttp://hcl.harvard.edu/loebmusic/audioproject.htm\nl \n[2] Library of Congress. Digital audio-visual \npreservation prototyping project. Retrieved July \n23, 2005 from http://www.loc.gov/rr/mopic/ \navprot \n[3] University of California, Santa Barbara. Cylinder \npreservation and digitization pilot project. \nRetrieved July 23, 2005 from \nhttp://www.library.ucsb.edu/speccoll/pa/cylinders\n.html \n[4] Dunn, J. “Beyond Variations: Creating a digital \nmusic library,” Proceedings of the International \nConference on Music Information Retrieval, \n2000. \n[5] Association of Research Libraries. Sound \nsavings: Preserving audio collection. Retrieved \nJuly 23, 2005 from http://www.arl.org/preserv/ \nsound_savings_proceedings/diamant.html \n[6] Library and Archives of Canada. Digital audio at \nthe National Library of Canada. Retrieved July \n23, 2005 from http://www.collectionscanada.ca/ \n9/1/p1-248-e.html \n[7] Kenney, A., and Rieger, O. Moving theory into \npractice: Digital imaging for libraries and \narchives. Research Libraries Group, Mountain \nView, CA, 2000. \n[8] Sistrunk, W. “Dating LPs,” Music Reference \nServices Quarterly, 8, 4 (2004), 47–55 \n[9] Mudge, S., and Hoek, D. “Describing jazz, blues, \nand popular 78 rpm sound recordings: Guidelines \nand suggestions,” Cataloging & Classification \nQuarterly, 29, 3 (2000), 21–48. \n[10] Simpkins, T. “Cataloging popular music \nrecordings,” Cataloging and Classification \nQuarterly, 31, 2 (2001), 1–35. \n[11] Smiraglia, R. Describing music materials, 3d ed. \nSoldier Creek Press, Lake Crystal, MN, 1997.  [12] Hemmasi, H. “Why not MARC?” Proceedings of \nthe International Conference on Music \nInformation Retrieval, 2002. \n[13] Categories for the Description of Works of Art. \nRetrieved July 23, 2005 from \nhttp://www.getty.edu/research/conducting_resear\nch/standards/cdwa/  \n[14] Visual Resources Association Data Standards \nCommittee. VRA Core Categories, Version 3.0. \nRetrieved July 23, 2005 from \nhttp://www.vraweb.org/vracore3.htm \n[15] Harvard University Library. “Administrative \nmetadata for digital still images, version 1.3”, \nHarvard University Library Specification, 2004. \nRetrieved July 23, 2005 from \nhttp://preserve.harvard.edu/resources/imagemetad\nata.pdf \n[16] National Information Standards Organization. \n“Data dictionary: Technical metadata for digital \nstill images,” Working draft, 2002. Retrieved July \n23, 2005 from http://www.niso.org/ \npdfs/DataDict.pdf \n[17] Hurley, B., Price-Wilkin, J., Proffitt, M., and \nBesser, H. The making of America II testbed \nproject: A digital library service model. The \nDigital Library Federation, 1999 \n[18] Library of Congress. Table of core metadata for \nLC digital repository development, 2000. \nRetrieved July 23, 2005 from \nhttp://www.loc.gov/standards/metable.html \n[19] Koenen, R., and Pereira, F. “MPEG-7: A \nstandardised description of audiovisual content,” \nSignal Processing: Image Communication, 16, 1 \n(Sept. 2000), 5–13.  \n[20] California Digital Library. “Digital image format \nstandards,” CDL Reports & Guidelines, 2001. \n[21] Bradley, K. ed. Guidelines on the production and \npreservation of digital audio objects, Aarhus, \nDenmark: International Association of Sound and \nAudiovisual, 2004.  \n[22] Ester, M. Digital image collections: Issues and \npractices. Commission on preservation and \naccess, Washington DC, 1996. \n[23] Yorkston, S., and Quantum-X. “What does the \n‘Quality’ setting mean?” Retrieved July 23, 2005 \nfrom http://www.vorbis.com/faq.psp#quality \n[24] Droettboom, M., MacMillan, K., and Fujinaga, I. \n“The Gamera framework for building custom \nrecognition systems,” Proceedings of the \nSymposium on Document Image Understanding \nTechnologies, 2003. \n575"
    },
    {
        "title": "Musical Genre Classification Enhanced by Improved Source Separation Technique.",
        "author": [
            "Aristomenis S. Lampropoulos",
            "Paraskevi S. Lampropoulou",
            "George A. Tsihrintzis"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416292",
        "url": "https://doi.org/10.5281/zenodo.1416292",
        "ee": "https://zenodo.org/records/1416292/files/LampropoulosLT05.pdf",
        "abstract": "We present a system for musical genre classification based on audio features extracted from signals which correspond to distinct musical instrument sources. For the separation of the musical sources, we propose an innovative technique in which the convolutive sparse coding algorithm is applied to several portions of the audio signal. The system is evaluated and its performance is assessed. Keywords: Musical Genre Classification, Source Separation, Convolutive Sparse Coding. 1",
        "zenodo_id": 1416292,
        "dblp_key": "conf/ismir/LampropoulosLT05",
        "keywords": [
            "Musical genre classification",
            "Source separation",
            "Convolutive sparse coding",
            "Audio features",
            "Signal processing",
            "Music information retrieval",
            "Instrument source separation",
            "Audio signal analysis",
            "Sparse coding algorithm",
            "Performance evaluation"
        ],
        "content": "MUSICALGENRE CLASSIFICATIONENHANCED BY IMPROVED\nSOURCE SEPARATIONTECHNIQUES\nAristomenis S. Lampropoulos\nUniversityof Piraeus\nDepartment of Informatics\nPiraeus 185 34, Greece.\narislamp@unipi.grParaskeviS. Lampropoulou\nUniversityof Piraeus\nDepartmentof Informatics\nPiraeus 185 34, Greece.\nvlamp@unipi.grGeorgeA. Tsihrintzis\nUniversityof Piraeus\nDepartmentof Informatics\nPiraeus 185 34, Greece.\ngeoatsi@unipi.gr\nABSTRACT\nWepresentasystemformusicalgenreclassiﬁcationbased\nonaudiofeaturesextractedfromsignalswhichcorrespond\nto distinct musical instrument sources. For the separation\nof the musical sources, we propose an innovative tech-\nniqueinwhichtheconvolutivesparsecodingalgorithmis\nappliedtoseveralportionsoftheaudiosignal. Thesystem\nisevaluatedand its performance is assessed.\nKeywords: Musical Genre Classiﬁcation, Source Sepa-\nration,ConvolutiveSparse Coding.\n1 INTRODUCTIONAND WORK\nOVERVIEW\nRecent advances in digital storage technology and the\ntremendous increase in the availability of digital music\nﬁles have led to the creation of large music collections\nfor use by broad classes of computer users. In turn, this\nfact gives rise to a need for systems that have the abil-\nity to manage and organize efﬁciently large collections of\nstored music ﬁles. Many currently available music search\nenginesandpeer-to-peersystems(e.g. Kazaa,emule,Tor-\nrent) rely on textual meta-information such as ﬁle names\nand ID3 tags as the retrieval mechanism. This textual de-\nscription of audio information is subjective and does not\nmake use of the musical content and the relevant meta-\ndata have to be entered and updated manually, which im-\npliessigniﬁcanteffortinbothcreatingandmaintainingthe\nmusic database. Therefore, it is expected that extracting\ntheinformationfromtheactualmusicdatathroughanau-\ntomatedprocesscouldovercomesomeoftheseproblems.\nThere have been many works on audio content analy-\nsis which use various features and methods (Lampropou-\nlos et al., 2004c; Dowling and Harwood, 1996; Aucoutier\nand Pachet, 2003; Tzanetakis and Cook, 2002, 2000),\nmost of which focus on automatic musical genre classiﬁ-\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitationon the ﬁrst page.\nc°2005Queen Mary,Universityof Londoncation. Thesemethodsprovidetechniquestoorganizedig-\nitalmusicintocategoricallabelscreatedbyhumanexperts\nusing objective features of the audio signal that relate to\ninstrumentation, timbral texture, rhythmic and pitch con-\ntent (Aucoutier and Pachet, 2003; Tzanetakis and Cook,\n2000). These techniques rely on pattern recognition algo-\nrithms and offer possibilities for content-based indexing\nand retrieval. However, all these works use the complex\nsound structure of the audio signal in a music ﬁle to ex-\ntract the feature vector.\nIn this paper, we propose a new approach for musical\ngenre classiﬁcation based on the features extracted from\nsignals that correspond to musical instrument sources.\nContrary to previous works, our approach uses ﬁrst a\nsound source separation method to decompose the audio\nsignalintoanumberofcomponentsignals,eachofwhich\ncorrespondstoadifferentmusicalinstrumentsource,(see\nFigure 1). In this way timbral, rhythmic and pitch fea-\ntures are extracted from separated instrument sources and\nused to classify a music clip, detect its various musical\ninstruments sources and classify them into a musical dic-\ntionary of instrument sources or instrument teams. This\nprocedureattemptstomimicahumanlistenerwhoisable\nto determine the genre of a music signal and, at the same\ntime, identify a number of different musical instruments\nin a complexsound structure.\nThe problem of separating the component signals that\ncorrespond to the musical instruments that generated an\naudio signal is ill-deﬁned as there is no prior knowl-\nedge about the instrumental sources. Many techniques\nhave been successfully used to solve the general blind\nsource separation problem in several application areas;\namongthese,theIndependentComponentAnalysis(ICA)\nmethod (Plumbley et al., 2002; Martin, 1999) appears to\nbe one of the most promising. ICA assumes that the indi-\nvidual source components in an unknown mixture have\nthe property of mutual statistical independence. This\nproperty is exploited in order to algorithmically identify\nthe latent sources. Moreover, ICA-based methods require\ncertain limiting assumptions, such as the assumption that\nthenumberofobservedmixturesignalsbeatleastashigh\nasthenumberofsourcesignalsandthatthemixingmatrix\nbefullrank. However,amethodhasbeenproposedwhich\nis based on ICA but relaxes the constraint on the number\nof observed mixture signals. This is called the Indepen-\ndent Subspace Analysis (ISA) method and can separate\n576Figure 1: Source Separation: 2 component signals\nindividualsourcesfromasingle-channelmixturebyusing\nsound spectra (Casey and Westner, 2000). Signal inde-\npendenceisthemainassumptionofboththeICAandISA\nmethods. In musical signals, however, there exist depen-\ndencies in both the time and frequency domains.To over-\ncome these limitations, we use in our system a recently\nproposed data-adaptive algorithm that is similar to ICA\nand called Convolutive Sparse Coding (CSC) (Virtanen,\n2004).\nInourﬁrstapproach(Lampropoulosetal.,2005a),we\napplied the CSC algorithm in the entire input signal that\ncorresponds to a music piece assuming that the same mu-\nsical instruments were active throughout the entire music\npiece duration. This assumption, however, is not realistic,\nasitiscommonfordifferentinstrumentstobeusedtogen-\nerate different parts of a music piece. For example, only\ntwoinstrumentsmaybeactiveintheintroductionofamu-\nsic piece, a third instrument may be added in the middle\nof the piece and so on. Thus, in Section 2.1, we propose\na new approach for music source separation, in which we\napplythe CSC algorithm to three parts of a music piece.\nMore speciﬁcally, the paper is organized as follows:\nAn overall architecture of our system is presented in Sec-\ntion 2, with Section 2.1 describing the source separation\nmethodindetailandSection2.2describingtheextraction\nof audio content-based features of music pieces. Classi-\nﬁcation methods and results are given in Section 3, while\nconclusions and suggestions for future work are given in\nSection4.\n2 SYSTEM OVERVIEW\nThearchitectureofoursystemconsistsofthreemainmod-\nules, as in Figure 1. The ﬁrst module realizes the separa-\ntion of the component signals in the input signal, while\nthesecondmoduleextractsfeaturesfromeachsignalpro-\nduced during the source separation stage. Finally, the last\nmodule is a supervised classiﬁer of genre and musical in-\nstrument. Eachmusicpiececanbestoredinanyaudioﬁle\nformat,suchas.mp3,.au,or.wav.,whichrequirestheap-\nplicationofaformatnormalizationprocessbeforefeature\nextraction. For this, we decode each music ﬁle into raw\nPulse Code Modulation (PCM), using a LAME decoder\n(Lame) and convert it to the .wav format with resolution\nof16 bit samples at a sampling rate of 22.050 Hz.\nFigure 2: Partsof the Signal\n2.1 ImprovedSourceSeparation Technique\nFor source separation, we make the assumption that por-\ntions of the signal are sufﬁcient for reliable application of\ntheCSCalgorithm. Theﬁrststepofthesourceseparation\ntechnique is to identify the music piece portions. Speciﬁ-\ncally, we take three parts, one at the beginning, one at the\nmiddleandoneattheendofthemusicsignal,asshownin\nFigure 2. The length of each part is 25%of the length of\ntheentiresignal. WeapplytheCSCalgorithmtothethree\nsignal parts in parallel. We choose the method of convo-\nlutive sparse coding because it solves, at least partially,\ntheassumptionsofspectrathatremainﬁxedovertimeand\nthe model ﬁtting criterion of the reconstruction error, as-\nsumptionswhicharenotvalidforaudiosignals. Thebasic\nsignal model in general sparse coding is that each obser-\nvation vector xiis a linear mixture of source vectors sj\n:\nxi=JX\nj=1ai;jsj; i= 1; :::; I; (1)\nwhere ai;jis the weight of jthsource in the ithobserva-\ntion signal.\nBoth the source vectors and the weights are assumed\nunknown. The sources are obtained by multiplying the\nobservation matrix by the estimate of an unmixing ma-\ntrix. The main assumption in sparse coding techniques is\nthat the sources are non-active most of the time, which\nmeans that the mixing matrix has to be sparse. The esti-\nmation can be done using a cost function that minimizes\nthe reconstruction error and maximizes the sparseness of\nthemixingmatrix. Morespeciﬁcally,thismethodiscalled\n577convolutive sparsecodingbecausethesourcemodelisfor-\nmulatedastheconvolutionofasourcespectrogramandan\nonsetvector. The suitability of this model over-coversthe\ncaseof respectivetransient sources.\nThe commonly used model ﬁtting criterion that con-\nsists of the sum of squared elements of the reconstruction\nerroremphasizesmusicsignalaspectsthatdifferfromthe\nhumansoundperception. Inordertoobtainhigherpercep-\ntual quality of separated sources, the CSC algorithm uses\ncompression,as explainedin the followingSection 2.1.1.\n2.1.1 Loudness Criterion :\nThe human auditory system allows humans to perceive\nvery low-amplitude sounds. The large dynamic range of\nthe human auditory system is mainly caused by the non-\nlinear response of the auditory cells, which can be mod-\neled as a separate compression of the input signal at each\nauditory channel (Virtanen, 2004). In our system, the\ncompression is modeled by calculating a weight for each\nfrequency bin in each frame. The weights are selected\nso that the sum of squared magnitudes be equal to the\nestimated loudness, since the separation algorithm uses\nthe squared error criterion as a ﬁtting criterion. This way\n”quantitativesigniﬁcance”correspondsto”perceptualsig-\nniﬁcance.”\nSpeciﬁcally, in our system, 24 separate bands are\nspaced uniformly on Bark scale and denoted by disjoint\nsetsFb,b= 1:::24. The ﬁxed response of the outer and\nmiddle ear is taken into account by multiplying each bin\nofspectrumbythecorrespondingresponse. IntheCSCal-\ngorithm, the term loudness index is used for the loudness\nestimate in a frame within a critical band. The loudness\nindex in frame tin critical band bis denoted by Lb;tand\ngivenas\nLb;t= [X\nf²Fb(hbxf;t)2+\"2\nb]º¡\"2º\nb;(2)\nwhere hbistheﬁxedresponseoftheouterandmiddleear\nwithin band b,\"bis a ﬁxed scalar with value 0.23 and is\nthe (ﬁxed) threshold of hearing on band b. In practice,\n\"bis not known, but it can be estimated from the input\nsignal, e.g. by calculating the average level of the signal,\nand scaling down 30 dB. This procedure has resulted into\nthevalue \"b= 0:23.\n2.1.2 The iterativealgorithm :\nEach part of the input signal is represented with a mag-\nnitude spectrogram, which is calculated as follows: ﬁrst,\nthe time domain input signal is divided into frames and\nwindowed with a ﬁxed 40 ms Hamming window with\n50%overlap between frames. Next, each frame is trans-\nformed into the frequency domain by computing its dis-\ncrete Fourier transform (DFT) of length equal to the win-\ndow size. Only positive frequencies are retained and\nphases are discarded by keeping only the magnitude of\nthe DFT spectra. This results in a magnitude spectrogram\nxf;t,where fisadiscretefrequencyindexand tisaframe\nindex. Atwo-dimensionalmagnitudespectrogramisused\ntocharacterizeoneeventofasourceatdiscretefrequency\nf,tframes as the onset variesbetween 0andD.The magnitudes xf;tand weights wf;tare calculated.\nThe number of sources Nis predeﬁned. Nshould be\nequal to the number of clearly distinguishable instru-\nments. If the spectrum of one source varies signiﬁcantly,\nforexamplebecauseofaccentuation,onemayhavetouse\nmore than one component per source. The model consid-\ners the different fundamental frequencies of each instru-\nment as separate sources. Initialize a1:::anwith the abso-\nlute valuesof Gaussian noise.\nIteration:\n1.Update sf;tusing the multiplicativestep\nsfp+1g=sfpg:¤¡\nATWT\nfWfxf¢\n:=³\nATWT\nfWfAsfpg´\n(3)\nwherethe sfp+1gistheupdated sfpgforpthiteration\ngiventhe A; W f.\n2.Calculate ran=@ctot(¸)\n@an.\n3.Update anÃan¡¹·ran. Set the negative anele-\nmentstozero. ¹·isthestepsize,whichisadaptively\nset.\n4.Evaluatethe cost function.\n5.Repeat Steps 1-4 until the value of the cost function\nremains unchanged.\nIn the synthesis mode, the convolutions are evalu-\nated to get frame-wide magnitudes of each source. To\nget the complex spectrum, phases are obtained from the\nphase spectrogram of the original mixture signal. The\ntime-domainsignalisobtainedbyinversediscreteFourier\ntransform and overlap-add. This procedure has been\nfound to produce best quality. The use of the original\nphases allows the synthesis without abrupt changes in\nphase.\n3 FEATUREEXTRACTION\nWe transform an audio signal at a certain level of infor-\nmationgranularity. Informationgranulesrefertoacollec-\ntion of data that contain only essential information. Such\ngranulation allows more efﬁcient processing for extract-\ningfeaturesandcomputingnumericalrepresentationsthat\ncharacterize a music signal. As a result, the large amount\nof detailed information in a signal is reduced to a collec-\ntion of features. Each feature captures some aspects of\nsignal and gives the essential information of that. In our\nsystem,weuseda30-dimensionalobjectivefeaturevector\nwhich was originally proposed by Tzanetakis (Tzanetakis\nand Cook, 2002, 2000) and used in other works (Tzane-\ntakis, 2002; Lampropoulos et al., 2004c; Lampropoulos\nand Tsihrintzis, 2004a,b; Lampropoulos et al., 2005a,b;\nFoote, 1999; M. Welsh et al., 1999). For the extraction\nof the feature vector, we used MARSYAS, 0.1 a public\nsoftware framework for computer audition applications\n(Tzanetakis and Cook, 2000). The feature vector con-\nsists of three different types of features rhythm related\n(Beat), timbral texture (musical surface: STFT, MFCCs)\nand pitch content related.\n578Figure 3: Source Separation: 3 component signals\n3.1 RhythmicFeatures\nRhythmicfeaturescharacterizethevariationofmusicsig-\nnals over time and contain such information as regular-\nity of the tempo. The feature set for representing rhythm\nis based on detecting the most silent periodicities of the\nsignal. Rhythm is extracted from beat histograms, that\nis curves describing the beat strength as a function of\ntempo values and the complexity of the beat in the mu-\nsic. The regularity of the rhythm, the relation of the main\nbeat to subbeats and the relative strength of subbeats to\nthe main beat, are used as some of the features in musical\ngenre recognition systems. The Discrete Wavelet Trans-\nform(DWT)isusedtodividethesignalintooctavebands\nand, for each band, full-wave rectiﬁcation, low pass ﬁll-\ntering, downsampling and mean removal are performed\nin order to extract an envelope. The envelopes of each\nband are summed up and the autocorrelation is calculated\nto capture the periodicities in the signal’s envelope. The\ndominant peaks in the autocorrelation function are accu-\nmulatedoverthewholeaudiosignalintoabeathistogram.\n3.2 TimbralTextrure\nIn short time audio analysis, the signal is broken into\nsmall, possibly overlapping temporal segments each seg-\nment is processed separately. These segments are called\n”analysis windows” and need to be short enough for the\nfrequencycharacteristicsofthemagnitudespectrumtobe\nrelativelystable. Theterm”texturewindow”describesthe\nlongestwindowthatisnecessarytoidentifymusictexture.\nThe timbral texture features are based on the Short Time\nFourier Transform and calculated for every analysis win-\ndows. Means and standard deviations are calculated over\nthetexturewindow.\n3.3 Pitch Features\nThe pitch features describe melody and harmony infor-\nmation about a music signal. A pitch detection algorithmdecomposes the signal into two frequency bands and am-\nplitude envelops are extracted for each frequency band.\nApplyinghalf-wayrectiﬁcationandlow-passﬁlteringper-\nformstheenvelopeextraction. Theenvelopesaresummed\nand an enhanced autocorrelation function is computed so\nthat the effect of integer multiples of the peak of frequen-\nciestomultiplepitchdetectionbereduced. Thedominant\npeaksoftheautocorrelationfunctionareaccumulatedinto\npitch histograms and the pitch content features extracted\nfromthepitchhistograms. Thepitchcontentfeaturestyp-\nically include: the amplitudes and periods of maximum\npeaks in the histogram, pitch intervals between the two\nmost prominent peaks and the overall sums of the his-\ntograms.\n4 CLASSIFICATION\nIn order to evaluate our source separation-based music\ngenre classiﬁcation technique, we have tried different\nclassiﬁers contained in the machine learning tool called\nWEKA (WEKA), which we have connected to our sys-\ntem.\nInthiswork,weutilizegenreclassiﬁersbasedonmul-\ntilayer perceptrons. The input to the artiﬁcial neural net-\nworks is the feature vector corresponding to the compo-\nnent signals produced by source separation. Speciﬁcally,\nthe source separation process produced two or three com-\nponent signals (see Figures 1 and 3, respectively) which\ncorrespondtoinstrumentteamssuchasstrings(bouzouki,\nguitar,etc), winds (greek clarinet, ﬂute, bagpipes, etc)\nand percussion instruments (drums, tabor, etc). We con-\nstructedtwodifferentmultilayerperceptrons,inwhichthe\nartiﬁcialneuralnetworksconsistedoffour(4)andten(10)\nhidden layers of neurons, respectively. The number of\nneuronsintheoutputlayerisdeterminedbythenumberof\naudio classes we want to classify into (four in this work:\nrebetico, dimotiko, laiko, entechno). The networks were\ntrainedwiththeback-propagationalgorithmandtheirout-\nput estimates the degree of membership of the input fea-\n579ture vector in each of the four audio classes. Thus, the\nvalueateachoutputnecessarilyremainsbetween0and1.\nClassiﬁcationresults werecalculatedusing10-foldcross-\nvalidation evaluation, where the dataset to be evaluated\nwas randomly partitioned so that 90 %be used for train-\ning and 10 %be used for testing. This process was iter-\nated with different random partitions and the results were\naveraged. This ensured that the calculated accuracy was\nnot biased because of the particular partitioning of train-\ningandtesting. Thespeciﬁcdatasetweusedconsistedof\n1049 music pieces from 4 genres of greek songs, namely\nRebetico(396pieces),Dimotiko(106pieces),Laiko(414\npieces),and Entechno (133 pieces).\nTable 1: Correctly Classiﬁed Instances without/with\nSourceSeparation\nClassiﬁer\nw/out SS\nwithSS\nNearest-Neighbour Classiﬁer\n67.6\n68.2\nMLP 4 hidden layers\n73.2\n74.2\nMLP 10 hidden layers\n73.9\n75.1\nTable2: CorrectlyClassiﬁedInstanceswithCSCandwith\nimprovedCSC\nClassiﬁer\nCSC\nimpCSC\nNearest-NeighbourClassiﬁer\n68.2\n69.2\nMLP4 hidden layers\n74.2\n75.8\nMLP10 hidden layers\n75.1\n75.9\nAs seen in Table 1 , the classiﬁcation results after im-\nplementationofthesourceseparationtechniquepresented\nan improvement of 1%-2%. This was due to the fact\nthat the source separation technique revealed more infor-\nmationabouttimbraltexture,rhythmandpitch(harmony)\ncontent, not only for the signal as a whole, but for a num-\nber of the separated instrument team sources. Moreover,\nas seen in Table 2, after implementation of the improved\nsourceseparationtechniquewehada 0:5%-1%improve-\nment over a previous work of ours (Lampropoulos et al.,\n2005a), in which the CSC algorithm was applied to the\nentire signal. Thus, the total improvement in our present\napproach is about 2%-2:5%over previous genre classi-\nﬁcation methods. Finally, the present CSC approach not\nonlyresultsinbettergenreclassiﬁcation,butisfasterthan\nthe existing CSC algorithms, as it is applied in parallel to\nsmall portions of duration of 30 - 50 sec and not on an\nentireaudio signal of duration of 3 - 3.5 min.\nTable 3: Confusion Matrix: MLP 4 hidden layers w/out\nSS73:2%\nRebetico\nDimotiko\nLaiko\nEntechno\nR\n304\n9\n73\n10\nD\n20\n67\n18\n1\nL\n88\n6\n307\n13\nE\n17\n2\n24\n90 Table4: ConfusionMatrix: MLP4hiddenlayerswithSS\nCSC74:2%\nRebetico\nDimotiko\nLaiko\nEntechno\nR\n325\n8\n57\n6\nD\n32\n62\n10\n2\nL\n101\n9\n296\n8\nE\n25\n0\n13\n95\nTable5: ConfusionMatrix: MLP4hiddenlayerswithSS\nimprovedCSC 75:8%\nRebetico\nDimotiko\nLaiko\nEntechno\nR\n299\n8\n80\n9\nD\n22\n67\n15\n2\nL\n67\n7\n330\n10\nE\n19\n0\n15\n99\nTo analyze further a musical genre classiﬁer (e.g.,\nthe multilayer perceptron with 4 hidden layers), we also\npresentthecorresponding confusionmatrices ,asshownin\nTables 3 (without Source Separation, classiﬁcation accu-\nracyof 73:2%),4(withSourceSeparationbasedonCSC,\nclassiﬁcationaccuracyof 74:2%)and5(withSourceSep-\naration based on improved CSC, classiﬁcation accuracy\nof75:8%). Inaconfusionmatrix,thecolumnscorrespond\nto theactualgenre, while the rows correspond to the pre-\ndictedgenre. For example in Table 3, the cell in row 2 of\ncolumn 4 has value 1, which means that 1 song (in a to-\ntal of 106 songs) from the ”dimotiko” class was wrongly\npredicted as ”entechno”. Similarly, 20 and 18 songs from\nthe ”dimotiko” class were predicted to be from the ”re-\nbetiko” and ”laiko” classes, respectively. Therefore, the\npercentage of correct classiﬁcation of songs from the ”di-\nmotiko” class is computed to equal 67*100/106=63,2 %\nfor this classiﬁer. The correct classiﬁcation percentages\nare, therefore, derived from the entries in the diagonal el-\nementsofaconfusionmatrixandthecorrespondingactual\nnumber of songs in the library.\n5 CONCLUSIONS - FUTURE WORK\nIt has been observed that audio signals corresponding to\nmusic of the same genre share certain common charac-\nteristics as they are performed by similar types of in-\nstruments and have similar pitch distribution and rhyth-\nmic patterns (Aucoutier and Pachet, 2003). Motivated by\nthis,wepresentedanovelapproachbasedonclassiﬁcation\nof features extracted from component signals that corre-\nsponded to musical instrument teams (sources), as these\nsources have been identiﬁed by a source separation pro-\ncess. For source separation, we presented an improved\nalgorithmbasedonconvolutivesparsecoding. Evaluation\nof the performance of our method showed clear improve-\nment in classiﬁcation accuracy and execution speed over\nour previousmethods (Lampropoulos et al., 2005a).\nCurrently, we are in the process of improving further\nthe classiﬁcation efﬁciency of our system by considering\nadditional low-level music features, speciﬁcally MPEG-7\n580low-levelaudiodescriptors,incorporatingotherclassiﬁers\nsuch as immune classiﬁcation algorithms (Lampropou-\nlos et al., 2005b) and additional classiﬁers included in\nthe WEKA tool (WEKA). Another direction of our fu-\ntureworkwillbetheidentiﬁcationofspeciﬁcinstruments\nfrom the separated component sources. This and related\nwork is currently in progress and its results will be an-\nnouncedshortly.\nREFERENCES\nJ. J. Aucoutier and F. Pachet. Representing musical\ngenre: A state of the art. Journal of New Music Re-\nsearch, 32(1):83–93, 2003.\nM. Casey and A. Westner. Separation of mixed audio\nsources by independent subspace analysis. In Proc.\nInternational Computer Music Conference, ICMA ,\nBerlin,August 2000.\nW. J. Dowling and D. L. Harwood. Music Congition .\nAcademicPress, 1996.\nJ. Foote. An overview of audio information retrieval.\nMultimediaSystems , 7(1):2–10, 1999.\nLame. http://lame.sourceforge.net.\nA. S. Lampropoulos, P. S. Lampropoulou, and G. A.\nTsihrintzis. Musical genre classiﬁcation of audio data\nusing source separation techniques. In Proc. IEEE\n5th EURASIP Conference on Speech and Image Pro-\ncessing, Multimedia Communications and Services ,\nSmolenice,The SlovakRepublic, July 2005a.\nA. S. Lampropoulos, D. N. Sotiropoulos, and G. A.\nTsihrintzis. Artiﬁcial immune system-based music\npiecesimilaritymeasuresanddatabaseorganization. In\nProc. IEEE 5th EURASIP Conference on Speech and\nImage Processing, Multimedia Communications and\nServices,Smolenice,TheSlovakRepublic,July2005b.\nA. S. Lampropoulos, D. N. Sotiropoulos, and G. A.\nTsihrintzis. Individualization of music similarity per-\nception via feature subset selection. In Proc. IEEE\nInternational Conference on Systems, Man, and Cy-\nbernetics 2004 , The Hague, The Netherlands, October\n2004c.\nA. S. Lampropoulos and G. A. Tsihrintzis. Agglomera-\ntivehierarchicalclusteringformusicaldatabasevisual-\nizationandbrowsing. In Proc.3rdHellenicConference\non Artiﬁcial Intelligence , Samos Island, Greece, May\n2004a.\nA. S. LampropoulosandG. A. Tsihrintzis. Semantically\nmeaningful music retrieval with content-based features\nand fuzzy clustering. In Proc. 5th International Work-\nshoponImageAnalysisforMultimediaInteractiveSer-\nvices.,Lisboa, Portugal,April 2004b.\nN. M. Welsh, B. von Behren, and A. Woo. Querying\nlarge collections of music for similarity. Technical\nreport UCB/CSD00-1096, Computer Science Depart-\nmentU.C. Berkeley,1999.\nK. Martin. Sound-sourcerecognition: Atheoryandcom-\nputationalmode . PhD thesis, MIT,1999.M. D. Plumbley, S. A. Abdallah, J. P. Bello, M. E.\nDavies, G. Monti, and M. Sandler. Automatic music\ntranscription and audio source separation. Cybernetics\nand Systems , 33(6):603–627, 2002.\nG. Tzanetakis. Manipulation,AnalysisandRetrievalSys-\ntems for Audio Signals . PhD thesis, Princeton Univer-\nsity,2002.\nG. Tzanetakis and P. Cook. Marsyas: A framework for\naudio analysis. OrganisedSound ,4(3), 2000.\nG. Tzanetakis and P. Cook. Musical genre classiﬁcation\nofaudiosignals. IEEETransactionsonSpeechandAu-\ndio Processing , 10(5), July 2002.\nT. Virtanen. Separation of sound sources by convolutive\nsparse coding. In Proc. Tutorial and Research Work-\nshop on Statistical and Perceptual Audio Processing,\nSAPA, 2004.\nWEKA. http://www.cs.waikato.ac.nz/ml/weka.\n581"
    },
    {
        "title": "Efficient Extraction of Closed Motivic Patterns in Multi-Dimensional Symbolic Representations of Music.",
        "author": [
            "Olivier Lartillot"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418129",
        "url": "https://doi.org/10.5281/zenodo.1418129",
        "ee": "https://zenodo.org/records/1418129/files/Lartillot05.pdf",
        "abstract": "An efficient model for discovering repeated patterns in symbolic representations of music is presented. Combinatorial redundancy inherent in the pattern discovery paradigm is usually filtered using global selective mechanisms, based on pattern frequency and length. The proposed approach is founded instead on the concept of closed pattern, and insures lossless compression through an adaptive selection of most specific descriptions in the multi-dimensional parametric space. A notion of cyclic pattern is introduced, enabling the filtering of another form of combinatorial redundancy provoked by successive repetitions of patterns. The use of cyclic patterns implies a necessary chronological scanning of the piece, and the addition of mechanisms formalising particular Gestalt principles. This study shows therefore that automated analysis of music cannot rely on simple mathematical or statistical approaches, but requires instead a complex and detailed modelling of the cognitive system ruling the listening processes. The resulting algorithm is able to offer for the first time compact and relevant motivic analyses of monodies, and may therefore be applied to automated indexing of symbolic music databases. Numerous additional mechanisms need to be added in order to consider all aspects of music expression, including polyphony and complex motivic transformations. Keywords: Closed pattern discovery, Gallois connection, Formal Concept Analysis, cyclic pattern, cognitive modelling. 1",
        "zenodo_id": 1418129,
        "dblp_key": "conf/ismir/Lartillot05",
        "keywords": [
            "closed pattern",
            "cyclic pattern",
            "combinatorial redundancy",
            "pattern discovery",
            "motivic analysis",
            "multi-dimensional parametric space",
            "Gallois connection",
            "Formal Concept Analysis",
            "cognitive modelling",
            "automated indexing"
        ],
        "content": "EFFICIENT EXTRACTION OF CLOSED MOTIVIC PATTERNS IN\nMULTI-DIMENSIONAL SYMBOLIC REPRESENTATIONS OF MUSIC\nOlivier Lartillot\nUniversity of Jyv ¨askyl ¨a\nPL 35(A)\n40014 Jyv ¨askyl ¨a, Finland\nlartillo@campus.jyu.fi\nABSTRACT\nAn efﬁcient model for discovering repeated patterns in\nsymbolic representations of music is presented. Com-\nbinatorial redundancy inherent in the pattern discovery\nparadigm is usually ﬁltered using global selective mech-\nanisms, based on pattern frequency and length. The\nproposed approach is founded instead on the concept of\nclosed pattern, and insures lossless compression through\nan adaptive selection of most speciﬁc descriptions in the\nmulti-dimensional parametric space. A notion of cyclic\npattern is introduced, enabling the ﬁltering of another\nform of combinatorial redundancy provoked by successive\nrepetitions of patterns. The use of cyclic patterns implies a\nnecessary chronological scanning of the piece, and the ad-\ndition of mechanisms formalising particular Gestalt prin-\nciples. This study shows therefore that automated analysis\nof music cannot rely on simple mathematical or statistical\napproaches, but requires instead a complex and detailed\nmodelling of the cognitive system ruling the listening pro-\ncesses. The resulting algorithm is able to offer for the ﬁrst\ntime compact and relevant motivic analyses of monodies,\nand may therefore be applied to automated indexing of\nsymbolic music databases. Numerous additional mecha-\nnisms need to be added in order to consider all aspects of\nmusic expression, including polyphony and complex mo-\ntivic transformations.\nKeywords: Closed pattern discovery, Gallois connec-\ntion, Formal Concept Analysis, cyclic pattern, cognitive\nmodelling.\n1 INTRODUCTION\nThis paper is focused on automated description of sym-\nbolic music, and presents an efﬁcient algorithm for dis-\ncovering repeated patterns. Repeated patterns are struc-\ntures easily perceived by listeners, experienced or not, and\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonrepresent therefore one of the most salient characteristics\nofmusicalworks(Ruwet,1987;  LerdahlandJackend-\noff, 1983). The pattern discovery system described in this\npaper is applied uniquely to symbolic representation. A\ndirect analysis on the signal level would arouse tremen-\ndous difﬁculties. A pattern extraction task on the sym-\nbolic level, although theoretically simpler, remains ex-\ntremely difﬁcult to carry out, and its automation has not\nbeen achieved up to now. Indeed, computer researches on\nthis subject hardly offer results close to listeners’ or musi-\ncologists’ expectations. Hence the pattern discovery task\nis too complex to be undertaken directly at the audio sig-\nnal, and needs rather a prior transcription from the audio\nto the symbolic representations, in order to carry out the\nanalysis on a conceptual level.\nWe previously showed that the pattern discovery task\nleads to a problem of combinatorial redundancy, which\nneeds to be carefully controlled (Lartillot, 2004). We\nproposed therefore a heuristic based on maximally spe-\nciﬁc description of pattern classes and introduced a notion\nof implication relation between multi-dimensional pattern\ndescription. This new paper relates both heuristics to\nthe concept of closed pattern (Zaki, 2005), and to to the\nsubconcept-superconcept relation deﬁned in the Formal\nConcept Analysis (FCA) theory (Ganter and Wille, 1999),\nstemming from the Gallois connection between pattern\ndescription and pattern class. The second part of the pa-\nper introduces the concept of cyclic patterns and presents\nan extension of the subconcept-superconcept relation to\nthis new paradigm. This enables a simple modelling and\nefﬁcient control of the complex structural conﬁgurations\nfound in every musical piece, even simple ones. A less\nformalised description of the whole theory can be found\ninLartillot (2005).\n2 CLOSED PATTERN DISCOVERY\nThis section presents the basic problem of pattern discov-\nery and introduces the notion of closed pattern.\n2.1 Deﬁnitions\nLetS=< a 1a2. . . a N>be a sequence of elements of\nsome set ai∈A. Asubsequence Si,lof index i∈[1, N]\nand of length l∈[1, N+ 1−i]is the sequence :\nSi,l=< aiai+1. . . a i+l−1> . (1)\n191A sub-sequence Si,kisincluded in another sub-\nsequence Sj,l, noted Si,k⊂Sj,lwhen j/lessorequalslantiandi+k/lessorequalslant\nj+l. In a ﬁrst simple version, a pattern oflength l,\ndenoted P∈ P (S), can be deﬁned as a repeated sub-\nsequence:\nP∈ P(S)⇐⇒ ∃ (i, j)∈[1, N]2, P=Si,l=Sj,l.\n(2)\nThesupport of a pattern P, denoted σ(P), is the num-\nber of occurrences of the pattern, i.e.\nσ(P) =/vextendsingle/vextendsingle/braceleftbig\ni∈[1, N], Si,l=P/bracerightbig/vextendsingle/vextendsingle. (3)\n2.2 Redundancy Filtering\nThe task of discovering repeated patterns leads to combi-\nnatorial problems. Indeed each pattern of length l, con-\ntains/summationtextl\ni=1i=l(l−1)\n2=O(l2)sub-patterns. All these\nsub-patterns are explicitly discovered by any basic pattern\ndiscovery algorithm (Zaki, 2005). One common way to\nsolve this problem consists in focusing on the maximal\npatterns Pof the sequence S, denoted P∈ M (S), which\nare patterns of Snot included in any other pattern of S:\nP∈ M (S)⇐⇒/braceleftbigg\nP∈ P(S)\n∄Q∈ P(S), P⊂Q.(4)\nThis heuristic enables a signiﬁcant reduction of the\nnumber of discovered pattern, but leads also to a loss of in-\nformation. Indeed, not all the sub-patterns may be imme-\ndiately reconstructed knowing the maximal patterns. For\ninstance, the grey sub-pattern in ﬁgure 1 is redundant as\nit can be simply retrieved as a sufﬁx of the black pattern.\nHowever, in ﬁgure 2, the same grey sub-pattern is not re-\ndundant any more, because its support (4) is larger than\nthe support of the black pattern (2).\nFigure 1: The grey sub-pattern is not a closed pattern: it\nis a simple sufﬁx of the black pattern.\nFigure 2: The grey sub-pattern is now closed: its support\nis bigger than the support of the black pattern.\nA pattern Pwill be called closed , denoted P∈ C(S),\nif and only if there exists no proper super-pattern Qof\nsame support :\nP∈ C(S)⇐⇒\n\nP∈ P(S)\n∄Q∈ P(S),/braceleftbigg\nP⊂Q\nσ(P) =σ(Q).(5)\nThe set of closed patterns offers a compact and lossless\ndescription of the musical piece.2.3 An Incremental and Chronological Approach\nThis paper proposes an incremental andchronological ap-\nproach to closed pattern discovery. Section 4 justiﬁes this\nstrategy, by showing the necessity to model successive\nrepetitions of a same pattern as a traversal through one sin-\ngle cyclic pattern, and hence to consider a chronological\napproach of music. A detailed and illustrated description\nof the algorithm summarised below has been presented in\n(Lartillot, 2004).\n2.3.1 Incremental approach\nThe successive preﬁxes of each pattern are discovered pro-\ngressively: they are considered as successive intermediary\nstates of apattern chain (PC) whose ﬁnal state represents\nthe whole pattern. At each step of the progressive con-\nstruction, all the occurrences of the last discovered preﬁx\nare considered. Identical continuations form new exten-\nsions of the preﬁx, represented as children of the current\nstate. Since each state can accept several children, the set\nof all patterns form a tree, called pattern tree (PT). An\nexample of PT can be seen in ﬁgure 4, above the score.\nSimilarly, pattern occurrences are also represented as\nchains – called pattern occurrence chains (POCs) – whose\nsuccessive states represent the successive preﬁxes (see ﬁg-\nure 4, below the score). Each state of a POC is related to\nits corresponding PC. As each pattern occurrence can fea-\nture several different possible continuations, the set of all\npattern occurrences that are initiated by one note forms\na tree, called pattern occurrence tree (POT). The root of\neach POT is associated to the root of the PT (node ain\nﬁgure 4), which represents the simple concept of note, and\nis therefore called note pattern . Since all notes can poten-\ntially initiate a POT, they are all occurrences of the note\npattern.\nThe inclusion relation between patterns may be de-\ncomposed as a product of two sub-relations: preﬁx and\nsufﬁx relations. Any sub-pattern will then be considered\nas a preﬁx of a sufﬁx of a pattern. The closure of a pattern\nPmay then be assessed following these two relations.\n1. A pattern Pispreﬁx-closed if there does not exist\npattern Qof which Pis a preﬁx of same support.\nSince all the preﬁxes of patterns are displayed in PTs,\nnon preﬁx-closed patterns are not discarded. The ex-\nplicit representation of preﬁxes as intermediary states\nof pattern chains induces a mere linear complexity, as\neach pattern of length lis represented by lstates.\n2. The problem of close patterns selection will therefore\nbe studied along sufﬁx relations only. A pattern Pis\nsufﬁx-closed if there does not exist any pattern Qof\nwhich Pis a sufﬁx of same support.\n2.3.2 Chronological approach\nThe pattern discovery process is chronological: the main\nroutine of the algorithm consists in a single traversal of the\nsequence S, from the ﬁrst element aito the last element\naN. Each new element aiinduces an update of the whole\npattern tree.\nFor this purpose, hash-tables memorise all the possi-\nblecontinuations ofP, that is, all the possible elements\n192appearing just after each of its occurrence. If a previous\noccurrence of pattern Phas been continued by an element\nidentical to ai, then an extension P/primeof pattern Pmay\nbe discovered (if not already) and represented as one of\nits child. The sufﬁx-closed condition should however ap-\nply: there should not exist a pattern Q/primeof which P/primeis a\nsufﬁx of same support. During the chronological analy-\nsis, a pattern P/primethat was ﬁrst considered as non-closed\nmay become closed once discovering a new occurrence\nthat is not an occurrence of the super-pattern Q/prime(an ex-\nample will be shown in paragraph 3.3). Super-patterns Q/prime\ncan easily be retrieved, as extensions of the corresponding\nsuper-patterns QofPby the same element ai. Not all\nthe super-patterns Qneed to be considered, but only those\ncontaining an occurrence concluded by previous element\nai−1.\n3 MULTI-DIMENSIONAL CLOSED\nPATTERNS\nIn previous subsection, pattern was searched in sequences\nof elements S=< a1a2. . . a N>, a i∈A. Music, on the\ncontrary, is expressed in a multi-dimensional parametric\nspace.\n3.1 Multi-Parametric Description of Music\nThe model presented in this paper only analyses monodic\nsequences, i.e. successions of notes without superposi-\ntion. Any monodic sequence can therefore be represented\nas previously :\nS=< n 1n2. . . n N>, n i∈ N, (6)\nwhere Nis the parametric space of notes. In the model,\nthis note space is simply reduced to :\nN=diat×chro×rhyt (7)\nwhere\n•diat, or diatonic pitch space , represents pitches as\npositions in the implicit tonal scale. Diatonic trans-\npositions will be detected in this space.\n•chro, orchromatic pitch space , represents pitches as\npositions on the piano keyboard. Following the MIDI\nstandard, with middle Cis associated the value 60.\n•rhyt, ormetrical space , represents temporal positions\nin term of distance from the beginning of the musical\nsequence. The rhythmic unit of the metrical space is\ngiven by the time signature.\nThe pattern discovery task cannot be directly applied\nto this note sequence S, because each successive note\nis related to a distinct metrical position and is therefore\ndistinct. Even if the metrical space is discarded, neither\nrhythmic patterns nor transposed melodic patterns may be\ndiscovered. Following common practice, the musical se-\nquences will be modelled as a succession of intervals be-\ntween successive notes :\nS= (− − →n1n2/follows− − →n2n3/follows. . ./follows− − − − − →nN−1nN). (8)An interval− − − − →nini+1∈− →Nis a vector between two\npoints of the note space N\nni= ( diat =di,chro =ci,rhyt =ti)\nni+1 = ( diat =di+1,chro =ci+1,rhyt =ti+1)(9)\nand can therefore be described by the three coordinates :\n− − − − →nini+1=\ndiat(− − − − →nini+1)\nchro (− − − − →nini+1)\nrhyt(− − − − →nini+1)\n=\ndi+1−di\nci+1−ci\nti+1−ti\n.\n(10)\ndiat: -1+20-3+1-1+2 0 +1\nchro: -1+30-5+2-2+3 0 +1\nrhyt:\npattern occurrences:\npattern abcde :1.5 1211.5.512 .5\ndiat: -1+20 +1\nrhyt:1.5 12 .5abcdeabcde\nabcde\nFigure 3: Multi-dimensional description of a musical\nsequence, which contains two occurrences of a pattern\nabcde .\n3.2 Formal Context Representation of Patterns\nWe will represent musical patterns with the help of a con-\nceptual framework that deﬁnes objects associated with\ndifferent kinds of attributes (Ganter and Wille, 1999).\nThese attributes consist not only of the different musi-\ncal dimensions, but also of the different sub-patterns and\nsuper-patterns. Following the incremental and chronolog-\nical approach explained in paragraph 2.3, we can restrict\nour study of the inclusion relations between patterns to\nsufﬁx relations. In this respect the objects of the pattern\ndescriptions are the successive notes of the musical se-\nquence forming the set N(S). Each note ni∈ N (S)\nrelates to a speciﬁc temporal context , deﬁned by the part\nof the musical sequence concluded by this note ni.\nEach note niis described ﬁrstly by the different musi-\ncal characteristics of the preceding interval:− − − − →ni−1ni.\nD0,p\ndesc(ni) : desc (− − − − →ni−1ni) =p,\ndesc∈ {diat,chro,rhyt}, p∈desc.(11)\nEach note niis also described by the musical characteris-\ntics of the previous intervals:\nDj,p\ndesc(ni) : desc (− − − − − − − →ni−j−1ni−j) =p,\ndesc∈ {diat,chro,rhyt}, p∈desc.(12)\nThen the pattern description of the sequence Smay be\nexpressed as a formal context (N(S),D, I)where :\n•the set of objects isN(S): the set of notes in S,\n•the set of attributes isD: the set of elementary musi-\ncal descriptions deﬁned by equations 11 and 12,\n193•andIis the binary relation between N(S)andD,\ncalled incidence , deﬁned by:\n(ni, δ)∈I⇐⇒ δ(ni)is true . (13)\nThe derived description C/primeof a set of notes C⊂\nN(S)is deﬁned as the common description of all these\nnotes:\nC/prime=/braceleftbig\nδ∈ D | ∀ n∈A,(n, δ)∈I/bracerightbig\n. (14)\nThe notes in Care therefore occurrences of a same pat-\ntern, which is maximally described by C/prime.\nThederived class D/primeof a complex description D⊂ D\nis dually deﬁned as the set of notes complying with this\ndescription:\nD/prime=/braceleftbig\nn∈ N (S)| ∀δ∈D,(n, δ)∈I/bracerightbig\n. (15)\nThe pattern discovery task consists in ﬁnding exhaustive\nclass D/primesharing a same description D. The trouble is, lots\nof different descriptions Dimay lead to same classes D/prime\ni.\n3.3 Formal Concept Representation of Patterns\nThederivators operations deﬁned by equation 14 and 15\nestablish a Gallois connection between the power set lat-\ntices on N(S)andD. The Gallois connection leads to a\ndual isomorphism between two closure systems, whose\nelements, called formal concepts of the formal context\n(S(S),D, I)corresponds exactly to the close patterns\nP= (C, D ), verifying:\nC⊂ N (S), D⊂ D, C/prime=D, and D/prime=C. (16)\nFor a close pattern P= (C, D ),Cis called the extent of\nDandDtheintent ofC. We may simply call CandD\nrespectively the class and the description ofP.\nHence, for a set of patterns Pi= (D/prime\ni, Di)of same\nclass D/prime\ni=C, the close pattern P= (C, D )is described\nusing the derived operator C/primedeﬁned in equation 14: it\ncontains all the elementary descriptions common to all\nnotes of the class C. In other words, closed patterns are\ndescribed as precisely as possible. However, as listeners\ntend to perceive only repetition of connex sub-sequences,\nonly the descriptions of the longest set of contiguous in-\ntervals (Dj/follows Dj−1/follows. . ./follows D0)leading to the context\nnote should be selected. Older descriptions Dj+kshould\nbe discarded if there is no description Dj+1associated to\nstepj+ 1. We have proposed a further continuity con-\nstraint, that seems to correspond more deeply to listeners\nperception, stating that contiguous intervals should be de-\nscribed by same dimensions:\n∀i∈[1, j],∃desc∈ {diat,chro,rhyt},\n∃(p, p/prime)∈desc2,Dj,p\ndescandDj−1,p/prime\ndesc(17)\nClosed patterns, as formal concepts, are naturally or-\ndered by the subconcept-superconcept relation deﬁned by\n(C1, D1)<(C2, D2)⇐⇒ C1⊂C2(⇐⇒ D2⊂D1).\n(18)\n(C1, D1)may therefore be considered as more speciﬁc\nthan (C2, D2).The incremental and chronological pattern discovery\nmethodology presented in section 2.3 may be generalised\nusing this multi-parametric deﬁnition of closed pattern.\nFor instance, pattern a/followsb/followsc/followsd/followse(more simply\ndenoted e), in ﬁgure 4, features melodic and rhythmical\ndescriptions:\n/parenleftBigdiat = 0\nrhyt =.5/followsdiat = 0\nrhyt =.5/followsdiat =−2\nrhyt =.5/followsrhyt = 4/parenrightBig\nwhereas pattern a/followsf/followsg/followsh/followsi(ori) only features\nits rhythmic part:\n(rhyt =.5/followsrhyt =.5/followsrhyt =.5/followsrhyt = 4).\nHence pattern eis more speciﬁc than pattern i. When\nonly the two ﬁrst occurrences are analysed, as both pat-\nterns have same support, only the more speciﬁc pattern e\nshould be explicitly represented. But the less speciﬁc pat-\nterniwill be represented once the last occurrence is dis-\ncovered, since it is not an occurrence of the more speciﬁc\npattern e.\nabc d e abcd e\nf\ng\nh\ni\nf\ng\nh\niabcg\ndh f\na\nfgh i0\n.5diat:\nrhyt:.5.5.5i\n4\n.5.50-2e4more specific than\nFigure 4: The rhythmic pattern afghi is less speciﬁc than\nthe melodico-rhythmic pattern abcde .\n3.4 Optimal score description\nIn order to reduce the space complexity of the pattern rep-\nresentation, and also to simplify as much as possible the\npattern description of the musical score, to each closed\npattern P= (C, D )will be associated a speciﬁc class\nSC(P)which consists of the set of occurrences that are\nnot included into classes of more speciﬁc patterns:\nSC(P) =C\\/uniondisplay\n(C/prime,D/prime)<(C,D)C/prime. (19)\nReversely, the general pattern class can be retrieved by\nunifying the speciﬁc class with the union of the speciﬁc\nclasses of all the more speciﬁc patterns :\nC=SC(P)∪/uniondisplay\nP/prime<PSC(P/prime). (20)\nDuring the chronological analysis of the musical\nscore, only the speciﬁc classes are constructed. But each\ntime a speciﬁc pattern occurrence is discovered, all the\nless speciﬁc patterns need to be recalled by the algorithm,\nbecause their extensions may lead to the discovery of new\nspeciﬁc patterns. For instance, in ﬁgure 5, groups 1 and 3\nare occurrences of pattern h, and groups 3 and 4 are occur-\nrences of pattern d. Since pattern dis more speciﬁc, the\nless speciﬁc pattern hdoes not need to be associated with\n194group 4 (that is why it is represented in grey). However\nin order to detect groups 2 and 5 as occurrences of pattern\nl, it is necessary to implicitly consider group 4 as an oc-\ncurrence of pattern h. Hence, even if pattern h, since less\nspeciﬁc than d, was not explicitly associated with group\n4, it had to be considered implicitly in order to construct\npattern l. Implicit information is reconstituted through\na traversal of the pattern network along the subconcept-\nsuperconcept relations.\nabcde abcde\nfghiabcg\ndhfj\n0\n.5diat:\nrhyt:.5.5.5i\n441kl\n1\n.5.50-2+1-2\ne4\na\nfg\nklhi\nj kljfghi1 3 42 5\nFigure 5: Group 4 can be simply considered as occurrence\nof pattern d. However, in order to detect group 5 as occur-\nrence of pattern l, it is necessary to implicitly infer group\n4 as occurrence of pattern htoo.\n3.5 Generalization of patterns\nNew patterns can be discovered through a simple general-\nisation of already known patterns. In bar 7 of ﬁgure 6, the\ntwo ﬁrst notes form an occurrence of pattern hdescribed\nbydiat = +1 and rhyt = 1. The third note cannot how-\never be associated with the known extension of pattern h\ninto pattern iwith description diat = 0and rhyt = 2, be-\ncause the melodic description diat = 0 does not match\nhere. However, as the rhythmic description rhyt = 2\nmatches, a new extension jis discovered as a generali-\nsation of pattern i.\nabc\nde\nabc\ndedeabc\nfg fghiabcabcabc\nhjabkabk\n1\n+2,10,2k2j2\n0,2\nfg-3,10,2hi\n+1,1(diat,rhyt)\n0,2hiabc7 6 8\nspecification\ngeneralizationgeneralization\nFigure 6: Progressive discovery of the pattern repetitions\non the score and the resulting pattern tree (below the\nscore). Pattern descriptions in grey are less speciﬁc than\nsimultaneous descriptions in black.\nThe less speciﬁc patterns, although usually not explic-\nitly represented in the analysis, should be updated if nec-\nessary. In particular, when a generalisation of a pattern is\ndiscovered, the generalisation of all its more general pat-\nterns should also be considered. For instance, as ihas\nbeen generalised into j,cshould be generalised into kinthe same way. In this way, the analysis of the next bar (8)\nconsists simply in recognising the already known general\npattern k.\n4 CYCLIC PATTERNS\nIn this section, we present another important factor of re-\ndundancy that, contrary to closed patterns, has not been\nstudied in current general algorithmic researches.\n4.1 Redundancy Due to Successive Repetitions\nCombinatory explosion can be caused by successive repe-\ntitions of a same pattern (for instance the pattern a/followsb/follows\ncin ﬁgure 7, of description (rhyt = 1/followsrhyt = 2) ). As\neach occurrence is followed by the beginning of a new oc-\ncurrence, each pattern can be extended – leading to pattern\ndof description (rhyt = 1/followsrhyt = 2/followsrhyt = 1) – by a\nnew interval whose description (rhyt = 1) is identical to\nthe description of the ﬁrst interval of the same pattern (i.e.,\npattern b). This extension can be prolonged recursively\n(into e,f, etc.), leading to a combinatorial explosion of\npatterns that are not perceived due to their complex inter-\ntwining.\ndefgdefg\nabcd\nab abc\nabcabc\nfg\ndede\nabc1rhyt:21212\nabc\nFigure 7: Multiple successive repetitions of pattern abc\nform a complex intertwining of non-perceived structures.\n4.2 Cyclic Patterns\nThe graph representation (ﬁgure 7) shows that the last\nstate of each occurrence of pattern cis superimposed on\nthe ﬁrst state of the following occurrence. Listeners seem\nto tend to fusion these two states, and to perceive a loop\nfrom the last state ( c) to the ﬁrst state ( a) (ﬁgure 8). The\ninitial acyclic pattern a/followsb/followscleads therefore to a cyclic\npattern that oscillates between two states b/prime/clockwisec/prime, corre-\nsponding to rhythmic values 1 and 2. Indeed, when lis-\ntening to the remainder of the rhythmic sequence, we ac-\ntually perceive this progressive oscillation between these\ntwo states b/primeandc/prime. Hence this cycle-based modelling ex-\nplains a common listening strategy, and resolves the prob-\nlem of combinatorial redundancy.\nb’c’b’c’11 rhyt:2\n2\n1abcb’c’\nabc b’c’\nFigure 8: The successive repetitions of pattern a/followsb/followsc\nlead to an oscillation between states b/prime/clockwisec/prime.\n195This cyclic PC b/prime/clockwisec/primeis considered as a continuation\nof the original acyclic PC a/followsb/followsc(ﬁgure 8) . Indeed,\nthe ﬁrst repetition of the rhythmic period is not perceived\nas a period as such but rather as a simple pattern: its suc-\ncessive notes are simply linked to the progressive states a,\nbandcof the acyclic PC. On the contrary, the following\nnotes extends the POC, which cannot therefore be associ-\nated to the acyclic PC anymore, and are therefore linked to\nthe successive states of the PC ( b/primeandc/prime). The whole peri-\nodic sequence constitutes then a single POC representing\nthe traversal of the acyclic PC followed by the cyclic PC.\nIt can be remarked also that, by property of the cyclic\nPC, no segmentation is explicitly represented between\nsuccessive repetitions. The periodic sequence in ﬁgure\n8 can be considered as a succession of periods quaver-\ncrochet, or on the contrary crochet-quaver. Listeners may\nbe inclined to segment at any phase of the cyclic PC, or\nnot to segment at all.\nThis additional concept immediately solves the redun-\ndancy problem. Indeed, the unique POC that is progres-\nsively extended is more speciﬁc than its sufﬁxes, which\ncannot therefore be extended any more.\nThis phenomenon of successive repetition, although\nvery frequent in musical expression, has been rarely stud-\nied. Cambouropoulos (1998) proposed to control the com-\nbinatorial explosion by selecting, once the analyses com-\npleted, patterns featuring minimal temporal overlapping\nbetween occurrences. The trouble is, as the selection\nis inferred globally, relevant patterns may be discarded.\nBesides combinatorial redundancy remains problematic\nsince the ﬁltering is carried out after the actual analysis\nphase. Our focus on local conﬁgurations enables a more\nprecise ﬁltering.\n4.3 General and Speciﬁc Cycles\nThe application of this concept on the multidimensional\nmusical space requires a generalisation of speciﬁcity re-\nlations, deﬁned in previous section, to cyclic patterns. A\ncyclic pattern Cis considered as more speciﬁc than an-\nother cyclic pattern Dwhen the sequence of description\nof pattern Dis included in the sequence of description of\npattern C.\nIn Figure 9, the seven ﬁrst notes of the cycle oscillate\naround the cyclic PC b/prime/clockwisec/primedescribed by:\nrhyt = 1/clockwiserhyt = 2\ndiat = 0.\nThen appears a more speciﬁc cycle d/prime/clockwisee/primedescribed by\nrhyt = 1\ndiat = +1/clockwiserhyt = 2\ndiat = 0\nand is generalised after four notes to cycle d/prime/prime/clockwisef/primethat\ndoes not feature the unison interval any more:\nrhyt = 1\ndiat = +1/clockwiserhyt = 2.\nMoreover, following the rule of generalisation of gener-\nalised patterns explained in paragraph 3.5, the more gen-\neral cycle b/prime−c/primetoo needs to be generalised into a cycle\na\nabcb’c’\nb0,2\n0,2\n1\n11\n1 1cb’c’\ng\n+1,1+1,1\n+1,1(diat,rhyt)\n2\nd0,2e\nf22b’c’b’\ngb’’g’b’’g’b’’g’f’aded’\nfd’’\n0,2\nd’e’\n+1,1\n+1,1d’’f’2\nb’’g’specification\ngeneralization generalizationFigure 9: More detailed analysis of the perceived cyclic\nconﬁgurations.\nb/prime/prime−g/primewhere the unison interval has been discarded:\nrhyt = 1/clockwiserhyt = 2.\nThese different cycles seem to be perceptible by listen-\ners. Moreover, the integration of this phenomenon into\nthe model helps insuring the relevance of the results\nand avoiding numerous unwanted combinatorial redun-\ndancies.\n4.4 The Figure/Ground Rule\nAnother kind of redundancy appears when occurrences of\na pattern – such as the melodico-rhythmic pattern cin ﬁg-\nure 10, described by diat =−2and rhyt = 1 – are su-\nperposed to a cyclic pattern ( b/prime), such that the pattern ( c)\nis more speciﬁc than the cycle period ( b/prime, simply rhyth-\nmic: rhyt = 1). In this case, the intervals that follow\nthese occurrences are identical, since they are related to\nthe same state ( b/prime) of the cyclic pattern. Logically the\npattern could be extended following the successive exten-\nsions of the cyclic patterns (leading to patterns d,e, etc.).\nThis phenomenon, which may frequently appear in a mu-\nsical piece, would lead to another combinatorial prolif-\neration of redundant structures if not correctly controlled\nby relevant mechanisms. On the contrary, following the\nGestalt Figure/Ground rule, listeners tend to perceive the\npattern cas a speciﬁc ﬁgure that emerges above the pe-\nriodic background. Following this rule, the ﬁgure cannot\nbe extended (into d) by a description that can be simply\nidentiﬁed with the background extension.\n5 RESULTS\nThis model was ﬁrst developed as a library of OpenMusic ,\ncalled OMkanthus . A new version will be included in the\nnext version 2.0 of MIDItoolbox (Eerola and Toiviainen,\n2004), a Matlab toolbox dedicated to music analysis. The\nmodel can analyse monodic musical pieces (i.e., consti-\ntuted by a series of non-superposed notes) and highlight\nthe discovered patterns on a score.\n196Treble\n-2\n 11\na\nabb’b’b’b’b’b’b’b’b’b’b’b’c\nd\ne ac\nd\ne1 1-2\n 11\n11\nabb’1\n1\n-2,1c 11111111111\n1\nd\ne1\n1(diat, rhyt)Figure 10: Pattern cis a speciﬁc ﬁgure, above a back-\nground generated by the cyclic pattern b/prime.\n5.1 Experiments\nThe model has been tested with different musical se-\nquences taken from several musical genres (classical mu-\nsic, pop, jazz, etc.). Table 1 shows some results. The\nexperiment has been undertaken with version 0.6.8 of\nOMkanthus on a 1-GHz PowerMac G4. A musicologist\nexpert has validated the analyses. The proportion of pat-\nterns considered as relevant is displayed in the table.\nThe analysis of a medieval song called Geisslerlied\n(Ruwet, 1987) – sometimes used as a reference test for\nformalised motivic analysis – gave quite relevant results.\nThe analysis has been actually carried out on a slight sim-\npliﬁcation of the actual piece presented by Ruwet, exclud-\ning local motivic variations out of reach of the current\nmodelling.\nThe melodico-rhythmic analysis of the French song\nAu clair de la lune posed problems: 21 patterns were dis-\ncovered from a 44-note long sequence. This is due to the\nfact that the successive steps of progressive generalisation\nor speciﬁcation of cycles are currently modelled using dis-\ntinct intermediary cyclic patterns. The inference of these\nredundant cyclic patterns will be avoided in further works.\nThe algorithm has been successfully applied on a\nmelodic analysis of a complete two-voice Invention by\nJ.S. Bach. Figure 11 shows the analysis of the 21 ﬁrst bars.\nThe repetition of ascending quarter notes in bars 3 and 4\nhas not been detected because the contour dimension was\nnot considered in the experiment. The cyclic patterns are\nrepresented by graduated lines, the graduation represent-\ning each return of one possible phase. As explained in\nsection 4.2, no preference is given by the model between\ndifferent possible phases of the same cycle. The rhyth-\nmic analysis of the piece, on the contrary, failed, due to\nthe alternation of sequences of either quarter notes or 8th\nnotes, which will require a formalisation through hierar-\nchical pattern chains (where successive states of higher-\nlevel patterns are linked to distinct lower-level patterns).\nThe analysis of The Beatles’ Obla Di Obla Da melody\nshows 14 relevant pattern classes, representing the cho-\nrus, verses, phrases and motives inside each of these struc-\ntures. The 4 irrelevant patterns are redundant patterns sub-\nsumed by the 14 relevant ones.\nIn all these pieces, some patterns are considered as ir-\nrelevant because they cannot be perceived as such by lis-\nteners. Additional mechanisms should be added to prevent\nthese irrelevant inferences, based on short-term memory,\ntop-down mechanisms, etc.5.2 About Algorithm Complexity\nThe algorithm complexity may be expressed ﬁrst in terms\nof discovered structures: proliferation of redundant pat-\nterns, for instance, would lead to combinatorial explosion,\nsince each new structure needs proper processes assessing\nits interrelationships with other structures, and inferring\npossible extensions. Hence a maximally compact descrip-\ntion insures in the same time the clarity and relevance\nof the results and the limitation of combinatorial explo-\nsion. Concerning technical implementation, the prototype\nneeds further optimisations. Yet the modelling has been\nconceived with a view to minimising computational costs.\nHence the identiﬁcation of similar descriptions is based on\nhash tables, as explained in paragraph 2.3.2, which reduce\ntime complexity.\nThe overall computational modelling results in a com-\nplex system formed by a large number of highly depen-\ndent mechanisms. Without a real synthetic vision of the\nwhole system, no general assessment of the global com-\nplexity of the modelling has been achieved yet. The com-\nplete rebuilding of the modelling currently undertaken\nshould enable a better awareness and control of complex-\nity.\n6 CURRENT RESEARCHES\nThe structures currently found are based solely on pattern\nrepetitions. Segmentation rules based on Gestalt princi-\nples of proximity and similarity (Lerdahl and Jackendoff,\n1983) (Cambouropoulos, 1998) need to be added. Al-\nthough this rule plays a signiﬁcant role in the percep-\ntion of large-scale musical structures, there is no com-\nmon agreement on its application to detailed structure,\nbecause it highly depends on the subjective choice of\nmusical parameters used for the segmentations (Deli `ege,\n1987). The study will focus in particular on the compet-\nitive/collaborative interrelations between the two mecha-\nnisms, in particular the masking effect of local disjunction\non pattern discovery.\nMusical transformations should be considered, such as\nlocal insertion or deletion of notes. Solutions have been\nproposed (Rolland, 1999) based on optimal alignments\nbetween approximate repetitions using dynamic program-\nming and edit distances. We are developing algorithms\nthat automatically discover, from the rough surface level\nof musical sequences, musical transformations revealing\nthe sequence of pivotal notes forming the deep structure\nof these sequences. These mechanisms induce new con-\nnections between non-successive notes, transforming the\nsyntagmatic chain of the original musical sequence into\na complex syntagmatic graph . The direct application of\nthe pattern discovery algorithm on this syntagmatic graph\nwill enable the detection of approximate repetitions and\nrequire the introduction of further perceptual heuristics in\norder to select precisely the relevant results.\nOur approach is limited to the detection of repeated\nmonodic patterns. Music in general is polyphonic, where\nsimultaneous notes form chords and parallel voices. Re-\nsearches have been carried out in this domain (Meredith\net al., 2002), focused on the discovery of exact repetitions\nalong different separate dimensions. Our model will be\n197Table 1: Results of analyses, either melodic (M) or melodico-rythmic (M+R), performed by OMkanthus 0.6.8.\nMusical sequence Analysis Pattern classes Computation\nName Notes type Discovered Relevant Success time\nGeisslerlied , slightly simpliﬁed 108 M 6 5 83% 2.2 sec.\nAu clair de la lune (folk song) 44 M+R 21 5 24% 5.6 sec.\nBach, Invention in D minor , BWV 775 283 M 49 34 69% 37.6 sec.\nMozart, Sonata in A , K331 36 M+R 14 10 71% 0.8 sec.\nﬁrst theme, ﬁrst half, melody\nThe Beatles, Obla Di Obla Da 390 M 14 10 71% 28.1 sec.\nFigure 11: Automated motivic analysis of J.S. Bach’s Invention in D minor BWV 775, 21 ﬁrst bars. The occurrences of\neach pattern class are designated in a distinct way.\ngeneralised to polyphony following the syntagmatic graph\nprinciple. We are developing algorithms that construct,\nfrom polyphonies, syntagmatic chains representing dis-\ntinct monodic streams. These chains may be intertwined,\nforming complex graphs along which the pattern discov-\nery algorithm will be applied. Pattern of chords may also\nbe considered in future works.\nThe automated discovery of repeated patterns can be\napplied to automated indexing of musical content in sym-\nbolic music databases. This approach may be generalised\nlater to audio databases, once robust and general tools for\nautomated transcription of musical sound into symbolic\nscores will be available. A new kind of similarity distance\nbetween musical pieces may be deﬁned, based on these\npattern descriptions, offering new ways of browsing inside\na music database using pattern-based similarity distance.\nACKNOWLEDGEMENTS\nI would like to thank Mich `ele Sebag, Marc Damez and the\nreviewers for their invaluable advice.\nREFERENCES\nE. Cambouropoulos. Towards a General Computational\nTheory of Musical Structure . PhD thesis, University of\nEdinburgh, 1998.\nI. Deli `ege. Grouping conditions in listening to music: An\napproach to lerdahl and jackendoffs grouping prefer-\nence rules. Music Perception , 4(4):325–350, 1987.\nT. Eerola and P. Toiviainen. Mir in matlab: The midi tool-box. In Proceedings of the International Conference on\nMusic Information Retrieval , 2004.\nB. Ganter and R. Wille. Formal Concept Analysis: Math-\nematical Foundations . Springer-Verlag, 1999.\nO. Lartillot. A multi-parametric and redundancy-ﬁltering\napproach to pattern identiﬁcation. In Proceedings of\nthe International Conference on Music Information Re-\ntrieval , 2004.\nO. Lartillot. Efﬁcient extraction of closed motivic pat-\nterns in multi-dimensional symbolic representations of\nmusic. In Proceedings of the IEEE/WIC/ACM Interna-\ntional Conference on Web Intelligence . IEEE Computer\nSociety Press, 2005.\nF. Lerdahl and R. Jackendoff. A Generative Theory of\nTonal Music . The MIT. Press, 1983.\nD. Meredith, K. Lemstr ¨om, and G. Wiggins. Algorithms\nfor discovering repeated patterns in multidimensional\nrepresentations of polyphonic music. Journal of New\nMusic Research , 31(4):321–345, 2002.\nP.-Y. Rolland. Discovering patterns in musical sequences.\nJournal of New Music Research , 28:334–350, 1999.\nN. Ruwet. Methods of analysis in musicology. Music\nAnalysis , 6(1-2):4–39, 1987.\nM. Zaki. Efﬁcient algorithms for mining closed item-\nsets and their lattice structure. IEEE Transactions\non Knowledge and Data Engineering , 17(4):462–478,\n2005.\n198"
    },
    {
        "title": "Challenges in Cross-Cultural/Multilingual Music Information Seeking.",
        "author": [
            "Jin Ha Lee 0001",
            "J. Stephen Downie",
            "Sally Jo Cunningham"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416706",
        "url": "https://doi.org/10.5281/zenodo.1416706",
        "ee": "https://zenodo.org/records/1416706/files/LeeDC05.pdf",
        "abstract": "Understanding and meeting the needs of a broad range of music users across different cultures and languages are central in designing a global music digital library. This exploratory study examines cross-cultural/multilingual music information seeking behaviors and reveals some important characteristics of these behaviors by analyzing 107 authentic music information queries from a Korean knowledge search portal Naver 지식 (knowledge) iN and 150 queries from Google Answers website. We conclude that new sets of access points must be developed to accommodate music queries that cross cultural or language boundaries.",
        "zenodo_id": 1416706,
        "dblp_key": "conf/ismir/LeeDC05",
        "keywords": [
            "music",
            "digital",
            "library",
            "music",
            "users",
            "cultural",
            "languages",
            "global",
            "music",
            "information"
        ],
        "content": "CHALLENGES IN CROSS-CULTURAL/MULTILINGUAL MUSIC \nINFORMATION SEEKING\nJin Ha Lee J. Stephen Downie Sally Jo Cunningham \nGraduate School of Library \nand Information Science \nUniversity of Illinois at  \nUrbana-Champaign \njinlee1@uiuc.edu  Graduate School of Library \nand Information Science \nUniversity of Illinois at  \nUrbana-Champaign \njdownie@uiuc.edu  Dept. of Computer Science \n \nUniversity of Waikato \nNew Zealand \nsallyjo@cs.waikato.ac.nz\nABSTRACT \nUnderstanding and meeting the needs of a broad range \nof music users across different cultures and languages are central in designing a global music digital library. This exploratory study examines cross-cultural/multi-lingual music information seeking behaviors and reveals some important characteristics of these behaviors by analyzing 107 authentic music information queries from \na Korean knowledge search portal Naver 지식  (knowl-\nedge) iN and 150 queries from Google Answers website. \nWe conclude that new sets of access points must be de-\nveloped to accommodate music queries that cross cul-\ntural or language boundaries.  \nKeywords: User behaviors, cross-cultural music infor-\nmation seeking, multilingual music information seeking, \nKorean users.   \n1 INTRODUCTION \nFutrelle and Downie  [1,2]  criticize the overemphasis \non Western music in MIR research and assert the neces-sity for developing MIR systems that are also suitable for non-Western music. Another aspect of this “multi-cultural challenge” in MIR re search concerns the diffi-\nculties that non-Western user s experience when they \nseek Western music information. In a broad sense, this \nproblem is applicable to all people who seek music in-\nformation from cultures or language of which they are not native.  \nIn everyday life, it would be rare for people to seek \ntextual materials in a language that they do not know (unless there is help availa ble for translation); however, \nit is quite common for people to seek music from differ-ent cultures or music in non-native languages. Witness, for example, the emergence of  ‘World Music’ as a cate-\ngory in the recording industry and in music stores. While music may be “a universal language” [3], there still exist important differences in the way people de-scribe and search for music according to their individual \ncultural and linguistic backgrounds. In short, this multi-cultural aspect of music can make music information seeking a difficult and frustrating task.  \nCurrently, there is little known about cross-cultural \ninformation seeking behaviors in the general field of information retrieval (IR), and much less in the MIR domain. Research on Cross Language Information Re-trieval (CLIR), the retrieval of information written in one language based on a query expressed in another [4], \nhas primarily been restricted to studies involving preci-sion/recall measurements over artificial queries, rather than examining use of CLIR  systems in real-life set-\ntings. Strictly speaking, the cases that we discuss in this \npaper are best considered as examples of multi-lingual \ninformation seeking  rather than cross language informa-\ntion seeking, as the searchers in this study describe their \nmusic information needs in multiple languages.  \n2 PROBLEMS AND OUR APPROACH \nHistorically, music libraries  have provided access points \nfor music retrieval via bibliographic information (e.g., composer, title) and/or some type of genre scheme [1]. One limitation of these traditional access points is the assumption of users’ prior musical knowledge [3]. To overcome this limitation, much of current MIR research focuses on supporting music s earches by exploiting con-\ntent-based characteristics of the music itself rather than \nits metadata (e.g., query-by-humming systems). Another \nassumption exists that audio MIR systems are more flexible than symbolic ones because the audio represen-\ntation features are culturally more neutral, but this as-sumption has not been specifically tested [2].  \nNotwithstanding the limitations of bibliographic in-\nformation, bibliographic information still remains the primary communication mech anism between music col-\nlections and users. Previous MIR user studies indicate that music queries often included bibliographic informa-tion. Of the analyzed queries, 81.3% in [5] and 75.2% in [6] included some sort of bibliographic description. For both studies, the second and the third most commonly provided data in music queries were genre and lyric information. Vignoli [7], and Lee and Downie [8] also found that genre was frequently viewed by users as an important organization and retrieval method. \nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full citation on the first page. \n© 2005 Queen Mary, University of London \n1   \n \n Would these three elements – bibliographic, genre, \nand lyric information – also be used predominantly in \ncross-cultural/multilingual musi c information seeking? \nWhat if people cannot remember, pronounce, or spell the name of the artist, song, or album? What if they do not know the ‘proper’ name for a musical genre, or per-haps know that genre label in a language other than \nEnglish? What if they cannot understand, memorize or \nreplicate the lyrics? The following translated  example \nfrom a Korean music Q&A site helps us illustrate these \nquestions: \n “First of all, it’s very long. Just the first movement \nseems to go over 10 minutes. The mood is dark and sad overall. The main melody is played by violin and the piano briefly appears. I only know that the composer’s name is four characters when it is spelled out in Ko-rean. Does anybody know the title of this work?\n1” \n  \nIn the above example, known difficulties in searching \nfor classical music – those characteristics such as in-strumentation and ‘mood’ are rarely searchable meta-data – are exacerbated because  the searcher is working \nin a different language, culture, and script than the cul-\nture from which the music originated.  \nThis exploratory study examines music information \nseeking behaviors of those seeking music from different \ncultures and in non-native languages. It should be noted that the purpose of this study is not to provide general-izble statements for all cross-cultural/ multilingual mu-\nsic information seeking; rather the main contribution of this work is to provide seed data and observations about these kinds of music information seeking behaviors in order to frame future research agendas.  \nIn the following, we presen t a brief analysis of the \ntypes of music information queries and the kinds of in-formation that searchers provided in those queries. Ad-ditionally, we present a number of real queries as exam-ples to illustrate the unique difficulties inherent in this type of information seeking. By doing so, we provide empirical evidence on why it a ppears to be important to \nconsider certain features in developing MIR systems, and some insights on how to better design MIR systems for international users.  \nAll example queries presented in this paper were \noriginally in Korean, were translated by the first author, \nand are presented in italics . Terms in bold italics  denote \nthat they originally appeared in English. Terms left in \nthe original Korean script are onomatopoeic words or Korean transliterations of English words. \n3 DATA COLLECTION \nPetrelli et al [4] asserted th e necessity for “in the field \nevaluations” in CLIR, and stat ed that “only with real \nusers performing real tasks and possibly in real envi-\nronments, we can arrive at  a definitive understanding of \n                                                           \n1 The answer to this query is: The Devil’s Trill  by the Italian composer, \nTartini, whose name is, in fact , four characters in Korean ( 타르티니 ). CLIR.” The same logic applies for this study: We sought music information requests made by real users based on their real needs in an ope rational system. Downie and \nCunningham [6] suggest Internet newsgroups, websites, and archives of mailing lists as sources of authentic mu-sic queries expressed in na tural language, unencumbered \nby query syntax or prescribed metadata. Following this \nsuggestion, we selected Naver 지식  (knowledge) iN\n2, a \npopular Korean ‘knowledge search’ portal as our data \nsource for collecting cro ss-cultural/multilingual music \nqueries. ‘Knowledge search’ system in Korea refers to a \nweb-based system where people build up a sort of online knowledge base by asking a nd answering questions, and \nsharing their personal knowledge [9]. Naver started its \n지식 iN service in 2002 and currently maintains a well-\ndeveloped and widely used  user-contributed Q&A sys-\ntem for collaborative information seeking [10]. Under \nthe music category alone, there were already 306,719 questions posted and answered as of April 17, 2005. The predominately Korean use r-base pose queries asking \nabout Western (English lyrics), Japanese, Chinese, Ko-rean music, etc. In order to compare the music informa-\ntion search behaviors of Koreans and Americans, we also selected Google Answers as our second data source to collect Western music queries. Google Answers is a web-based online reference service in a Q&A format which was also used in the previous MIR user study by Bainbridge et al [5].  \nAs an exploratory study, we looked at a limited num-\nber of queries to suggest interesting aspects for future research. We analyzed a total of 107 queries posted un-der the category “Pop” on Naver. Two days were arbi-trarily selected – April 1 & 2, 2005 – and all “Pop” que-ries posted on those dates were collected. Two queries were discarded as they were off-topic. We also analyzed \na total of 150 queries from Google Answers – March & April, 2005 – under the music category. Nine were dis-carded as they were off-topic or redundant queries with the same content posted by the same searcher. \n4 OBSERVATIONS AND DISCUSSIONS \n4.1 Comparison: Naver and Google Answers \nSearchers frequently provided vague, incorrect, and in-\ncomplete information when they described their music \ninformation needs in both Naver and Google Answers \nqueries. Surprisingly, many of these queries were an-swered and the answers were verified to be correct or \nrelevant by the searchers. Na ver affords the searcher the \nability to signify correct/u seful answers, and Google \nAnswers allow searchers to comment on the answers as \nthey rate them.  \nSome interesting types of queries that caught our at-\ntention that were unique to Naver data set include re-\nquests for writing the pronunciations of English lyrics in Korean such as: \n \n                                                          \n \n2 kin.naver.com \n2   \n \n “Can someone write the lyrics for ‘ Lady Marma-\nlade’ sung by ‘ Moulin Rouge ’ in Korean? Gitchi gitchi \nya ya da da (hey hey hey)  - 키치  키치  야 야 다 다 \n(헤이  헤이  헤이 ) – Like this.” \n \nNot knowing the language or meaning of songs cer-\ntainly does not stop people from wanting to sing them. \nThe influence of karaoke cu lture was evident in a num-\nber of Naver queries analy zed. Naver searchers often \nsought suggestions of songs that are easy to sing in their \nown vocal ranges and singing styles: this behavior was not observed in the Google Answers data set. Asking for appropriate songs for sp ecial occasions (e.g., birth-\nday, parties) and for specific person(s) (e.g., significant others, friends, co-workers) were also quite common: \n   “I have three friends whose birthday is in April and I \nwant to sing a song for them. Can you recommend any good songs related to friendship?” \n The types of queries posted on two websites are \nsummarized in Table 1 (Naver) and Table 2 (Google Answers). The queries are categorized by the music information needs expressed in them. The most com-mon music information need in both Naver (48.6%) and Google Answers (32.6%) data sets was “Identify art-ist/work.” The rest of the information needs were or-dered fairly differently for two data sets: “Get recom-mendations” was the second most common information needs in the Naver data set (36.2%), whereas only 7 queries (5.0%) from Google Answers sought music rec-ommendations. In the Google Answers data set, queries seeking general information related to music (e.g., about an artist, work, instrument) (25.5%) and queries for locating a musical work (17.0%) were the second and third most common music information needs, contrary to the low proportions in the Naver data set.  \nSeveral assumptions can be made about the reasons \nfor this difference between two data sets. First, the cul-tural difference between Ko reans and Americans may \naffect the information seeking and sharing behaviors of the two groups. Korea’s collectivist culture and the United States’ individualist culture are often contrasted in the studies of organizational behaviors. Hofstede and Bond found that individualists rely more on their own experience when making decisions than do collectivists in the organizational setting [11]. If we consider online communities as a type of organization, we may specu-late that this cultural difference affects the degree of how much an individual is willing to request and accept music recommendations from other members of the community. Second, the cost of using the service may also affect the kinds of information requests submitted.  \nNaver 지식 iN service is semi-free to users: Naver users \nearn points by answering other people’s questions, \nwhich in turn they can use to ‘buy’ opportunities to ask \ntheir own questions. On the other hand, the Google An-swers service asks for a minimum charge ($2.50) per question to its users. Searchers using Google Answers service may choose other methods without charge for \nthose queries (e.g., getting recommendations) that do not necessarily need an expert’s searching skills. These assumptions should be further tested in future studies employing qualitative methods  (e.g., interviewing the \nsearchers using both systems). \nTable 1. Naver queries (105): music information needs categories \nMusic Information Need Percentage \nIdentify artist/work 48.6 \nGet recommendations 36.2 \nAcquire lyrics 5.7 \nRequest translation 2.9 \nLocate specific version of work 1.9 \nSeek information 1.9 \nRequest transliteration 1.9 \nLocate work 1.0 \nTotal 100.0 \nTable 2. Google Answers queries (141): music information needs categories \nMusic Information Need Percentage \nIdentify artist/work 32.6 \nSeek information 25.5 \nLocate work 17.0 \nAcquire lyrics 6.4 \nGet recommendations 5.0 \nReady reference 4.3 \nLocate specific version of work 2.8 \nOthers 2.8 \nSeeking score/tab 2.1 \nRequest translation 0.7 \nRequest research 0.7 \nTotal 100.0 \n In the following tables, we  compared the features \n(types of information) that searchers provided in the music queries from Naver (Table 3) and Google An-swers (Table 4) websites. The number of features pro-vided in a query is highly variable depending on the information needs expressed in the query. Therefore, we \ndelimited our analysis to the single most common cate-\ngory “Identify artist/work” in order to control this vari-ablity. 24 feature categoreis were found in the queries \nfrom both data sets.  \nBy examining the relative frequency of the categories \nof user-provided  information found in the queries be-\ntween the more Western-centric Google, and the Ko-rean-centric Naver, we hope to make the first steps to-ward classifying t hose features that we might consider \n“universal” and those “culturally determined”. This in-\nformation can be very useful in the creation of system-\nprovided  access points and interfaces that afford access \nto both the “universal” and the “culturally determined” \napproaches to MIR query construction. \n3   \n \n Table 3. Features provided by searchers in 51 \nNaver “Identify artist/work” queries  \nFeature  Percentage \nAssociated use (e.g., movie, ad) 58.8 \nPhonetic sound of lyrics 31.4 \nAudio/Video example 23.5 \nLyric word 21.6 \nName of artist 19.6 \nDescription of musical style 19.6 \nGender of artist 17.6 \nName of musical genre  15.7 \nMood/Affect of music 15.7 \nDescription of work 13.7 \nMedia (e.g., radio, TV) 11.8 \nTitle of work 9.8 \nTempo of music 9.8 \nDescription of artist 7.8 \nDate of music released/heard 7.8 \nName of musical instrument 5.9 \nVoice range of artist 5.9 \nPlace where music heard 5.9 \nSubject of music 2.0 \nNationality of music/artist  2.0 \nSimilar work/artist 2.0 \nStoryline of work 2.0 \nName of related event 0.0 \nName of region 0.0 \nTable 4.  Features provided by searchers in 46 Google Answers “Identify artist/work” queries \nFeature Percentage \nDate of music released/heard 50.0 \nName of musical genre 50.0 \nLyric word 45.7 \nAssociated use (e.g., movie, ad) 39.1 \nName of artist 30.4 \nDescription of musical style 30.4 \nTitle of work 28.3 \nGender of artist 23.9 \nMedia (e.g., radio, TV) 21.7 \nName of musical instrument 19.6 \nDescription of artist 15.2 \nAudio/Video example 13.0 \nNationality of music/artist 13.0 \nSimilar work/artist 13.0 \nDescription of work 10.9 \nStoryline of work 10.9 \nMood/Affect of music 10.9 \nName of region 10.9 \nTempo of music 8.7 \nPlace where music heard 6.5 \nName of related event 4.3 \nSubject of music 2.2 \nVoice range of artist 2.2 \nPhonetic sound of lyrics 0.0 \n The statistical analysis wa s performed using Fisher’s \nexact test. The proportion of each feature used in each \ndata set was pair-wise compared between two data sets \nand tested for significant di fferences. When the p-value \nwas smaller than 0.05, we considered the difference to \nbe statistically significant. Table 5 lists the features that had p-values (two-tailed) less than 0.05.  \n \nTable 5. Result of the statistical analysis \nVariable P-value \n(2-tailed) \nDate of music released/heard .000 \nPhonetic sound of lyrics .000 \nName of musical genre .000 \nLyric word .017 \nName of region .021 \n \nIn the queries where search ers were trying to identify \nan artist or a musical work, Google Answers searchers \nprovided date, genre , lyrics , and region  information \nsignificantly more often than  Naver searchers. Naver \nsearchers exploited the phonetic sound of lyrics  signifi-\ncantly more than Google Answers searchers. It is \nthrough this preliminary examination that we begin to \nsee the distinctions between the “universal” and the “culturally determined” query approaches users con-struct. For example, the phonetic sound of lyrics ap-\nproach is found in a non-trivial percentage of the Naver queries. This implies that we must design MIR systems that can afford access using this “culturally determined” \nquery approach. \nIn Section 4.2, we discuss difficulties that Naver \nsearchers encountered with commonly provided access points in MIR, and describe what they did to success-fully find music information despite these limitations. \n4.2 Use of Traditional Access Points in Naver Queries \n4.2.1 Bibliographic information  \nNaver searchers often failed to provide any biblio-\ngraphic metadata such as composer, performer, or title in their queries, which are tr aditionally the primary access \npoints of MIR systems. Searchers sometimes managed to \nremember the characteristics of the performer such as “a \nsexy blonde singer who was we aring a skirt made with \nbeads”  – details that might be helpful in jogging the \nmemory of a human music expert, but not useful in a \nMIR system. Even when bibliographic metadata is known, searchers did not always spell the artist name or work title in English: Instead, a mix of English names and Korean pronunciation of those names was often used. Inconsistency in transliterating names from Eng-lish to Korean frequently resulted in fatal search errors, as the following case illustrates:   \n“I heard this song in my friend’s car. I asked what it \nwas and he said he was sure (about the information), but I tried searching and got no results! Think he made \n4   \n \n a mistake? He told me the singer was 메리호키스  and \nthe title was 도즈월드데이즈 , but I can’t find it! I tried \n메리허키즈 , 메리호키즈 , 더즈월드데이즈 , \n더즈월드데이스  as search terms, but didn’t even find a \nsimilar one! Please tell me if somebody knows this1.”   \nNaver searchers often exploited “context metadata” \nto compensate for the difficulties in providing reliable \nbibliographic information. Context metadata indicates the “extrinsic aspects, uses, and relationships of music” and one category of context metadata is “associative metadata” which regards the use of music in other mul-\ntimedia objects such as movies, commercials, etc. [8]. \nAssociative metadata was found in 30 (58.8%) of the “Identify work/artist” queries from Naver:   \n“This pop song has been around for a while. I really \nwant to know what it is! It was used in a TV commercial where Cha In-Pyo (Korean actor) was running some-\nwhere, and it was also in the OST  for Powerpuff girls. It \ngoes like\n파이틀  오버 ~ 먼먼머  파이틀  오버 ~ \n먼먼먼먼먼먼먼  파이야이야이야이야  와우 !2.”  \n“What’s the song played when the TV show Happy \nSunday at 6pm on KBS2  starts? I heard 오 미키3.”  \n“Can you tell me the title of this song? The first part \ngoes like 라라라 ~ 라라라라라 ~ and it’s a pop song by \na female singer. I heard it is played a lot in night-\nclubs…I only remember the words everynight ~ every-\nday~.4”  \nTV shows, commercials, movies, and music videos \nseemed to be major sources for Naver searchers to en-\ncounter new pop music. It is not surprising that OST (Original Soundtrack) is often one of the main catego-ries of music on Korean music-related websites [see Section 4.2.2]. Associative metadata was also found in \n39.1% of the “Identify work/artist” queries from Google \nAnswers, which is a smaller proportion than Naver que-ries, but still a significant proportion.  \n4.2.2 Genre information  \nStudies of music genre and classification schemes [7], \n[13] conclude that genre definitions appear to be neither consistent nor objective, and there is no consensus on genre descriptions among users [4]. People generally find it difficult to clearly distinguish one genre from another, especially when th ey are faced with an unfamil-\niar genre label [12]. Pachet [14] added that the terms among different genre classifications are not consensual and the taxonomy structures do not match. A compari-son of Korean and American  genre classifications well \nillustrates this problem.  \nIn Korea, the term pop music  denotes very different \nsongs than in America. “POP” is loosely used to indi-cate any Western music from outside of Korea, thus \n                                                          \n \n1 Mary Hopkin ( 메리홉킨 ) - Those Were The Days ( 도즈워더데이즈 ) \n2 Knack – My Sharona \n3 Toni Basil – Mickey  \n4 Kylie Minogue – Can’t get you out of my head people simply do not have much information about spe-\ncific genres and how to distinguish among them. The Recording Industry of Association of Korea (miak.or.kr) collects record sales statistics under two \nmajor genres – 가요  (Korean popular music, also some-\ntimes referred as K-POP) and POP (Overseas) – with no \nsubgenres listed. Major Korean music Websites like \nClickBox\n5 has 가요  (K-POP), POP, OST (movies), \n클래식  (Classical), 뮤직비디오  (music video), and \n연예방송 /뉴스  (Entertainment news) as the main cate-\ngories. Another website, iLikepop,6 has 가요  (K-POP), \nPOP, J-POP (Japanese popular music), OST (movie-\ndrama), 기타  (etc.), and 뮤직비디오  (music video) as \nthe main categories. Genre can  sometimes be useful in \nsuggesting additional similar songs that a user may en-\njoy [7], however this may not be the case for Korean \nwebsites with very broad genres that include vastly dif-ferent items in each category. For instance, on iLike-\npop.com, the artists found under the POP category in-\nclude Britney Spears, Marilyn Manson, Wu-Tang Clan, Moscow Boys Choir, and the Chicago Symphony Or-chestra. Consequently, it can be confusing and difficult for Korean people to apply genre labels to pop songs. In fact, many queries failed to provide ‘proper’ genre in-formation and when searchers did suggest a genre, they often expressed uncertainty.   \n“I’m curious about Linkinpark ’s music style. Is that \nalternative metal?”  \nSome Naver searchers explicitly stated that they \nwanted to explore a new genre and they were just start-ing to learn about these different kinds of music.   \n“I’d like to listen to pop, but I don’t know much \nabout it. Among Sweetbox’s songs , ‘life is cool’ is my \nfavorite. Please suggest pop songs like this that are easy to listen to.”  \nFor Naver searchers, the association-based concept \n“Give me some music similar to this particular song(s) or artist(s)” [7] was a common key to finding new music. Among all Naver queries analyzed, 25 (23.8%) asked for suggestions similar to given titles/artists. Korean people may feel more comfortable asking for similar music by providing one or more titles/artists as exam-ples, rather than providing detailed genre information \n(e.g., “… songs like Sk8ter boi  by Avril Levine, just \nSk8ter boi  and not any of the rest of her songs ”). De-\nscriptions of personal music preference were often \nadded:   \n“\n비욘세  – crazy in love , 제니퍼로페즈  – play, \n스테이시오리코  – stuck , 에미넴  – without me . I \nreally like these kinds of music, but I don’t know any \nother songs like them. If you know these kinds of songs, \nplease let me know as many as you can! I especially like \nthe ones by 스테이시  and 비욘세 .”  \n                                                           \n5 Clickbox.co.kr \n6 iLikepop.com \n5   \n \n “Can someone suggest some depressing songs? For \nexample, do you know ‘ I need some sleep ’ by Eels? \nThat kind of style or I also like Portishead ’s ‘Wander-\ning stars ’ and ‘ Roads .’ I want pop songs and not just \n‘kind of’ depressing, but se riously depressing songs!”  \n“I’m starting to like pop songs lately and want some \nsuggestions. 1) I like female singers. 2) I don’t like rock \n– I like ballad and lively songs. 3) I want the names of the singers. 4) Also I heard a very cheerful carol song by some female singer the other day – if you know what it is, let me know. 5) Give me more descriptions than \njust titles. If you know songs similar to ‘ as long as you \nlove me ,’ please recommend as well.”   \nBainbridge et al [5] have suggested compensating for \nusers’ widely varying music descriptions by providing \nquery-by-example facilities in MIR systems to allow users to ask for ‘more things like this.’ In [5], they found that few music queries posed to Google Answers included an audio representation of the desired music, likely due to limitations in the interface. In our data set, \n12 of analyzed Naver queries (11.4%) used Naver’s embedding and linking features to provide an au-dio/video example while only 6 of analyzed Google Answers queries (4.3%) included links to audio/video representation of sought music. Interestingly, when there was no pre-existing example on the Web to link to, a few Naver searchers even created their own examples \nby capturing part of the audio, or recording their own performance like the query below.   \n“I heard this song in the late 70s or early 80s, so \nwhen I was really young. I can’t remember the singer or the title. The unique voice and saxophone were really impressive. I tried to play the part that I remember. I think the basic melody is at least 85% correct. Please help me find this song in my memory. <Audio file in-cluded>”  \nIn Korea, 12 years of formal music education is a \nstate-mandated requirement, and we speculate that this may have some effects on music information seeking behaviors like such given above. The majority of Ko-rean people know how to read sheet music and play some sort of musical instrument, thus rather than pro-viding inaccurate or uncertain  bibliographic or textual \ninformation, some may feel more confident in providing self-made music examples. This assumption should be further tested in the future user studies of Naver search-\ners. \n4.2.3 Lyric information \n23 of analyzed Naver queries (21.9%) and 25 of ana-\nlyzed Google Answers queries (17.7%) included lyric information. In the Naver data set, 10 queries included \nlyric information in English and 15 queries included transliteration of English lyric words in Korean. The average number of lyric words given in a single query was 6.1 for the Naver data, and 33.96 for the Google Answers data. One of the Google Answers queries had a transcription of lyrics based on an audio example of the \nsought music which highly influenced the average num-ber of lyric words – without  this outlier, the average \nnumber of words drops to 19.38, which is still much higher than the Naver queries. The Naver query with the longest lyric information in English had only 14 words, shorter than the average number of lyric words (19.38) \nin Google Answers queries. The kinds of lyric words \nthat Korean searchers were able to catch and remember \nwere common words (e.g., baby, love, eyes, funky) that may lack discriminating power in IR. In the Naver que-ries, it seemed unlikely that English lyric information alone would effectively help others to successfully an-swer the queries. In most successful cases, other kinds of information were additionally provided to help the an-swerer as the following example illustrates:  \nQ: This was a popular dance song in late 80s. They \noften play this song in the nightclubs. I used to listen to it without much thought, but since when I heard it again \non mbc\n일요일밤에  대단한도전  (TV show) where Lee \nYun-seuk was riding rollerblades, I’ve been wanting to \nknow the title of this song. I don’t know the lyrics very \nwell because the rhythm’s fast, but I think funky funky  \nwas repeated and also pronunciation like 숑숑  [shong \nshong] was repeated. It was sung by a male singer. I’m \nsorry that my hints are poor, but I would appreciate it if \nsomeone can tell me what this song is.     \nA: I’m just guessing since you said late 80s. If it is \nFrench, it may be correct… \n \nDebut de Soiree – Nuit De Folie \nYear: 1988 Country: France \n \nEt tu chantes, chantes, chantes, ce refrain qui te plait  \n(\n에뚜  숑뜨  숑뜨  숑 쎄 레퀘테펫 ) – It  is possible that \nthis is the 숑숑  [shong shong] part… La cadence du \nfunk au plus haut t’emmene - I think this may be the \nfunky part…If the song that you are looking for is in \nEnglish or the singer is black, this is probably not the \none. Check it on 벅스  뮤직  (a Korean music website). \n5 CONCLUSION AND FUTURE WORK \nOur study has revealed some  interesting characteristics \nof cross-cultural/multilingual music information seeking \nbehaviors. Meeting the needs of a broad range of music \nusers across different cultures and languages is a chal-lenging task, but is nonethele ss necessary for the realiza-\ntion of truly global music digital libraries. Our analysis supports the idea that research should commence on the \nestablishment of new kinds  of access points beyond the \nbibliographic, genre, and lyric information in order to \naccommodate queries that cross cultural and/or language boundaries.  \nOne option may be to consider developing metadata \nthat are culturally neutral a nd thus can easily be picked \nup by multicultural/multilingual users. For example, the \n6   \n \n gender of performer is often used as a query feature but \nrarely formally included in any metadata system. Re-search into capturing and better understanding the uni-versal aspects of music mood and affect would also en-hance the cross-cultu ral/multilingual retrieval of music.  \nIn the future works, we will address the following \nlimitations of this study. First, we will explore new \nsources other than Naver 지식 iN and Google Answers \nfor data collection. These Q&A websites can develop \ncertain “culture” and group norms over time, which may \nreinforce users to behave in certain ways. In order to eliminate this variation in users’ information searching behaviors due to the individual website’s own culture and study the true variation between Western and Non-western music information s earchers, queries must be \ncollected from more than one  representative website for \neach group. Second, the queries themselves do not pro-\nvide in-depth information on the reasons why searchers \nasked certain kinds of questions, and why they did or \ndid not provide certain information in their music in-formation need descriptions. Studies with more qualita-tive approaches employing interviews or a focus group like [12] should be done to obtain such information. \n6 ACKNOWLEDGEMENTS \nWe thank the Andrew W. Mellon Foundation for their \nmoral and financial support for the HUMIRS (Human Use of Music Information Retrieval Systems) project. This project is also supported by the National Science Foundation (NSF) under Grant Nos. NSF IIS-0340597 and NSF IIS-0327371. We also thank Dr. Swicegood in the Sociology department at the University of Illinois at \nUrbana-Champaign (UIUC) for statistical consultation, \nand M. Cameron Jones at the Graduate School of Li-brary and Information Science at UIUC for assisting in data collection.  \nREFERENCES \n[1] Downie, J. S. Music information retrieval. In Annual \nReview of Information Science and Technology 37, ed. Blaise Cronin, Medford, NJ: Information Today, 2003, 295-340.  \n[2] Futrelle, J., and Downie, J. S. “Interdisciplinary communities and research issues in music information retrieval”, Proceedings of the Third International Conference on Music Information \nRetrieval, Paris, France, 2002, 215-221. \n[3] Lippincott, A. Issues in content-based music information retrieval. Journal of Information Science, 28, 2002, 137-142. \n[4] Petrelli, D., Beaulieu, M., Sanderson, M., and Hansen, P. User requirement elicitation for cross-language information retrieval. The New Review of Information Behaviour Research: Studies of Information seeking in context, 3, 2002.  [5] Bainbridge, D., Cunningham, S. J., and Downie, J. S.  “How people describe their music information needs: A grounding theory analysis of music queries”, Proceedings of the Fourth  International Conference \non Music Information Retrieval, Baltimore, USA, 2003. 221-222. \n[6] Downie, J. S., and Cunningham, S. J. “Toward a theory of music information retrieval queries: System design implications”, Proceedings of the Third International Conference on Music Information Retrieval, Paris, France, 2002, 299-300.  \n[7] Vignoli, F. “Digital music interaction concepts: a user study”, Proceedings of  the Fifth International \nConference on Music Information Retrieval, Barcelona, Spain, 2004. \n[8] Lee, J. H., and Downie, J. S. “Survey of music information needs, uses, and seeking behaviours: preliminary findings”, Proceedings of the Fifth International Conference on Music Information Retrieval, Barcelona, Spain, 2004, 441-446. \n[9] Park, J., and Jeong, D. An empirical study on web-based Question-Answer services. Journal of the Korean Society for Information Management, 21(3), 2004, 83-98. \n[10] Lee, T., and Kang, S. “A study on knowledge \nmanagement of web search engine based on knowledge search community”, The Tenth Proceedings of the Korean  Society for Information \nManagement Conference, 2003, 143-152. \n[11] Earley, P. C., and Gibson, C. B. Taking stock in \nour progress on individualism-Collectivism: 100 \nyears of solidarity and community. Journal of Management, 24(3), 1997, 265-304. \n[12] Cunningham, S. J., Reeves, N., and Britland, M. \n“An ethnographic study of music information seeking: implication for the design of a music digital library”, Proceedings of  the Third ACM/IEEE-CS \njoint conference on Digital libraries, 2003, 27-31. \n[13] Fabbri, F. Browsing music spaces: categories \nand the musical mind. Presented at the Third Triennial British Musicological Societies Conference (Guildford, UK, 1991). Retrieved from http://www.theblackbook.ne t/acad/others/fabbri9907\n17.pdf. \n[14] Pachet, F. Content mana gement for electronic \nmusic distribution. Communications of the ACM, \n46(4), 2003, 71-75. \n \n7"
    },
    {
        "title": "Genre Classification via an LZ78-Based String Kernel.",
        "author": [
            "Ming Li",
            "Ronan Sleep"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415162",
        "url": "https://doi.org/10.5281/zenodo.1415162",
        "ee": "https://zenodo.org/records/1415162/files/LiS05.pdf",
        "abstract": "We develop the notion of normalized information distance (NID) [7] into a kernel distance suitable for use with a Support Vector Machine classifier, and demonstrate its use for an audio genre classification task. Our classification scheme involves a relatively small number of low-level audio features, is efficient to compute, yet generates an accuracy which compares well with recent works.",
        "zenodo_id": 1415162,
        "dblp_key": "conf/ismir/LiS05",
        "keywords": [
            "Genre classification",
            "LZ78 string kernel",
            "Support Vector Machine (SVM)",
            "Normalized information distance (NID)",
            "Audio genre classification",
            "Low-level audio features",
            "Kernel distance",
            "Efficiency",
            "Machine learning",
            "Audio analysis"
        ],
        "content": "GENRE CLASSIFICATION VIA AN LZ78-BASED STRING KERNEL\nMing Li Ronan Sleep \nSchool of Computing Sciences\nUniversity of East Anglia \nNORWICH NR47TJ, UK \nmli@cmp.uea.ac.uk  School of Computing Sciences \nUniversity of East Anglia \nNORWICH NR47TJ, UK \nmrs@cmp.uea.ac.uk  \nABSTRACT \nWe develop the notion of normalized information dis-\ntance (NID) [7] into a kern el distance suitable for use \nwith a Support Vector Machine classifier, and demon-strate its use for an audio genre classification task. Our classification scheme involves a relatively small number of low-level audio features, is  efficient to compute, yet \ngenerates an accuracy which compares well with recent \nworks. \n \nKeywords: genre, classification, LZ78 String Kernel, \nSVM.  \n1 INTRODUCTION \nAs collections of music, both personal and public, be-\ncome ever larger, the desire for increasingly intelligent music retrieval systems grows. A key problem for such systems is the audio genre classification problem. This has become an increasingly active area of research in \nrecent years. The state of the art is that automatic genre \nclassification systems are beginning to approach, and \nsometimes exceed, human performance. \nGenerally, such a system can be divided into two \nparts: feature extraction and classification. In the first \npart, audio sampled are converted into a sequence of \nacoustic features. In the second part, these features are input to a classifier which yields an estimate of the sam-ple genre. \nFeatures used typically fall into three categories: tim-\nbral texture, rhythm and pitch [3]. Intuitively, the pres-ence of features from all three categories would seem desirable, but good performances have been reported using only some. For example, [1] reports good results using only pitch contour. Advanced signal processing ideas can be used to extract  higher level features: for \nexample in [9], a wavelet-based feature called DWCHs is used to obtain good results. \nA wide variety of classifier s are available for the sec-\nond stage, e.g. Gaussian Mixture Modelling (GMM), K \nNearest Neighbour (KNN), Support Vector Machine (SVM) as described in [3,9,10]. Artificial Neural Net-works (ANNs) and learning tr ee vector quantizer have \nalso been used [11,12]. \nOne difficulty is that most classifier technology is \noriented towards fixed length vector data. Thus some-where between the raw data and the classifier, the vari-able length signal is converted to a fixed length vector. \nA simple way of converting variable to fixed length \nis to use some sort of averaging over the whole music piece. This might work well where timbral features are \nparticularly important in discriminating between genres, \nespecially when used in conjunction with a good classi-fier e.g. as in [2]. However, it seems a pity to lose se-quence information by such averaging, and this has mo-tivated the use of sequence or iented approaches such as \nHidden Markov Model (HMM) and Statistical ( n-gram) \nLanguage Models. These have been explored respec-tively in [19] for genre classification and in [15] for singing language identification. \nHMM and n-gram models seem attractive but for \nvarious technical reasons they tend to be used only with small numbers of states.  Our main contribution in this paper is to introduce and explore a novel approach to passing information about variable length data se-\nquences to a standard vector  classifier in a way which \ntakes account of the patterns which actually appear in \nthe training data, as opposed to those restricted to some arbitrary maximum length. \nFor this work we concentrate on using SVM for the \nclassification stage. This has become a popular tool in the last few years, and it achieves good performance in a wide range of pattern recognition tasks [1,4]. \nSpecifically, we present a ne w class of string kernel, \nwhich is based on Lempel-Ziv-type coding algorithm [16], and use it to drive an SVM classifier for music genre classification. In our scheme, the audio sample represented by a sequence of spectrum-based feature vectors are approximated and converted by a Vector Quantizer (VQ) into a sequence of 1-dimensional vector. Then, we use our string kernel to calculate the similarity of two such vector sequences by analyzing the temporal characteristics shared betw een them. Our scheme for \ncalculating the similarity between variable length fea-ture sequences is explained below. Our results indicate that we obtain an improved performance compared with fixed n-gram techniques.  \nThe rest of this paper is organized as follows: in the \nnext section, we give an overview of our classification scheme; section 3 explains briefly the notion of normal-ized information distance (NID) – this is a similarity function proposed in [7] and based on the concept of Kolmogorov Complexity (KC); in section 4, we explain \nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full citation on the first page. \n© 2005 Queen Mary, University of London \n252   \n \n how we extend NID into a valid kernel distance and \nused it for sequence classification. Essentially, our method maps a sequence to a high-dimensional feature space which is indexed by all the phrases identified by a simple adaptive coding parse of the sequence. Experi-ment results and discussion are shown in section 5 and 6. Finally, we conclude in section 7.  \n2 OVERVIEW OF CLASSIFICATION \nSCHEME \nAudio signal are temporal in nature, and our feature ex-traction scheme is designed to respect this. As shown in Figure 1, it begins with a sequence of audio samples representing a particular piece of music, and then applies the following steps: 1) convert the sequence of audio samples into a sequence of cep stral features inspired by \nspeech recognition technology; 2) apply a vector quan-\ntizer to discretize the featur e space into small blocks and \nlabel them by index; this converts the sequences of cep-\nstral feature vectors into a (variable length) sequence of VQ codebook indices; in step 3), pairs of these event sequences are used to compute a similarity measure (in a manner developed in section 4) to construct a kernel matrix for an SVM classifier. \n \nFigure 1 Block diagram of classification scheme \nOur scheme has aspects in common with Tsai and \nWang’s work [15], but we use only one codebook to be generated across all the classes instead of one for each class. This means that the pr e-processing stages (steps 1 \nand 2) are independent of the actual classes of interest, and so the pre-processing result (i.e. codebook) learnt from one task might be regarded as background knowl-edge and used for a variety of classification tasks. A more fundamental difference is that the fixed-bi-gram language model used in [15]  is replaced by our string \nkernel method that can build a set of characteristic pat-terns of varying length from the data. 2.1 Audio Parameterization \nSince our work focuses on a novel string kernel and its \nperformance in audio genre classification, we decided to adopt a simple but very popular low level signal feature \nset that is perceptually mo tivated and has been success-\nfully used in speech recognition: MFCC\n1 coefficients \nplus an energy term. Moreover, since the performance of a speech recognition system can  be greatly enhanced by \nadding time derivatives to the basic static parameters, we explored their use. In our experiments, the final feature set includes delta values and acceleration values that are calculated using the following regression formula:  \n log\n12∑\n==N\nnns  energy                             (1) \n \n2) (\n_\n121\n∑∑\n=− =+−\n=M\niitM\ni it\niv vi\n  derivative time        (2) \nsn is the signal energy for audio samples, vi is a delta \ncoefficient at time t computed in terms of the corre-\nsponding static coefficients vi-t to vi+t. \n2.2 Vector Quantization (VQ) \nThe VQ step maps the potentially huge space of MFCC-\nlevel signal sample to much smaller element space of codebook indexes. \nFormally, a vector quantization of n dimensions can \nbe described as a mapping of vector V from an n-\ndimensional space S to a subset S’ of it, where S’ con-\nsists of K codewords in S. The VQ scheme block dia-\ngram is shown in Figure 2. In the encoding procedure, \nthe input vector searches the codebook and finds the closest codeword for that input and the corresponding vector reference index is output. In our work, a diago-nal-covariance Mahalonobis distance metric is em-ployed and a linear codebook is constructed for which minimizes the quantization error. \n \nFigure 2 Block diagram of vector quantization \n2.3 Key Concepts in SVM \nSVM is a powerful supervised learning algorithm, which \nmaps data into a high dimensional feature space where data is more likely to be linearly separable. A linear de-cision boundary is obtained by maximizing the geomet-\n                                                          \n \n1 Mel Frequency Cepstral Coeficients Input vector VQ index Search en-\ngine \n(Nearest  \nneighbour \nrule) Sequence of acoustic \nfeature vectors \nSequence of  \nvector indices Codebook Sequence of Acoustic Signals \nSequence of Acoustic Feature Vectors \nStep 2: Codebook Generation \nvia Vector Quantization\nSequence of Vector Indices \nStep 3: Calculating Pairwise Similarity via LZ78Ex\nKernel Matrix \nStep 4: Classification via Support Vector MachineStep 1: Feature Extraction \n253   \n \n ric margin in order to make it as far away from the data \nas possible whilst separating the two classes correctly. The decision boundary learnt by SVM is entirely based on the information provided by a kernel matrix (called Gram Matrix) with entries of similarity score measured by pairwise inner product. \nAt the heart of SVM is a constraint quadratic optimi-\nzation solver which works with the dual problem using \nonly the inner product of input data pairs (this is the similarity measure). This inner product may be specified in terms of a kernel function which does the mapping to the high dimensions, but by working with the dual prob-lem only inner products are needed and these may be evaluated in R\n2 or even specified dir ectly as a similarity \nmeasure between pairs. \nWhen the data is not linearly separable in the high \ndimensions, a modified SVM procedure can be used, \nwhich finds a trade-off between maximizing geometric margin and minimizing the cost of misclassification. This is achieved by introducing “slack” variables, which \nallow the margin constraint to be violated \n3 NORMALIZED INFORMATION \nDISTANCE \nA Normalized Information Distance (NID) as proposed \nin [7] is a metric measuring pairwise similarity between sequences. Informally, it is the ratio of the information not shared by the two sequences to the total information content of the pair of sequences. This is illustrated in Figure 1 where circle A is K( x) that represents the Kol-\nmogorov complexity of object x, circle B is K( y) and the \ntotal area of two circles (A+B+C) is K( xy). Two identi-\ncal sequences will have NID=0, whilst two sequences \nwith no common information content will have NID=1. \n \nFigure 1 . Conceptual illustration of normalized \ninformation distance. \n3.1 Conditional Kolmogorov Complexity  \nGiven an object encoded as a binary string x, its Kolmo-\ngorov complexity K(x) or algorithm entropy is the mini-\nmum number of bits into which the string can be com-\npressed without losing info rmation [13]. Intuitively, \nKolmogorov complexity indicates the descriptive com-plexity contained in an object and it is the length of the shortest program for some universal machine which, when run without any input, outputs that string. This is an idealized notion, because  it is not computable. How-\never, any compression algorithm gives an upper bound and this can be taken as an estimate of the Kolmogorov complexity.  A random string has relatively high complexity since \nno structural pattern can be recognized to help reduce the size of program. Strings like structured texts and melodies should have lower complexity due to repeated terms and musical structure. \nAs for conditional Kolmogorov complexity, K(x|y) , it \nis defined as the shortest program that can regenerate the sequence x from the sequence y. K(x) is the special \ncase of K(x|λ) where λ is the empty sequence. In [7], \nK(x|y)  is estimated as the difference of the unconditional \ncomplexity estimates K\nc (xy) and Kc (y): \nKc (x|y) = K c (xy) − Kc (y) \nHere xy stands for the concatenation of sequences x \nand y. A minor issue arises when using algorithms such \nas compressors to estimate KC: in general the order of \nconcatenation affects the size of the compressed con-catenation, so that the relation  K\nc(xy) = K c(yx) may not \nhold for our estimates. This issue can be handled by using the average of the two ordering. \n3.2 Normalized Information Distance \nThe information distance [14] between two sequences x \nand y can be defined as the length of a shortest binary \nprogram that computes x given y, and also computes y \ngiven x. However, such a distance does not take the \nlength of the sequence into account. This motivates the \ndesire for a relative (or normalized) measure that takes account of sequence length. If two pairs of sequences have the same information distance but with different lengths, the longer pair should be given a smaller dis-tance measure than the shorte r pair, reflecting the fact \nthat more information is shared between longer se-quences. \nThe authors in [7] use conditional Kolmogorov com-\nplexity as the basis of a nor malized information distance \nD(x, y)  for measuring the similarity relations between \nsequences. Two versions of the similarity metric are proposed in [7]. \n )() ( ) () (1xyKy|x K x|yK  x, yd+=                    (3) \n))( )(( max)) ( ) (( max) (2y , KxKy|x ,K x|yK  x, y d =            (4) \nThe second definition can be shown to satisfy the \nconditions enumerated above without qualification, and \nin that sense is more satisfactory than the first. But con-ditional Kolmogorov complexity is not computable, so we have to rely on estimates of KC. This makes it less clear that strict application of the mathematical elegance \ncriterion is the most important guideline for practical \nclassification work. Indeed, in  our earlier work in [1], \nwe found the practical performance of equation 3 is slightly better than that of equation 4. \n4 THE LZ78-BASED STRING KERNEL \nIn [1], we used an LZ78 parser to provide the estimates \nof the Normalized Informa tion Distance (NID) between \n254   \n \n pitch contours extracted from two MIDI files. The re-\nsults were encouraging. We then began looking how to analyze sequences of audio instead of symbolic data: here the ‘symbols’ which should be chosen as a basis for a NID method are not so clear – this is the purpose of our VQ preprocessing stages. \nWe also wanted to expl ore the recent success of \nstring kernel methods in the context of music. It oc-\ncurred to us that we could use the dictionaries generated by an LZ78 parser to identify commonly occurring words in a sequence, and th en use a string-kernel like \napproach to drive an SVM classifier. Hence we intro-\nduce a new class of string kernel, called LZ78Ex kernel. \nGiven two sequences x and y, our kernel distance is \nestimated by the degree of sharing between the phrases \nof two dictionaries D\nlz78ex(x) and Dlz78ex(y) produced by \na modified LZ78 parse of the sequences. The modified \nLZ78 generates frequency counts, and these are used in computing the kernel distance. Note that LZ78 is run exactly once for each piece of music in the data set: the \npairwise kernel metric takes as input the two dictionar-ies. \n4.1 A Modified LZ78 Parsing Scheme \nAt the heart of our kernel is a simple adaptive parsing \nalgorithm based on the LZ78 compression scheme, \nwhich is originally proposed in [16]. It works by identi-fying patterns, called phrases, of the data and stores them in a dictionary (i.e. encoding table) that defines commonly occurring substrings, and associates them with dictionary indexes that  are used to represent the \nphrases in the compressed output. The outline code shown below captures the essence of our modified LZ78 encoder omitting irrelevant details: \n \nClear dictionary;  \nw = λ\n1; \nwhile (more input) \n  C = next symbol; \n  Pattern=wC \n  if ( Pattern in dictionary) \n    w = Pattern; \n  else \n    add Pattern to dictionary \n    w = λ; \n  endif \n UpdateFrequency(pattern) \nendwhile  Return dictionary;  \nNormally, the LZ78 parsing algorithm starts with a \ndictionary initialized with codes for each possible value \nin the sequence and emits code consisting of a compact \nreference to the each new en try. As we are only inter-\nested in the entries which actually occur in the data se-quences, in our implementation, we initialize the dic-tionary to be empty and update pattern frequency at \n                                                          \n \n1 ‘λ’ represent the empty string. each step. Note that (1) unlike the n-gram approach, \nLZ78 parsing produce a set of features with varying \nlength (2) it identifies incr easingly long initial portions \nof repeated phrases gradually: that is, it will need to see a phrase of length L on L occasions before it remembers \nthe whole phrase in its dictionary. \n4.2 LZ78Ex Kernel \nRecall that the pre-processing steps of our scheme con-\nvert a sequence of acoustic feature vectors into a se-quence S of symbols from a finite alphabet (the set of \nVQ indexes). When this sequence is parsed using our modified LZ78 parser, we obtain a dictionary D(S)  of \ncommonly occurring ‘words’ in the VQ index alphabet ∑. The set of all such words ∑* provides an infinite set \nof features. We take as the basis for our similarity meas-ure between two sequences S and S’ the inner product of \nthe vectors representing D(S)  and D(S’) . In fact our rep-\nresentation of a sequence S in this space is a weighted \nvector Φ\nlz78ex with i-th element indicating the number of \ntimes i-th pattern occurring in S. The LZ78Ex kernel \ndistance between two sequences S and T is then defined \nas the inner product of their feature vectors in which the \nelements are weighted by their occurrence frequencies: \n> =< )( ),( ),(78 78 78 tΦsΦ ts Kex lz ex lz ex lz        (5)  \nFinally we normalise our kernel to take account of \nthe actual size of the two f eature vectors as follows: \n),( ),(),(),(\n78 7878\n78tt Kss Kts Kts K\nex lz ex lzex lz norm\nex lz =       (6)  \nNote that while the featur e space has infinite dimen-\nsionality, computing the inner product is a linear in the size of the features actually  present in the data. The \nLZ78 parse itself is highly efficient, especially when using appropriate trie structures and/or hashing. It fol-\nlows that the overall cost of computing LZ78Ex kernel \nis bounded above by O(n). Note also that the LZ78Ex \nkernel satisfies Mercer’s conditions (symmetry and positive semi-definiteness). This follows from its defini-tion as the inner product of two feature vectors. \n5 EXPERIMENTAL SETUP  \n5.1 Dataset and Tools \nThe dataset2 used in our experiments appears to be that \nused in [3,9], though we are not certain of this. The set \nconsists of 10 genres: classical(Cl), country(Co), disco(Di), hip-hop(Hi), jazz( Ja), rock(Ro), blues(Bl), \nreggae(Re), pop(Po) and heavy-metal(Me). Each genre consists of about 100, 30 second samples. For all LZ78-related experiments, the feat ure vectors were extracted \nand quantized using HTK.3.2.1\n3. All the classification \n                                                           \n2 http://opihi.cs.uvic.ca/sound/genres \n3 A toolkit for building hidden markov model available at \nhttp://htk.eng.cam.ac.uk. \n255   \n \n experiments were conducted using LIBSVM  package \n[17], which provides an inte rface to utilize user-defined \nkernel matrix. Performance evaluation was accom-\nplished using a stratified 10 cross validation (CV), which avoids overlapping test sets and approximate the original proportions of labels within  each subset. Extensive ex-\nperiments have shown that this is the best choice to get \nan accurate estimate. There is also some theoretical justi-\nfication for this approach. \n5.2 Sub-Genre Effect: Biased and Unbiased 10-Fold \nCross Validation \nOne of the many drawbacks of not having properly con-\nstructed corpuses for running experiments on music genre classification is that it is necessary to either build ones own corpus, or borrow from others. An uncon-scious effect of this is that present data sets are stratified into sub-genres – for example the songs from a special singer or a sub-source with other special characteristics (e.g. vocal classical as opposed to concerti). This can lead to subtle effects on performance evaluation, and it is not clear that all work to date has guarded against this effect. \nTo see how important the effect is, we use two dif-\nferent strategies to conduct the 10-fold cross validation which will be called un-biased  and biased  respectively: \n1) Un-biased  Strategy:  this approach refers to stan-\ndard stratified 10-fold CV that firstly shuffles the \ndata randomly and then cross validates them. We call this un-biased since the examples from all the \nsub-genre can be well presented in both training and test dataset.  \n2) Biased  Strategy : in this approach, each genre is \nmanually divided into 10 groups (sub-genres) ac-cording to the number used to label the audio files (e.g. metal.00000 to metal.00009 belong to the first group in genre metal' etc). We found that two closely numbered files in each genre tend to sounds similar than the files numbered far away from each other. Thus, each group is presumably to represent one special sub-genre. In i-th cross validation, the \ntest data are constructed by combining the i-th \ngroup from each genre and the rest groups are used for training. We call this ‘biased’ since the stratifi-cation effect may lead to biased presence of some sub-genres in the training/test sets. \n6 RESULT AND ANALYSIS \nPreliminary experiments desc ribed in 6.1 and 6.2 were \nrun to determine reasonable choices of MFCC granular-\nity, codebook size for the vector quantization processing, and the choice of audio frame length. Sections 6.3 and 6.4 compare our method with other approaches. Unless otherwise stated, all the resu lts are obtained with biased \ndata using pairwise SVM\n1.  \n6.1 Performance Comparison of 5 and 12 Coefficients \nWe looked at the performance of 12 coefficients as \nagainst 5 coefficients. The la tter are reported in [3] as \nproviding the best genre classification results when us-ing a Gaussian mixture model (GMM). In contrast\n2, as \ncan be seen in Table 1, the choice of 5 gives worse per-formance than 12 when we use our string kernel based method. This suggests that our technique is exploiting \nthe additional information provided. We obtained a small improvement in performance by adding one addi-tional energy term. The effect of this addition is shown \nin table 1 below. \nTable 1. Results based on 100ms hamming-\nwindowed frame without overlap. Codebook size is 512. \nAccuracy(STD) \nFeatures \nWithout the \nenergy term With  the \nenergy term \n5 MFCC  62.86 (6.14) 63.84 (7.22)\n12 MFCC 67.15 (6.99) 67.65 (7.15)\n6.2 Effects of Codebook Size and Audio Frame \nLength \nThe choice of codebook size relates in part to the struc-\nture and amount of the audio training data. Although a value of 256 is commonly used in speech recognition, there is no commonly agreed va lue in audio genre classi-\nfication. We found that the codebook with 512 VQ sym-bol appears to give a stable good performance. Also, \nfrom Table 2, the average cl assification accuracy based \non 25ms frame length is up to 75%, which is signifi-cantly higher than 68% on 100ms frame length. \nTable 2. Accuracy based on different codebook size and frame length. \nCodebook Size 256 512 1024 \n100ms audio \nframe 64.44 (5.18) 67.65 (7.15) 68.57 (6.48)\n25ms audio frame 71.04 (6.94) 73.69 (5.99) 74.45 (6.01)\n Parameters such as codebook size and frame length \ncan be tuned to affect performance significantly, as \nshown for example by Aucouturier and Pachet [8]. These authors report detailed experiments exploring a \n                                                          \n \n1 SVM is a binary classifier in na ture. Pairwise and one-against-the-\nrest are two prediction strategies  to extend SVM for multi-class prob-\nlem. More details are shown in [18]  \n2 Perhaps the discriminating power of the extra 7 coefficients is blunted \nsomewhat by the constrained nature of a GMM. \n256   \n \n large parameter space, and suggest that there is a ‘glass \nceiling’ of around 65% on performance for the tech-niques which they investigated. \n6.3 Classification Results \nTable 3 shows the details of the confusion matrix gener-\nated by our method with a 25ms Hamming-windowed frame without frame overlap. The columns are placed in an order such that the most confused genres are put close to each other. The ma trix column corresponds to \nthe predicted genre and the row shows the real genre. For example, the value 8 in row 2 column 1 indicates that around 8% of hip-hop music (column 1) was mis-classified as reggae. \nTable 3. Confusion matrix for the 10-genre task.\n \n Hi Re Di Po Me Ro Co Bl Ja Cl\nHi 77 14 5 3 0 2 0 0 0 0\nRe 8 68 6 1 0 5 1 2 0 0\nDi 4 5 73 3 1 5 4 3 0 0\nPo 4 4 7 85 0 6 8 0 0 1\nMe 2 1 0 0 85 5 0 3 0 0\nRo 3 3 9 4 12 54 16 10 4 1\nCo 2 2 0 4 0 12 63 3 4 1\nBl 1 3 0 0 1 8 4 76 40\nJa 0 0 0 0 1 2 4 4 80 4\nCl 0 0 1 0 0 2 0 0 9 94\nThe genres and their two letter codes are: classi-\ncal(Cl), country(Co) , disco(Di), hip-hop(Hi), jazz(Ja), \nrock(Ro), blues(Bl), reggae(Re), pop(Po) and heavy-\nmetal(Me). \nThe general level of performance is high, the main \nexception being for Rock musi c which is fairly evenly \nconfused with a number of other popular music genres: \nthis may indicate that rock is better treated as a super \ngenre in a hierarchical classifier. \n6.4 Comparison with Fixed- n-gram Methods \nThe N-Gram model is a simple and effective approach \nwhich has been successfully app lied in series of tasks for \nmodelling temporal constraints in the sequences. One of its variants called k-spectrum kernel is implemented here for comparison purpose. The k-spectrum of a sequence input is the set of all the k-grams (contiguous) contained in it. Given two sequences, the k-spectrum kernel is de-fined as the inner product of their k-spectrum feature vectors [5]. The results in Table 4 demonstrate the ad-\nvantage of our method over fixed-n-gram model. We attribute this to the one of the principal differences be-tween n-gram approach and ours, i.e. the n-gram restric-tion over the pattern (i.e. feature) length. Our method effectively builds an appropriate variable length set of n-grams from the data, without pre-determining a limit on n. The effect of artificially constraining n is to cut off the \ntail from the distribution shown in figure 2 – not surpris-ingly, this affects performance adversely. In principle of course, an n-gram approach which \nsets n according to the largest word encountered in an \nLZ78 scan of the training set could be used. However, this approach involves a pre-scan, and encounters prob-lems of increasing computa tional cost. There are also \nproblem with increasing sparsity in the resulting kernel matrix for n-gram methods as n gets larger. \n191.8700.3\n177.0\n28.36.7 2.3\n0.0100.0200.0300.0400.0500.0600.0700.0800.0\n123456\nSequential Pattern (i.e. Phrases) LengthOccurrence_Freqeuency\n \nFigure 2 . Averaged phrase length distribution \nwithin a single audio piece (audio signal is ana-\nlyzed with 25ms hamming windowed frame with-\nout overlap). \nTable 4. LZ78Ex compared with fix-n-gram. \nMethod Accuracy (STD) \nLZ78Ex Kernel 74.45 (6.01) \n2-Gram Kernel 70.77 (5.01) \n3-Gram Kernel 62.81 (5.75) \n‘2+3’-Gram Kernel 69.50 (5.64) \n6.5 Comparison with Other Methods \nWe believe that our dataset is similar, if not identical to \nthat used by Tzanetakis fo r the work published in [3]. \nTzanetakis used a comprehensive genre features and obtained an accuracy of around 61(±4)% using a GMM method. Li[9] uses Tzanetakis’s feature set but employs a SVM classifier instead of a GMM: this gives an im-\nprovement in performance of nearly 10%. In the same \npaper, the author explores a new wavelet based feature set called Daubechies Wavelet Coefficient Histogram (DWCHs)\n1, and reports further improvement in per-\nformance achieving 75% to 78% respectively with pair-\nwise and one-against-the-rest SVM [18]. To compare our performance more objectively with those previously published results, we re-implement the method proposed in [9] and use MARSYAS \n2 to extracting the genre fea-\ntures proposed in [3]: MFCCs, FFT, Beat and Pitch. \n                                                           \n1 DWCH consists of the first three moments of the histogram plus the \nsub-band energy. Since not all the frequency sub-bands are informative, \nthe authors in [9] use only selective sub-bands \n2 A public software framework for co mputer audition application that \ncan be downloaded from \nhttp://www.cs.princeton.edu/ ~gtzan/marsyas .html. \n257   \n \n Table 5. Accuracy over the dataset. SVM1 and SVM2  respectively denote the pairwise SVM and the one-\nagainst-the-rest SVM. \nBiased Evaluation Unbiased Evaluation Feature Category Kernel \nFunction \nSVM1 SVM2 SVM1 SVM2 \nGenre Feature (MFCC + \nFFT + Beat + Pitch) RBF 69.96 (5.52) 70.39 (3.73) 72.24 (4.89) 72.60 (4.15)\nDWCH (MFCC + FFT + statistical moments from selective sub-bands) RBF 71.01 (7.15) 70.65 (5.67) 74.90 (3.82) 78.16 (3.62)\nMFCC + Energy  LZ78Ex 74.45 (6.01) 74.45 (5.37) 80.35 (5.64) 80.72 (4.08)\n \nTable 5 display the accuracy of various features and \nclassifiers over the dataset. The bottom row shows that our method outperforms the others consistently in both biased and unbiased evaluation. Comparing the first and second row, DWCH performs better than Tzanetakis’s genre feature, we attribute this to wavelet decomposi-tion scheme which matches the models of sound octave \ndivision for perceptual scales and provides good time and frequency resolution. Li’s works in [9] are repeated \nin our experiments and the best performance of DWCH (74.9% and 78%) is confirmed: note that they occurred only in unbiased evaluation. As shown in second row of Table 5, when training and testing data are constructed in a biased way, DWCH’s performance is underesti-mated as 71%. Apparently, this may be due to the sub-genre effect mentioned previously in section 5.2. \nOur LZ78Ex method is also affected by this problem. \nHowever, by taking advantage of temporal information inherit in the sequence, our method still outperform al-ternative techniques in both situations. \n7 CONCLUSION AND FUTURE WORK \nFeature extraction is important  for music genre classifi-\ncation. To date, much work in this area has used aver-\naged features such as FFT, MFCC, Beat, Pitch or \nDWCHs. These time averages , especially when com-\nbined, can produce good performance. However, as re-ported in [8] there seems to be a ‘glass ceiling’ on per-formance. \nWe have shown that, by utilizing temporal structure \nin a sequence of spectrum-ba sed features, rather than \naverages over sampled windows, we can achieve sig-\nnificantly improved performance. Our new techniques, called the LZ78Ex kernel, is now one of a number of methods which outperforms humans, yielding signifi-cantly better than 70% human musical genre classifica-tion accuracy reported in [6]. \nWe believe that our new highly efficient LZ78Ex \ntechnique for mapping pairs of variable length se-quences to a pair of finite dictionaries, and then using the dictionary pair to compute a similarity measure, is our most important contribution. However, some of our performance may be due to our choice of classifier: we used the increasingly popular SVM (Support Vector Machine) technique. This, and other factors such as the \ndifferent choices of features, and the lack of very high quality labelled reference data corpora, make compari-son with other techniques hard. To take just one aspect, we showed in our experiments that the use of biased evaluations may ‘under-estimate’ the performance by 8% due to the imbalanced representation of sub-genres within the training data. We took the view that the bi-ased approach gives a more reliable estimation since it is hard to seek accurate performance over a full range of instances in the real worl d classifier application. \nFuture work needs to expl ore the effectiveness of \nother feature extraction and fast vector quantization techniques, which may improve our audio genre classi-\nfication system both in accur acy and computational cost. \nIn [19], an Octave-based Sp ectral Contrast feature is \nproposed and proved to be more discriminative than MFCC in terms of genre classification. In [12], a super-vised tree-based quantization is proposed. It attempts to label feature vectors from diffe rent classes with a differ-\nent label. Thus, the new quantizer could discretize the feature space into many more regions than the conven-tional minimum distortion vector quantizers. \n8 ACKNOWLEDGMENTS \nWe thank Tao Li for useful discussions and the http://opihi.cs.uvic.ca/sound/genres contributor for pro-viding us the data set. We also thank Kris West and Stephen Cox for interesting and useful discussion, and Gavin Cawley who gave invaluable technical advice on the properties of our kernel. \nREFERENCES \n[1] Li M. and Sleep M.R. “Melody classification using a similarity metric based on Kolmogorov complexity”. Proceeding of Conference on Sound and Music Computing, Paris, 2004 \n258   \n \n [2] Kristopher West and Stephen Cox. “Features and \nclassifiers for the automatic  classification of musical \naudio signal”, In Proceedings of the 5th International Symposium on Music Information Retrieval, Barcelona, SPAIN 2004. \n[3] George Tzanetakis and Perry Cook, “Music genres \nclassification of audio si gnals”, IEEE Transactions \non Speech and Audio Processing, 2002. \n[4] C. Xu et al., “Musical Genre Classification using \nSupport Vector Machines,” in Proc. ICASSP ’03, \nHong Kong, China, Apr.2003, pp. 429–432. \n[5] Leslie, Eskin et al. “The  Spectrum Kernel: A String \nKernel for SVM Protei n Classification”. In \nProceedings of the Pacific Symposium on Biocomputing, 2002, pp. 564-575. \n[6] D. Perrot and R. Gjerdigen. “Scanning the dial: An exploration of factors in identification of musical style”, in Proc. Society for Music Perception and Cognition, 1999, p.88, (abstract). \n[7] Li M., Chen X., Ma B. and Vitanyi P. “The similarity metric” Proc. 14th ACM-SIAM Symposium on Discrete Algorithms 2003. \n[8] Aucouturier, J.-J. and Pachet F. “Improving Timbre Similarity: How high is the sky?”. Journal of \nNegative Results in Speech and Audio Sciences, \n1(1), 2004. \n[9] Tao Li, Mitsunori Ogihara and Qi Li. “A Comparative Study on Content-Based Music Genre Classification”. In Pro ceedings of Annual ACM \nConference on Research and Development in Information Retrieval (SIGIR 2003), Pages 282-289.  \n[10] H. Deshpande, R. Singh, and U. Nam. “Classification of music signals in the visual domain”. In Proceedings of the COST-G6 Conference on Digital Audio Efffects, 2001. [11] H. Soltau, T. Schultz, and M. Westphal. \n“Recognition of music type s”. In Proceedings of \nthe 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, 1998. \n[12] J. Foote. “Content-based Retrieval of music and audio”. In Multimedia Storage and Archiving \nSystems II, Proceedings of SPIE, pages 138-147, \n1997. \n[13] Li M. and Vitanyi P. “An Introduction to Kolmogorov Complexity and Its Applications”, Springer 1997 \n[14] Bennett C.H., Gacs P., Li M., Vitanyi P.M.B. and Zurek W. “Information Distance”. IEEE \nTransactions on Information Theory , 44:4 (1998), \npp 1407-1423 \n[15] Wei-Ho Tsai and Hsin-Min Wang. “Towards Automatic Identification of Singing Language in Popular Music Recordings”.  In Proceedings of the \n5th International Symposium on Music Information Retrieval, Barcelona, SPAIN 2004 \n[16] Jacob Ziv and Abraham Lempel. “Compression of Individual Sequences via Variable-Rate Coding”. IEEE Transactions on Information Theory , vol. 1T-\n24, no.5, Sep. 1978 \n[17] H.-T. Lin and C.-J. Lin. “A study on sigmoid kernels for svm and the training of non-psd kernels by smo-type methods”. Technical Report, Department of Computer Science and Information Engineering, National Taiwan University. Available at http://www.csie.ntu.edu.tw /cjlin/papers/ tanh .pdf \n[18] C.-W. Hsu and C.-J. Lin. “A comparison of methods for multi-class support vector machines”. IEEE Transactions on Neural Networks, 13(2002), 415-425. \n[19] Igor Karpov, “Hidden Markov classification for musical genres”, Tech. Rep., Rice University, 2002. \n \n \n259"
    },
    {
        "title": "A Hierarchical Approach for Audio Stream Segmentation and Classification.",
        "author": [
            "Wei Liang 0009",
            "Shuwu Zhang",
            "Bo Xu 0002"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415166",
        "url": "https://doi.org/10.5281/zenodo.1415166",
        "ee": "https://zenodo.org/records/1415166/files/LiangZX05.pdf",
        "abstract": "This paper describes a hierarchical approach for fast audio stream segmentation and classification. With this approach, the audio stream is firstly segmented into audio clips by MBCR (Multiple sub-Bands spectrum Centroid relative Ratio) based histogram modeling. Then a MGM (Modified Gaussian modeling) based hierarchical classifier is adopted to put the segmented audio clips into six pre-defined categories in terms of discriminative",
        "zenodo_id": 1415166,
        "dblp_key": "conf/ismir/LiangZX05",
        "keywords": [
            "Audio stream segmentation",
            "Audio classification",
            "Hierarchical approach",
            "MBCR (Multiple sub-Bands spectrum Centroid relative Ratio)",
            "MGM (Modified Gaussian modeling)",
            "Background sounds",
            "Speech classification",
            "Music classification",
            "Noise environment",
            "Real-world TV program recordings",
            "Accuracy and recall rate"
        ],
        "content": "A HIERARCHICAL APPROACH FOR AUDIO STREAM \nSEGMENTATION AND CLASSIFICATION\nWei Liang Shuwu Zhang Bo Xu \nInstitute of Automation, Chi-\nnese Academy of Sciences, \nBeijing, 100080, China \nwliang@hitic.ia.ac.cn  Institute of Automation, Chi-\nnese Academy of Sciences, \nBeijing, 100080, China \nswzhang@hitic.ia.ac.cnInstitute of Automation, Chi-\nnese Academy of Sciences, \nBeijing, 100080, China \nxubo@hitic.ia.ac.cn  \nABSTRACT \nThis paper describes a hierar chical approach for fast \naudio stream segmentation and classification. With this \napproach, the audio stream is  firstly segmented into au-\ndio clips by MBCR (Multiple sub-Bands spectrum Cen-troid relative Ratio) based histogram modeling. Then a MGM (Modified Gaussian modeling) based hierarchical classifier is adopted to put the segmented audio clips \ninto six pre-defined categories in terms of discriminative \nbackground sounds, which is pure speech, pure music, song, speech with music, speech  with noise and silence. \nThe experiments on real TV program recordings showed that this approach has highe r accuracy and recall rate for \naudio classification with a fast speed under noise envi-ronments.  \nKeywords: Audio classification, MBCR, MGM, histo-\ngram \n1 INTRODUCTION \nIn real world, media streams, such as TV programs, \nbroadcasts, etc., are a rich source of multimedia informa-tion, containing audio, speech , text, image, motion, and \ns o  o n .   H o w  t o  b u i l d  a n  efficient mechanism for AV \n(Audio/Video) indexing and retrieval is becoming ex-tremely important, which require automatic understand-ing of semantic contents. As a counterpart of visual in-\nformation in video sequence, audio stream got more at-\ntention recently for its sema ntic content discrimination \ncapability. As the first step of audio content processing, audio stream segmentation and classification are re-quired.  \nAs for audio classification,  most studies are focused \non speech/music/silence/others separation [1,2]. Scheirer and Slaney [1] proposed to use thirteen features in time, frequency, and cepstrum domains and model-based (MAP, GMM, KNN, etc.) classifier, which achieved an accuracy rate over 90% on r eal-time discrimination be-\ntween speech and music. As in general, speech and mu-sic have quite different spectral distribution and tempo-ral changing patterns, it is not very difficult to reach a relatively high level of disc rimination accuracy. Further \nclassification of audio data may take other sounds into consideration besides speech a nd music. Srinivasan, et al \n[3] proposed an approach to detect and classify audio that consists of mixed classes such as combinations of speech and music together with environment sounds. The accuracy of classification is more than 80%. An acoustic segmentation approach was also proposed by Kimber and Wilcox[4], where audio recordings were segmented into speech , sile nce, laughter and non-speech \nsounds. They used cepstral coefficients as features and the hidden Markov model (HMM) as the classifier.  \nIn this paper, we propose a MGM-based (Modified \nGaussian Modeling) hierarch ical classifier for audio \nstream classification. Compared  to traditional classifiers, \nMGM can automatically optimize the weights of differ-\nent kinds of features based on training data. It can raise the discriminative capability of audio classes with lower computing cost. The experiments on real TV program recordings show that the av erage precision of classifica-\ntion can reach above 89%.  \nThe remainder of the paper is organized as follows: \nSection 2 is an overview of proposed segmentation and classification. Section 3 and section 4 explain the details of the segmentation and classification algorithms re-spectively. The experiments is describes in section 5; and final conclusion is given in section 6. \n2 OVERVIEW \nAudio \nStream\nsegm entation\nclassification\nspeech with \nmusic components (SW M)Song (S)pure music \n(PM )Based on background \nsound\nSilence \n(SIL)speech under silent \nenvironment (Pure Speech) sounds under non-silent \nenvironment\nspeech with noise \nbackground (SW N)\n Figure1: The flowchart of segmentation and classifica-\ntion algorithm \nFigure 1 shows the flowchart of proposed audio seg-\nmentation and classification al gorithm. It is a hierarchi-\ncal structure. In the first level, a long audio stream can be segmented into some audio clips according to the change of background s ound by MBCR based histo-\ngram modeling. Then a two level MGM (Modified Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full citation on the first page. \n© 2005 Queen Mary, University of London \n582   \n \n Gaussian modeling) classifier is adopted to hierarchi-\ncally put the segmented audio clips into six pre-defined categories in terms of discriminative background sounds, which is pure speech (PS), pure music (PM), song (S), speech with music (SWM), speech with noise (SWN) and silence (SIL). \n3 SEGMENTATION ALGORITHM \nSince background sounds always change with the change of scenes, the acoustic skip point of an audio stream may be checked by background sounds. As shown in Figure 2, the MBCR feature vectors are firstly extracted from the audio stream. We set a sliding win-dow which consists of two sub-windows with equal time length. The window on input signal is shifted with a \nrange of overlapping. Then two histograms are created \nfrom each sliding sub-windows. The similarity between two sub-windows can be measured by histogram match-ing. The skip point can thus be detected by searching the local lowest similarity below a threshold.   \naudio stream\nslide window\nboundary\nfeature \nextracionfeature \nextracion\nhistogram\nmodeling\ncalculating \nsim ilarityjudgementThreshold\nresulthistogram\nmodelingforward\n Figure2: Block diagram of segmentation algorithm \n3.1 Feature extraction \nConsidering the lower frequency spectrum are too \nsensitive to even a bit of changes of the scenes and \nspeakers, it could cause segmented clips too small. It will have effects on succeeding audio classification. We, \nthus, use Multiple sub-Bands spectrum Centroid relative \nRatio (MBCR) [5] over 800Hz as basic feature. This feature may depict centroid movement trend in a time-frequency-intensity space. Its mathematical description \ncan be described as follows.   \n)),((max),(),(\n:1jiSCjiSCji SCR\nNj==                                   (1) \n∑\n==N\nkki FrmEnji FrmEnjfjiSC\n1),(),( *)(),(                              (2) \nwhere ),(ji SCR  is MBCR of the ithframe and the \njthsub-band, ),(jiSC  is the frequency centroid of the ithframe and the jthsub-band, and N denotes the num-\nber of frequency sub-bands. The element of )(jf  is the \nnormalized central frequency. \n)),( log(),()(\n)(ωωω\nωdiF ji FrmEnj\njH\nL∫=                   (3) \nwhere )(jLω  and )(jHω are lower and upper bound \nof  sub-band j respectively, ),(ωiF represent denotes \nthe Fast Fourier Transform (FFT) at the frequency ω \nand frame i, and  |),(|ωiF  is square root of  the power \nat the frequency ω and frame i.  \n3.2 Histogram modeling and similarity measurement \nAfter feature extraction, we  need to train model for \neach sub-window. Some modeling approaches, such as \nGMM, HMM, SVM, have al ready been employed for \naudio modeling. However, b ecause of computationally \nexpensive processing, it is hard to meet the speed de-\nmand of quick audio segmentation.  Histograms can be \nused as a type of non-parametric signal model. It doesn't need computationally expensive processing and it is relatively stable under adverse environments [5].In or-der to remove the influence of noises, the feature vector firstly need to be quantized (VQ) before modeling his-togram.    \nThe similarity distance between the two sub-window \nfeature vector histogram can be measured by histogram intersection. The histogram intersection for a window is defined as \n∑\n==L\njj j\nn nhnhLnhnhS\n12 1 2 1 ))(),( min(1))(),((         (4) \nwhere )(1nh and )(2nh  is the histogram of two sub-\nwindow at frame n; )(1nhjand )(2nhj  represent the \nvalue of two sub-window histogram’s jth bins at frame \nn  respectively; Ldenotes the number of bins. \n4 CLASSIFICATION ALGORITHM \n4.1 Feature Extraction \nThe classification process consists of two levels. The \nfirst level is used to discriminate the pure speech and \nnon-pure speech by setting thre shold simply. The second \nlevel adopts MGM to classify the non-pure speech based \non six kinds of features. On the first level of classification, we adopt Energy Change Ratio (ECR), Silence Ratio (SR) and Spectrum \nEntropy(SE) as basic features . Its mathematical descrip-\ntion can be described as follows. \n1)))1( ),( max()1( )((\n)(1\n12\n−−−−\n=∑−\n=\nNiEiEiEiE\nk ECRN\nik kk k\n               (5) \nwhere )(k ECR  is average energy change ratio of the \nkth time  clip; )(iEk denotes the ith frame energy of \n583   \n \n the kthtime clip; And ), max( yx denotes the maximum of \nx and y. \n)()()(k nTotalFrmskms nSilenceFrkSR=                (6) \nwhere )(kSR  is silence ratio of the kth  clip; \n)(kms nSilenceFr  denotes the number of silence frames \nin the kthclip. \n))),( ((log*),( )(20fn TFDP fnP nSEfFm\nff∑==          (7)                                                                                      \nwhere  \n∑==Fm\nff\nfn TFDfn TFDfn TFDP\n0),(),()),( (                        (8) \nwhere, ),(fn TFD represent the energy of the signal at \ntime frame n and frequency index f; andmFrefers to the \nmaximum frequency. \nOn the second level of the classification, the features used in work are modified fro m those describe in [4-5]. \nSpectrum flux   \n[]∑∑−\n=−\n=+−−+−−=1\n11\n12)),1(log()),(log()1(*)1(1N\nnK\nkknA knAK NSF δ δ (9)                                                                \nSpectrum Centroid: \n       \n∑∑\n===Fm\nfFm\nf\nfn TFDfn fTFD\nnf\n00\n),(),(\n)(                                 (10) \nBand-Width: \n∑∑\n==−\n=Fm\nfFm\nf\nfn TFDfn TFDnf f abs\nnB\n00\n),()),( )( (\n)(             (11) \nHZCRR (High Zero-Crossing Rate Ratio): \n[]∑−\n=+ − =1\n01) 5.1)( sgn(21N\nnavZCR n ZCRNHZCRR  (12) \n∑−\n==1\n0)(1N\nnn ZCRNavZCR                                        (13) \nLSTER (Low Short-Time Energy Ratio ): \n[]∑−\n=+ − =1\n01))( 5.0 sgn(21N\nnn STE avSTENLSTER    (14)   \n∑−\n==1\n0)(1N\nnn STENavSTE                                         (15) \nwhere, STE(n) denotes the short-time energy of nth \nframe.  \nSpectrum Entropy   [see formula 7-8] \n \nSince the dynamic ranges of these features differ a lot, \nwe normalize them by their standard deviation, which \nare computed based on the training data. \n4.2 MGM classifier \nCompared to traditional Gaussian model (GM), \nMGM has modified the drawback  that all features are of \nthe same weight. The MGM classifier consists of two processes: model training and similarity measurement.  Model Training  \nFirstly GM of each dimensi on feature is created for \neach audio class by statistics. Then the model parame-\nters, imμ and imσ, can be calculated. Here  imμ andimσ \nrepresent the mean and variance of the mth dimension \nfeature of audio class i respectively. \nBased on imμ and imσ of all audio classes, we can \ncalculate the weight of all f eatures. For a feature weight \nmψ, this can be calculated by the following equation: \n       \n∑\n==M\nimm\nm\nww\n1ψ                                                (16)  \nwhere, \n \n∑∑∑\n=−\n=+=−\n=N\nii\nmN\niN\nijj\nmi\nm\nmw\n11\n11| |\nσμμ                               (17) \n∑−\n==1\n0)(1N\nji\nmi\nm jfNμ                                        (18) \nNj fN\nji\nmi\nm\ni\nm∑−\n=−\n=1\n02) )( ( μ\nσ                     (19) \nM m ,....3,2,1=  \nwhere )(jfi\nm  denotes jth feature value of  the ith \ndimension feature of  audio class m. M is the dimen-\nsion of the features, Nis the number of audio classes. \nSimilarity measurement  \nBased on the feature vector []Mf ff f ,...,,2 1= ,the \nsimilarity distance between feature f and class i can be \ncalculated by the following equation: \n)(maxiiiθ=                        (20) \nwhere \n∑\n==M\nmi\nm m i\n1ηψθ                                                 (21)  \n22\n2) (\n21 i\nmi\nm mf\ni\nmi\nm eσμ\nσπη−−\n=             (22) \nwhere iθ represent the similarity between the feature \nvector fand audio class i. \n5 EXPERIMENTS  \nWe conducted a series of experiments based on pro-\nposed audio segmentation a nd classification approach. \nThe experimental platform we used is a workstation \nPentium4 2.4G CPU, 256M memory. The performance \nwas evaluated on the recordings of real TV program. \nThe segmentation and cla ssification results were \nevaluated by the recall rate δ , accuracy rate ξ , and \naverage precision η. These are defined as \n584   \n \n   be                 \ncorrect should that objectsof numbertheobjects correctlyof numberthe=δ     (23)        \nobjectsgetallof numbertheobjects correctlyof numberthe\n             =ξ               (24) \nξ) 0.5*(δδ*ξη+=                                                      (25) \nWe randomly picked out 6 hours TV news program \nfrom CCTV1 (China Central Television Station Channel \n1) as test set of audio stream. With MBCR-based seg-\nmentation, the testing audio stream was split into about 12000 audio clips. The segmentation accuracy was cal-culated by the alignment of segmenting result and hu-man examination. When the background sound change acutely or there is a silent interval of more than 0.6s, we consider there should be a skip point. As a result, the segmentation accuracy is 92.5%, recall ratio is 93.2%, and the average precision is 92.85%. \nWe pre-defined six categories as audio classes, which \nis pure speech (PS), pure music (PM), song (S), speech with music (SWM), speech with noise (SWN) and si-lence (SIL). Table1 and Tabl2 give the first level and the second level classification results.  \nTable1 gives the result of first level classifying. It \nfirstly puts the audio clips into three classes, pure speech, silence and others. Th e others is further classi-\nfied into four classes: speech with music, song, pure music, and speech with noise, in the second level classi-fying. The result is shown in Table 2. \nSince the performance of classification is partially \ndependent on the result of segm entation, we also give \nthe results based on simple equal time segmentation in comparison with MBCR-based segmentation. In the mean time, we also comp ared MGM with traditional \nGM approach.  \nThe results showed that  MGM classifier with \nMBCR-based segmentation can achieve an average pre-\ncision of about 89%. It outperforms GM classifier with \neither equal time segmentation or MBCR-based seg-mentation. Table1: The results of first level classification \nAlgorithm Audio \nType Accu-racy Recall Preci-sion \nPure speech (PS) 85.15% 87.52% 86.32%\nSilence (SIL) 97.10% 86.14% 91.29%Equal Time(2s) \nOthers \n77.95% 95.08% 85.67%\nPure speech (PS) 91.33% 93.65% 92.47%\nSilence (SIL) 98.22% 92.97% 95.52% MBCR \nOthers  85.68% 95.45% 90.3%\nTable2: The results of the second  level classification \nAlgorithm model type Acc. \n(%) Rec. \n(%) Pre.\n(%)\nEqual GM SWN 44.55 72.77 55.2SWM 65.12 56.72 60.6\nS 61.05 86.56 71.6Time (2s)  \nPM 79.86 61.15 69.2\nSWN 71.42 74.3 72.8\nSWM 69.43 74.19 71.7\nS 84.88 89.46 87.1GM \nPM 85.31 82.54 83.9\nSWN 83.08 87.48 85.2\nSWM 73.28 83.3 77.9\nS 95.44 93.78 94.6MBCR \nMGM\nPM 97.1 86.93 91.7\nHere,   SWM: speech with music, SWN: speech with \nnoise  \n6 CONCLUSION \nIn this paper, we have introduced a MBCR-based seg-\nmentation and MGM hierarchical classification ap-proach for audio stream scene processing. The experi-\nmental results reported here are meant to show the \npromise of applying the method in AV stream scene classification. Based on this  work, we will further ex-\nplore more robust features and modeling approaches for \nfiner column classification on TV progra m.                                  \n7 ACKNOWLEDGEMENTS \nThis work was partially supported by the National Natu-\nral Science Foundation of China (NSFC) under the grant No. 60475014 and National Hi-tech Research Plan under \nthe grant No. 2003AA115520 & 2005AA114130.. \nREFERENCES \n[1] Scheirer E., Slaney M. (1997). \"Construction and \nEvaluation of a Robust Multifeature Music/Speech \nDiscriminator\", Proc. of ICASSP97, vol II,pp \n1331-1334,April 1997. \n[2] Williams G. and Ellis D. (1999), \"Speech/music \ndiscrimination based on posterior probability \nfeatures\", Proc European Conf. on Speech \nCommunication and Tec hnology, Budapest, \nHungary,  pp.687-690,Sept.1999. \n[3] Srinivasan,S., Petkovic,D., Ponceleon, D. (1999), \n\"Towards robust features for classifying audio in \nthe Cue Video System\". Proc. of the 7th ACM Intel. Conf. on Multimedia'99, pp.393-400,1999 \n[4]\n Kimber D. and Wilcox L. (1997), \"Acoustic \nsegmentation for audio browsers\". Computing \nScience and Statistics. Vol.28. Graph-Image-\nVision. Proceedings of the 28th Symposium on the \nInterface. Interface'96, (295-304),1997. \n[5] Lu, L., Zhang, H.-J., Li, S. (2003), “Content-based \naudio classification and segmentation by using \nsupport vector machines”, Multimedia Systems \n8:482-492(2003). \n585"
    },
    {
        "title": "A Histogram Algorithm for Fast Audio Retrieval.",
        "author": [
            "Wei Liang 0009",
            "Shuwu Zhang",
            "Bo Xu 0002"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415812",
        "url": "https://doi.org/10.5281/zenodo.1415812",
        "ee": "https://zenodo.org/records/1415812/files/LiangZX05a.pdf",
        "abstract": "This paper describes a fast audio detection method for specific audio retrieval in the AV stream. The method is a histogram matching algorithm based on structural and perceptual features. This algorithm extracts audio features based on human perception on the sound scene and locates the special audio clip by fast histogram matching. Experimental results based on the advertisement detection in TV program showed that the algorithm can achieve a very high overall precision and  recall rate both about 97% with very fast search time about 1/40 on real time.",
        "zenodo_id": 1415812,
        "dblp_key": "conf/ismir/LiangZX05a",
        "keywords": [
            "audio detection",
            "specific audio retrieval",
            "AV stream",
            "histogram matching",
            "structural and perceptual features",
            "sound scene",
            "fast search time",
            "real time",
            "overall precision",
            "recall rate"
        ],
        "content": "A HISTOGRAM ALGORITHM  FOR FAST AUDIO RETRIEVAL\nWei Liang Shuwu Zhang Bo Xu \nInstitute of Automation, Chi-\nnese Academy of Sciences, Beijing, 100080, China \nwliang@hitic.ia.ac.cn  Institute of Automation, Chi-\nnese Academy of Sciences, \nBeijing, 100080, China \nswzhang@hitic.ia.ac.cnInstitute of Automation, Chi-\nnese Academy of Sciences, \nBeijing, 100080, China \nxubo@hitic.ia.ac.cn  \nABSTRACT \nThis paper describes a fast audio detection method for \nspecific audio retrieval in the AV stream. The method is a histogram matching algorithm based on structural and perceptual features. This algorithm extracts audio fea-\ntures based on human perception on the sound scene and locates the special audio clip by fast histogram matching. Experimental results based on the advertisement detec-\ntion in TV program showed that the algorithm can \nachieve a very high overall precision and  recall rate both about 97% with very fast search time about 1/40  on real time.  \nKeywords: audio Retrieval , histogram.  \n1 INTRODUCTION \nTV broadcasting is a rich multimedia information source. \nThere are large amounts of advertisements (Ad) in TV programs of different channels per day. Traditionally, the work of Ad monitoring is done mainly by human checking. However, it is really a stuffy work for human to listen and watch so many TV plays chronically. Thus, how to precisely detect and locate these Ad segments \nfrom long Audio and Video (AV) recordings becomes a \nreal requirement for advertisers. \nThis paper address the problem of detecting and lo-\ncating sound objects from a stream of TV audio data quickly, while using computationally inexpensive proc-essing. This has potential applications for multimedia data management. \nIn recent years, there is an increasing interest on AV \ndata retrieval. Relevant techniques on audio feature ex-traction, modeling and search algorithm have been widely studied. For instance,  Wold et al. employed \nloudness, brightness, pitch, timbre as audio perceptual features [1]; Liu et al. used subband energy ratio feature [2]; Foote also proposed 12 dimensions MFCC and En-ergy features [4]; some audio modeling approaches are GMM, HMM, VSM, Histogram, and so on.  \nAimed at TV Ad monitoring, this paper describes a fast histogram based audio retrieval algorithm. This al-gorithm extracts audio features based on human’s sense of sound and locates the specific audio clip by fast his-togram matching. Experimental results based on TV Ad detection showed that the algorithm can achieve a very \nhigh overall precision above 96% with very fast search time. \nThe paper is organized as follows: Section 2 explains \nthe details of the algorithm including audio feature ex-traction and modeling. Section 3 introduces the experi-ment based on TV Ad monitoring. And section 4 gives the conclusion. \n2 AUDIO RETRIEVAL ALGORITHM \n2.1 Overview \nwindow\nwindow\nskip width\nfeature extraction\ncreating histogram\ntemplate \nmatchingreference audiocalculating \nskip width\ndecisionthreshold\nresultinput \naudio\nFigure1: Block diagram of the Audio Retrieval Algo-\nrithm \nFigure 1 outlines the block diagram of proposed au-\ndio search algorithm. Firstly , the feature vectors are \nextracted from both reference audio and test audio. The window on the input signal is shifted with a range of overlapping. The Window length may be the same as \nthe reference audio duration. Then, the histogram is \ncreated from the feature vector s. In the next step, the \ntemplate matching is carried out to detect and locate the reference audio segment from input audio. \n2.2 Feature Extraction \nAs mentioned in above section, there are some different \ntypes of audio features already used in audio retrieval. For example: Wold et al. used loudness, brightness, pitch, timbre [1]; Liu et al. used subband energy ratio [2]; Foote used 12 dimensions MFCC and Energy [4]. These features have reported a good precision in quiet envi- Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full citation on the first page. \n© 2005 Queen Mary, University of London \n586   \n \n ronment. However, it could be relatively distorted in \nstrong noise environments. It, thus, is necessary to de-\nsign a group of features, which could be beneficial for both computing cost and noise resistance, for robust modeling under noise environment. \nIn general, multimedia signal is a mixed signal of \nmany different sources. Human ear has the nature to \ndistinguish individual sound from mixed sources. This \nfunction of human ear is called stream segregation. Stream segregation involves two steps, which are de-composing the signal into its continent parts (partials) and grouping these parts into streams - one stream for each sound source. At a basic level, one can model audio representation in the human mind as a series of peak amplitude tracks in a time-fre quency-intensity space [4].  \nConsidering the unstable factor in TV signals, we \nadopt multiple bands energy relative ratio as basic audio feature. This type of feature may depict energy move-ment trend in a time-frequency-intensity space. Its mathematical description can be described as follows.   \nAn audio feature vector  \n)(n feature   is written as \n))(),(()( ngnf n feature =                                     (1) \n))( ),...,(),(),(()(3 2 1 nf nfnfnf nfM =                    (2) \n))( ),...,(),(),(()(3 2 1 ng ngngng ngM =                        (3) \nwhere n is the  time frame. M denotes the num-\nber of frequency sub-bands. An element of )(nf  is the \nnormalized short-time power spectrum, which is given \nas \n)( )( )( nEn nfi i ×=α                             (4) \nAn element of )(ng  is the normalized short-time \npower spectrum change ratio by time, which is given as \n)( )( )( n ECRn ngi i ×=β               (5)\n                 \n)1(/))1( )(()( − −−= nE nEnE n ECRi i i i           (6) \nwhere )(nEi  denotes the energy of output of the \nI-th sub-band filter at n-th frame. Because short -time \nenergy is sensitive to high level voltage, this algorithm \nuses short-time average amplit ude to carve the change of \nsignal amplitude, which is given as \n∑−+\n==1*)1(\n*|)(| )(L n\nLnti i tx nE               (7) \nwhere, L is the length of a frame, and )(txidenotes the \namplitude of ith sub-band   at sampling point t . And \n)(nα and )(nβ is a normalized constant defined as  \n        ))((max1)(nEn\nii=α                                        (8) \n        ))( (max1)(n ECRn\nii=β                                    (9) \n Figure 2 shows the short-time energy curve of an audio clip that is processed through multiple band pass filters. \n Figure 2: the short-time ener gy curve of an audio clip \nafter passing multiple band pass filters  \nIn order to reduce the bad influence of noise and \nvolume and lift the perceptual features of the audio clip, the short-time energy curve need to be filtered by low pass filters again. Figure 3 shows the result filtered by \nlow pass filters. \n \nFigure 3: the curve of short-time energy curve through \nlow pass filters.  \nFurthermore, normalization across frequency bands \nis taken to delete the influence of absolute energy. Fig-ure 4 shows the result of normalization across frequency bands. \n \nFigure 4: the result of normalization across frequency bands \n2.3 Histogram Modeling \nAfter feature extraction, we need to train model for each \naudio clip. Some modeling approaches, such as GMM, \nHMM, SVM, have already been employed for audio \nmodeling. However, because of computationally expen-\nsive processing, it is hard to meet the speed demand of quick audio search and retrieval. Histograms can be used as a type of non-parametric signal model for both the reference and input signals over a shifted window. It \n587   \n \n doesn't need computationally expensive processing \nwhile it is relatively stable under adverse environments [5]. We, thus, adopt histogram modeling for specific audio detection.      . \nFor the sake of removing the influence of noises, \nthe feature vector, firstly, n eed to be quantized (VQ) \nbefore modeling histogram. We use the codebook of VQ \nto build histogram. The similarity distance between the \nreference and input feature vector histogram can be measured by histogram intersection. The histogram in-tersection for a window is defined as \n∑\n==L\njT\njR\njT R\nn nhhLnhhS\n1))(, min(1))(,(                     (10) \nwhere Rh is the histogram for the reference; )(nhT\nj  is \nthe histogram started from the i-th frame; and Ldenotes \nthe number of bins. \n2.4 The Prediction of Similarity Upper Bound and Skip \nWidth \nAs the window for input signal shifts forward in time, \nthe similarity based on reference and input feature vector histograms changes with regard to the correlated over-\nlapping between reference and object segment in input stream. We, thus, may predict the next upper bound of the similarity in terms of current value. The upper bound \non \n),(T\niR\nihhS   can be defined as: \niT\niR\niT\niR\ni ubpn nnhhS nhhS1 2))1(,( ))2(,(−+ =              (11) \nWhere )1(nhT\ni  and )2(nhT\ni  are the histograms for \nwindows started from n1 and n2 frame respectively, \n2 1n n<  ;ipdenotes the total number of frames in each \nhistogram. When the window is shifted the n2-th frame, \nthe similarity should be no larger than  \n))2(,( nhhST\niR\ni ub . We, thus, may set the threshold to \nskip the durations where the similarity is lower than the threshold. Using Eq.(9), the skip width can be calculated \nas: \n⎩⎨⎧ +−=11)) (( S p floorwiθ otherwiseSif ) (θ<                       (12) \nWhere )(x floor  means the greatest integral value less \nthan x; θ is the threshold; and Sdenotes the value of \ncurrent similarity. \n3 EXPERIMENTS \nThe experiments on TV Ad retrieval have been con-\nducted based on the recordings of real TV broadcast. We randomly picked out 200 different commercial Ad tem-\nplates of different durations from real TV broadcasting \nand edited a 20 hours’ test set of TV program from six channels. In the audio feature extraction, the audio of recording was first digitized at 8 kHz sampling fre-\nquency and 16 bit quantization accuracy. The experimental platform  we used was a work-\nstation Pentium4 2.4G CPU, 256M memory. The per-\nformance was evaluated with regard to search speech \nand accuracy under the recording of real TV broadcast. \n \n3.1  Search Speed  \nThe time cost for the search c onsists of two parts: (1) the \ntime cost during feature extr action, and (2) the search \ntime based on the extracted feature vectors.  \n(1) The feature extraction in the experiment was per-\nformed on 20 hours’ testing TV program and 200 commercial ads. It took about 240 seconds of \nCPU time totally;  \n(2)\n The search time depends on many factors, such as \nthe length of reference clip, the length of shifted window of input signal, numbers of histogram \nbins, threshold, and so on. Averagely, it takes \nabout 10-15 seconds for each reference Ad clip to search the number of occurrences through whole testing TV program.   \n \n3.2 Search Accuracy \nThe search accuracy was ev aluated by the recall rate δ , \nprecision rate ξ , and average accuracy η. These are \ndefined as \n \nretrieve d be shouldthat objectsof numbertheobjects retrieved correctlyof numberthe\n            =δ            (13) \nobjects retrievedallof numbertheobjects retrieved correctlyof numberthe\n          =ξ            (14)                         \n2ξδη+=                  (15) \nTable 1 lists the performance of histogram search in \ncomparison with correlation coefficient matching. It has \nshown that the histogram s earch algorithm can achieve a \nvery high precision about 97% on average with very fast search time about 1/40 times of real time within 200 Ad \naudio clips. \nTable 1: The performance of histogram search com-\npared to correlative coefficient matching. \nAlgo-\nrithm Len. of Ads (sec) Preci-sion (%) Recall  (%) Accu-racy  (%) CPU time \n<=5  64.8% 78.2% 71.5% \n6-10 91.1% 88.5% 89.8% \n11-20  97.1% 85.8% 91.4% \n>20  100% 89.7% 94.8% Cor-rel. Coeff.On \nAve. 88.3% 85.6% 86.89%21hr5\n6m \n<=5  93.0% 94.2% 93.6% \n6-10  95.3% 96.0% 95.7% \n11-20 99.2% 97.5% 98.4% \n>20. 100% 98.2% 99.1% Histo-gram \nOn Ave. 96.9% 96.5% 96.7% 30m14s. \n* search time is the cost of detecting 200 Ads through 20 hrs TV program \n \n588   \n \n 4 CONCLUSION \nThis paper has discussed a fast histogram search algo-\nrithm that can quickly detect  and locate a reference audio \nsegment in a long audio stream. The experiment based \non TV Ad detection showed that the performance on \nboth precision and speed are significantly acceptable for \nreal applications. This algorithm has been truly applied in the Ad monitoring system by the Bureau of Beijing \nBusiness Administration.  \nA potential improvement point of the algorithm is \nthat since the disturbance of AV signals in transmission \nline as well as the volume change of acceptor, the audio signal would be distorted in some sense. We are plan-ning to study a kind of spectrum masking technique of robust feature selection for quick audio retrieval. We \nalso will further revise the algorithm enable to search a segment in a reference clip by dynamically changing the length of search window.   \n \nAcknowledgement \nThis work was partially supported by the National \nNatural Science Foundation of China (NSFC) under the \ngrant No. 60475014 and National Hi-tech Research Plan \nunder the grant No. 2003AA115520 & 2005AA114130. \nREFERENCES \n[1] Wold, E, Blum, T, Keislar, D, and Wheaton, J (1996), \n“Content-based classification, search and retrieval of audio”, IEEE Multimedia Mag.,Vol. 3, pp.27-36, \nJuly 1996. \n[2]\n Liu. Z.,  Huang. J.,  Wang Y.,  and Chen T. (1997), \n“Audio feature extraction and analysis for scene classification,” in I EEE Signal Processing,1997. \n[3]\n Foote J. et al. (1997), “Content-based retrieval of music and audio,”in Proc.SPIE Multimedia Storage Archiving Systems II, vol.3229,C.C.J.Kuo et al., \nEds.,1997,pp.138-147. \n[4]\n Ellis D. P. W. ,  and Vercoe B. L. (1992), “A \nPerceptual Representation of Audio for Auditory signal Separation”, presented at the 23th meeting of \nthe Acoustical Society of America, Salt Lake City, \nMay 1992. \n[5]\n Smith G., Murase H., and Kashino K. (1998): “Quick \nAudio Retrieval Using Active Search”, Proc. of ICASSP-98, Vol.6(1998). \n[6]\n Han K.-P., Park Y.-S., Jeon S.-G., Lee G.-C., and Ha Y.-H (1998). “Genre classification system of TV sound signals based on a spectrogram analysis”. \nIEEE Transactions on Consumer \nElectronics ,44(1):33-42, February 1998. \n[7]\n Jiang D.-N., Lu L., Zhang H.-J., Tao J.-H., and Cai \nL.-H. (2002), “Music Type classification by spectral contrast feature”. Proceedings 2002 IEEE \nInternational Conference on Multimedia and Erpo (Cat.No.02TH8604), 1:113-116, August 2002. \nConference date 26-29 Aug. 2002. \n[8]\n Kimber D. and Wilcox L. (1997), “Acoustic \nsegmentation for audio browsers”. Computing Science and Statistics. Vol.28. Graph-Image-Vision. \nProceedings of the 28\nth Symposium on the Interface. \nInterface’96, (295-304),1997. \n[9] Albiol A., Torres L., and Delp E. J. (2003), “The \nindexing of persons in ne ws sequences using audio-\nvisual data”. Proceedings of the 2003 IEEE \nInternational Conference on Acoustics,Speech, and \nSignal Processing, ICASSP’03, 3:137-140, April 2003. \n[10]\n Abe M. and Nishiguchi N. (2002), \"Self-\noptimized spectral correlation method for \nbackground music indentif ication\". Proceedings \n2002 IEEE International Conference on Multimedia \nand Expo(Cat. No02TH8604), 1(1):333-336, August 2002. \n[11]\n Chien J.-T. (2003), “Linear regression based \nBayesian predictive classification for speech \nrecognition”. IEEE Transactions on Speech and \nAudio Processing, 11(1):70-79,January 2003. \n[12] Gouyon F., Pachet F., and Delerue F. (2000), \n\"On the use of zero-crossing rate for an application \nof classification of percu ssive sounds\". Proceedings \nof the COST G-6 Conference on Digital Audio \nEffects (DAFX-00), December 2000. \n \n  \n589"
    },
    {
        "title": "Evaluation of Feature Extractors and Psycho-Acoustic Transformations for Music Genre Classification.",
        "author": [
            "Thomas Lidy",
            "Andreas Rauber"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416856",
        "url": "https://doi.org/10.5281/zenodo.1416856",
        "ee": "https://zenodo.org/records/1416856/files/LidyR05.pdf",
        "abstract": "We present a study on the importance of psycho-acoustic transformations for effective audio feature calculation. From the results, both crucial and problematic parts of the algorithm for Rhythm Patterns feature extraction are identified. We furthermore introduce two new feature representations in this context: Statistical Spectrum Descriptors and Rhythm Histogram features. Evaluation on both the individual and combined feature sets is accomplished through a music genre classification task, involving 3 reference audio collections. Results are compared to published measures on the same data sets. Experiments confirmed that in all settings the inclusion of psycho-acoustic transformations provides significant improvement of classification accuracy. Keywords: content-based retrieval, psycho-acoustic, audio feature extraction, music genre classification 1",
        "zenodo_id": 1416856,
        "dblp_key": "conf/ismir/LidyR05",
        "keywords": [
            "psycho-acoustic transformations",
            "audio feature calculation",
            "Rhythm Patterns feature extraction",
            "Statistical Spectrum Descriptors",
            "Rhythm Histogram features",
            "music genre classification",
            "content-based retrieval",
            "psycho-acoustic",
            "audio feature extraction",
            "music genre classification"
        ],
        "content": "EVALUATION OF FEATURE EXTRACTORS AND PSYCHO-ACOUSTIC\nTRANSFORMATIONS FOR MUSIC GENRE CLASSIFICATION\nThomas Lidy Andreas Rauber\nVienna University of Technology\nDepartment of Software Technology and Interactive Systems\nFavoritenstrasse 9-11/188, A-1040 Vienna, Austria\n{lidy, rauber }@ifs.tuwien.ac.at\nABSTRACT\nWe present a study on the importance of psycho-acoustic\ntransformations for effective audio feature calculation.\nFrom the results, both crucial and problematic parts of\nthe algorithm for Rhythm Patterns feature extraction are\nidentiﬁed. We furthermore introduce two new feature rep-\nresentations in this context: Statistical Spectrum Descrip-\ntors and Rhythm Histogram features. Evaluation on both\nthe individual and combined feature sets is accomplished\nthrough a music genre classiﬁcation task, involving 3 ref-\nerence audio collections. Results are compared to pub-\nlished measures on the same data sets. Experiments con-\nﬁrmed that in all settings the inclusion of psycho-acoustic\ntransformations provides signiﬁcant improvement of clas-\nsiﬁcation accuracy.\nKeywords: content-based retrieval, psycho-acoustic,\naudio feature extraction, music genre classiﬁcation\n1 INTRODUCTION\nDigital music databases are continuously gaining popular-\nity both in terms of professional repositories and personal\naudio collections. Ongoing advances in network band-\nwidth and popularity of internet services anticipate even\nfurther growth of the number of people involved with au-\ndio libraries. However, organization of large music reposi-\ntories is a tedious and time-intensive task, especially when\nthe traditional solution of manually annotating semantic\ndata to the audio is chosen. Fortunately, research in music\ninformation retrieval has made substantial progress in re-\ncent years. Approaches from music information retrieval\naccomplish content-based audio analysis and are funda-\nmental to tasks like browsing by similarity, automatic re-\ntrieval, organization or classiﬁcation of music. Content-\nbased descriptors form the base for these tasks and are\nable to add semantic meta-data to music. However, there\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonis no absolute deﬁnition of what deﬁnes the content, or\nsemantics, of a piece of audio. It is a matter of the spe-\nciﬁc application domain – and also of ongoing research –\nwhether feature extractors lay their focus on musical el-\nements such as timbre, pitch, tempo, energy distribution,\nrhythm or other content.\nAccording to Aucouturier and Pachet (2003) musical\ngenre is probably the most popular metadata for the de-\nscription of music content. Music industry promotes the\nuse of genres and home users like to organize their au-\ndio collections by this annotation. Consequently, the need\nof automatic classiﬁcation of audio data into genres in-\ncreased substantially, as did the number of researchers ad-\ndressing this problem. Besides recent advances in genre\nclassiﬁcation there is still the question, what exactly de-\nﬁnes a genre, or whether it is mainly dependent on a user’s\nexperience and taste. Aucouturier and Pachet (2003) deal\nwith this question and the problem of inconsistent genre\ntaxonomies.\nThough the concept of musical genre might be ill-\ndeﬁned, recent approaches that use audio feature extrac-\ntion combined with machine learning techniques achieve\npromising results. Genre classiﬁers typically work well\nwith clearly described, well-distinguishable genres.\nOne of our main contributions to research in this area\nhas been a feature extractor that describes rhythmic struc-\nture on a variety of frequency bands considering psycho-\nacoustic phenomenons according to human perception.\nThe feature set, called Rhythm Patterns (RP), is neither\na mere description of rhythm nor does it represent plain\npitch information. Rather, it describes the modulation of\nthe sensation of loudness for different bands, by means of\na time-invariant frequency representation. We created an\napproach making the Rhythm Patterns feature set audible,\nenabling humans to get a notion of the calculated features\n(Lidy et al., 2005). An overview of the entire SOMeJB\nsystem is given by Neumayer et al. (2005).\nOne of the primary characteristics of the feature set\nis the integration of a range of psycho-acoustic process-\ning steps. A question that was raised several times by\nreviewers and fellow researchers in this ﬁeld was on the\nnecessity and impact of these transformations. The repli-\ncation of the human auditory system for computing sim-\nilarity between signals was questioned. In this paper we\naddress this issue, performing a range of experiments on\n3 standard music IR reference collections, evaluating the\n34impact of the different psycho-acoustic processing steps.\nWe furthermore introduce 2 new feature representations in\nthis context and evaluate their performance both individ-\nually as well as in combination with the Rhythm Patterns\nfeatures.\n2 RELATED WORK\nThe domain of content-based music retrieval experienced\na major boost in the late 1990’s when mature techniques\nfor the description of audio content became available.\nFrom that time on a range of researchers has been working\non different methods for content-based retrieval. As man-\nifold as the feature calculation approaches are the similar-\nity measures and the evaluation methods. Here, we brieﬂy\nreview the major contributions on content-based feature\nextraction from audio.\nOne of the ﬁrst works on content-based retrieval of au-\ndio (Foote, 1997) presents a search engine which retrieves\naudio from a database by similarity to a query sound. For\nsimilarity, two different distance measures are described\nin the paper.\nAn early work on musical style recognition (Dannen-\nberg et al., 1997) investigates various machine learning\ntechniques applied for building style classiﬁers.\nLiu and Huang (2000) propose a new approach for\ncontent-based audio indexing using Gaussian Mixture\nModels and describe a new metric for distance measur-\ning between two models. Logan and Salomon (2001)\nperform content-based audio retrieval based on K-Means\nclustering of MFCC features and deﬁne another novel dis-\ntance measure for comparison of descriptors. Aucouturier\nand Pachet (2002) introduce a timbral similarity measure\nbased on Gaussian Mixture Models of MFCCs, but also\nquestion the use of such measures in very large databases\nand propose a measure of “interestingness”.\nPampalk et al. (2003) conduct a comparison of several\ncontent-based audio descriptors on both small and large\naudio databases, including those of Logan and Salomon\n(2001) and Aucouturier and Pachet (2002) as well as a fea-\nture set called Fluctuation Patterns, similar to the Rhythm\nPatterns we used in our experiments. They report that in\nthe large scale evaluation the simple spectrum histograms\noutperform all other descriptors.\nLi et al. (2003) propose Daubechies Wavelet Coefﬁ-\ncient Histograms as a feature set suitable for music genre\nclassiﬁcation. The feature set characterizes amplitude\nvariations in the audio signal. Experiments with several\nlearning classiﬁers, including Support Vector Machines,\nhave been conducted.\nA large-scale evaluation with both subjective and\ncontent-based similarity measures was performed by\nBerenzweig et al. (2003). They addressed the question\nof comparing different existing music similarity measures\nand also raised the demand for a common evaluation\ndatabase.\nBasili et al. (2004) present a study on different ma-\nchine learning algorithms (and varying dataset partition-\ning) and their performance in music genre classiﬁcation.\nDixon et al. (2004) conduct experiments with paral-\nlels to ours: they utilize rhythmic patterns combined with\nadditional features derived from them and evaluate on thesame database as one of the three we used.\nFacing the number of different approaches and evalua-\ntion measures, the call for common evaluation among the\nMIR research groups has grown substantially (Downie,\n2003). Much effort has been put in organizing a Music\nIR contest, that was ﬁrst held during ISMIR 2004, eval-\nuating MIR performance in 5 different tasks, and which\nis now being continued as the MIREX contest (MIREX\n2005).\n3 FEATURE SETS\n3.1 Rhythm Patterns Features\nThe Rhythm Patterns form the core of the SOM-enhanced\nJukeBox (SOMeJB) system, which was ﬁrst introduced\nby Rauber and Fr ¨uhwirth (2001) without any psycho-\nacoustic processing. The approach was later drastically\nenhanced by incorporating psycho-acoustic phenomena\n(Rauber et al., 2002). In the current incarnation of the fea-\nture set, audio at 44 kHz sampling resolution is processed\ndirectly, in mono format. Several improvements and code\noptimizations regarding processing time have been made\nand numerous options have been introduced, such as au-\ntomatic choice of window step width. A number of the\nfollowing steps which are carried out during audio feature\nextraction are now optional. The algorithm for extracting\nthe Rhythm Patterns is as follows:\npreprocessing 1 convert audio from au, wav or mp3 for-\nmat to raw digital audio\npreprocessing 2 if audio contains multiple channels, av-\nerage them to 1 channel\npreprocessing 3 take a 6 second excerpt from the audio,\naccording to current processing position and consid-\nering lead-in, fade-out and step-width options\nstep [S1] transform audio segment into spectrogram rep-\nresentation using Fast Fourier Transform (FFT) with\nhanning window function (23 ms windows) and 50 %\noverlap\nstep [S2] apply Bark scale (Zwicker and Fastl, 1999) by\ngrouping frequency bands into 24 critical bands\nstep [S3] apply spreading function to account for spectral\nmasking effects (Schr ¨oder et al., 1979)\nstep [S4] transform spectrum energy values on the criti-\ncal bands into decibel scale [dB]\nstep [S5] calculate loudness levels through incorporating\nequal-loudness contours [Phon]\nstep [S6] compute speciﬁc loudness sensation per critical\nband [Sone]\nstep [R1] apply Fast Fourier Transform (FFT) to the\nSone representation. The result is a time-invariant\nrepresentation of the 24 critical bands that captures\nreocurring patterns in the audio signal and thus is\nable to show rhythmic structure on each of the criti-\ncal bands, i.e. amplitude modulation with respect to\nmodulation frequencies. The transformation obtains\namplitude modulation in the range from 0 to 43 Hz,\nhowever only the range from 0 through 10 Hz is con-\nsidered in the Rhythm Patterns, as higher values are\nbeyond what humans can perceive as rhythm.\n35step [R2] weight modulation amplitudes according to\nﬂuctuation strength sensation. According to human\nhearing sensation amplitude modulations are per-\nceived most intense at 4 Hz and decreasing towards\n15 Hz.\nstep [R3] apply a gradient ﬁlter to emphasize distinctive\nbeats and perform Gaussian smoothing to increase\nsimilarity between two feature descriptors by dimin-\nishing un-noticeable variations.\npostprocessing from all the Rhythm Patterns descriptors\nretrieved from the 6 second segments of a given piece\nof music, calculate the median as a descriptor for the\nwhole piece of music\nThe steps [S2] through [S6] as well as [R2] incorpo-\nrate psycho-acoustic phenomenons, based on studies of\nthe human hearing system. Steps [S3], [S4], [S5], [S6],\n[R2] and [R3] can be performed optionally. It is their con-\ntribution to similarity representation that is of interest in\nthis paper.\n3.2 Statistical Spectrum Descriptor\nDuring feature extraction we compute a Statistical Spec-\ntrum Descriptor (SSD) for the 24 critical bands. The spec-\ntrum transformed into Bark scale in step [S2] in Section\n3.1 represents rhythmic characteristics within the speciﬁc\nfrequency range of a critical band. According to the oc-\ncurrence of beats or other rhythmic variation of energy on\na speciﬁc band, statistical measures are able to describe\nthe audio content. We intend to describe the rhythmic\ncontent of a piece of audio by computing the following\nstatistical moments on the values of each of the 24 crit-\nical bands: mean, median, variance, skewness, kurtosis,\nmin- and max-value. They can be calculated after any\nof the steps during Rhythm Patterns feature calculation,\nhowever we usually retrieve them after step [S2] or [S6].\nThe resulting Statistical Spectrum Descriptor contains 168\nfeature attributes.\n3.3 Rhythm Histogram Features\nThe Rhythm Histogram features we use are a descriptor\nfor general rhythmics in an audio document. Contrary to\nthe Rhythm Patterns and the Statistical Spectrum Descrip-\ntor, information is not stored per critical band. Rather,\nthe magnitudes of each modulation frequency bin of all\n24 critical bands are summed up, to form a histogram of\n“rhythmic energy” per modulation frequency. The his-\ntogram contains 60 bins which reﬂect modulation fre-\nquency between 0 and 10 Hz. For a given piece of audio,\nthe Rhythm Histogram feature set is calculated by taking\nthe median of the histograms of every 6 second segment\nprocessed, resulting in a 60-dimensional feature space.\n4 EXPERIMENTS\n4.1 Audio collections and Experiment setup\nWe present a range of experiments performed on the\nRhythm Patterns Feature Set, the Statistical Spectrum De-\nscriptor and the Rhythm Histogram Features, as well ascombinations of them. For a quantitive evaluation of each\nof the feature sets we measure their performance in classi-\nﬁcation tasks. The task is to classify the music documents\ninto a predetermined list of classes, i.e. genres, accord-\ning to a previously annotated ground-truth. The experi-\nments were performed on three different audio collections\nin order to gain information about the generalization of\nthe results to different music repositories and thus differ-\nent musical styles, or to possibly detect speciﬁc problems\nwith certain types of audio. The ﬁrst audio collection\nis the one that was used by George Tzanetakis in pre-\nvious experiments (Tzanetakis, 2002), consecutively de-\nnoted as GTZAN. It consists of 1000 pieces of audio equi-\ndistributed among 10 popular music genres. The second\ncollection is the one used in the ISMIR 2004 Rhythm clas-\nsiﬁcation contest (ISMIR2004contest), which consists of\n698 excerpts of 8 genres from ballroom dance music. The\nthird collection is from the ISMIR 2004 Genre classiﬁca-\ntion contest (ISMIR2004contest) and contains 1458 com-\nplete songs, the pieces being unequally distributed over 6\ngenres. For details about the genres involved in each col-\nlection and the numbers of documents in each class refer\nto Table 1.\nTable 1: Three audio collections used in the experiments\nlisting classes and number of titles per class.\nGTZAN 1000 ISMIRrhythm 698 ISMIRgenre 1458\nblues 100 ChaChaCha 111 classical 640\nclassical 100 Jive 60 electronic 229\ncountry 100 Quickstep 82 jazz blues 52\ndisco 100 Rumba 98 metal punk 90\nhiphop 100 Samba 86 rock pop 203\njazz 100 SlowWaltz 110 world 244\nmetal 100 Tango 86\npop 100 VienneseWaltz 65\nreggae 100\nrock 100\nFor classiﬁcation, we used Support Vector Machines\nwith pairwise classiﬁcation. A 10-fold cross validation\nwas performed in each experiment from which we report\nmacro-averaged precision and recall, deﬁned as:\nPM=/summationtext|C|\ni=1πi\n|C|, RM=/summationtext|C|\ni=1ρi\n|C|(1)\nwhere |C|is the number of classes in a collection, and\nprecision πiand recall ρiper class are deﬁned as:\nπi=TPi\nTPi+FPi, ρi=TPi\nTPi+FNi(2)\nwhere TPiis the number of true positives in class i,\nFPiis the number of false positives in class i, i.e. doc-\numents identiﬁed as class ibut actually belonging to an-\nother class, and FNiis the number of false negatives of\na class i, i.e. documents belonging to class i, but which\nthe classiﬁer assigned to another class. We report macro-\naveraged precision and recall in order to make up for the\nunequal distribution of classes in the ISMIRgenre and IS-\nMIRrhythm data collections. As globally comparable cri-\nterion we report the F1measure\nF1=2·PM·RM\nPM+RM(3)\n36which is a combined measure of precision and recall,\nattributing the same weight to both as it is their harmonic\nmean. Additionally, for comparability to other studies, we\nreport Accuracy, deﬁned as\nA=/summationtext|C|\ni=1TPi\nN(4)\nNbeing the total number of audio documents in a col-\nlection.\n4.2 Rhythm Patterns Variants\nIn the ﬁrst series of our experiments we compared vari-\nations of our original algorithm for the extraction of the\nRhythm Patterns features. Our speciﬁc interest is the im-\npact of the various psycho-acoustic transformations. With\nthe results from this experiments, we obtain information\nabout the important parts of the feature extraction algo-\nrithm as well as an indication of which parts potentially\npose problems to the performance of the feature set.\nTable 2 provides an overview of the experiments. Each\nexperiment is identiﬁed by a letter. The table lists the steps\nof the feature extraction process involved in each experi-\nment. Experiment A represents the baseline, where all\nthe feature extraction steps are involved. Experiments K\nthrough N completely omit the transformations into the\ndB, Phon and Sone scales. Experiments G to I and K to\nQ extract features from the audio without accounting for\nspectral masking effects. A number of experiments evalu-\nates the effect of ﬁltering/smoothing and/or the ﬂuctuation\nstrength weighting.\nIn Table 3 our results from experiments A through\nQ on the three audio collections are presented (best and\nsecond-best result in each column printed in boldface).\nFrom the results of the experiments we make several in-\nteresting observations. Probably the most salient observa-\ntion is the low performance of the experiments J through\nN (with the exception of the precision in the ISMIRgenre\ncollection). These experiments do not involve transforma-\ntion into decibel scale nor successive transformation into\nthe Phon and Sone scales. Also, experiments E and F as\nwell as H and I deliver quite poor results, at least on the\nGTZAN and ISMIRgenre data sets. Those experiments\nperform decibel transformation but skip the transforma-\ntion into Phon and/or Sone. All these results indicate\nclearly that transformation into the logarithmic decibel\nscale is very important, if not essential, for the audio fea-\nture extraction and subsequent classiﬁcation or retrieval\ntasks. The successive application of the equal loudness\ncurves (i.e. Phon transformation) and the calculation of\nSone values appear also as important steps during feature\nextraction (experiment A compared to E and F, or experi-\nment G compared to H and I).\nSpectral Masking (i.e. step S3) was the subject of nu-\nmerous experiments. We wanted to measure the inﬂuence\nof the use or omission of the spreading function for spec-\ntral masking together with variations in the other feature\nextraction steps. Table 3 clearly shows, that most exper-\niments without Spectral Masking achieved better results.\nThe ISMIRrhythm collection constitutes an exception to\nthis. Nevertheless, the degradation of results incorporat-ing spectral masking raises the question whether the spec-\ntral masking spreading function is inappropriate for music\nof certain styles.\nFurther focus of investigation were the effects of the\nﬂuctuation strength weighting curve (step R2) and the ﬁl-\ntering/smoothing of the Ryhthm patterns (step R3). Both\nthe GTZAN and ISMIRgenre collections perform signif-\nicantly better with gradient ﬁlter and smoothing turned\noff. The ISMIRrhythm collection, however, shows con-\ntrary results. Its results improve when omitting the ﬂuc-\ntuation strength weighting, but degrade when ﬁltering &\nsmoothing is omitted.\nAs we see in several experiments, the ISMIRrhythm\ncollection behaves quite contrary to the two other collec-\ntions. At this point we must note, that the overall results of\nthe ISMIRrhythm collection are by far better than the ones\ncarried out with the two other collections. The reason why\nthis collection behaves differently might be that the re-\nsults are already at a high level and variations in the algo-\nrithm only cause small ﬂuctuations in the result values. On\nthe other hand, contrary to the GTZAN collection and IS-\nMIRgenre collection, ISMIRrhythm contains music from\n8 different dances. The discrimination of ballroom dances\nrelies heavily, if not exclusively, on rhythmic structure,\nwhich makes our Rhythm Patterns feature set an ideal de-\nscriptor (and thus justiﬁes the good results). Apparently,\nsmoothing the Rhythm Patterns is important for making\ndances from the same class with slightly different rhythms\nmore similar – whereas in the two other collections, ﬁlter-\ning & smoothing has negative effects. The ISMIRrhythm\nset appears to be independent of the spectral masking ef-\nfects. Best results with ISMIRrhythm were retrieved with\nexperiment C, which omits ﬂuctuation strength weighting\n[R2], closely followed by experiment P, which addition-\nally omits spectral masking [S3].\nFor the GTZAN and ISMIRgenre collections best re-\nsults both in terms of F1measure and Accuracy were\nachieved in experiment O, which is the original Rhythm\nPatterns feature extraction without spectral masking [S3]\nand without ﬁltering & smoothing [R3].\n4.3 Statistical Spectrum Descriptor Experiments\nIn the experiments with the Statistical Spectrum Descrip-\ntor (SSD) we mainly investigate the performance of the\nfeatures depending on which position in the Rhythm Pat-\nterns feature extraction process they are computed. Two\npositions were chosen to test the SSD: First, the statis-\ntical measures are derived directly after step [S2], when\nthe frequency bands of the audio spectrogram have been\ngrouped to critical bands. In the second experiment, the\nfeatures are calculated after the critical bands spectrum\nhad undergone logarithmic dB transformation as well as\ntransformation into Phon and Sone, i.e. after step [S6]. In\norder to ﬁnd an adequate representation of an audio track\nthrough a Statistical Spectrum Descriptor, we evaluated\nboth the calculation of the mean and the median of all\nsegments of a track.\nTable 4 gives the results of the 4 experiment variants.\nFrom the results we ﬁnd, that in any case the calculation\nafter step [S6] is superior to deriving the SSD already at\n37Table 2: Experiment IDs and the steps of the Rhythm Patterns feature extraction process involved in each experiment.\nstep A B C D E FG H I JK LM N O PQ\nS1 FFT × × × × × × × × × × × × × × × × ×\nS2 Critical bands × × × × × × × × × × × × × × × × ×\nS3 Spectral masking × × × × × × ×\nS4 dB transform × × × × × × × × × × × ×\nS5 Equal loudness (Phon) × × × × × × × × × ×\nS6 Spec. loudness Sens. (Sone) × × × × × × × ×\nR1 FFT Modulation Amplitude × × × × × × × × × × × × × × × × ×\nR2 Fluctuation Strength × × × × × × × × × × ×\nR3 ﬁlter/smoothing × × × × × × × × × × ×\nTable 3: Results of the Rhythm Patterns feature extraction experiments, for 3 audio collections, using 10-fold cross\nvalidation, in terms of macro-averaged precision ( PM), macro-averaged recall ( RM),F1measure and Accuracy ( A). All\nvalues in %. Highest and second highest value in each column are boldfaced.\nGTZAN ISMIRrhythm ISMIRgenre\nExp. PMRMF1 A PMRMF1 A PMRMF1 A\nA 58.51 58.50 58.50 58.50 82.50 81.28 81.88 81.66 59.83 56.07 57.89 70.99\nB 62.64 62.30 62.47 62.30 83.35 81.56 82.45 82.38 62.42 61.80 62.11 72.63\nC 59.67 59.40 59.53 59.40 83.39 82.30 82.84 82.81 59.65 56.28 57.92 71.19\nD 62.64 62.30 62.47 62.30 83.20 81.35 82.26 82.24 62.45 61.60 62.02 72.63\nE 55.51 55.80 55.65 55.80 81.74 80.85 81.29 81.38 59.63 57.98 58.80 70.44\nF 53.57 53.60 53.58 53.60 82.04 81.14 81.59 81.66 57.20 54.73 55.94 68.24\nG 62.96 62.90 62.93 62.90 82.59 81.61 82.10 81.95 65.62 60.83 63.13 73.73\nH 59.06 59.50 59.28 59.50 81.94 80.58 81.25 81.38 59.63 58.63 59.13 71.47\nI 59.71 60.20 59.95 60.20 82.39 81.00 81.69 81.81 59.40 57.88 58.63 70.30\nJ 53.06 52.30 52.68 52.30 74.09 72.71 73.39 73.50 64.50 51.97 57.56 69.27\nK 53.85 53.10 53.47 53.10 74.06 72.25 73.15 73.35 66.80 52.65 58.89 70.03\nL 55.08 54.40 54.74 54.40 67.05 66.50 66.77 67.77 63.80 54.48 58.77 69.62\nM 54.46 53.90 54.18 53.90 74.86 72.44 73.63 73.50 66.40 52.22 58.46 69.20\nN 55.36 54.70 55.03 54.70 66.98 66.26 66.62 67.34 63.37 53.82 58.20 69.14\nO 64.22 64.40 64.31 64.40 80.73 79.34 80.03 80.09 65.08 64.50 64.79 75.03\nP 60.51 60.50 60.50 60.50 83.15 81.88 82.51 82.24 66.15 61.57 63.78 73.94\nQ 64.22 64.40 64.31 64.40 81.58 80.16 80.86 80.95 64.87 64.13 64.50 74.90\nthe earlier stage [S2]. As in the experiments with the\nRhythm Patterns feature set, logarithmic transformation\nappears to be essential for the results of the content-based\naudio descriptors. Comparing the summarization of an au-\ndio track by mean and by median, results of the GTZAN\nand ISMIRgenre collection argue for the use of the mean.\nAgain, the ISMIRrhythm collection indicates contrary re-\nsults, however the differences in result measures vary only\nbetween 0.04 and 1.4 percentage points.\nNote, that the SSD feature set calculated after step\n[S6] outperforms the Rhythm Patterns descriptor both in\nthe GTZAN and ISMIRgenre collections. This is espe-\ncially remarkable as the statistical descriptors have a di-\nmensionality 8.5 times lower than the Rhythm Patterns\nfeature set.\n4.4 Experiments on Rhythm Histogram Features\nThe Rhythm Histogram Features (RH) describe global\nrhythmic content of a piece of audio by a measure of en-\nergy per modulation frequency. They are calculated fromthe time-invariant representation of the Rhythm Patterns.\nOur experiments tried to evaluate different performance\nwhen computing the Rhythm Histogram Features after\nfeature extraction step R1, R2 or R3, respectively. Eval-\nuation showed, that regardless to the stage, RH features\nvirtually always produce equal results. We thus omit a ta-\nble with detailed results; performance of the Rhythm His-\ntogram features can be seen in the row denoted ’RH [R1]’\nof Table 5.\nResults of the RH features in the ISMIRrhythm collec-\ntion achieve nearly the results of the Rhythm Patterns fea-\nture set. Note that dimensionality is 24 times lower than\nthat of the latter one. Performance of GTZAN and IS-\nMIRgenre collections is rather low, nevertheless, though\nbeing a simple descriptor, the Rhythm Histogram feature\nset seems eligible for audio content description.\n4.5 Comparison and Combined Feature sets\nTable 5 displays a comparison of the baseline Rhythm Pat-\nterns (RP) algorithm (experiment A) to the best results\n38Table 4: Results of the experiments with Statistical Spectrum Descriptor (3 data sets, 10-fold cross val., best results bold).\nGTZAN ISMIRrhythm ISMIRgenre\nExp. PMRMF1 A PMRMF1 A PMRMF1 A\nSSD[S2] (mean) 60.87 60.20 60.53 60.20 36.56 21.08 26.74 25.64 40.58 25.57 31.37 51.58\nSSD[S2] (median) 57.70 57.00 57.35 57.00 43.54 39.96 41.67 43.84 68.17 49.90 57.62 67.76\nSSD[S6] (mean) 72.85 72.70 72.77 72.70 54.35 52.81 53.57 54.73 76.93 67.95 72.16 78.53\nSSD[S6] (median) 71.57 71.30 71.43 71.30 54.39 53.80 54.09 55.44 75.78 66.70 70.95 77.50\nof the Rhythm Patterns extraction variants, the Statistical\nSpectrum Descriptor (SSD) and the Rhythm Histogram\nfeatures (RH). Best results in Rhythm Patterns extraction\nwere achieved with the GTZAN, ISMIRrhythm and IS-\nMIRgenre audio collections in experiments O, C, and O\nrespectively. Accuracy was 64.4, 82.8, and 75.0 %, re-\nspectively. The Statistical Spectrum Descriptor performed\nbest when calculated after psycho-acoustic transforma-\ntions, and taking the simple mean of the segments of a\npiece of audio. Accuracy was 72.7, 54.7, and 78.5 %\nin the GTZAN, ISMIRrhythm and ISMIRgenre data set,\nrespectively, which exceeds the Rhythm Patterns feature\nset in 2 of the 3 collections. Rhythm Histogram Features\nachieved 44.1, 79.94, and 63.17 % accuracy, which rival\nthe Rhythm Patterns features regarding the ISMIRrhythm\ndata collection. Obviously a combination of feature sets\noffers itself for further improvement of classiﬁcation per-\nformance.\nVarious experiments on 2 set combinations have been\nevaluated. The combination of Rhythm Patterns features\nwith the Statistical Spectrum Descriptor achieves 72.3 %\naccuracy in the GTZAN data set, which is slightly lower\nthan the performance of the SSD alone. Contrary, in\nthe ISMIRrhythm data set, the combination achieves a\nslight improvement. In the ISMIRgenre audio collection,\nthis combination results in a signiﬁcant improvement and\nachieves the best result of all experiments on this data set\n(80.32 % accuracy).\nCombination of Rhythm Patterns features with\nRhythm Histogram Features changes the results of the\nRhythm Patterns features only insigniﬁcantly, a noticeable\nimprovement can be seen only in the ISMIRrhythm data\nset, which is the data set where the Rhythm Histogram\nfeatures performed best.\nVery interesting are the results of combining the Sta-\ntistical Spectrum Descriptor with Rhythm Histogram fea-\ntures: With the GTZAN collection, this combination\nachieves the best accuracy (74.9 %) of all experiments\n(including the 3 set experiments). The result on the IS-\nMIRrhythm collection is comparable to the best Rhythm\nPatterns result. The 2 set combination without Rhythm\nPatterns features performs also very well on the ISMIR-\ngenre data set, achieving the best F1measure (73.3 %).\nThere is a notably high precision value of 76.67 %, how-\never, recall is only at 70.22 %. Accuracy is 79.63 % and\nthus slightly lower than in the Rhythm Patterns + SSD\ncombination.\nFinally, we investigated the combination of all 3 fea-\nture sets, which further improved the results only on the\nISMIRrhythm data set. Accuracy increased to 84.24 %,\ncompared to 82.81 % using only the Rhythm Patterns fea-\ntures. As stated, results on the ISMIRrhythm collectionwere rather high from the beginning, consequently im-\nprovements on classiﬁcation in this data set were mod-\nerate.\nOverall improvement, regarding best accuracy values\nachieved in each data collection compared to baseline ex-\nperiment A, was +16.4 percentage points on the GTZAN\nmusic collection, +2.58 percentage points on the ISMIR-\nrhythm collection and +9.33 percentage points on the IS-\nMIRgenre music collection.\n4.6 Comparison with other results\n4.6.1 GTZAN data set\nThe GTZAN audio collection was assembled and used\nﬁrst in experiments by Tzanetakis (2002). The original\ncollection was organized in a three level hierarchy in-\ntended for discrimination into speech/music, classiﬁcation\nof music into 10 genres and subsequent classiﬁcation of\nthe two genres classical and jazz into subgenres. In our\nexperiments we used the organization of 10 musical gen-\nres in the second level, and thus compare our results to the\nperformance of Tzanetakis (2002) on that level. The best\nclassiﬁcation result reported was 61 % accuracy (4 % stan-\ndard deviation on 100 iterations of a 10-fold cross valida-\ntion) using Gaussian Mixture Models and the 30 dimen-\nsional MARSYAS genre features.\nLi et al. (2003) used the same audio collection in their\nstudy and compare “Daubechies Wavelet Coefﬁcient His-\ntograms” (DWCHs) to combinations of MARSYAS fea-\ntures. DWCHs achieved 74.9 % classiﬁcation accuracy in\na 10-fold cross validation using Support Vector Machines\n(SVM) with pairwise classiﬁcation and 78.5 % accuracy\nusing SVM with one-versus-the-rest classiﬁcation.\nOur current best performance is 74.9 %, which consti-\ntutes an improvement of 16.4 percentage points regarding\noriginal Rhythm Patterns feature descriptor.\nTable 6: Comparison with other results on the GTZAN\naudio collection (10-fold cross validation).\nGTZAN A\nTzanetakis (2002) (GMM) 61.0\nLi et al. (2003) (SVM pairwise) 74.9\nLi et al. (2003) (SVM one-vs-the-rest) 78.5\nour best result (SVM pairwise) 74.9\n4.6.2 ISMIRrhythm data set\nThough not participating in the ISMIR Rhythm classiﬁca-\ntion contest, two papers of ISMIR 2004 report experiment\n39Table 5: Comparison of feature sets and combinations (3 data sets, 10-fold cross validation, best results boldfaced).\nGTZAN ISMIRrhythm ISMIRgenre\nExp. PMRMF1 A PMRMF1 A PMRMF1 A\nRP(A) 58.51 58.50 58.50 58.50 82.50 81.28 81.88 81.66 59.83 56.07 57.89 70.99\nRP(best) O/C/O 64.22 64.40 64.31 64.40 83.39 82.30 82.84 82.81 65.08 64.50 64.79 75.03\nSSD [S6] (mean) 72.85 72.70 72.77 72.70 54.35 52.81 53.57 54.73 76.93 67.95 72.16 78.53\nRH [R1] 43.55 44.10 43.82 44.10 82.09 79.14 80.59 79.94 41.58 39.20 40.36 63.17\nRP(best)+SSD 72.17 72.30 72.23 72.30 84.38 82.88 83.62 83.52 72.33 72.00 72.17 80.32\nRP(best)+RH 64.06 64.20 64.13 64.20 84.45 83.08 83.76 83.67 65.27 64.55 64.91 75.51\nSSD+RH 74.79 74.90 74.84 74.90 83.13 81.44 82.27 82.66 76.67 70.22 73.30 79.63\nRP(best)+SSD+RH 72.25 72.40 72.32 72.40 85.00 83.43 84.21 84.24 71.85 71.27 71.56 79.97\nresults on the same data collection. The approach used by\nGouyon and Dixon (2004) is based on tempo probability\nfunctions for each of the 8 ballroom dances and successive\npairwise or three-class classiﬁcation and reports 67.6 %\noverall accuracy.\nDixon et al. (2004) speciﬁcally address the problem\nof dance music classiﬁcation, and achieve an astounding\nresult of 96 % accuracy when using a combination of var-\nious feature sets. Besides soundly elaborated descriptors,\nthe approach also incorporates a-priori knowledge about\ntempo and thus drastically reduces the number of possible\nclasses for a given audio instance.\nThe ground-truth-tempo approach has been previously\ndescribed by Gouyon et al. (2004), where classiﬁcation\nbased solely on the pre-annotated tempo attribute reached\n82.3 % accuracy (k-NN classiﬁer, k=1). The paper also\ndescribes a variety of descriptor sets and reports 90.1 %\naccuracy on the combination of MFCC-like descriptors\nwith ground-truth tempo and 78.9 % accuracy when us-\ning computed tempo instead.\nAll results presented in Table 7 have been evaluated\nthrough a 10-fold cross validation, except for the ﬁrst one,\nwhich used the ISMIR contest training/test set split.\nTable 7: Comparison with other results on the ISMIR-\nrhythm audio collection (10-fold cross validation).\nISMIRrhythm A\nLidy et al. in (ISMIR2004contest) 82.0\nGouyon and Dixon (2004) 67.6\nGouyon et al. (2004) wo/tempo-gt. 78.9\nGouyon et al. (2004) w/tempo-gt. 90.1\nDixon et al. (2004) wo/tempo-gt. 85.7\nDixon et al. (2004) w/tempo-gt. 96.0\nour current best result 84.2\n4.6.3 ISMIRgenre data set\nThe ISMIRgenre data set was assembled for the ISMIR\n2004 Genre classiﬁcation contest. Results from the Genre\nclassiﬁcation contest are shown in Table 8 in terms of Ac-\ncuracy, and Accuracy normalized by the genre frequency\n(which is equal to macro-averaged Recall). In order to be\nable to compare our current results to the values stated in\nthe table, instead of a 10-fold cross-validation we repeated\nour experiment with the combination of RP(O)+SSD fea-\ntures using the same training and test set partitioning as inthe contest. Though not surpassing the winner of the 2004\ncontest, the results of our current evaluation represent a\nsubstantial improvement to the approach submitted to the\n2004 contest, making it theoretically rank second place.\nTable 8: Comparison with the results from the ISMIR\n2004 Genre classiﬁcation contest (50:50 training and test\nset split).\nISMIRgenre AA(norm.)\nThomas Lidy and Andreas Rauber 70.4 55.7\nDan Ellis and Brian Whitman 64.0 51.0\nKris West 78.3 67.2\nElias Pampalk 82.3 78.8\nGeorge Tzanetakis 71.3 58.6\nour current approach 79.7 70.4\n5 SUMMARY\nWe performed a study on the contribution of psycho-\nacoustic transformations in the calculation of Rhythm Pat-\nterns for efﬁcient content-based music description. Nu-\nmerous experiments have been arranged to identify the\nimportant parts in the feature extraction process. More-\nover, two additional descriptors calculated together with\nthe Rhythm Patterns – namely the Rhythm Histogram fea-\ntures and the Statistical Spectrum Descriptor – were pre-\nsented, and evaluated in their efﬁciency compared to other\nfeature sets. Performance on all experiments was mea-\nsured by the results in a music genre classiﬁcation task.\nThe feature sets, besides being suitable for music similar-\nity retrieval, are intended to perform automatic organiza-\ntion tasks by classiﬁcation into different semantical gen-\nres. In order to be able to assess the general applicability\nin various genre taxonomies, three different standard MIR\naudio collections have been used in the evaluation. Be-\nsides measuring the performance of each individual fea-\nture set, we investigated whether combinations of the fea-\nture sets would signiﬁcantly increase the results. Com-\npared to the original Rhythm Patterns audio descriptor,\nthe experiments on the three music collections achieved\naccuracy improvements of 16.4, 9.33, and 2.58 percent-\nage points, respectively.\nEvaluation of the Rhythm Patterns experiment vari-\nants showed that the implementation of spectral masking\nin the feature extraction might pose a potential issue in\n40the audio description, at least regarding speciﬁc types of\nmusic. Furthermore, ﬁltering and smoothing procedures\nas well as the weighting of ﬂuctuation strength have been\nidentiﬁed to have quite unpredictable inﬂuence in audio\nclassiﬁcation for different taxonomies. However, a series\nof psycho-acoustic transformations, namely the transfor-\nmation into the logarithmic dB scale, equal loudness in\nthe Phon scale and speciﬁc loudness sensation in terms of\nthe Sone scale, has been identiﬁed to be crucial for the\naudio description task.\nFuture tasks involve further investigation of the ﬁlter-\ning and weighting processes as well as their inﬂuence de-\npending on varying audio repositories.\nReferences\nJ.-J. Aucouturier and F. Pachet. Music similarity mea-\nsures: What’s the use? In Proceedings of the Interna-\ntional Symposium on Music Information Retrieval (IS-\nMIR) , October 2002.\nJ.-J. Aucouturier and F. Pachet. Representing musical\ngenre: A state of the art. Journal of New Music Re-\nsearch , 32(1):83–93, 2003.\nR. Basili, A. Seraﬁni, and A. Stellato. Classiﬁcation of\nmusical genre: a machine learning approach. In Pro-\nceedings of the International Conference on Music In-\nformation Retrieval (ISMIR) , Barcelona, Spain, Octo-\nber 2004.\nA. Berenzweig, B. Logan, D. P. W. Ellis, and B. Whitman.\nA large-scale evaluation of acoustic and subjective mu-\nsic similarity measures. In Proceedings of the Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR) , October 2003.\nR. B. Dannenberg, B. Thom, and D. Watson. A ma-\nchine learning approach to musical style recognition. In\nProceedings of the International Computer Music Con-\nference (ICMC) , pages 344–347, Thessaloniki, Greece,\nSeptember 25-30 1997.\nS. Dixon, F. Gouyon, and G. Widmer. Towards character-\nisation of music via rhythmic patterns. In Proceedings\nof the International Conference on Music Information\nRetrieval (ISMIR) , pages 509–516, Barcelona, Spain,\nOctober 2004.\nJ. S. Downie. Toward the scientiﬁc evaluation of mu-\nsic information retrieval systems. In Proceedings of\nthe International Conference on Music Information Re-\ntrieval (ISMIR) , Baltimore, Maryland, USA, October\n26-30 2003.\nJ. T. Foote. Content-based retrieval of music and audio. In\nProceedings of SPIE Multimedia Storage and Archiving\nSystems II , volume 3229, pages 138–147, 1997.\nF. Gouyon and S. Dixon. Dance music classiﬁcation: A\ntempo-based approach. In Proceedings of the Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR) , Barcelona, Spain, October 2004.\nF. Gouyon, S. Dixon, E. Pampalk, and G. Widmer. Eval-\nuating rhythmic descriptors for musical genre classiﬁ-\ncation. In Proceedings of the AES 25th InternationalConference , pages 196–204, London, UK, June 17-19\n2004.\nISMIR2004contest. ISMIR 2004 Audio Description Con-\ntest. Website, 2004. http://ismir2004.ismir.\nnet/ISMIR_Contest.html .\nT. Li, M. Ogihara, and Q. Li. A comparative study on\ncontent-based music genre classiﬁcation. In Proceed-\nings of the International ACM SIGIR Conference on\nResearch and Development in Information Retrieval ,\npages 282 – 289, Toronto, Canada, 2003.\nT. Lidy, G. P ¨olzlbauer, and A. Rauber. Sound re-synthesis\nfrom rhythm pattern features - audible insight into a\nmusic feature extraction process. In Proceedings of\nthe International Computer Music Conference (ICMC) ,\nBarcelona, Spain, September 5-9 2005.\nZ. Liu and Q. Huang. Content-based indexing and\nretrieval-by-example in audio. In Proceedings of the\nIEEE International Conference on Multimedia and\nExpo (ICME) , New York, USA, July 30 - Aug. 2 2000.\nB. Logan and A. Salomon. A music similarity func-\ntion based on signal analysis. In Proceedings of\nthe IEEE International Conference on Multimedia and\nExpo (ICME) , Tokyo, Japan, August 2001.\nMIREX 2005. 2nd annual Music Information Re-\ntrieval Evaluation eXchange. Website, 2005.\nhttp://www.music-ir.org/mirexwiki/\nindex.php/Main_Page .\nR. Neumayer, T. Lidy, and A. Rauber. Content-based or-\nganization of digital audio collections. In Proceedings\nof the 5th Open Workshop of MUSICNETWORK , Vi-\nenna, Austria, July 4-5 2005.\nE. Pampalk, S. Dixon, and G. Widmer. On the evalua-\ntion of perceptual similarity measures for music. In\nProceedings of the International Conference on Digi-\ntal Audio Effects (DAFx-03) , pages 7–12, London, UK,\nSeptember 8-11 2003.\nA. Rauber and M. Fr ¨uhwirth. Automatically analyzing\nand organizing music archives. In Proceedings of the\nEuropean Conference on Research and Advanced Tech-\nnology for Digital Libraries (ECDL) , Darmstadt, Ger-\nmany, September 4-8 2001.\nA. Rauber, E. Pampalk, and D. Merkl. Using psycho-\nacoustic models and self-organizing maps to create a\nhierarchical structuring of music by musical styles. In\nProceedings of the International Conference on Music\nInformation Retrieval , pages 71–80, Paris, France, Oc-\ntober 13-17 2002.\nM. Schr ¨oder, B. Atal, and J. Hall. Optimizing digital\nspeech coders by exploiting masking properties of the\nhuman ear. Journal of the Acoustical Society of Amer-\nica, 66:1647–1652, 1979.\nG. Tzanetakis. Manipulation, Analysis and Retrieval Sys-\ntems for Audio Signals . PhD thesis, Computer Science\nDepartment, Princeton University, 2002.\nE. Zwicker and H. Fastl. Psychoacoustics - Facts and\nModels , volume 22 of Springer Series of Information\nSciences . Springer, Berlin, 1999.\n41"
    },
    {
        "title": "Fast Capture of Sheet Music for an Agile Digital Music Library.",
        "author": [
            "Richard Lobb",
            "Tim Bell 0001",
            "David Bainbridge 0001"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417989",
        "url": "https://doi.org/10.5281/zenodo.1417989",
        "ee": "https://zenodo.org/records/1417989/files/LobbBB05.pdf",
        "abstract": "A personal digital music library needs to be “agile”, that is, it needs to make it easy to capture and index material on the fly. A digital camera is a particularly effective way of achieving this, but there are several issues with the quality of the captured image, including distortions in the shape of the image due to the camera not being aligned properly with the page, non-planarity of the page, lens distortion from close-up shots, and inconsistent lighting across the page. In this paper we explore ways to improve the quality of music images captured by a digital camera or an inexpensive scanner, where the user is not expected to pay a lot of attention to the process. Such pre-processing will significantly aid Music Information Retrieval indexing through Optical Music Recognition, for example. The research presented here is primarily based around using a Fast Fourier Transform (FFT) to determine the orientation of the page. We find that a windowed FFT is effective at correcting rotational errors, and we make significant progress towards removing perspective distortion introduced by the camera not being parallel with the music. Keywords: image capture, FFT, digital camera 1",
        "zenodo_id": 1417989,
        "dblp_key": "conf/ismir/LobbBB05",
        "keywords": [
            "digital camera",
            "Optical Music Recognition",
            "Fast Fourier Transform (FFT)",
            "pre-processing",
            "Music Information Retrieval",
            "indexing",
            "orientation correction",
            "perspective distortion",
            "rotational errors",
            "page alignment"
        ],
        "content": "FAST CAPTURE OFSHEET MUSIC FOR ANAGILE DIGIT ALMUSIC\nLIBRAR Y\nRichard Lobb,\nTimBell\nDepartment ofComputer Science and\nSoftw areEngineering\nUniversity ofCanterb ury\nChristchurch, NZ\n{richard.lobb,tim.bell}@canterbury.ac.nzDavidBainbridge\nDepartment ofComputer Science\nUniversity ofWaikato\nHamilton, NZ\nd.bainbridge@cs.waikato.ac.nz\nABSTRA CT\nApersonal digital music library needs tobeagile, that\nis,itneeds tomakeiteasy tocapture andindexmaterial on\nthe\u0003y.Adigital camera isaparticularly effectivewayof\nachie ving this, butthere areseveralissues with thequality\nofthecaptured image, including distortions intheshape\noftheimage duetothecamera notbeing aligned prop-\nerlywith thepage, non-planarity ofthepage, lens distor -\ntionfrom close-up shots, andinconsistent lighting across\nthepage. Inthispaper weexplore waystoimpro vethe\nquality ofmusic images captured byadigital camera or\naninexpensi vescanner ,where theuser isnotexpected to\npayalotofattention totheprocess. Such pre-processing\nwill signi\u0002cantly aidMusic Information Retrie valindex-\ningthrough Optical Music Recognition, forexample. The\nresearch presented here isprimarily based around using\naFastFourier Transform (FFT) todetermine theorien-\ntation ofthepage. We\u0002nd thatawindo wed FFT isef-\nfectiveatcorrecting rotational errors, andwemakesig-\nni\u0002cant progress towards remo ving perspecti vedistortion\nintroduced bythecamera notbeing parallel with themu-\nsic.\nKeywords: image capture, FFT,digital camera\n1INTR ODUCTION\nProfessionally maintained digital sheet music libraries go\ntoconsiderable trouble tocapture clean images thatare\nsuitable foravariety ofapplications (Rile yandFujinaga,\n2003). However,increasingly there isademand forper-\nsonal digital libraries ofsheet music toreplace theshelv es\nandbriefcases thatmusicians traditionally use. The per-\nsonal digital library might bethebasis ofadigital mu-\nsicstand, andalso provides \u0003exible access andbackup of\nmaterial thatthemusician uses intheir daytodaywork,\nincluding thepossibility ofOptical Music Recognition\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondon(OMR) toassist with searching andpreviewing themu-\nsic(Bainbridge andBell, 2001).\nWhile itisfeasible toaquire high-quality digital scores\nfrom internet stores andweb sites, musicians willhavea\nlegacyofpaper -based music thatisnotavailable indig-\nitalform (such aspersonal compositions, transcriptions,\nandout-of-print works), andthese need tobecaptured\n(format-shifted) intothedigital library .\nSuch alibrary needs tobeagile, asthelibrarian isalso\ntheuser,andoften thetaskofacquisition andcataloguing\nmusic will beseen only asananno ying step witness\nthestate ofcataloguing ofsome musicians' paper based\nlibraries! Furthermore, theformat-shifting process may\nneed tooccur inapressured andmobile environment, such\nasarough chord chart being written outduring abreak in\naperformance duetoanunexpected request.\nThe main wayofcapturing paper -based music isto\nuseascanner oradigital camera. Acarefully con\u0002gured\nscanner canproduce excellent reproductions, butaper-\nsonal scanner islikelytobeslow(only oneortwopages\nperminute), notportable, anddif\u0002cult tousewith bulky\noriginals. Incontrast adigital camera isveryportable and\nimages canbecaptured inafraction ofasecond (Tay-\nloretal.,1999). Afour mega-pix elcamera givesfull-\ncolour capture ofatypical page ataround 200dpi,which\nisquite adequate forreading music from ascreen orfor\noptical music recognition. While theresolution ofcam-\neras issuf\u0002cient forthereproduction ofsheet music, un-\nlessacarefully controlled environment isused, thequality\nofthecapture islowerbecause ofunevenorpoor lighting,\nanddistortions canbecaused bythelensandtheangle of\nthecamera tothepage. Placing thecamera directly overa\npage isgenerally dif\u0002cult inanuncontrolled environment\nbecause itislikelytocreate shado wsonthepage. Distor -\ntions intheshape ofthedocument canalsobeworse than\nwith ascannner; forexample, thebinding ofabook can\ncause theedge ofapage tobecurved,andthisisnotso\npronounced onascanner because thepage iscompressed\nonto thescanner glass.\nCorrecting forthese distortions isimportant forOMR\nbased indexing, forexample, because theaccurate place-\nment ofsymbols onthestaveiscrucial, particularly for\nidentifying thepitch. This may beanissue ifadigital\ncamera hasbeen used forcapture, oriftheimage hasbeen\nstored onmicro\u0002lm. EvenifOMR isnotbeing performed\nonthemusic, thequality ofthedisplay canbegreatly en-\n145hanced because evenslight distortions inthecaptured im-\nagemay produce distracting aliasing effects (jaggies),\nparticularly inthehorizontal stavelines.\nThecorrection ofdistortion caused byscanners hasre-\nceivedconsiderable attention intheOMR literature. Dis-\ntortion correction isgenerally combined with stafflinede-\ntection, which isusually performed byeither:\n\u000fprojections, typically horizontal oraseries ofnear-\nhorizontal projections (looking forscan lines with\nmainly black pixels)e.g.Matsushima etal.(1985);\nMartin andBellissant (1991); Baumann andDengel\n(1992); Clark eetal.(1988); Fujinaga etal.(1991);\nRoth (1994), or\n\u000fvertical scan lines (looking forregular white gaps be-\ntween black stavelines) e.g.Carter (1989); Kato and\nInokuchi (1990); Reed (1995).\nFurthermore, prior workgenerally assumes that the\ndeformation canbecorrected byrotation, partly because\nmusic hastraditionally been scanned ona\u0003at-bed scanner\nwhere rotation isthemost likelyerror .\nItisinteresting toobserv ethatexisting OMR systems\ngenerally assume bi-levelimages, presumably based on\ntheargument thatmusic itself isblack andwhite. How-\never,agray-scale orcolour image contains more informa-\ntionandistherefore more useful forimage processing. It\nalsoprovides arelati velyalias-free image forviewing. We\narguethatformusic inadigital library ,content should be\npreserv edincolour foraslong aspossible (storage space\npermitting) andonly converted tobi-levelwhere strictly\nnecessary ,forinstance when using abi-levelimage seg-\nmentation algorithm.\nInthepaper weinvestigate techniques based onafast\nfourier transform (FFT) forremo ving thedistortion that\niscaused bycapturing apage using adigital camera in\nuncontrolled conditions. Westart byconsidering waysto\nremo verotation, andthen welook atperspecti vedistortion\nintroduced bynothaving thecamera directly abovethe\npage.\n2FOURIER TRANSFORMS OFMUSIC\nIMA GES\nInthis section weshowtheFourier transforms oftwo\ntestimages anddiscuss what information canbeobtained\nfrom them. Wealso introduce theidea ofwindo wed\nFourier transforms.\n2.1 The FFT ofanentir erotated page ofmusic\nFigure 1ashowsa1700 \u00022340 pixeltestimage: apage\nofsheet music scanned with arotation ofapproximately 6\ndegrees asagrey-scale image. Note thatthebackground\nintensity varies signi\u0002cantly overtheimage area. Fig-\nure1bshowsthemagnitude oftheFourier transform ofthe\nimage inFigure 1a.Throughout therestofthispaper we\nwillloosely refer tothemagnitude oftheFourier Trans-\nform asthe Fourier transform, since wenevermake\nuseofthephase information ofthetransform.\nIncomputing Figure 1bthetransform hashadthezero\nfrequenc yorDC component settozero andhasthen\n(a) (b)\nFigure 1:Testimage one, Angels, rotation only (a)the\ninitially captured image using a\u0003at-bed scanner and(b)\nitsFFT.\nbeen scaled sothatthelargest remaining component has\nthearbitrary value of32.Thetransform isshifted sothat\nthezero frequenc ypoint isinthecentre oftheimage. A\nwhite pixeldenotes avalue of1ormore, andablack pixel\ndenotes 0.Note thefollo wing:\n\u000fThe transform issymmetric about theorigin inthe\nsense thatF(\u0000x;\u0000y)=F(x;y).1\n\u000fThemost pronounced feature oftheimage isalineof\nstrong Fourier components atanangle tothevertical\naxis. TheFourier components onthislinearise from\ntherotated stavelines, sowewill calltheline the\nFourier Stave Char acteristic orFSC.\n\u000fTheFSC ismade upofthevarious frequenc ycom-\nponents thatde\u0002ne thesetofstavelines. There are\ntwoprimary frequenc ycomponents present: ahigh-\nfrequenc ycomponent corresponding tothespacing\nbetween adjacent stavelines and alow-frequenc y\ncomponent due tothespacing between connected\nlines ofmusic (referred toassystems bymusicians).\n\u000fTheangle between theFSC andthevertical axiscan\nbedetermined by\u0002nding thepixelcoordinates ofany\nofthestrong components ontheFSC well awayfrom\ntheorigin. Forexample there isastrong compo-\nnent at(85,1086) with respect totheorigin, i.e., a\ncomponent with ahorizontal frequenc yof85cycles\nper1700 pixels(theimage width) andavertical fre-\nquenc yof1086 cycles per2340 pixels(the image\nheight). This corresponds toarotation angle inthe\noriginal image of\n\u0012=tan\u0000185=1700\n1086=2340=6:149\u000e(1)\n\u000fThere isafainter andless well-de\u0002ned tilted hori-\nzontal line, arising from thebarlines andnote stems.\nThis lineisnotatright angles totheFSC because the\nfrequenc yunits onthetwoaxesaredifferent.\n1Throughout thispaper weusexandytodenote thehorizon-\ntalandvertical coordinates respecti velyinboth thespace domain\nandthefrequenc ydomain. Inthespatial domain xandymea-\nsure distances inpixels,whereas inthefrequenc ydomain they\nmeasure spatial frequencies incycles perimage width/height.\n146\u000fThere isavery\u0002ne sharp horizontal line exactly\nalong thefrequenc y-space x-axis (only visible inen-\nlargedversions oftheimage). This arises from a\nscanner artifact: athin greylinethatruns vertically\ndownmost oftheleft-hand edge oftheimage.\n2.2 Remo ving asimple rotation\nThe simple testimage shownabovecanbeeasily cor-\nrected byapplying arotation of6:149\u000e.Figure 2ashows\nthewhole page ofmusic after applying thatrotation, and\nFigure 2bshowsaclose upofapartofthepage.\n(a) (b)\nFigure 2:Testimage one: (a)rotated by6:149\u000e,and(b)a\nclose upportion ofit.\n2.3 Aperspecti ve-distorted image\nFigure 3ashowsamuch more dif\u0002cult case: a1536\u00022048\npixeldigital camera image ofapiece ofsheet music. This\nexample showssigni\u0002cant perspecti vedistortion, andis\nnotamenable tocorrection byasimple rotation ofthe\nwhole image. There isalso additional deformation intro-\nduced bythecamera lens: notice thecurvature ontheleft\nhand sideofthemusic sheet, forexample.\nFigure 3bshowsthe Fourier transform ofthe\nperspecti ve-distorted image. Itcanbeseen thattheFSC is\nnowrather blurred, particularly well awayfrom theorigin,\nwhere ithasabraided appearance. Itisclear thatwhile a\nbest \u0002trotation angle could undoubtedly befound from\ntheFFT andapplied totheimage, thisisnotgoing tobe\nsuf\u0002cient tocorrect theproblems. Amore general and\n\u0003exible approach isrequired.\nTheblurriness oftheFSC inFigure 3barises because\nthevarious distortions, most notably perspecti vedistor -\ntion, haveresulted indifferent regions oftheimage being\nsubject todifferent rotation angles. TheFFT ofsmall re-\ngions oftheimage will notsufferfrom thisproblem be-\ncause thestavelines willberelati velystraight andparallel\noveranysmall region. Hence wewould expect tobeable\ntoobtain agood estimate oftheimage rotation angle over\nsmall regions oftheimage, particularly regions centred on\nstavelines. Therestofthispaper addresses thefollo wing\nquestion: what information canwegetbytaking Fourier\ntransforms ofmanysmall portions ofanimage, andhow\n(a) (b)\nFigure 3:Testimage two,Moonlight: (a)aperspecti ve\ndistorted music image and(b)itsFFT.\ncanweusethatinformation toremo vedistortions inthe\nimage?\n2.4 Windo wed Fourier transf orms\nThe Fourier transform ofasmall portion ofanimage is\ncalled aWindowed Fourier Transform ,because wecan\nconsider that thewhole image hasbeen weighted bya\nwindo wfunction that isnon-zero only near asingle\npoint ofinterest. Thesimplest windo wfunction isabox\nfunction thatisunity within arectangular region andzero\noutside that region. However,theuseofbox windo ws\ncanresult inringing artifactsintheFourier transform,\nconsequently windo wswith aless sharp cutof farepre-\nferred. The windo wing function wehavechosen isthe\nHanning Window ,which uses acosine function (between\ntwotroughs) toweight thepoints inthewindo w.\nFigure 4showsthewindo wedFourier transforms com-\nputed atanumber ofdifferent places inthetestimage of\nFigure 3.Itcanbeseen thattheFFTs canbeexpected to\nyield good estimates ofthelocal rotation angle whene ver\nthewindo wispositioned overthemusic staves,butmight\nyield completely different angles elsewhere.\n3PERSPECTIVE DIST ORTION\nAsanexample ofhowtheknowledge oflocal rotation an-\ngle(s) might beused inremo ving deformations, wefocus\nontheproblem ofremo ving perspecti vedistortion, such\nasthatexhibited byFigure 3.Note thatthistreatment in-\ncludes remo valofpure image rotation asasimple subset.\nThroughout thissection weassume thatthescene be-\ningphotographed isarectangular page ofmusic which we\nwillrefer toasthesourcedocument .This may beaniso-\nlated sheet ofmusic orapage inabook ofmusic. Wecall\nthephotograph ofthesource document thesourceimage.\nOurgoal istoremo veallgeometric deformations includ-\ningrotation from thesource image toproduce anewimage\nthatwecallthetargetimage.Unless stated otherwise we\nwillalsoassume thatthesource document isplanar .\nThe geometry ofphotography can, foranideal cam-\nera,bemodelled mathematically asaperspecti veprojec-\ntion. Iftheaxisofthecamera isperpendicular totheplane\nofthesource document, thesource image isascaled and\n147Figure 4:Some subimages andtheir (Hanning) windo wed\nFFTs.\nrotated butotherwise faithful reproduction ofthedocu-\nment. Ingeneral, however,misalignment ofthecamera\nintroduces aform ofdistortion called perspecti vedistor -\ntion. This section discusses theproperties ofperspecti ve-\ndistorted images, provides amathematical model forit,\nanddevelops apractical procedure forremo ving much of\nthatdistortion.\n3.1 Properties ofperspecti ve-distorted images\nPerspecti veprojection isthemapping ofpoints inascene\ntopoints inanimage bymeans ofstraight lines called\nprojectors thatpass through acommon viewpoint called\nthecentre ofprojection. Foragivencentre ofprojection\nC,each pointPinthesource document maps toaunique\npointP0inthesource image, namely thepoint where the\nprojector CPintersects theplane ofprojection, orimage\nplane .\nUnder perspecti veprojection straight lines project to\nstraight lines, butangles between lines arealtered bythe\nprojection process. Inparticular ,lines that areparallel\ninthedocument arenotparallel inthesource image: all\nsource lines parallel toaparticular reference lineinthe\nscene convergeinthesource image toasingle vanishing\npoint. Anylineinthedocument canbeused asarefer -\nence lineandhasacorresponding vanishing point. The\nvanishing points associated with thexandyaxesofthe\ndocument arecalled theyxandy-axis vanishing points re-\nspecti vely(see Figure 5),andthetwotogether arecalled\ntheprincipal vanishing points .Weassume thattheorigi-\nnaldocument isrectangular andthatthexandyaxesare\naligned with itsboundaries.\nGeneral perspecti veprojection maps thebounding\nrectangle ofthedocument toanarbitrary quadrilateral in\nthesource image. Theboundaries ofthequadrilateral con-vergeontheprincipal vanishing points asshowninFig-\nure5.\nFigure 5:Testimage three anarti\u0002cally constructed\ndocument anditsperspecti ve-distorted image.\n3.2 Determining theperspecti vetransf ormation\nIfwecanidentify inthesource image thefour corners\nofthedocument page being photographed, and ifwe\nalso knowtheaspect ratio oftheoriginal document itis\nstraightforw ardtocompute aninversetransformation that\nwilltransform theentire page back toitsinitial rectangu-\nlarshape. Forexample MATLAB ,which weused forall\nthecalculations inthispaper ,hasafunction maketform\nthatcomputes therequired transformation. Thefunction\ntakesasinput thecoordinates ofthefour corners ofanar-\nbitrary quadrilateral both before andafter perspecti vepro-\njection (orafter andbefore fortheinversetransformation).\nInpractice, however,identifying thedocument edges and\ncorners inanimage canbeproblematic, forthefollo wing\nreasons\n\u000fSome edges orcorners ofthedocument may beto-\ntally missing from theimage, either because ofinac-\ncurac yinlining upthephotograph, ortointentionally\nmaximise theresolution oftheimage bynotcaptur -\ningthemargins.\n\u000fBoundaries ofdocuments canbeill-de\u0002ned when\nphotographed against asimilar -coloured back-\nground.\n\u000fWhen photographing orscanning book pages, the\nbound edge may becurvedorill-de\u0002ned asacon-\nsequence ofthepage being non-planar .These prob-\nlems canalso occur with loose pages, which may\ncurl.\n\u000fThe corners ofthe document photographed or\nscanned may bedog-eared, making theboundary\nnon-rectangular ,especially inolder music that has\nhadconsiderable use.\n148\u000fCamera distortions, likepincushion andbarrel distor -\ntion, giverisetonon-straight edges.\nRather than using image-processing methods tode-\ntermine thecorners andedges oftheoriginal document\nwithin thesource image, ourmethod adopts adifferent\napproach todetermining theperspecti vemap bytaking\nadvantage ofthestavelines that will bepresent inmu-\nsicimages. Using awindo wed FFT,wedetermine thelo-\ncalorientation oftheprincipal axesatalargenumber of\nplaces throughout theimage. From allthatdata wecan\ndetermine best-\u0002t estimates ofthetwoprincipal vanishing\npoints. Giventhevanishing points wecanthen determine\ntheperspecti vemap. This approach ispotentially more\nrobust,immune toalltheproblems itemized above.We\nnowexplain thisprocedure inmore detail.\n3.3 Estimating local axis orientation\nOurmethod fordetermining vanishing points requires es-\ntimates oftheorientations ofthedocument' sxandy-axes\natvarious points intheimage. Ateach point ofinterest\nwetakeawindo wed Fourier transform andattempt toes-\ntimate therotation angle ofboth thexandyaxes.Asindi-\ncated byFigure 4,estimates ofthex-axis rotation areex-\ncellent when thesample ispositioned overthestavelines\nbutcanbepoor elsewhere. Estimating they-axis rotation\nisrather more problematic everywhere. Wewillreturn to\nthequestion ofestimating y-axis rotation later,focusing\nonx-axis rotation tobeginwith.\nGivenawindo wed Fourier transform weestimate the\nx-axis rotation by\u0002nding themaximum Fourier compo-\nnent inthevicinity oftheFourier y-axis, which isthe\naxis onto which horizontal structures inthespatial do-\nmain map. More formally ,we\u0002ndthemaximum Fourier\ncomponent F(xi;yi)where yi>=25andjtan\u00001xi\nyij<\n20\u0019=180.This means wearecon\u0002ning ourattention to\nspatial components with frequencies greater than 25cy-\ncles perwindo wwidth and with anorientation that is\nwithin 20\u000eofhorizontal.\nThe\u0002gure of20\u000esetsanarbitrary limit ontheamount\nofrotation and/or distortion ourmethod canhandle. We\ncon\u0002ne oursearch toFourier components whose fre-\nquenc yisgreater than 25cycles pertransform windo w\nheight inorder toensure reasonable accurac yinouresti-\nmate oftherotation angle, giventhediscretization errors.2\nWefurther reduce theeffectofdiscretization errors byes-\ntimating theposition ofthemaximum value tosub-pix el\naccurac yalong alineperpendicular totheFourier axis of\ninterest.\nInthecase oftheFourier y-axis, ifthemaximum value\nisatlocation (xi;yi)with respect totheFourier origin and\nhasmagnitude Fi,we\u0002ndthemagnitudes Fi\u00001andFi+1\nofthecomponents at(xi\u00001;yi)and(xi+1;yi).Themag-\nnitude Fmaxandposition xmaxofthemaximum along the\nliney=yiisthen estimated by\u0002tting atentfunction to\nthethree values.3Weusethevalue\u0012=tan\u00001xmax\nyias\n2Although thestavelines themselv esprobably haveaspatial\nfrequenc ylessthan 25cycles across thewindo w,theyproduce\nstrong harmonics athigher frequencies.\n3Thetentfunction isf(x)=max(0;abs(x\u0000xi)).ourestimate oftherotation angle ofthespatial x-axis at\nthecentre oftheFourier windo w.Also, since strong spa-\ntialcomponents likestavelines giverisetostrong spatial\nfrequencies, weuseFmax asacon\u0002dence estimator for\nthesubsequent determination ofvanishing points.\nDetermining thelocal rotation angle ofthespatial y-\naxisisexactly thesame asforthex-axis butapplying the\nfunction tothetranpose oftheimage. Asweshall see\nlater,thedifference here isthaty-direction lines areless\ncommon, anditcanbeachallenge toaccurately identify\nthey-axis rotation.\n3.4 Determining vanishing points inaperspecti ve\ndistorted image\nLetus\u0002rstconsider thevanishing point foralllines par-\nallel tothex-axis. Assume thatwehaveestimates atvar-\nious points intheimage oftheorientation oflines that\nwere horizontal inthedocument, using themethod ofSec-\ntion3.3. Wetakesamples using aregular grid approach,\nbuttoavoidaliasing problems4wejitter sample posi-\ntions byrandomly positioning each sample within agrid\ncell.\nThe goal isto\u0002nd thecoordinates ofthevanishing\npoint givensuch data. However,thatisanill-conditioned\nproblem, since thelines arenearly parallel andthevan-\nishing point isalong wayfrom theimage andmay even,\nintheextreme case ofaperfectly takenpicture, beatin-\n\u0002nity .Soweinstead specify thevanishing point interms\noftwowell-conditioned measures: therotation angle\u00120at\ntheimage origin andtherecipr ocal,\u000bofthex-coordinate\nofthevanishing point.\nWithreference toFigure 6,assume thetoplefthand\ncorner oftheimage isatlocation (0;0),with acoordinate\nsystem inwhich yincreases downw ards. Suppose thatwe\ncanestimate from windo wed Fourier transforms theangle\n\u0012(x;y)ofthestavelines tothehorizontal atanarbitrary\nsetofpoints f(x;y)gintheimage. Assume allthehori-\nzontal lines meet atavanishing pointV=(vx;vy).Then\nateach pointP=(x;y)forwhich wehavea\u0012estimate:\ntan\u0012=vy\u0000y\nvx\u0000x(2)\nq\n(vx, vy)V0\nq(0,0)\n(x,y)PQ\ndP'x\ny\nFigure 6:Thevanishing point forhorizontal lines.\nLett=tan\u0012andt0=tan\u00120=vy=vx.Eliminating\nvyfrom Equation 2andusing\u000b=1\nvxgives:\n4Anextreme example ofanaliasing problem would beifall\noursamples were tomiss thestavelines because thevertical\nsample spacing wasequal tothestavelinespacing.\n149t=t0+\u000b(xt\u0000y) (3)\nEquation 3applies ateach pointPforwhich wehave\nanestimate of\u0012.Hence, givenasetof(x;y;\u0012)tuples,\nEquation 3yields asetofequations, linear intheparam-\neterst0and\u000b,thatcanbesolvedinaleast-squares sense\nfort0and\u000b.Weuseaweighted least-squares method in\nwhich theFourier magnitudes, Fmax inSection 3.3,are\nused astheweights.\nThey-axis vanishing point isfound using thesame\napproach.\n3.5 Computing theperspecti vemap fromthe\nvanishing points\nHaving determined thevanishing points forthesource\nimage, wenowwish to\u0002nd thetransformation from the\nsource image tothetargetimage, which isanimage thatis\nanon-distorted non-rotated representation oftheoriginal\ndocument. Atthisstage wehaveanimage andestimates\nofitstwovanishing points. Wedonotknowwhat portion\noftheimage isoccupied bythedocument, northeaspect\nratio ofthedocument.\nOurmethod istousethevanishing points toconstruct\naquadrilateral thatistheperspecti veprojection ofanaxis-\naligned rectangle intheoriginal document space. For\nsmall deformations, ourquadrilateral occupies most ofthe\nimage. Wethen compute thetransformation thatwillmap\nthequadrilateral toarectangle inthetargetimage with the\nsame dimensions asthesource image. This approach cor-\nrectly restores allhorizontal andvertical lines inthedoc-\nument butmay introduce some aspect-ratio distortion. If\ntheperspecti vedistortion issmall theaspect-ratio distor -\ntionisalso small andisunlik elytobeasigni\u0002cant prob-\nlemwith music images.\nWithreference toFigure 7,thesteps indetermining\ntheperspecti vemap giventhetwoprincipal vanishing\npoints PandQare:\n\u000fTakethetoplefthand corner oftheimage asoneof\nthequadrilateral vertices A.\n\u000fDetermine vertices BandC,which arethepoints\nwhere thelinesAP andAQ intersect theimage\nboundaries.\n\u000fFind thevertexDwhere thelinesCPandBQinter-\nsect.\nThevanishing points, computed using Equation 3,are\nin(tan\u0012;\u000b)form. Theyaregenerally along distance\nfrom theimage andmay evenbeatin\u0002nity .Hence the\nabovegeometric calculation should bedone inawaythat\navoids explicit computation ofthecartesian coordinates\nofthevanishing points. Our approach istocompute an\nalgebraic solution, asfollo ws.\nInFigure 7,wandharethewidth andheight respec-\ntivelyofthesource image, which hasprincipal vanishing\npoints PandQ.ABDC istheinscribed quadrilateral that\nwillbemapped bytheinverseperspecti vetransformation\ntotheboundary ofthetargetimage, asdescribed above.Using anotation likethatofEquation 3,PandQarethe\nCartesian points (1=\u000b;tan\u0012=\u000b)and(1=\f;tan\u001e=\f).\nBandCarethepoints (w;wtan\u0012)and(htan\u001e;h)\nrespecti vely.Disthepoint ofintersection ofthelinesBQ\nandCP.Solving forDyields\n\u0012hw\u000bs\u0000hs+hw\f\u0000w\nh\u000bw\f\u00001;h\u000bw\u0000h+wth\f\u0000wt\nh\u000bw\f\u00001\u0013\n(4)\nwhere t=tan\u0012ands=tan\u001e.Note thatthisexpres-\nsion iswell conditioned when thevanishing points areat\nin\u0002nity ,i.e.,when\u000band/or \farezero.\nFigure 7:Animage rectangle, itsvanishing points, and\ntheinscribed quadrilateral\n4RESUL TS\nInthissection weevaluate themethod oftheprevious sec-\ntiononthree testimages:\n\u000fAsimple synthetic testdocument, shownontheleft\nofFigure 5,which wesubjected toavariety ofrota-\ntions andperspecti vedistortions.\n\u000fAtestimage (Figure 1)which hashadnegligible per-\nspecti vedistortion butsigni\u0002cant rotation. Werefer\ntothisastheAngelstestimage.\n\u000fThe motivating test image (Figure 3)which has\nhadsigni\u0002cant perspecti vedistortion plus some non-\nlinear distortions, most noticeably barrel distortion.\nWecallthistheMoonlight testimage.\n4.1 The synthetic image, rotated\nThesynthetic testimage wasrotated by7\u000e.Orientations\nwere estimated ina5\u00027grid, with oneorientation es-\ntimation atarandom position within each grid cell. A\n100\u0002100windo wwasused forthewindo wed FFT,so\nthecellgrid isinset from theimage boundary by50pix-\nels(half thewindo wsize) allaround. Theoriginal image\nis1810 \u00021396 pixels.Asexpected, good estimations of\norientation were obtained only inthevicinity ofstrong\n150vertical orhorizontal spatial structures. The estimations\nofthevertical orientation aregenerally veryweak except\naround theimage boundary andthebarlines attheends\nofthestaves.Nonetheless, thebest-\u0002t estimations ofboth\nthehorizontal andvertical vanishing points were generally\nveryreliable because only thehigh-con\u0002dence estimates\ncontrib uted signi\u0002cantly .\nWeran10different trials onthetestimage with dif-\nferent random samplings each time. Theworstresult oc-\ncurred when oneofthesamples wastakeninthedown-\nwardcurveofthe`S'character inthetitle, returning a\nstrong x-axis rotation estimate of\u000014\u000e.Withonly 35\nsamples intotal, andmanyofthose having verylowcon-\n\u0002dence, thesingle outlier hadasigni\u0002cant effectonthe\nx-axis vanishing point estimate. However,evenwith that\nlargeanerror ,thecomputed targetimage stilllookedvery\ngood; itshowed only averysmall amount ofintroduced\nperspecti vedistortion along thex-axis. Errors ofthissort\ncould bereduced either byamore sophisticated \u0002tting\nmethod that eliminated outliers orsimply byincreasing\nthenumber ofsamples. Using alargerwindo wsize for\ntheFFT would also help: anoptimal windo wsizeshould\naccommodate afullsetofstavelines andbeappreciably\nlargerthan anytextpresent. Thewindo wsizeof100pix-\nelsused forallthetests inthissection issuitable forthe\nother twotestimages butisabitsmall forthissynthetic\ntestimage, which hasalargefont andawide setofstave\nlines.\nWealso created aperspecti vedeformed synthetic im-\nage(see Figure 8a), andused ourmethod todetermine\nthevanishing points in11randomised trials. Figure 8b\nshowsatypical targetimage. Therecoveryofthesource\ndocument isessentially perfect apart from thefactthatthe\nimage needs tobecropped.\n(a) (b)\nFigure 8:(a)The perspecti ve-deformed synthetic test\nimage, and (b)thetargetimage recovered from the\nperspecti ve-deformed synthetic testimage.\n4.2 Rotation andperspecti vedistortion testimages\nWeapplied ouralgorithm tothegenuine testimage ofFig-\nure1,which isanimage with verylittle perspecti vedis-\ntortion butsigni\u0002cant rotation. Theresults ofSection 2.1\nsuggest thattherotation angle is6:149\u000esowewould ex-\npect togetx\u0000andy-vanishing points of(0.1077,0) and\n(-0.1077,0) respecti vely.\nIn10randomised trials todetermine thevanishing\npoints wefound thatwhile theresults forthex-axis van-ishing point were consistently good, theestimates forthe\ny-axis vanishing point were highly variable. Manyof\nthey-axis vanishing points were seriously wrong andled\ntounsatisf actory targetimages inwhich thestavelines\nwere horizontal butthere wassevereperspecti vedistor -\ntionalong they-axis.\nWetried increasing thesampling gridto10\u000214,giv-\ning140samples intotal, four times asmanyasbefore.\nHowever,poor results stilloccurred fairly frequently ,and\nmanyofthehighest-con\u0002dence orientation estimates were\nwrong. Particularly bad estimates arose from samples\ntakeninthetreble-clefs andinaportion ofitalicized text.\nTheMoonlight testimage, which suffersfrom severe\nperspecti vedistortion, yields similar results totheAngels\nimage. Figure 9showsthehorizontal vectors thatwere\ndetected inonetrial. Again weobtained consistent es-\ntimation ofthehorizontal vanishing point (avariation of\n\u00067%intan\u0012and\u00065%in\u000b),butmuch more erratic esti-\nmations ofthevertical vanishing point.\nFigure 9:The vectors found bytheFFT windo wofthe\nMoonlight test.\nThe music intheMoonlight test image waspho-\ntographed onawood-grain table andthegrain ofthista-\nbleadds totheproblems indetermining vanishing points.\nWhen werestricted thesampling grid tothecentre 80%\noftheimage, thereby avoiding thewoodgrain regions, the\nestimations ofthex-axis vanishing point areslightly im-\nproved,though notinastatistically signi\u0002cant way,but\ntheestimations ofthey-axis vanishing point aresigni\u0002-\ncantly different from before andevenmore variable. Ifwe\nusethose vanishing points tocorrect forperspecti vedis-\ntortion wetend togetresults inwhich thestavelines are\nhorizontal andparallel buttheimages showsevereandin-\ndeed increased perspecti vedistortion along they-axis.\n4.3 The best wecando\nThe aboveresults showthatwecanobtain good results\nforthex-axis vanishing point. This allowsustocorrect\nimages likethetheMoonlight testinsuch awaythatthe\nstavelines areallhorizontal. Subjecti velythisyields an\nenormous impro vement. However,attempts tofurther cor-\nrectforperspecti vedistortion along theyaxishaveproved\ndiscouraging. While itmight bepossible tousesampled\n151windo wed FFTs toestimate localy-axis orientation, we\nbelie vethatthesampling would havetobeconstrained,\nsay,byafeature detection algorithm, ifthemethod isto\nbear fruit.\nUsing only FFT-based techniques, thebest compro-\nmise wehavecome upwith istouseonly thex-axis van-\nishing point tocorrect perspecti vedistortion. Weposition\nthey-axis vanishing point atin\u0002nity onalinethrough the\ncentre oftheimage, perpendicular tothelinethrough the\nsame centre tothex-axis vanishing point.\nThis approach works perfectly forrotated images like\ntheAngels testimage andallowsustoatleast obtain hor-\nizontal stavelines incases liketheMoonlight testimage,\nasillustrated inFigure 10a. Figure 10bshowsaclose up\nportion ofthecorrected image.\n(a) (b)\nFigure 10:Theresult ofapplying ourbest compromise\nmethod totheMoonlight testimage: a)fullpage andb)a\ncloseup.\nTheexperimental implementations were notdesigned\nforspeed, andwere implemented inMatlab .Processing of\neach page typically takesaround 30seconds, andthere are\nlikelytobemanyopportunities forspeed impro vement in\nanimplementation.\n5CONCLUSIONS\nAgile digital music libraries bene\u0002t from having fastand\nsimple capture techniques, andthispaper hasillustrated a\nuseful approach thatuses adigital camera tocapture mu-\nsicquickly ,without theuser having tobesoconcerned\nabout introducing rotation orperspecti vedistortion. Ap-\nplying awindo wed FFT tomusic isveryeffectiveatde-\ntermining where thelocalx-axis isduetothepredomi-\nnance ofstavelines. This leads toareliable method that\ncorrects forrotations andforx-axis perspecti vedistortion.\nStavelines inthecorrected image arehorizontal andfree\nofobvious aliasing artifacts. However,determining the\nlocaly-axis orientation with thewindo wed FFT ismore\ndif\u0002cult because vertical lines arenotsodominant. As\naconsequence, correcting y-axis perspecti vedistortion is\nproblematic. While thispaper hasmade some progress\ntowards aviable solution, areliable algorithm willproba-\nblyneed toapply feature-e xtraction methods \u0002rst, sothat\nvertical orientation estimates canbemade only inregions\nlikelytoyield good results. Further automation ofthepro-cess would bene\u0002t from adapti vethresholding toallowfor\nunevenlighting onthepage, andautomatic cropping tore-\nmoveartifactsaround theadjusted image.\nREFERENCES\nD.Bainbridge andT.C.Bell. The challenge ofoptical\nmusic recognition. Computer sandtheHumanities ,35\n(2):95121, May 2001.\nS.Baumann andA.Dengel. Transforming printed pi-\nanomusic into MIDI. InH.Bunk e,editor ,Advances\ninStructur alandSyntactic Pattern Reco gnition (Pro-\nceedings ofInternational Workshop onStructur aland\nSyntactic Pattern Reco gnition) ,volume 5ofSeries in\nMachine Perception and Arti\u0002cial Intellig ence,pages\n363372, Bern, 1992. World Scienti\u0002c.\nN.P.Carter .Automatic Reco gnition ofPrinted Music in\ntheConte xtofElectr onic Publishing .Ph.D. thesis, De-\npartments ofPhysics andMusic, University ofSurre y,\nGuildford, UK, Feb.1989.\nA.T.Clark e,B.M.Brown,andM.P.Thorne. Inexpen-\nsiveoptical character recognition ofmusic notation: A\nnewalternati veforpublishers. InProceedings ofthe\nComputer sinMusic Resear chConfer ence,pages 84\n87,Lancaster ,UK, Apr.1988.\nI.Fujinaga, B.Penn ycook, andB.Alphonce. The opti-\ncalmusic recognition project. Computer sinMusic Re-\nsearch,3:139142, 1991.\nH.Kato andS.Inokuchi. Arecognition system forprinted\npiano music using musical knowledge andconstraints.\nInProceedings oftheInternational Association forPat-\nternReco gnition Workshop onSyntactic andStructur al\nPattern Reco gnition ,pages 231248, Murray Hill, New\nJerse y,June 1990.\nP.Martin andC.Bellissant. Low-levelanalysis andrecog-\nnition ofmusic drawing images. InProceedings ofFirst\nInternational Confer ence onDocument Analysis ,vol-\nume 1,pages 417425, Saint-Malo, France, 1991.\nT.Matsushima, T.Harada, I.Sonomoto, K.Kanamori,\nA.Uesugi, Y.Nimura, S.Hashimoto, andS.Ohteru.\nAutomated recognition system formusical score the\nvision system ofWABO T-2. InBulletin ofScience and\nEngineering Resear chLabor atory ,volume 112, pages\n2552. Waseda University ,Tokyo,Japan, Sept. 1985.\nT.Reed. Optical Music Reco gnition .M.Sc. thesis, De-\npartment ofComputer Science, University ofCalgary ,\nCanada, Sept. 1995.\nJ.RileyandI.Fujinaga. Recommended bestpractices for\ndigital image capture ofmusical scores. OCLC Systems\nandServices ,19(2):6269, 2003. ISSN 1065-075X.\nM.Roth. Anapproach torecognition ofprinted music.\nM.Sc. thesis, Eidgen ¨ossische Technische Hochschule,\nZ¨urich, Jan.1994.\nM.J.Taylor ,A.Zappala, W.M.Newman, and C.R.\nDance. Documents through cameras. ImageVision\nComput. ,17(11):831844, 1999.\n152"
    },
    {
        "title": "SoniXplorer: Combining Visualization and Auralization for Content-Based Exploration of Music Collections.",
        "author": [
            "Dominik Lübbers"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418021",
        "url": "https://doi.org/10.5281/zenodo.1418021",
        "ee": "https://zenodo.org/records/1418021/files/Lubbers05.pdf",
        "abstract": "Music can be described best by music. However, current research in the design of user interfaces for the exploration of music collections has mainly focused on visualization aspects ignoring possible benefits from spatialized music playback. We describe our first development steps towards two novel user-interface designs: The Sonic Radar arranges a fixed number of prototypes resulting from a content-based clustering process in a circle around the user’s standpoint. To derive an auralization of the scene, we introduce the concept of an aural focus of perception that adapts well-known principles from the visual domain. The Sonic SOM is based on Kohonen’s SelfOrganizing Map. It helps the user in understanding the structure of his music collection by positioning titles on a two-dimensional grid according to their high-dimensional similarity. We show how our auralization concept can be adapted to extend this visualization technique and thereby support multimodal navigation. Keywords: Content-based Music Retrieval, Exploration, Visualization, Auralization, User Interface 1",
        "zenodo_id": 1418021,
        "dblp_key": "conf/ismir/Lubbers05",
        "keywords": [
            "Music",
            "Visualization",
            "Spatialized playback",
            "User interface",
            "Content-based clustering",
            "Sonic Radar",
            "Aural focus of perception",
            "Kohonens SelfOrganizing Map",
            "Multimodal navigation",
            "Auralization"
        ],
        "content": "SONIXPLORER : COMBINING VISUALIZATION AND AURALIZATION\nFOR CONTENT-BASED EXPLORATION OF MUSIC COLLECTIONS\nDominik L¨ ubbers\nLehrstuhl Informatik V\nRWTH Aachen University\nAhornstr. 55\n52072 Aachen, Germany\nluebbers@cs.rwth-aachen.de\nABSTRACT\nMusic can be described best by music. However, current\nresearch in the design of user interfaces for the explo-\nration of music collections has mainly focused on visual-\nization aspects ignoring possible beneﬁts from spatialize d\nmusic playback. We describe our ﬁrst development steps\ntowards two novel user-interface designs:\nTheSonic Radar arranges a ﬁxed number of proto-\ntypes resulting from a content-based clustering process in\na circle around the user’s standpoint. To derive an aural-\nization of the scene, we introduce the concept of an au-\nralfocus of perception that adapts well-known principles\nfrom the visual domain.\nThe Sonic SOM is based on Kohonen’s Self-\nOrganizing Map. It helps the user in understanding the\nstructure of his music collection by positioning titles on a\ntwo-dimensional grid according to their high-dimensional\nsimilarity. We show how our auralization concept can be\nadapted to extend this visualization technique and thereby\nsupport multimodal navigation.\nKeywords: Content-based Music Retrieval, Explo-\nration, Visualization, Auralization, User Interface\n1 INTRODUCTION\nOngoing technological advances especially in the ﬁeld\nof data compression, storage capacity and network band-\nwidth have lead to a drastic increase in the size of music\ncollections that are available to today’s listener. Online\nmusic portals offer their users direct access to an over-\nwhelming number of songs.\nTo efﬁciently use this huge amount of music, new ac-\ncess methods have to be developed. These can be roughly\ncategorized into two groups:\n1. If the user has a dedicated song in mind and is able to\narticulate his information demand in some way, well-\nPermission to make digital or hard copies of all or part of thi s\nwork for personal or classroom use is granted without fee pro -\nvided that copies are not made or distributed for proﬁt or com -\nmercial advantage and that copies bear this notice and the fu ll\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonknown music information retrieval techniques can\nbe applied. These include standard database queries\non song metadata such as title or artist and content-\nbased queries e.g. following the popular Query-By-\nHumming application scenario.\n2. Sometimes the information retrieval goal cannot be\ndeﬁned precisely. Instead of this, the user might want\ntoexplore the music collection, take a closer look at\npieces that he ﬁnds interesting and move around fur-\nther.\nDespite our belief, that the latter paradigm resembles\nthe way customers often behave in music stores, current\nresearch in the music information retrieval community has\nmainly focused on the ﬁrst approach.\n1.1 Related Work\nIf we restrict ourselves to standard metadata of songs,\nwell-known data visualization techniques developed in the\ndata mining community are applicable. Torrens et al. re-\nview three of them (discs, rectangles and tree-maps) in the\ncontext of the visualization of personal music libraries [1 ].\nPampalk calculates an overall song similarity based on\nperceptual features that mainly model rhythmic aspects of\nthe piece and organizes them on a Self-Organizing Map\nthat preserves the topology of the song space [2]. Titles\nthat are perceptually similar are visualized by so called\n”Islands of Music”.\nDespite their difference in the features they use, both\napproaches rely solely on visual communication between\nthe system and the user. The impedance mismatch re-\nsulting from a visual representation of audio information\nseems unnatural and unnecessary having in mind the hu-\nman capabilities to process sound information. The well\nknown cocktail party effect can be seen as an example of\nthe powerful audio information processing that our brain\nis capable of.\nBrazil et al. investigate combinations of visual and au-\nral access methods for sound collections [3]: In their ﬁrst\nimplementation, sound objects are placed on a grid ac-\ncording to selectable properties. The user can navigate\nthis visualization by positioning a cursor on the plane that\nis surrounded by a circular aura : All sound objects that\nare placed inside this shaded area are played simultane-\nously and spatialized according to their relative position\n590to the cursor that models the user’s actual position in the\nsound space.\nWith the help of the Marsyas audio information re-\ntrieval framework the Sonic Browser has been extended\nto an Audio Retrieval Browser that facilitates the use of\ncontent-based features as visualization dimensions [4].\nThe SoundSpace browser contained in Tzanetakis’ au-\ndio suite Marsyas3D [5] follows a similar approach: Au-\ntomatically generated audio thumbnails for music in the\nneighborhood of the actual selection are played simultane-\nously. Tailored to its usage in the context of the Princeton\nDisplay Wall equipped with a 16-speaker surround sys-\ntem, he is thereby able to realize an intuitive and immer-\nsive browsing environment.\nDespite these ﬁrst approaches we believe that more ef-\nfort is needed to develop content-based multimodal music\nexploration tools to complement the search-centered ac-\ntivities in the music information retrieval community.\nIn the next section we brieﬂy discuss two measures\nto quantify song similarity. This is followed by an in-\ntroduction to our content-based aural music exploration\nenvironment soniXplorer . We present two alternative ap-\nproaches, namely the Sonic Radar and the Sonic SOM .\nWe proceed with some more technical remarks on our pro-\ntotype and conclude with ideas for future extensions of\nthis work in progress.\n2 SONG SIMILARITY\nWhat makes two songs similar? Similarity is a very high-\ndimensional measure that can incorporate many different\naspects of music, e.g. its melody, harmony, genre, lyrics,\netc. Additionally, the notion of similarity is highly user-\nand context-dependent. So it seems unrealistic to assume\na formula that is capable of modelling overall song sim-\nilarity precisely. Nevertheless, there has been some re-\nsearch to ﬁnd approximations concentrating on different\nperspectives.\nPampalk reviewed ﬁve different sound similarity mod-\nels [6] and found out in a simple evaluation, that the sim-\nilarity measures by Logan et al. and by Aucouturier et al.\noutperformed the other approaches considered. Both mea-\nsures concentrate on the spectral characteristics of a song :\nLogan and Salomon [7] calculate for each piece a song\nsignature that is basically a weighted set of spectral seg-\nment clusters. They compare two signatures using the\nEarth Mover’s distance with a symmetric variant of the\nKullback-Leibler divergence as ground distance.\nAucouturier and Pachet [8] represent the ”timbral\nquality” of a song by a Gaussian mixture model (GMM)\nfor the space of MFCCs calculated on short segments. The\nsimilarity between two pieces A and B is modelled as the\nlikelihood that A’s GMM generates B’s feature values. It\nis approximated using Monte Carlo sampling.\nWe decided to utilize the approach by Logan and Sa-\nlomon in our prototype, since its computational complex-\nity is signiﬁcantly lower. However, this choice can eas-\nily be replaced by other alternatives for distance calcu-\nlation. With the help of classical multidimensional scal-\ning (CMDS) we assign vector coordinates to each piece\nso that the Euclidean distance between two song vec-tors resembles the value in the distance matrix. When\nthe number of songs in the collection increases, CMDS\nmight become too time-consuming since its complexity is\nquadratic in the number of pieces. In this case, the linear\nFastMap algorithm [9] might be an alternative, although\nCMDS seems to lead to mappings of higher quality in the\ncontext of audio visualization systems [10].\n3 SONIC RADAR\nTzantakis et al. mention that only 8 simultaneous audio\nstreams have proven to be practical in the scenario of the\n16-speaker Princeton Display Wall [5]. This supports the\nexperience we gained in some initial experiments: Play-\ning back more than two complex songs (like e.g. modern\npop/rock-music that are characterized by high loudness\nvalues over the whole spectrum) on full level in parallel\nasks too much from the user’s listening capabilities even\nif the audio streams are spatialized to different stereo-\nchannels: Instead of providing orientation and fast access\nto the user, he is not even able to recognize one song any\nmore.\nTherefore we enhance Brazil’s concept of a surround-\ning aura by introducing a focus of perception , that mim-\nics the concentration of our visual perception to the en-\nvironment of the point we are looking at. We transfer\nthis idea to the audio domain by adjusting the playback\nlevel not only according to the distance of the song from\nthe standpoint but also depending on the angle between\nthe beams towards the focus of perception and the piece’s\nposition (see ﬁgure 1). Although song 1and song 2share\nFigure 1: Standpoint and Focus of Perception\nthe same distance to the user’s standpoint, song 1is played\nback louder since |ϕ1|is signiﬁcantly lower than |ϕ2|. We\nmodel this inﬂuence on the sound level with a Gaussian\ndensity function that is centered at ϕ= 0◦and has an\nuser-adjustable variance σ2. Other windowing functions\ncould be used likewise.\nSince we consider a 2-speaker or headphone environ-\nment we simply spatialize the mono song according to\na linear function depending on ϕ. If the piece is on the\nbeam between the standpoint and the focus of perception\n(ϕ= 0◦), its signal is panned to the left and the right\nchannel equally. For positive ϕ, the signal is mapped to\nthe left, for negative ϕit is mapped to the right channel.\nThe resulting discontinuity at ϕ= 180◦has turned out\nto be no problem since the Gaussian density function for\n591reasonable ϕdisappears in this area.\nWe combine this approach with a simple visualization\ntechnique:\nFigure 2: Screenshot of the Sonic Radar prototype\nFirst, the music collection is hierarchically partitioned\nby successively applying k-means clustering for a user-\nadjustable number of clusters k. For each cluster a proto-\ntype song is identiﬁed based on the smallest distance to the\ncluster center. The prototypes of the current subclusters\nare equally distributed on a circle around the standpoint\n(see ﬁgure 2). This arrangement in conjunction with the\ndirectional listening simulation explains the name Sonic\nRadar for this exploration interface.\nThe user can rotate the circle to quickly scan through\nthe song prototypes. He can narrow or widen the variance\nσ2to focus on the currently playing piece ”in front” or to\nintegrate the surrounding titles in the playback, resp.\nAs shown in ﬁgure 2, the most central prototype is\nhighlighted. Furthermore, all pieces in this cluster are\nlisted on the right hand side of the window. Clicking on\none of these titles starts the playback of the song in the me-\ndia player below. Double-clicking in the Sonic Radar -\narea allows to step down in the cluster hierarchy and ex-\nplore the subclusters of the current prototype visually and\naurally. Additionally, the user can rearrange the clustere d\npiece by dragging a title from the list and dropping it to a\ncluster on the left hand side.\n4 SONIC SOM\nPartitioning a song collection into disjoint clusters we lo se\nmuch of the similarity information between pieces: The\nvector space coordinates of a title are reduced to a crisp\nmembership to one cluster; information about the degree\nof this membership given by the distance to the center and\nthe song’s similarity to pieces of other clusters is dropped .\nSince humans are used to estimate distances between\npoints on a two-dimensional plane, visualization tech-\nniques that map a high-dimensional space to a low-\ndimensional representation preserving the similarity rel a-\ntionships as far as possible are in widespread use.\nOne approach based on artiﬁcial neural networks pro-\nposed by Kohonen [11] is known as the Self-Organizing\nMap. The (typically 2-dimensional) visualization space\nis divided into disjoint cells {yi}. Each cell is associated\nwith a weight vector wifrom the data vector space. In\neach iteration step ta randomly chosen data point xjis as-sociated with cell ycjsuch that ||xj−wcj||is minimized.\nAfter ﬁnding this Best Matching Unit the weight vectors\nofycjand its topological neighbors on the map are up-\ndated according to the following equation:\nwi(t+ 1) = wi(t) +α(t)·hicj(t)[xj−wi(t)]\nwhere α(t)denotes the learning rate and hicjmodels\nthe neighborhood relation between cell yiand the Best\nMatching Unit ycj, typically by some Gaussian-like func-\ntion. As both factors inﬂuencing the adaption strength de-\ncrease with time t, the SOM converges to a conﬁguration\nwhere the Best Matching Units of similar data points are\nlocated close to each other on the map.\nWe apply this sequential training algorithm to calcu-\nlate a rectangular SOM of 50x50 cells. Depending on the\nusers’ current choice of visible area and zoom level this\nmap is converted to a visualization plane of pixels with\nassociated weight vectors that are calculated by bicubic\ninterpolation. The grey value of a pixel is given by the\ndistance of its weight vector to the closest data point. Ad-\nditionally, a pixel is colored red if its weight vector is the\nnearest neighbor to a title in the music collection.\nFigure 3: Screenshot of the Sonic SOM prototype\nAs can be seen in ﬁgure 3, this leads to bright areas\naround clusters of similar titles. The user can change his\nstandpoint in the map, zoom in and out and rotate the\nSOM plane.\nThe visual exploration of the SOM is supported by\nan auralization of the surrounding titles, that resembles\nthe approach described for the Sonic Radar : A focus of\nperception determines the playback level of the songs in\nthe neighborhood of the standpoint. However, in contrast\nto the Sonic Radar , pieces can be located very close to\neach other on the SOM leading to an intransparent mix of\ndifferent sounds if played simultaneously. Therefore we\npartition the environment of the standpoint into kdisjunct\nslices and select the closest song in each of these direction\nclasses for playback. Moving to another standpoint, the\ncurrently playing pieces are preferably chosen as the new\nsegment prototypes. Thereby we have reduced the SOM\nauralization problem to a Sonic Radar -like situation.\nThe currently playing pieces are visualized by blink-\ning pixels. The user can change the current selection of ti-\ntles by right-clicking on songs to toggle their membership\nin the playback. Furthermore, the right hand side of the\nwindow lists all currently displayed pieces in the collec-\ntion sorted by their distance from the focus of perception.\n5925 PROTOTYPE\nWe are currently developing a prototype to test and eval-\nuate our interfaces. Since the described music similarity\nmeasures are readily available in the Matlab toolbox by\nPampalk [6], we chose to use it for the calculation of the\ntitle distance matrix and the mapping to a vector space by\nmultidimensional scaling.\nTheSonic Radar and the Sonic SOM user interfaces\nare realized as Java applications, that access the precalcu -\nlated Matlab ﬁles. We tested Sun’s Java Sound implemen-\ntation for Windows and found that its playback latency\nis not acceptable for use in an immersive exploration sce-\nnario. To overcome this problem and to be open for exten-\nsions to multichannel playback we realized a Java-ASIO-\nbridge that consists of a thin Java layer communicating\nvia JNI with a C++ layer that handles the function calls\nand callback hooks to the ASIO interface, for which low-\nlatency drivers even for semi-professional soundcards ex-\nist.\nFor simplicity reasons, the SOM calculation is cur-\nrently done in Matlab utilizing the SOM Toolbox1, al-\nthough we plan to implement it in Java since we consider\nit to be part of the user interface.\n6 CONCLUSIONS AND OUTLOOK\nFirst experiments with our prototype revealed that aural\nclues indeed improve the user’s exploration experience\nand can help him to navigate the music collection. To\nﬁnd out whether these improvements are signiﬁcant and\nwhich of the proposed interaction concepts is more suit-\nable in which contexts remains to be done in upcoming\nuser studies.\nThere are still a lot of other open issues we plan to\ninvestigate with our prototype:\n•How well do the applied algorithms scale for larger\nmusic collections?\n•What is the effect of reducing the playback to auto-\nmatically generated song thumbnails as proposed by\nTzanetakis [5]? What summarization algorithms are\nmost suitable?\n•How can the system be extended to home theater en-\nvironments with e.g. 5 speakers? How strong is the\nbeneﬁt of such an extension?\n•What improvements in the exploration experience\ncan be achieved if not all ”activated” songs in the\nenvironment sound simultaneously (kind of time-\ndomain multiplexing )?\n•What improvements can be achieved by emphasiz-\ning different frequencies of simultaneously playing\nsongs (kind of frequency-domain multiplexing )?\nArranging our test music collection in the Sonic SOM\nthe presented content-based similarity features sometime s\nresulted in incomprehensible clusters containing pieces\nthat could hardly be judged as similar. To our belief sig-\nniﬁcant improvements in estimating similarity of music\n1http://www.cis.hut.ﬁ/projects/somtoolboxcan only be achieved by combining different sources of\nuser-adaptive similarity estimates like e.g. presented by\nBaumann et al. [12].\nStrengthening the integration of the user into the IR\nloop can be seen as a promising way to tackle the com-\nplex music information retrieval problem. Our ongoing\nresearch on multimodal exploration environments is a step\ntowards this demand for more sophisticated user interac-\ntion concepts.\nREFERENCES\n[1] M. Torrens, P. Hertzog, and J.-L. Arcos. Visualizing\nand Exploring Personal Music Libraries. In Proc.\nISMIR , pages 421–424, Barcelona, 2004.\n[2] E. Pampalk, A. Rauber, and D. Merkl. Content-\nbased Organization and Visualization of Music\nArchives. In Proc. ACM Multimedia , Juan les Pins,\nFrance, 2002.\n[3] E. Brazil and M. Fernstr¨ om. Audio Information\nBrowsing with the Sonic Browser. In Coordi-\nnated and Multiple Views In Exploratory Visualiza-\ntion (CMV’03) , London, 2003.\n[4] E. Brazil, M. Fernstr¨ om, G. Tzanetakis, and P. Cook.\nEnhancing Sonic Browsing Using Audio Informa-\ntion Retrieval. In Proc. International Conference on\nAuditory Display , Kyoto, Japan, 2002.\n[5] G. Tzanetakis and P. Cook. Marsyas3D: A Proto-\ntype Audio Browser-Editor Using a Large Scale Im-\nmersive Visual and Audio Display. In Proc. Interna-\ntional Conference on Auditory Display , Espoo, Fin-\nland, 2001.\n[6] E. Pampalk. A Matlab Toolbox to Compute Music\nSimilarity from Audio. In Proc. ISMIR , Barcelona,\n2004.\n[7] B. Logan and A. Salomon. A Music Similarity Func-\ntion Based on Signal Analysis. In Proc. ICME , 2001.\n[8] J.-J. Aucouturier and F. Pachet. Finding Songs That\nSound the Same. In IEEE Workshop on Model\nbased Processing and Coding of Audio (MPCA-\n2002) , Leuven, Belgium, 2002.\n[9] C. Faloutsos and K.-I. Lin. Fastmap: A fast algo-\nrithm for indexing, data-mining and visualization of\ntraditional and multimedia datasets. In Proc. ACM\nSIGMOD , pages 163–174, 1995.\n[10] P. Cano, M. Kaltenbrunner, F. Gouyon, and E. Bat-\ntle. On the Use of FastMap for Audio Retrieval and\nBrowsing. In Proc. ISMIR , 2002.\n[11] T. Kohonen. Self-Organizing Maps . Springer,\nBerlin, 3rd edition, 2001.\n[12] S. Baumann, T. Pohle, and V . Shankar. Towards a\nSocio-Cultural Compatibility of MIR Systems. In\nProc. ISMIR , Barcelona, 2004.\n593"
    },
    {
        "title": "Song-Level Features and Support Vector Machines for Music Classification.",
        "author": [
            "Michael I. Mandel",
            "Dan Ellis"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415024",
        "url": "https://doi.org/10.5281/zenodo.1415024",
        "ee": "https://zenodo.org/records/1415024/files/MandelE05.pdf",
        "abstract": "Searching and organizing growing digital music collections requires automatic classification of music. This paper describes a new system, tested on the task of artist identification, that uses support vector machines to classify songs based on features calculated over their entire lengths. Since support vector machines are exemplarbased classifiers, training on and classifying entire songs instead of short-time features makes intuitive sense. On a dataset of 1200 pop songs performed by 18 artists, we show that this classifier outperforms similar classifiers that use only SVMs or song-level features. We also show that the KL divergence between single Gaussians and Mahalanobis distance between MFCC statistics vectors perform comparably when classifiers are trained and tested on separate albums, but KL divergence outperforms Mahalanobis distance when trained and tested on songs from the same albums. Keywords: Support vector machines, song classification, artist identification, kernel spaces 1",
        "zenodo_id": 1415024,
        "dblp_key": "conf/ismir/MandelE05",
        "keywords": [
            "support vector machines",
            "classifier",
            "songs",
            "features",
            "artist identification",
            "pop songs",
            "artists",
            "dataset",
            "1200",
            "18"
        ],
        "content": "SONG-LEVEL FEA TURES AND SUPPOR TVECT ORMACHINES FOR\nMUSIC CLASSIFICA TION\nMichael I.Mandel andDaniel P.W.Ellis\nLabR OSA, Dept. ofElec. Eng., Columbia University ,NYNYUSA\nfmim,dpwe g@ee.columbia.edu\nABSTRA CT\nSearching andorganizing growing digital music collec-\ntions requires automatic classi\u0002cation ofmusic. This pa-\nperdescribes anewsystem, tested onthetask ofartist\nidenti\u0002cation, thatuses support vector machines toclas-\nsifysongs based onfeatures calculated overtheir entire\nlengths. Since support vector machines areexemplar -\nbased classi\u0002ers, training onandclassifying entire songs\ninstead ofshort-time features makesintuiti vesense. On\nadataset of1200 popsongs performed by18artists, we\nshowthatthisclassi\u0002er outperforms similar classi\u0002ers that\nuseonly SVMs orsong-le velfeatures. Wealso show\nthattheKLdivergence between single Gaussians andMa-\nhalanobis distance between MFCC statistics vectors per-\nform comparably when classi\u0002ers aretrained andtested\nonseparate albums, butKLdivergence outperforms Ma-\nhalanobis distance when trained andtested onsongs from\nthesame albums.\nKeywords: Support vector machines, song classi\u0002ca-\ntion, artist identi\u0002cation, kernel spaces\n1INTR ODUCTION\nInorder toorganize andsearch growing music collections,\nwewillneed automatic tools thatcanextract useful infor -\nmation about songs directly from theaudio. Such infor -\nmation could include genre, mood, style, andperformer .\nInthispaper ,wefocus onthespeci\u0002c task ofidentifying\ntheperformer ofasong outofagroup of18.Since each\nsong hasaunique performer ,weuseasingle 18-w ayclas-\nsi\u0002er .\nWhile previous authors haveattempted such classi-\n\u0002cation tasks bybuilding models oftheclasses directly\nfrom short-time audio features, weshowthat aninter-\nmediate stage ofmodeling entire songs impro vesclas-\nsi\u0002cation. Further gains arealso seen when using Sup-\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonport Vector Machines (SVMs) astheclassi\u0002er instead of\nk-nearest neighbors (kNN) orother direct distance-based\nmeasures. These advantages become evident when com-\nparing four combinations ofclassi\u0002ers andfeatures. Not\nonly does song-le velmodeling impro veclassi\u0002cation ac-\ncurac y,italso decreases classi\u0002er training times, allow-\ningrapid classi\u0002er construction fortasks such asactive\nretrie val.\nWealso explore thespace ofsong-le velfeatures by\ncomparing three different distance measures forboth\nSVM andkNN classi\u0002cation. The\u0002rstdistance measure is\ntheMahalanobis distance between so-called MFCC statis-\nticsfeatures asused inMandel etal.(2005). Asrecom-\nmended inMoreno etal.(2004), wealso model songs\nassingle, full-co variance Gaussians andmixtures of20\ndiagonal-co variance Gaussians, measuring distances be-\ntween them with thesymmetric Kullback Leibler diver-\ngence.\nOur dataset, asubset ofuspop2002 ,contained 1210\nsongs from 18artists. When itwasbrokenupsothat\ntraining andtesting songs came from different albums,\nanSVM using theMahalanobis distance performed the\nbest, achie ving aclassi\u0002cation accurac yof69%. When\nthesongs were randomly distrib uted between cross val-\nidation sets, anSVM using theKLdivergence between\nsingle Gaussians wasable toclassify 84% ofsongs cor-\nrectly .\n1.1 Previous Work\nThepopularity ofautomatic music classi\u0002cation hasbeen\ngrowing steadily forthepast fewyears. Manyauthors\nhaveproposed systems thateither model songs asawhole\noruseSVMs tobuildmodels ofclasses ofmusic, butto\nourknowledge none hascombined thetwoideas.\nWestandCox (2004) useneither song levelfeatures\nnorSVMs. Instead, theytrain acomplicated classi\u0002er on\nmanytypes ofaudio features, butstillmodel entire classes\nwith frame-le velfeatures. Theyshowpromising results on\n6-waygenre classi\u0002cation tasks, with nearly 83% classi\u0002-\ncation accurac yfortheir bestsystem.\nAucouturier andPachet (2004) model individual songs\nwith GMMs anduseMonte Carlo methods toestimate the\nKLdivergence between them. Their system isdesigned\nasamusic-retrie valsystem, andthus itsperformance is\nmeasured interms ofretrie valprecision. Theydonotuse\n594anadvanced classi\u0002er ,astheir results arerankedbykNN.\nTheydoprovide some useful parameter settings forvar-\nious models thatweuseinourexperiments, namely 20\nMFCC coef\u0002cients and20Gaussian components inour\nGMMs.\nLoganand Salomon (2001) also model individual\nsongs asGMMs, trained using k-means instead ofEM.\nTheyapproximate theKLdivergence between GMMs as\ntheearth mover'sdistance based ontheKLdivergences of\ntheindividual Gaussians ineach mixture. Since their sys-\ntemisdescribed asadistance measure, there isnomention\nofanexplicit classi\u0002er .Theydo,however,suggest gener -\nating playlists with thenearest neighbors ofaseed song.\nTzanetakis andCook (2002) also calculate song-le vel\nfeatures. Theyclassify songs intogenre with kNN based\nonGMMs trained onsong features. Eventhough they\nonly had100feature vectors perclass, theywere stillable\ntomodel these classes with GMMs having asmall num-\nberofcomponents because oftheir parsimonious useof\nfeature dimensions.\nOftheresearchers classifying music with SVMs,\nWhitman etal.(2001) andXuetal.(2003) both train\nSVMs oncollections ofshort-time features from entire\nclasses, classify individual frames intestsongs, andthen\nlettheframes votefortheclass oftheentire song.\nMoreno etal.(2004) use SVM classi\u0002cation on\nvarious \u0002le-le velfeatures forspeak eridenti\u0002cation and\nspeak erveri\u0002cation tasks. Theyintroduce theSymmetric\nKLdivergence based kernel andalsocompare modeling a\n\u0002leasasingle, full-co variance Gaussian oramixture of\nGaussians.\n2ALGORITHM\n2.1 Song-Le velFeatur es\nAllofourfeatures arebased onmel-frequenc ycepstral\ncoef\u0002cients (MFCCs). MFCCs areashort-time spectral\ndecomposition ofanaudio signal that conveysthegen-\neralfrequenc ycharacteristics important tohuman hearing.\nWhile originally developed todecouple vocal excitation\nfrom vocal tract shape forautomatic speech recognition\n(Oppenheim, 1969), theyhavefound applications inother\nauditory domains including music retrie val(Log an,2000;\nFoote, 1997). Attherecommendation ofAucouturier and\nPachet (2004), weused 20-coef \u0002cient MFCCs.\nOurfeatures aremost accurately described astimbr al\nbecause theydonotmodel anytemporal aspects ofthe\nmusic, only itsshort-time spectral characteristics. We\nmakethestrong assumption that songs with thesame\nMFCC frames inadifferent order should beconsidered\nidentical. Some authors callthistype ofmodeling abag\nofframes, after thebag ofwords models used intext\nretrie val,which arebased ontheidea thateach wordisan\nindependent, identically distrib uted (IID) sample from a\nbagcontaining manywords indifferent amounts.\nOnce wehaveextracted theMFCCs foraparticular\nsong, wedescribe thatsong inanumber ofways,compar -\ningtheeffectiveness ofeach model. Themean andcovari-\nance oftheMFCCs overtheduration ofthesong describe\ntheGaussian with themaximum likelihood ofgenerat-\ningthose points under thebag offrames model. Thosestatistics, however,canalso beunwrapped into avector\nandcompared using theMahalanobis distance. Equiv-\nalently ,thevectors canbenormalized overallsongs to\nbezero-mean andunit-v ariance, andcompared toonean-\nother using theEuclidean distance. Going beyond the\nsimple Gaussian model, amixture ofGaussians, \u0002ttothe\nMFCCs ofasong using theEMalgorithm, isricher ,able\ntomodel nonlinear correlations.\n2.2 Support Vector Machines\nThesupport vector machine isasupervised classi\u0002cation\nsystem that\u0002nds themaximum marginhyperplane sepa-\nrating twoclasses ofdata. Ifthedata arenotlinearly sep-\narable inthefeature space, asisoften thecase, theycan\nbeprojected intoahigher dimensional space bymeans of\naMercer kernel, K(\u0001).Infact,only theinner products of\nthedata points inthishigher dimensional space arenec-\nessary ,sotheprojection canbeimplicit ifsuch aninner\nproduct canbecomputed directly .\nThe space ofpossible classi\u0002er functions consists of\nweighted linear combinations ofkeytraining instances in\nthiskernel space (Cristianini andShawe-T aylor, 2000).\nTheSVM training algorithm chooses these instances (the\nsupport vectors) andweights tooptimize themarginbe-\ntween classi\u0002er boundary andtraining examples. Since\ntraining examples aredirectly emplo yedinclassi\u0002cation,\nusing entire songs asthese examples aligns nicely with the\nproblem ofsong classi\u0002cation.\n2.3 Distance Measur ements\nInthispaper ,wecompare three different distance mea-\nsurements, allofwhich areclassi\u0002ed using aradial basis\nfunction kernel. TheMFCC statistics aretheunwrapped\nmean andcovariance oftheMFCCs ofanentire song. The\ndistance between twosuch vectors ismeasured using the\nMahalanobis distance,\nDM(u;v)=(u\u0000v)T\u0006\u00001(u\u0000v); (1)\nwhere \u0006isthecovariance matrix ofthefeatures across all\nsongs, approximated asadiagonal matrix oftheindividual\nfeature' svariances.\nThesame means andcovariances, when reinterpreted\nasasingle Gaussian model, canbecompared toonean-\nother using theKullback Leibler divergence (KL diver-\ngence). Fortwodistrib utions, p(x)andq(x),theKLdi-\nvergences isde\u0002ned as,\nKL(pjjq)\u0011Z\np(x)logp(x)\nq(x)dx (2)\n=Ep\u001a\nlogp(X)\nq(X)\u001b\n: (3)\nForsingle Gaussians, p(x)=N(x;\u0016p;\u0006p)andq(x)=\nN(x;\u0016q;\u0006q),there isaclosed form fortheKLdiver-\ngence (Penn y,2001),\n2KL(pjjq)=2KLN(\u0016p;\u0006p;\u0016q;\u0006q) (4)\n=logj\u0006qj\nj\u0006pj+Tr(\u0006\u00001\nq\u0006p)\n+(\u0016p\u0000\u0016q)T\u0006\u00001\nq(\u0016p\u0000\u0016q)\u0000d:(5)\n595Unfortunately ,there isnoclosed form solution forthe\nKLdivergence between twoGMMs, itmust beapproxi-\nmated using Monte Carlo methods. Anexpectation ofa\nfunction overadistrib ution, p(x),canbeapproximated\nbydrawing samples fromp(x)andaveraging thevalues\nofthefunction atthose points. Inthiscase, bydrawing\nsamples X1;:::;Xn\u0018p(x),wecanapproximate\nEp\u001a\nlogp(X)\nq(X)\u001b\n\u00191\nnnX\ni=1logp(Xi)\nq(Xi): (6)\nWeused theKernel Density Estimation toolbox from Ihler\n(2005) forthese calculations.\nAlso, note therelationship between theaboveMonte\nCarlo estimate oftheKLdivergence andmaximum like-\nlihood classi\u0002cation. Instead ofdrawing samples from a\ndistrib ution modeling acollection ofMFCC frames, the\nmaximum likelihood classi\u0002er uses theMFCC frames di-\nrectly asevaluation points. IfM1;:::;MnareMFCC\nframes from asong, drawnfrom some distrib utionp(m),\ntheKLdivergence between thesong andanartist model\nq(m)canbeapproximated as\nEp\u001a\nlogp(M)\nq(M)\u001b\n=Epflogp(M)g\u0000Epflogq(M)g\n(7)\n\u0019Hp\u00001\nnnX\ni=1q(Mi); (8)\nwhere Hp,theentrop yofp(m),andnareconstant fora\ngivensong andthus donotaffecttheoptimization. For\nagivensong, then, choosing theartist model with the\nsmallest KLdivergence isequivalent tochoosing theartist\nmodel under which thesong' sframes havethemaximum\nlikelihood.\nSince theKLdivergence isneither symmetric norpos-\nitivede\u0002nite, wemust modify ittosatisfy theMercer con-\nditions inorder touseitasanSVM kernel. Tosymmetrize\nit,weaddthetwodivergences together ,\nDKL(p;q)=KL(pjjq)+KL(qjjp): (9)\nExponentiating theelements ofthismatrix will create a\npositi vede\u0002nite matrix, soour\u0002nal gram matrix hasele-\nments\nK(Xi;Xj)=e\u0000\rDKL(Xi;Xj); (10)\nwhere \risaparameter thatcanbetuned tomaximize clas-\nsi\u0002cation accurac y.Calculating these inner products isrel-\nativelycostly andhappens repeatedly ,soweprecompute\nDKL(Xi;Xj)offlineandonly perform lookups online.\n3EVALU ATION\n3.1 Dataset\nWeranourexperiments onasubset oftheuspop2002 col-\nlection (Berenzweig etal.,2003; Ellis etal.,2005). To\navoidthesocalled producer effect oralbumeffect\n(Whitman etal.,2001) inwhich songs from thesame al-\nbumshare overall spectral characteristics much more thanTable 1:Artists from uspop2002 included indataset\nAerosmith Beatles Bryan Adams\nCreedence Clear -\nwaterRevivalDaveMatthe ws\nBandDepeche Mode\nFleetw oodMac Garth Brooks Genesis\nGreen Day Madonna Metallica\nPink Floyd Queen Rolling Stones\nRoxette TinaTurner U2\nKLKLMFCCsArtist 1 Artist 2\nTest SongMin ArtistTraining\nGMMs\nFigure 1:Classi\u0002cation ofartist levelfeatures without us-\ninganSVM. Theshaded region indicates calculations per-\nformed during training.\nsongs from thesame artist' sother albums, wedesignated\nentire albums astraining, testing, orvalidation. Thetrain-\ningsetwasused forbuilding classi\u0002ers, thevalidation set\nwasused totune model parameters, and\u0002nal results were\nreported forsongs inthetestset.\nInorder tohaveameaningful artist identi\u0002cation task,\nweselected artists who hadenough albums inuspop2002\ntopartition inthisway,namely three albums fortraining\nandtwofortesting. Thevalidation setwasmade upofany\nalbums theselected artists hadinuspop2002 inaddition\ntothose \u0002ve.18artists (out of400) met these criteria,\nseeTable 1foracomplete listoftheartists included in\nourexperiments. Intotal, weused 90albums bythese 18\nartists which contained atotal of1210 songs divided into\n656training, 451testing, and103validation songs.\nInaddition tothis\u0002xedgrouping ofalbums, wealso\nevaluated ourclassi\u0002ers with three-fold cross-v alidation.\nEach song wasrandomly assigned tooneofthree groups\nand theclassi\u0002er wastrained ontwogroups and then\ntested onthethird. Allthree sets were tested inthis\nwayand the\u0002nal classi\u0002cation accurac yused thecu-\nmulati vestatistics overallrounds. Werepeated these\ncross-v alidation experiments for\u0002vedifferent divisions of\nthedata andaveraged theaccurac yacross allrepetitions.\nThis cross-v alidation setup divides songs, notalbums, into\ngroups, sothealbumeffect isreadily apparent initsre-\nsults.\n3.2 Experiments\nInourexperiments, wecompared allfour combinations\nofsong-le velversus artist-le velfeatures, andSVM ver-\nsusnon-SVM classi\u0002ers. Wealso investig ated theeffect\nofdifferent distance measures onSVM andkNN classi-\n\u0002cation. SeeFigures 1and2foragraphical depiction of\nthefeature extraction andclassi\u0002cation processes forartist\n596DDDDDDMFCCsArtist 1 Artist 2Song Features\nDAG SVM\nTest SongArtistTraining\nFigure 2:Classi\u0002cation ofsong levelfeatures with a\nDAG-SVM. Theshaded region indicates calculations per-\nformed during training. Note thatthesong-le velfeatures\ncould beGMMs andthedistance function could bethe\nKLdivergence, butitisnotrequired.\nandsong levelfeatures, respecti vely.\nThe \u0002rst experiment used neither song-le velfeatures\nnorSVMs, training asingle GMM ontheMFCC frames\nfrom allofanartist' ssongs atonce. Thelikelihood ofeach\nsong' sframes wasevaluated under each artist model and\nasong waspredicted tocome from themodel with the\nmaximum likelihood ofgenerating itsframes. Weused\n50Gaussians ineach artist GMM, trained on10% ofthe\nframes from alloftheartist' straining songs, forapproxi-\nmately 12000 frames perartist.\nThesecond experiment used SVMs, butnotsong-le vel\nfeatures. Bytraining an18-w ayDAG-SVM (Platt etal.,\n2000) onasubset oftheframes used inthe\u0002rst exper-\niment, weattempted tolearn toclassify MFCC frames\nbyartist. Toclassify asong, we\u0002rst classi\u0002ed allofits\nframes andthen predicted thesong' sclass tobethemost\nfrequently predicted frame class. Unfortunately ,wewere\nonly able totrain on500 frames perartist, notenough\ntoachie veaclassi\u0002cation accurac ysigni\u0002cantly above\nchance levels.\nExperiments with song levelfeatures compared theef-\nfectiveness ofthree different distance measures andsong\nmodels. TheMahalanobis distance andKLdivergence be-\ntween single Gaussians shared anunderlying representa-\ntion forsongs, themean andcovariance oftheir MFCC\nframes. These twomodels were \u0002xedbythesongs them-\nselves,except fortheSVM' s\rparameter .The KLdi-\nvergence between GMMs, however,hadanumber ofad-\nditional parameters thatneeded tobetuned. Inorder to\nmakethecalculations tractable, wetrained ourGMMs on\n3000 MFCC frames from each song, roughly 10-20% of\nthetotal. Wedecided on20Gaussian components based\nonestimates ofthenumber ofsamples needed perGaus-\nsian giventheprevious constraint andtheadvice ofAu-\ncouturier andPachet (2004). Wealsoselected thenumber\nofMonte Carlo samples used toapproximate theKLdi-\nvergence. Inthiscase 500seemed tobehigh enough to\ngivefairly consistent results, while stillbeing fastenough\ntocalculate for1.4million pairs ofsongs.\nThe third experiment used song-le velfeatures, buta\nsimple k-nearest neighbors classi\u0002er .Forallthree song-\nlevelfeatures andcorresponding distance measures, we\nused akNN classi\u0002er tolabel testsongs with thelabel\nmost prevalent among thektraining songs thesmallest\ndistance away.Forthese experiments kwasvaried from 1to10,withk=1performing either thebest orcompeti-\ntively.\nThe\u0002nal experiment used song-le velfeatures andan\nSVM classi\u0002er .Again, forallthree song-le velfeatures\nandGram matrices ofdistances, welearned an18-w ay\nDAG-SVM classi\u0002er forartists. Wetuned the\rparam-\neteroftheSVMs tomaximize classi\u0002cation accurac y.In\ncontrast tothe\u0002rsttwoexperiments, which were only per-\nformed forthe\u0002xedtraining andtesting setsseparated by\nalbum, thethird andfourth experiments were also per-\nformed oncross-v alidation datasets.\n3.3 Results\nSeeTable 2forthebest performance ofeach ofourclas-\nsi\u0002ers andFigure 3foragraph oftheresults forseparate\ntraining andtesting albums. These results clearly show\ntheadvantage ofusing both song-le velfeatures andSVM\nclassi\u0002ers, a15percentage point gainin18-w ayclassi\u0002-\ncation accurac y.\nItshould also benoted that training times forthe\ntwoclassi\u0002ers using low-levelfeatures were considerably\nhigher than forthose using song-le velfeatures. While\nsong-le velfeatures involveaninitial investment inex-\ntracting features andmeasuring distances between pairs of\nsongs, theclassi\u0002ers themselv escanbetrained quickly on\nanyparticular subset ofsongs. Fasttraining makesthese\nmethods useful forrelevance feedback andactivelearning\ntasks, such asthose described inMandel etal.(2005).\nIncontrast, artist levelclassi\u0002ers spend little time ex-\ntracting features from songs, butmust train directly ona\nlargequantity ofdata upfront, making retraining justas\ncostly astheinitial computational expense. Inaddition,\nclassifying each song isalso relati velyslow,asframes\nmust beclassi\u0002ed individually andtheresults aggre gated\ninto the\u0002nal classi\u0002cation. Forboth ofthese reasons, it\nwasdif\u0002cult toobtain cross-v alidation data fortheartist-\nlevelfeature classi\u0002ers.\nSee Table 3fortheperformance ofthethree dis-\ntance measures used forsong-le velfeatures. The Maha-\nlanobis distance andKLdivergence forsingle Gaussians\nperformed comparably ,since forthe451testpoints, adif-\nference of0.039 isnotstatistically signi\u0002cant. Surpris-\ningly ,however,theKLdivergence between single Gaus-\nsians greatly surpassed theMahalanobis distance when\ntrained andtested onsongs from thesame albums.\nAlloftheSVM results inTable 3were collected for\noptimal values of\r,which differed between distance mea-\nsures, butnotbetween groups ofsongs. Since training\nSVMs andchanging \rtook solittle time after calculating\ntheGram matrix, itwaseasy to\u0002ndthebestperforming \r\nbysearching theone-dimensional parameter space.\n4DISCUSSION\nModeling songs instead ofdirectly modeling artists makes\nintuiti vesense. Models likeGMMs assume stationarity\noruniformity ofthefeatures theyaretrained on. This\nassumption ismuch more likelytohold overindividual\nsongs than overanartist' sentire catalog. Individual songs\nmight evenbetoovaried, asinthecase ofextended-form\n597Table 2:Classi\u0002cation accurac yon18-w ayartist identi\u0002-\ncation reported fortraining andtesting onseparate albums\n(Sep) andtraining andtesting ondifferent songs from the\nsame albums (Same). Forseparate albums (N=451)sta-\ntistical signi\u0002cance isachie vedforadifference ofaround\n.06.Forsongs from thesame album(N=2255 )statisti-\ncalsigni\u0002cance isachie vedforadifference ofaround .02.\nClassi\u0002er Song-Le vel? SVM? Sep Same\nArtist GMM No No .541 \nArtist SVM No Yes .067 \nSong KNN Yes No .524 .722\nSong SVM Yes Yes .687 .839\nTable 3:Classi\u0002cation accurac yfordifferent song-le vel\ndistance measures.\nClassi\u0002er Distance Sep Same\nKNN Mahalanobis .481 .594\nKNN KL-Di v1G .524 .722\nKNN KL-Di v20G .365 .515\nSVM Mahalanobis .687 .792\nSVM KL-Di v1G .648 .839\nSVM KL-Di v20G .431 .365\ncompositions inwhich theoverall timbre changes dramat-\nically between sections. Such songs callforsummaries\novershorter interv als,perhaps atthelevelofseconds in-\nstead ofminutes, sothatthere isenough data tosupport a\nrichmodel, butnotsomuch data thatthemodel averages\noutinteresting detail.\nTable 3alsoclearly showsthealbumeffect inwhich\nalmost everyclassi\u0002er performs signi\u0002cantly better when\ntrained andtested onsongs from thesame albums. De-\npending onthesituation, oneevaluation might bemore\nuseful than theother .Forexample, ifaperson hears a\nsong ontheradio thatheorshelikes,itwould makesense\ntolook forsimilar songs thatcould come from thesame\nalbum.Ontheother hand, ifashopper islooking fornew\nalbums tobuybased onhisorhercurrent collection, a\nrecommendation system would wanttoavoidthetraining\nalbums.\nOne reason theKLdivergence onGMMs performed\nsobadly might bethenumber ofsamples weused inour\nMonte Carlo estimates ofKLdivergence. 500 samples\nisjustbarely enough togetareasonable estimate ofthe\nKLdivergence, butapparently thisestimate istoonoisy\ntohelp SVM orkNN classi\u0002cation. Averygood approxi-\nmation would probably havetakenthousands ofsamples,\nantherefore tentimes aslong tocompute the1.4million\nelement Gram matrix, already pushing thelimits ofour\ncomputational power.\nWehaveshownthataudio-based music classi\u0002cation\nisaided bycomputing features atthesong levelandby\nclassifying thefeatures with support vector machines in-\nstead ofsimple k-nearest neighbors classi\u0002ers.Art GMM Art SVM Maha 1G 20G00.10.20.30.40.50.60.70.80.91\nSVM\nnon−SVM\nFigure 3:Classi\u0002cation accurac yonseparate training\nandtesting albums. From lefttoright, thecolumns are:\nGMMs trained onartist-le velfeatures, SVMs trained on\nartist-le velfeatures, andthen kNN andSVMs using the\nMahalanobis distance, theKLdivergence between single\nGaussians, andtheKLdivergence between mixtures of20\nGaussians.\n4.1 Futur eWork\nAsasimple extension tothiswork, wecould useafea-\nturemid-w aybetween thesong andframe levels.Bydi-\nviding asong into dozens ofpieces, extracting thefea-\ntures ofthose pieces andclassifying them individually ,\nwewould getmanyoftheadvantages ofboth approaches.\nThere would bearelati velysmall number offeature vec-\ntors persong, making training andtesting fast,andthe\nsmaller pieces would bemore likelytobetimbrally uni-\nform. This division could also allowaclassi\u0002er tocon-\nsider asong' stemporal structure, emplo ying, forexample,\nahidden Mark ovmodel. Other authors haveused hidden\nMark ovmodels formusic classi\u0002cation anddescription,\nbuttheinput tothose models hasbeen individual MFCCs\norspectral slices, notlargerstructures.\nACKNO WLEDGEMENTS\nThis workwassupported bytheFuFoundation School of\nEngineering andApplied Science viaaPresidential Fel-\nlowship, theColumbia Academic Quality Fund, andthe\nNational Science Foundation (NSF) under Grant No.IIS-\n0238301. Anyopinions, \u0002ndings andconclusions orrec-\nommendations expressed inthismaterial arethose ofthe\nauthors anddonotnecessarily re\u0003ect theviewsofthe\nNSF.\nREFERENCES\nJean-Julien Aucouturier andFrancois Pachet. Impro ving\ntimbre similarity :Howhigh' sthesky? Journal of\nNegative Results inSpeec handAudio Sciences ,1(1),\n2004.\nAdam Berenzweig, Beth Logan,Dan Ellis, and Brian\nWhitman. Alarge-scale evaluation ofacoustic and\n598subjecti vemusic similarity measures. InInternational\nSymposium onMusic Information Retrie val,October\n2003.\nNello Cristianini andJohn Shawe-T aylor .Anintroduction\ntosupport Vector Machines: and other kernel-based\nlearning methods .Cambridge University Press, New\nYork,NY,USA, 2000. ISBN 0-521-78019-5.\nDan Ellis, Adam Berenzweig, and Brian Whitman.\nThe uspop2002 pop music data set, 2005.\nhttp://labrosa.ee.columbia.edu/projects/musicsim/\nuspop2002.html.\nJonathan T.Foote. Content-based retrie valofmusic and\naudio. InC.-C. J.Kuo,Shih-Fu Chang, andVenkat N.\nGudi vada, editors, Proc.SPIE Vol.3229, p.138-147,\nMultimedia Storageand Archiving Systems II,pages\n138147, October 1997.\nAlexIhler.Kernel density estimation toolbox formatlab,\n2005. http://ssg.mit.edu/ ihler/code/.\nBeth Logan.Mel frequenc ycepstral coef\u0002cients formu-\nsicmodelling. InInternational Symposium onMusic\nInformation Retrie val,2000.\nBeth LoganandAriel Salomon. Amusic similarity func-\ntion based onsignal analysis. InICME 2001 ,Tokyo,\nJapan, 2001.\nMichael I.Mandel, Graham E.Poliner ,andDaniel P.W.\nEllis. Support vector machine activelearning formu-\nsicretrie val.ACMMultimedia Systems Journal ,2005.\nSubmitted forreview.\nPedro J.Moreno, Purdy P.Ho, andNuno Vasconcelos.\nAkullback-leibler divergence based kernel forSVM\nclassi\u0002cation inmultimedia applications. InSebastian\nThrun, Lawrence Saul, andBernhard Sch¨olkopf, edi-\ntors, Advances inNeur alInformation Processing Sys-\ntems 16.MIT Press, Cambridge, MA, 2004.\nAlan V.Oppenheim. Aspeech analysis-synthesis system\nbased onhomomorphic \u0002ltering. Journal oftheAcosti-\ncalSociety ofAmerica ,45:458465, February 1969.\nWilliam D.Penn y.Kullback-liebler divergences ofnor-\nmal, gamma, dirichlet andwishart densities. Technical\nreport, Wellcome Department ofCogniti veNeurology ,\n2001.\nJohn C.Platt, Nello Cristianini, andJohn Shawe-T aylor .\nLargemargindags formulticlass classi\u0002cation. InS.A.\nSolla, T.K.Leen, andK.-R. Mueller ,editors, Advances\ninNeur alInformation Processing Systems 12,pages\n547553, 2000.\nGeor geTzanetakis andPerry Cook. Musical genre classi-\n\u0002cation ofaudio signals. IEEE Transactions onSpeec h\nandAudio Processing ,10(5):293302, July 2002.\nKristopher WestandStephen Cox. Features andclassi-\n\u0002ers fortheautomatic classi\u0002cation ofmusical audio\nsignals. InInternational Symposium onMusic Infor -\nmation Retrie val,2004.\nBrian Whitman, Gary Flake,andSteveLawrence. Artist\ndetection inmusic with minno wmatch. InIEEE Work-\nshop onNeur alNetworks forSignal Processing ,pages559568, Falmouth, Massachusetts, September 1012\n2001.\nChangsheng Xu, Namunu CMaddage, XiShao, Fang\nCao, andQiTian. Musical genre classi\u0002cation using\nsupport vector machines. InInternational Confer ence\nonAcoustics, Speec h,and Signal Processing .IEEE,\n2003.\n599"
    },
    {
        "title": "jAudio: An Feature Extraction Library.",
        "author": [
            "Daniel McEnnis",
            "Cory McKay",
            "Ichiro Fujinaga",
            "Philippe Depalle"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416648",
        "url": "https://doi.org/10.5281/zenodo.1416648",
        "ee": "https://zenodo.org/records/1416648/files/McEnnisMFD05.pdf",
        "abstract": "jAudio is a new framework for feature extraction designed to eliminate the duplication of effort in calculating features from an audio signal. This system meets the needs of MIR researchers by providing a library of analysis algorithms that are suitable for a wide array of MIR tasks. In order to provide these features with a minimal learning curve, the system implements a GUI that makes the process of selecting desired features straight forward. A command-line interface is also provided to manipulate jAudio via scripting. Furthermore, jAudio provides a unique method of handling multidimensional features and a new mechanism for dependency handling to prevent duplicate calculations. The system takes a sequence of audio files as input. In the GUI, users select the features that they wish to have extracted—letting jAudio take care of all dependency problems—and either execute directly from the GUI or save the settings for batch processing. The output is either an ACE XML file or an ARFF file depending on the user’s preference. Keywords: Java Audio Environment, Audio Feature Extraction, Music Information Retrieval. 1",
        "zenodo_id": 1416648,
        "dblp_key": "conf/ismir/McEnnisMFD05",
        "keywords": [
            "jAudio",
            "new framework",
            "feature extraction",
            "eliminate duplication",
            "MIR researchers",
            "library of analysis algorithms",
            "GUI",
            "command-line interface",
            "multidimensional features",
            "dependency handling"
        ],
        "content": "JAUDIO: A FEATURE EXTRACTION LIBRARY\nDaniel McEnnis\nFaculty of Music\nMcGill University\nMontreal Canada\ndaniel.mcennis\n@mail.mcgill.caCory McKay\nFaculty of Music\nMcGill University\nMontreal Canada\ncory.mckay\n@mail.mcgill.caIchiro Fujinaga\nFaculty of Music\nMcGill University\nMontreal Canada\nich\n@music.mcgill.caPhilippe Depalle\nFaculty of Music\nMcGill University\nMontreal Canada\ndepalle\n@music.mcgill.ca\nABSTRACT\njAudio is a new framework for feature extraction designed\nto eliminate the duplication of effort in calculating fea-\ntures from an audio signal. This system meets the needs\nof MIR researchers by providing a library of analysis al-\ngorithms that are suitable for a wide array of MIR tasks.\nIn order to provide these features with a minimal learn-\ning curve, the system implements a GUI that makes the\nprocess of selecting desired features straight forward. A\ncommand-line interface is also provided to manipulate\njAudio via scripting. Furthermore, jAudio provides a\nunique method of handling multidimensional features and\na new mechanism for dependency handling to prevent du-\nplicate calculations.\nThe system takes a sequence of audio ﬁles as input.\nIn the GUI, users select the features that they wish to\nhave extracted—letting jAudio take care of all depen-\ndency problems—and either execute directly from the\nGUI or save the settings for batch processing. The out-\nput is either an ACE XML ﬁle or an ARFF ﬁle depending\non the user’s preference.\nKeywords: Java Audio Environment, Audio Feature\nExtraction, Music Information Retrieval.\n1 INTRODUCTION\njAudio is a feature extraction system designed to meet the\nneeds of MIR researchers by providing a collection of fea-\nture extraction algorithms bundled with both an easy-to-\nuse GUI and a command-line interface. The system ac-\ncepts audio ﬁles as input and produces either ACE XML\nﬁles (McKay et al. 2005) or ARFF ﬁles. Furthermore, the\nsystem includes multidimensional features and a new way\nto handle dependencies between features.\nExtracting high-quality features is of critical impor-\ntance for many MIR projects (Fujinaga 1998; Jensen\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of London1999). Since these features are the only information that\na classiﬁer or other interpretive construct has about the\noriginal data, a failure to capture information and patterns\ninherent in the signal will result in poor performance, no\nmatter how good the interpretive layer is.\nOne such problem is the difﬁculty in extracting per-\nceptual features such as meter or pitch from a signal.\nThese features, though useful, are typically not used be-\ncause they are generally too complicated to create. This\nis particularly true if the main focus of the research is in\nother areas, not the creation of new pitch or meter detec-\ntion algorithms.\nSince features are the sole mechanism by which in-\nterpretive layers can gain access to the latent information\nof the original data source, having many different fea-\ntures is desirable. Especially if feature selection or feature\nweighting is used, having a multitude of features permits\nthe interpretive layer to have as many perspectives on the\nincoming data as possible. However, especially if the in-\nterpretive layer is the main focus of the researchers’ ef-\nforts, creating and maintaining a large array of features is\na signiﬁcant effort that may not be feasible.\nThe current state of feature extraction techniques has\nsome additional difﬁculties. There is no central repository\nof algorithms dedicated to extracting features. This means\nthat researchers are dependent on often sparse descriptions\nfrom conference proceedings to identify the best features\nto obtain for a given topic. Unfortunately, due to space\nconstraints, these descriptions tend to be terse to the point\nof obscurity, greatly increasing the chance that an algo-\nrithm may be implemented incorrectly due to a misunder-\nstanding of a source. jAudio alleviates this problem by\nproviding a central repository for placing features.\nThere is also a problem in how the feature extrac-\ntors communicate with their interpreters. There has been\nsome progress made in this ﬁeld as Weka’s ARFF format\nhas become a de facto standard (Witten and Frank 1999).\nYet, with the exception of Marsyas (Tzanetakis and Cook\n2000), no existing feature extraction system provides their\noutput data in a standard output format.\nMore critically, feature extraction source code is either\nnot made available or is tightly coupled to the classiﬁca-\ntion or analysis code. This prevents the reuse of this code\nin other contexts, limiting the ability of researchers to ex-\nchange feature extraction algorithms.\nFurthermore, many algorithms for feature extraction\n600are implemented in a platform dependent way that does\nnot necessarily function properly on computers of another\narchitecture or sometimes on systems of the same archi-\ntecture but different conﬁguration.\nAnother concern that needs to be addressed is how\neasily the feature extraction platform can be extended. A\ncomplicated setup means that few features will be added\nby anyone but the maintainers, drastically limiting the use-\nfulness of the project.\n2 RELATED RESEARCH\nEfforts to extract a large number of features in a single ex-\nperiment have been done before. Papers such as Fujinaga\n(1998), Hong (2000), and Jensen (1999) are known for\ntheir large feature collection. However, the creation of li-\nbraries to avoid duplication of the effort of writing feature\nextraction algorithms is a relatively new phenomenon.\n2.1 Marsyas\nMarsyas by George Tzanetakis is a pioneer in this area.\nHis system is implemented in C++. The system is both\nefﬁcient and open source. Despite being integrated into a\ngeneral classiﬁcation system, Marsyas retains the ability\nto output feature extraction data to Weka’s ARFF format.\nOne drawback is the complicated interface for controlling\nthe features selected for extraction in the extraction sub-\nsystem (Tzanetakis and Cook 2000).\n2.2 CLAM\nCLAM is produced by the Music Technology Group at\nPompeu Fabra University (Amatrain et al. 2002). The sys-\ntem is an analysis/synthesis system and is implemented in\nC++. While a good general system with a good GUI user\ninterface, the system was not intended for extracting fea-\ntures for classiﬁcation problems.\n2.3 M2K\nM2K is built upon the D2K data mining software devel-\noped at NCSA at the University of Illinois (Downie et al.\n2004). The system utilizes the GUI architecture of D2K\npatches—a very intuitive method for building large hier-\narchies of feature sets. Unfortunately, the system is cur-\nrently in an alpha state. Further complicating this difﬁ-\nculty is the commercial license of the underlying D2K sys-\ntem. While M2K is available under a free license, the D2K\nsystem is only available for academic use. This makes the\nsystem more difﬁcult to obtain by researchers outside the\nUnited States and raises the possibility of licensing prob-\nlems for those researchers whose work with M2K blurs\nthe edge between a research tool and a commercial open-\nsource application.\n2.4 Maaate\nMaaate is produced by the Commonwealth Scientiﬁc and\nIndustrial Research Organization. It was built primarily to\nextract features from MPEG 1 audio rather than uncom-\npressed audio. While it has a GUI front end, the GUIis geared towards visualization rather than controlling the\nfeature extraction process (Pfeiffer et al. 2005).\n3 DESIGN DECISIONS\nIn order to address the issues introduced in Section 1, it\nwas necessary to make a number of design decisions that\nshaped jAudio.\n3.1 Java based\njAudio was written in Java in order to capitalize on Java’s\ncross-platform portability and design advantages. A cus-\ntom low-level audio layer was implemented in order to\nsupplement Java’s limited core audio support and allow\nthose writing jAudio features to deal directly with arrays\nof sample values rather than needing to concern them-\nselves directly with low-level issues such as buffering and\nformat conversions.\n3.2 XML and Weka output\njAudio supports multiple output formats, including both\nthe native XML format of the ACE (Autonomous Classi-\nﬁer Engine) system, which is a framework for optimizing\nclassiﬁers (McKay et al. 2005), and the ARFF format used\nby the popular Weka analysis toolkit (Witten and Frank\n1999). This permits utilizing both the ACE environment\nfor classiﬁcation problems and providing a more estab-\nlished format. Export to ARFF is accomplished by treat-\ning all multidimensional features as collections of individ-\nual features.\n3.3 Handling dependencies\nIn order to reduce the complexity of calculations, it is of-\nten advantageous to reuse the results of an earlier calcu-\nlation in other modules. jAudio provides a simple way\nfor a feature class to declare which features it requires in\norder to be calculated. An example is the magnitude spec-\ntrum of a signal. It is used by a number of features, but\nonly needs to be calculated once. Just before execution\nbegins, jAudio reorders the execution of feature calcula-\ntions such that every feature’s calculation is executed only\nafter all of its dependencies have been executed. Further-\nmore, unlike any other system, the user need not know\nthe dependencies of the features selected. Any feature se-\nlected for output that has dependencies will automatically\nand silently calculate dependent features as needed with-\nout replication.\n3.4 Support for multidimensional features\njAudio has the capacity to accept features that provide an\narbitrary number of dimensions. This is an extremely use-\nful way to group related features calculated at once such as\nMFCC. This is in contrast to the ARFF format from Weka\nwhere all features are unidimensional. Furthermore, the\ndimensionality of each feature is exported. This permits\nderivatives and other metafeatures to have the same num-\nber of dimensions as the feature they are calculated from,\neven though the same code is used for all features.\n601Figure 1: Screenshot of jAudio GUI.\n3.5 Intuitive interface\njAudio permits control of downsampling of the input sig-\nnal, signal normalization, window size, window overlap,\nand control of which features are extracted and saved with\nan easy to use GUI (See Figure 1). The GUI also per-\nmits users to conﬁgure settings and save them for batch\nprocessing.\n3.6 License\nAll source code is publicly available on the Internet\n(http://coltrane.music.mcgill.ca/ACE) under the Lesser\nGNU Public License (LGPL).\n3.7 Extensibility\nEffort was taken to make it as easy as possible to add\nnew features and associated documentation to the system.\nAn abstract class is provided that includes all the features\nneeded to implement a feature.\n3.8 Metafeatures\nMetafeatures are templates that can be applied against any\nfeature to create new features. Examples of metafeatures\ninclude Derivative, Mean, and Standard Deviation. Each\nof these metafeatures are automatically applied to all fea-\ntures without the user needing to explicitly create these\nderivative features.\n4 IMPLEMENTED FEATURES\nThere are 27 distinct features implemented in jAudio. The\nfollowing is a non-exhaustive list.•Zero Crossing\nZero Crossing is calculated by counting the num-\nber of times that the time domain signal crosses zero\nwithin a given window. ’Crossing zero’ is deﬁned as\n(xn−1<0andxn>0) or (xn−1>0andxn<0)\nor (xn−1/negationslash= 0andxn= 0) .\n•RMS\nRMS is calculated on a per window basis. It is de-\nﬁned by the equation:\nRMS =/radicalBigg/summationtextN\nnx2n\nN(1)\nwhere N is the total number of samples provided in\nthe time domain. RMS is used to calculate the am-\nplitude of a window.\n•Fraction of Low Amplitude Frames\nThis feature is deﬁned as the fraction of previous\nwindows whose RMS is less than the mean RMS.\nThis gives an indication of the variability of the am-\nplitude of windows.\n•Spectral Flux\nSpectral Flux is deﬁned as the spectral correlation be-\ntween adjacent windows (McAdams 1999). It is of-\nten used as an indication of the degree of change of\nthe spectrum between windows.\n•Spectral Rolloff\nSpectral rolloff is deﬁned as the frequency where\n85% of the energy in the spectrum is below this point.\n602It is often used as an indicator of the skew of the fre-\nquencies present in a window.\n•Compactness\nCompactness is closely related to Spectral Smooth-\nness as deﬁned by McAdams (1999). The difference\nis that instead of summing over partials, compactness\nsums over frequency bins of an FFT. This provides an\nindication of the noisiness of the signal.\n•Method of Moments\nThis feature consists of the ﬁrst ﬁve statistical mo-\nments of the spectrograph. This includes the area (ze-\nroth order), mean (ﬁrst order), Power Spectrum Den-\nsity (second order), Spectral Skew (third order), and\nSpectral Kurtosis (fourth order). These features de-\nscribe the shape of the spectrograph of a given win-\ndow (Fujinaga 1997).\n•2D Method of Moments\nThis feature treats a series of frames of spectral data\nas a two dimensional image which are then analyzed\nusing two-dimensional method of moments (Fuji-\nnaga 1997). This gives a description of the spectro-\ngraph, including its changes, over a relatively short\ntime frame.\n•MFCC\nMel-Frequency Cepstral Coefﬁcients (MFCCs) are\ncalculated according to the formula by Bogert et al.\n(1963). The calculations are implemented here using\nthe code taken from the Orange Cow voice recogni-\ntion project (Su et al. 2005). This is useful for de-\nscribing a spectrum window.\n•Beat Histogram\nThis feature autocorrelates the RMS for each bin in\norder to construct a histogram representing rhyth-\nmic regularities. This is used as a base feature for\ndetermining best tempo match (Scheirer and Slaney\n1997).\n5 CONCLUSIONS\njAudio provides a comprehensive solution to the problem\nof the duplication of work in programming feature extrac-\ntion. This system permits general use of a large number\nof features in a fashion that is both easy to use and ex-\ntensible. The GUI permits the system to be easily conﬁg-\nured with minimal effort and the command-line interface\npermits easy batch processing. The system also provides\na central repository for the storing of feature algorithms\nwith an unambiguous meaning with output that can be\nread by either ACE or Weka.\n6 FUTURE WORK\nThe set of features provided by jAudio is by no means\ncomprehensive. Numerous additional features remain to\nbe added. In particular, the system needs an implemen-\ntation of LPC and the ability to process multiple window\nsizes concurrently.Also, if the license issues can be resolved, we would\nlike to merge our development efforts into the M2K\nproject. This would allow the project to take advantage\nof the extensive GUI support while maintaining the exist-\ning beneﬁts of jAudio.\nREFERENCES\nX. Amatrain, P. Arumi, and M. Ramirez. Clam: Yet an-\nother library for audio and music processing? In Pro-\nceedings of the ACM Conference on Object Oriented\nProgramming, Systems, and Applications , 2002. 22–3.\nB. Bogert, M. Healy, and M. Healy. The quefrency\nalanysis of time series for echoes: cepstrum, pseudo-\nautocovariance, cross-cepstrum, and saphe-cracking. In\nProceedings of the Symposium Time Series Analysis ,\n1963. 209–43.\nS. Downie, J. Futrelle, and D. Tcheng. The international\nmusic information retrieval systems evaluation labora-\ntory. International Conference on Music Information\nRetrieval , 2004. 9–14.\nI. Fujinaga. Machine recognition of timbre using steady-\nstate tone of acoustic musical instruments. In Proceed-\nings of the International Computer Music Conference ,\n1998. 207–10.\nI. Fujinaga. Adaptive optical music recognition . PhD the-\nsis, McGill University, 1997.\nT. Hong. Salient feature extraction of musical instrument\nsignals. Master’s thesis, Dartmouth College, 2000.\nK. Jensen. Timbre models of musical sounds . PhD thesis,\nKobenhavens Universitet, 1999.\nS. McAdams. Perspectives on the contribution of timbre\nto musical structure. Computer Music Journal , 23:85–\n102, 1999.\nC. McKay, D. McEnnis, R. Fiebrink, and I. Fujinaga.\nAce: A framework for optomizing music classiﬁca-\ntion. International Conference on Music Information\nRetrieval , 2005.\nS. Pfeiffer, C. Parker, and T. Vincent. Maate, 2005. URL\nhttp://www.cmis.csiro.au/maaate/ . [Ac-\ncessed April 14, 2005].\nE. Scheirer and M. Slaney. Construction and evaluation\nof a robust multi-feature speech/music discriminator. In\nProceedings of the International Conference on Acous-\ntics, Speech, and Signal Processing , 1997.\nC. Su, K. Fung, and A. Leonov. Oc volume, 2005. URL\nhttp://ocvolume.sourceforge.net/ . [Ac-\ncessed April 14, 2005].\nG. Tzanetakis and P. Cook. Marsyas: A framework for\naudio analysis. Organized Sound , 10:293–302, 2000.\nI. Witten and E. Frank. Data mining: Practical machine\nlearning tools and techniques with Java implementa-\ntions. San Fransisco: Morgan Kaufmann, 1999.\n603"
    },
    {
        "title": "ACE: A Framework for Optimizing Music Classification.",
        "author": [
            "Cory McKay",
            "Rebecca Fiebrink",
            "Daniel McEnnis",
            "Beinan Li",
            "Ichiro Fujinaga"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415720",
        "url": "https://doi.org/10.5281/zenodo.1415720",
        "ee": "https://zenodo.org/records/1415720/files/McKayFMLF05.pdf",
        "abstract": "This paper presents ACE (Autonomous Classification Engine), a framework for using and optimizing classifiers. Given a set of feature vectors, ACE experiments with a variety of classifiers, classifier parameters, classifier ensembles and dimensionality reduction techniques in order to arrive at a good configuration for the problem at hand. In addition to evaluating classification methodologies in terms of success rates, functionality is also being incorporated into ACE allowing users to specify constraints on training and classification times as well as on the amount of time that ACE has to arrive at a solution. ACE is designed to facilitate classification for those new to pattern recognition as well as provide flexibility for those with more experience. ACE is packaged with audio and MIDI feature extraction software, although it can certainly be used with existing feature extractors. This paper includes a discussion of ways in which existing general-purpose classification software can be adapted to meet the needs of music researchers and shows how these ideas have been implemented in ACE. A standardized XML format for communicating features and other information to classifiers is proposed. A special emphasis is placed on the potential of classifier ensembles, which have remained largely untapped by the MIR community to date. A brief theoretical discussion of ensemble classification is presented in order to promote this powerful approach.",
        "zenodo_id": 1415720,
        "dblp_key": "conf/ismir/McKayFMLF05",
        "keywords": [
            "framework",
            "classifier experiments",
            "dimensionality reduction",
            "classification methodologies",
            "constraint specification",
            "pattern recognition",
            "flexibility",
            "audio and MIDI feature extraction",
            "existing feature extractors",
            "standardized XML format"
        ],
        "content": "ACE: A FRAMEWORK FOR OPTIMIZING MUSIC CLASSIFICATION \nCory McKay Rebecca Fiebrink Daniel McEnnis Beinan Li Ichiro Fujinaga \nMusic Technology \nMcGill University \nMontreal, Canada \ncory.mckay@ \nmail.mcgill.ca Music Technology \nMcGill University \nMontreal, Canada \nrfiebrink@ \nacm.org Music Technology \nMcGill University \nMontreal, Canada \ndaniel.mcennis@ \nmail.mcgill.ca Music Technology \nMcGill University \nMontreal, Canada \nbeinan.li@ \nmail.mcgill.ca Music Technology \nMcGill University \nMontreal, Canada \nich@ \nmusic.mcgill.ca \nABSTRACT \nThis paper presents ACE (Autonomous Classification \nEngine), a framework for using and optimizing classifi-\ners. Given a set of feature vectors, ACE experiments with \na variety of classifiers, classifier parameters, classifier \nensembles and dimensionality reduction techniques in \norder to arrive at a good configuration for the problem at \nhand. In addition to evaluating classification methodolo-\ngies in terms of success rates, functionality is also being \nincorporated into ACE allowing users to specify con-\nstraints on training and classification times as well as on \nthe amount of time that ACE has to arrive at a solution. \nACE is designed to facilitate classification for those \nnew to pattern recognition as well as provide flexibility \nfor those with more experience. ACE is packaged with \naudio and MIDI feature extraction software, although it \ncan certainly be used with existing feature extractors. \nThis paper includes a discussion of ways in which ex-\nisting general-purpose classification software can be \nadapted to meet the needs of music researchers and \nshows how these ideas have been implemented in ACE. \nA standardized XML format for communicating features \nand other information to classifiers is proposed. \nA special emphasis is placed on the potential of clas-\nsifier ensembles, which have remained largely untapped \nby the MIR community to date. A brief theoretical dis-\ncussion of ensemble classification is presented in order \nto promote this powerful approach. \n \nKeywords: music classification, classifier ensembles, \ncombining classifiers, optimization, MIR \n1 INTRODUCTION \nClassification techniques play an essential role in many \nMIR-related research areas. These include genre classifi-\ncation, similarity analysis, music recommendation, per-\nformer identification, composer identification and in-\nstrument identification, to name just a few. An examina-\ntion of the MIREX evaluation topics clearly demon-\nstrates the importance of classification in MIR. \nDespite this importance, there has been relatively lit-tle work on developing standardized and easy-to-use \nclassification software with the particular needs of music \nin mind. A survey of published MIR papers reveals that \nmany researchers either implement their own custom-\nbuilt systems or use off-the-shelf pattern recognition \nsoftware that was developed for fields other than music. \nThe former approach results in time wasted through \nduplication of effort and, potentially, relatively limited \nsoftware, as one only has so much time to devote to \nbuilding classifiers if this is only a part of a larger re-\nsearch project. Using general pattern recognition frame-\nworks can work well with some limited applications, but \none inevitably encounters complications, limitations and \ndifficulties due to the particularities of music. \nStandardized classification software especially \nadapted to MIR could therefore be of significant benefit. \nFortunately, some work has been done in this area. Mar-\nsyas (Tzanetakis and Cook 1999) in particular has been \nused effectively by many researchers, and M2K (Downie \n2004) has great promise. ACE (Autonomous Classifica-\ntion Engine) is proposed here as a framework that builds \nupon these important systems and addresses a number of \nareas that remain to be dealt with. \nSection 2 of this paper discusses the shortcomings of \ngeneral-purpose pattern recognition frameworks with \nrespect to music and proposes specific improvements. A \nparticular emphasis is put on the importance of a stan-\ndardized method of transmitting features from feature \nextractors to classification software. Several XML file \nformats are proposed in order to address this issue. \nSection 3 of this paper concentrates on grouping clas-\nsifiers into ensembles. Many MIR researchers perform \nexperiments with a variety of classifiers in order to find \nthe ones that are best suited to their particular tasks. \nOnly a few experiments, such as the Bodhidharma genre \nclassification system (McKay 2004), however, have \nbeen conducted on combining these classifiers into en-\nsembles.  \nThis is surprising, given the proven effectiveness of \nensemble algorithms such as AdaBoost (Freund and \nShapire 1996). Classifier ensembles have been gaining \nincreasing attention in the machine learning and pattern \nrecognition communities over the past decade, and the \nMIR community could certainly benefit from experi-\nmenting with the wide variety of potentially very power-\nful approaches that are available. This is particularly \ntrue considering the asymptotic behaviour that success \nrates appear to be demonstrating in a variety of MIR \nareas, as observed by Aucouturier and Pachet (2004). \nOf course, classifier ensembles do come at the cost of \nadded complexity, and the variety of approaches avail-Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full \ncitation on the first page. \n© 2005 Queen Mary, University of London \n42   \n \n able can be daunting. Section 3 of this paper presents a \nbrief survey of the field in order to encourage experi-\nmentation with classifier ensembles in MIR research. \nEven when only one classifier is used, the variety and \nsophistication of classification techniques can make it \ndifficult to decide which techniques and parameters to \nuse. Even the most experienced pattern recognition re-\nsearchers must often resort to experimentation. The ACE \nframework has been designed to deal with this problem \nautomatically. ACE performs optimization experiments \nusing different dimensionality reduction techniques, \nclassifiers, classifier parameters and classifier ensemble \narchitectures. Particular efforts have been made to inves-\ntigate the power of feature weighting (Fiebrink, McKay, \nand Fujinaga 2005). \nFunctionality is also being built into ACE that allows \nit to analyze the effectiveness of different approaches \nnot only in terms of classification accuracy, but also \ntraining time and classification time. This allows users to \nexperimentally determine the best set of techniques to \nuse for their particular priorities.  \nACE may also be used directly as a classifier. Once \nusers have selected the classifier(s) that they wish to use, \nwhether through ACE optimization or using pre-existing \nknowledge, they need only provide ACE with feature \nvectors and model classifications. ACE then trains itself \nand presents users with trained classifier(s). \nAn important advantage of ACE is that it is open-\nsource and freely distributable. ACE is implemented in \nJava, which means that the framework is portable and \neasy to install. ACE has also been built with a modular \nand extensible design philosophy. It is a simple and \nwell-documented matter for users to build upon ACE. \nSection 4 of this paper presents the ACE framework \nitself. The implementation and functionality of the soft-\nware are discussed, as are the benefits that it offers the \nMIR community. \nAny classification system is only as good as the fea-\ntures that it receives. ACE is packaged with easy-to-use, \nflexible and, perhaps most importantly, highly extensible \nfeature extraction software for extracting features from \nboth audio and MIDI files. There is no requirement to \nuse ACE with these feature extractors, however, as ACE \nis designed to work well with any feature extractors that \ncan generate appropriately formatted output. Section 5 \npresents a brief overview of the bundled feature extrac-\ntion software. \nBoth feature extraction and classification can be com-\nputationally intensive. This is particularly true of a sys-\ntem such as ACE, given its exploratory approach. Sec-\ntion 6 details future plans for adapting ACE so that it \ncan distribute its workload over multiple computers. \nSection 7 shows the results of several test classifica-\ntion tasks performed with ACE in order to evaluate its \neffectiveness. These include tests on standard bench-\nmarks from the UCI Machine Learning Repository as \nwell as two MIR-specific tasks. \nSection 8 summarizes this paper and Section 9 pre-\nsents some ideas for future additions to ACE. 2 DEVELOPING A CLASSIFICATION \nFRAMEWORK SUITED TO MIR \n2.1 Limitations of existing systems \nThe development of a general pattern recognition soft-\nware package is not trivial. Each application domain has \nits own needs and peculiarities that might not occur to \nresearchers in other fields. It is therefore no surprise that \nwhat general pattern recognition frameworks are avail-\nable have important weaknesses with respect to MIR. \nIn general, it appears that PRTools (van der Heijden \net al. 2004) and Weka (Witten and Frank 2000) are the \ntwo most often used general frameworks in MIR. \nPRTools is a Matlab toolbox and Weka is a Java appli-\ncation and code library. Both of these frameworks are \nvery well-designed and powerful tools, but they do have \nseveral limitations when applied to MIR. \nPRTools has the disadvantage that it is reliant upon \nMatlab, a proprietary software package. Although \nPRTools itself is free for academic use, one must still \npurchase Matlab in order to use it. Furthermore, one \nmust pay for PRTools if one wishes to use it commer-\ncially, and its licence does not permit it to be redistrib-\nuted. This means that any software that is developed \nusing PRTools cannot be distributed without special \npermission, and it cannot be distributed with an open \nlicence. So, although PRTools is certainly suitable for \nbasic research and prototyping, it is problematic with \nrespect to serious application development. \nThis introduces some of the important concerns with \nrespect to MIR software. The general consensus in the \nMIR community appears to be supportive of free, open \nsource and fully distributable software. This is important \nin ensuring research transparency and sharing of results, \nand it is essential in allowing researchers to build upon \neach other’s work. \nRelated to this is the importance of extensibility and \nmodularity. In an open research community, not only \nshould code be freely distributable, but it must be de-\nsigned so that others can expand upon it easily. \nPortability, documentation and ease of use and instal-\nlation are also important considerations. Although lip \nservice is often paid to these principles, they should be \ntaken very seriously. It is not at all an uncommon ex-\nperience for potential users to become discouraged by \ninstallation difficulties, such as linking errors, or by ar-\ncane code documentation. \nFurthermore, good MIR software should be usable \nand understandable by users with a variety of skill lev-\nels. The MIR community is composed of experts in a \nwide variety of fields, and it is not reasonable to expect \nall of them to be highly knowledgeable about classifica-\ntion, even though it might be of benefit to their research.  \nThe Weka data mining framework largely meets these \nrequirements. It is freely distributable, open source, rela-\ntively well documented, implemented with all of Java’s \nplatform-independence, beautifully designed and truly a \npleasure to work with. It also includes a variety of inter-\n43   \n \n faces for users with different needs and abilities. It is, \nhowever, as is inevitable with any general system, miss-\ning some important qualities with respect to MIR. \nThe most significant issues are related to the Weka \nARFF file format that is used to store features and com-\nmunicate them to classifiers. To begin with, there is no \ngood way to assign more than one class to a given in-\nstance. One possible solution is to break one multi-class \nproblem into many binary classification problems, so \nthat there is a separate ARFF file for every class, with all \ninstances classified as either belonging or not belonging \nto each class. Alternatively, one could create a separate \nclass for every possible combination of classes, with a \nresulting exponential increase in the numbers of classes. \nIt is clear that neither of these solutions is ideal. Un-\nfortunately, this is a problem with classification systems \nin general, not just Weka. This is understandable, as \nmost pattern recognition tasks require classification into \none and only one class. Unfortunately, a great deal of \nmusicological research involves certain unavoidable \nambiguities, and the imposition of only one class mem-\nbership on each instance is unrealistic for tasks such as \ngenre classification and many types of similarity-related \nclassification, for example. \nA second problem is that ARFF files do not permit \nany logical grouping of features. Each feature is treated \nas an independent quantity with no relation to any other \nfeature. One often encounters multi-dimensional features \nin music, and it can be useful to maintain some logical \nrelationship between the components of such features. \nPower spectra, MFCC’s, bins of a beat histogram and \ninstruments present are just a few examples. Maintaining \na logical relationship between the values of multi-\ndimensional features allows one to perform classifica-\ntions in particularly fruitful ways that take advantage of \ntheir interrelatedness, particularly with respect to classi-\nfier ensembles. Training one neural net on MFCC’s, for \nexample, and using another classifier for features such as \nRMS or spectral centroid could prove much more fruit-\nful than mixing the MFCC’s in with the other features. \nA third problem is that ARFF files do not allow any \nlabelling or structuring of instances. Each instance is \nstored only as a collection of feature values and a class \nidentifier, with no identifying metadata. In music, it is \noften appropriate to extract features over a number of \nwindows. Furthermore, some features may be extracted \nfor each window, some only for some windows and \nsome only for a recording as a whole. Weka and its \nARFF files provide no way of associating the features of \na window with the recording that it comes from, nor do \nthey provide any means of identifying recordings or of \nstoring time stamps associated with each window. This \nmeans that this information must be stored, organized \nand processed by some external software using some \nunspecified and non-standardized file format. \nA fourth problem is that there is no way of imposing \na structure on the class labels. One often encounters hi-\nerarchical structures in music, such as in the cases of \ngenre categories or structural analyses. Weka treats each class as distinct and independent. This means that there \nis no native way to use classification techniques that \nmake use of structured taxonomies. \nThese criticisms are not meant to denigrate Weka in \nany way. Quite to the contrary, in fact, as Weka is sin-\ngled out here only because it is arguably the best frame-\nwork available. One of the many positive aspects of \nWeka is that it is easy to write Java code that makes use \nof the excellent existing Weka code and adds functional-\nity to it, which is precisely what ACE does. \nMany of the issues discussed above apply to existing \nsystems developed specifically with music in mind as \nwell. As mentioned in Section 1, the two most well-\nknown and powerful such systems are Marsyas and \nM2K. \nMarsyas is a pioneering system that has been used \nvery effectively in a number of research projects. Unfor-\ntunately, there can be some portability and installation \nissues with this C++ based system. Marsysas is also cur-\nrently centred around audio classification, and does not \ncurrently include MIDI functionality.  \nIt is also unfair to compare Marsyas to general classi-\nfication systems such as Weka, as Marsyas was origi-\nnally designed primarily as a feature extractor, and per-\nforms very well at this task. Marsyas is, however, regu-\nlarly maintained by its creator, George Tzanetakis, and \nthere are plans to extend its functionality and possibly \nport increasing amounts of Weka’s functionality to it. \nM2K is a graphical feature extraction and classifica-\ntion framework based on the D2K parallel data mining \nand machine learning system. Although still in alpha \nrelease, and therefore impossible to fairly evaluate, M2K \npromises to be an extremely powerful and flexible sys-\ntem for MIR prototyping.  \nUnfortunately, M2K does inherit several licensing \nproblems from D2K that potentially limit its use beyond \nprototyping. D2K’s licence can make it complicated for \nresearchers outside the U.S.A. to obtain it, and forbids \nits use in commercial applications. This means that any \nsystem that uses D2K cannot itself be used for any non-\nresearch-based tasks. Furthermore, D2K is not open \nsource. \n2.2 Feature file formats \nIt is clear from Section 2.1 that there is an important \nneed for a standardized and flexible file format for stor-\ning feature values and communicating them to classifiers. \nExisting formats such as ARFF, while certainly suitable \nfor the types of tasks their designers had in mind, are \ninsufficient for the particular needs of MIR researchers. \nSeveral XML-based file formats are presented here in \norder to attempt to meet this need. XML is chosen be-\ncause it is not only a standardized format for which \nparsers are widely available, but is also extremely flexi-\nble. It is a verbose format, with the result that it is less \nspace efficient than formats such as ARFF, but this ver-\nbosity has the corresponding advantage that it allows \nhumans to easily read the files. This is particularly use-\n44   \n \n ful when one is working on debugging feature extrac-\ntors. \nAn important priority when developing a feature file \nformat is to enforce a clear separation between the fea-\nture extraction and classification tasks, as particular re-\nsearchers may have reasons for using particular feature \nextractors or particular classification systems. The file \nformat should therefore make it possible to use any fea-\nture extractor to communicate any features of any type \nto any classification system. This portability makes it \npossible to use features generated with different extrac-\ntors with the same classification system, or a given set of \nextracted features with multiple classification systems. \nThe reusability of files is another important consid-\neration. For example, it could be useful to use the same \nset of extracted features for a variety of tasks, such as \ngenre classification as well as artist identification. Simi-\nlarly, it could be convenient to reuse the same model \nclassifications with different sets of features. For exam-\nple, one could classify a given corpus of audio re-\ncordings and then later perform the same task on sym-\nbolic recordings of the same corpus using the same \nmodel classifications. Unfortunately, most current fea-\nture file formats merge feature values and model classi-\nfications, making this kind of reusability difficult. \nThe use of two separate files is therefore proposed for \nwhat is traditionally contained in one file, namely one \nfile for storing feature values and another for storing \nmodel classifications. Unique keys such as file names \ncan be used to merge the two files. The model classifica-\ntion file can be omitted when using unsupervised learn-\ning or classifying unknown patterns. \nWe also propose the use of an additional optional file \nfor specifying taxonomical structures. This enables one \nto specify the relationships between classes, information \nwhich can be very useful for tasks such as hierarchical \nclassification. This file can be omitted if only flat classi-\nfication is to be used. \nOne final optional file format is proposed for storing \nmetadata about features, such as basic descriptions or \ndetails about the cardinality of multi-dimensional fea-\ntures. Although not strictly necessary, such a file helps \nsolidify the potential for full independence between fea-\nture extractors and classifiers. A researcher with a classi-\nfier could be e-mailed a feature values file and a feature \ndefinitions file by other researchers, for example, and \nwould need no additional information at all about the \nfeature extractor used or the features it extracted. \nThe explicit Document Type Definitions (DTD’s) of \nthe four proposed ACE XML formats are shown in Fig-\nures 1 through 4. It can be seen from Figure 1 that fea-\ntures may be stored for overall instances, called data \nsets, which may or may not have sub-sections. This can \ncorrespond to a recording and its windows, for example. \nEach sub-section has its own features, and each data set \nmay have overall features as well. Each sub-section may \nhave start and stop stamps in order to indicate what por-\ntion of the data set it corresponds to. This makes it pos-\nsible to have windows of arbitrary and varying sizes that    \n<!ELEMENT feature_vector_file (comments,  \n                               data_set+)> \n<!ELEMENT comments (#PCDATA)> \n<!ELEMENT data_set (data_set_id, \n                  section*, \n                  feature*)> \n<!ELEMENT data_set_id (#PCDATA)> \n<!ELEMENT section (feature+)> \n<!ATTLIST section start CDATA \"\" \n                stop CDATA \"\"> \n<!ELEMENT feature (name, v+)> \n<!ELEMENT name (#PCDATA)> \n<!ELEMENT v (#PCDATA)> \nFigure 1. XML DTD of the ACE XML file for-\nmat for storing feature values. \n \n<!ELEMENT classifications_file(comments, \n                               data_set+)> \n<!ELEMENT comments (#PCDATA)> \n<!ELEMENT data_set (data_set_id, \n                  misc_info*, \n                  role?, \n                  classification)> \n<!ELEMENT data_set_id (#PCDATA)> \n<!ELEMENT misc_info (#PCDATA)> \n<!ATTLIST misc_info info_type CDATA \"\"> \n<!ELEMENT role (#PCDATA)> \n<!ELEMENT classification (section*, \n                        class*)> \n<!ELEMENT section (start, \n                 stop, \n                 class+)> \n<!ELEMENT class (#PCDATA)> \n<!ELEMENT start (#PCDATA)> \n<!ELEMENT stop (#PCDATA)> \nFigure 2. XML DTD of the proposed ACE XML \nfile format for storing classifications. \n \n<!ELEMENT taxonomy_file (comments, \n                         parent_class+)> \n<!ELEMENT comments (#PCDATA)> \n<!ELEMENT parent_class (class_name, \n                        sub_class*)> \n<!ELEMENT class_name (#PCDATA)> \n<!ELEMENT sub_class (class_name,  \n                     sub_class*)> \nFigure 3. XML DTD of the optional ACE XML \nfile format for storing class taxonomies.  \n \n<!ELEMENT feature_key_file (comments, \n                            feature+)> \n<!ELEMENT comments (#PCDATA)> \n<!ELEMENT feature (name, \n                   description?,  \n                   is_sequential, \n                   parallel_dimensions)> \n<!ELEMENT name (#PCDATA)> \n<!ELEMENT description (#PCDATA)> \n<!ELEMENT is_sequential (#PCDATA)> \n<!ELEMENT parallel_dimensions (#PCDATA)> \nFigure 4. XML DTD of the optional ACE XML \nfile format for storing feature definitions. \ncan overlap. Each feature has a name identifying it, \nwhich makes it possible to omit features from some data \nsets or sub-sections if appropriate. Each feature may also \nhave one or more values (denoted by the <v> element) in \norder to permit multi-dimensional features. \n45   \n \n Figure 2 shows the DTD for storing model classifica-\ntions. This format may also be used to output classifica-\ntion results. Each data set may have optional metadata \nassociated with it. Each data set can be broken into po-\ntentially overlapping sub-sections if desired, and each \nsub-section can be assigned one or more classes. Each \ndata set may be assigned one or more overall classes as \nwell. Each sub-section is given start and stop stamps to \nshow the region of influence of particular classes. \nThe DTD for the optional taxonomy format is shown \nin Figure 3. This format allows the representation of \nhierarchically structured taxonomies of arbitrary depth. \nThe final optional file format, for storing feature defi-\nnitions, is shown in Figure 4. This format enables one to \nstore the name of each possible feature, a description of \nit, whether or not it can be applied to sub-sections or to \noverall data sets only and how many dimensions it has. \n3 CLASSIFIER ENSEMBLES \n3.1 Motivation for using classifier ensembles \nAs noted in Section 1, many MIR researchers have per-\nformed experiments with multiple classifiers to see which \nare best suited to particular tasks, but few have attempted \nto combine these classifiers into ensembles. This section \nprovides justification for doing so. \nThe practice of combining classifiers into ensembles \nis inspired by the notion that the combined opinions of a \nnumber of experts is more likely to be correct than that \nof a single expert. Ideally, an ensemble will perform \nbetter than any of its individual component classifiers. \nAlthough this will often be the case, it is not necessarily \nguaranteed.  \nOne might question whether it is worth the increases \nin computational demands and implementation complex-\nity that often accompany ensemble classification if one \nis not guaranteed an increase in performance. Dietterich \n(2000) has proposed three reasons why classifier ensem-\nbles can be beneficial. \nThe first reason, referred to by Dietterich as the sta-\ntistical reason, is as follows. Suppose one has a number \nof trained classifiers. One knows how well they each \nperformed on the training, testing and potentially the \nvalidation data, but this is only an estimate of how well \nthey will each generalize to the universe of all possible \ninputs. If all of the classifiers performed similarly on the \ntesting and validation data, there is no way of knowing \nwhich is in fact the best classifier. If one chooses a sin-\ngle classifier, one runs the risk of accidentally choosing \none of the poorer ones. The statistical argument is par-\nticularly strong in cases where only limited training and \ntesting data is available, as the evaluation of individual \nclassifiers using test sets is likely to have a high error. \nThe second reason, referred to as the computational \nreason, applies to classifiers that train using hill-\nclimbing or random search techniques. Training multiple \nneural networks, for example, on the same training data \ncan very well result in significantly different trained classifiers, depending on the randomly generated initial \nconditions. Aggregating such classifiers into an ensem-\nble can take advantage of the multiplicity of solutions \noffered by the different classifiers. The computational \nargument highlights the particular appropriateness of \ninstable classifiers for ensemble classification, as they \ncan lead to a variety of useful solutions using only \nslightly modified training data. \nThe final reason, termed referential, is based on the \nfact that there is no guarantee that the types of classifiers \nthat one is using for a particular problem could ever \nconverge to a theoretically optimal solution. To provide \na simplified example, say a researcher mistakenly be-\nlieves that a given problem is linear, and decides to use \nonly linear classifiers. In reality, the optimal classifier \nwill be non-linear, so it is not possible that any of the \nlinear classifiers under consideration could perform op-\ntimally individually. However, an ensemble of linear \nclassifiers could approximate a non-linear decision \nboundary, and could therefore potentially perform better \nthan any single linear classifier ever could.  \nAn essential element in the effectiveness of classifier \nensembles is their diversity. If all of the classifiers in an \nensemble tend to misclassify the same instances, then \ncombining their results will have little benefit. In con-\ntrast, a greater amount of independence between the \nclassifiers can result in errors by individual classifiers \nbeing overlooked when the results of the ensemble are \ncombined. Many of the most successful ensemble tech-\nniques, such as bagging and boosting (see Section 3.2), \nare based on increasing classifier diversity. \nThe well-known effectiveness of algorithms such as \nAdaBoost (Freund and Shapire 1996) provide convinc-\ning experimental evidence for the efficacy of classifier \nensembles. It is therefore not surprising that many influ-\nential researchers, such as Josef Kittler (2000), continue \nto emphasize their value.  \n3.2 Overview of classifier ensemble techniques \nAlthough an in-depth survey of ensemble classification is \nbeyond the scope of the paper, a brief overview is pre-\nsented here in order to promote the use of classifier en-\nsembles in the MIR community. Kuncheva’s book \n(2004) is an excellent resource for those looking for \nmore information.  \nMethods for combining classifiers into ensembles are \noften divided among two groups. The first, classifier \nfusion, involves merging the results of all classifiers \nthrough a method such as voting. The second, classifier \nselection, involves using some system to dynamically \nselect which specialist classifiers to use for each particu-\nlar input pattern. The mixture of experts method, also \ncalled stacking, is an example of a hybrid method where \na classifier is trained to weight the votes of other classi-\nfiers in the ensemble. \nThe way in which features and training data are di-\nvided up among the component classifiers can play an \nimportant role in the success or failure of ensembles. \n46   \n \n Bagging and boosting are two powerful techniques that \nmake use of this fact in order to attempt to maximize \ndiversity and, correspondingly, ensemble effectiveness. \nBagging involves using bootstrapping to train the \nclassifiers. This means that each classifier acquires a \ntraining set by sampling all available training instances \nwith replacement. \nBoosting involves iteratively training classifiers so \nthat the instances that previous classifiers performed \npoorly on are emphasized in the training sets for subse-\nquent classifiers. The AdaBoost approach, of which \nthere are now many variants, is particularly well known \nfor its success. Boosting tends to perform better than \nbagging given enough training data, but bagging is better \nwith smaller training sets. \n4 ACE \nThe ACE system is designed with the dual goals of in-\ncreasing classification success rates and facilitating the \nprocess of classification for users of all skill levels. \nACE is implemented in Java using the Weka frame-\nwork. As discussed in Section 2, Weka is powerful, \nflexible and well designed, but it has some limitations \nwith respect to MIR research needs. A key aspect of \nACE is that it adapts Weka to meet these needs, includ-\ning multi-class membership, hierarchical taxonomies, \nmulti-dimensional features, instance sub-sections, etc. \nOne of the most important ways in which this is done \nis through the implementation of the ACE XML file \nformats, presented in Section 2.2. Although conversion \nutilities are included to convert between ACE XML and \nWeka’s ARFF format, arguably the current de facto \nstandard in MIR, the use of ACE XML is encouraged \nbecause of its superior expressive power. \nACE’s use of Weka makes it possible to take advan-\ntage of Weka’s many classification tools. These include \nclassifiers such as feedforward neural nets, support vec-\ntor machines, nearest neighbour classifiers, decision tree \nclassifiers and Bayesian classifiers, to name just a few. \nA variety of dimensionality reduction tools are also \navailable, such as principle component analysis and fea-\nture selection through genetic algorithms, exhaustive \ncomparisons and best first searches. Finally, a number of \nclassifier combination techniques are available, includ-\ning AdaBoost, bagging, majority voting and stacking. \nOne of the main features of ACE is that it automati-\ncally performs experiments with these approaches and \ntheir various parameters in order to find those that are \nwell suited to each problem’s particular needs. Different \napproaches often involve tradeoffs between classifica-\ntion success rates and processing times, and functional-\nity is being built into ACE to make it possible to meet \nthe needs of particular problems by allowing users to set \ntraining or testing time constraints. \nFunctionality is also being built into ACE that allows \nusers to specify limits on how long the system has to \narrive at a solution, with the result that ACE will initially \npursue the most promising approaches, based on past experiments with similar data, and output the best ap-\nproaches that it has found in the given time. This is ac-\ncomplished by having ACE monitor its own perform-\nance. \nACE’s incorporation of classifier ensembles has the \npotential to bring significantly improved classification \nrates to MIR research. Of course, it may be true in some \ncases that a pattern recognition expert could recommend \na specialized solution to a given problem that is as good \nor better than one found experimentally by ACE. ACE is \nnot intended to replace such experts, but rather to auto-\nmatically provide good solutions relatively quickly and \neffortlessly to users with diverse skill levels. \nACE allows those with only a peripheral background \nin pattern recognition to easily perform high-quality \nclassifications using a variety of methods. This is impor-\ntant, as pattern recognition experts rarely have special-\nized knowledge in applied fields such as music, and ex-\nperts in applied fields rarely have expertise in pattern \nrecognition. ACE makes sophisticated pattern recogni-\ntion accessible to all MIR researchers. ACE also pro-\nvides an excellent tool for those with more pattern rec-\nognition experience who wish to perform benchmarking \ncomparisons of new approaches. \nMuch like Weka itself, ACE includes several inter-\nfaces for users with different needs. The first way to use \nACE is through a GUI that allows users to build tax-\nonomies, label and manage training and testing in-\nstances, manage features, control classifier settings, \ncarry out comparisons of classification methodologies, \ntrain and use classifiers and view results of experiments \nand classifications. \nThe second way of using ACE is through a simple \ncommand-line interface. This interface is useful for us-\ners who already have the appropriate configuration files \nset up and would like a quick and easy method of per-\nforming tasks such as batch processing. \nThe final way of using ACE is for users to directly \naccess the ACE Java classes from their own software. \nACE is entirely open source, is well documented and is \nimplemented in an intuitive manner.  \n5 FEATURE EXTRACTION \nFeature extraction is a key part of any classification task. \nACE is therefore packaged with two feature extraction \napplications, jAudio and jSymbolic, for extracting fea-\ntures from audio and symbolic recordings respectively. \nThese feature extractors are powerful, flexible and, most \nimportantly, extensible. They are designed with the same \nportability and ease of use of the ACE system itself. \nThey have also been designed with an emphasis on the \nimportance of the logical separation of feature extractors \nand classifiers, and could easily be used with classifica-\ntion frameworks other than ACE. \nSimilarly, ACE is designed to work with arbitrary ex-\nisting feature extraction systems that can produce ARFF \nor, preferably, ACE XML files. Users are free to use \nwhatever feature extraction software they wish, and they \n47   \n \n may take advantage of ACE’s portability to install ACE \non whatever platform their feature extraction software \nalready runs on. jAudio and jSymbolic are provided for \nusers who do not already have feature extraction soft-\nware or who are interested in trying powerful new tools.  \njSymbolic is based on the Bodhidharma symbolic fea-\nture library (McKay 2004), the most extensive such li-\nbrary currently available. McEnnis et al. (2005) have \npublished further information on jAudio. \n6 DISTRIBUTING THE WORKLOAD \nClassification techniques can be computationally inten-\nsive, especially when many features are used or there are \nlarge training sets. This issue is amplified when multiple \nclassifiers are used. Functionality is therefore being built \ninto ACE to allow it run trials on multiple computers in \nparallel in order to achieve efficient and effective reduc-\ntions in execution time.  \nTwo distributed computing systems are currently be-\ning considered for use, namely Grid Weka (Khoussainov \net al. 2004) and M2K/D2K. Grid Weka has the advan-\ntage of being built directly on Weka. D2K is a powerful \nand well-established environment, and M2K holds great \npromise, but there is the drawback that D2K has certain \nlicensing issues, as discussed in Section 2.1. \nBoth Grid Weka and M2K/D2K allow computation to \nbe distributed among either multi-purpose workstations \nor dedicated machines, and both are compatible with a \nrange of hardware and operating system configurations. \nACE’s parallel capabilities could thus be exploited by \nanyone with access to a typical computing lab. \nOnce the distributed aspect of the system is complete, \na server-based sub-system will be designed that contains \na coordination system and database. Although not neces-\nsary for using ACE, users may choose to dedicate a \ncomputer to this server, allowing ACE to run perpetu-\nally. The server will keep a record of performances of all \nACE operations run on a particular user’s cluster and \ngenerate statistics for self-evaluation and improvement. \nACE will then make use of any idle time to attempt to \nimprove solutions to previously encountered but cur-\nrently inactive problems. \n7 BENCHMARK TESTING \nTwo groups of tests were performed to verify ACE’s \neffectiveness. The first group consisted of two MIR-\nrelated tasks, namely a beat-box recognition experiment \nand a reproduction of a previous seven-class percussion \nidentification experiment (Tindale et al. 2004). ACE \nachieved a classification success rate of 95.6% with the \nfive-class beat-box experiment using AdaBoost. Tin-\ndale’s best success rate of 94.9% was improved to 96.3% \nby ACE, a reduction in error rate of 27.5%.  \nThe second set of tests involved running ACE on ten \nUCI datasets (Blake and Merz 1998) from a variety of \nresearch domains. The results are shown in Table 1: Table 1. ACE’s classification success rate on ten \nUCI datasets using ten-fold cross-validation com-\npared to a published baseline (Kotsiantis and Pin-\ntelas 2004). \nData Set ACE’s \nSelected   \nClassifier Kotsiantis  \nSuccess \nRate ACE \nSuccess  \nRate \nanneal AdaBoost -- 99.6% \naudiology AdaBoost -- 85.0% \nautos AdaBoost 81.7% 86.3% \nbalance \nscale Naïve Bayes -- 91.4% \ndiabetes Naïve Bayes 76.6% 78.0% \nionosphere AdaBoost 90.7% 94.3% \niris FF Neural Net 95.6% 97.3% \nlabor k-NN 93.4% 93.0% \nvote Decision Tree 96.2% 96.3% \nzoo Decision Tree -- 97.0% \n \nIt can be seen that ACE performed very well, particu-\nlarly given the difficulty of some of these data sets.  This \nis emphasized by ACE’s excellent performance relative \nto a recently published algorithm, which was itself \nshown to be better than a wide variety of alternative al-\ngorithms (Kotsiantis and Pintelas 2004). Although statis-\ntical uncertainty makes it impossible to claim that ACE’s \nresults are inherently superior, it does show that ACE \ncan certainly achieve results probably as good as or bet-\nter than sophisticated state-of-the-art algorithms. \nWhat is particularly impressive is that ACE was \nforced to restrict each of its learning schemes to one \nminute or less for both training and testing on a typical \nPC (2.8 GHz P4). This was done in order to investigate \nACE’s ability to rapidly evaluate a wide variety of clas-\nsifiers. Although even higher success rates could likely \nhave been achieved with more training time, the per-\nformance achieved by ACE in this limited time demon-\nstrates its efficiency in exploratory research. \nTable 1 is also revealing in that it demonstrates that a \nvariety of classifiers will perform best given a variety of \ndata sets. Furthermore, AdaBoost was selected by ACE \n4 times out of 10, demonstrating the efficacy of ensem-\nble classification. These results support the appropriate-\nness of ACE’s experimental approach as well as its utili-\nzation of ensemble classification. \nThe demonstrated effectiveness of ACE with respect \nto both musical and general data is particularly encour-\naging given that there are still many pattern recognition \nschemes to be incorporated into ACE and that the use of \ndistributed computing in the future will make the alloca-\ntion of increased training times justifiable.  \n48   \n \n 8 CONCLUSIONS \nThe goals of ACE and this paper can be summarized as \nfollows: \n· Highlight the limitations of traditional pattern recogni-\ntion software when applied to MIR and propose and \nimplement a number of solutions. \n· Encourage experimentation with classifier ensembles in \nthe MIR community. \n· Provide portable classification software as well as \nMIDI and audio feature extraction software that em-\nphasize extensibility, ease of use and effectiveness. \n· Provide software that allows users to automatically \nperform experiments with various classifiers, classifier \nparameters, data reduction techniques, ensemble archi-\ntectures and ensemble parameters in order to find ap-\nproaches well suited to particular problems.  \n9 FUTURE RESEARCH \nAside from the plans to incorporate distributed process-\ning into ACE, as discussed in Section 6, there are a num-\nber of other future improvements planned. These include \nthe implementation of learning schemes important to \nMIR that are currently missing from Weka, such as hid-\nden Markov models. The inclusion of classifiers with \nmemory (e.g., neural networks with feedback) is also an \nimportant area for expansion, as these can play an impor-\ntant role in music research. \nThere are also plans to implement modules for facili-\ntating post-processing. The implementation of a tool for \ngenerating model classification files is another priority. \nAn additional goal is to strengthen ACE’s support of \nWeka’s unsupervised learning functionality. It would \nalso be beneficial to include tools for constructing black-\nboard systems, in particular ones that integrate knowl-\nedge sources based on heuristics. This would comple-\nment ACE’s machine learning approach nicely. \nACKNOWLEDGEMENTS \nThe generous financial support from the Social Sciences \nand Humanities Research Council of Canada, the Cen-\ntre for Interdisciplinary Research in Music, Media and \nTechnology (CIRMMT) and the McGill Alma Mater \nFund is greatly appreciated. \nREFERENCES \nAucouturier, J., and F. Pachet. 2004. Improving Timbre \nSimilarity: How high is the sky? Journal of Negative \nResults in Speech and Audio Sciences 1(1). \nBlake, C., and C. Merz. 1998. UCI repository of \nmachine learning databases. Retrieved April 13, 2005, \nfrom www.ics.uci.edu/~mlearn/MLRepository.html. \nUniversity of California, Irvine, Department of \nInformation and Computer Sciences.  Dietterich, T. G. 2000. Ensemble methods in machine \nlearning. In Multiple classifier systems, J. Kittler and F. \nRoli eds. New York: Springer. \nDownie, J. S. 2004. International music information \nretrieval systems evaluation laboratory (IMIRSEL): \nIntroducing D2K and M2K. Demo Handout at the \n2004 International Conference on Music Information \nRetrieval. \nFiebrink, R., C. McKay and I. Fujinaga. 2005. \nCombining D2K and JGAP for efficient feature \nweighting for classification tasks in music information \nretrieval. Proceedings of the 2005 International \nConference on Music Information Retrieval. \nFreund, Y., and R. E. Schapire. 1996. Experiments with \na new boosting algorithm. Proceedings of the \nInternational Conference on Machine Learning. 148–\n56. \nvan der Heijden, F., R. P. W. Duin, D. de Ridder and D. \nM. J. Tax. 2004. Classification, parameter estimation \nand state estimation: An engineering approach using \nMATLAB. New York: Wiley. \nKhoussainov, R., X. Zuo, and N. Kushmerick. 2004. \nGrid-enabled Weka: A toolkit for machine learning on \nthe grid. ERCIM News 59. \nKittler, J. 2000. A framework for classifier fusion: Is it \nstill needed? Proceedings of the Joint IAPR \nInternational Workshops on Advances in Pattern \nRecognition. 45–56. \nKotsiantis, S., and P. Pintelas. 2004. Selective voting. \nProceedings of the International Conference on \nIntelligent Systems Design and Applications. 397–402. \nKuncheva, L. 2004. Combining pattern classifiers. \nHoboken, NJ: Wiley. \nMcEnnis, D., C. McKay, I. Fujinaga, and P. Depalle. \n2005. jAudio: A feature extraction library. Proceedings \nof the 2005 International Conference on Music \nInformation Retrieval. \nMcKay, C. 2004. Automatic genre classification of MIDI \nrecordings. M.A. Thesis. McGill University, Canada. \nTindale, A., A. Kapur, G. Tzanetakis,  and I. Fujinaga. \n2004. Retrieval of percussion gestures using timbre \nclassification techniques. Proceedings of the \nInternational Conference on Music Information \nRetrieval. 541–4. \nTzanetakis, G., and P. Cook. 1999. MARSYAS: A \nframework for audio analysis. Organized Sound 4 (3): \n169–75. \nWitten, I., and E. Frank. 2000. Data mining: Practical \nmachine learning tools and techniques with Java \nimplementations. San Francisco: Morgan Kaufmann. \n49"
    },
    {
        "title": "An Investigation of Feature Models for Music Genre Classification Using the Support Vector Classifier.",
        "author": [
            "Anders Meng",
            "John Shawe-Taylor"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416052",
        "url": "https://doi.org/10.5281/zenodo.1416052",
        "ee": "https://zenodo.org/records/1416052/files/MengS05.pdf",
        "abstract": "In music genre classification the decision time is typically of the order of several seconds, however, most automatic music genre classification systems focus on short time features derived from 10 −50ms. This work investigates two models, the multivariate Gaussian model and the multivariate autoregressive model for modelling short time features. Furthermore, it was investigated how these models can be integrated over a segment of short time features into a kernel such that a support vector machine can be applied. Two kernels with this property were considered, the convolution kernel and product probability kernel. In order to examine the different methods an 11 genre music setup was utilized. In this setup the Mel Frequency Cepstral Coefficients were used as short time features. The accuracy of the best performing model on this data set was ∼44% compared to a human performance of ∼52% on the same data set. Keywords: Feature Integration, Product Probability Kernel, Convolution Kernel, Support Vector Machine, Music Genre 1",
        "zenodo_id": 1416052,
        "dblp_key": "conf/ismir/MengS05",
        "keywords": [
            "music genre classification",
            "decision time",
            "short time features",
            "multivariate Gaussian model",
            "multivariate autoregressive model",
            "kernel",
            "support vector machine",
            "Mel Frequency Cepstral Coefficients",
            "convolution kernel",
            "product probability kernel"
        ],
        "content": "AN INVESTIGATION OF FEATURE MODELS FOR MUSIC GENRE\nCLASSIFICATION USING THE SUPPORT VECTOR CLASSIFIER\nAnders Meng\nInformatics and Mathematical Modelling - B321\nTechnical University of Denmark\nam@imm.dtu.dkJohn Shawe-Taylor\nUniversity of Southampton\njst@ecs.soton.ac.uk\nABSTRACT\nInmusicgenreclassiﬁcationthedecisiontimeistypically\nof the order of several seconds, however, most automatic\nmusicgenreclassiﬁcationsystemsfocusonshorttimefea-\nturesderivedfrom 10−50ms. Thisworkinvestigatestwo\nmodels, the multivariate Gaussian model and themulti-\nvariateautoregressivemodel formodellingshorttimefea-\ntures. Furthermore, it was investigated how these models\ncan be integrated over a segment of short time features\ninto a kernel such that a support vector machine can be\napplied. Two kernels with this property were considered,\ntheconvolution kernel andproduct probability kernel . In\nordertoexaminethedifferentmethodsan 11genremusic\nsetup was utilized. In this setup the Mel Frequency Cep-\nstral Coefﬁcients were used as short time features. The\naccuracyofthebestperformingmodelonthisdatasetwas\n∼44%compared to a human performance of ∼52%on\nthe same data set.\nKeywords: Feature Integration, Product Probability\nKernel, Convolution Kernel, Support Vector Machine,\nMusic Genre\n1 INTRODUCTION\nThe ﬁeld of audio mining covering areas such as audio\nclassiﬁcation, retrieval, ﬁngerprinting etc. has receive d\nquitealotofattentionlatelybothfromacademicandcom-\nmercial groups. Some of this interest stems from an in-\ncreasedavailabilityoflargeonlinemusicstoresandgrow-\ningaccesstoliveradio-programs,musicstations,newson\nthe internet etc. The big task for the academic world is\nto ﬁnd methods for effectively searching and navigating\nthese large amounts of data.\nThegenreisprobablythemostimportantdescriptorof\nmusicineverydaylife,however,itisnotanintrinsicprop-\nerty of music such as e.g. tempo, which makes it more\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrstpage.\nc/circlecopyrt2005 Queen Mary, University of Londondifﬁcult to grasp with computational methods. Still, for a\nlimited amount of data and for coherent music databases\nthere seem to be a link between computational methods\nand human assessment, see e.g. [1, 2].\nIt is a well established fact that the success of a pat-\ntern recognition system is closely related to the task of\nﬁnding descriptive features. There exist a large amount\nof descriptive audio features, each designed for a speciﬁc\naudiominingtask. Thevariousfeaturescanbegroupedas\nperceptualfeaturessuchaspitch,loudness,beatorasnon-\nperceptualfeaturesastheMelFrequencyCepstralCoefﬁ-\ncients(MFCC).TheMFCCshavebeenappliedinarange\nofaudiominingtasks,andhaveshowngoodperformance\ncompared to other features at a similar time scale.\nIn music genre classiﬁcation the typical time horizon\nfor a human to classify a piece of music as belonging to\na speciﬁc genre is of the order of a quarter of a second\nup to several seconds, see [3]. Typically for automatic\nmusic genre classiﬁcation systems whole pieces of music\nareavailable,sothedecisiontimeisgenerallylongerthan\njust a few seconds.\nShort time features such as the MFCCs are typically\nderivedattimehorizonsaround 10−50msdependingon\nthestationarityoftheaudiosignal. Afewauthors[4,5,1]\nhave looked at methods for integrating (modelling) the\nshort time features to classify at longer time horizons. In-\ntegrationofshorttimefeatures( featureintegration )isalso\nknown as early information fusion. Late information fu-\nsion is another way of classifying at larger time horizons.\nThe idea of late information fusion is to combine the se-\nquence of outputs from a classiﬁer, like e.g. majority vot-\ning. Some techniques of information fusion (both early\nand late) have been considered in more detail in [4, 2].\nThe focus of this work was to extend the model of\n[2]formodellingthetemporalstructureofshorttimefea-\ntures and secondly to investigate different methods for\nhandling audio data using kernel methods such as the\nSupport Vector Machine (SVM) . The support vector ma-\nchineisknownforitsgoodgeneralizationperformancein\nhigh-dimensional spaces, furthermore, its ability to work\nimplicitly in a possible high-dimensional feature space\nmakes it possible to investigate non-linear relations in th e\ndata.\nThe paper is structured as follows. An overview of\nthe investigated features as well as a description of the\ntwo feature integration models the multivariate Gaussian\n604model (GM) and themultivariate autoregressive model\n(MAR)are given in section 2. Section 3brieﬂy explains\nthe classiﬁers applied to a music genre setup and further-\nmore explains the idea of information fusion. Section\n4presents the results of an 11genre music genre setup.\nLast, but not least a conclusion in section 5.\n2 FEATURES\nThe work presented in this paper will focus on construct-\ning descriptive features at larger time scales by modelling\nshort time features. Earlier work by [2, 1, 5] suggested\nto work with an intermediate time scale around 1second.\nHere three time scales have been considered, a short time\nscaleof30ms where short time features are extracted, a\nmedium time scale at2seconds (selected from the data\nset, see section 4) and a long time scale of30seconds,\nlimitedbythelengthofthemusicsnippets. Thelongtime\nscalecontainsinformationsuchasthe”mood”ofthesong\nas well as long-structural correlations.\n2.1 Short Time Features ( 30ms)\nThe short time feature extraction stage is really impor-\ntant in all audio processing applications, since it is the\nﬁrstleveloffeatureintegrationperformed1. Earlierresults\n[4, 5] indicate good performance in music genre classiﬁ-\ncation using the MFCCs and therefore these will be the\npreferredchoiceinthisinvestigation. Thesefeatureswer e\noriginallydevelopedforclassiﬁcationofspeech,however ,\nthey have been applied in various audio mining tasks, see\ne.g. [6] where they were used in a timbre similarity ex-\nperiment. The low order MFCCs contain information of\ntheslowlychangingspectralenvelopewhilethehigheror-\nder MFCCs explains the fast variations of the envelope.\nSeveral authors report success using only the ﬁrst 6−10\nMFCCs. In the music genre classiﬁcation setup, see sec-\ntion 4, we found that the ﬁrst seven MFCCs were ade-\nquate. Furthermore, a hop- and frame-size of 10ms and\n30ms, respectively, were used. The larger overlap results\nin more smooth transitions between consecutive feature\nvectors.\n2.2 Feature Integration ( >30ms)\nFeature integration is a method for capturing the tempo-\nral information in the features. With a good model the\nmost salient structural information remains and the noisy\npartissuppressed. Theideaofusingfeatureintegrationin\naudio classiﬁcation is not new, but has been investigated\nin earlier work by e.g. [1, 5, 2] where a performance in-\ncrease was observed. The idea of feature integration can\nbe stated more strict by observing a sequence of consecu-\ntive features\nxn+1,... ,xn+L→f(xn+1,... ,xn+L) =z, (1)\nwherethesequence {xn+1,... ,xn+L} ∈ RD×Lareinte-\ngratedintoanewfeaturevectordenotedas z∈ RMwhere\ntypically M << D ·LandLindicatesthenumberofshort\n1Basically this ﬁrst step is denoted as feature extraction and\nnot feature integration.time features used in the integration step. A commonly\nusedfeatureintegrationtechniqueisthe mean-variance of\nfeatures,whichprovidesaperformanceincrease,butgen-\nerally does not capture the temporal structure of the short\ntime features. An improvement to this is the ﬁlter-bank\napproach considered in [5] to capture the frequency con-\ntents of the temporal structure in the short time features.\nThisimprovementindicatedaperformanceincreasecom-\npared to the mean-variance model, see [2]. Recently an\nautoregressive model [2] was suggested for feature inte-\ngrationandprovided aperformance increasecompared to\nthe mean-variance and ﬁlter-bank approach.\nFigure 1 shows the ﬁrst seven normalized MFCCs of\na10secondexcerptofthemusicpiece MasterofRevenge\nby the heavy metal group Body Count . As observed from\nthecoefﬁcientsthereisbothtemporalcorrelationsaswell\nas correlations among features dimensions.\n0 200 400 600 800 10000123456\nFrames\nMFCC−coefficientsMFCC coefficients of \"Body Count − Masters Of Revenge\" (10 seconds)\nFigure 1: The ﬁrst seven normalized MFCCs of a 10second\nsnippet of ”Body Count - Masters of Revenge”. The temporal\ncorrelation and correlations among feature dimensions are very\nclear from this piece of music.\n2.2.1 Multivariate autoregressive model ( MAR)\nThemultivariate autoregressive model handles both tem-\nporal and correlations among feature dimensions, which\nmakes it a good candidate for feature integration. In [2]\na simple autoregressive model was suggested where sim-\nple refers to considering each feature dimension indepen-\ndently. The MAR model is popular in time-series mod-\nelling and prediction being both simple and well under-\nstood, see e.g. [7]. For a stationary time series of state\nvectors xn∈ RDthe MAR model is deﬁned by\nxn=K/summationdisplay\np=1Apxn−I(p)+µµµ+un, (2)\nwherethenoiseterm un(error-term)isassumedtobezero\nmean Gaussian distributed, hence un∼ N(un;0,C).\nTheD-dimensional parameter vector µµµis a vector of\nintercept terms that is included to allow for a non-zero\nmean of the time-series, see [8]. The matrices Ap∈\nRD×Dforp= 1... Kare the coefﬁcient matrices of\ntheK’th order multivariate autoregressive model. They\n605encode how much of the previous information given in\nxn−I(1),xn−I(2),..,xn−I(K)is present in xn. The above\nformulation is quite general as Irefers to a general set.\nFor a model order of K= 4, the set could be selected\nasI={1,2,3,4}orI={1,2,4,8}indicating that xn\nis predicted from these previous state vectors. In this pa-\nper we focus on the standard multivariate autoregressive\nmodel where I={1,2,3,... ,K }. When estimating the\nparameters of the model there is several methods avail-\nable, see e.g. [7]. The authors have used the ARFITpack-\nage, a regularized ordinary least squares approach, de-\nscribed in [8]. This package ensures the uniqueness of\nthe estimated parameters of the model.\n2.2.2 Multivariate Gaussian model ( GM)\nNeglecting the temporal correlations in the data, hence\nsetting the Apmatrices for p= 1,... ,Kin equation (2)\nto zero leads to the much simpler model\nxn=µµµ+un, (3)\nwhere µµµencode the mean value of the time series and\nun∼ N(un;0,C)is denoted the multivariate Gaussian\nmodel. The previous mentioned mean-variance model\nis the mean value µµµand the variance components given\nfromthediagonalofthecovariancematrix v= diag {C}.\nIf the full covariance matrix is used, only the upper (or\nlower) triangular coefﬁcients are needed due to the sym-\nmetry. The multivariate Gaussian model will be consid-\nered as the ”base-line” against the MAR model in the ex-\nperimentalsectionsinceitperformsbetterthanthetypica l\nmean-variance model.\nThe two feature integration techniques described\nabove can be used to derive features at the medium time\nscaleor used directly to derive features at the long time\nscale. The model order for the MAR model can be se-\nlected from e.g. Schwarz’s Bayesian Criterion (SBC) [8],\nwhich is implemented in the ARFITpackage or as in our\nexperimental setup, where a separate validation set was\nusedtodeterminetheoptimalmodelorderacrossdataex-\namples (music snippets).\n2.3 Unique Solutions\nPerforming feature integration the model parameters are\ntypicallyusedasnewfeaturevectorsatthenewtimescale.\nIf the model does not have a unique solution, two similar\naudiopiecescouldriskbeingclassiﬁedasdissimilar. Con-\nsider using a mixture of Gaussian (MoG) , given as\np(x|θθθ) =K/summationdisplay\nk=1p(k)p(x|k,θθθ),\nwhere p(k)(and/summationtextK\nk=1p(k) = 1) are the mixing pro-\nportions and p(x|k,θθθ)∼ N(x;µµµk,Ck), as a feature in-\ntegration model. Optimizing the model parameters from\nthe likelihood function using e.g. the EM-algorithm does\nnotnecessarilyprovideaglobalmaximumsincethelikeli-\nhood function has many local maximums. So using these\nmodelparameters(mixingproportions,meansandcovari-\nances) directly in a classiﬁer2would make no sense. Re-\n2Stacked in a vector.cent studies in kernels indicate that it is possible to inte-\ngrate this type of complicated models in a kernel, see e.g.\n[9,10]. ThemixtureofGaussianmodelwasconsideredas\nmodellingmusicsnippetsin[6]andwillbeinvestigatedas\na feature integration model in section 4.\n3 CLASSIFIERS\nEarlier work in the ﬁeld of music information retrieval\n(MIR) considered simple yet efﬁcient classiﬁers such as\nK-nearest neighbors, however, lately more computation-\nally demanding algorithms have been investigated. Only\na few researchers within the ﬁeld of MIRhave consid-\nered support vector machines ( SVM), see e.g. [11, 12].\nIn the following subsections the support vector classiﬁer\n(SVC)andthelinearneuralnetworkclassiﬁer(LNN)will\nbe brieﬂy discussed.\n3.1 Support Vector Classiﬁer\nThe challenge of machine learning is to provide the\nlearner with as broad a range of functions as possible\nwhilestillensuringthataccuratelearningcanbeachieved .\nUsing high-dimensional feature spaces satisﬁes the ﬁrst\nconstraintofensuringhighﬂexibility, butappears tobeat\nodds with the second since it is undermined by the curse\nofdimensionality. Asaresultwewouldexpectthatagood\nﬁt on the training data could still leave the generalization\nvery poor. Support vector machines [13] manage to avoid\nthisdifﬁcultybyoptimizingaboundonthegeneralization\nerror in terms of quantities that do not depend on the di-\nmension of the feature space [14], hence enabling good\nperformanceunaffectedbythecurseofdimensionality. In\nthe present work, the C-library LIBSVM [15] was used.\nThislibraryimplementstheone-against-onevotingtermi-\nnology to handle more than two classes.\n3.1.1 Kernels\nAtypicalappliedkernelforthesupportvectorclassiﬁeris\nthelinear kernel , which is deﬁned as\nκ(x,x/prime) =xTx/prime, hence an inner product between the in-\nput vectors. Another well known kernel is the Gaussian\nkernel(or RBF-kernel )withwidthparameter σdeﬁnedas\nκ(x,x/prime) = exp( − /bardblx−x/prime/bardbl2/2σ2). Using this kernel\nthe support vector classiﬁer is basically ﬁnding discrimi-\nnating dimensions in an inﬁnite feature space.\nThe linear and RBF kernel can be used in comparing\nvector data, however, when handling audio we are typi-\ncally forced to calculate the distance between two audio\nsnippetsofvaryinglengths,whichfortwopiecesofaudio\nis presented by the sequence of short time features: X=\n[x1,x2,... ,xL]∈ RD×LandX/prime= [x/prime\n1,x/prime\n2,... ,x/prime\nL/prime]∈\nRD×L/prime. The two audio ﬁles are not required to be of\nsame length, though in the present investigation they are\n(L=L/prime). Two different kernels have been investigated,\nwhich calculate a similarity between sequences of data,\ntheconvolution kernel [16] and the product probability\nkernel[9]. These kernels naturally incorporate feature in-\ntegration.\nConvolution Kernel - CK\nThe convolution kernel [16] handles all kinds of discrete\n606structures such as strings, trees and graphs. In this work\ntheconvolutionkernelmeasuresthedistance(correlation )\nbetween two audio pieces (between their feature vectors).\nThe kernel is deﬁned as\nκ(X,X/prime) =1\nL2L/summationdisplay\nv=1L/summationdisplay\nv/prime=1κI(xv,x/prime\nv/prime), (4)\nwhere κI(x,z)must be a valid kernel. It is interesting to\nnotethatifalinearkernelisusedafastcalculationcanbe\nobtained.\nProduct Probability Kernel - PPK\nTheproductprobabilitykernel introducedin[9]measures\nthe distance between probability models of the feature\nvectors. Other divergence based kernels have been sug-\ngested, see e.g. [10], for measuring a similar distance. In\n[6] the Kullback-Leibler similarity measure was applied\nto measure the distance between timbre models of mu-\nsic snippets modelled by a mixture of Gaussian, however,\nno closed form solution could be found using this diver-\ngence measure. With the product probability kernel , a\nclosed form solution can be determined for e.g. a mixture\nofGaussian,furthermore,the PPKfulﬁllstherequirement\nforakerneltobepositivesemi-deﬁnite. From[9]the PPK\nis given as\nκ(θθθ,θθθ/prime) =/integraldisplay\np(x|θθθ)ρp(x|θθθ/prime)ρdx, (5)\nwhere θθθ(θθθ/prime)are the parameters from modelling X(X/prime),\nρ >0andp(x|θθθ)is the probabilistic model of the short\ntime features of a music piece. ρcontrols the weight-\ning of low or high density areas of the probability dis-\ntribution. Selecting ρ= 1/2theBhattacharyya afﬁnity\nbetween distributions is found. A nice bi-product of se-\nlecting ρ= 1/2is a normalized kernel structure, since\nκ(θθθ,θθθ) =/integraltext\np(x|θθθ)dx= 1. This kernel can directly com-\nputethedistancebetweenthemodelssuggestedinsection\n2.2, and thus incorporates feature integration. As men-\ntioned in section 2.3 the problem of uniqueness is allevi-\nated for this kernel, since probabilistic models are com-\npared instead of model parameters.\nClosed form solutions of the kernel for the multivari-\nateGaussianandmixtureofGaussiancanbefoundin[9].\nAdditionally,wehavecalculatedaclosedformsolutionof\ntheMARmodel,butthedetailshavebeenomittedthrough\nlack of space3.\n3.2 Linear Neural Network classiﬁer (LNN)\nThe linear Neural Network has coutputs and is trained\nusingasquaredlossfunction[17]. Thisclassiﬁerhaspre-\nviously been applied withsuccess inmusic genre classiﬁ-\ncation, see e.g. [2, 4].\n3.3 Fusion Techniques\nTheearlyinformationfusion(featureintegration)wasdis -\ncussedinsection2.2. Lateinformationfusionistheprob-\n3Regarding computational complexity the methods ranked\nafter numerical complexity are (top: least computational inten-\nsive): GM,MAR,MoG.TheGMandMARarecloserrelatedin\ncomplexity than the MARand MoG.lemofcombiningtheresultsfromtheclassiﬁer. Thereex-\nistseveralwaysofperforminglateinformationfusion,see\n[18]. Inthepresentwork,themajorityvotingrulewasap-\nplied due to the SVM classiﬁer. In the majority vote rule,\nthe votes received from the classiﬁer are counted and the\nclass with the largest amount of votes is selected, hereby\nperforming consensus decision.\n4 EXPERIMENTS\nTo evaluate the different feature integration techniques a n\n11 genre music setup was investigated. As discussed in\nthe introduction, decisions can be made at different time\nscales. In the present work, the best achievable perfor-\nmanceat 30secondswillbepursued,usingtheabovefea-\nture integration techniques, voting technique and combi-\nnations of the two.\n4.1 Data set\nThedatasetconsistsof 11musicgenresdistributedevenly\namong the following categories: Alternative, Country,\nEasy Listening, Electronica, Jazz, Latin, Pop&Dance,\nRap&Hiphop,R&BandSoul,ReggaeandRock . Thedata\nset consists of a training set of 1098music snippets, 100\nfromeachgenreexceptforlatin,ofeach 30secondsanda\nseparatetestsetof 220musicsnippetseachof 30seconds\nin length. The music snippets were MP3encoded music\nwith a bit-rate ≥128kBdown-sampled with a factor two\nto22050Hz.\n4.1.1 Human evaluation\nTo test the integrity of the data set a human evaluation\nwasperformedonthemusicsnippets(ata 30secondtime\nscale)ofthetestset. Eachtestpersonoutof 9wasaskedto\nclassify each music snippet into one of the 11genres on a\nforcedchoicebasis. Eachpersonevaluated 33musicsnip-\npets out of the 220music pieces. No information except\nfor the genre of the music pieces was given prior to the\ntest. Theaverageaccuracyofthehumanevaluationacross\npeople and across genre was 51.8%as opposed to ran-\ndom guessing, which is ∼9.1%. The lower/upper 95%\nconﬁdence limits were 46.0%/57.7%(results shown in\nﬁgure 2, upper ﬁgure). The human evaluation shows that\nthecommongenredeﬁnitionislessconsistentforthisdata\nset, however, it is still interesting to observe how an auto-\nmatic genre system works in this setup.\n4.1.2 Results & Discussion\nIn each genre 90out of the 100music snippets from the\ntrainingsetwererandomlyselected 10timestoassessthe\nvariations in the data. In each of these runs the remaining\nmusicpieces( 10ineachgenre,except latin)wasusedasa\nvalidation set for tuning parameters such as Cin the sup-\nport vector classiﬁer and σin the RBF kernel. Optimal\nmodel order selection for the MAR models were deter-\nmined across music samples and evaluated on the valida-\ntionset. Amodelorderof K= 3atboth 2and30seconds\nwas found optimal.\nThemediumtimescalewasselectedbyevaluatingthe\nperformanceat 30secondsusingboththe GMMVandthe\n607Table1: Descriptionofthedifferentcombinationsinvestigated.\nAll investigations with the product probability kernel, ρ= 1/2\nwas used.\nScheme Description\nMOG,PPKMixture of Gaussian applied to each 30\nsecond music snippet. A PPK kernel was\ngenerated (dimension 990×990).\nGM,PPKA multivariate Gaussian is ﬁtted for each\n30second music snippet. A PPK kernel\nwas generated.\nGM,PPK,MVA multivariate Gaussian is ﬁtted for each\n2seconds of music data. A PPK kernel\nisgenerated(samplingappliedusingonly\n3samples from each music piece result-\ning in a kernel of 2970×2970). After\nclassiﬁcation with SVM, majority voting\nisapplied.\nGM,CONVA multivariate Gaussian is ﬁtted for each\n2seconds of music data and a linear con-\nvolutionkernelisapplied(takingmeanof\nthe parameters).\nGM,MVA multivariate Gaussian is ﬁtted for each\n2secondsofmusicdataandmajorityvot-\ning is applied to the outputs of the clas-\nsiﬁers. For the SVM a RBF-kernel was\napplied.\nGMA multivariate Gaussian is ﬁtted for each\n30second music snippet. For the SVM a\nRBF-kernelwas applied.\nMAR,PPKSame as above (see GMPPK), just with a\nmultivariate ARprocess.\nMAR,PPK,MVSame as above, just with a multivariate\nARprocess.\nMAR,PPK,CONVSame as above, just with a multivariate\nARprocess.\nMAR,CONVSame as above, just with a multivariate\nARprocess.\nMAR,MVSame as above, just with a multivariate\nARprocess.\nMARSame as above, just with a multivariate\nARprocess.\nMARMV method explained in table 1 varying the frame-\n/hop size of the medium time scale4. No big performance\nﬂuctuation was observed in this investigation, however, a\nsmall favor of a frame-/hop size of 2/1second was ob-\nserved. The various combinations investigated have been\ndescribed in more detail in table 1. For the mixture of\nGaussianmodelincorporatedinaproductprobabilityker-\nnel (MOG,PPK) the optimal model order for each music\nsnippet of 30seconds were selected by varying the model\norder between 2−6mixtures, and selecting the optimal\norder fromthe Bayesian Information Criterion (BIC).\nThe average accuracy over the ten runs of the various\ncombinationsillustratedintable1havebeenplottedinﬁg-\nure 2 (upper ﬁgure) with a 95%binomial conﬁdence ap-\nplied to the average values. From the accuracy plot there\nis a clear indication that the MAR model is performing\nbetter than the GM for both the SVMandLNNclassiﬁer.\nPerforming a McNemar test, see e.g. [19], on the mixture\nofGaussianmodel(MOGPPK)andtheGaussianmodelin\naproductprobabilitykernel(GMPPK)theprobabilitythat\n4Theinvestigatedframe-/hopsizeswere: {1s/0.5s,1.5s/0.75s,\n2s/1s, 2.5s/1.25s, 3s/1.5s,3.5s/1.75s }.0.250.30.350.40.450.50.55\nMOGPPK\nGMPPKGMPPKMVGMCONV\nGMPPKCONVGMMV\nGMMARPPK\nMARCONVMARMVMAR\nGMMVGMMARMV\nMARHUMAN\nMARPPKMVMARPPKCONV\nSVM LNNAccuracy on Test−setAccuracy\nModels\n−0.4−0.200.20.40.60.81\nMARMV(LNN)\nMARPPKHumanAccuracy of each genreAccuracy\nAlternative\nCountryEasy listeningElectronicaJazzLatin\nPop & Dance\nRap & HiphopR&b and SoulReggaeRockFigure 2: Upper:Average accuracy at 30seconds shown with\na95%binomial conﬁdence interval for all investigated combi-\nnations. Thelargerconﬁdenceintervalforhumansisduetoonly\nnine persons evaluating a part of the test-data. Lower:Aver-\nage accuracy with 95%conﬁdential interval of each genre at a\ntime scale of 30seconds using the two best performing combi-\nnations,MARMV andMARPPK . The average human accuracy\nin each genre isalso shown with a 75%conﬁdence interval.\nthetwomodelsareequalis 76%,hencethehypothesisthat\nthe models are equal cannot be rejected on a 5%signiﬁ-\ncance level. This observation, together with the good per-\nformance of the MAR model illustrate the importance of\nthe temporal information in the short time features. Even\nwith the various techniques applied in this setup we are\nstill around ∼8%from the average human accuracy of\n∼52%on this data set, but it is interesting to notice that\nreasonableperformanceisachievedwithfairlysimplefea-\nture integration models and fusion techniques using only\nthe ﬁrst seven MFCCs. The two best performing mod-\nels are the MAR model in a product probability kernel\n(MARPPK) and the MAR model modelled at 2seconds,\nafterwhichmajorityvotingisappliedontheLNNoutputs\n(MARMV), see ﬁgure 2 (upper). The McNemar test on\nthese two models showed a 43%signiﬁcance level thus it\ncan not be rejected that the two models are similar.\nTheadvantageoftheMARPPKmodelisthatweonly\nneed to store the model parameters at 30seconds, while\n608for the MARMV model a sequence of model parameters\nneed to be saved for each music snippet. The computa-\ntionalworkloadthough,isalittlelargerfortheMARPPK\nmodel when compared to the MARMV model.\nFigure 2 (lower) shows the accuracy on each of the\n11genres of the two models MARMV and MARPPK.\nThe MARPPK seem to be more robust in classifying all\ngenres, whereas the MARMV is much better at speciﬁc\ngenres such as Rap & Hiphop andReggae. However,\nthe MARMV does not capture any of the Rockpieces,\nbut generally confuses them with Alternative (not shown\nhere). Also illustrated in this ﬁgure is the human perfor-\nmance in the different classes. A conﬁdence interval of\n75%has been shown on the human performance, due to\nthe few test persons involved in the test. The humans are\nmuch better at genres such as Rap & Hiphop andReggae\nthan, e.g. Alternative , which also corresponds to some of\nthe behavior observed with the MARMV method.\n5 CONCLUSION\nThe purpose of this work has partly been to illustrate\nthe importance of modelling the temporal structure in the\nshort time features, and secondly how models of short\ntime features can be integrated into kernels, such that the\nsupportvectormachinecanbeapplied. Inthemusicgenre\nsetup the best performance was achieved with the MAR\nmodel in a product probability kernel (MARPPK) used in\ncombinationwithanSVMandwiththeMARmodelused\nin combination with majority voting (MARMV) in a lin-\near neural network. The average accuracy of these two\nmethods were ∼43%compared to a human average ac-\ncuracy of ∼52%.\nEventhoughtheresultspresentedinthisarticlewerea\nmusic genre setup, the general idea of feature integration\nand generating a kernel function, which efﬁciently evalu-\nates the difference between audio-models can be general-\nized and used in other ﬁelds of MIR.\nACKNOWLEDGEMENTS\nThe work was supported by the European Commission\nthrough the sixth framework IST Network of Excellence:\nPatternAnalysis,StatisticalModellingandComputationa l\nLearning (PASCAL), contract no. 506778.\nREFERENCES\n[1] G.TzanetakisandP.Cook. Musicalgenreclassiﬁca-\ntion of audio signals. IEEE Transactions on Speech\nand Audio Processing , 10(5), July 2002.\n[2] A. Meng, P. Ahrendt, and J. Larsen. Improving mu-\nsicgenreclassiﬁcationbyshort-timefeatureintegra-\ntion. InProc. of ICASSP , pages 1293–1296, 2005.\n[3] D. Perrot and R. Gjerdigen. Scanning the dial: An\nexploration of factors in identiﬁcation of musical\nstyle. In Proc. of Soc. Music Perception Cognition ,\n1999.[4] P. Ahrendt, A. Meng, and J. Larsen. Decision time\nhorizon for music genre classiﬁcation using short-\ntime features. In Proc. of EUSIPCO , pages 1293–\n1296, 2004.\n[5] M.F.McKinneyandJ.Breebaart. Featuresforaudio\nand music classiﬁcation. In Proc. of ISMIR , pages\n151–158, 2003.\n[6] J-J. Aucouturier and F. Pachet. Music similarity\nmeasures: What’stheuse? In Proc.ofISMIR ,2002.\n[7] HelmutL ¨utkepohl. IntroductiontoMultipleTimeSe-\nries Analysis . Springer-Verlag, 1993.\n[8] A.NeumaierandT.Schneider. Estimationofparam-\neters and eigenmodes of multivariate autoregressive\nmodels. ACM Transactions on Mathematical Soft-\nware, 27(1):27–57, March 2001.\n[9] T. Jebara, R. Kondor, and A. Howard. Probability\nproduct kernels. Journal of Machine Learning Re-\nsearch, pages 819–844, July 2004.\n[10] P. J. Moreno, P. P. Ho, and N. Vasconcelos. A\nkullback-leibler divergence based kernel for svm\nclassiﬁcation in multimedia applications. In Ad-\nvances in Neural Information Processing Systems\n16, Cambridge, MA, 2004. MIT Press.\n[11] L. Lu, S. Z. Li, and Zhang H.-J. Content-based au-\ndio segmentation using support vector machines. In\nACM Multimedia Systems Journal , volume 8, pages\n482–492, March 2003.\n[12] S.-Z.LiandG.Guo. Content-basedaudioclassiﬁca-\ntion and retrieval using svm learning. In First IEEE\nPaciﬁc-RimConferenceonMultimedia,InvitedTalk,\nAustralia, 2000.\n[13] N. Cristianini and J. Shawe-Taylor. An introduction\nto Support Vector Machines . Cambridge University\nPress, Cambridge, UK, 2000.\n[14] J.Shawe-TaylorandN.Cristianini. Onthegenerali-\nsationofsoftmarginalgorithms. IEEETransactions\non Information Theory , 48(10):2721–2735, 2002.\n[15] C-C. Chang and C-J. Lin. Libsvm : A li-\nbrary for support vector machines, 2001.\n(http://www.csie.ntu.edu.tw/ cjlin/libsvm).\n[16] David Haussler. Convolution kernels on discrete\nstructures. Technical report, University of Califor-\nnia at Santa Cruz, July 1999.\n[17] C. M. Bishop. Neural Networks for Pattern Recog-\nnition. Oxford University Press, 1995.\n[18] J. Kittler, M. Hatef, Robert P.W. Duin, and J. Matas.\nOncombiningclassiﬁers. IEEETransactionsonPat-\ntern Analysis and Machine Intelligence , 20(3):226–\n239, 1998.\n[19] T.G Diettereich. Approximate statistical tests for\ncomparing supervised classiﬁcation learning algo-\nrithms.NeuralComputation ,(10):1895–1924,1998.\n609"
    },
    {
        "title": "Comparing Pitch Spelling Algorithms.",
        "author": [
            "David Meredith 0001",
            "Geraint A. Wiggins"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416366",
        "url": "https://doi.org/10.5281/zenodo.1416366",
        "ee": "https://zenodo.org/records/1416366/files/MeredithW05.pdf",
        "abstract": "A pitch spelling algorithm predicts the pitch names of the notes in a musical passage when given the onset-time, MIDI note number and possibly the duration and voice of each note. Various versions of the algorithms of LonguetHiggins, Cambouropoulos, Temperley and Sleator, Chew and Chen, and Meredith were run on a corpus containing 195972 notes, equally divided between eight classical and baroque composers. The standard deviation of the accuracies achieved by each algorithm over the eight composers was used as a measure of its style dependence (SD). Meredith’s ps1303 was the most accurate algorithm, spelling 99.43% of the notes correctly (SD = 0.54). The best version of Chew and Chen’s algorithm was the least dependent on style (SD = 0.35) and spelt 99.15% of the notes correctly. A new version of Cambouropoulos’s algorithm, combining features of all three versions described by Cambouropoulos himself, also spelt 99.15% of the notes correctly (SD = 0.47). The best version of Temperley and Sleator’s algorithm spelt 97.79% of the notes correctly, but nearly 70% of its errors were due to a single sudden enharmonic change. LonguetHiggins’s algorithm spelt 98.21% of the notes correctly (SD = 1.79) but only when it processed the music a voice at a time. Keywords: pitch spelling, transcription, algorithms, evaluation. 1",
        "zenodo_id": 1416366,
        "dblp_key": "conf/ismir/MeredithW05",
        "keywords": [
            "pitch spelling",
            "pitch names",
            "musical passage",
            "onset-time",
            "MIDI note number",
            "duration",
            "voice",
            "composer",
            "standard deviation",
            "style dependence"
        ],
        "content": "COMPARING PITCH SPELLING ALGORITHMS\nDavid Meredith Geraint A. Wiggins\nCentre for Cognition, Computation and Culture\nDepartment of Computing\nGoldsmiths’ College, University of London\nNew Cross, London, SE14 6NW.\ndave@titanmusic.com, g.wiggins@gold.ac.uk\nABSTRACT\nA pitch spelling algorithm predicts the pitch names of the\nnotes in a musical passage when given the onset-time,\nMIDI note number and possibly the duration and voice of\neach note. Various versions of the algorithms of Longuet-\nHiggins, Cambouropoulos, Temperley and Sleator, Chew\nand Chen, and Meredith were run on a corpus contain-\ning 195972 notes, equally divided between eight clas-\nsical and baroque composers. The standard deviation\nof the accuracies achieved by each algorithm over the\neight composers was used as a measure of its style de-\npendence ( SD). Meredith’s ps1303 was the most ac-\ncurate algorithm, spelling 99.43% of the notes correctly\n(SD = 0.54). The best version of Chew and Chen’s al-\ngorithm was the least dependent on style ( SD = 0.35)\nand spelt 99.15% of the notes correctly. A new version\nof Cambouropoulos’s algorithm, combining features of\nall three versions described by Cambouropoulos himself,\nalso spelt 99.15% of the notes correctly ( SD= 0.47). The\nbest version of Temperley and Sleator’s algorithm spelt\n97.79% of the notes correctly, but nearly 70% of its errors\nwere due to a single sudden enharmonic change. Longuet-\nHiggins’s algorithm spelt 98.21% of the notes correctly\n(SD= 1.79) but only when it processed the music a voice\nat a time.\nKeywords: pitch spelling, transcription, algorithms,\nevaluation.\n1 INTRODUCTION\nApitch spelling algorithm is an algorithm that attempts to\ncompute the correct pitch names (e.g., C /sharp4, B/flat5 etc.) of\nthe notes in a passage of tonal music, when given only the\nonset-time, MIDI note number and possibly the duration\nand voice of each note.\nThere are good practical reasons for attempting to de-\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of London\nFigure 1: Three perceptually similar patterns with differ-\nent chromatic pitch interval structures (from the ﬁrst bar\nof the Prelude in C minor (BWV 871/1) from Book 2 of\nJ. S. Bach’s Das Wohltemperirte Clavier ).\nvelop a reliable pitch spelling algorithm. First, until such\nan algorithm is devised, it will be impossible to construct\na reliable MIDI-to-notation transcription system —that is,\na system that reliably computes a correctly notated score\nof a passage when given only a MIDI ﬁle of the passage as\ninput. Second, existing audio transcription systems gen-\nerate not notated scores but MIDI-like, ‘piano roll’ repre-\nsentations as output (Abdallah and Plumbley, 2004). So, if\none needs to produce a notated score from a digital audio\nrecording, one needs not only an audio transcription sys-\ntem but also a MIDI-to-notation transcription algorithm\n(incorporating a pitch spelling algorithm).\nThird, knowing the letter-names of the pitch events in\na passage is useful in music information retrieval and mu-\nsical pattern discovery (see, for example, Meredith et al.,\n2002, pp. 328–330). In particular, two occurrences of a\nmotif on different degrees of a scale might be perceived\nto be similar even if the corresponding chromatic inter-\nvals in the patterns differ. Figure 1, for example, shows an\ninstance of tonal melodic sequence in which the three pat-\nterns A, B and C are perceived to be three occurrences of\nthe same motif even though the corresponding chromatic\nintervals are different in the three patterns. Note that, in\nthis example, one important aspect of the perceived simi-\nlarity between patterns A, B and C is nicely represented in\nthe notation by the fact that they all have the same scale-\nstep interval structure (i.e., a descending step followed by\ntwo ascending steps). In other words, one result of the\nchoice of pitch names for the notes in this passage is that\nthe scale-step interval structures are the same for these\nthree perceptually similar but chromatically different pat-\n280terns.\nIf the pitch names of the notes are encoded, matches\nsuch as the ones in Figure 1 can be found using fast\nexact-matching algorithms (e.g., Knuth et al., 1977; Galil,\n1979). However, if just MIDI note numbers are used,\nmatches such as the ones in Figure 1 can only be found\nusing slower and more error-prone approximate-matching\nalgorithms (e.g., Cambouropoulos et al., 2002).\nIn the study reported here, pitch spelling algorithms\nproposed by several authors were analysed, evaluated\nand, in some cases, improved. The algorithms studied\nwere those of Longuet-Higgins (1987), Cambouropoulos\n(1996, 2001, 2003), Temperley (2001), Chew and Chen\n(2003, 2005) and Meredith (2003, 2005). A number of\ndifferent versions of each of these algorithms were run on\na test corpus containing 195972 notes, equally divided be-\ntween eight classical and baroque composers.\nSection 2 below is a discussion of the methodology\nused in this study to evaluate the algorithms. The vari-\nous versions of the algorithms that were tested are then\nbrieﬂy described in section 3. The results obtained are\nsummarised and discussed in section 4. Finally, in sec-\ntion 5 we present the main conclusions that can be drawn\nfrom this study and suggest ways in which the research\nreported here could be continued.\n2 METHODOLOGY\nWhen comparing algorithms, one must ﬁrst identify rel-\nevant evaluation criteria —that is, speciﬁc ways in which\nthe performance of one algorithm might be considered in-\nterestingly different from that of another. Then appropri-\nateperformance metrics have to be deﬁned for these eval-\nuation criteria. A performance metric for a particular eval-\nuation criterion is a way of measuring the performance of\nan algorithm with respect to that criterion. When compar-\ning pitch spelling algorithms, these performance metrics\nare used to measure how well an algorithm performs on\nsome speciﬁed test corpus of works.\nIn this paper, we use two principal evaluation criteria:\nspelling accuracy , that is, how well an algorithm predicts\nthe pitch names of the notes; and style dependence , that\nis, how much the spelling accuracy of an algorithm is af-\nfected by the style of the music being processed. The per-\nformance metric used here to measure spelling accuracy is\nnote accuracy : the note accuracy of an algorithm Aover\na set of movements Sis deﬁned to be the proportion of\nnotes in Sspelt correctly by A. In the study reported here,\nthe note accuracies were measured over the complete test\ncorpus and over each of the eight subsets of this corpus\ncontaining the works by a particular composer. The stan-\ndard deviation of the note accuracies over these eight com-\nposer subsets was used as a measure of style dependence\n(SD).\nA test corpus should, ideally, be a large, representa-\ntive sample of the population of works that the algorithms\nwill be used to process in the future. Unfortunately, most\nof the test corpora used in previous publications for eval-\nuating pitch spelling algorithms have not been properly\nrepresentative of any wider population of works. In some\ncases, this has been because the test corpus was eitherfar too small (Longuet-Higgins, 1987; Cambouropoulos,\n1996; Chew and Chen, 2003, 2005) or too small to repre-\nsent the target population of works (Stoddard et al., 2004).\nIn other cases, the test corpus has consisted of movements\nthat are too closely related in terms of genre, composer\nand instrumentation (Meredith, 2003; Cambouropoulos,\n2001, 2003). In yet other cases, the test corpora consisted\nof small fragments from works that were either incip-\nits and main themes (Cambouropoulos, 1996) or extracts\nspecially chosen to illustrate particular music-theoretical\nphenomena (Temperley, 2001). Such corpora cannot be\nconsidered representative of some interesting wider pop-\nulation of complete musical works. The test corpus used\nby Meredith (2005) was relatively large (1729886 notes),\nbut it was very unevenly divided between the nine com-\nposers represented, ranging from 2962 notes from works\nby B. Marcello to 627083 notes from works by J. S. Bach.\nThis meant that the results obtained for the different com-\nposers were not comparable and thus not appropriate for\nmeasuring style dependence.\nIn the study described here, the test corpus contained\n195972 notes, consisting of 216 movements from works\nby eight baroque and classical composers (Corelli, Vi-\nvaldi, Telemann, J. S. Bach, Handel, Haydn, Mozart and\nBeethoven). This corpus was chosen so that it contained\nalmost exactly 24500 notes for each of the eight com-\nposers represented.\nSeveral algorithms tested here achieved note accu-\nracies higher than 99% which prompts one to question\nwhether the differences between these values are statis-\ntically signiﬁcant. To date, only Meredith (2005) has at-\ntempted to measure the statistical signiﬁcance of the dif-\nference between the spelling accuracies achieved by pitch\nspelling algorithms. However, he used McNemar’s test\nfor this purpose (McNemar, 1969, pp. 54–8), and this test\nis not strictly appropriate in this situation because its va-\nlidity depends on the correctness of any particular pitch\nname being independent of the correctness of the pitch\nnames assigned to the notes around it—which is usually\nnot the case since pitch spelling algorithms typically use\nthe context surrounding a note to determine its pitch name.\nIn fact, it seems that there is no straightforward statistical\nmethod that is entirely appropriate for measuring the sig-\nniﬁcance of the difference between two spelling accura-\ncies. In order to avoid the possibility of giving misleading\nestimates of signiﬁcance, we have therefore decided not\nto provide such estimates in this paper.\nOccasionally in tonal music, a modulation occurs that\nresults in a passage being in an extremely ﬂat or extremely\nsharp key that requires many double ﬂats or double sharps.\nComposers often choose to notate such passages in en-\nharmonically equivalent keys that require fewer acciden-\ntals because this usually makes the music easier to read.\nWhen this happens, a sudden enharmonic change occurs\nin the score in which the notated key suddenly changes\nto a very distant key even though no such modulation is\nheard by the listener (Temperley, 2001, p. 135). The test\ncorpus used here contained just one example of such a\nsudden enharmonic change. This occurs at bar 166 in the\nfourth movement of Haydn’s Symphony No. 100 in G ma-\njor (‘Military’) (Hob. I:100) where the notated key sud-\n281Pitch name class\nSharpness. . .\n. . .F/flat\n−8C/flat\n−7G/flat\n−6D/flat\n−5A/flat\n−4E/flat\n−3B/flat\n−2F\n−1C\n0G\n1D\n2A\n3E\n4B\n5F/sharp\n6C/sharp\n7G/sharp\n8D/sharp\n9A/sharp\n10E/sharp\n11B/sharp\n12. . .\n. . .\nFigure 2: The ‘line of ﬁfths’ showing the sharpness associated with each pitch name class.\ndenly changes from D /flatmajor to C /sharpmajor, even though\nno change in key is perceived by the listener. As no mod-\nulation is perceived by the listener, it can be argued that\nspelling this movement without the enharmonic change\n(i.e., staying in D /flatmajor) would also be correct. It was\ntherefore decided that the output of each algorithm for\nthis movement should be compared with two “correct”\nspellings: one in which the notes are spelt as they are\nin the original score; and a second, modiﬁed version, in\nwhich the enharmonic change is omitted. When an algo-\nrithm performed better on the modiﬁed version than on\nthe original, an alternative value for its note accuracy will\nbe given in the results.\n3 THE ALGORITHMS TESTED\nThe algorithms considered in this study were those of\nLonguet-Higgins (1987), Cambouropoulos (1996, 2001,\n2003), Temperley (2001), Chew and Chen (2003, 2005)\nand Meredith (2003, 2005). A number of different ver-\nsions of each of these algorithms were run on the test cor-\npus. These algorithms will now be brieﬂy described.\n3.1 Longuet-Higgins’s pitch spelling algorithm\nPitch spelling is one of the tasks performed by Longuet-\nHiggins’s (1987) music.p program, which was designed\nto be used only on monophonic melodies (Longuet-\nHiggins, 1987, p. 114). Given the MIDI note number and\nonset time of each note, Longuet-Higgins’s algorithm esti-\nmates a value of sharpness ,q, which is an integer indicat-\ning the position of the pitch name of the note on the line of\nﬁfths (Temperley, 2001, p. 117) (see Figure 2). The pitch\nname of each note can then be computed from its MIDI\nnote number and its sharpness. Longuet-Higgins’s algo-\nrithm is based on a “theory of tonality” (Longuet-Higgins,\n1987, p. 115) which consists of six rules. The ﬁrst of these\nrules ensures that each note is spelt so that it is as close as\npossible to the local tonic on the line of ﬁfths (Longuet-\nHiggins, 1987, pp. 112–113). The other rules control the\nway in which the algorithm deals with chromatic intervals\nand modulations. In particular, the second rule states that\nif the current key implies two consecutive chromatic in-\ntervals (i.e., intervals spanning more than 6 steps on the\nline of ﬁfths), then the key should be changed so that both\nintervals become diatonic (i.e., they span less than 6 steps\non the line of ﬁfths) (Longuet-Higgins, 1987, p. 113). The\nalgorithm is also restricted to assigning pitch names be-\ntween G /flat/flatand A\n on the line of ﬁfths.\nIn the version of the algorithm implemented in\nmusic.p the second of the six rules in Longuet-\nHiggins’s theory is not implemented correctly. Speciﬁ-\ncally, this rule implies that a subdominant preceded and\nfollowed by a sharpened subdominant ( /sharpˆ4−ˆ4−/sharpˆ4) should\ntrigger a modulation, as should the sequence /flatˆ2−ˆ2−/flatˆ2.\nHowever, neither of these sequences actually triggers a\nmodulation in Longuet-Higgins’s implementation. A newr r r r r r r r r r r r r r r r r r r r\nFigure 3: Cambouropoulos’s own caption to this ﬁgure\nreads: “Shifting overlapping window technique. For each\nwindow only the middle section of spelled pitches (bold\nline) is retained. Dots represent the pitches of the input\nsequence.” (Reproduced (with minor corrections and al-\nterations) from Cambouropoulos, 2003, p. 420, Fig. 6 and\nCambouropoulos, 1996, p. 245, Fig. 8.)\nversion of the algorithm was therefore constructed in\nwhich the implementation of this second rule was cor-\nrected. Both the original and corrected versions were also\nfurther modiﬁed to produce other versions that were not\nrestricted to assigning pitch names within any particular\nrange on the line of ﬁfths.\nEach of these versions of Longuet-Higgins’s algo-\nrithm was run on two different versions of the test cor-\npus: one in which the notes within each movement were\nsorted so that the voices were arranged ‘end-to-end’; and\na second version in which the notes were sorted by onset\ntime and MIDI note number (with preference given to on-\nset time). Longuet-Higgins’s insistence that his program\nshould only be used on monophonic music led us to ex-\npect his algorithm to perform better on the version of the\ndataset in which the voices were arranged ‘end-to-end’.\n3.2 Cambouropoulos’s pitch spelling algorithms\nCambouropoulos (1996, 2001, 2003) has published de-\nscriptions of three versions of his pitch spelling algorithm.\nIn all three, it is assumed that the passage of music to be\nprocessed has been represented as a sequence of MIDI\nnote numbers in which the notes are in the same order\nas in the music. This sequence of MIDI note numbers\nis then processed using a “shifting overlapping window-\ning technique” (Cambouropoulos, 2003, p. 420), in which\neach window contains a certain number of contiguous el-\nements in the input sequence (see Figure 3).\nCambouropoulos allows ‘white note’ pitch classes\n(i.e., 0, 2, 4, 5, 7, 9 and 11) to be spelt in three differ-\nent ways (e.g., pitch class 0 can be spelt as B /sharp, C/naturalor D/flat/flat)\nand ‘black note’ pitch classes to be spelt in two different\nways (e.g., pitch class 6 can be spelt as F /sharpor G/flat) (see,\nfor example, Cambouropoulos, 1996, p. 242). All possi-\nble spellings for each window are then generated and a\npenalty score is computed for each spelling. This penalty\nscore is found by computing a penalty value for each pitch\ninterval in the spelling and summing these interval penalty\nvalues. A given interval is penalised more heavily the\nless frequently it occurs in the major and minor scales,\na principle that Cambouropoulos (2003, p. 421) calls in-\nterval optimisation . An interval is also penalised if either\n282TPR 1 (Pitch Variance Rule) Prefer to label nearby events so that they are\nclose together on the line of ﬁfths.\nTPR 2 (Voice-Leading Rule) Given two events that are adjacent in time and\na half-step apart in pitch height: if the ﬁrst event is remote from the\ncurrent center of gravity, it should be spelled so that it is ﬁve steps\naway from the second on the line of ﬁfths.\nTPR 3 (Harmonic Feedback Rule) Prefer TPC representations which re-\nsult in good harmonic representations.\nFigure 4: Temperley’s preference rule system for pitch\nspelling (from Temperley, 2001, pp. 124–132, 359).\nof the pitch names forming the interval is a double-sharp\nor a double-ﬂat, a principle that Cambouropoulos (2003,\np. 421) calls notational parsimony . For each window, the\nalgorithm chooses the spelling that has the lowest penalty\nscore.\nThe earliest published version of this method\n(Cambouropoulos, 1996) was designed for processing\nmonophonic melodies and includes a rule, based on\nKrumhansl’s (1990, pp. 150–151) principle of contextual\nasymmetry, that is applied as a ‘tie-breaker’ when two\nor more spellings for a given window achieve the least\npenalty score. The two more recent published versions\n(Cambouropoulos, 2001, 2003) are adapted for processing\npolyphonic music. However, they differ from each other\nin that the earlier of the two (Cambouropoulos, 2001, p. 5)\nuses a “variable length window” which always contains a\nﬁxed number of distinct MIDI note numbers.\nAn analysis of the versions of this algorithm described\nby Cambouropoulos in his publications revealed a num-\nber of ways in which two versions might differ in their\ndetailed features. For example, one might use a variable\nlength window and the other a ﬁxed length window; one\nmight use the ‘tie breaker’ function and the other might\nnot; and so on. Twenty-six versions of the algorithm\nwere tested in this study, including those described by\nCambouropoulos himself together with other versions that\nwere carefully selected so that an estimate could be ob-\ntained as quickly as possible of the best combination of\ndetailed features. This ‘optimal’ combination was then\ntested by implementing it in a ﬁnal version of the algo-\nrithm which was run on the test corpus.\n3.3 Temperley and Sleator’s pitch spelling algorithm\nTemperley’s (2001) theory of music cognition consists of\npreference rule systems for six aspects of musical struc-\nture: metre, phrasing, counterpoint, harmony, key and\npitch spelling. Most of this theory has been implemented\nby Daniel Sleator in a suite of computer programs called\nMelisma .1These programs take “note list” representations\nas input (Temperley, 2001, pp. 9–12) in which the pitch of\neach note (or sequence of tied notes) is represented by its\nMIDI note number and its onset-time and offset-time are\ngiven in milliseconds.\nTemperley’s theory of pitch spelling—or, as he calls\nit, “tonal-pitch-class labeling” (Temperley, 2001, pp. 123–\n132)—consists of the three tonal-pitch-class preference\n1Available online at\n<http://www.link.cs.cmu.edu/music-analysis/ >.rules (TPRs) shown in Figure 4. In Temperley’s theory,\nthetonal pitch class (TPC) of a note is an integer that in-\ndicates the position of the pitch name class of the note\non the line of ﬁfths (Temperley, 2001, pp. 118, 123–125).\nTemperley (2001, p. 125) claims that TPR 1 (see Fig-\nure 4) is “the most important” TPR and that “in many\ncases, this rule is sufﬁcient to ensure the correct spelling\nof passages”. TPR 2 is designed to account for the\nway that notes are typically spelt in chromatic scale seg-\nments (Temperley, 2001, pp. 127–130). TPR 3 states that\nthe system should “prefer TPC representations which re-\nsult in good harmonic representations” (Temperley, 2001,\np. 131). Temperley formally deﬁnes the concept of a\n“good harmonic representation” in the ﬁrst rule of his the-\nory of harmony, HPR 1 (Temperley, 2001, p. 149), which\nstates that, in choosing the roots for chords, certain spec-\niﬁed TPC-root relationships should be preferred over oth-\ners. Temperley’s theories of pitch spelling and harmony\nare therefore interdependent and this is reﬂected in the fact\nthat they are both implemented in the harmony program\ninMelisma .\nThe complexity of Temperley’s pitch spelling algo-\nrithm is increased still further by the fact that his the-\nory of harmony depends on his theory of metrical struc-\nture. For example, the second harmonic preference rule\nstates that the system should “prefer chord spans that start\non strong beats of the meter” (Temperley, 2001, pp. 151,\n359). The harmony program therefore requires as in-\nput both a “note list” and a representation of the metri-\ncal structure of the passage in the form of a “beat list”\nof the type generated by the Melisma meter program.\nConsequently, if one wishes to use Temperley’s theory to\ndetermine the pitch names of the notes in a passage, one\nmust carry out a process which we denote here by “MH”\nin which one ﬁrst uses the meter program to generate a\nbeat list from a note-list and then processes both the note\nlist and the beat list using the harmony program to com-\npute the pitch names of the notes in the passage.\nIn an attempt to take harmonic rhythm into ac-\ncount when computing metrical structure, Temperley and\nSleator also experimented with a “two-pass” method\n(Temperley, 2001, pp. 46–47), in which the meter and\nharmony programs are both run twice, once in a special\n“prechord” mode and then again in “normal” mode. We\ndenote this “two-pass” method by “MH2P”.\nThe output of the meter program depends on tempo.\nTherefore, in the evaluation described here, both MH and\nMH2P were run on six different versions of the test cor-\npus, one in which the music is at a “natural” tempo and\nﬁve other versions in which the tempo is multiplied by 2,\n4,1\n2,1\n4and1\n6. To test the extent to which pitch spelling\ndepends on metrical structure, the half-speed version of\nthe test corpus was run through the meter program and\nthen the beat list generated for each movement was mod-\niﬁed so that every beat had the same strength. These beat\nlists were then used to generate pitch names using the\nharmony program. We denote this procedure by HNM\n(for “Harmony-No-Meter”). Finally, a very simple imple-\nmentation of TPR 1 was run on the dataset to test Temper-\nley’s (2001, p. 125) claim that “in many cases, this rule is\nsufﬁcient to ensure the correct spelling of passages” .\n2833.4 Chew and Chen’s pitch spelling algorithm\nChew and Chen (2003, 2005) describe several variants of a\nreal-time pitch spelling algorithm based on Chew’s (2000)\n“Spiral Array Model” which is a geometric model of tonal\npitch relations. In the spiral array, the pitch name classes\nare arranged on a helix so that adjacent pitch name classes\nwithin this helix are a perfect ﬁfth apart and adjacent pitch\nname classes along the length of the cylinder in which the\nhelix is embedded are a major third apart. Let Sbe a set of\nnotes, let p(n)denote the vector representing the position\nin the spiral array of the pitch name class of the note n, and\nletd(n)be the duration of note n. Chew and Chen (2005,\np. 67) deﬁne the center of effect (CE) of S, denoted by\nCE(S), to be\nCE(S) =/summationtext\nn∈Sd(n)p(n)/summationtext\nn∈Sd(n).\nThat is, the CE of a set of notes is the weighted centroid\nof the position vectors of the pitch name classes of the\nnotes in the spiral array, each note being weighted by its\nduration.\nIn Chew and Chen’s algorithm, it is assumed that the\ninput data gives the MIDI note number, together with the\nonset and duration in milliseconds of each note. The data\nis then divided into equal time slices called chunks (Chew\nand Chen, 2005, p. 67) and the algorithm spells the notes\na chunk at a time. Let’s denote by Wsound(i, j)the set of\nnotes that are sounding in chunks itoj; and let Wstart(i, j)\ndenote the set of notes that start in chunks itoj. Let’s sup-\npose that the algorithm is about to spell the notes in chunk\nj. According to Chew and Chen (2005, pp. 70–71), the al-\ngorithm ﬁrst computes the global CE ,CE global,j, which is\nthe CE of the set of notes in a sliding global context win-\ndowthat consists of the wschunks immediately preceding\nthejth chunk. In other words, the algorithm computes the\nvalue\nCE global,j=CE(Wsound(j−ws, j−1)), (1)\nwhere wsis a parameter of the algorithm. Next, each note\nin chunk jis assigned a pitch name which is as close as\npossible to CE global,jin the spiral array. Then a local CE ,\nCE local,j, is computed which is the CE of the set of notes\nin a local context window consisting of the chunk jthat\nhas just been spelt, together with the (wr−1)chunks im-\nmediately preceding the jth chunk. That is, the algorithm\ncomputes the value of\nCE local,j=CE(Wsound(j−wr+ 1, j)), (2)\nwhere wris another parameter of the algorithm. Next the\nalgorithm computes the cumulative CE ,CE cum,j, which\nis the CE of the notes in a cumulative window consist-\ning of all the chunks preceding the jth chunk. That is, it\ncomputes the value of CE cum,j=CE(Wsound(1, j−1)).\nFinally, the notes in chunk jare re-spelt so that their\npitch names are as close as possible to the hybrid CE ,\nCE hybrid,j=f.CE local,j+ (1 −f).CE cum,j, where fis a\nparameter with a value between 0 and 1 which determines\nthe relative weight given to the local and cumulative CEs.Table 1: The sets of parameter values used to evaluate\nChew and Chen’s algorithm.\nParameter Set of values used\nws{0,4,8,16}\nwr{2,4,6}\nf {0,0.25,0.5,0.75,1}\nr/h/braceleftBig/radicalbig\n2/15,/radicalbig\n15/2/bracerightBig\nChunk size in ms {500,1000,2000}\nWsound orWstart {Wsound, Wstart}\nSpiral array or line of ﬁfths {Spiral array ,Line of ﬁfths }\nRange of permitted pitch name classes {F/flat/flat . . . B\n,F/flat/flat/flat . . . B\n/sharp}\nIn our implementation of Chew and Chen’s algorithm,\nanother parameter is r/h, the aspect ratio of the spiral ar-\nray, where ris the radius of the cylinder in which the helix\nis embedded and his the distance parallel to the axis of the\nhelix corresponding to one step along the spiral array (i.e.,\ntwo pitch name classes a perfect ﬁfth apart). Other param-\neters in our implementation allow the user to: (1) choose\nto use the line of ﬁfths instead of the spiral array; (2) spec-\nify the size of each chunk in milliseconds; (3) specify the\nsegment of the spiral array (or line of ﬁfths) from which\nthe algorithm is permitted to choose pitch names; and (4)\nreplace WsoundwithWstartin Eqs. 1 and 2. Our implemen-\ntation of Chew and Chen’s algorithm was run on the test\ncorpus 1260 times, each time with a different combination\nof parameter values. All possible combinations were used\nof the parameter values shown in Table 1.\n3.5 Meredith’s pitch spelling algorithms\nMeredith (2003, 2005) presents an algorithm called ps13\nwhich takes as input a sequence of ordered pairs, I, in\nwhich each ordered pair /angbracketleftton, n/angbracketrightgives the onset time ton\nand the MIDI note number, n, of a note or sequence of\ntied notes in the passage to be analysed.2The elements\nofIare sorted by increasing onset time and MIDI note\nnumber (with preference given to onset time).\nIfSis an ordered set, let |S|denote the length of S\nand let S[i]denote the (i+ 1)th element of S(e.g., S[0]\nis the ﬁrst element in S). An ordered set of pitch classes,\nC, is ﬁrst derived from Isuch that |C|=|I|andC[i]\nis the pitch class of the note represented by I[i]for all\n0≤i < |I|.ps13 consists of two stages, Stage 1 and\nStage 2 .\nStage 1 consists of the following steps:\n1. For each 0≤i <|C|and each pitch class 0≤p≤\n11, compute a value CNT (i, p)giving the number of\ntimes that poccurs within a context surrounding C[i]\nthat includes C[i],Kprenotes immediately preceding\nC[i]andKpost−1notes immediately following C[i].\nKpreis called the precontext andKpostis called the\npostcontext .\n2. For each 0≤i <|C|and each pitch class 0≤p≤\n11, compute the number of diatonic steps ( mod7 ),\n2ps13 is currently the subject of patent applications in the\nUK (GB0406166.9) and US (US10/821962). Please contact the\nauthor at dave@titanmusic.com if you wish to use the al-\ngorithm. Permission will normally be granted for free use of the\nalgorithm for non-commercial purposes.\n284Figure 5: Examples of the types of passing and neighbour note errors corrected in Stage 2 ofps13.\nD(i, p), that there would be from the tonic to the\npitch name of C[i]ifpwere the pitch class of the\ntonic at the point where C[i]occurs. Assume that\nthe notes are spelt so that they are as close as possi-\nble to the pitch name of pon the line of ﬁfths, with\nevery note 6 semitones away from p(mod 12 ) being\nassigned a pitch name which is an augmented fourth\nabove that of p. In other words, the notes are spelt as\nthey would be in a harmonic chromatic scale whose\ntonic has pitch class p.\n3. For each 0≤i <|C|and each pitch class 0≤p≤\n11, compute the value\nD/prime(i, p) = (D(i, p)−D(0, p)) mod 7 .\nD/prime(i, p)gives the number of diatonic steps ( mod 7 )\nfrom the pitch name of the ﬁrst note (i.e., the note\ncorresponding to C[0]) to the pitch name of C[i]if\nthe tonic pitch class is p.\n4. For each 0≤i < |C|and each diatonic interval\n0≤d≤6, compute the set of tonic pitch classes,\nX(i, d) ={p|D/prime(i, p) =d}.\nX(i, d)contains the tonic pitch classes that would\nlead to the diatonic interval from the ﬁrst note to C[i]\nbeing d.\n5. For each 0≤i <|C|and each diatonic interval 0≤\nd≤6, compute the sum, N(i, d), of the values of\nCNT (i, p)for all the tonic pitch classes p∈X(i, d).\nThat is,\nN(i, d) =/summationdisplay\np∈X(i,d)CNT (i, p).\n6. For each 0≤i <|C|, compute dmax(i), the value\nofdfor which N(i, d)is a maximum.\n7. Assign a letter name to the ﬁrst note, C[0]. This can\nbe done, for example, by using the following table\nC[0] 0 1 2 3 4 5\nLetter name of C[0] C C D E E F\nC[0] 6 7 8 9 10 11\nLetter name of C[0] F G A A B B\n8. For each 0≤i < |C|, make the letter name of\nthe note corresponding to C[i],dmax(i)steps above\nthe letter name assigned to the note corresponding to\nC[0].\nStage 2 of the algorithm corrects those instances in the\noutput of Stage 1 where a neighbour note or passing note\nis erroneously predicted to have the same letter name aseither the note preceding it or the note following it (see\nFigure 5).\nps13 was run on the test corpus used in this study\nwith Kpre= 33 andKpost= 23 , these being the val-\nues that resulted in the highest note accuracy in a pilot\nstudy in which the algorithm was run on the ﬁrst book of\nJ. S. Bach’s Das Wohltemperirte Clavier (Meredith, 2003;\nMeredith, 2005, pp. 182–183).\nA second version of this algorithm, called ps1303 , was\nalso run on the test corpus. ps1303 ﬁrst predicts the pitch\nnames of the notes in the passage using ps13. Then, for\neach note, it determines whether the pitch name predicted\nbyps13 is relatively distant from the pitch names in its\ncontext along the line of ﬁfths. If the pitch name assigned\nto a note is relatively distant from its neighbouring notes\nalong the line of ﬁfths, and it can be made closer to these\nneighbours by transposing it either up or down a dimin-\nished second, then it is transposed by the appropriate in-\nterval.\n4 RESULTS AND DISCUSSION\nTable 2 gives the results obtained for selected versions of\nthe algorithms tested (including the best versions for each\nauthor), sorted by decreasing overall note accuracy. The\nresults in parentheses in this table give the note accura-\ncies obtained when the output for the fourth movement of\nHaydn’s ‘Military’ Symphony was compared with the ver-\nsion in which the sudden enharmonic change was omitted\n(see section 2).\nOf the versions of Longuet-Higgins’s algorithm tested,\nthe original version performed best, spelling 98.21% of\nthe notes correctly when the notes were sorted so that\nthe voices were ‘end-to-end’. Correcting the implementa-\ntion of Longuet-Higgins’s second rule had no effect when\nthe notes in the data were sorted by onset time and MIDI\nnote number, and increased the number of errors when the\nnotes were sorted so that the voices were ‘end-to-end’. All\nversions of Longuet-Higgins’s algorithm made roughly\nhalf as many errors when the voices were end-to-end as\nwhen the notes were sorted by onset time and MIDI note\nnumber. This supports Longuet-Higgins’s prediction that\nthe algorithm would work less well on polyphonic mu-\nsic. Removing the restriction that pitch names must be be-\ntween G /flat/flatand A\n on the line of ﬁfths more than doubled\nthe number of errors made by the algorithms. Finally, the\nstyle dependence of the versions of the algorithm tested\nincreased monotonically with note error rate so that the\nmost accurate version of the algorithm was also the one\nwhose accuracy was least affected by musical style.\nOf the 26 versions of Cambouropoulos’s algorithm\ntested here, the one that performed best was based on\nthe variable-length window version described by Cam-\nbouropoulos (2001). This version achieved a note accu-\nracy of 99.07% and a style dependence of 0.46. In gen-\n285Table 2: Summary of results for selected versions of the algorithms tested. Values in columns 2 to 10 are note accuracies,\nexpressed as percentages. Each value in column headed SDmeasures style dependence and gives the standard deviation\nof the percentages in the same row in columns 2 to 9.\nAlgorithm Bach Beethoven Corelli Handel Haydn Mozart Telemann Vivaldi Complete SD\nps1303 99.92 98.31 99.99 99.71 99.19 99.28 99.69 99.32 99.43 0.54\nps13 99.86 98.27 99.92 99.71 98.90 99.04 99.57 99.21 99.31 0.56\nTemperley-Sleator (MH2P, half-speed) 99.88 96.67 99.98 99.93 86.82 (98.91) 99.31 99.89 99.87 97.79 (99.30) 4.57 (1.13)\nTemperley-Sleator (MH, half-speed) 99.87 96.61 99.98 99.91 86.72 (98.82) 99.33 99.91 99.89 97.78 (99.29) 4.61 (1.16)\nChew-Chen (sounding) 99.29 98.73 99.38 99.44 98.51 99.06 99.39 99.40 99.15 0.35\nChew-Chen (starting) 99.39 98.80 99.44 99.46 98.28 99.10 99.37 99.37 99.15 0.42\nCambouropoulos (optimal) 99.08 99.01 99.44 99.49 98.07 99.36 99.28 99.43 99.15 0.47\nCambouropoulos (2001) 98.85 99.05 99.42 99.48 98.06 99.18 99.30 99.22 99.07 0.46\nTemperley’s TPR 1 alone 99.38 97.83 99.51 99.32 98.25 99.04 99.39 99.57 99.04 0.65\nTemperley-Sleator (HNM, half-speed) 99.71 96.72 99.96 99.87 86.95 (99.04) 99.38 99.88 95.48 97.25 (98.76) 4.49 (1.70)\nLonguet-Higgins (original, voices end-to-end) 95.74 98.64 99.70 99.83 95.09 98.93 99.14 98.62 98.21 1.79\nLonguet-Higgins (original, sorted by onset) 99.53 96.37 99.58 97.22 89.13 84.94 98.29 95.82 95.11 5.28\neral, the versions using a variable-length window achieved\nhigher note accuracies than those using a ﬁxed-length\nwindow of the same size. In the version of the algo-\nrithm described by Cambouropoulos (1996), the penalty\nscore for each window spelling is computed by summing\nthe penalty values only for intervals between contigu-\nous notes. However, in the versions described by Cam-\nbouropoulos (2001, 2003), the penalty score for each win-\ndow is computed by summing the penalty values for all\nthe intervals between both contiguous and non-contiguous\nnotes within the window. Our results showed that ignor-\ning intervals between non-contiguous notes caused both\na relatively large drop in spelling accuracy and a rela-\ntively large increase in style dependence. Nearly all the\nversions of Cambouropoulos’s algorithm achieved higher\nnote accuracies than the best version of Longuet-Higgins’s\nalgorithm and were markedly less dependent on style than\nthose of Longuet-Higgins. Increasing the size of the win-\ndow increased spelling accuracy but also exponentially\nincreased running time. If each window contained more\nthan 12 notes, the algorithm was too slow to be practical.\nThe results were used to estimate an ‘optimal’ combina-\ntion of features which were then implemented in a new\nversion of the algorithm. This version achieved a note ac-\ncuracy of 99.15% and a style dependence of 0.47 on the\ntest corpus used here. However, it took nearly 7 times\nlonger to process the test corpus than the most accurate of\nthe other 26 versions tested.\nIt was found that both the MH and MH2P versions of\nTemperley and Sleator’s algorithm performed very simi-\nlarly and did best when the music in the corpus was at\nhalf speed (see Table 2). Both systems were highly sen-\nsitive to tempo: when the music was at 4 times its nat-\nural tempo, the note accuracy of MH was only 82.74%\nand that of MH2P was 74.58%. However, 70% of the er-\nrors made by MH and MH2P on the half-speed version of\nthe corpus were caused by the sudden enharmonic change\nin the fourth movement of Haydn’s ‘Military’ Symphony\ndiscussed in section 2. When the outputs of MH and\nMH2P were compared with the version of this movement\nwith the enharmonic change omitted, their overall note\naccuracies rose to 99.3%. Ignoring metrical information\nin the HNM procedure caused the overall number of er-\nrors to rise by 24% when the original score of the fourthmovement of Haydn’s ‘Military’ Symphony was used as a\nground truth and by 75% when the modiﬁed version was\nused. However, this fall in note accuracy was primarily\ndue to HNM’s inability to cope with one particular chro-\nmatic passage in bars 85–96 of the third movement of Vi-\nvaldi’s Concerto in G minor (‘L’estate’) from ‘Le Quattro\nStagioni’ (Op. 8, No. 2, RV 315). This suggests that the\nuse of metrical structure in Temperley and Sleator’s sys-\ntem helps it to cope with certain types of chromatic pas-\nsage, which, in turn, makes it less likely to spell whole\nsegments in the wrong key. It was found that a very sim-\nple implementation of TPR 1 alone, without any metrical\ninformation, was able to spell 99.04% of the notes in the\ntest corpus correctly ( SD= 0.65). Also, the performance\nof this version was independent of tempo.\nOf the 1260 versions of Chew and Chen’s algorithm\ntested here, 12 achieved a note accuracy of 99.15%. These\n12 versions of the algorithm were those in which ws=\n8,wr= 2,f= 0.5and the chunk size was set to 500\nmilliseconds. The parameters that were critical for high\nnote accuracy were therefore the duration of the windows\nused and the relative weighting given to local and global\ncontext. Amongst these 12 best versions of the algorithm,\nit did not matter whether the spiral array or the line of\nﬁfths was used, nor did the aspect ratio of the spiral array\nmake any difference. Of these 12 versions, the six that\nconsidered the notes sounding within each window were\nslightly less dependent on style ( SD= 0.35) than those\nthat considered only the notes starting in each window\n(SD= 0.42).\nBoth versions of Meredith’s algorithm achieved higher\nnote accuracies in this study than any of the other al-\ngorithms tested. ps13 spelt 99.31% of the notes in\nthe corpus correctly. This value is only slightly more\nthan that achieved by Temperley and Sleator’s MH and\nMH2P systems when the enharmonic change in the fourth\nmovement of Haydn’s ‘Military’ Symphony was omit-\nted. However, ps13 was considerably less dependent\non style ( SD = 0.56) than the Temperley-Sleator algo-\nrithms ( SD= 1.13,1.16) as well as being independent of\ntempo. ps1303 made even fewer errors than ps13, spelling\n99.43% of the notes in the corpus correctly ( SD= 0.54).\n2865 CONCLUSIONS\nVarious versions of the pitch spelling algorithms\nof Longuet-Higgins, Cambouropoulos, Temperley and\nSleator, Chew and Chen and Meredith were compared by\nrunning them on a test corpus containing 195972 notes,\nequally divided between eight classical and baroque com-\nposers. Meredith’s ps1303 algorithm performed best,\nspelling 99.43% of the notes in the corpus correctly.\nThe next best algorithm was Meredith’s ps13 which spelt\n99.31% of the notes correctly, only slightly more than the\n99.30% spelt correctly by the best version of Temperley\nand Sleator’s algorithm (MH2P). However, MH2P only\nachieved this note accuracy when the music in the cor-\npus was at half-speed and a sudden enharmonic change in\none movement in the corpus was ignored. Furthermore,\nTemperley and Sleator’s algorithms were very sensitive\nto tempo and considerably more dependent on style than\nthose of Meredith which are unaffected by tempo. An\nattempt was made to ﬁnd optimal versions of the Cam-\nbouropoulos and Chew-Chen algorithms. The best ver-\nsions of these algorithms tested spelt 99.15% of the notes\ncorrectly and were slightly less dependent on style than\nMeredith’s algorithms.\nIt would be interesting to compare the algorithms\ntested here with other pitch spelling algorithms described\nin the literature (e.g., that of Stoddard et al., 2004). It\nwould also be useful to devise an appropriate statistical\nmethod for measuring the signiﬁcance of the difference\nbetween two note accuracies achieved on the same test\ncorpus. It would also be worth exploring the extent to\nwhich pitch spelling algorithms can be used to improve\nthe performance of key-ﬁnding algorithms and content-\nbased music information retrieval systems. Finally, it\nwould be interesting to compare the algorithms studied\nhere on a large, high-quality test corpus containing en-\ncodings of post-classical tonal works (e.g., 19th century\nromantic music, jazz). Unfortunately, such a collection of\nencodings does not yet exist in the public domain.\nACKNOWLEDGEMENTS\nThe research reported here was funded by EPSRC grant\nGR/S17253/02.\nREFERENCES\nS. A. Abdallah and M. D. Plumbley. Polyphonic transcrip-\ntion by non-negative sparse coding of power spectra. In\nProceedings of the Fifth International Conference on\nMusic Information Retrieval (ISMIR 2004) , Universitat\nPompeu Fabra, Barcelona, Spain, 2004.\nE. Cambouropoulos, M. Crochemore, C. S. Iliopoulos,\nL. Mouchard, and Y. J. Pinzon. Computing approx-\nimate repetitions in musical sequences. International\nJournal of Computer Mathematics , 79(11):1135–1148,\n2002.\nE. Cambouropoulos. A general pitch interval representa-\ntion: Theory and applications. Journal of New Music\nResearch , 25(3):231–251, 1996.E. Cambouropoulos. Automatic pitch spelling: From\nnumbers to sharps and ﬂats. In VIII Brazilian Sym-\nposium on Computer Music (SBC&M 2001) , Fortaleza,\nBrazil, 2001.\nE. Cambouropoulos. Pitch spelling: A computational\nmodel. Music Perception , 20(4):411–429, 2003.\nE. Chew and Y.-C. Chen. Determining context-deﬁning\nwindows: Pitch spelling using the spiral array. In\nFourth International Conference on Music Information\nRetrieval (ISMIR 2003) (October 26–30) , Baltimore,\nMD., 2003.\nE. Chew and Y.-C. Chen. Real-time pitch spelling using\nthe Spiral Array. Computer Music Journal , 29(2):61–\n76, 2005.\nE. Chew. Towards a Mathematical Model of Tonality . PhD\nthesis, Massachusetts Institute of Technology, Cam-\nbridge, MA., 2000.\nZ. Galil. On improving the worst case running time of the\nBoyer-Moore string matching algorithm. Communica-\ntions of the ACM , 22(9):505–508, 1979.\nD. E. Knuth, J. H. Morris, and V. R. Pratt. Fast pattern\nmatching in strings. SIAM Journal on Computing , 6:\n323–350, 1977.\nC. L. Krumhansl. Cognitive Foundations of Musical Pitch ,\nvolume 17 of Oxford Psychology Series . Oxford Uni-\nversity Press, New York and Oxford, 1990.\nH. C. Longuet-Higgins. The perception of melodies. In\nH. C. Longuet-Higgins, editor, Mental Processes: Stud-\nies in Cognitive Science , pages 105–129. British Psy-\nchological Society/MIT Press, London, England and\nCambridge, Mass., 1987.\nQ. McNemar. Psychological Statistics . John Wiley and\nSons, New York, 4th edition, 1969.\nD. Meredith, K. Lemstr ¨om, and G. A. Wiggins. Algo-\nrithms for discovering repeated patterns in multidimen-\nsional representations of polyphonic music. Journal of\nNew Music Research , 31(4):321–345, 2002.\nD. Meredith. Pitch spelling algorithms. In R. Kopiez,\nA. C. Lehmann, I. Wolther, and C. Wolf, editors, Pro-\nceedings of the Fifth Triennial ESCOM Conference\n(ESCOM5) (8-13 September 2003) , pages pp. 204–207,\nHanover University of Music and Drama, Hanover,\nGermany, 2003.\nD. Meredith. Comparing pitch spelling algorithms on a\nlarge corpus of tonal music. In U. K. Wiil, editor,\nComputer Music Modeling and Retrieval, Second In-\nternational Symposium, CMMR 2004, Esbjerg, Den-\nmark, May 26–29, 2004, Revised Papers , volume 3310\nofLNCS , pages 173–192, Berlin, 2005. Springer.\nJ. Stoddard, C. Raphael, and P. E. Utgoff. Well-tempered\nspelling: A key-invariant pitch spelling algorithm. In\nProceedings of the Fifth International Conference on\nMusic Information Retrieval (ISMIR 2004) (October\n10–14) , Universitat Pompeu Fabra, Barcelona, Spain,\n2004.\nD. Temperley. The Cognition of Basic Musical Structures .\nMIT Press, Cambridge, MA, 2001.\n287"
    },
    {
        "title": "The Mel-Frequency Cepstral Coefficients in the Context of Singer Identification.",
        "author": [
            "Annamaria Mesaros",
            "Jaakko Astola"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417539",
        "url": "https://doi.org/10.5281/zenodo.1417539",
        "ee": "https://zenodo.org/records/1417539/files/MesarosA05.pdf",
        "abstract": "The singing voice is the oldest and most complex musical instrument. A familiar singer’s voice is easily recognizable for humans, even when hearing a song for the first time. On the other hand, for automatic identification this is a difficult task among sound source identification applications. The signal processing techniques aim to extract features that are related to identity characteristics. The research presented in this paper considers 32 Mel-Frequency Cepstral Coefficients in two subsets: the low order MFCCs characterizing the vocal tract resonances and the high order MFCCs related to the glottal wave shape. We explore possibilities to identify and discriminate singers using the two sets. Based on the results we can affirm that both subsets have their contribution in defining the identity of the voice, but the high order subset is more robust to changes in singing style. Keywords: sound source identification, singing voice, MFCC 1",
        "zenodo_id": 1417539,
        "dblp_key": "conf/ismir/MesarosA05",
        "keywords": [
            "singing voice",
            "oldest and most complex musical instrument",
            "familiar singer’s voice",
            "sound source identification",
            "automatic identification",
            "signal processing techniques",
            "features related to identity characteristics",
            "vocal tract resonances",
            "glottal wave shape",
            "robust to changes in singing style"
        ],
        "content": "THE MEL-FREQUENCY CEPSTRAL COEFFICIENTS IN THE CONTEXT\nOF SINGER IDENTIFICATION\nAnnamaria Mesaros1,2\n1Technical University of Cluj Napoca\nCommunications Department\nCluj Napoca, ROMANIA\nannamaria.mesaros@com.utcluj.roJaakko Astola2\n2Institute of Signal Processing\nTampere University of Technology\nTampere, FINLAND\njaakko.astola@tut.fi\nABSTRACT\nThe singing voice is the oldest and most complex musi-\ncal instrument. A familiar singer’s voice is easily rec-\nognizable for humans, even when hearing a song for the\nﬁrst time. On the other hand, for automatic identiﬁca-\ntion this is a difﬁcult task among sound source identi-\nﬁcation applications. The signal processing techniques\naim to extract features that are related to identity char-acteristics. The research presented in this paper considers\n32 Mel-Frequency Cepstral Coefﬁcients in two subsets:\nthe low order MFCCs characterizing the vocal tract res-onances and the high order MFCCs related to the glottal\nwave shape. We explore possibilities to identify and dis-\ncriminate singers using the two sets. Based on the resultswe can afﬁrm that both subsets have their contribution in\ndeﬁning the identity of the voice, but the high order subset\nis more robust to changes in singing style.\nKeywords: sound source identiﬁcation, singing voice,\nMFCC\n1 INTRODUCTION\nConsidering the wide area of signals from our everyday\nlife, problems concerning the singing voice characteriza-tion arise naturally after the interest on speaker and in-\nstrument recognition. Because of its particularities in pro-\nduction and control, the singing voice falls between the\nspeech and musical instruments sounds, having common\ncharacteristics with each of these, but being also very dif-\nferent from both of them. Singing is composed mostlyof sustained vowels with almost perfectly harmonic spec-\ntrum, resembling with the sustained sounds of musical in-\nstruments. In the mean time, the shape of the vocal tract\nthat determines the sounds is a characteristic of human ar-\nticulator system, intensely studied in speech recognitiontasks. Because singers have to sustain vowels as long as\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-mercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonpossible, they learn to develop a control technique over\nthe pronunciation of the vowels (Barnes et al., 2004), thusputting difﬁculties in the use of techniques and models\nfrom speech processing (Youngmoo, 2003).\nThe cepstral coefﬁcients are a set of features reported\nto be robust in some different pattern recognition tasks\nconcerning human voice. They are widely used in speech\nrecognition and also in speaker identiﬁcation. Lately,research on musical instrument identiﬁcation techniques\nproved the cepstral coefﬁcients to be a useful set of fea-\ntures in this task also. The human voice is very welladapted to the ear sensitivity, most of the energy devel-\noped in speech being comprised in the lower frequency\nenergy spectrum, below 4 kHz. In speech recognition\ntasks, usually the ﬁrst 12 coefﬁcients are retained, consid-\nering that they represent the slow variations of the spec-\ntrum of the signal (Rabiner and Juang, 1993), character-\nizing the vocal tract shape, the spectrum of the uttered\nwords.\nAttempts of using the same features in speaker recog-\nnition had proved that also identity features are coded intothe cepstral coefﬁcient representation of a sound. Exper-\niments conducted on different number of speakers, with\nthe use of neural networks in the modeling of categories\nand in the identiﬁcation stage showed satisfactory results\nusing a number of 12-14 coefﬁcients (Seddik et al., 2004;\nFredrickson and Tarassenko, 1995; Mafra and Simoes,\n2004). Cepstral coefﬁcients were also successfully used\nin instrument recognition: the use of 18 cepstral coefﬁ-\ncients derived from a constant Q transform gives a good\ndiscrimination rate between oboe and clarinet (Brown,\n1999), and the combination with temporal features can re-\nsult in good instrument classiﬁcation results (Eronen andKlapuri, 2000).\nIn this paper, a study of Mel-Frequency Cepstral Co-\nefﬁcients is proposed, concerning the identiﬁcation ofsinging voices. In speaker identiﬁcation systems, the low\norder coefﬁcients were used, comprising vocal tract fre-\nquency information. The singing voice has a much largervariability than speech and much higher frequency com-\nponents, starting with pitch, that can be up until 1200 Hz\nin soprano voices. The aim in this study is to determineif it is appropriate to characterize the singing voices us-\ning higher order cepstral coefﬁcients, that are related to\npitch and ﬁne spectral structure rather than to the forman-\ntic structure. We try to determine if the lower or the upper\n610subset of MFCCs encodes more individuality-related in-\nformation.\nThe paper is organized as it follows: ﬁrst we present\na short review of the methods and processing for obtain-\ning the cepstral coefﬁcients, in section 2. Subsection 2.2\npresents some basic concepts about neural networks and\nalso the steps used in implementing, training and test-\ning on different subsets of data. Section 3 will describethe study material, grouping of the data and training of\nthe networks, and ﬁnally section 4 will present the results\nobtained for different network complexities in each case,giving the possibility to generalize the posed problem.\n2 SIGNAL PROCESSING METHODS\nAND TOOLS\n2.1 The Mel-Frequency Cepstral Coefﬁcients\nThe cepstrum of a time domain signal s(n)is the Inverse\nFourier Transform of the log-magnitude spectrum of the\nsignal. The log-magnitude spectrum of a real signal is a\nreal and even function, thus the cepstrum is normally com-puted via Discrete Cosine Transform which is equivalent\nwith the Fourier transform in case of even functions.\nAn important preprocessing step in the analysis of\nspeech signals is the pre-emphasis of high frequencies.\nThis is done because the amount of energy carried in the\nhigh energy components is small compared to low fre-quencies. For the singing voice, the high frequency com-\nponents are all the more important for the perceived qual-\nity. Preemphasis is usually done by ﬁltering the signalwith a FIR ﬁlter whose transfer function in time domain\nis:\ny(n)=x(n)−ax(n−1) (1)\nwhere ais close to 1, with typical values around 0.95.\nThe processing continues with a Fourier analysis of\nthe windowed signal. A Hamming window of 20 ms was\nconsidered. The Mel-frequency scaling is done by a bank\nof triangular band-pass ﬁlters, nonuniformly distributed\nalong the frequency axis. The Mel-scale equivalent value\nfor frequency fexpressed in Hz is:\nmel(f) = 2595 log\n10(1 +f\n700) (2)\nThe MFCCs are computed by redistributing the linearly-\nspaced bins of the log-magnitude FFT into Mel-spaced\nbins according to eq. 2, and applying DCT on the re-distributed spectrum. A relatively small number of co-\nefﬁcients (typically 13) provide a smoothed version of the\nspectral envelope, leading to the isolation of the vocal tractresponse by the simple retention of the desired amount of\ninformation. An additional advantage in using MFCCs is\nthat they have a decorrelating effect on the spectral data,maximizing the variance of the coefﬁcients, similar to the\neffect of Principal Component Analysis. This allows the\nelimination of one of the preprocessing steps in the neu-\nral network training, which is the actual PCA to eliminate\ndata redundancy.2.2 Feed-Forward Neural Networks\nThe simplest architecture of a neural network is the feed-\nforward network, consisting of one or more hidden layers\nthrough which the signal travels one way only, from the\ninput to output. This architecture is extensively used in\npattern recognition because of its basic task of associat-\ning inputs with outputs. Properly trained backpropagationnetworks are able to generalize problems and to handle\nreasonably inputs they have never seen.\nFor improving the generalization of the neural net-\nworks during training and not get to the situation of over-ﬁtting the data, the early stopping method was used. The\ndata set is divided into three subsets: a training set whichwill be used in training, a validation set and a test set. The\nerror on the validation set is monitored during the training\nprocess; when the network begins to overﬁt the data, theerror on the validation set will tend to rise, and the training\nwill be stopped. This leads to a much faster training of the\nnetwork, as long as we take upon the error, which will be\nlarger than the imposed goal.\nTo improve the training of a network, certain prepro-\ncessing techniques can be performed. The one used inthis study is normalization of mean and standard devia-\ntion of the training set so that the training and the target\nsets will have zero mean and unity standard deviation. The\nMFCCs are decorrelated and there is no need to check for\ndata redundancy with PCA. Post-training analysis is used\nto check the performance of the trained networks.\n3 SETTING UP THE EXPERIMENTS\n3.1 The Database\nThe studied material consists in a number of 20 untrained\nvoices. For each voice, there are two common musical\nphrases of medium length 3 seconds and a third different\none of medium length 4 seconds, all sampled at 44100 Hz.\nThe two common phrases were used as training data, andthe third one for testing. It should be noted that while the\nmodels are constructed based on the same utterance, the\nidentiﬁcation uses different phrases for all the subjects.Four groups consisting of ﬁve voices were set up for ini-\ntial experiments concerning identity characterization, and\none group containing 10 voices was used to test the ca-pabilities of neural networks to model the data in case of\nextending the database.\n3.2 The Feature Set\nThe voice signals were pre-emphasized using a FIR ﬁl-\nter as presented in eq. 1, with a=0.95. MFCCs were\ncalculated using the described method, as the DCT of the\nlog-magnitude spectrum with 1024-point FFT. 32 MFCCs\nwere calculated for each frame of the signal. The coefﬁ-\ncients were partitioned for two different situations: coefﬁ-\ncients 1–15 that characterize the smoothed spectrum, and\ncoefﬁcients 15–32 for the ﬁne structure of the spectrum.\nThe two subsets represent the input for training the neu-\nral network. Neural networks were trained also with the\nentire set of cepstral coefﬁcients to check if any improve-\nment is obtained by using all the available information.\n6113.3 The Neural Networks\nFor the groups of ﬁve voices, the neural network was cho-\nsen to have one 20-neurons hidden layer. One of the ﬁve\nneurons in the output layer was assigned to each voice by\ngiving a positive unity answer. The number of neuronsin the hidden layer was increased to 40 for modeling the\ngroup of 10 voices. We chose a training function that uses\na variable learning rate set to 0.09 and with early stopping\nmethod. The validation data for this task was 1/4 of the\nwhole training set. Initial experiments showed that neu-rons with tan-sigmoid transfer function perform much bet-\nter in this recognition task than neurons with log-sigmoid\ntransfer function. The input data was normalized so thatall the coefﬁcients have zero mean and unity variance. Us-\ning each subset of MFCCs, several networks were trained\nto ensure that we obtain the best results.\n4 TRAINING RESULTS AND\nSIMULATIONS\nFor each group of ﬁve voices, a neural network was\ntrained with the two common phrases. The early stoppingmethod implies monitoring the error on the validation set\nduring training. At ﬁrst, the error will decrease, in the\ndata ﬁtting process, but in case the network starts to over-\nﬁt the data, the error will rise and the training will return\nthe weights from the minimum attended error. Usually\nthe training stopped around 0.15 to 0.20 error, depend-\ning on the difﬁculty of modeling the data. The closer the\nvalue is to 1, the better the data ﬁt for the correspondingvoice. Based on the ﬁt values we would expect best re-\nsults in identiﬁcation with the whole set of MFCCs. In\nthe conditions of these results, we test the network withunknown data. The test phrase was processed through the\nsame steps in order to obtain the sets of MFCCs and the\ncoefﬁcients were presented frame by frame as input to the\ntrained network. We emphasize the fact that the test data\nis different for each voice, so in some cases it might re-\nsemble to the training data, while in others it can be very\ndifferent. Table 1 summarizes the percent of correctly la-\nbeled frames and the degree of data ﬁt for one group of\nﬁve voices.\nAlthough the correlation test shows better modeling\nof classes with the entire set of coefﬁcients, it is not al-\nways necessary to use them all. Some voices can be dis-\ntinguished by using the ﬁrst 15 cepstral coefﬁcients, while\nfor others, the information in the upper coefﬁcients gives\nthe difference. In the mean time, using all of the coefﬁ-\ncients in the same classiﬁcation does not always provide a\nmore reliable result.\nFor increasing the number of voices used in the study,\nwe trained a neural network with one 40-neurons hidden\nlayer, using a set of ten voices, in the same conditions.Probability density estimates can be constructed based on\nthe response of each neuron in each frame. The positive\nresponse neuron for one voice should have a PDE withmean close to 1, while the rest of the neurons should\nhave PDEs close to 0. Figures 1-3 illustrate PDEs of\nthe responses of the 10 neurons for the test phrase, esti-\nmated in 100 equidistant points, in one case that cannot\nbe solved using low order MFCCs. The positive response−0.3 −0.2 −0.1 0 0.1 0.2 0.3 0.4 0.5 0.6012345678910PROBABILITY DENSITY ESTIMATES\nFigure 1: Probability distribution estimates of neurons re-\nsponses for the test phrase; modeling with MFCCs 1-15\n−0.3 −0.2 −0.1 0 0.1 0.2 0.3 0.4 0.501234567PROBABILITY DENSITY ESTIMATES\nFigure 2: Probability distribution estimates of neurons re-\nsponses for the test phrase; modeling with MFCCs 15-32\n−0.4 −0.3 −0.2 −0.1 0 0.1 0.2 0.3 0.4 0.5 0.6012345678910PROBABILITY DENSITY ESTIMATES\nFigure 3: Probability distribution estimates of neurons re-\nsponses for the test phrase; modeling with MFCCs 1-32\nneuron is represented by the ’+’ line. The generalization\nof the results state that the upper order cepstral coefﬁcientscontain at least the same quantity of information as the\nlower order ones. The cepstrum decomposes the problem\nin resonance-related information (low-order coefﬁcients)and source-related information (high-order coefﬁcients).\nAs expected, both have their contribution to deﬁning the\nidentity of a voice, in singing, thus the source-related co-\nefﬁcients can be used to characterize the identity of the\nvoice, and seem to behave well to changing the singing\n612Table 1: Correlation coefﬁcient between target and network output for the training set and identiﬁcation percent on the\ntest phrase for one 5-categories experiment\nv01\n v02\n v03\n v04\n v05\nﬁt\n identif\n ﬁt\n identif\n ﬁt\n identif\n ﬁt\n identif\n ﬁt\n identif\ncoeff 1-15\n 0.69\n 0.46\n 0.67\n 0.79\n 0.63\n 0.60\n 0.60\n 0.81\n 0.62\n 0.47\ncoeff 15-32\n 0.68\n 0.53\n 0.59\n 0.63\n 0.65\n 0.64\n 0.61\n 0.64\n 0.56\n 0.47\ncoeff 1-32\n 0.78\n 0.50\n 0.78\n 0.80\n 0.78\n 0.74\n 0.79\n 0.84\n 0.79\n 0.54\nstyle. Compared with the results obtained on speaker\nidentiﬁcation, it can be argued that in speech the ﬁlter part\nof the system does not have such a great variability as insinging, that is why the use of upper coefﬁcients was gen-\nerally not considered.\n5 CONCLUSIONS\nThis paper presented a study of Mel-frequency cepstralcoefﬁcients in the context related to singing voice identi-\nﬁcation. The human articulator system in voicing is mod-\neled in signal processing as a system with a speciﬁc signal\n- the glottal wave - as input to a linear time-invariant ﬁlter- the vocal tract. The low order cepstral coefﬁcients repre-\nsent information about the vocal tract shape, and the high\norder coefﬁcients characterize the source signal. Bothparts contain important information about voice identity.\nIn the case of singing voice, the input of the system is\nmore invariant than the ﬁlter part. Cases difﬁcult to handlewith low-order MFCCs can eventually be solved correctly\nby using the high-order MFCCs. In this study no special\ncare was taken for best trained neural networks; the pur-pose was rough and fast training for testing the selected\nfeatures. For reliable results with neural networks in case\nof working with a large number of classes, parallel net-works are used in order to achieve low complexity, fast\ntraining and small error rates in training each network.\n6 FUTURE WORK\nThe results of the study lead to searching for a different\nway of characterizing the source in the articulator system,independently of the vocal tract parameters. A widely\nused method for estimating the glottal ﬂow is through the\nLiljencrantz-Fant model; the processing involves deter-\nmination of the closed glottis period, for correct inverse\nﬁltering. In singing and in high-pitched voices this is a\nreal problem, because the closed glottis period may be tooshort for correct estimation of the inverse ﬁlter parame-\nters. Also, authors of such studies used the voice signal\nand the simultaneous electroglottograph signal in order tolocate speciﬁc instants in the voice signal. This method is\ninappropriate outside of laboratories, that is why we aim\nfor an equivalent method of describing the glottal wave\ncharacteristics using information extracted only from the\nsignal.References\nJ. Barnes, P. Davis, J. Oates, and J. Chapman. The re-\nlationship between professional operatic soprano voice\nand high range spectral energy. The Journal of the\nAcoustical Society of America , 116(1):530–538, July\n2004.\nJ. C. Brown. Computer identiﬁcation of musical instru-\nments using pattern recognition with cepstral coefﬁ-\ncients as features. The Journal of the Acoustical Society\nof America , 105:1933–1941, 1999.\nA. Eronen and A.; Klapuri. Musical instrument recogni-\ntion using cepstral coefﬁcients and temporal features.IEEE International Conference on Acoustics, Speech,\nand Signal Processing , 2:753–756, June 2000.\nS.E. Fredrickson and L. Tarassenko. Text-independent\nspeaker recognition using neural network techniques.\nFourth International Conference on Artiﬁcial Neural\nNetworks , pages 13–18, June 1995.\nS. Hayakawa and F. Itakura. Text-dependent speaker\nrecognition using the information in the higher fre-\nquency band. IEEE International Conference on Acous-\ntics, Speech, and Signal Processing , 1:137–140, April\n1994.\nA.T. Mafra and M.G. Simoes. Text independent automatic\nspeaker recognition using selforganizing maps. 39th\nIAS Annual Meeting Conference Record of the IndustryApplications Conference , 3:1503–1510, October 2004.\nL. Rabiner and B-H. Juang. Fundamentals of speech\nrecognition . PTR Prentice Hall, Englewood Cliffs, New\nJersey, 1993.\nH. Seddik, A. Rahmouni, and M. Sayadi. Text indepen-\ndent speaker recognition using the mel frequency cep-\nstral coefﬁcients and a neural network classiﬁer. First\nInternational Symposium on Control, Communicationsand Signal Processing , pages 631–634, 2004.\nF. Sun, B. Li, and H. Chi. Some key factors in speaker\nrecognition using neural networks approach. IEEE In-\nternational Joint Conference on Neural Networks ,3 :\n2752–2756, November 1991.\nJ Sundberg. Research on the singing voice in retrospect.\nTMH-QPSR Speech, Music and Hearing , 45:11–22,\n2003.\nE.K. Youngmoo. Singing voice analysis/synthesis . PhD\nthesis, Massachusetts institute of Technology, 2003.\n613"
    },
    {
        "title": "Databionic Visualization of Music Collections According to Perceptual Distance.",
        "author": [
            "Fabian Mörchen",
            "Alfred Ultsch",
            "Mario Nöcker",
            "Christian Stamm"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417967",
        "url": "https://doi.org/10.5281/zenodo.1417967",
        "ee": "https://zenodo.org/records/1417967/files/MorchenUNS05.pdf",
        "abstract": "We describe the MusicMiner system for organizing large collections of music with databionic mining techniques. Low level audio features are extracted from the raw audio data on short time windows during which the sound is assumed to be stationary. Static and temporal statistics were consistently and systematically used for aggregation of low level features to form high level features. A supervised feature selection targeted to model perceptual distance between different sounding music lead to a small set of non-redundant sound features. Clustering and visualization based on these feature vectors can discover emergent structures in collections of music. Visualization based on Emergent Self-Organizing Maps in particular enables the unsupervised discovery of timbrally consistent clusters that may or may not correspond to musical genres and artists. We demonstrate the visualizations capabilities of the U-Map, displaying local sound differences based on the new audio features. An intuitive browsing of large music collections is offered based on the paradigm of topographic maps. The user can navigate the sound space and interact with the maps to play music or show the context of a song. Keywords: audio features, music similarity, perception, clustering, visualization 1",
        "zenodo_id": 1417967,
        "dblp_key": "conf/ismir/MorchenUNS05",
        "keywords": [
            "MusicMiner",
            "databionic mining",
            "low level audio features",
            "stationary sound",
            "static and temporal statistics",
            "supervised feature selection",
            "emergent structures",
            "timbrally consistent clusters",
            "U-Map",
            "topographic maps"
        ],
        "content": "DATABIONIC VISU ALIZA TION OFMUSIC COLLECTIONS\nACCORDING TOPERCEPTU ALDIST ANCE\nFabian M¨orchen AlfredUltsch Mario N¨ocker Christian Stamm\nData Bionics Research Group\nPhilipps-Uni versity Marb urg\n35032 Marb urg,German y\nfabian,ultsch,noeckerm,stammi@informatik.uni-marburg.de\nABSTRA CT\nWedescribe theMusicMiner system fororganizing large\ncollections ofmusic with databionic mining techniques.\nLowlevelaudio features areextracted from therawau-\ndiodata onshort time windo wsduring which thesound\nisassumed tobestationary .Static andtemporal statis-\nticswere consistently andsystematically used foraggre-\ngation oflowlevelfeatures toform high levelfeatures.\nAsupervised feature selection targeted tomodel percep-\ntualdistance between different sounding music lead toa\nsmall setofnon-redundant sound features. Clustering and\nvisualization based onthese feature vectors candisco ver\nemer gent structures incollections ofmusic. Visualization\nbased onEmer gent Self-Or ganizing Maps inparticular en-\nables theunsupervised disco veryoftimbrally consistent\nclusters thatmay ormay notcorrespond tomusical genres\nandartists. Wedemonstrate thevisualizations capabilities\noftheU-Map, displaying local sound differences based\nonthenewaudio features. Anintuiti vebrowsing oflarge\nmusic collections isoffered based ontheparadigm ofto-\npographic maps. The user cannavigatethesound space\nandinteract with themaps toplay music orshowthecon-\ntextofasong.\nKeywords: audio features, music similarity ,perception,\nclustering, visualization\n1INTR ODUCTION\nHumans consider certain types ofmusic assimilar ordis-\nsimilar .Toteach acomputer systems tolearn anddis-\nplay thisperceptual concept ofsimilarity isadif\u0002cult task.\nTherawaudio data ofpolyphonic music isnotsuited for\ndirect analysis with data mining algorithms. High qual-\nityaudio data contains various sound impressions thatare\noverlayed inasingle (orafewcorrelated) time series. In\norder tousemachine learning anddata mining algorithms\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonformusical similarity ,anumerical measure ofperceptual\nmusic similarity isneeded. These time series cannot, how-\never,becompared directly inameaningful way.Acom-\nmon technique istodescribe thesound byextracting audio\nfeatures, e.g. fortheclassi\u0002cation ofmusic intomusical\ngenre categories [1]. Manyfeatures arecommonly ex-\ntracted onshort time windo wsduring which thesound is\nassumed tobestationary .This produces adownsampled\nmulti variate time series ofsound descriptors. These low\nlevelfeatures areaggre gated toform ahigh levelfeature\nvector describing thesound ofasong. Only fewauthors\nhaveincorporated thetemporal structure ofthelowlevel\nfeature time series when summarizing them todescribe\nthemusic [2]. Wegeneralized manyexisting lowlevel\nfeatures andevaluated alargesetoftemporal andnontem-\nporal statistics forthehigh leveldescription ofsound [3].\nThis resulted inahuge setofcandidate sound descriptors.\nWedescribe amathematical method toselect asmall set\nofnon-redundant sound features torepresent perceptual\nsimilarity based onatraining setofmanually labeled mu-\nsic.\nClustering and visualization based onthese feature\nvectors canbeused todisco veremer gent structures in\ncollections ofmusic that correspond totheconcept of\nperceptual similarity .Wedemonstrate theclustering\nandvisualization capabilities ofthenewaudio features\nwith theEmer gent Self-or ganizing Map (ESOM) [4,5].\nThe ESOM belongs thecategory ofdatabionic mining\ntechniques, where information processing techniques are\ntransferred from nature todata processing. The ESOM\nismotivated bytherecepti ve\u0002elds inthehuman brain.\nHigh dimensional data areprojected inaselforganizing\nprocess onto alowdimensional gridanalogous tosensory\ninput inapart ofthebrain. Inorder tovisualize struc-\ntures byemer gence itisveryimportant tousemaps with\nalargeamount ofneurons. Visualization based onU-Map\n[6]displays inparticular enables theunsupervised disco v-\neryoftimbrally consistent clusters thatmay ormay not\ncorrespond tomusical genres andartists. Possible clusters\nshould correspond todifferent sounding music, indepen-\ndently ofwhat genre amusical expert would place itin.\nTheclusters, ifthereareany,canstillcorrespond tosome-\nthing likeagenre oragroup ofsimilar artists. Outliers can\nbeidenti\u0002ed andtransitions between overlapping clusters\nwillbevisible. Both global andlocal structures inmusic\ncollections aresuccessfully detected. The visualizations\n396based ontheparadigm oftopographic maps enables anin-\ntuitivenavigation ofthehigh dimensional feature space.\nFirst some related workisdiscussed inSection 2in\norder tomotivateourapproach. The datasets arebrie\u0003y\ndescribed inSection 3.The method togenerate andse-\nlecttheaudio features wehaveused will bebrie\u0003y ex-\nplained inSection 4,including theresults ofacompari-\nsontoexisting features. Visualization ofmusic collections\nwith U-Map displays ofEmer gent SOMs areexplored in\nSection 5.Results andfuture research isdiscussed inSec-\ntion6.TheMusicMiner softw areimplementing theresult\nofthisresearch isoutlined inSection 7,followed bya\nbrief summary inSection 8.\n2RELA TED WORK\nEarly approaches ofmusical similarity are[7]and[8].\nBoth usealargesetofMel Frequenc yCepstral Coef \u0002-\ncients (MFCC) feature vectors fortherepresentation of\neach song bymixture models. Anarchitecture forlarge\nscale evaluation ofaudio similarity based onthese bag\nofframes methods isdescribed in[9]. The model based\nrepresentation makesdistance calculations between songs\nproblematic. Theycannot easily beused with data mining\nalgorithms requiring thecalculation ofacentroid. Italso\nscales badly with thenumber ofsongs.\nTheseminal workofTzanetakis [1]isthefoundation\nformost research inmusical genre classi\u0002cation. Asingle\nfeature vector isused todescribe asong, opening theprob-\nlemformanystandard machine learning methods. The\nclassi\u0002cation accurac yreported is66%. Misclassi\u0002cation\ne.g.among sub-genres ofjazzareexplained duetosimilar\nsounding pieces. Note, thatwhen using clustering andvi-\nsualization thiswillnotbeaproblem. Ifpieces sound sim-\nilar,theyshould beclose, nomatter which subgenre they\nbelong to.The problem with genre classi\u0002cation isthe\nsubjecti vityandambiguity ofthecategorization used for\ntraining andvalidation [2].Existing genre classi\u0002cations\nfrom popular websites were found tobenotcomparable\nandtheauthors also gaveuponcreating their owngenre\nhierarch y.Classi\u0002cation approaches arecriticized forsu-\npervised learning with fewandarbitrary prior classes. Of-\ntengenre doesn' tevencorrespond tothesound ofthemu-\nsicbuttothetime andplace where themusic came up\northeculture ofthemusicians creating it.Some authors\ntrytoexplain thelowperformance oftheir classi\u0002cation\nmethods bythefuzzy andoverlapping nature ofgenres\n[1].Ananalysis ofmusical similarity showed badcorre-\nspondence with genres, againexplained bytheir inconsis-\ntencyandambiguity [10]. Similar problems arepresent\nforartist similarity [11].Manyartists havecreated mu-\nsicofvarious styles. Apopular example isQueen ,who\nwould generally considered tobeaRock band, butthe\nlong rowofalbums coversawide variety ofgenres. In[2]\nthedataset istherefore chosen tobetimbrally consistent\nirrespecti velyofthegenre.\nRecently ,interest invisualization ofmusic collections\nhasbeen increasing. Some authors consider manual col-\nlaging [12]ofalbums, others visualize thesimilarity of\nartists based ongraph drawing [13]algorithms. Song\nbased visualizations offeramore detailed viewintoamu-siccollection. In[14]disc plots, rectangle plots andtree\nmaps areused todisplay thestructures ofacollection\nde\u0002ned bythemeta information onthesongs likegenre\nandartist. Butthevisualizations donotdisplay similar -\nityofsound, thequality ofthedisplays thus depends on\nthequality ofthemeta data. In[15]FastMap andmulti-\ndimensional scaling areused tocreate a2Dprojection of\ncomple xdescriptions ofsongs including audio features.\nPrincipal component analysis isused in[16] tocompress\nintrinsic sound features to3Ddisplays.\nIn[17]itwasalready demonstrated, that SOMs are\ncapable ofdisplaying music collections. Small maps were\nused, however,resulting inak-Means likeprocedure [18].\nInthese SOMs each neuron istypically interpreted asa\ncluster .The topology preserv ation oftheSOM projec-\ntionisoflittle usewhen using small maps. Fortheemer -\ngence ofhigher levelstructure, alarger,socalled Emer -\ngent SOM (ESOM) [4,19]isneeded. Withlargermaps a\nsingle neuron does notrepresent acluster anymore. Itis\nrather apixelinahigh resolution display oftheprojection\nfrom thehigh dimensional data space tothelowdimen-\nsional map space. Clusters arenowformed byconnected\nregions ofneurons with similar properties. Thestructure\nemer gesfrom thelargescale cooperation ofthousands of\nneurons during theESOM training. Notonly global clus-\nterstructure isvisualized, butalsolocal inner cluster rela-\ntions arepreserv ed.\nTheSmoothed Data Histogram (SDH) visualization of\nSOMs used in[17] represents anindirect estimation ofthe\nhigh dimensional probability density .WeusetheP-Matrix\ntodisplay density information, based onthePareto Den-\nsityEstimation (PDE) [20],amore direct estimator using\ninformation optimal sets. The U*Matrix [21] combines\ndistance anddensity information. Further ,thefeature vec-\ntorsused in[17, 22,10]areveryhigh dimensional. This\nisproblematic fordistance calculations because these vec-\ntorsspaces areinherently empty [23].Finally ,incontrast\nto[17],weusetoroid maps [6]toavoidborder effects.\nOnmaps with atopology limited byborders theprojected\ndata points areoften concentrated ontheborders ofthe\nmap andthecentral region islargely empty .Withtoroid\ntopologies thedata points aredistrib uted onthemap ina\nmore uniform fashion.\nTheextraction ofnon-redundant map viewsfrom tiled\ndisplays [6]ofatoroid ESOM creates theisland-lik edis-\nplays showninSection 5.Note, that incontrast tothe\nIslands ofmusic [17] where severalislands corresponding\ntodensity modes ofthedata space aredisplayed, weonly\ndisplay asingle island representing thecomplete ESOM.\nThestructures inthedata space arevisualized bytheto-\npograph yontheisland de\u0002ned bytheU-Map.\n3DATA\nWehavecreated twodatasets totestthevisualization of\nmusic collections. Ourmotivation forcomposing thedata\nsetswastoavoidgenre classi\u0002cation andcreate clusters of\nsimilar sounding pieces within each group, while achie v-\ninghigh perceptual distances between songs from differ-\nentgroups. Weselected 200 songs in\u0002veperceptually\nconsistent groups (Acoustic ,Classic ,Hiphop ,Metal/Roc k,\n397Electr onic)andwillrefer tothisdataset as5G.There are\npieces from avariety ofsocalled genres ineach group,\ne.g. forAcoustic: Alternati ve(Beck), Blues (John Lee\nHook er),Country (Johnn yCash), Grunge (Stone Temple\nPilots), Rock (Bob Dylan, The Beatles, Lenn yKravitz),\nandevenRap (Beastie Boys). The validation data was\ncreated inasimilar wayasthetraining data. Eight in-\nternally consistent butgroup wise verydifferent sound-\ningpieces totalling 140 songs were compiled: Alterna-\ntiveRock, Stand-up Comedy ,German Hiphop, Electronic,\nJazz, Oldies, Opera, Reggae.This dataset will becalled\n8G.\nAthird dataset istheMusical Audio Benchmark\n(MAB) dataset collected fromwww.garageband.com\nbyMiersw aetal.1.There are7genre groups: Alterna-\ntive,Blues, Electronic, Jazz, Pop, Rap, andRock. This\ndataset waschosen tocheck howwell theperceptual fea-\ntures candistinguish genres andtoprovide values forper-\nformance comparison based onpublically available data.\n4AUDIO FEA TURES\nWebrie\u0003y present ourmethod ofgenerating alargesetof\naudio features andselecting asubset formodelling per-\nceptual distances. The fulldetails aregivenin[3]. The\nrawaudio data wasreduced tomono andasampling fre-\nquenc yof22kHz. Toreduce processing time andavoid\nlead inandlead outeffects, a30ssegment from thecenter\nofeach song wasextracted. Thewindo wsizewas23ms\n(512 samples) with 50% overlap. Thus foreach lowlevel\nfeature, atime series with 2582 time points atasampling\nrateof86Hz wasproduced.\nWeused more than 400lowlevelfeatures, including\ntime series descriptions likevolume orzerocrossings [24]\nandspectral descriptions likespectral bandwidth, rollof f\n[24],slope, andintercept [25]. Manyfeatures were gen-\neralized. TheMel frequenc yscale oftheMFCC wasre-\nplaced with theBark, ERB, andOctavescales tocreate\nBFCC, EFCC, andOFCC, respecti vely.Other lowlevel\nfeatures include chroma strength [26]andtheBark/Sone\nrepresentation of[22]that performs sophisticated psy-\nchoacoustic preprocessing. Asimple psychoacoustic vari-\nantwascreated foralllowlevelspectral features byapply-\ningthePhon weighting totheshort time spectrum prior to\nfurther calculations. Theaggre gation oflowleveltime se-\nriestohigh levelfeatures describing thesound ofasong\nwith oneorafewnumbers wassystematically performed.\nTemporal statistics were consistently applied, disco vering\nthepotential lurking inthebehavior oflowlevelfeatures\novertime. Standard androbustestimates ofthe\u0002rstfour\nmoments were obtained from thetime series andthe\u0002rst\nand second order differences. Features extracted from\ntheautocorrelation function andfrom thespectrum ofthe\nlowleveltime series provided more comple xtemporal\ndescriptions. Moti vated by[25], aggre gations based on\nthereconstructed phase representations [27]oftime series\nwere added. This non-linear analysis offersanalternati ve\nwayofdescribing temporal structure thatiscomplemen-\ntarytotheanalysis oflinear correlation andspectral prop-\n1http://www- ai.cs.uni- dortmund.de/\nFORSCHUNG/mbench2.htmlerties. More than 150 static andtemporal aggre gations\nwere applied toeach lowlevelfeature time series.\nThe cross product ofthelowlevelfeatures andhigh\nlevelaggre gations resulted inahuge setofabout 66.000\nmostly newaudio features. Afeature selection wasnec-\nessary toavoidnoisy andredundant attrib utes andselect\nfeatures thatmodel perceptual distance. Weperformed a\nsupervised selection based ontheperceptually different\nsounding musical pieces inthetraining data. Note, that\ntheaimofachie ving largedistances offeature vectors ex-\ntracted from different sounding music isnotequivalent to\nthatofhaving high classi\u0002cation accurac y.Theability of\nasingle feature toseparate agroup ofmusic from therest\nwasmeasured with anovelscore based onPareto Density\nEstimation (PDE) [20]oftheempirical probability densi-\nties. Figure 1showstheestimated densities forasingle\nfeature andtheElectronic group vs.allother groups. It\ncanbeseen thatthevalues ofthisfeature forsongs from\ntheAcoustic group arelikelytobedifferent from other\nsongs, because there isfewoverlap ofthetwodensities.\nUsing thisfeature asonecomponent ofafeature vector\ndescribing each song willsigni\u0002cantly contrib utetolarge\ndistance oftheElectronic group from therest. This in-\ntuition isformulated astheSepar ation scorecalculated as\noneminus theareaunder theminimum ofboth probability\ndensity estimates.\n0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50123456789\n2nd CC 7th Sone BandLikelihoodElectronic\nDifferent music\nML Decision\nError\nFigure 1:Probability densities forfeature with high sepa-\nration score ofElectronic music vs.different music\nBased onthisscore afeature selection isperformed in-\ncluding acorrelation \u0002lter toavoidredundancies. Togive\nanexample, thebestfeature wasthemean ofthedistances\ninthe2dimensional phase space representation with de-\nlay2ofthesquare root ofthe22nd Bark/Sone loudness.\nThetop20features areused forclustering andvisualiza-\ntion inthenextsection. This feature setshowslowre-\ndundanc yandseparates perceptually different music. It\nalsohasahigh potential forexplaining clusters ofsimilar\nmusic, because each feature hasahigh separation score\nindividually .\nWecompared ourfeature settosevensetsoffeatures\npreviously proposed formusical genre classi\u0002cation or\nclustering: MFCC (mean andstandard deviation ofthe\n\u0002rst20MFCC andthe\u0002rstorder differences) [28],McK-\ninney(modulation energyinfour frequenc ybands forthe\n\u0002rst13MFCC) [29],Tzanetakis [1],Mier swa[25], Spec-\n398Table 1:Distance scores fordifferent feature setsontrain-\ning(5G), validation (8G), andgenre data (MAB)\nFeatur es Datasets\n5G 8G MAB 5G\nMusicMiner 0.41 0.42 0.18\nMFCC 0.16 0.20 0.11\nMcKinne y 0.26 0.30 0.13\nTzanetakis 0.18 0.20 0.10\nMiersw a 0.12 0.16 0.03\nFP 0.10 0.04 0.08\nPH 0.07 0.07 0.02\nSH 0.05 0.09 0.04\ntrum Histo gram(SH), Periodicity Histo grams (PH), and\nFluctuation Patterns (FP) [10]. The comparison ofthe\nfeature setsfortheir ability ofclustering andvisualizing\ndifferent sounding music wasperformed using ameasure\nindependent from theranking scores: theratio oftheme-\ndian ofallinner cluster distances tothemedian ofallpair-\nwise distances, similar to[10]. One minus thisratio is\ncalled thedistance score, listed inTable 1forallfeature\nsets.\nTheMusicMiner features perform best bylargemar-\ngins onallthree datasets. Thebestoftheother feature sets\nisMcKinne y,followed byMFCC andTzanetakis. The\nfactthatMcKinne yisthebest among therest, might be\nduetotheincorporation ofthetemporal behavior ofthe\nMFCC inform ofmodulation energies. The worstper-\nforming feature sets inthis experiment were Spectrum\nHistograms andPeriodicity Histograms. This issurpris-\ning, because SHwasfound tobethebest intheevalu-\nation of[10]. Insummary ,ourfeature setsshowed su-\nperior behavior increating small inner cluster andlarge\nbetween cluster distances inthetraining andvalidation\ndataset. Anydata mining algorithms forvisualization or\nclustering will pro\u0002t from this. The distance scores for\nthegenre data (MAB) were ingeneral much worse than\nforthetwohand selected datasets created byus.Allfea-\nture sets perform much worse than forthetraining and\nvalidation datasets. Thebest score of0.18 isachie vedby\ntheMusicMiner features. Performing thefeature selec-\ntion based onthegroups oftheMAB dataset impro ved\nthedistance score slightly to0.22, buttheperformance of\nthese MAB optimized features scored only 0.27 onthe5G\ndata compared to0.41 bytheMusicMiner features. This\nindicates thatthegenre labeling ofthedatasets probably\ndoes notcorrespond totimbrally consistent groups. We\ncheck edthisassumption bylistening toparts ofthecollec-\ntion. While songs from different genres usually arevery\ndifferent, wealso observ edlargeinconsistencies within\nthegroups. The feature setsMiersw a,PH, andSHper-\nform verypoorly with scores close tozero.\n5VISU ALIZA TION\nEquipped with anumerical description ofsound thatcor-\nresponds toperceptual similarity ,ourgoal wasto\u0002nd a\nvisualization method, that\u0002tstheneeds andconstraints of\nbrowsing amusic collection. A20dimensional space ishard tograsp. Clustering canbeused revealgroups ofsim-\nilarmusic within acollection inanunsupervised process.\nClassi\u0002cation canbeused totrain amodel thatreproduces\nagivencategorization ofmusic onnewdata. Inboth cases\ntheresult willstillbeastrict partition ofmusic inform of\ntextlabels. Projection methods canbeused tovisualize\nthestructures inthehigh dimensional data space andoffer\ntheuser anadditional interf acetoamusic collection apart\nfrom traditional textbased listsandtrees.\nThere aremanymethods that offeratwodimen-\nsional projection w.r.t.some quality measure. Most com-\nmonly used areprincipal component analysis preserving\ntotal variance and multidimensional scaling preserving\ndistances asgood aspossible. Theoutput ofthese meth-\nodsare,however,merely coordinates inatwodimensional\nplane. Unless there areclearly separated clusters ina\ndataset itwill behard torecognize groups, see[3]for\nexamples. Music collections inparticular ,often contain\noverlapping clusters, ifany,which cannotbeclearly sep-\narated. Often there will beclumps ofsimilar music cor-\nresponding toacertain type ofmusic theuser likes.But\nthetransition from onecoherent type ofmusic todiffer-\nentsounding artists will notalwaysbesharp, butrather\nbecharacterized bysmooth transitions. Clear clusters are\nonly tobeexpected ifthere is,e.g. some classical music\ninacollection ofmostly modern music.\nEmer gent SOMs offermore visualization capabilities\nthan simple lowdimensional projections. Inaddition to\nalowdimensional projection preserving thetopology of\ntheinput space, theoriginal high dimensional distances\ncanbevisualized with thecanonical U-Matrix [4]display .\nForeach map position thelocal distances totheimme-\ndiate neighbors areaveraged tocalculate aheight value\nrepresenting thelocal distance relations. Recently ,addi-\ntional methods havebeen developed todisplay theden-\nsityinthehigh dimensional space with theP-Matrix [6].\nDensity information canbeused todisco verareas with\nmanysimilar songs. Allthese visualizations canbeinter-\npreted asheight values ontopoftheusually twodimen-\nsional gridoftheESOM, leading toanintuiti veparadigm\nofalandscape. Withproper coloring, thedata space can\nbedisplayed inform oftopographical maps, intuiti vely\nunderstandable alsobyusers without scienti\u0002c education.\nClearly de\u0002ned borders between clusters, where largedis-\ntances indata space arepresent, arevisualized intheform\nofhigh mountains. Smaller intra cluster distances orbor-\nders ofoverlapping clusters form smaller hills. Homoge-\nneous regions ofdata space areplaced in\u0003atvalleys.To\nremo vetheredundanc ypresent inatiled display ofthe\nU-Matrix, anon-rectangular U-map wascreated [6].\n5.1 TRAINING DATA\nForthe5Gdata setused inthefeature selection method,\nwetrained atoroid 50\u000280ESOM with theMusicMiner\nfeatures using theDatabionics ESOM Tools[19 ]2.Fig-\nure2showstheU-Map. Dark shades represent large\ndistances intheoriginal data space (mountaints), bright\nshades imply similarity w.r.t.theextracted features (val-\nleys).Thesongs from the\u0002vegroups aredepicted bythe\n2http://databionic- esom.sf.net\n399\u0002rstletter ofthegroup name. Inthefollowing paragraphs\nweanalyze theperformance ofthismap.\nInter cluster relations :TheClassical music isplaced\nintheupper right corner .Itiswell separated from the\nother groups. Butattheborder totheAcoustic group,\nneighboring tothelowerleft,themountains range isalit-\ntlelower.This means, thatthere isaslowtransition from\nonegroup totheother .Songs attheborderline will be\nsome what similar totheother group. The Metal group\nisplaced inthecenter part ofthemap. The border to\ntheAcoustic group ismuch more emphasized, thus songs\nfrom these groups differmore than between Acoustic and\nClassic. TheElectronic andHiphop groups resides inthe\nupper andlowerleftparts ofthemap, respecti vely.The\ndistinction ofboth these groups from Metal isagainrather\nstrong. TheElectronic group isclearly recognized asthe\nleast homogeneous one, because thebackground isgen-\nerally much darker.Allother groups haveacentral area\nwith white background, representing high similarity .This\ncanbeseen asthecore ofthegroup with themost typical\npieces. Insummary ,asuccessfully global organization\nofthedifferent styles ofmusic wasachie ved.The pre-\nviously knowngroups ofperceptually different music are\ndisplayed incontiguous regions onthemap andthein-\ntercluster similarity ofthese groups isvisible duetothe\ntopology preserv ation oftheESOM.\nIntra cluster relations :TheESOM/U-Map visualiza-\ntionoffersmore than manyclustering algorithms. Wecan\nalsoinspect therelations ofsongs within avalleyofsimi-\nlarmusic. IntheMetal/Rock region onthemap twovery\nsimilar songs Boys SetsFire-After theEulo gyandAtThe\nDrive In-One Armed Scissor arearranged nexttoeach\nother onaplane (see Figure 3).These twosongs aretyp-\nicalAmerican hard rock songs oftherecent years. They\naresimilar infastdrums, fastguitar ,andloud singing, but\nboth haveslowandquiet parts, too. Thesong Bodycount\n-Bodycount intheHouse isin\u0003uenced bytheHiphop\ngenre. The singing ismore spok enstyle andtherefore\nitisplaced closer totheHiphop area andinamarkable\ndistance totheformer twosongs.\nFigure 3:Detailed viewofmap region showinner cluster\nrelations between Metal/Rock songs\nSuspected outliers :The Electronic group also con-\ntains some outliers, both within areas ofelectronic mu-\nsicaswell asinregions populated byother music. The\nlonely song center ofthemap, surrounded byablack\nmountain ranges isAphr odite -Heat Haze ,theonly Drum\n&Bass song. The Electronic song placed intheClas-\nsical group atthefarright isLeft\u0002eld -Song OfLife.\nNote, thatthissong isn'treally thatfarfrom 'home', be-\ncause ofthetoroid topology oftheESOM. The leftend\nofthemap isimmediately neighboring totheright sideandthetoporiginally connected tothebottom. Thesong\ncontains spheric synthesizer sounds, sounding similar to\nbackground strings with only afewvariations. TheElec-\ntronic song intheAcoustic group isMolok o-HoHumm .\nThesong isarather quiet piece with fewbeats andafe-\nmale singer .Twenty seconds oftheextracted segment\nhappened toconsist only ofsinging andbackground pi-\nano. The twoMetal songs placed between theHiphop\nandtheElectronic group intheupper leftcorner areIn-\ncubus-Rede\u0002ne andFilter -Under .The former hasa\nstrong break beat, synthesizer effects andscratches, more\ntypically found inHiphop pieces. The latter happens to\nhaveseveralperiods ofquietness between theaggressi ve\nrefrains. This probably 'confused' thetemporal feature\nextractors andcreated arather random outcome.\nInsummary ,most ofthesongs presumably placed in\nthewrong regions ofthemap really didsound similar to\ntheir neighbors andwere inawaybadexamples forthe\ngroups weplaced them in.This highlights thedif\u0002cul-\ntiesincreating aground truth formusical similarity ,be\nitgenre ortimbre. Visualization andclustering with U-\nMaps canhelp indetecting outliers andtimbrally consis-\ntentgroups ofmusic inunlabeled datasets.\n5.2 VALID ATION DATA\nForthe8Gvalidation dataset, theU-Map ofatoroid\nESOM trained with theMusicMiner features isshownin\nFigure 4.Eventhough thismusical collection contains\ngroups ofmusic which aresigni\u0002cantly different from\nthose ofourtraining data (e.g. Jazz, Reggae,Oldies), the\nglobal organization ofthedifferent styles works verywell.\nSongs from theknowngroups ofmusic arealmost always\ndisplayed immediately neighboring each other .Again,\ncluster similarity isshownbytheglobal topology .For\nexample Comedy ,placed intheupper left, neighbors the\nHiphop region, probably because both contain alotof\nspok en(German) word. Similar tothe5Gdata, Hiphop\nblends into Electronic, what canbeexplained bysimilar\nbeats. There isatotal of\u0002vesuspected outliers, most\nofwhich canagainbeexplained byanotsowell cate-\ngorization oftheparticular songs onourbehalf. Note,\nthatcontrary toourexpectations, there isnotacomplete\nhigh mountain range around each group ofdifferent mu-\nsic. While there isawallbetween Alternati veRock and\nElectronic, there isalso agateinthelowercenter partof\nthemap where these twogroups blend into oneanother .\nWithreal lifemusic collections thiseffect will beeven\nstronger ,stressing theneed forvisualization thatcandis-\nplay these relations rather than applying strict categoriza-\ntions.\nTogetavisual impression ofthesuperiority ofthe\nMusicMiner features overthefeatures previously used in\nSOM visualizations ofmusic collections, wealso trained\nanESOM onthevalidation data with theSpectrum His-\ntogram features from [10].Figure 5showtheU-Map.\nThe map display ofthiscollection does notshowahigh\ncorrespondence with theperceptually different groups of\nmusic. Thegroups Jazz, Comedy ,andElectronic aredis-\ntributed alloverthemap. Opera andAlternati veRock are\ntheonly groups where thesongs some what stick together .\n400Figure 2:U-Map ofthe5Gdata andtheMusicMiner features with successful global organization ofknowngroups\nM=Metal/Rock, A=Acoustic, C=Classical, H=HipHop, E=Electronic\nFigure 4:U-Map ofthe8Gvalidation data andtheMusicMiner features\nA=Alternati veRock, O=Opera, G=Oldies, J=Jazz, E=Electronic, H=Hiphop, C=Comedy ,R=Reggae\nHigh mountain ranges appear around manyoftheElec-\ntronic songs. This indicates thattheyareextreme outliers\ntothesurrounding songs w.r.t.thefeature setused. They\narethus recognized tobedifferent from thesurrounding\nmusic, butthemap failstogroup them together according\ntoourperception ofsimilarity .\n6DISCUSSION\nClustering andvisualization ofmusic collections with the\nperceptually motivated MusicMiner features workedsuc-\ncessfully onthetraining data andthevalidation data. The\nvisualization based ontopographical maps enables end\nusers tonavigatethehigh dimensional space ofsound de-scriptors inanintuiti veway.Theglobal organization ofa\nmusic collection worked,timbrally consistent groups are\noften shownasvalleyssurrounded bymountains. Incon-\ntrast tothestrict notion ofgenre categories, softtransi-\ntionbetween groups ofsome what similar sounding music\ncanbeseen. Most songs inthetraining data that were\nnotplaced close totheother songs oftheir timbre groups\nturned outtobetimbrally inconsistent after all.\nIncomparison totheIslands ofMusic [17], the\u0002rst\nSOM visualization ofmusic collection, wehaveused less\nbutmore powerful features, largermaps forahigher reso-\nlution viewofthedata space, toroid topologies toavoid\nborder effects, and distance based visualizations. The\nSpectrum Histogram features didnotshowgood cluster -\n401Figure 5:Map ofthe8Gvalidation dataset andtheSpectrum Histogram features\nA=Alternati veRock, O=Opera, G=Oldies, J=Jazz, E=Electronic, H=Hiphop, C=Comedy ,R=Reggae\ningandvisualization performance.\nThe MusicMiner audio features aresurely some what\nbiased towards thetraining data wehaveused forthese-\nlection offeatures. Also, theperceptual ground truth we\nused isofcourse inawaysubjecti ve.Butatthissmall\nscale wehavesucceeded atcreating features thatmodel\nhuman perception ofthesound, notonly onthetraining\ndata butalso ondifferent music. The results ofthisre-\nsearch should notbeinterpreted asthebestaudio features\never,butrather asamethodology thatcanberepeated with\ndifferent candidate features, different training data sets,\nandperceptual ground truth agreed upon bymore people.\nPerforming listening tests with theMAB dataset might be\nawaytocreate apublically available dataset including\ntimbre ground truth information.\nThedatasets weused were necessarily small, because\naground truth ontimbre similarity wasneeded. Themeth-\nodsitself scales uptomuch largercollections, however.\nTheESOM training islinear inthenumber ofsongs and\nthenumber ofneurons, theU-Matrix calculation islinear\ninthenumber ofneurons andconstant w.r.t.thecollection\nsize. Note, thatthisisnottruefordensity based visualiza-\ntions likeSDH.\n7MUSICMINER\nInorder tomaketheresults ofourresearch available to\nmusic fanswestarted theMusicMiner3project. Thegoal\nistoenable users toextract features fortimbre discrimina-\ntionfrom their personal music collections. Thesoftw are\ncanbeused tocreate maps ofaplaylist orthewhole music\ncollection with afewmouse clicks. Theaudio features are\nextracted andatoroid ESOM istrained tocreate amap of\nthepersonal sound space. TheESOMs arevisualized with\nU-Matrix andU-Map displays inform ofatopographic\nmap with small dots forthesongs. The user may inter-\nactwith themap indifferent ways. Songs canbeplayed\n3http://musicminer.sf.netdirectly offthemap. Artist andgenre information canbe\ndisplayed asacoloring ofthesongs. Newmusic cate-\ngories canbecreated byselecting regions onthemap with\nthemouse. Playlists canbecreated from regions andpaths\nonthemap. Newsongs canbeautomatically placed onex-\nisting maps according totheir similarity togivetheuser a\nvisual hintoftheir sound. Theinnovativemap viewsare\ncomplemented bytraditional treeandlistviewsofsongs\ntodisplay andeditthemeta information. TheMusicMiner\nisbased ontheDatabionics ESOM Tools fortraining and\nvisualization ofthemaps andtheYale[30 ]4softw arefor\ntheextraction ofaudio features. Allrelevantdata isstored\ninanSQL database. Thesoftw areiswritten inJavaandis\nfreely available under theGNU Public Licence (GPL)5.\n8SUMMAR Y\nWedescribed theMusicMiner method forclustering and\nvisualization ofmusic collections. Alargescale evalua-\ntionlead tofeatures thatcapture thenotion ofperceptual\nsound similarity .Clustering andvisualization based on\nthese features with theU-Map offersanadded value com-\npared toother lowdimensional projections thatisparticu-\nlarly useful formusic datawith noorfewclearly separated\nclusters. Thedisplays inform oftopographical maps of-\nferanintuiti vewaytonavigatethecomple xsound space.\nTheresults ofthestudy areputtouseintheMusicMiner\nsoftw arefortheorganization andexploration ofpersonal\nmusic collections.\nACKNO WLEDGEMENTS The authors would like\ntothank Ingo L¨ohken,Michael Thies, NikoEfthymiou,\nandMartin K¨ummerer forfruitful discussion onthisre-\nsearch andforthedevelopment oftheMusicMiner .\n4http://yale.sf.net\n5http://www.gnu.org/licenses/gpl.html\n402REFERENCES\n[1]G.Tzanetakis andP.Cook. Musical genre classi\u0002ca-\ntionofaudio signals. IEEE Transactions onSpeec h\nandAudio Processing ,10(5), 2002.\n[2]J.J.Aucouturier andF.Pachet. Representing musical\ngenre: astate ofart.JNMR ,31(1), 2003.\n[3]F.M¨orchen, A.Ultsch, Michael Thies, Ingo L¨ohken,\nMario N¨ocker,Christian Stamm, NikoEfthymiou,\nandMartin K¨ummerer .MusicMiner: Visualizing\ntimbre distances ofmusic astopograpical maps.\nTechnical Report 47, CSDepartment, Philipps-\nUniversity Marb urg,German y,2005.\n[4]A.Ultsch. Self-or ganizing neural netw orks forvi-\nsualization andclassi\u0002cation. InProc.GfKl, Dort-\nmund, Germany ,1992.\n[5]T.Kohonen. Self-Or ganizing Maps .Springer ,1995.\n[6]A.Ultsch. Maps fortheVisualization ofhigh di-\nmensional Data Spaces. InProc.WSOM, Hibikino,\nJapan ,2003.\n[7]B.LoganandA.Salomon. Amusic similarity func-\ntion based onsignal analysis. InIEEE Interna-\ntional Confer ence onMultimedia and Expo ,page\n190, 2001.\n[8]J.-J. Aucouturier andF.Pachet. Finding songs that\nsound thesame. InProc.ofIEEE Benelux Workshop\nonModel based Processing and Coding ofAudio,\n2002.\n[9]Jean-Julien Aucouturier andFrancois Pachet. Tools\nandarchitecture fortheevaluation ofsimilarity mea-\nsures: case study oftimbre similarity .InProc.IS-\nMIR,2004.\n[10] E.Pampalk, S.Dixon, andG.Widmer .Ontheeval-\nuation ofperceptual similarity measures formusic.\nInProc.DAFX,2003.\n[11] D.Ellis, B.Whitman, A.Berenzweig, and\nS.Lawrence. Thequest forground truth inmusical\nartist similarity .InProc.ISMIR ,2002.\n[12] David Bainbridge, Sally JoCunningham, and\nJ.Stephen Downie. Visual collaging ofmusic ina\ndigital library .InProc.ISMIR ,2004.\n[13] Fabio Vignoli, Rob vanGulik, andHuub vandeWe-\ntering. Mapping music inthepalm ofyour hand, ex-\nplore anddisco veryour collection. InProc.ISMIR ,\n2004.\n[14] Marc Torrens, Patrick Hertzog, andJosep Lluis Ar-\ncos. Visualizing andexploring personal music li-\nbraries. InProc.ISMIR ,2004.\n[15] P.Cano, M.Kaltenbrunner ,F.Gouyon, andE.Bat-\ntle. Ontheuseoffastmap foraudio retrie valand\nbrowsing. InProc.ISMIR ,2002.[16] G.Tzanetakis, A.Ermolinsk yi,andP.Cook. Be-\nyond thequery-by-e xample paradigm: Newquery\ninterf aces formusic. InProc.Int.Computer Music\nConfer ence,2002.\n[17] E.Pampalk, A.Rauber ,andD.Merkl. Content-\nbased organization and visualization ofmusic\narchi ves.InProc.ACMMultimedia ,pages 570579.\nACM, 2002.\n[18] A.Ultsch. Self organizing neural netw orks per-\nform different from statistical k-means clustering. In\nProc.GfKl, Basel, Suisse ,1995.\n[19] A.Ultsch and F.M¨orchen. ESOM-Maps: tools\nforclustering, visualization, andclassi\u0002cation with\nEmer gent SOM. Technical Report 46,CSDepart-\nment, Philipps-Uni versity Marb urg,German y,2005.\n[20] A.Ultsch. Pareto Density Estimation: Probability\nDensity Estimation forKnowledge Disco very.In\nProc.GfKl, Cottb us,Germany ,2003.\n[21] A.Ultsch. U*-Matrix: ATooltovisualize Clus-\ntersinhigh dimensional Data. Technical Report 36,\nCSDepartment, Philipps-Uni versity Marb urg,Ger-\nmany,2004.\n[22] E.Pampalk, S.Dixon, andG.Widmer .Exploring\nmusic collections bybrowsing different views. In\nProc.ISMIR ,2003.\n[23] Charu C.Aggarwal,Alexander Hinneb urg,and\nDaniel A.Keim. Onthesurprising behavior ofdis-\ntance metrics inhigh dimensional space. InProc.In-\nternational Confer ence onDatabase Theory ,2001.\n[24] D.Li,I.K.Sethi, N.Dimitro va,andT.McGee. Clas-\nsi\u0002cation ofgeneral audio data forcontent-based re-\ntrieval. Pattern Reco gnition Letter s,22:533544,\n2001.\n[25] I.Miersw aandK.Morik. Automatic feature extrac-\ntion forclassifying audio data. Machine Learning\nJournal ,58:127149, 2005.\n[26] M.Goto. Achorus-section detecting method formu-\nsical audio signals. InProc.ICASSP ,pages 437\n440, 2003.\n[27] F.Takens.Detecting strange attractors inturbulence.\nInD.A. Rand andL.S. Young, editors, Dynamical\nsystems andturbulences ,pages 366381. Springer ,\n1981.\n[28] A.Berenzweig, D.Ellis, andS.Lawrence. Anchor\nspace forclassi\u0002cation andsimilarity measurement\nofmusic. InProc.ICME ,pages I2932, 2003.\n[29] M.F.McKinne yandJ.Breebaart. Features foraudio\nandmusic classi\u0002cation. InProc.ISMIR ,2003.\n[30] OliverRitthof f,Ralf Klink enber g,Simon Fischer ,\nIngo Miersw a,andSvenFelsk e.Yale: Yetanother\nmachine learning environment. InLLWA01,Dort-\nmund, Germany ,pages 8492, 2001.\n403"
    },
    {
        "title": "Audio Matching via Chroma-Based Statistical Features.",
        "author": [
            "Meinard Müller",
            "Frank Kurth",
            "Michael Clausen"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416800",
        "url": "https://doi.org/10.5281/zenodo.1416800",
        "ee": "https://zenodo.org/records/1416800/files/MuellerKC05.pdf",
        "abstract": "In this paper, we describe an efficient method for audio matching which performs effectively for a wide range of classical music. The basic goal of audio matching can be described as follows: consider an audio database containing several CD recordings for one and the same piece of music interpreted by various musicians. Then, given a short query audio clip of one interpretation, the goal is to automatically retrieve the corresponding excerpts from the other interpretations. To solve this problem, we introduce a new type of chroma-based audio feature that strongly correlates to the harmonic progression of the audio signal. Our feature shows a high degree of robustness to variations in parameters such as dynamics, timbre, articulation, and local tempo deviations. As another contribution, we describe a robust matching procedure, which allows to handle global tempo variations. Finally, we give a detailed account on our experiments, which have been carried out on a database of more than 110 hours of audio comprising a wide range of classical music. Keywords: audio matching, chroma feature, music identification 1",
        "zenodo_id": 1416800,
        "dblp_key": "conf/ismir/MuellerKC05",
        "keywords": [
            "audio matching",
            "audio database",
            "classical music",
            "chroma-based audio feature",
            "harmonic progression",
            "robustness",
            "robust matching procedure",
            "global tempo variations",
            "experiments",
            "database"
        ],
        "content": "AUDIO MATCHING VIA CHROMA-BASED STATISTICAL FEATURES\nMeinard M ¨uller Frank Kurth Michael Clausen\nUniversit ¨at Bonn, Institut f ¨ur Informatik III\nR¨omerstr. 164, D-53117 Bonn, Germany\n{meinard, frank, clausen }@cs.uni-bonn.de\nABSTRACT\nIn this paper, we describe an efﬁcient method for audio\nmatching which performs effectively for a wide range of\nclassical music. The basic goal of audio matching can\nbe described as follows: consider an audio database con-\ntaining several CD recordings for one and the same piece\nof music interpreted by various musicians. Then, given\na short query audio clip of one interpretation, the goal is\nto automatically retrieve the corresponding excerpts from\nthe other interpretations. To solve this problem, we in-\ntroduce a new type of chroma-based audio feature that\nstrongly correlates to the harmonic progression of the au-\ndio signal. Our feature shows a high degree of robustness\nto variations in parameters such as dynamics, timbre, ar-\nticulation, and local tempo deviations. As another contri-\nbution, we describe a robust matching procedure, which\nallows to handle global tempo variations. Finally, we give\na detailed account on our experiments, which have been\ncarried out on a database of more than 110hours of audio\ncomprising a wide range of classical music.\nKeywords: audio matching, chroma feature, music\nidentiﬁcation\n1 INTRODUCTION\nContent-based document analysis and retrieval for mu-\nsic data has been a challenging research ﬁeld for many\nyears now. In the retrieval context, the query-by-example\nparadigm has attracted a large amount of attention: given\na query in form of a music excerpt, the task is to automat-\nically retrieve all excerpts from the database containing\nparts or aspects similar to the query. This problem is par-\nticularly difﬁcult for digital waveform-based audio data\nsuch as CD recordings. Due to the complexity of such\ndata, the notion of “similarity” used to compare different\naudio clips is a delicate issue and largely depends on the\nrespective application as well as the user requirements.\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of LondonIn this paper, we consider the subproblem of audio\nmatching . Here the goal is to retrieve all audio clips from\nthe database that in some sense represent the same musical\ncontent as the query clip. This is typically the case when\nthe same piece of music is available in several interpre-\ntations and arrangements. For example, given a twenty-\nsecond excerpt of Bernstein’s interpretation of the theme\nof Beethoven’s Fifth, the goal is to ﬁnd all other corre-\nsponding audio clips in the database; this includes the rep-\netition in the exposition or in the recapitulation within th e\nsame interpretation as well as the corresponding excerpts\nin all recordings of the same piece interpreted by other\nconductors such as Karajan or Sawallisch. It is even more\nchallenging to also include arrangements such as Liszt’s\npiano transcription of Beethoven’s Fifth or a synthesized\nversion of a corresponding MIDI ﬁle. Obviously, the de-\ngree of difﬁculty increases with the degree of variations\none wants to permit in the audio matching.\nA straightforward, general strategy for audio match-\ning works as follows: ﬁrst convert the query as well as\nthe audio ﬁles of the database into sequences of suitable\naudio features. Then compare the feature sequence ob-\ntained from the query with feature subsequences obtained\nfrom the audio ﬁles by means of some suitably deﬁned\ndistance measure. To implement such a procedure, one\nhas to account for the following fundamental questions.\nWhich kind of music is to be considered? What is the un-\nderlying notion of similarity to be used in the audio match-\ning? How can this notion of similarity be incorporated in\nthe features and the distance measure? What are typical\nquery lengths? Furthermore, in view of large data sets, the\nquestion of efﬁciency also is of fundamental importance.\nOur approach to audio matching follows these lines\nand works for Western tonal music based on the 12 pitch\nclasses also known as chroma . Given a query clip between\n10 and 30 seconds of length, the goal in our retrieval sce-\nnario is to ﬁnd all corresponding audio clips regardless\nof the speciﬁc interpretation and instrumentation as de-\nscribed in the above Beethoven example. In other words,\nthe retrieval process has to be robust to changes of parame-\nters such as timbre, dynamics, articulation, and tempo. To\nthis end, we introduce a new kind of audio feature consid-\nering short-time statistics over chroma-based energy dis-\ntributions (see Sect. 3). It turns out that such features are\ncapable of absorbing variations in the aforementioned pa-\nrameters but are still valuable to distinguish musically un -\n288related audio clips. The crucial point is that incorporatin g\na large degree of robustness into the audio features allows\nus to use a relatively rigid distance measure to compare\nthe resulting feature sequences. This leads to robust as\nwell as efﬁcient matching algorithms, see Sect. 4. There,\nwe also explain how to handle global tempo variations\nby independently processing suitable modiﬁcations of the\nquery clip. We evaluated our matching procedure on a\ndatabase containing more than 110 hours of audio mate-\nrial, which consists of a wide range of classical music and\nincludes complex orchestral and vocal works. In Sect. 5,\nwe will report on our experimental results. Further mate-\nrial and audio examples can be found at www-mmdb.iai.\nuni-bonn.de/projects/audiomatching . In Sect. 2,\nwe give a brief overview of related work and conclude in\nSect. 6 with some comments on future work and possible\nextensions of the audio matching scenario.\n2 RELATED WORK\nThe problem of audio matching can be regarded as an ex-\ntension of the audio identiﬁcation problem. Here, a query\ntypically consists of short audio fragment obtained from\nsome unknown audio recording. Then the goal is to iden-\ntify the original recording contained in a given large au-\ndio database. Furthermore, the exact position of the query\nwithin this recording is to be speciﬁed. The identiﬁcation\nproblem can be regarded as a largely solved problem, even\nin the presence of noise and slight temporal distortions of\nthe query, see, e.g., Allamanche et al. (2001); Kurth et al.\n(2002); Wang (2003) and the references therein. Current\nidentiﬁcation systems, however, are not suitable for a less\nstrict notion of similarity.\nIn the related problem of music synchronization ,\nwhich is sometimes also referred to as audio matching,\none major goal is to align audio recordings of music to\nsymbolic score or MIDI information. One possible ap-\nproach, as suggested by Turetsky and Ellis (2003) or Hu\net al. (2003), is to solve the problem in the audio do-\nmain by converting the score or MIDI information into\na sequence of acoustic features (e.g., spectral, chroma or\nMFCC vectors). By means of dynamic time warping, this\nsequence is then compared with the corresponding feature\nsequence extracted from the audio version. Note that the\nobjective of our audio matching scenario is beyond the\none of audio synchronization: in the latter case the goal\nis to time-align two given versions of the same underlying\npiece of music, whereas in the audio matching scenario\nthe goal is to identify short audio fragments similar to the\nquery hidden in the database.\nThe design of audio features that are robust to varia-\ntions of speciﬁc parameters is of fundamental importance\nto most content-based audio analysis applications. Among\na large number of publications, we quote two papers rep-\nresenting different strategies, which will be applied in ou r\nfeature design. The chroma-based approach as suggested\nby Bartsch and Wakeﬁeld (2005) represents the spectral\nenergy contained in each of the 12traditional pitch classes\nof the equal-tempered scale. Such features strongly cor-\nrelate to the harmonic progression of the audio, which\nare often prominent in Western music. Another generalstrategy is to consider certain statistics such as pitch his-\ntograms for audio signals, which may sufﬁce to distin-\nguish different music genre, see, e.g., Tzanetakis et al.\n(2002). We will combine aspects of these two approaches\nin evaluating chroma-based audio features by means of\nshort-time statistics.\n3 AUDIO FEATURES\nIn this section, we give a detailed account on the design\nof audio features, possessing a high degree of robustness\nto variations of parameters such as timbre, dynamics, ar-\nticulation, and local tempo deviations as well as to slight\nvariations in note groups such as trills or grace notes. Cor-\nrelating strongly to the harmonics information contained\nin the audio signals, the features are well suited for our au-\ndio matching scenario. In the feature design, we proceed\nin two-stages: in the ﬁrst stage, we use a small analysis\nwindow to investigate how the signal’s energy locally dis-\ntributes among the 12 chroma classes (Sect. 3.1). In the\nsecond stage, we use a much larger (concerning the ac-\ntual time span measured in seconds) statistics window to\ncompute thresholded short-time statistics over these en-\nergy distributions (Sect. 3.2). In Sect. 3.3, we then discus s\nthe qualities as well as drawbacks of the resulting features .\n3.1 Chroma Feature\nThe local chroma energy distributions (ﬁrst stage) are\ncomputed as follows.\n(1) Decompose the audio signal into 88frequency bands\ncorresponding to the musical notes A0 to C8 (MIDI\npitches p= 21 top= 108 ). To properly separate ad-\njacent notes, we use a ﬁlter bank consisting of elliptic\nﬁlters with excellent cut-off properties as well as the\nforward-backward ﬁltering strategy as described by\nM¨uller et al. (2004).\n(2) Compute the short-time mean-square power\n(STMSP) for each of the 88 subbands by convolving\nthe squared subband signals with a rectangular\nwindow corresponding to 200 ms with an overlap of\nhalf the size.\n(3) Compute STMSPs of all chroma classes by adding\nup the corresponding STMSPs of all pitches belong-\ning to the respective class. For example, to compute\nthe STMSP of the chroma class A, add up the STM-\nSPs of the pitches A0,A1, ...,A7. This yields a real\n12-dimensional vector /vector v= (v1,... ,v 12)∈R12for\neach analysis window.\n(4) Finally, for each window compute the energy distri-\nbution relative to the 12 chroma classes by replacing\nthe vectors /vector vfrom Step (3) by /vector v/(/summationtext12\ni=1vi).\nAltogether, the audio signal is converted into a se-\nquence of 12-dimensional chroma distribution vectors—\n10vectors per second, each vector corresponding to 200\nms. For the Beethoven example, the resulting 12curves\nare shown in Fig. 1. To suppress random-like energy dis-\ntributions occurring during passages of extremely low en-\nergy, (e.g., passages of silence before the actual start of\nthe recording or during long pauses), we assign an equally\ndistributed chroma energy to these passages.\n28901C \n01C#\n01D \n01D#\n01E \n01F \n01F#\n01G \n01G#\n01A \n01A#\n0 2 4 6 8 10 12 14 16 18 2001B \nFigure 1: The ﬁrst 21seconds (ﬁrst 20measures) of Bern-\nstein’s interpretation of Beethoven’s Fifth Symphony. The\nlight curves represent the local chroma energy distribu-\ntions (10 features per second). The dark bars represent the\nCENS features (1 feature per second).\n3.2 Short-time statistics\nIn view of our audio matching application, the local\nchroma energy distribution features are still too sensitiv e,\nparticularly when looking at variations in the articulatio n\nand local tempo deviations. Therefore, we introduce a\nsecond, much larger statistics window and consider suit-\nable statistics concerning the energy distributions over t his\nwindow. The details of the second stage are as follows:\n(5) Quantize each normalized chroma vector /vector v=\n(v1,... ,v 12)from Step (4) by assigning the value 4\nif a chroma component viexceeds the value 0.4(i.e.,\nif it contains more than 40percent of the signal’s to-\ntal energy in the ith chroma component for the re-\nspective analysis window). Similarly, we assign the\nvalue 3if0.2≤vi<0.4, the value 2if0.1≤\nvi<0.2, the value 1if0.05≤vi<0.1, and the\nvalue 0otherwise. For example, the chroma vector\n/vector v= (0.02,0.5,0.3,0.07,1.1,0,... , 0)is thus trans-\nformed into the vector /vector vq:= (0,4,3,1,2,0,... , 0).\n(6) Convolve the sequence of the quantized chroma vec-\ntors from Step (5) component-wise using a Hann\nwindow of length 41. This again results in a sequence\nof12-dimensional vectors with non-negative entries,\nrepresenting a kind of weighted statistics of the en-\nergy distribution over a window of 41consecutive\nchroma vectors. In a last step, downsample the se-\nquence by a factor of 10and normalize the vectors\nwith respect to the Euclidean norm.\nThus, after Step (6) we obtain one vector per second,\neach spanning roughly 4100 ms of audio. For short,\nthese features are simply referred to as CENS features\n(Chroma Energy distribution Normalized Statistics),\nwhich are elements of the set Fof vectors deﬁned by\nF:=/braceleftbig\n/vector x= (x1,... ,x 12)∈R12|xi≥0,/summationtext12\ni=1x2\ni= 1/bracerightbig\n.\nFig. 1 shows the resulting sequence of CENS features for\nour running example.01C \n01C#\n01D \n01D#\n01E \n01F \n01F#\n01G \n01G#\n01A \n01A#\n0 2 4 6 8 10 12 14 16 18 2001B \nFigure 2: CENS features for the ﬁrst 21seconds of Sawal-\nlisch’s recording corresponding to the same measures as\nthe Beethoven example of Fig. 1.\n3.3 Discussion of CENS features\nAs mentioned above, the CENS feature sequences corre-\nlate closely with the smoothed harmonic progression of\nthe underlying audio signal. Such sequences, as illustrate d\nby Fig. 1 and Fig. 2, often characterize a piece of music\naccurately but independently of the speciﬁc interpretatio n.\nOther parameters, however, such as dynamics, timbre, or\narticulation are masked out to a large extent: the normal-\nization in Step (4) makes the CENS features invariant to\ndynamic variations. Furthermore, using chroma instead of\npitches (see Step (3)) not only takes into account the close\noctave relationship in both melody and harmony as typical\nfor Western music (see Bartsch and Wakeﬁeld (2005)), but\nalso introduces a high degree of robustness to variations\nin timbre. Then, applying energy thresholds (see Step (5))\nmakes the CENS features insensitive to noise components\nas may arise during note attacks. Finally, taking statistic s\nover relatively large windows not only smoothes out local\ntime deviations as may occur for articulatory reasons but\nalso compensates for different realizations of note groups\nsuch as trills or arpeggios.\nA major problem with the feature design is to satisfy\ntwo conﬂicting goals: robustness on the one hand and ac-\ncuracy on the other hand. Our two-stage approach admits\na high degree of ﬂexibility in the feature design to ﬁnd\na good tradeoff. The small window in the ﬁrst stage is\nused to pick up local information, which is then statisti-\ncally evaluated in the second stage with respect to a much\nlarger window—note that simply enlarging the analysis\nwindow in Step (2) without using the second stage may\naverage out valuable local harmonics information leading\nto less meaningful features. Furthermore, modifying pa-\nrameters of the second stage such as the size of the statis-\ntics window or the thresholds in Step (5) allows to enhance\nor mask out certain aspects without repeating the cost-\nintensive computations in the ﬁrst stage. We will make\nuse of this strategy in Sect. 4.2, when dealing with the\nproblem of global tempo variations.\nFinally, we want to mention some problems concern-\ning CENS features. The usage of a ﬁlter bank with ﬁxed\n290frequency bands is based on the assumption of well-tuned\ninstruments. Slight deviations of up to 30–40 cents from\nthe center frequencies can be tackled by the ﬁlters, which\nhave relatively wide pass bands of constant amplitude re-\nsponse. Global deviations in tuning can be compensated\nby employing a suitably adjusted ﬁlter bank. However,\nphenomena such as strong string vibratos or pitch oscilla-\ntion as is typical for, e.g., kettle drums lead to signiﬁcant\nand problematic pitch smearing effects. Here, the detec-\ntion and smoothing of such ﬂuctuations, which is certainly\nnot an easy task, may be necessary prior to the ﬁltering\nstep. However, as we will see in Sect. 5, the CENS fea-\ntures generally still lead to good matching results even in\npresence of the artifacts mentioned above.\n4 AUDIO MATCHING\nIn this section, we ﬁrst describe the basic idea of our audio\nmatching procedure, then explain how to incorporate in-\nvariance to global tempo variations, and close with some\nnotes on efﬁciency.\n4.1 Basic matching procedure\nThe audio database consists of a collection of CD audio\nrecordings, typically containing various interpretation s for\none and the same piece of music. To simplify things,\nwe may assume that this collection is represented by one\nlarge document Dby concatenating the individual record-\nings (we keep track of the boundaries in a supplemental\ndata structure). The query Qconsists of a short audio\nclip, typically lasting between 10and30seconds. In the\nfeature extraction step, as described in Sect. 3, the docu-\nmentDas well as the query Qare transformed into se-\nquences of CENS-feature vectors. We denote these fea-\nture sequences by F[D] = (/vector v1,/vector v2,... ,/vector vN)andF[Q] =\n(/vector w1, /vector w2,... , /vector wM)with/vector vn∈ F forn∈[1 :N]and\n/vector wm∈ F form∈[1 :M].\nThe goal of audio matching is to identify audio clips\ninDthat are similar to Q. To this end, we compare\nthe sequence F[Q]to any subsequence of F[D]consist-\ning of Mconsecutive vectors. More speciﬁcally, letting\n/vectorX= (/vector x1,... ,/vector xM)∈ FMand/vectorY= (/vector y1,... ,/vector yM)∈\nFM, we set dM(/vectorX,/vectorY) := 1 −1\nM/summationtextM\nm=1/an}brack⌉tl⌉{t/vector xm,/vector ym/an}brack⌉tri}ht, where\n/an}brack⌉tl⌉{t/vector xm,/vector ym/an}brack⌉tri}htdenotes the inner product of the vectors /vector xmand\n/vector ym(thus coinciding with the cosine of the angle between\n/vector xmand/vector ym, since /vector xmand/vector ymare assumed to be normal-\nized). Note that dMis zero in case /vectorXand/vectorYcoincide and\nassumes values in the real interval [0,1]⊂R. Next, we\ndeﬁne the distance function ∆ : [1 : N]→[0,1]with\nrespect to F[D]andF[Q]by\n∆(i) :=dM((/vector vi,/vector vi+1... ,/vector vi+M−1),(/vector w1, /vector w2,... , /vector wM))\nfori∈[1 :N−M+ 1] and∆(i) := 1 fori∈[N−\nM+ 2 : N]. In particular, ∆(i)describes the distance\nbetween F[Q]and the subsequence of F[D]starting at\nposition iand consisting of Mconsecutive vectors. The\ncomputation of ∆is also illustrated by Fig. 3.\nWe now determine the best matches of Qwithin Dby\nsuccessively considering minima of the distance function· · ·\n∆(1) ∆(2) ∆(3) · · · ∆(N−M+ 1)/vector v1/vector v2· · · /vector vM/vector vM+1· · · /vector vN\n/vector w1/vector w2.../vector wM\nFigure 3: Schematic illustration of the computation\nof the distance function ∆with respect to F[Q] =\n(/vector w1,... , /vector wM)andF[D] = (/vector v1,... ,/vector vN).\n∆: in the ﬁrst step, we determine the index i∈[1 :N]\nminimizing ∆. Then the audio clip corresponding to\nthe feature sequence (/vector vi,/vector vi+1... ,/vector vi+M−1)is our best\nmatch. We then exclude a neighborhood of length M\nof the best match from further considerations by setting\n∆(j) = 1 forj∈[i−⌈M/2⌉:i+⌈M/2⌉]∩[1 :N], thus\navoiding matches with a large overlap to the subsequent\nmatches. In the second step, we determine the feature in-\ndex minimizing the modiﬁed distance function, resulting\nin the second best match, and so on. This procedure is\nrepeated until a predeﬁned number of matches has been\nretrieved or until the distance of a retrieved match exceeds\na speciﬁed threshold.\nAs an illustrating example, let’s consider a database\nDconsisting of four pieces: one interpretation of Bach’s\nToccata BWV565, two interpretations (Bernstein, Sawal-\nlisch) of the ﬁrst movement of Beethoven’s Fifth Sym-\nphony op. 67, and one interpretation of Shostakovich’s\nWaltz 2 from his second Jazz Suite. The query Qagain\nconsists of the ﬁrst 21seconds ( 20measures) of Bern-\nstein’s interpretation of Beethoven’s Fifth Symphony (cf.\nFig. 1). The upper part of Fig. 4 shows the resulting\ndistance function ∆. The lower part shows the feature\nsequences corresponding to the ten best matches sorted\nfrom left to right according to their distance. Here, the\nbest match (coinciding with the query) is shown on the\nleftmost side, where the matching rank and the respec-\ntive ∆-distance ( 1/0.011) are indicated above the fea-\nture sequence and the position ( 0−21, measured in sec-\nonds) within the audio ﬁle is indicated below the feature\nsequence. Corresponding parameters for the other nine\nmatches are given in the same fashion.\nNote that the distance 0.011for the best match is not\nexactly zero, since the interpretation in Dstarts with a\nsmall segment of silence, which has been removed from\nthe query Q. Furthermore, note that the ﬁrst 20measures\nof Beethoven’s Fifth, corresponding to Q, appear again in\nthe repetition of the exposition and once more with some\nslight modiﬁcations in the recapitulation. Matches 1,2,\nand5correspond to these excerpts in Bernstein’s inter-\npretation, whereas matches 3,4, and 6to those in Sawal-\nlisch’s interpretation. In Sect. 5, we continue this discus -\nsion and give additional examples.\n4.2 Global tempo variations\nSo far, our matching procedure only considers subse-\nquences of F[D]having the same length MasF[Q]. As\na consequence, a global tempo difference between two\n291Bach Beethoven/Bernstein Beethoven/Sawallisch Shostakovich\n011 / 0.011C \n01C#\n01D \n01D#\n01E \n01F \n01F#\n01G \n01G#\n01A \n01A#\n01\n0 − 21B 2 / 0.015\n101 − 1223 / 0.072\n1 − 224 / 0.073\n95 − 1165 / 0.153\n297 − 3186 / 0.194\n275 − 2967 / 0.291\n448 − 4698 / 0.292\n236 − 2579 / 0.297\n417 − 43810 / 0.303\n486 − 507\nFigure 4: Distance function ∆(top) and CENS feature se-\nquences of the ﬁrst ten matches for a data set Dconsisting\nof four pieces and query Qcorresponding to Fig. 1.\naudio clips, even though representing the same excerpt\nof music, will typically lead to a larger distance than it\nshould. For example, Bernstein’s interpretation of the ﬁrs t\nmovement of Beethoven’s Fifth is much slower (roughly\n85percent) than Karajan’s interpretation. While there are\n21CENS feature vectors for the ﬁrst 20measures com-\nputed from Bernstein’s interpretation, there are only 17in\nKarajan’s case. To account for such global tempo vari-\nations in the audio matching scenario, we create several\nversions of the query audio clip corresponding to differ-\nent tempos and then process all these query versions in-\ndependently. Here, our two-stage approach exhibits an-\nother beneﬁt, since such tempo changes can be simulated\nby changing the size of the satistics window as well as\nthe downsampling factor in Steps (5) and (6) of the CENS\nfeature computation. For example, using a window size\nof53(instead of 41) and a downsampling factor of 13\n(instead of 10) simulates a tempo change by a factor of\n10/13≈0.77of the origianl query. In our experiments,\nwe used 8different query versions as indicated by Table 1,\ncovering global tempo variations of roughly −40to+40\npercent.\nNext, for each of the eight resulting CENS-feature\nsequences we compute a distance function denoted by\n∆7,... , ∆14(the index indicating the downsampling fac-\ntor). In particular, the original distance function ∆equals\n∆10. Finally, we deﬁne ∆min: [1 :N]→[0,1]by set-\nting∆min(i) := min(∆7(i),... , ∆14(i))fori∈[1 :N].\nWe then proceed with ∆minas described in Sect 4.1 to\ndetermine the best audio matches. Fig. 5 illustrates how−1 0 1 2 3 4 5 6 7 8 9 1000.20.40.60.81\n∆9\n∆10\n∆11\n∆12\n∆13\n∆(i)\nii\nFigure 5: Top: ∆9,... , ∆13(ﬁrst eleven values) for the\n21second Bernstein query applied to Karajan’s interpre-\ntation. Bottom: ∆7,... , ∆14and∆min-distance function.\nws 29 33 37 41 45 49 53 57\ndf 7 8 9 10 11 12 13 14\ntc 1.43 1.25 1.1 1.0 0.9 0.83 0.77 0.7\nTable 1: Tempo changes (tc) simulated by changing statis-\ntics window sizes (ws) and downsampling factors (df).\nchanging the query tempo affects the distance function.\nIn conclusion, we note that global tempo deviations\nare accounted for by employing several suitably modiﬁed\nqueries, whereas local tempo deviations are absorbed to a\nhigh degree by using CENS features.\n4.3 Efﬁcient implementation\nAt this point, we want to mention that the distance func-\ntion∆given by ∆(i) = 1 −1\nM/summationtextM\nm=1/an}brack⌉tl⌉{t/vector vi+m−1, /vector wm/an}brack⌉tri}htcan\nbe computed efﬁciently. Here, one has to note that each\nof the 12components of the vector/summationtextM\nm=1/an}brack⌉tl⌉{t/vector vi+m−1, /vector wm/an}brack⌉tri}ht\ncan be expressed as a convolution, which can then be\nevaluated efﬁciently using FFT-based convolution algo-\nrithms. By this technique, ∆can be calculated with\nO(DN logM)operations, where D= 12 denotes the di-\nmension of the vectors. In other words, the query length\nMonly contributes a logarithmic factor to the total arith-\nmetic complexity. Thus, even long queries may be pro-\ncessed very efﬁciently. The experimental setting as well\nas the running time to process a typical query is described\nin the next section.\n5 EXPERIMENTS\nWe implemented our audio matching procedure in MAT-\nLAB and tested it on a database containing 112 hours\nof uncompressed audio material (mono, 22050 Hz), re-\nquiring 16.5GB of disk space. The database comprises\n1167 audio ﬁles reﬂecting a wide range of classical mu-\nsic, including, among others, pieces by Bach, Bartok,\nBernstein, Beethoven, Chopin, Dvorak, Elgar, Mozart,\nOrff, Ravel, Schubert, Shostakovich, Vivaldi, and Wag-\nner. In particular, it contains all Beethoven symphonies,\nall Beethoven piano sonatas, all Mozart piano concertos,\nseveral Schubert and Dvorak symphonies—many of the\n292pieces in several versions. Some of the orchestral pieces\nare also included as piano arrangements or synthesized\nMIDI-versions. In a preprocessing step, we computed the\nCENS features for all audio ﬁles of the database, resulting\nin a single sequence F[D]as described in Sect. 4.1. Stor-\ning the features F[D]requires only 40.3MB (opposed to\n16.5GB for the original data), amounting in a data reduc-\ntion of a factor of more than 400. Note that the feature\nsequence F[D]is all we need during the matching proce-\ndure. Our tests were run on an Intel Pentium IV , 3 GHz\nwith 1 GByte RAM under Windows 2000. Processing a\nquery of 10to30seconds of duration takes roughly one\nsecond w.r.t. ∆and about 7−10seconds w.r.t. ∆min. As\nis also mentioned in Sect. 6, the processing time may fur-\nther be reduced by employing suitable indexing methods.\n5.1 Representative matching results\nWe now discuss in detail some representative matching\nresults obtained from our procedure, using the query clips\nshown in Table 2. For each query clip, the columns con-\ntain from left to right an acronym, the speciﬁcation of the\npiece of music, the measures corresponding to the clip,\nand the interpreter. Demo audio material of the examples\ndiscussed in this paper is provided at www-mmdb.iai.\nuni-bonn.de/projects/audiomatching , where ad-\nditional matching results and visualizations can be found\nas well.\nWe continue our Beethoven example. Recall that the\nquery, in the following referred to as “BeetF” (see Ta-\nble 2), corresponds to the ﬁrst 20measures, which ap-\npear once more in the repetition of the exposition and\nwith some slight modiﬁcations in the recapitulation. Since\nour database contains Beethoven’s Fifth in ﬁve different\nversions—four orchestral version conducted by Bernstein,\nKarajan, Kegel, and Sawallisch, respectively, and Liszt’s\npiano transcription played by Scherbakov—there are alto-\ngether 15occurrences in our database similar to the query\n“BeetF”. Using our matching procedure, we automatically\ndetermined the best 15matches in the entire database w.r.t.\n∆min. Those 15matches contained 14of the 15“correct”\noccurences—only the 14th match (distance 0.217) corre-\nsponding to some excerpt of Schumann’s third symphony\nwas “wrong”. Furthermore, it turned out that the ﬁrst\n13matches are exactly the ones having a ∆min-distance\nof less than 0.2from the query, see also Fig. 6 and Ta-\nble 3. The 15th match (excerpt in the recapitulation by\nKegel) already has a distance of 0.220. Note that even\nthe occurrences in the exposition of Scherbakov’s piano\nversion were correctly identiﬁed as 11th and 13th match,\neven though differing signiﬁcantly in timbre and articula-\ntion from the orchestral query. Only the occurrence in the\nrecapitulation of the piano version was not among the top\nmatches.\nAs a second example, we queried the piano version\n“BeLiF” of about 26seconds of duration (see Table 2),\nwhich corresponds to the ﬁrst part of the development\nof Beethoven’s Fifth. The ∆min-distances of the best\ntwenty matches are shown in Table 3. The ﬁrst six of\nthese matches contain all ﬁve “correct” occurrences in the\nﬁve interpretations corresponding to the query excerpt, se e\nalso Fig 7. Only the 4th match comes from the ﬁrst move-Query Piece measures interpreter\nBachA n Bach BWV 988, Goldberg “Aria” 1- n MIDI\nBeetF Beethoven Op. 67 “Fifth” 1-20 Bernstein\nBeLiF Beethoven Op. 67 “Fifth” (Liszt) 129-170 Scherbakov\nOrff Carmina Burana 1-4 Jochum\nSchuU Schubert D759 “Unﬁnished” 9-21 Abbado\nShoW n Shostakovich Jazz Suite 2, Waltz 2 1- n Chailly\nVivaS RV269 No.1 “Spring” 44-55 MIDI\nTable 2: Query audio clips used in the experiments. If not\nspeciﬁed otherwise, the measures correspond to the ﬁrst\nmovement of the respective piece.\nNo. BachA8 BeetF BeLiF Orff ShoW22 SchuU VivaS\n1 0.005 0.011 0.010 0.005 0.017 0.024 0.095\n2 0.020 0.015 0.139 0.037 0.051 0.052 0.139\n3 0.090 0.044 0.142 0.065 0.098 0.061 0.154\n4 0.093 0.051 0.168 0.138 0.104 0.070 0.155\n5 0.093 0.058 0.168 0.148 0.109 0.071 0.172\n6 0.095 0.069 0.172 0.150 0.140 0.072 0.210\n7 0.098 0.072 0.200 0.152 0.148 0.073 0.221\n8 0.102 0.073 0.203 0.155 0.163 0.091 0.238\n9 0.104 0.143 0.204 0.158 0.167 0.097 0.241\n10 0.107 0.180 0.214 0.165 0.173 0.100 0.244\n11 0.107 0.183 0.221 0.166 0.186 0.101 0.248\n12 0.108 0.195 0.221 0.166 0.187 0.103 0.257\n13 0.110 0.197 0.225 0.167 0.188 0.107 0.262\n14 0.110 0.217 0.229 0.179 0.192 0.108 0.267\n15 0.112 0.220 0.230 0.179 0.193 0.122 0.268\n16 0.114 0.224 0.231 0.172 0.194 0.151 0.271\n17 0.117 0.225 0.232 0.173 0.197 0.158 0.273\n18 0.120 0.229 0.234 0.174 0.198 0.205 0.275\n19 0.122 0.237 0.235 0.174 0.199 0.207 0.276\n20 0.122 0.238 0.236 0.176 0.199 0.214 0.279\nTable 3: Each column shows the ∆min-distances of the\ntwenty best matches to the query indicated by Table 2.\nBernstein Karajan Kegel Scherbakov Sawallisch\nFigure 6: Bottom: ∆min-distance function for the en-\ntire database w.r.t. the query “BeetF”. Top: Enlargement\nshowing the ﬁve interpretations of the ﬁrst movement of\nBeethoven’s Fifth containing all of the 13matches with\n∆min-distance <0.2to the query.\nment (measures 200–214) of Mozart’s symphony No. 40,\nKV 550. Even though seemingly unrelated to the query,\nthe harmonic progression of Mozart’s piece exhibits a\nstrong correlation to the Beethoven query at these mea-\nsures. As a general tendency, it has turned out in our ex-\nperiments that for queries of about 20seconds of duration\nthe “correct” matches have a distance lower than 0.2to the\nquery. In general, only few “false” matches have a ∆min-\ndistance to the query lower than this distance threshold.\nA similar result was obtained when querying “SchuU”\ncorresponding to measures 9–21 of the ﬁrst theme of\nSchubert’s “Unﬁnished” conducted by Abbado. Our\ndatabase contains the “Unﬁnished” in six different inter-\npretations (Abbado, Maag, Mik, Nanut, Sacci, Solti), the\ntheme appearing once more in the repetition of the exposi-\ntion and in the recapitulation. Only in the Maag interpreta-\n293Bernstein Karajan Kegel Scherbakov Sawallisch Mozart\nFigure 7: Section consisting of the ﬁve interpretations\nof the ﬁrst movement of Beethoven’s Fifth and the\nﬁrst movement of Mozart’s symphony No. 40, KV 550.\nThe ﬁve occurences in the Beethoven interpretations are\namong the best six matches, all having ∆min-distance\n<0.2to the query “BeLiF”.\nAbbado Maag Mik Nanut Sacci Solti\nFigure 8: Section consisting of the ﬁve interpretations of\nthe ﬁrst movement of Schubert’s Unﬁnished. The 17oc-\ncurences exactly correspond to the 17matches with ∆min-\ndistance <0.2to the query “SchuU”.\ntion the exposition is not repeated, leading to a total num-\nber of 17occurrences similar to the query. The best 17\nmatches retrieved by our algorithm exactly correspond to\nthese 17occurences, all of those matches having a ∆min-\ndistance well below 0.2, see Table 3 and Fig. 8. The\n18th match, corresponding to some excerpt of Chopin’s\nScherzo Op. 20, already had a ∆min-distance of 0.205.\nOur database also contains two interpretations\n(Jochum, Ormandy) of the Carmina Burana by Carl Orff,\na piece consisting of 25short episodes. Here, the ﬁrst\nepisode “O Fortuna” appears again at the end of the piece\nas25th episode. The query “Orff” corresponds to the ﬁrst\nfour measures of “O Fortuna” in the Jochum interpreta-\ntion ( 22seconds of duration), employing the full orches-\ntra, percussion, and chorus. Again, the best four matches\nexactly correspond to the ﬁrst four measures in the ﬁrst\nand25th episodes of the two interpretations. The ﬁfth\nmatch is then an excerpt from the third movement of Schu-\nmann’s Symphony No. 4, Op. 120. When asking for all\nmatches having a ∆min-distance of less than 0.2to the\nquery, our matching procedure retrieved 75matches from\nthe database. The reason for the relatively large num-\nber of matches within a small distance to the query is the\nrelatively unspeciﬁc, unvaried progression in the CENS-\nfeature sequence of the query, which is shared by many\nother pieces as well. In Sect. 5.2, we will discuss a sim-\nilar example (“BachA n”) in more detail. It is interesting\nto note that among the 75matches, there are 22matches\nfrom various episodes of the Carmina Burana, which are\nvariations of the original theme.\nTo test the robustness of our matching procedure to\nthe respective instrumentation and articulation, we also\nused queries synthesized from uninterpreted MIDI ver-\nsions. For example, the query “VivaS” (see Table 2) con-\nsists of a synthesized version of the measures 44–55 of\nVivaldi’s Spring RV269, No. 1. This piece is contained\nin our database in 7different interpretations. The best\nseven matches were exactly the “correct” excerpts, wherequery ShoW 12 ShoW 20 ShoW 27\nduration (sec) 13 22 29\n#(matches, ∆min≤0.2) 590 23 8\nChailly 1/2/6/10 1/2/7/3 1/2/7/4\nYablonsky 119/59/103/138 4/5/36/6 3/5/8/6\nTable 4: Total number of matches with ∆min-distance\nlower than 0.2for queries of different durations.\nthe ﬁrst 5of these matches had a ∆min-distance of less\nthan 0.2from the query (see also Table 3). The robust-\nness to different instrumentations is also shown by the\nShostakovich example in the next section.\n5.2 Dependence on query length\nNot surprisingly, the quality of the matching results de-\npends on the length of the query: queries of short duration\nwill generally lead to a large number of matches in a close\nneighborhood of the query. Enlarging the query length\nwill generally reduce the number of such matches. We\nillustrate this principle by means of the second Waltz of\nShostakovich’s Jazz Suite No. 2. This piece is of the form\nA1A2BA3A4, where the ﬁrst theme consists of 38mea-\nsures and appears four times (parts A1,A2,A3,A4), each\ntime in a different instrumentation. In part A1the melody\nis played by strings, then in A2by clarinet and wood in-\nstruments, in A3by trombone and brass, and ﬁnally in A4\nin a tutti version. The Waltz is contained in our database\nin two different interpretations (Chailly,Yablonsky) lea d-\ning to a total number of 8occurrences of the theme.\nThe query “ShoW n” (see Table 2) consists of the ﬁrst\nnmeasures of the theme in the Chailly interpretation. Ta-\nble 4 compares the total number of matches to the query\nduration. For example, the query clip “ShoW 12” (dura-\ntion of 13seconds) leads to 590 matches with a ∆min-\ndistance lower than 0.2. Among these matches the four\noccurrences A1,A2,A3, andA4in the Chailly interpre-\ntation could be found at position 1(the query itself), 2,\n6and10, respectively. Similarly, the four occurrences in\nthe Yablonsky interpretation could be found at the posi-\ntions 119/59/103/138. Enlarging the query to 20mea-\nsures ( 22seconds) led to a much smaller number of 23\nmatches with a ∆min-distance lower than 0.2. Only the\ntrombone theme in the Yablonsky version ( 36th match\nwith ∆min-distance of 0.207) was not among the ﬁrst 23\nmatches. Finally, querying “ShoW 27” led to 8matches\nwith a ∆min-distance lower than 0.2, exactly correspond-\ning to the eight “correct” occurrences, see Fig. 9. Among\nthese matches, the two trombone versions have the largest\n∆min-distances. This is caused by the fact that the spectra\nof low-pitched instruments such as the trombone gener-\nally exhibit phenomena such as oscillations and smearing\neffects resulting in degraded CENS features.\nAs a ﬁnal example, we consider the Goldberg Varia-\ntions by J.S. Bach, BWV 988. This piece consists of an\nAria, thirty variations and a repetition of the Aria at the\nend of the piece. The interesting fact is that the varia-\ntions are on the Aria’s bass line, which closely correlates\nwith the harmonic progression of the piece. Since the\nsequence of CENS features also closely correlates with\nthis progression, a large number of matches is to be ex-\npected when querying the theme of the Aria. The query\n294Chailly Yablonsky\nclarinet strings trombone tutti clarinet strings trombone tutti\nFigure 9: Second to fourth row: ∆min-distance func-\ntion for the entire database w.r.t. the queries “ShoW27”,\n“ShoW20”, and “ShoW12”. The light bars indicate the\nmatching regions. First row: Enlargement for the query\n“ShoW27” showing the two interpretations of the Waltz.\nNote that the theme appears in each interpretation in four\ndifferent instrumentations.\n“BachA n” consists of the ﬁrst nmeasures of the Aria\nsynthesized from some uninterpreted MIDI, see Table 2.\nQuerying “BachA4” ( 10seconds of duration) led to 576\nmatches with ∆min-distance of less than 0.2. Among\nthese matches, 214correspond to some excerpt originat-\ning from a variation of one of the four Goldberg interpre-\ntations contained in our database. Increasing the duration\nof the query, we obtained 307such matches for “BachA8”\n(20seconds), 195of them corresponding to some Gold-\nberg excerpt. Similarly, one obtained 144such matches\nfor “BachA12” ( 30seconds), 127of them corresponding\nto some Goldberg excerpt.\n6 CONCLUSIONS AND FUTURE WORK\nIn this paper, we have introduced an audio matching pro-\ncedure which, given a query audio clip of between 10\nand 30 seconds of duration, automatically and efﬁciently\nidentiﬁes all corresponding audio clips in the database\nirrespective of the speciﬁc interpretation or instrumen-\ntation. A representative selection of our experimental\nresults, including the ones discussed in this paper, can\nbe found at www-mmdb.iai.uni-bonn.de/projects/\naudiomatching . As it turns out, our procedure performs\nwell for most of our query examples within a wide range\nof classical music proving the usefulness of our CESN\nfeatures. The top matches almost always include the “cor-\nrect” occurrences, even in case of synthesized MIDI ver-\nsions and interpretations in different instrumentations.\nIn conclusion, our experimental results suggest that a\nquery duration of roughly 20seconds seems to be sufﬁ-\ncient for a good characterization of most audio excerpts.\nEnlarging the duration generally makes the matching pro-cess even more stable and reduces the number of “false”\nmatches. Our matching process may produce a large\nnumber of “false” matches (false positives) or miss “cor-\nrect” matches (false negatives) in case the underlying mu-\nsic does not exhibit characteristic harmonics information ,\nas is, for example, the case for music with an unchang-\ning harmonic progression or for purely percussive mu-\nsic. “False” matches with small ∆min-distance generally\ndiffer considerably from the query (accidentally having a\nsimilar harmonic progression). Here, our future goal is to\nprovide the user with a choice of additional, orthogonal\nfeatures such as beat, timbre, or dynamics, to allow for a\nranking adapted to the user’s needs.\nFor the future, we also plan to employ indexing meth-\nods to signiﬁcantly reduce the query times of our match-\ning algorithm (in the present implementation it requires\n7–10 seconds for processing single query w.r.t. ∆min).\nAs a further extension of our matching procedure, we also\nwant to retrieve audio clips that differ from the query by\na global pitch transposition. This, e.g., includes arrange -\nments played in different keys or themes appearing in var-\nious keys as is typically the case for a sonata. First exper-\niments show that such pitch transpositions can be handled\nby cyclically shifting the components of the CENS fea-\ntures extracted from the query.\nAs an application, we plan to employ our audio match-\ning strategy to substantially accelerate music synchroniz a-\ntion. Here, the idea is to identify salient audio matches,\nwhich can then be used as anchor matches as suggested\nby M ¨uller et al. (2004).\nFinally, note that we evaluated our experiments manu-\nally, by comparing the retrieved matches with the expected\noccurrences as a ground truth (knowing exactly the conﬁg-\nuration of our audio database). Here, an automated proce-\ndure allowing to conduct large-scale tests is an important\nissue to be considered.\nREFERENCES\nE. Allamanche, J. Herre, B. Fr ¨oba, and M. Cremer. AudioID:\nTowards Content-Based Identiﬁcation of Audio Material. In\nProc. 110th AES Convention, Amsterdam, NL , 2001.\nM. A. Bartsch and G. H. Wakeﬁeld. Audio thumbnailing of pop-\nular music using chroma-based representations. IEEE Trans.\non Multimedia , 7(1):96–104, Feb. 2005.\nN. Hu, R. Dannenberg, and G. Tzanetakis. Polyphonic audio\nmatching and alignment for music retrieval. In Proc. IEEE\nWASPAA, New Paltz, NY , October 2003.\nF. Kurth, M. Clausen, and A. Ribbrock. Identiﬁcation of highly\ndistorted audio material for querying large scale data bases,\n2002.\nM. M ¨uller, F. Kurth, and T. R ¨oder. Towards an efﬁcient algo-\nrithm for automatic score-to-audio synchronization. In Proc.\nISMIR, Barcelona, Spain , 2004.\nR. J. Turetsky and D. P. Ellis. Force-Aligning MIDI Syntheses\nfor Polyphonic Music Transcription Generation. In Proc. IS-\nMIR, Baltimore, USA , 2003.\nG. Tzanetakis, A. Ermolinskyi, and P. Cook. Pitch histograms\nin audio and symbolic music information retrieval. In Proc.\nISMIR, Paris, France , 2002.\nA. Wang. An Industrial Strength Audio Search Algorithm. In\nProc. ISMIR, Baltimore, USA , 2003.\n295"
    },
    {
        "title": "Speech/Music Discrimination Using a Single Warped LPC-Based Feature.",
        "author": [
            "J. Enrique Muñoz Expósito",
            "Sebastián García Galán",
            "Nicolás Ruiz-Reyes",
            "Pedro Vera-Candeas",
            "F. Rivas-Peña"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417711",
        "url": "https://doi.org/10.5281/zenodo.1417711",
        "ee": "https://zenodo.org/records/1417711/files/Munoz-ExpositoGRVR05.pdf",
        "abstract": "Automatic discrimination of speech and music is an important tool in many multimedia applications. The paper presents a low complexity but effective approach for speech/music discrimination, which exploits only one simple feature, called Warped LPC-based Spectral Centroid (WLPC-SC). A three-component Gaussian Mixture Model (GMM) classifier is used because it showed a slightly better performance than other Statistical Pattern Recognition (SPR) classifiers. Comparison between WLPC-SC and the timbral features proposed in Tzanetakis and Cook (2002) is performed, aiming to assess the good discriminatory power of the proposed feature. Experimental results reveal that our speech/music discriminator is robust and fast, making it suitable for real-time multimedia applications. Keywords: speech/music discrimination, LPC, spectral centroid, GMM. 1",
        "zenodo_id": 1417711,
        "dblp_key": "conf/ismir/Munoz-ExpositoGRVR05",
        "keywords": [
            "speech/music discrimination",
            "LPC",
            "spectral centroid",
            "GMM",
            "timbral features",
            "Statistical Pattern Recognition",
            "SPR",
            "robust",
            "real-time",
            "multimedia applications"
        ],
        "content": "SPEECH/MUSIC DISCRIMINATION USING A SINGLE WARPED\nLPC-BASED FEATURE\nJ.E. Mu˜noz-Exp´osito, S. Garcia-Gal ´an, N. Ruiz-Reyes, P. Vera-Candeas and F. Rivas-Pe ˜na\nElectronics and Telecommunication Engineering Department, University of Ja ´en\nPolytechnic School, C/ Alfonso X el Sabio, 28\n23700 Linares, Ja ´en, SPAIN\n{jemunoz,sgalan,nicolas,pvera,rivas }@ujaen.es\nABSTRACT\nAutomatic discrimination of speech and music is an im-\nportant tool in many multimedia applications. The pa-\nper presents a low complexity but effective approach\nfor speech/music discrimination, which exploits only one\nsimple feature, called Warped LPC-based Spectral Cen-\ntroid (WLPC-SC). A three-component Gaussian Mix-\nture Model (GMM) classiﬁer is used because it showed\na slightly better performance than other Statistical Pat-\ntern Recognition (SPR) classiﬁers. Comparison between\nWLPC-SC and the timbral features proposed in Tzane-\ntakis and Cook (2002) is performed, aiming to assess the\ngood discriminatory power of the proposed feature. Ex-\nperimental results reveal that our speech/music discrimi-\nnator is robust and fast, making it suitable for real-time\nmultimedia applications.\nKeywords: speech/music discrimination, LPC, spectral\ncentroid, GMM.\n1 INTRODUCTION\nAutomatic discrimination between speech and music has\nbecome a research topic of interest in the last few years.\nSeveralapproacheshavebeendescribedintherecentliter-\nature for different applications (Saunders, 1996; Scheirer\nand Slaney, 1997; El-Maleh et al., 2000; Harb and Chen,\n2003;Wangetal.,2003). Eachoftheseusesdifferentfea-\ntures and pattern classiﬁcation techniques and describes\nresults on different material.\nSaunders (1996) proposed a real-time speech/music\ndiscriminator, which was used to automatically monitor\nthe audio content of FM audio channels. Four statistical\nfeatures on the zero-crossing rate and one energy-related\nfeature were extracted, a multivariate-Gaussian classiﬁer\nwas applied, which resulted in an accuracy of 98%.\nInAutomaticSpeechRecognition(ASR)ofbroadcast\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonnews, it’s desirable to disable the input to the speech rec-\nognizerduringthenon-speechportionoftheaudiostream.\nScheirerandSlaney(1997)developedaspeech/musicdis-\ncrimination system for ASR of audio sound tracks. Thir-\nteen features to characterize distinct properties of speech\nand music, and three classiﬁcation schemes (MAP Gaus-\nsian, GMM and k-NN classiﬁers) were exploited, result-\ning in an accuracy of over 90%.\nAnother application that can beneﬁt from distinguish-\ning speech from music is low bit-rate audio coding. De-\nsigning an universal coder to reproduce well both speech\nand music is the best approach. However, it is not a\ntrivial problem. An alternative approach is to design a\nmulti-modecoderthatcanaccommodatedifferentsignals.\nThe appropriate module is selected using the output of a\nspeech-music classiﬁer as in ISO-IEC (1999) or Tancerel\net al. (2000).\nAutomatic discrimination of speech and music is an\nimportant tool in many multimedia applications. El-\nMalehetal.(2000)combinedthelinespectralfrequencies\nandzero-crossings-basedfeaturesforframe-levelnarrow-\nbandspeech/musicdiscrimination. Theclassiﬁcationsys-\ntemoperatesusingonlyaframedelayof20ms,makingit\nsuitableforreal-timemultimediaapplications. Anemerg-\ning multimedia application is content-based indexing and\nretrieval of audiovisual data. Audio content analysis is\nan important task for such application (Zhang and Kuo,\n2001). Minami et al. (1998) proposed an audio-based ap-\nproach to video indexing, where a speech/music detector\nis used to help users to browse avideo database.\nComparative view of the value of different types of\nfeatures in speech music discrimination is provided in\nCarey et al. (1999), where four types of features (ampli-\ntudes,cepstra,pitchandzero-crossings)arecomparedfor\ndiscriminating speech and music signals. Experimental\nresults showed cepstra and delta cepstra bring the best\nperformance. Mel Frequencies Spectral or Cepstral Co-\nefﬁcients (MFSC or MFCC) are very often used features\nforaudioclassiﬁcationtasks,providingquitegoodresults.\nInHarbandChen(2003),MFSC’sﬁrstorderstatisticsare\ncombined with neural networks to form a speech music\nclassiﬁer that is able to generalize from a little amount\nof learning data. MFCC are a compact representation of\nthe spectrum of an audio signal taking into account the\nnonlinear human perception of pitch, as described by the\nmel scale. They are one of the most used features in\n614speech recognition and have recently proposed in musi-\ncal genre classiﬁcation of audio signals (Tzanetakis and\nCook, 2002; Burred and Lerch,2004).\nUnlike the previous works, speech/music discrimina-\ntionapproachesbasedononlyonetypeoffeaturesarepre-\nsentedinKarneback(2001)andWangetal.(2003),which\nresult in fast and robust classiﬁcation systems. The ap-\nproach in Karneback (2001) takes psychoacoustic knowl-\nedgeintoaccountinthatitusesthelowfrequencymodula-\ntion amplitudes over 20 critical bands to form a good dis-\ncriminator for the task, while the approach in Wang et al.\n(2003) exploits a new energy-related feature, called mod-\niﬁed low energy ratio, that improves the results obtained\nwith the classical low energy ratio.\nIn this paper, we present our contribution to the\ndesign of a robust speech/music discrimination system.\nThe paper presents a low complexity but effective ap-\nproach,whichalsoexploitsonlyonesimplefeature,called\nWarped LPC-based Spectral Centroid (WLPC-SC). This\nnew feature is also psychoacoustic-based. Its simplic-\nity and robustness make its application scope very wide,\nespecially for the applications where low system cost is\nstrongly demanded.\n2 SPEECH/MUSIC DISCRIMINATION\n2.1 New Warped LPC-basedfeature\nIn our system, an analysis window of 23 ms (1024 sam-\nples at 44100 Hz sampling rate) and a texture window of\n1 s (43 analysis windows) are deﬁned. Overlapping with\nahopsizeof512samplesisperformed. Hence,thevector\nfor describing the proposed feature consists of 85 values,\nwhich are updated each 1 s-length texture window. This\nlarge dimensional feature vector is difﬁcult to be handled\nforclassiﬁcationtasks,givingrisetotwomaindrawbacks:\n1)toomuchcomputationalcost,2)possibletoohighmis-\nclassiﬁcation rate. Therefore, it is required reducing the\nfeature space to a few statistical values each 1 s-length\ntexture window. In this work, the mean and variance of\neach feature vector are only computed.\nWe propose the use of the centroid frequency each\nanalysis window to discriminate between speech and mu-\nsic excerpts. Usually, speech signals has a low cen-\ntroidfrequency,whichvariessharplyatavoiced-unvoiced\nboundary. Instead, music signals show a quite chang-\ning behavior. There is no a speciﬁc pattern for such sig-\nnals. We compute the centroid frequency by a one-pole\nlpc-ﬁlter. Geometrically, the lpc-ﬁlter minimizes the area\nbetween the frequency response of the ﬁlter and the en-\nergyespectrumofthesignal. Theone-polefrequencytells\nus where the lpc-ﬁlter is frequency-centered. Therefore,\nsomeway, the one-pole frequency informs us where most\nof the signal energy is frequency-localized.\nHowever, the human auditory system is nonuniform\nin relation to the frequency. According to this statement,\nthe Mel, the Bark and the ERB (Equivalent Rectangular-\nBandwidth)scales(H ¨arm¨aetal.,2000)aredeﬁnedforau-\ndioprocessing. Forspeech/musicdiscrimination,itwould\nbedesirabletouseafeaturethatworksdirectlyonsomeof\ntheseauditoryscales,resultinginfrequency-warpedaudio\nprocessing.The transformation from frequency to Bark scale is a\nwell studied problem (H ¨arm¨a et al., 2000; III and Abel,\n1999). Generally, the Bark scale is performed via the all-\npass transformation deﬁned by the substitution in the z\ndomain:\nz=Aρ(ζ)≡ζ+ρ\n1 +ζρ(1)\nwhichtakestheunitcircleinthe zplanetotheunitcir-\ncle in the ζplane, in such a way that, for 0< ρ < 1, low\nfrequencies are stretched and high frequencies are com-\npressed. Parameter ρdepends on the sampling frequency\nof the original signal (III and Abel, 1999). Applying (1),\ntheBarkscalevaluescanbeapproximatedfromfrequency\npositions as follows (H ¨arm¨a et al., 2000):\nb= 13 arctan (0.76f(kHz)) + 3 .5arctan (f(kHz)\n7.5)2\n(2)\nWe propose the use of a one-pole warped-lpc ﬁl-\nter based on this bilinear transformation to compute the\nWLPC-SC feature each analysis window.\nAscanbeseeninFig. 1,theWLPC-SCfeatureshows\nclear differences between voiced and unvoiced phonemes\ndue to the frequency-warped processing. Besides, these\ndifferences are bigger than in a drum-based music signal.\nThe results in Fig. 1 suggest us that WLPC-SC could be\naproﬁtablelowcomplexityfeaturetodesignarobustmu-\nsic/speech discriminator. It will be assessed in section 3.\n0123400.20.40.60.81(a) LPC−SC for speech\ntime (s)normalized frequency\n0123400.20.40.60.81(b) WLPC−SC for speech\ntime (s)normalized frequency\n0123400.20.40.60.81(c) LPC−SC for drum music\ntime (s)normalized frequency\n0123400.20.40.60.81(d) WLPC−SC for drum music\ntime (s)normalized frequency\nFigure1: ExampleillustratingthevaluesthatLPC-SCand\nWLPC-SC takes for both speechand music signals.\n2.2 Classiﬁcation\nFor classiﬁcation purposes, a number of standard Statis-\ntical Pattern Recognition (SPR) classiﬁers (Duda et al.,\n2000) were evaluated. The basic idea behind SPR is to\nestimatetheprobabilitydensityfunction(pdf)forthefea-\nturevectorsofeachclass. Insupervisedlearningalabeled\ntraining set is used to estimate the pdf of each class. In\nthe simple Gaussian (GS) classiﬁer, each pdf is assumed\n615to be a multidimensional Gaussian distribution whose pa-\nrametersareestimatedusingthetrainingset. IntheGaus-\nsian Mixture Model (GMM) classiﬁer, each class pdf is\nassumed to consist of a mixture of a speciﬁc number K\nofmultidimensionalGaussiandistributions. Unlikethe k-\nNN classiﬁer, which needs to store all the training feature\nvectors in order to compute the distances to the input fea-\nturevector,theGMMclassiﬁeronlyneedstostoretheset\nof estimated parameters for each class. The iterative EM\nalgorithm can be used to estimate the parameters of each\nGaussian component and themixture weights.\nIn this work a three-component GMM classiﬁer with\ndiagonal covariance matrices is used because it showed\na slightly better performance than other SPR classiﬁers.\nThe performance of the system does not improve when\nusingahighernumberofcomponentsintheGMMclassi-\nﬁer. TheGMMclassiﬁerisinitializedusingthe K-means\nalgorithm with multiple random starting points. Modern\nclassiﬁcation techniques, such as Neural Networks (NN),\nSupport Vector Machines (SVM), fuzzy systems and dy-\nnamic programming, could also be used. We decided to\nuse standard SPR classiﬁers because this work is mainly\nfocussed on the feature selection task for speech/music\ndiscrimination, proposing a new psychoacoustic-based\nfeature (WLPC-SC), rather on the classiﬁcation task.\n3 Experiment evaluation\nFirst of all, the audio test database is carefully prepared.\nThe speech data come from news programs of radio and\nTV stations, as well as dialogs in movies, and the lan-\nguagesinvolveEnglish,Spanish,FrenchandGermanwith\ndifferentlevelsofnoise,especiallyinnewsprograms. The\nspeakersinvolvemaleandfemalewithdifferentages. The\nlengthofthewholespeechdataisaboutanhour. Themu-\nsic consists of songs and instrumental music. The songs\ncoverasmorestylesasposible,suchasrock,pop,folkand\nfunky, and they are sung by male and female in English\nandSpanish. Theinstrumentalmusicwehavechosencov-\ners different instruments (piano, violin, cello, pipe, clar-\ninet) and styles (symphonic music, chamber music, jazz,\nelectronic music). Some music pieces in movies are also\nincluded, which are played by multiple different instru-\nments. The length of the whole music data is also about\nan hour.\nNext, we intend to assess the speech/music discrimi-\nnationcapabilityoftheproposedfeature. Toachievesuch\ngoal, comparison with the timbral features proposed in\nTzanetakisandCook(2002)isperformed. TheWLPC-SC\nfeature is separately compared to all timbral texture fea-\nturesproposedinTzanetakisandCook(2002). Thevector\nfordescribingourpsychoacousticbasedfeatureconsistof\nthe mean and variance over each texture window.\nThefollowingspeciﬁcfeaturesareusedinTzanetakis\nand Cook (2002) to represent timbral texture: Spectral\nCentroid (SC), Spectral Rolloff (SR), Spectral Flux (SF),\nTime Domain Zero Crossings (ZC), Mel-Frequency Cep-\nstral Coefﬁcients (MFCC) and Low Energy (LE) feature\n(Tzanetakis and Cook, 2002). The last one (LE) is the\nonly feature that is based on the texture window rather\nthantheanalysiswindow. Table1showstheclassiﬁcationaccuracypercentageresultswhenWLPC-SCiscompared\nto the timbral features.\nTable 1: Classiﬁcation accuracy percentage. WLPC-SC\nvs. timbral features\nFEATURE SPEECH MUSICGLOBAL\n(%) (%) (%)\nSC 92.26 95.54 93.90\nSR 95.24 90.18 92.60\nSF 90.09 71.81 80.56\nZC 93.26 88.54 90.80\nMFCC 92.08 99.20 95.83\nLE 88.75 78.00 86.19\nWLPC-SC 94.87 91.63 93.20\nAt the sight of the results in table 1, we can say\nthat the proposed feature performs better than most of\nthe timbral features in Tzanetakis and Cook (2002) for\nspeech/music discrimination. The Spectral Centroid (SC)\nperforms as well as the Warped LPC-based Spectral Cen-\ntroid(WLPC-SC),whiletheMel-FrequencyCepstralCo-\nefﬁcients (MFCC) give slightly better accuracy percent-\nagesthanthesamefeature. Thegooddiscriminationcapa-\nbility provided by the SC and MFCC features is achieved\nat the cost of a complexity increase regarding the WLPC-\nSCfeature,whichismuchhigherinthecaseoftheMFCC\nfeature. Note that the WLPC-SC feature does not require\na DFT computation, while both SC and MFCC features\nneed this computation. As shown in table 1, the proposed\nfeature achieves high accuracy percentages while main-\ntaining the complexity at areduced degree.\nNow, we are interested in comparing MFCC with all\ntimbral features and all timbral features plus WLPC-SC.\nWe intend to know if the proposed feature improves the\nclassiﬁcation accuracy percentage when it is added to all\ntimbral features for speech/music discrimination. Table 2\nshows how the inclusion of the WLPC-SC feature within\nthe feature set used for speech/music discrimination en-\ntailsadiscriminationcapabilityimprovement. Theclassi-\nﬁcation accuracy percentage goes up a value of 2% up to\nreach the value of 97%. However, it must be noted that\nno improvement is accomplished when all timbral tex-\nture features in Tzanetakis and Cook (2002) are used for\nspeech/music discrimination regarding the case of using\nonly the MFCC feature.\nTable2: Discriminationcapabilityimprovementwhenthe\nWLPC-SC feature is included within the feature set.\nFEATURE SPEECH MUSICGLOBAL\n(%) (%) (%)\nMFCC 92.08 99.20 95.83\nAll timbral features 93.27 97.27 95.36\nAll timbral features + 94.86 99.45 97.28\nWLPC-SC\nFinally,weareinterestedinknowinghowmuchwarp-\ningtransformationinﬂuencesinspeech/musicdiscrimina-\ntion. Table 3 compares the classiﬁcation accuracy results\n616for both the proposed feature (WLPC-SC) and the same\nfeature without warping transformation (LPC-SC).\nTable 3: Classiﬁcation accuracy percentage. WLPC-SC\nvs. LPC-SC\nFEATURE SPEECH MUSICGLOBAL\n(%) (%) (%)\nWLPC-SC 94.87 91.63 93.20\nLPC-SC 89.72 76.36 82.75\nFrom the results in table 3, it can be said that warping\ntransformation is a very important operation for the good\nperformanceofthefeatureproposedinthispaper,because\nit entails psychoacoustic information is taken into ac-\ncount. Table3showsanimprovementinthespeech/music\ndiscrimination capability higher than 10% regarding the\ncase of not using the warpingtransformation.\n4 CONCLUSIONS\nThis paper presents a simple but robust approach to dis-\ncriminate speech and music. The method exploits only\none feature, called Warped LPC-based Spectral Centroid\n(WLPC-SC). This new feature is is the main contribution\nof the paper. Its performance is assessed by different ex-\nperimental tests. The classiﬁcation stage is performed by\na three-component GMM classiﬁer with diagonal covari-\nance matrices. We have proved that a higher number of\ncomponents in the GMM classiﬁer does not improve the\nperformance of the system.\nThe experiment evaluation compares the proposed\nfeature to other features commonly used in audio classi-\nﬁcation tasks. In particular, the timbral texture feature\nin Tzanetakis and Cook (2002) are used. The proposed\nfeature performs better than most of the timbral features\nin Tzanetakis and Cook (2002) for speech/music discrim-\nination with a reduced complexity. It is also assessed\nthe improvement due to taking into account the nonlin-\near nature of the human earing system. The proposed\nfeature (WLPC-SC) computed in the frequency domain\n(LPC-SC) seems to hide much potential. We achieve an\nimprovement in the discrimination capability higher than\n10% regarding the case of not using the warping trans-\nformation. The experiment results also demonstrate the\nrobustness of the system. The classiﬁcation accuracy per-\ncentageishigherthan93%withawiderangeofstyles. At\nthesametime,itssimplicitybringsobviousadvantagesin\nconstructing low cost systems.\nReferences\nJ.J. Burred and A. Lerch. Hierarchical automatic audio\nsignal classiﬁcation. Journal of the Audio Engineering\nSociety, 52:724–739, 2004.\nM.J.Carey,E.S.Parris,andH.Lloyd-Thomas. Acompar-\nisonoffeaturesforspeech,musicdiscrimination. Proc.\nIEEE ICASSP’99 , pages 1432–1435, 1999.\nR. Duda, P. Hart, and D. Stork. Pattern classiﬁcation. Wi-\nley, New York , 2000.K. El-Maleh, M. Klein, G. Petrucci, and P. Kabal.\nSpeech/music discrimination for multimedia applica-\ntions.Proc. IEEE ICASSP’2000 , 6:2445–2448, 2000.\nH. Harb and L. Chen. Robust speech music discrimina-\ntion using spectrum’s ﬁrst order statistics and neural\nnetworks. Proc. IEEE Int. Symp. on Signal Processing\nand Its Applications , 2:125–128, 2003.\nA. H¨arm¨a, M. Karjalainen, L. Savioja, V. V ¨alim¨aki, U.K.\nLaine, and J. Huopaniemi. Frequency-warped signal\nprocessingforaudioapplications. JournaloftheAudio\nEngineering Society , 48(11):1011–1031, 2000.\nJ.O Smith III and J.S. Abel. Bark and erb bilinear trans-\nforms.IEEE Trans. Speech and Audio Processing , 7:\n697–708, 1999.\nISO-IEC. Mpeg-4 overview. ISO/IEC JTC1/SC29/WG11\nN2995 Document , 1999.\nS. Karneback. Discrimination between speech and music\nbasedonalowfrequencymodulationfeature. European\nConf. on Speech Comm. and Technology , pages 1891–\n1894, 2001.\nK. Minami, A. Akutsu, H. Hamada, and Y. Tonomura.\nVideohandlingwithmusicandspeechdetection. IEEE\nMultimedia , 5(3):17–25, 1998.\nJ. Saunders. Real-time discrimination of broacast\nspeech/music. Proc.IEEEICASSP’96 ,pages993–996,\n1996.\nE.ScheirerandM.Slaney. Constructionandevaluationof\narobustmultifeaturespeech/musicdiscriminator. Proc.\nIEEE ICASSP’97 , pages 1331–1334, 1997.\nL. Tancerel, S. Ragot, V.T. Ruoppila, and R. Lefebvre.\nCombined speech and audio coding by discrimination.\nProc.IEEEWorkshoponSpeechCoding ,pages17–20,\n2000.\nG. Tzanetakis and P. Cook. Musical genre classiﬁcation\nof audio signals. IEEE Trans. on Speech and Audio\nProcessing , 10(5), 2002.\nW.Q. Wang, W. Gao, and D.W. Ying. A fast and ro-\nbust speech/music discrimination approach. Proc. 4th\nPaciﬁc Rim Conference on Multimedia , 3:1325–1329,\n2003.\nT. Zhang and J. Kuo. Audio content analysis for online\naudiovisual data segmentation and classiﬁcation. IEEE\nTrans. on Speech and Audio Processing , 9(4), 2001.\n617"
    },
    {
        "title": "PlaySOM and PocketSOMPlayer, Alternative Interfaces to Large Music Collections.",
        "author": [
            "Robert Neumayer",
            "Michael Dittenbach",
            "Andreas Rauber"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414818",
        "url": "https://doi.org/10.5281/zenodo.1414818",
        "ee": "https://zenodo.org/records/1414818/files/NeumayerDR05.pdf",
        "abstract": "With the rising popularity of digital music archives the need for new access methods such as interactive exploration or similarity-based search become significant. In this paper we present the PlaySOM, as well as the PocketSOMPlayer, two novel interfaces that enable one to browse a music collection by navigating a map of clustered music tracks and to select regions of interest containing similar tracks for playing. The PlaySOM system is primarily designed to allow interaction via a large-screen device, whereas the PocketSOMPlayer is implemented for mobile devices, supporting both local as well as streamed audio replay. This approach offers content-based organization of music as an alternative to conventional navigation of audio archives, i.e. flat or hierarchical listings of music tracks that are sorted and filtered by meta information. Keywords: User Interaction, Music Collections, Information Discovery and Retrieval, Audio Clustering, Audio Interfaces, Mobile Devices. 1",
        "zenodo_id": 1414818,
        "dblp_key": "conf/ismir/NeumayerDR05",
        "keywords": [
            "digital music archives",
            "new access methods",
            "interactive exploration",
            "similarity-based search",
            "music collection browsing",
            "clustered music tracks",
            "similar tracks selection",
            "playing music",
            "large-screen device",
            "mobile devices"
        ],
        "content": "PLA YSOM AND POCKETSOMPLA YER, ALTERN ATIVE INTERF ACES\nTOLARGE MUSIC COLLECTIONS\nRobert Neumay er\nVienna University ofTechnology\nDepartment ofSoftw areTechnology\nandInteracti veSystems\nFavoritenstr .9-11 /188\nA-1040 Wien,Austria\nrobert.neumayer@univie.ac.atMichael Dittenbach\neCommerce Competence Center\niSpaces Group\nDonau-City Strasse 1\nA-1220 Wien,Austria\nmichael.dittenbach@ec3.atAndr easRauber\nVienna University ofTechnology\nDepartment ofSoftw areTechnology\nandInteracti veSystems\nFavoritenstr .9-11 /188\nA-1040 Wien,Austria\nandi@ifs.tuwien.ac.at\nABSTRA CT\nWiththerising popularity ofdigital music archi vesthe\nneed fornewaccess methods such asinteracti veexplo-\nration orsimilarity-based search become signi\u0002cant. In\nthispaper wepresent thePlaySOM ,aswell asthePock-\netSOMPlayer ,twonovelinterf aces that enable one to\nbrowse amusic collection bynavigating amap ofclus-\ntered music tracks andtoselect regions ofinterest con-\ntaining similar tracks forplaying. ThePlaySOM system is\nprimarily designed toallowinteraction viaalarge-screen\ndevice, whereas thePocketSOMPlayer isimplemented for\nmobile devices, supporting both local aswell asstreamed\naudio replay .This approach offerscontent-based organi-\nzation ofmusic asanalternati vetoconventional naviga-\ntion ofaudio archi ves,i.e.\u0003atorhierarchical listings of\nmusic tracks thataresorted and\u0002ltered bymeta informa-\ntion.\nKeywords: User Interaction, Music Collections, Infor -\nmation Disco veryandRetrie val,Audio Clustering, Audio\nInterf aces, Mobile Devices.\n1INTR ODUCTION\nTheincreasing popularity andsizeofdigital music repos-\nitories drivestheneed foradvanced methods toorganize\nthose archi vesforboth privateaswell ascommercial use.\nSimilarity-based organization ofmusic archi vesal-\nlowsusers toexplore pieces ofmusic thataresimilar to\nones theyknowandlike.Moreo ver,itprovides aclear\nandeasy navigation formusic collections thatusers are\nfamiliar with andallowsusers toabstract from manually\nassigned genre information which is,atleast inprivate\ncollections, often inappropriate.\nOvercoming traditional genre boundaries canimpro ve\nsearch results, e.g.concerning tracks from samplers or\nmoviesoundtracks which donothaveany(reliable) genre\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonassigned atall. Further ,single songs that areverydif-\nferent from therestofanalbumcould distort theresult\nforaquery ifrelying ongenre information, asitiscom-\nmon forallsongs ofanalbumtobeassigned thesame\ngenre. This could lead toproblems foralbums contain-\ningremix esorrather inhomogenous songs. Concerning\ntheaccess torapidly growing andchanging collections,\nthesimilarity-based organization ismuch more satisfy-\ningthan conventional search methods because users do\nnothavetoknownewsongs byname astheyareoffered\nwithin their usual queries. This problem becomes more\nimportant with thesizeofacollection. Thebrowsing of\nafewhundred songs with which auser isfamiliar might\nnotbemuch ofaproblem using metadata, butnavigating\nthrough thousands ofsongs oneisnotfamiliar with may\nlead torestrictions, preventing theuser from gaining ac-\ncess tothemajority ofsongs.\nThis paper describes twonovelinterf aces foraccess-\ningmusic collections, organizing tracks spatially ona\ntwo-dimensional map display based onthesimilarity of\nextracted sound features. Ourworkfocuses types ofinter-\naction itself, aswepresent userinterf aces forboth desktop\napplications andmobile devices.\nSection 2brie\u0003y reviewstherelated workfollowed by\nanintroduction tothefundamentals oftheSelf-Or ganizing\nMap,aneural netw ork-based clustering algorithm andthe\nRhythm Patterns feature extraction model thatwere used\nforourexperiments inSection 3.Wethen describe the\nexperimental results ofclustering thecollection oftheIS-\nMIR04 genre contest anddescribe thepresented user in-\nterfaces indetail inSection 4andSection 5provides some\nconclusions.\n2RELA TED WORK\nScienti\u0002c research hasparticularly been conducted in\nthearea ofcontent-based music retrie val(Downie, 2003;\nFoote, 1999). Recently ,content analysis forsimilarity-\nbased organization anddetection hasgained signi\u0002cant\ninterest. The MARSY ASsystem uses awide range of\nmusical surfacefeatures toorganize music into differ-\nentgenre categories using aselection ofclassi\u0002cation al-\ngorithms (Tzanetakis andCook, 2000, 2002). This pa-\nperwill usetheRhythm Patterns features tocluster a\nmusic collection, previously used intheSOMeJB sys-\ntem(Rauber etal.,2002).\n618Regarding intelligent playlist generation, anex-\nploratory study using anaudio similarity measure tocre-\nateatrajectory through agraph ofmusic tracks isreported\ninLogan(2002). Furthermore, manyapplications canbe\nfound ontheInternet thatarenotdescribed inscienti\u0002c\nliterature. Animplementation ofamap-lik eplaylist in-\nterfaceistheSynapse Media Player1.This player tracks\ntheuser' slistening behavior and generates appropriate\nplaylists based onprevious listening sessions andaddi-\ntionally offersamap-interf aceformanually arranging and\nlinking pieces ofmusic foranevenmore sophisticated\nplaylist generation. Another example ofplayers offer-\ningautomatic playlist generation istheIntellig entMul-\ntimedia Mana gement System2which isbased ontrack-\ningtheuser' slistening habits andrecommends person-\nalized playlists based onlistening behavior aswell as\nacoustic properties likeBPM orasong' sfrequenc yspec-\ntrum. Anovelinterf aceparticuarly developed forsmall-\nscreen devices waspresented inVignoli etal.(2004). This\nartist map-interf aceclusters pieces ofaudio based oncon-\ntentfeatures aswell asmetadata attrib utes using aspring\nmodel algorithm. The need foradvanced visualization\ntosupport selection ofaudio tracks ineverlargeraudio\ncollection wasalso addressed inTorrens etal.(2004),\nwhereby different representation techniques ofgrouping\naudio bymetadata attrib utes using Tree-Maps andadisc\nvisualization ispresented.\n3SELF ORGANIZING MAPS FOR\nCLUSTERING AUDIO COLLECTIONS\n3.1 Self-Or ganizing Map\nForclustering weusetheSelf-Or ganizing Map (SOM) ,\nanunsupervised neural netw orkthat provides amap-\nping from ahigh-dimensional input space tousually two-\ndimensional output space (Kohonen, 1982, 2001). ASOM\nconsists ofasetofiunits arranged inatwo-dimensional\ngrid, each attached toaweight vectormi2<n.Ele-\nments from thehigh-dimensional input space, referred to\nasinput vectors x2<n,arepresented totheSOM andthe\nactivation ofeach unitforthepresented input vector iscal-\nculated using anactivation function (usually theEuclidean\nDistance). Inthenextstep, theweight vector ofthewinner\nismovedtowards thepresented input signal byacertain\nfraction oftheEuclidean distance asindicated byatime-\ndecreasing learning rate\u000b.Consequently ,thenexttime\nthesame input signal ispresented, thisunit'sactivation\nwillbeevenhigher .Theresult ofthislearning procedure\nisatopologically ordered mapping ofthepresented input\nsignals intwo-dimensional space. ASOM canbetrained\nusing allkinds offeature sets. Forourexperiments we\nwillusetheRhythm Patterns features asinput data.\n3.2 Audio Featur eExtraction Using Rhythm\nPatter ns\nThe feature extraction process consists oftwomain\nstages, incorporating severalpsycho-acoustic transforma-\n1www.synapseai.com\n2www.luminal.orgtion (Zwick erandFastl, 1999). First thespeci\u0002c loud-\nness sensation indifferent frequenc ybands iscomputed.\nThis isthen transformed intoatime-in variant representa-\ntionbased onthemodulation frequenc y.\nThe audio data isdecomposed into frequenc ybands,\nwhich arethen grouped according totheBark critical-\nband scale. Then, loudness levelsarecalculated, re-\nferred toasphon using theequal-loudness contour matrix,\nwhich issubsequently transformed intothespeci\u0002c loud-\nness sensation percritical band, referred toassone.\nToobtain atime-in variant representation, recurring\npatterns intheindividual critical bands areextracted in\nthesecond stage ofthefeature extraction process. These\nareweighted according tothe\u0003uctuation strength model,\nfollowed bytheapplication ofa\u0002nal gradient \u0002lter and\ngaussian smoothing. The resulting 1.440 dimensional\nfeature vectors capture rhythmic information upto10Hz\n(600bpm), more detailed descriptions ofthisapproach can\nbefound inRauber etal.(2003)\n3.3 Visualization Techniques oftheSOM\nSeveralvisualization techniques havebeen developed to\nvisualize atrained SOM ,themost appealing inthisconte xt\nbeing thevisualization ofcomponent planes.\nHere, only asingle component oftheweight vectors is\nused tocolor -code themap representation.\nInother words, thevalues ofaspeci\u0002c component\noftheweight vectors aremapped onto acolor palette to\npaint units accordingly allowing toidentify regions that\naredominated byaspeci\u0002c feature.\nSince single component planes donotdirectly trans-\nlateinto psychoacoustic sensation noticed bythehuman\near,theRhythm Patterns uses four combinations ofcom-\nponent planes according topsychoacoustic characteris-\ntics(Pampalk etal.,2002). More precisely ,maximum\n\u0003uctuation strength evaluates tothemaximum value of\nallvector components representing music dominated by\nstrong beats. Bass denotes theaggre gation ofthevalues\ninthelowest twocritical bands indicating music with bass\nbeats faster than 60beats perminute. Non-a ggressiveness\ntakesinto account values with amodulation frequenc y\nlowerthan 0.5Hz ofallcritical bands. Hence, thisfeature\nindicates rather calm songs with slowrhythms. Finally ,\ntheratio ofthe\u0002velowest andhighest critical bands mea-\nsures inhowfarlowfrequencies dominate .These charac-\nteristics canbeused tocolor theresulting map, providing\nweather -chart kind ofvisualizations ofthemusic located\nindifferent parts ofthemap. Figure 1showsexamples for\nallfour kinds ofvisualizations.\n4PLA YSOM AND\nPOCKETSOMPLA YER\nWepresent twointerf aces todigital music collections that\narebased ontheSelf-Or ganizing Map clustering algo-\nrithm andallowinteracti veexploration ofmusic collec-\ntions according tofeature similarity ofaudio tracks. The\nPlaySOM andPocketSOMPlayer applications both enable\nusers tobrowsecollections, select tracks, export playlists\naswell aslisten totheselected songs. The PlaySOM\n619(a)Maximum \u0003uctuation strength.\n (b)Bass.\n(c)Non-aggressi veness.\n (d)Lowfrequencies dominant.\nFigure 1:PlaySOM interf acewith different visualizations ofRhythm Patterns.\npresents afullinterf ace,offering different selection mod-\nels,arange ofvisualizations, advanced playlist re\u0002ne-\nment, export toexternal player devices orsimply play-\nback ofselected songs. The PocketSOMPlayer ,onthe\nother hand, offersaslim version ofthedesktop applica-\ntion, optimized forthePocketPC platform, implemented\nforaniPaqusing JavaandSWT tobeused inastreaming\nenvironment.\n4.1 Data Collection andTrained SOM\nThe audio collection used intheISMIR 2004 genre\ncontest comprises 1458 titles, organized into 6genres,\nthemajor part ofwhich isClassical music (640), fol-\nlowed byWorld (244), RockPop(203), Electr onic (229),\nMetal Punk (90) andJazzBlues (52). Yet,these genre la-\nbelsareonly used asanindicator during evaluation, asthe\nkind orofganization provided bytheSOM isintended also\ntoovercome therestrictions ofmanually assigned genre\ninformation. The Rhythm Patterns ofthat collection of\nsongs were extracted anditssongs were mapped onto a\nSelf-Or ganizing Map consisting of20\u000214units.\nTheassessment ofclustering quality isgenerally dif\u0002-\ncultduetothehighly subjecti venature ofthedata andthe\nbroad spectrum ofindividual similarity perception. We\nstilltrytoprovide anovervie wofthemap-based organi-zation ofthiscollection andpick some sample areas ofthe\nmap todemonstrate theresults based ontheinterf aces.\n4.2 PlaySOM\nFigures 1(a)-(d) showthecomplete map visualizing the\nfour different Rhythm Patterns sub-groups described in\ntheprevious section.\nAlinear gray scale comprising 16colors from dark\ngray towhite representing feature values thatrange from\nlowtohigh isused forprinting purposes. (Foron-screen\nuse, weemphasize themap metaphor byusing a\u0002ne-\ngrained color palette ranging from blue viayello wto\ngreen re\u0003ecting geographical properties similar totheIs-\nlands ofMusic (Pampalk, 2001)).\nTheorganization ofthesongs according tothemaxi-\nmum \u0003uctuation strength feature isclearly visible inFig-\nure1(a) where pieces ofmusic having high values are\nlocated primarily ontheleft-hand side ofthemap. Es-\npecially Metal Punk and RockPopaswell assome of\ntheElectr onic songs thatarelessbass-dominated canbe\nfound there. Contrarily ,songs with lowvalues arelocated\nonthemap' sright-hand side. Some examples ofrather\ntranquil music aretracks belonging tothegenres Classic\norWorld aswell assingle PopRocksongs.\nFigure 1(b) showsthatthefeature bass isconcentrated\n620(a)Lowlevelofdetail -thenumber ofsongs mapped are\nwritten ontherespecti veunits.\n(b)High zooming level-song names aredisplayed onthe\nrespecti veunits.\nFigure 2:Semantic zooming anditsimpact onthedisplayed data.\nontheupper leftcorner andbasically consists ofbass-\ndominated tracks belonging toElectr onic genre. This\ncluster isthemost homogenous onthemap (along with a\ncluster ofclassical music) according togenre tags, almost\nnoother genres arefound inthisarea.\nFinally ,asmall cluster where lowfrequencies domi-\nnate islocated intheupper leftofthemap asshownin\nFigure 1(d) andcorresponds totheresults ofbass setting,\nleading tolowvalues inthisregion.\nThe different types ofclassical music areagood ex-\nample ofsimilarity-based clustering thatovercomes genre\nboundaries. Whereas manysongs from operas arelocated\nonthelowerleft-hand side ofthemap, manyother tracks\nthat also belong totheClassical genre, butsound very\ndifferent from operas arelocated ontheupper right. This\nmapping isbased onthefactthatmanysongs from the\nWorld genre share manycharacteristics with slowpieces\nofclassical music, butdifferfrom operas, apossibility\nwhich isnotcaptured bystatic genre assignments what-\nsoever.\nThe PlaySOM allowsusers tointeract with themap\nmainly bypanning, semantic zooming andbyselecting\ntracks. Users canmoveacross themap, zoom intoareas\nofinterest andselect songs theywanttolisten to.They\ncanthereby browsetheir privatecollections ofafewthou-\nsand songs, generating playlists based ontrack similarity\ninstead ofclicking through metadata hierarchies, andei-\ntherlistening tothose selected playlists orexporting them\nforlater use. Users canabstract from albums orgenres\nwhich canoften lead torather monotonous playlists of-\ntenconsisting ofcomplete albums ormanysongs from\nonegenre. This approach enables users toexport playlists\nbased onthetrack itself notonmetadata similarity orman-\nualorganization.\nThemain PlaySOM userinterf ace'slargestpartiscov-\nered bytheinteracti vemap ontheright, where squares\nrepresent single units oftheSOM. Controls forselecting\ndifferent visualizations andexporting themap dataandthe\ncurrent visualization forthePocketSOMPlayer arepartof\nthemenubar onthetop. Thelefthand side oftheuser in-terfacecontains (1)aplaylist ofcurrently selected titles,\n(2)abirds-e ye-vie wshowing which partofthepotentially\nverylargemap iscurrently depicted inthemain viewon\ntheright and(3)controls forthecurrently selected visu-\nalization (asdemonstrated bythedifferent settings ofthe\nRhythm Patterns inFigure 1).\nTheicons ontheupper leftallowtheusertoswitch be-\ntween thetwodifferent selection models andtoautomati-\ncally \u0002tthemap tothecurrent screen size. ThePlaySOM\ncurrently supports twointeraction models. Therectangu-\nlarselection model allowstheuser todrag arectangle and\nselect thesongs belonging tounits inside thatrectangle\nwithout preserving anyorder oftheselected tracks. This\nmodel isused toselect music from oneparticular cluster\norregion onthemap.\nOntheother hand, thelineselection model allowsse-\nlection ofsongs belowatrajectory initsspeci\u0002c order .\nInthiscase thesequence ofselected units isofparticular\nimportance, because thislinechooses avariety ofsongs\naccording totheir position onthemap, i.e.their similar -\nity.Hence thelineselection model makesitpossible to\ngenerate playlists thatprovide smooth transitions between\nclusters oftracks. This might beofspeci\u0002c interest when\nbrowsing verylargemusic collections orwhen rather long\nplaylists shall begenerated (forexample ifaplaylist for\nseveralhours should begenerated andseveralchanges in\nmusical style shall occur overtime, similar toanauto-dj\nfunctionality).\nAnother vital aspect oftheinterf aceisthat itsup-\nports semantic zooming, i.e.thezooming levelin\u0003uences\ntheamount andtype ofdata displayed. Asoutlined in\nFigure 2,thehigher thezooming level,themore in-\nformation isdisplayed ranging from information about\nthenumber ofsongs mapped toaparticular unit (Fig-\nure2(a)) todetailed information about thetracks (Fig-\nure2(b)), i.e.artist- and trackname. Furthermore, the\nmain PlaySOM application caneasily andef\u0002ciently be\nused onaTablet PCandused asatouch screen applica-\ntion because ofitsportable Javaimplementation (alive\ndemo isshownin4(b)).\n621(a)ThePocketSOMPlayer 'smain panel\nshowing atrajectory selection.\n(b)PocketSOMPlayer user re\u0002nement\npanel.\nFigure 3:ThePocketSOMPlayer interf aceshowing different interaction views.\n4.3 PocketSOMPlay er\nThePocketSOMPlayer application offerssimilar butsim-\npli\u0002ed functionality asthePlaySOM isdesigned formo-\nbiledevices such asPDAsorSmartphones. Therefore it\nprovides only thebasic functinality ofselecting bydraw-\ningtrajectories andasimpli\u0002ed re\u0002nement section, omit-\ntingmeans tozoom orpanthemap. Itsoperational area\nislikelytobeaclient ina(wireless) audio streaming en-\nvironment forentertainment purposes. Regarding thecur-\nrentmemory restrictions ofPDAs,theuseofastreaming\nserverasamusic repository seems evenmore appealing\nthan forthedesktop application. Nevertheless, themobile\ninterf acecould besynchronized with itsdesktop pendant\ntotaketheroleofamobile audio player within thePDA's\nmemory limits.\nFigure 3(a)showsthePocketSOMPlayer 'smain inter-\nface, atrajectory selection with anunderlying map. Its\nuser re\u0002nement viewwhich allowstheuser tomodify the\npreviously selected playlist before listening totheresult is\ndepicted inFigure 3(b). (Due totheanon ymized format\noftheISMIR collection weemphasized ongenres instead\nonindividual track names. Inrealapplication scenarios,\n\u0002lenames orID3-tag information would beused fordis-\nplaying information onthemap.) Themain panel allows\ntheuser todrawtrajectories andtoselect theunits under -\nneath those trajectories. Allsongs mapped totheselected\nunits areadded totheplaylist. Theuser re\u0002nement panel\npops upassoon asaselection is\u0002nished andprovides\nsimilar functionality asthePlaySOM 'splaylist controls,\nnamely theuser candelete single songs from theplaylist\ntore\u0002ne her/his selection. Theresulting playlist canthen\nbeplayed, retrie ving theMP3s either from thelocal stor-\nageorastreaming server.\nFigure 4(a) showsthePocketSOMPlayer running onaniPaqPDAwithout atrajectory selection. Themap de-\nscribes amusic repository located onastreaming server\nrunning onanother machine, accessible viaWLAN, in\ncontrast tokeeping themusic \u0002les locally (note thatla-\nbels aremanually assigned toclusters according tothe\nmost prominent genres inthisexample). Selecting tracks\nviadrawing oftrajectories onatouch schreen isstraight-\nforw ard,easy tolearn andintuiti veasopposed toclicking\nthrough genre hierarchies andtherefore particularly inter-\nesting formobile devices andtheir handling restrictions.\n5CONCLUSIONS\nWepresented thePlaySOM ,anoveluser interf acetomap\nrepresentations ofmusic collections created bytraining\naSelf-Or ganizing Map,i.e.aneural netw orkwith unsu-\npervised learning function using automatically extracted\nfeature values tocluster audio \u0002les. Theinterf aceallows\nuser interaction andinteracti veexploration based onthose\nmaps, which wasdescribed indetail inourexperiments.\nThePlaySOM offersatwo-dimensional map with spatial\norganization ofsimilar tracks andisespecially appealing\nforlargeorunkno wncollections. Theapplication allows\nusers tobrowse their collections bysimilarity andthere-\nfore \u0002nd songs similar toones theyknowbyname in\ncontrast tometadata-based approaches. Moreo ver,wein-\ntroduced aPDAapplication offering similar functionality .\nBoth user interf aces arewell suited forinteracti veexplo-\nration ofcollections ofdigital music duetotheir different\nlevelsofinteracti vefeatures, including semantic zooming\noron-the-\u0003y playlist generation.\n622(a)The PocketSOMPlayer application\nrunning onaniPaqPDA.\n(b)PocketSOMPlayer running onaTablet PC.\nFigure 4:Both presented interf aces running onaniPaqandTablet PCrespecti vely.\nACKNO WLEDGEMENTS\nPartofthisworkwassupported bytheEuropean Union\ninthe6.Frame workProgram, IST,through theDELOS\nNoE onDigital Libraries, contract 507618, andtheMUS-\nCLE NoE onMultimedia Understanding through Seman-\ntics, Computation andLearning, contract 507752.\nREFERENCES\nJ.S.Downie. Annual ReviewofInformation Science and\nTechnolo gy,chapter Music information retrie val,pages\n295340. Information Today,2003.\nJ.Foote. Anovervie wofaudio information retrie val.Mul-\ntimedia Systems ,7(1):210, 1999.\nT.Kohonen. Self-or ganized formation oftopologically\ncorrect feature maps. Biolo gical Cybernetics ,43:59\n69,1982.\nT.Kohonen. Self-Or ganizing Maps ,volume 30of\nSpring erSeries inInformation Sciences .Springer ,\nBerlin, 3rdedition, 2001.\nB.Logan.Content-based playlist generation: Exploratory\nexperiments. InProc.3rdAnn. Symp. onMusic Infor -\nmation Retrie val(ISMIR 2002) ,France, 2002.\nE.Pampalk. Islands ofmusic: Analysis, organization, and\nvisualization ofmusic archi ves.Master' sthesis, Vienna\nUniversity ofTechnology ,December 2001.\nE.Pampalk, A.Rauber ,andD.Merkl. Content-based\norganization andvisualization ofmusic archi ves. InProceedings ofACMMultimedia 2002 ,pages 570579,\nJuan-les-Pins, France, December 1-62002. ACM.\nA.Rauber ,E.Pampalk, andD.Merkl. Using psycho-\nacoustic models andself-or ganizing maps tocreate a\nhierarchical structuring ofmusic bymusical styles. In\nProceedings oftheInternational Confer ence onMusic\nInformation Retrie val,pages 7180, Paris, France, Oc-\ntober 13-17 2002.\nA.Rauber ,E.Pampalk, and D.Merkl. The SOM-\nenhanced JukeBox: Organization andvisualization of\nmusic collections based onperceptual models. Journal\nofNewMusic Resear ch,32(2):193210, June 2003.\nM.Torrens, P.Hertzog, andJ.L.Arcos. Visualizing and\nexploring personal music libraries. InISMIR 2004,\nUser Interfaces ,pages 421424, Barcelona, Spain, Oc-\ntober 2004.\nG.Tzanetakis andP.Cook. Marsyas: Aframe workfor\naudio analysis. Organized Sound ,4(30), 2000.\nG.Tzanetakis andP.Cook. Musical genre classi\u0002cation of\naudio signals. IEEE Transactions onSpeec handAudio\nProcessing ,10(5):293302, July 2002.\nF.Vignoli, R.vanGulik, andH.vandeWetering. Map-\nping music inthepalm ofyour hand, explore anddis-\ncoveryour collection. InE.FoxandN.Rowe, ed-\nitors, ISMIR 2004, User Interfaces ,pages 409414,\nBarcelona, Spain, October 2004.\nE.Zwick erand H.Fastl. Psychoacoustics, Facts and\nModels ,volume 22ofSeries ofInformation Sciences .\nSpringer ,Berlin, 2edition, 1999.\n623"
    },
    {
        "title": "Experiments on Segmentation Techniques for Music Documents Indexing.",
        "author": [
            "Giovanna Neve",
            "Nicola Orio"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416996",
        "url": "https://doi.org/10.5281/zenodo.1416996",
        "ee": "https://zenodo.org/records/1416996/files/NeveO05.pdf",
        "abstract": "This paper presents an overview of different approaches to melody segmentation aimed at extracting music lexical units, which can be used as content descriptors of music documents. Four approaches have been implemented and compared on a test collection of real documents and queries, showing their impact on index term size and on retrieval effectiveness. From the results, simple but extensive approaches seem to give better performances than more sophisticated segmentation algorithms. Keywords: indexing, melodic segmentation. 1",
        "zenodo_id": 1416996,
        "dblp_key": "conf/ismir/NeveO05",
        "keywords": [
            "melody segmentation",
            "music lexical units",
            "content descriptors",
            "music documents",
            "index term size",
            "retrieval effectiveness",
            "test collection",
            "queries",
            "simple approaches",
            "sophisticated segmentation algorithms"
        ],
        "content": "Experiments on Segmentation Techniques for Music Documents Indexing\nNicola Orio\nDepartment of Information Engineering\nVia Gradenigo, 6/A\n35131 Padova – Italy\norio@dei.unipd.itGiovanna Neve\nDepartment of Information Engineering\nVia Gradenigo, 6/A\n35131 Padova – Italy\ngiovanna.neve@virgilio.it\nABSTRACT\nThis paper presents an overview of different approaches\nto melody segmentation aimed at extracting music lexical\nunits, which can be used as content descriptors of mu-\nsic documents. Four approaches have been implemented\nand compared on a test collection of real documents and\nqueries, showing their impact on index term size and on\nretrieval effectiveness. From the results, simple but ex-\ntensive approaches seem to give better performances than\nmore sophisticated segmentation algorithms.\nKeywords: indexing, melodic segmentation.\n1 INTRODUCTION\nThe access to music digital libraries is usually content-\nbased. The user provides an example that describes his in-\nformation need, the system analysis the query document,\nextracts a number of features and compares them with the\nones extracted from documents in the collection: docu-\nments that are more similar, under a given metrics, to the\nquery document are presented to the ﬁnal user in order\nof similarity. The main idea underlying content-based ap-\nproaches is that a document can be described by a set of\nits features which, for most of the approaches in the liter-\nature, are based on the melody.\nThe research work on content-based music accessing\nand retrieval can be roughly divided in two categories:\non-line searching techniques , which compute a match be-\ntween the sequence representing the query and the ones\nrepresenting the documents each time a new query is sub-\nmitted to the system; indexing techniques , which extract\noff-line all the relevant information that is needed at re-\ntrieval time and perform the match directly between query\nand documents indexes. On-line techniques can be com-\nputationally expensive, because their complexity is pro-\nportional to the collection size; the work presented in [1]\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc°2005 Queen Mary, University of Londonreports a comparison of different variants of an on-line\ntechnique, discussing their computational complexity and\nscalability.\nResearch work on off-line document indexing is usu-\nally based on text information retrieval techniques, which\ngive high scalability but do not model possible mis-\nmatches between the query and the documents. Text re-\ntrieval techniques can be applied providing that a music\ndocument can be described by a set of lexical units, which\nplay the same role of words in a text. The approaches pro-\nposed in the literature differ on the way document indexes\nare computed and how they are eventually normalized to\novercome sources of mismatch between documents and\nqueries. In [2] work, melodies have been indexed through\nthe use of N-grams, each N-gram being a sequence of N\npitch intervals, while note duration was not used as a con-\ntent descriptor. Another approach to document indexing\nhas been presented in [3], where indexing has been carried\nout by automatically highlighting lexical units using an\nautomatic segmentation algorithm based on music theory.\nUnits could undergo a number of different normalization,\nfrom the complete information of pitch intervals and dura-\ntion to the simple melodic proﬁle. Melodic and rhythmic\npatterns have been used in [4] as lexical units. The com-\nputation has been carried out without using knowledge on\nmusic structure or cognition. Separate indexes has been\ncomputed for melodic and rhythmic patterns, using a data\nfusion approach for merging results.\nThis paper reports a study on the effects of different\nsegmentation techniques aimed at off-line document in-\ndexing. Four algorithms based on different approaches\nhave been tested tested using a set of real queries and a\ncollection of documents of popular music from which the\nmelodies have been automatically extracted.\n2 APPROACHES TO MELODIC\nSEGMENTATION\nMusic, both in acoustic and in notated form, is a continu-\nous ﬂow of events without explicit separators. It is there-\nfore necessary to automatically detect the lexical units of a\nmusic document to be used as content descriptors to build\nthe index of the collection. Different strategies to melodic\nsegmentation can be applied, each one focusing on partic-\nular aspects of melodic information.\n6242.1 Segmentations Based on Document Content\nA simple segmentation approach consists in the extraction\nfrom a melody of all the subsequences of exactly Nnotes,\ncalled N-grams (NG approach). N-grams may overlap,\nbecause no assumption is made on the possible starting\npoint of a theme, neither on the possible repetitions of rel-\nevant music passages. The idea underlying this approach\nis that the effect of musically irrelevant N-grams will be\ncompensated by the presence of all the relevant ones. It\nmay be advisable to choose small values for N, in order to\nincrease recall, which is considered more signiﬁcant than\nthe subsequent lowering in terms of precision.\nSegmentation can be performed considering that typi-\ncal passages of a given melody tend to be repeated many\ntimes, because of the presence of different choruses in the\nscore or of the use of similar melodic material. Each se-\nquence that is repeated at least Ktimes can be used for the\ndescription of a music document. This approach based on\nthe analysis of repetitions (AR) is an extension NG, be-\ncause AR units can be of any length, with the limitation\nthat they have to be repeated inside the melody. Segments\ncan be truncated by applying a given threshold, because\nit is unlikely that a user will remember long sequences of\nnotes.\n2.2 Segmentations Based on A Priori Knowledge\nMelodies can be segmented by exploiting a priori knowl-\nedge on the music domain. For instance, accordingly to\ntheories on human perception, listeners have the ability\nto segment the unstructured auditory stream into smaller\nunits, which may correspond to melodic phrases or mo-\ntifs. It is likely that perceptually based (PB) units are good\ndescriptors of a document content, because they capture\nmelodic information that appears to be relevant for users.\nEven if the ability of segmenting music may vary depend-\ning on the level of musical training, a number of strategies\ncan be generalized for all listeners. Computational ap-\nproaches have been proposed in the literature for the auto-\nmatic emulation of listeners behavior [5]. PB units do not\noverlap and are based on information on note pitch and\nduration of monophonic melodies.\nAnother approach to segmentation is based on knowl-\nedge on music theory, in particular for classical music.\nAccording with music theorists, music is based on the\ncombination of musical structures [6], which can be in-\nferred by applying a number of rules. It is likely that a\nmusicologically oriented (MO) approach can be extended\nalso to less structured music, like popular music. MO\nunits should be computed using the complete score, but\ncomparable results have been obtained by an algorithm\nthat exploits local information only [7]. Structures may\noverlap in principle, but the current implementations do\nnot take into account this possibility.\n3 QUERY AND DOCUMENT\nPROCESSING\nContent-based retrieval requires that both documents and\nqueries are processed correspondingly to compute a mea-\nsure of similarity between them. As a ﬁrst step, themelody has been automatically extracted from documents,\nusing a classiﬁcation technique based on the Nearest\nNeighbor approach. A set of 50documents have been\nused as training set, while 50more documents have been\nused to validate the results. Automatic extraction of\nmelodies gave a rate of correct classiﬁcations of 80%\nwhen the ﬁrst nearest neighbor was used, which increased\nto100% for the training set and 92% for the validation set\nwith three nearest neighbors. The use of three neighbors\nincreases the number of false alarms, but this has the only\nside effect that more than one melodic line per document\nis indexed.\nThe extracted melodies have been segmented using\nfour different algorithms, which implement the four ap-\nproaches to segmentations presented in the previous sec-\ntion. It is important to note that the tested algorithms are\nonly particular implementations of general approaches to\nsegmentation, and the experimental results may be differ-\nent if other implementations had been used. Nevertheless,\nwe believe that results may show a general trend of the\nrelationship between the segmentation technique and the\nretrieval effectiveness. Several tests have been carried for\neach algorithm to evaluate experimentally the best conﬁg-\nuration of its parameters. Results have been obtained with\nan optimal conﬁguration for each algorithm, which is re-\nlated to the particular collection used for the experiments.\nThe information on pitch and timing has been\nprocessed for all the segmented melodies, by consider-\ning the pitch intervals and the ratio of interonset intervals.\nBoth dimensions have been quantized. In particular, the\npitch intervals have been divided in 7classes: unison and\nascending or descending intervals within a major third, in-\nterval within a major sixth, and interval over a minor sev-\nenth. The ratios of note durations have been uniformly\nquantized in 9classes, from a ratio lower than 1=3to a\nratio higher than 3. These choices of quantizations are the\nones that gave the best results for all the algorithms.\nQuery processing poses additional problems. Queries\nnormally are audio recordings of users singing melodic\nexcerpts, and have to be translated in a suitable form to\nbe matched with documents. Any automatic transcription\nintroduces a number of errors in pitch and onset detection\nthat may affect the retrieval performances. Moreover, it\nis not guaranteed that the user will sing in the same key\nof the original melody, or that the tempo will be compa-\nrable to the original one. Finally, queries are error prone,\nbecause the user cannot remember exactly the excerpt –\nsince the user is using a search engine, this is usually the\ncase – and because untrained singers may make mistakes\nin pitch contour or may introduce tempo ﬂuctuations. In\nour experiments, we used a set of 36queries provided\nby the MAMI - Musical Audio Mining project [8]. The\ntranscriptions of the audio queries, in the format of pitch\nin Hertz plus onset and offset times, are already avail-\nable together with the original queries and a description\nof how the recordings have been carried out [9]. The use\nof the MAMI material allows for experimenting the effect\nof different segmentation techniques independently from\nthe particular pitch tracker used to transcribe the queries.\nThe same segmentation processing cannot be directly\napplied to queries. In fact, it has to be considered that\n625users normally provide short examples, which do not al-\nlow for the computation of patterns, and query boundaries\nmay not correspond to patterns, perceptual units, or mu-\nsicological structures. With the aim of partially overcome\nthis problem, the extraction of lexical units from queries\nhas been carried out taking all possible sequences of notes.\nMost of these sequences will be musically irrelevant, giv-\ning no contribution to the similarity computation. On the\nother hand, this exhaustive approach guarantees that all\npossible lexical units are used to query the system. We\ntested also the application of PB and MO algorithms also\nto queries, in order to segment the query consistently with\ndocuments, but the experimental results did not show any\nimprovement on the retrieval effectiveness. After segmen-\ntation, the queries have been quantized using the same ap-\nproach applied to documents.\n4 EXPERIMENTAL COMPARISON OF\nSEGMENTATION TECHNIQUES\nA music test collection of popular music has been created\nusing 1004 documents in MIDI format; the melodies auto-\nmatically extracted by documents had an average length of\n372:6notes, ranging from 124up to 1407 notes. Queries\nhave been added to the collection by downloading 36an-\nnotated queries from the MAMI Web site [9]; average\nquery length was 38:2notes, from 5to a very long query\nof79notes. All the segmentation algorithms have been\ntested with the same retrieval engine, which was based\non the Vector Space Model, using the classical tf¢id f\nmeasure to compute the similarity between the query and\nthe documents. The retrieval engine has been extensively\ntested and evaluated in previous work, using different col-\nlections and sets of automatically created queries: The\nevaluation showed that the performances of the retrieval\nengine are comparable to the ones of other systems de-\nscribed in the literature.\nExperimental results presented for NG have been ob-\ntained with N-grams of three notes, while AR has been\ncomputed applying a threshold of ﬁve notes. PB and MO\nperformances have been computed using the algorithms\npresented in [10], where the clangs of PB [5] have been\nused as lexical units and all the local maxima have been\nused as evidence of a boundary in MO [7]. These choices\nare the ones that gave the best performances for the four\nalgorithms.\nTable 1 shows the main characteristics of lexical units\nextracted by the four algorithms. As it can be seen, AR has\nan index size four/ﬁve times bigger than the other algo-\nrithms, because of the high overlapping among segments\nof different lengths, requiring more storage and longer ac-\ncess times. On the other hand, the four algorithms gave\ncomparable results in terms of average length of segments.\nAccording to a study on manual segmentation [11], all the\napproaches had the tendency to oversegment melodies.\nThe average number of unique segments per docu-\nment, and the average number of documents that have a\nparticular segment are also shown in Table 1. Consistently\nwith results on index size, AR had the highest number of\nunique segments per document, while the other three algo-\nrithms had similar values. It is interesting to note that theTable 1: Index size and average values for the different\nsegmentations algorithms.\nNG AR PB MO\nIndex size 21;620 105 ;093 25 ;385 23 ;047\nSeg length 3:0 4 :1 5 :7 4 :4\nSeg/Doc 53:0 159 :6 40 :1 43 :8\nDoc/Seg 2:5 1 :5 1 :6 1 :9\nnumber of segments which are present in different docu-\nments is quite low for all the algorithms. This result may\nsuggest that a weighting scheme based on the tf¢id fmea-\nsure may not be completely suitable for a music informa-\ntion retrieval task, at least because the inverse document\nfrequency ( id fcomponent) will give a small contribution\nto the ﬁnal ranking of relevant documents. We carried out\na number of tests using only the term frequency ( tfcom-\nponent) in the weighing scheme and we found that all the\nalgorithms had only a small decrease in their retrieval per-\nformances.\nThe results in terms of retrieval effectiveness are pre-\nsented in Table 2, which reports the percentage queries\nthat gave the correct document within the ﬁrst kpositions\n(with k2 f1;5;10;20g), and the ones that did not re-\ntrieve the relevant document (“not found”), as representa-\ntive measures.\nThe success rate of presenting the correct document at\ntop rank was quite low for all the algorithms. For AR and\nNG, which gave the best results, respectively only 15:1%\nand12:6of the queries gave the corresponding document\nat top rank, while for PB and MO the percentages drop\nto5:6and2:8. Analyzing the results at different levels of\nretrieved documents, it can be seen that for AR the correct\ndocument was presented within the ﬁrst ﬁve documents in\n50% of the queries, and that more than 3=4of the queries\npresented it within the ﬁrst 20documents. NG had al-\nmost comparable performances, which slightly lower val-\nues. PB and MO algorithms gave poorer results, with PB\nbetter than MO for each measured level.\nThe number of documents that are not retrieved at all\nis much lower with NG and AR approach than with PB\nand MO, as it can be seen from the last row of Table 2.\nIt is interesting to note that PB had an higher number of\nunretrieved documents than MO, showing that the better\nresults in terms of precision are paired to poorer results in\nterms of recall.\nFrom these results, it seems that the presence of over-\nlapping segments – as for NG and AR algorithms – can\nimprove retrieval effectiveness. Moreover, the increase of\nthe index size, due to a small overlap between documents\nand a high number of unique segments per document – as\nfor AR algorithm – can give a further improvement. More\ncomplex approaches to segmentation did not seem to com-\npete with simpler ones, even if it has to be noted that these\nresults may be biased by the particular implementations\nof the corresponding algorithms, which had not been de-\nveloped for a music information retrieval task but for mu-\nsicological studies [10].\nAnother measure that can show the performances of\n626Table 2: Retrieval effectiveness. \nNG AR PB MO\n= 1 12 :6 15 :1 5 :6 2 :8\n·5 38 :5 50 :0 16 :7 6 :9\n·10 62 :3 72 :2 16 :7 13 :9\n·20 69 :0 77 :8 25 :0 16 :7\nnot found 2:8 2 :8 33 :3 23 :6\nthe different techniques is the average precision, which is\ndirectly computed from the rank of the correct document.\nAverage precision is related to the values presented in Ta-\nble 2, and in fact from the ﬁrst row of Table 3 it can be\nseen that AR gave better results than NG. The values of\nPB and MO are lower, yet they are somehow biased by\nthe number of unretrieved documents. For this reason, av-\nerage precision has been computed also on a reduced set\nof queries, which is different for each of the approaches\nbecause it contains only the ones that retrieved the correct\ndocument in any position. From the second row of Table 3\nit can be seen that MO has an higher average precision,\nshowing that, when the correct document was retrieved,\nits rank was usually higher than for the other approaches.\nTable 3: Average precision for all the queries and for a\nreduced set.\nNG AR PB MO\nWhole query set 0:29 0 :31 0 :21 0 :12\nReduced query set 0:30 0 :32 0 :30 0 :39\n5 CONCLUSIONS\nThis paper presents an overview of different approaches\nto segmentation for extracting music lexical units. At the\nstate of the art, it is not clear which approach is more suc-\ncessful for an effective description of music documents\nbeing also robust to limitations in the query content. To\nthis end, a comparison of four different algorithms in\nterms of index terms size and in terms of retrieval effec-\ntiveness has been carried out using a test collection.\nFrom these results, algorithms based on simple ap-\nproaches, which also allows the presence of overlapping\nlexical units, seem to give better performances when com-\npared to more complex approaches, yet this can be due to\nthe particular implementations of the algorithms and to\nthe repertoire of the test collection. Probably a pattern\nanalysis approach is more suitable for popular music than\napproaches to segmentation created in the context of clas-\nsical music. Moreover, local errors in the queries may be\ncompensated by the overlap between segments, affecting\nthe retrieval results only partially.\nAn interesting result is that, for two of the tested al-\ngorithms, the percentage of queries that did not retrieve\nat all the correct document is very low. Hence, the over-\nall performances in terms of retrieval can be improved by\nreranking the documents using an on-line approach. Thisway, off-line indexing can be used to ﬁlter out great part of\nthe document collection, and a pattern matching approach\ncan be used on a reduced set of the collection with the aim\nof giving a high similarity score to the relevant documents.\nACKNOWLEDGMENTS\nThe work is partially supported by the DELOS Network\nof Excellence on Digital Libraries, as part of the Informa-\ntion Society Technologies (IST) Program of the European\nCommission (Contract G038-507618).\nReferences\n[1]Hu, N., Dannenberg, R.B.: A Comparison of Melodic\nDatabase Retrieval Techniques Using Sung Queries\nInProc. of the ACM/IEEE JCDL , Portland, OR (2002)\n301–307\n[2]Downie, S., Nelson, M.: Evaluation of a Simple\nand Effective Music Information Retrieval Method In\nProc. of the ACM-SIGIR , Athens, GR (2000) 73–80\n[3]Melucci, M., Orio, N.: Combining Melody Process-\ning and Information Retrieval Techniques: Methodol-\nogy, Evaluation, and System Implementation JASIST,\nWiley, V ol. 55, Issue 12 (2004) 1058–1066\n[4]Neve, G., Orio, N.: Indexing and Retrieval of Mu-\nsic Documents through Pattern Analysis and Data Fu-\nsion Techniques In Proc. of the ISMIR , Barcelona, ES\n(2004) 216–223\n[5]Tenney, J., Polansky, L.: Temporal Gestalt Perception\nin Music Journal of Music Theory, V ol. 24, Issue 2\n(1980) 205–241\n[6]Lerdhal, F., Jackendoff, R. A Generative Theory of\nTonal Music MIT Press, Cambridge, MA (1983)\n[7]Cambouropoulos, E.: Musical Rhythm: a Formal\nModel for Determining Local Boundaries In Leman,\nM. (Ed.) Music, Gestalt, and Computing , Springer\nVerlag, Berlin (1997) 277–293\n[8]Lesaffre, M. et al.: The MAMI Query-By-V oice Ex-\nperiment: Collecting and annotating vocal queries for\nmusic information retrieval In Proc. of the ISMIR ,\nBaltimore, MA (2003) 65–71\n[9]MAMI Musical Audio Mining\nhttp://www.ipem.ugent.be/MAMI/ , visited\non April 2005.\n[10] Eerola, T., Toiviainen, P.: MIR in Matlab: The Midi\nToolbox In Proc. of the ISMIR , Barcelona, ES (2004)\n22–27\n[11] Melucci, M., Orio, N.: Evaluating Automatic\nMelody Segmentation Aimed at Music Information\nRetrieval In Proc. of the ACM/IEEE JCDL , Portland,\nOR (2002) 310-311\n627"
    },
    {
        "title": "Factors Affecting Automatic Genre Classification: An Investigation Incorporating Non-Western Musical Forms.",
        "author": [
            "Noris Mohd. Norowi",
            "Shyamala Doraisamy",
            "Rahmita Wirza O. K. Rahmat"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418067",
        "url": "https://doi.org/10.5281/zenodo.1418067",
        "ee": "https://zenodo.org/records/1418067/files/NorowiDR05.pdf",
        "abstract": "The number of studies investigating automated genre classification is growing following the increasing amounts of digital audio data available. The underlying techniques to perform automated genre classification in general include feature extraction and classification. In this study, MARSYAS was used to extract audio features and the suite of tools available in WEKA was used for the classification. This study investigates the factors affecting automated genre classification. As for the dataset, most studies in this area work with western genres and traditional Malay music is incorporated in this study. Eight genres were introduced; Dikir Barat, Etnik Sabah, Inang, Joget, Keroncong, Tumbuk Kalang, Wayang Kulit, and Zapin.  A total of 417 tracks from various Audio Compact Discs were collected and used as the dataset. Results show that various factors such as the musical features extracted, classifiers employed, the size of the dataset, excerpt length, excerpt location and test set parameters improve classification results.",
        "zenodo_id": 1418067,
        "dblp_key": "conf/ismir/NorowiDR05",
        "keywords": [
            "automated genre classification",
            "increasing amounts of digital audio data",
            "underlying techniques",
            "MARSYAS",
            "Weka",
            "dataset",
            "western genres",
            "traditional Malay music",
            "genres",
            "tracks"
        ],
        "content": "FACTORS AFFECTING AUTOMATIC GENRE  CLASSIFICATION:        \nAN INVESTIGATION INCORPORAT ING NON-WESTERN MUSICAL \nFORMS\nNoris Mohd Norowi, Shyamala Doraisamy, Rahmita Wirza  \nFaculty of Computer Science and Information Technology \nUniversity Putra Malaysia \n43400, Selangor, MALAYSIA \n{noris,shyamala,rahmita}@fsktm.upm.edu.my  \nABSTRACT \nThe number of studies investigating automated genre \nclassification is growing follo wing the increasing amounts of \ndigital audio data available. The underlying techniques to perform automated genre classification in general include \nfeature extraction and classification. In this study, MARSYAS \nwas used to extract audio features and the suite of tools \navailable in WEKA was used for the classification. This study \ninvestigates the factors affecting automated genre \nclassification. As for the dataset, most studies in this area work with western genres and traditional Malay music is \nincorporated in this study. Eight genres were introduced; Dikir \nBarat, Etnik Sabah, Inang, J oget, Keroncong, Tumbuk \nKalang, Wayang Kulit , and Zapin .  A total of 417 tracks from \nvarious Audio Compact Discs were  collected and used as the \ndataset. Results show that various factors such as the musical \nfeatures extracted, classifiers employed, the size of the dataset, \nexcerpt length, excerpt location and test set parameters \nimprove classification results.  \n \nKeywords: Genre Classification, Feature Extraction, Music \nInformation Retrieval, Traditional Malay Music \n1 INTRODUCTION \nImprovements in audio compression along with \nincreasing amounts of processing power, hard disk capacity and network bandwidth have resulted in \nincreasingly large number of music files. Easier \ndistribution of digital music through peer-to-peer file \nsharing has also made possible creation of large, digital \npersonal music collection,  typically containing \nthousands of popular songs. Users can now store large \npersonal collections of digital music. These growing \ncollection of digital audio data needs to be classified, \nsorted, organized and retrieved in order to be of any value at all. \nAt present, metadata such as the filename, date \ncreated, etc., are used at large but is very labor-\nintensive, costly and time consuming. A system that \nallows classification of music based on the audio characteristics is therefore highly sought. \nMusical genre is used universally as a common \nmetadata for describing musical content. Genre \nhierarchies are widely used to structure the large \ncollections of music available on the Web. Musical \ngenres are labels created and used by humans for \ncategorizing and describing the vast universe of music [1]. Humans possess the ability to recognize and analyze \nsound immediately based on instrumentation, the \nrhythm and general tone. Furthermore, humans are able to draw connections to other songs that have a similar \nsound and feel. These commonalities make it possible \nfor humans to classify music into different genres.  \nAn automatic genre classification is a system that \nallows structuring and organization of the huge number of archived music automatical ly. The system should also \nbe able to analyze and possibly extract the implicit \nknowledge of these mu sical files that infers the structure \nthat underlies beneath the audio information into a \ncomprehensible form. The analysis includes evaluation \nand comparisons of the feature sets that attempt to \nrepresent the musical content. The general framework for automatic genre classification includes feature \nextraction and classification. Various studies have been \ncarried out toward the de velopment [1,2,3,4].  \nWold et al. [2] extensively discuss the essence of \naudio classification, search and retrieval. Many audio \nfeatures were analysed and compared, such as rhythms, \npitch, duration, loudness and instrument identification, in \nparticular. Their work is not only limited to classification of music but includes classi fication of speech, laughter, \ngender, animal sounds and sound effects. \n Tzanetakis [1] also fou nd that although there has \nbeen significant work in the development of features for \nspeech recognition and music-speech discrimination, \nrelatively little work has been done on developing features to specifically design music signals. Here, he \ndeduces three specific featur es for musical content; \ntimbral texture, rhythmic structure and pitch content to \nbe specific. This is similar to the conclusions from \nAucoturier [3].  \n The features are not the only basis of a genre \nclassification system. Kaminskyj [4] proves that classifiers or machine learni ng algorithms are just as \ncrucial, where Gaussian Mixture Model was used. These approaches work conceivabl y well on western musical \nforms. However, it is very likely that the representation \nused for extraction of one genre is bad at describing \nother genres. Thus, a whole new set of features may be \nrequired in order to cater other musical forms. \nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee \nprovided that copies are not made  or distributed for profit or \ncom-mercial advantage and that copies bear this notice and the \nfull citation on the first page. \n© 2005 Queen Mary, University of London \n13   \n \n Additionally, the classifier, test set parameters and \ndataset pre-processing issues are also taken into consideration in this study in order to get the real \ndescription of their effects on classification.  \nThis paper investigates the factors affecting \nclassification results and e xpands the customary work \nscope of using western musical forms onto other non-\nwestern musical forms, i.e. traditional Malay musical \nforms. The remainder of this paper comprises six \nsections. The effect of comb ining different feature sets \non classification is discussed in Section 2 followed by analysis of direct comparison  between two classifiers in \nSection 3. Section 4 describes the experimental \nframework of this study. Results are revealed in Section \n5. Finally, conclusion and future work are presented in \nSection 6. \n2 FEATURE SETS \nIn music genre recognition, one challenge is the ability \nto differentiate between musical styles. Feature \nextraction is important for this. Feature extraction is the \npart where knowledge of music, psychoacoustics, signal \nprocessing and many other fields is considered. It is a \nprocess where a segment of an audio is characterized into a compact numerical repr esentation. Audio data are \nfairly large to be stored and processing them can take a lot of space and time. This is the reason why typically, the features for audio and speech analysis algorithms are \ncomputed on a frame basis. By doing so, the amount of \ndata that needed to be processed could be reduced.  \nNumerous audio features have been identified by \nvarious studies, each individual one or combination of them is best suited for classi fying a certain audio class, \nbe it between music and speech, between noise and \nmusic, male or female identifier, or musical genre classification. However, it is important to find the right \nfeatures as any classification algorithm will always \nreturn with some kind of result, but a poor feature representation will only yield results that do not reflect \nthe real nature of the underlying data. The extracted \nfeatures will be useful during classification when standard machine learning techniques are used in the \nnext step. \nWorks in Tzanetakis [1] an d Aucoturier [3] proposed \nthat for automatic musical ge nre classification, there are \nthree prescriptive sets of fe ature that should be looked \ninto thoroughly: timbral related features, rhythm related \nfeatures and pitch related features. In order to exploit \nthese features, it is vital to identify the feature space \nwhere all samples belonging to a particular genre must \ncluster closely. At the same  time, clusters corresponding \nto different genres must have a large distance between them. \n2.1 Timbral Related \nThe calculated features ar e based on the short time \nFourier transform and are cal culated for every short-time \nframe of the sound instead of calculating only one timbre value for the entire song although timbral feature is \nsometimes referred to global timbral. \n2.1.1 Spectral Centroid \nThis is the gravity centre of the spectral distribution \nwithin a frame. The centroi d measures the spectral \nshape. Higher centroid values indicate higher \nfrequencies. \n2.1.2 Spectral Roll-Off The Roll-Off is another measure of spectral shape. It is \nthe point where frequency that is below some percentage \n(usually at 85%) of the power spectrum resides. \n2.1.3 Spectral Flux \nThis feature measures frame-to-frame spectral \ndifference. In short, it tells the changes in the spectral \nshape. It is defined as the squared difference between the \nnormalized magnitudes of successive spectral distribution. \n2.1.4 Time Domain Zero Crossing \nThis refers to the number of time –domain zero crossings \nwithin a frame. Zero cro ssings occur when successive \nsamples in a digital signal have different signs. In simple signals, ZCR is directly related to the fundamental frequency. This feature can be used as a measure of \nnoisiness in a signal. \n2.1.5 Mel-Frequency Cepstral Coefficients MFCCs are based on the spectral information of a sound, \nbut are modelled to capture the perceptually relevant \nparts of the auditory spectrum. It is the inverse of Fourier transform. They are thought to capture the perceptually \nrelevant part of the auditory spectrum. Naturally, there \nare thirteen coefficients th at have been found to be \nuseful in speech representation. However, for genre classification, the first five co rrelations appear to be of \nany importance in terms of performance. \n2.2 Rhythm Related \nOther than the timbral related features, another feature \nset that can be made useful is the rhythm related \nfeatures. The beat and rhythmic  structure of a song is a \ngood genre indicator. In Tzanetakis [1], a beat histogram \nis built from autocorrelation function of the signal. The \nbeat histogram provides an outline of the strongness and \ncomplexity of the beat in the music. This is an especially \nremarkable feature in order to discriminate between an \nenergetic genre such as rock and classical where the beat \nis not so accentuated.  \nBeat tracking and rhythm detection is a growing area \nof research nowadays despite it being remarkably \ndifficult to be developed for automated systems. There are a number of reasons why this beat tracking appeared \nso difficult whilst humans manage to recognize the beat \nof the music effortlessly. Humans can easily determine \nthe beat even if the tempo an d metrical structure are not \nexplicitly specified in the beginning of the song. In \n14   \n \n addition, humans can also handle it when the tempo \nchanges throughout the song as opposed to current systems where these are considered major and unsolved \nobstacles yet. Nevertheless, Kosina [5] gives a good \noverview on beat tracking methods.  \nIn terms of traditional Malay musical forms, this is \nseen as a very useful feature set indeed as the genre \nitself is made up of repeating rhythmic patterns. \n2.3 Pitch Related \nErmolinskij [6] attempts at building a genre classifier for \naudio based on pitch features through the use of pitch \nhistogram feature vectors. The histograms reveal some \ngenre-specific information such genre with higher \ndegree of harmonic variation tends to have a fair amount \nof well-pronounced peaks in comparison to those genre \nwith lower degree of harmonic variation such as rock. \n3 CLASSIFIERS \nClassification relies on the basic assumption that \neach observed pattern belong s to a category. Individual \nsignals may be different from one another, but there is a \nset of features that are similar to patterns belonging in \nsame class and different patterns for a different class. The feature sets are the ba se that can be used to \ndetermine class membership. Classification is domain-independent and provides many fast, elegant and well-understood solutions that can be adopted for use in \nmusic recognition. \nThere are a number of key aspects in designing a \ngood classification system. Th e classification system in \nitself must be robust, with tolerable complexity in terms \nof speed and memory usage when used in software.  \nVarious number of classification techniques are \navailable today. Some are better than others in detecting a pattern between the feature vectors. The dissimilarities caused by different underlying models are one of the \nproblems. Likewise, the fact that classification only \ndeals with the feat ure vector representation of the actual \ndata makes it difficult to classify accurately owing to the feature values of signals that  can vary considerably even \nwhen they belong in the same category. Another \ncontributor towards the test of classification is the \nconsiderable variation that is caused by noise. \nCreating a classifier usually means specifying its \ngeneral form. The unknown parameters are estimated through training. Training can be defined as the process using sample data to determine the parameter settings of \nthe classifier, and is essential in all real-world \nclassification system [5]. Classification is often called supervised learning. It involves the first stage of training \nwhere models of a few musical genres are built with \nsome manually labelled data. The second stage involves testing or recognition, where these models are used to \nclassify unlabelled data. It is crucially important to ensure that the test data was not used in any way to create classifier during trai ning. This means that the \ndataset should be divided in some way so that a fraction of it can be used for testing and the remaining part for training. In general, the larger the training sample the better the classifier and the larger the test sample, the more accurate the error estimat e. In order to get a good \nclassifier, a certain kind of arrangement must be made so that one does not jeopardize the other [7].  \nA machine-learning scheme called WEKA (Waikato \nEnvironment for Knowledge Analysis) was engaged to \nevaluate the computer audition applications using trained statistical pattern recognition classifiers. It enables pre-\nprocessing, classifying, clustering, attributes selections \nand data visualizing. WEKA is employed when applying \na learning method to a datase t and during analysis of its \noutput to extract information about the data. The \nlearning methods are called classifiers  [7]. \nAn example of such classifi er is the OneR classifier. \nThis is one of the most primitive schemes. It produces simple rules based on one attribute only. Although it is a \nminimal form of classifier, it can be useful for \ndetermining a baseline performance as a benchmark for other learning schemes. \nAnother well-known classifier is J48. This is just one \nof the many practical learning schemes that can be \napplied to any dataset. J48 classifier forms rules from \npruned partial decision trees built using C4.5’s \nheuristics. C4.5 is Qu inlan’s most recent non-\ncommercial tree-building algorithm. The main goal of \nthis scheme is to minimize the number of tree levels and \ntree nodes, thereby maximizing data generalization. It uses a measure taken from information theory to help \nwith the attribute selection process. For any choice point \nin the tree, it selects the attribute that splits the data so as to show the largest mount of gain in information. \n The J48 classifier described above builds a C4.5 \ndecision tree. Each time the Java virtual machine \nexecutes J48; it creates an instance of this class by \nallocating memory for building and storing a decision tree classifier. The algorithm, the classifier it builds, and \na procedure for outputting the classifier, are all part of \nthat instantiation of the J48 class. \nThe J48 class does not actually contain any code for \nbuilding a decision tree. It includes references to instances of other classes th at do most of the work. It \nalso combines the divide-and-conquer strategy for \ndecision tree and separate divide-and-conquer one for \nrule learning. Such approach adds flexibility and speed.\n \n4 EXPERIMENTAL FRAMEWORK \nA general methodology is formulated in this study with \nthe aim of improving classification results by \ndistinguishing the factors involved and also through \nparameter optimisation. As this problem falls into the \ncategory of supervised mach ine learning, it is a common \napproach to map the training  data into feature vectors \nfirst. Once mapped, one or more classification techniques are applied on this data and a model for distribution underlying the data is created. This model, \nin the final stage will be used to estimate the likelihood \nof a particular category given the test data.  \n15   \n \n 4.1 Experimental Set Up \nTable 1 below sums up the five different experimental \nsets carried out during this study. \nAs can be seen in Table 1, five different experimental \nsets were used. In the first experimental set, the dataset \nsize was used as the variable. This was done to examine \nwhether dataset size play a major role in determining the \nclassification results. The sizes were changed between a \nminimum of ten songs per genre and thirty songs per \ngenre. Since not all genres contain these amounts of \nsong, some genre had to be eliminated altogether. The \nsecond set focused on finding whether the track length \nhas a significant role. Afterwar ds, the starting points of \neach dataset were altered to from starting after minute \ninto the song to starting at point zero of the songs. The \nremaining of the sets dealt with classification parameters such as the number of cross-validation folds \nand classifiers utilized. \n4.2 Dataset Outlook \nIn total, eight traditional Malay musical genres were \nused in this study namely  Dikir Barat, Etnik Sabah, \nInang, Joget, Keroncong, Tumbuk Kalang, Wayang \nKulit , and Zapin . These eight genres were run against \nfive common western genres  (Blues, Classical, Jazz, \nPop and Rock).  \n \nTable 1 . Different experimental sets used when \nconducting the study \n \nThe performing arts of Malaysia are mainly derivative \n[8], influenced by the initial overall Indian and Middle Eastern music during the trade era and later from \ncolonial powers such as Th ailand, Indonesia, Portuguese \nand British who introduce their own culture including \ndance and music.  \nThe taxonomy of traditional Malay music depends on \nthe nature of the theatre forms they serve and their \ninstrumentations. The musical ensembles usually \ninclude gendangs  or drums (membranophone) that is \nused to provide the constant rhythmic beat of the songs, \ngongs (idiophone) to mark the end of a temporal cycle at \nspecific part of the songs, and some other traditional Malay instruments such as the rebab  (chordophone), \nand serunai  or seruling  which resemble a lot like a \nwooden oboe and flute made up of bamboo respectively \n(aerophone). These instruments, in contrast to western \nmusic, which is based on the western tempered scale of twelve semitones, are without such standard [8]. \nHowever, two types of tonal systems are generally applicable; heptatonic seven tone scale and pentatonic \nfive tone scale.  \n4.3 Dataset Treatment  \n417 dataset in total were used. Although this might \nseem like a passable amount of dataset, the number of \nsongs per genre was not consistent. Western musicals were much easier to obtain and undeniably, huge \nportion of the dataset were made up of them. However, \nan adequate amount of dataset for traditional Malay \nsongs managed to be collected. \nThe dataset were obtained from various sources, \nmainly from Audio Compact Di scs and some were also \ndownloaded via the Internet . While downloading from \nthe Internet appeared  a trivial task for western songs, the \nsame could not be said for traditional Malay songs. As \nconfirmed in [9], it is evident that traditional Malay \nmusical culture is in the verge the corrosion, which signifies the complications in getting large dataset as \noriginally intended. Various individuals and \norganizations provided these musical dataset including the Malaysia National Arts Academy, Sultan Salahuddin \nAbdul Aziz Shah’s Cultural and Arts Centre at \nUniversiti Putra Malaysia and also personal collections of audio CDs from many individuals. \nThe dataset came in an asso rtment of audio formats. \nThese were later converted into wav format using standard audio editing tool. Wav format was chosen as \nit was the only format supported by the free feature \nextractor called MARSYAS, which will be discussed \nnext.  \nAfter format conversion, these data were trimmed \ninto a uniformed le ngth each. A standard length for all \nwas required because each song varied lengthwise and it was thought that with this could be one of the factors affecting classification results. In addition, based \nexisting work of [1], the dataset used were also trimmed \ninto a neat collection of thirty seconds each. \nFurthermore, using full-length music takes up too much \nspace and is likely to incr ease computational load. \nThese clips were used throughout the experiments, both \nfor training and testing. \n4.2 Feature Extraction \nFor each of the song clips, th e features were extracted to \nfacilitate automatic music genre classification. This was \ndone through MARSYAS; a fr ee framework that enables \nthe evaluation of computer audition applications. The \nMusical Research System fo r Analysis and Synthesis \n(MARSYAS) is a semi-autom atic music classification \nsystem that is developed as an alternative solution for the \nexisting audio tools that are incapable of handling the \nincreasing amount of computer data [10].  \nWhen used for music genre classification, it performs \nnotably better than by chance. It utilizes the three feature \nsets for representing the timbral texture, rhythmic content and pitch content of the music signals and uses  Set Factors \n1 Dataset Size \n2 Dataset Track Length \n3 Dataset Starting Point \n4 Number of Cross-Validation Folds \n5 Classifiers \n16   \n \n  \n \n \ntrained statistical pattern recognition classifiers for \nevaluation. The feature ex tractor will end up in \nnumerical results in the form of an ARFF file. \nAs of the time of the experiment was conducted, the \nexact same feature was used to extract audio features \nfrom the signals, i.e. the means and variances of multi-\ndimensional features such as Spectral Centroid, Spectral \nFlux, Spectral Roll-Off, Zero Crossings and the Low Energy were engaged in this experiment.  \n4.3 Classification \nThe ARFF files that were generated by the feature \nextractor containing representa tions of all the music files \nwere used to train a J48 classifier in WEKA. It enables pre-processing, classifying, clustering, attributes selection and data visualizing. In order to achieve the \nobjective of this study, the features that distinctively \nclassify traditional Malay music must be identified and \nutilized.  \n     Classification results were tested using stratified \nthree-fold cross-validation, six-fold cross-validation and \nten-fold cross validation resp ectively. Cross-validation is \na standard evaluation technique in pattern classification, in which the dataset is split into n parts (folds) of equal \nsize. n-1 folds are used to tr ain the classifier. The nth \nfold that was held out is then used to test it. \n5 RESULTS \n   The first part of this study was to observe the \nbehaviour of the classification system, as this was a first \nattempt at incorporating non-western music. Table 2 shows the confusion   \n \n \nmatrix where the columns correspond to the actual genre \nand the rows to the predicted genre. In this particular \nconfusion matrix, the labe ls each correspond to a \nparticular genre. The name of each label can be referred \nbelow the table along with the number of songs available per genre.  \nIt can be clearly seen that the number of songs per \ngenre are not consistent. Some may have as little as 7 songs per genre ( Bongai  and Muzik Asli) , which is \nespecially true with trad itional Malay music, whereas \nsome can be over a hundred songs per genre.  \nThe imbalance in terms of number might result in a \nbiased classification. In order to avoid this, these genres had to be eliminated altogether. \nThe diagonal pattern starting from the top left hand \ncorner of the table towards the bottom right hand of the \ntable illustrate number of correct classification. The dispersed numbers outside the diagonal pattern tell the \nnumber of misclassifications. In this particular test set, \nthe expected diagonal pattern is present, though the classification itself is not pe rfect, even when using the \nexact same dataset for both training and testing. This suggests that there are more  factors that contribute \ntowards developing an ideal system for automatic genre classification. \nSince the result dataset were not pre-processed \nbeforehand, and there is no standardized number of dataset for any particular genr e, it was believed that with \ndata pre-processing, classification results could be \nenhanced. \n     Classification was then performed using the modified dataset based on the factors listed in Table 1. Results \nwere evaluated in terms of accuracy and reliability [11].   A B C D E F G H I J K L M N O P Acc \nA 26         1   2  1  87 \nB  5        2        7 1  \nC 1  33              9 4  \nD    30        1      9 7  \nE     12            1 0 0  \nF      7      1     88 \nG 1       7 1     1    70 \nH 1  1  1   13  2      1 68 \nI 1   1     10    1 1 1  67 \nJ   2  1     31       9 1  \nK   1   1     5      7 1  \nL 1   1   1   1  63     94 \nM    2 1   1 2   4 95    90 \nN   1            12   92 \nO            1    15 1 88 \nP             3  1   4 40 \nRel 84 100 85 86 80 88 88 87 83 84 100 91 93 80 88 67  \nA : Blues                                      E : Etnik  Sabah                               I : Joget                              M : Rock \nB : Bongai                                    F : Gamelan                                    J : Keroncong                      N : Tumbuk  Kalang  \nC : Classical                                G : Inang                                          K : Muzik  Asli                    O : Wayang  Kulit  \nD : Dikir  Barat                             H : Jazz                                           L : Pop                                P : Zapin  Table 2.  Confusion Matrix for Set 1 (General ) \n17   \n \n Classification accuracy indicates how many of the test \nsamples were correctly classified whilst classification reliability discloses the confidence level that can be \nplaced on the classifier results. The results are discussed \nbelow. \n1) Larger dataset size is favourable \n     Whilst appointing J48 classifier at 10 songs per \ngenre, 84% of correct classi fication had been achieved.  \nThe examination was then continued by setting a higher \nnumber of minimum songs per genre. At 30 songs per \ngenre, some genres had to be removed from the list, \nleaving only six genres altogether (Blues, Classical, \nDikir Barat , Etnik Sabah , Pop, Rock).  Again, the \nparameters were kept as above and ultimately, there was \na slight increase (90%) in th e classification result. This \nshows consistent numbers of songs per genre, results in \nbetter and unbiased classi fication accuracy. Figure 1 \nfurther illustrates the results of the two dataset sizes. \n \n80859095\nMean Accuracy Mean Reliab ilityPercentage Correct10\nsongs\n30songs\n  \nFigure 1 . Classification performance between \ntwo different dataset sizes  \n2) Excerpt length need not be long.  \nIt was originally thought that the longer the duration of a \nmusical excerpt, the better the classification accuracy. \n30 seconds was chosen as the default value for classification in all the experimental set in this study as \nused by previous researchers in their respective research \n[1]. Maintaining the same parameters as before, tracks of 10 seconds, 30 seconds and 60 seconds were tested.  \nIn Figure 2, it can be seen that for 90% accuracy and \nreliability were achieved with 30 seconds tracks and \n93% when used with 60 seconds tracks. Although \nresults appear better when the track length is extended to 60 seconds, th e increase is minute in comparison to \nthe storage cost required an d the heavier computational \nload, as audio data are huge in size. Interestingly, when tested with 10 seconds tracks, the result was better than \nthe previous two. This outcome implies that the standard \n30 seconds used by many researchers may not be the \nbest length for genre classification. 859095100\nMean Accuracy Mean Reliab ilityPercentage Correct10 sec\n30 sec\n60 sec\n \nFigure 2 . Classification performance between \nthree varying track lengths \n3) The first few seconds of a song are crucial genre \nindicator \nIt was hypothesized that in order to get a better ‘feel’ \nof the genre of a song, it is better to process the tracks halfway into the song where all the actions and \ninstruments can be heard. In  this set, the exact tracks \nwere processed twice; the first batch was trimmed at one \nminute into the song while the second batch was taken \nfrom the beginning. The length of both batches was kept at 30 seconds long. \nIt is astounding to find that classification performed \nbetter on tracks that started from the beginning \ncompared to tracks that ar e halfway into the song \n(Figure 3). When a song reaches the middle part, the chorus comes in, all the inst ruments are played, the song \nusually becomes more dynamic and alive, and the \noverall energy of the audio signal gets higher. This \nexplains why sometimes classical can be misclassified \nas rock. It is possible that due to these reasons, it is \neasier to obtain higher classification results with tracks that began from the beginning of the song. \n \n859095100\nMean Accuracy Mean Reliab ilityPercentage Correct0:01:00\n0:00:00\n  \nFigure 3.  Classification performance between                             \ntwo different starting points \n18   \n \n 4) Higher Cross Validation Folds may not always \nattain better results  \nInvestigation on the effect of  increasing number of folds \nin cross-validation were done to uncover whether it \nreally is better to have at least ten or more folds as ten seems to be the default value used by many [7].  \nAt three folds, the accuracy and reliability of the \nresult was at 72%. When increased to six folds, the \nperformance also increased  to 75%. When increased \nfurther at ten folds, the performance dropped slightly to 74%. The number remained at  74% when further tested \nat 15 folds. \nIt appears that after a certain point, increasing the \nnumber of folds would not be of any aid in classification \nperformance. With this particular dataset, six-folds is \nseen to be the optimal fo lds (see Figure 4). However, \nthis does not mean that six folds is best for every \nclassification problem, and that  perhaps ten folds is also \na safe number to rely on as it does not effect the results in a major way.   \n5) Classifiers must be chosen according to specific needs to ensure higher results accuracy \nTwo different classifiers were tested; OneR and J48 to \ndemonstrate this point. With OneR classifier, results \nwere just above satisfactor y at 67% accuracy and 44% \nreliability. J48 classifier, on the other hand, boosted the \nclassification performance to  75% overall. This just \nshows that some classifiers are suitable in performing a \nclassification problem while others may not. \n6 CONCLUSION \nResults show that the audio classification can be \nimproved by taking into consideration the factors such as dataset size, track length and track location, the number \nof cross-validation folds and utilizing the suitable \nclassifiers. \nOverall, it is best to use tracks that are no more than \n30 seconds in length, starts from the beginning of the \ntrack and apply J48 classifier for categorical \nclassification.  In addition, this study supports the theory \nthat states larger dataset are favourable in order to come \nup with a better classification result.  \nIt is also clear that audio classification of traditional \nMalay music is possible with existing genre \nclassification tools which thus far has been used only \ninvestigated using western musical genres. Expanding \nthe scope of automatic genre classification beyond \nwestern musical forms proves that classification of other non-western music is also achievable and can be used \njust as good.  \nFuture work includes investigating the specific \nfeatures that improve classification performance \ntraditional Malay music and to work on a larger dataset. \n 70727476\n0369 1 2 1 5 1 8\nNumber of FoldsPercentage Correct\n \nFigure 4.  Classification performance between \ndifferent numbers of cross-validation folds \n  \n01020304050607080\nMean Accuracy Mean Reliab ilityPercentage CorrectOneR\nJ48\n \nFigure 5 . Classification performance between \ntwo different classifiers \nREFERENCES \n[1] Tzanetakis, George and Perry Cook \"Musical Genre \nClassification of Audio Signals\", IEEE Transactions \non Speech and Audio Processing , 10(5), July 2002. \n[2] Wold, E., Blum, T., Keislar, D., and Wheaton, J. \n“Content-based Classification, Search, and Retrieval \nof Audio.” IEEE Multimedia, Vol. 3, No.3, pp. 27-\n36, Fall 1996. \n[3] Aucoturier, J., and Pachet , F. “Representing Musical \nGenre: A State of the Art”. Journal of New Music \nResearch. Vol. 32, No. 1, Nov. 1993, pp. 83-93. \n \n \n \n19   \n \n [4] Kaminskyj, I. “Multi-f eature Musical Instrument \nSound Classifier.” Procee dings ACMA Conference, \npp. 46-54. \n[5] Kosina, K. Music Genre Recognition. Diploma \nThesis,  Hagenberg University, June 2002. \n[6] Ermolinskiy, A., Cook, P., and Tzanetakis, G. \n“Musical Genre Classificati on based on the analysis \nof harmonic content features in audio and midi. Work \nReport,  Princeton University. \n[7] Witten, I., and Frank E. Data Mining: Practical \nMachine Learning Tools and Techniques with Java \nImplementations. Morgan Kauffman Publisher, 1999. \n[8]  Mohd Ghouse Nasuruddin, The Malay Traditional \nMusic. Kuala Lumpur: Dewan Bahasa dan Pustaka, \n(1992). \n[9] Shriver, R. “Digital Stereo Recording of Traditional \nMalaysian Musical Instruments. Audio Engineering \nSociety Convention Paper”. March 2003. \n[10] George Tzanetakis and Perry Cook \"MARSYAS: A \nFramework for Audio Analysis\" Organized Sound, \nCambridge University Press 4(3), 2000 \n[11] Pandya A. S., and Macy, R. B. “Pattern Recognition \nwith Neural Networks in C++”, CRC Press, 1996. \n \n20"
    },
    {
        "title": "A Probabilistic Model for Chord Progressions.",
        "author": [
            "Jean-François Paiement",
            "Douglas Eck",
            "Samy Bengio"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416922",
        "url": "https://doi.org/10.5281/zenodo.1416922",
        "ee": "https://zenodo.org/records/1416922/files/PaiementEB05.pdf",
        "abstract": "Chord progressions are the building blocks from which tonal music is constructed. Inferring chord progressions is thus an essential step towards modeling long term dependencies in music. In this paper, a distributed representation for chords is designed such that Euclidean distances roughly correspond to psychoacoustic dissimilarities. Estimated probabilities of chord substitutions are derived from this representation and are used to introduce smoothing in graphical models observing chord progressions. Parameters in the graphical models are learnt with the EM algorithm and the classical Junction Tree algorithm is used for inference. Various model architectures are compared in terms of conditional out-of-sample likelihood. Both perceptual and statistical evidence show that binary trees related to meter are well suited to capture chord dependencies. 1",
        "zenodo_id": 1416922,
        "dblp_key": "conf/ismir/PaiementEB05",
        "keywords": [
            "Chord progressions",
            "tonal music",
            "modeling long term dependencies",
            "dissimilarities",
            "Euclidean distances",
            "probabilities of chord substitutions",
            "graphical models",
            "EM algorithm",
            "Junction Tree algorithm",
            "model architectures"
        ],
        "content": "AProbabilistic Model forChord Progressions\nJean-Franc ¸oisPaiement\nIDIAP Research Institute\nRueduSimplon 4\nCase Postale 592\nCH-1920 Martign y\nSwitzerland\npaiement@idiap.chDouglas Eck\nDep. ofComputer Science\nandOperations Research\nUniversity ofMontreal\nCP6128 succ Centre-V ille\nMontr ´eal,QC\nH3C 3J7, Canada\neckdoug@iro.umontreal.caSamy Bengio\nIDIAP Research Institute\nRueduSimplon 4\nCase Postale 592\nCH-1920 Martign y\nSwitzerland\nbengio@idiap.ch\nABSTRA CT\nChord progressions arethebuilding blocks from which\ntonal music isconstructed. Inferring chord progressions\nisthus anessential step towards modeling long term de-\npendencies inmusic. Inthispaper ,adistrib uted repre-\nsentation forchords isdesigned such thatEuclidean dis-\ntances roughly correspond topsychoacoustic dissimilar -\nities. Estimated probabilities ofchord substitutions are\nderivedfrom thisrepresentation andareused tointroduce\nsmoothing ingraphical models observing chord progres-\nsions. Parameters inthegraphical models arelearnt with\ntheEM algorithm andtheclassical Junction Treealgo-\nrithm isused forinference. Various model architectures\narecompared interms ofconditional out-of-sample like-\nlihood. Both perceptual andstatistical evidence showthat\nbinary trees related tometer arewell suited tocapture\nchord dependencies.\n1Introduction\nProbabilistic models foranalysis andgeneration ofpoly-\nphonic music would beuseful inabroad range ofapplica-\ntions, from conte xtual music generation toon-line music\nrecommendation andretrie val.However,modeling music\ningeneral involveslong term dependencies intime series\nthathaveprovedverydif\u0002cult tocapture with traditional\nstatistical methods. Note thattheproblem oflong-term\ndependencies isnotlimited tomusic, nortooneparticular\nprobabilistic model (Bengio etal.,1994). This dif\u0002culty\nmotivates ourexploration ofchord progressions. Chord\nprogressions constitute a\u0002xed,non-dynamic structure in\ntime andthus canbeused toaidindescribing long-term\nmusical structure.\nOne ofthemain features oftonal music isitsorgani-\nzation around chordprogressions .Achord isagroup of\nthree ormore notes (generally \u0002veorless). Achord pro-\ngression issimply asequence ofchords. Ingeneral, the\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonchord progression itself isnotplayed directly inagiven\nmusical composition. Instead, notes comprising thecur-\nrentchord actascentral polarities forthechoice ofnotes\natagivenmoment inamusical piece. Giventhatapartic-\nulartemporal region inamusical piece isassociated with\nacertain chord, notes comprising that chord orsharing\nsome harmonics with notes ofthatchord aremore likely\ntobepresent. Intypical tonal music, most chord progres-\nsions arerepeated inacyclic fashion asthepiece unfolds,\nwith each chord having ingeneral alength equal tointeger\nmultiples oftheshortest chord length.\nChord changes tend toalign with metrical boundaries\ninapiece ofmusic. Meter isthesense ofstrong andweak\nbeats thatarises from theinteraction among ahierarch yof\nnested periodicities. Such ahierarch yisimplied inWest-\nernmusic notation, where different levelsareindicated by\nkinds ofnotes (whole notes, halfnotes, quarter notes, etc.)\nandwhere bars establish measures ofanequal number of\nbeats (Handel, 1993). Forinstance, most contemporary\npopsongs arebuiltonfour-beat meters. Insuch songs,\nchord changes tend tooccur onthe\u0002rst beat, with the\n\u0002rstandthird beats (orsecond andfourth beats insynco-\npated music) being emphasized rhythmically .Chord pro-\ngressions strongly in\u0003uence melodic structure inaway\ncorrelated with meter .Forexample, injazz impro visa-\ntionnotes perceptually closer tothechord progression are\nmore likelytobeplayed onmetrically-accented beats with\nmore dissonant notes played onweak erbeats. Fora\ncomplete treatment oftheroleofmeter inmusical struc-\nture, seeCooper andMeyer(1960).\nThis strong link between chord structure andoverall\nmusical structure motivates ourattempt tomodel chord\nsequencing directly .Thespace ofsensible chord progres-\nsions ismuch more constrained than thespace ofsensible\nmelodies, suggesting thatalow-capacity model ofchord\nprogressions could form animportant partofasystem that\nanalyzes orgenerates melodies. Asanexample, consider\nblues music. Most blues compositions arevariations of\nabasic same 12barchord progression1.Identi\u0002cation of\nthatchord progression inasequence would greatly con-\ntributetogenre recognition.\nInthispaper wepresent agraphical model thatcap-\ntures thechord structures inagivenmusical style using as\n1Inthispaper ,chord progression areconsidered relati veto\nthekeyofeach song. Thus, transposition ofawhole piece has\nnoeffectonouranalysis.\n312evidence alimited amount ofsymbolic MIDI2data. One\nadvantage ofgraphical models istheir \u0003exibility ,suggest-\ningthatourmodel could beused either asananalytical or\ngenerati vetooltomodel chord progressions. Moreo ver,a\nmodel likeours canbeintegrated into amore comple x\nprobabilistic transcription model (Cemgil, 2004), genre\nclassi\u0002er ,orautomatic composition (Eck andSchmidhu-\nber,2002).\nCemgil (2004) uses asome what comple xgraphical\nmodel thatgenerates amapping from audio toapiano-\nrollusing asimple model forrepresenting note transitions\nbased onMark ovian assumptions. This model takesasin-\nputaudio data, without anyform ofpreprocessing. While\nbeing verycostly ,thisapproach hastheadvantage ofbeing\ncompletely data-dependent. However,strong Mark ovian\nassumptions arenecessary inorder tomodel thetempo-\nraldependencies between notes. Hence, aproper chord\ntransition model could beappended tosuch atranscrip-\ntion model inorder toimpro vepolyphonic transcription\nperformance.\nRaphael andStoddard (2003) usegraphical models for\nlabeling MIDI data with traditional Western chord sym-\nbols. Inthiswork,aMark ovian assumption ismade such\nthat each chord symbol depends only onthepreceding\none. This assumption seems suf\u0002cient toinfer chord sym-\nbols, butweshowinSection 4thatlonger term dependen-\nciesarenecessary tomodel chord progressions bythem-\nselvesinagenerati veconte xt,without regardtoanyform\nofanalysis.\nLavrenk oand Pickens(2003) propose agenerati ve\nmodel ofpolyphonic music thatemplo ysMark ovrandom\n\u0002elds. Though themodel isnotrestricted tochord pro-\ngressions, thedependencies itconsiders aremuch shorter\nthan inthepresent work. Also, octaveinformation isdis-\ncarded, making themodel unsuitable formodeling realis-\nticchord voicings. Forinstance, lownotes tend tohave\nmore salience inchords than high notes (Levine, 1990).\nAllan andWilliams (2004) designed aharmonization\nmodel forBach chorales using Hidden Mark ovModels.\nAharmonization isaparticular choice ofnotes givena\nsequence ofchord labels. While generating excellent mu-\nsical results, thismodel hastobeprovided sequences of\nchords asinput, restricting itsapplicability inmore gen-\neralsettings. Our workgoes astep further bymodeling\ndirectly chord progressions inanunsupervised manner .\nThis allowsourproposed model tobedirectly appended\ntoanysupervised model without theneed foradditional\ndata labeling.\nInSection 2,weintroduce asimilarity measure for\nchords guided bypsychoacoustic considerations. Aprob-\nabilistic model forchord progressions isthen described in\nSection 3.Themodel uses ourproposed similarity mea-\nsure forchords todistrib utetheprobability mass ofthe\ntraining settounseen eventsappropriately .InSection 4.1\nweevaluate thelikelihood ofthemodel against reference\ndata. Finally ,inSection 4.2weshowthatchord sequences\ngenerated bytheproposed model aremore realistic than\ntheones generated bysimpler models interms ofglobal\ndependencies.\n2Inourwork,weonly consider notes onsets andoffsets inthe\nMIDI signal.2Chord Similarities\nThegeneralization performance ofagenerati vemodel de-\npends strongly onhowobserv eddata isrepresented. If\nwehadanin\u0002nite amount ofdata, wecould simply repre-\nsenteach chord asadistinct observ ation without including\ninthemodel anyspeci\u0002c knowledge about psychoacous-\nticsimilarities between chords. Unfortunately ,typical\nsymbolic music databases areverysmall compared tothe\ncomple xityofthepolyphonic music signal. Hence, ause-\nfulstatistical model forchord progressions hastoinclude\nnotions ofpsychoacoustic similarity between chords inor-\ndertoredistrib uteef\u0002ciently acertain amount ofprobabil-\nitymass tounseen eventsduring training.\nOne possibility wechose nottoconsider wastorep-\nresent directly some attrib utes ofWestern chord notation\nsuch asminor, major, diminished, etc.Though in-\nferring these chord qualities could haveaided inbuilding\nasimilarity measure between chords, wefound itmore\nconvenient tobuildamore general representation directly\ntiedtotheacoustic properties ofchords. Another possi-\nbility fordescribing chord similarities isset-class theory ,\namethod thathasbeen compared topercei vedcloseness\n(Kuusi, 2001) with some success. Afullreviewofpsy-\nchological issues such aspercei vedchord distance isout\nofthescope ofthispaper .SeeKrumhansl (1990) fora\ngood introduction.\nInthispaper ,weconsider asimpler approach where\neach group ofobserv ednotes forming achord isseen as\nasingle timbre (Vassilakis, 1999). From thistimbre infor -\nmation, wederiveacontinuous distrib uted representation\nwhere perceptually similar chords tend also tobeclose\ninEuclidean distance. One candesign agraphical model\nthatdirectly observ esthese continuous representations of\nchords. However,thisapproach would sufferfrom two\nmajor drawbacks. First, itisunnatural tocompress dis-\ncrete information inacontinuous space; onecould easily\nthink ofaone-dimensional continuous representation that\nwould over\u0002tanydiscrete dataset. Second, since thesetof\nlikelychords is\u0002nite, onewants toobserv edirectly dis-\ncrete variables with a\u0002nite number ofpossible states.\nOurproposed solution tothisproblem istoconvertthe\nEuclidean distances between chord representations into\nprobabilities ofsubstitution between chords. Chords can\nthen berepresented asindividual discrete events. These\nprobabilities can beincluded directly into agraphical\nmodel forchord progressions, asdescribed inSection 3.\nItisinteresting tonote that theproblem ofconsidering\nsimilarities between discrete objects instatistical models\nisnotrestricted tomusic andencompasses alargespan\nofapplications, including natural language processing and\nbiology .\n2.1 Continuous Repr esentation\nThe frequenc ycontent ofanidealized musical noteiis\ncomposed ofafundamental frequenc yf0;iand integer\nmultiples ofthat frequenc y.The amplitude oftheh-th\nharmonic fh;i=hf1;iofnoteicanbemodeled with ge-\nometric decaying \u001ah,with0<\u001a<1(Valimaki etal.,\n1996).\n313Consider thefunction\nm(f)=12(log2(f)\u0000log2(8:1758))\nthat maps frequenc yftoMIDI notem(f).LetX=\nfX1:::Xsgbethesetoftheschords present inagiven\ncorpus ofchord progressions. Then, foragivenchord\nXj=fi1;:::;itjgwithtjthenumber ofnotes inchord\nXj,weassociate toeach MIDI notenapercei vedloud-\nness\nlj(n)=max\nh2N;i2Xj(f\u001ahjround (m(fh;i))=ng[f0g)\n(1)\nwhere thefunctionround maps arealnumber tothenear-\nestinteger.Themax function isused instead ofasum in\norder toaccount forthemasking effect (Moore, 1982).\nThe quantization givenbytherounding function corre-\nsponds tothefactthatmost ofthetonal music iscomposed\nusing thewell-temper edtuning .Forinstance, the3rdhar-\nmonic f3;icorresponds toanotei+7which islocated one\nperfect \u0002fth (i.e.7semi-tones) overthenoteicorrespond-\ningtothefundamental frequenc y.Building thewhole set\nofpossible notes from that principle leads toasystem\nwhere \u0003atandsharp notes arenotthesame, which was\nfound tobeimpractical bymusical instrument designers\ninthebaroque era. Since then, most Western musicians\nused acompromise called thewell-tempered scale, where\nsemi-tones areseparated byanequal ratio offrequencies.\nHence, therounding function inEquation (1)provides a\nfrequenc yquantization thatcorresponds towhat anaver-\nagecontemporary music listener experiences onaregular\nbasis.\nForeach chord Xj,wethen haveadistrib uted rep-\nresentation lj=flj(n1);:::;lj(nd)gcorresponding to\nthepercei vedstrength oftheharmonics related toev-\nerynotenkofthewell-tempered scale, where wecon-\nsider thed\u0002rst notes ofthisscale toberelevant. For\ninstance, onecansettherange ofthenotesn1tondto\ncorrespond toaudible frequencies. Using octaveinvari-\nance, wecangofurther andde\u0002ne achord representation\nvj=fvj(0);:::;vj(11)gwhere\nvj(i)=X\nnk:1\u0014k\u0014d;(nkmod12)=il(nk): (2)\nThis representation givesameasure oftherelati vestrength\nofeach pitch class3inagivenchord. Forinstance, value\nvj(0)isassociated with pitch classc,valuevj(1)topitch\nclasscsharp, andsoon.\nThroughout thispaper ,wede\u0002ne chords bygiving the\npitch class letter ,sometimes followedbysymbol#(sharp)\ntoraise agivenpitch class byonesemi-tone. Finally ,each\npitch class isfollowed byadigit representing theactual\noctavewhere thenote isplayed. Forinstance, thesymbol\nc1e2a#2d3 stands forthe4-note chord\u0000\u0002\u0001\u0001\u0001\u0001\n\u0003 \u0004\n3Allnotes with thesame note name (e.g. C#)aresaid tobe\npartofthesame pitchclass .Table 1:Euclidean distances between thechord inthe\u0002rst\nrowandother chords when chord representation isgiven\nbyEquation (2),choosing \u001a=0:97.\nc1a2e3g3 0.000 c1d#2a#2d3 0.000\nc1a2c3e3 1.230 c1a#2d#3g3 1.814\nc1a2d3g3 1.436 c1e2a#2d#3 2.725\nc1a1d2g2 2.259 c1a#2e3g#3 3.442\nc1a#2e3a3 2.491 c1e2a#2d3 3.691\na0c3g3b3 2.920 a#0d#2g#2c3 3.923\nc1e2b2d3 3.162 a#0d2g#2c3 4.155\nc1g2c3e3 3.398 g#1g2c3d#3 4.363\na0g#2c3e3 3.643 c1e2a#2c#3 4.612\nc1f2c3e3 3.914 a#1g#2d3g3 4.820\nc1d#2a#2d3 4.295 f1a2d#3g3 5.030\ne1e2g2c3 4.548 d1f#2c3f3 5.267\ng1a#2f3a3 4.758 a0c3g3b3 5.473\ne0g2d3f#3 4.969 g1f2a#2c#3 5.698\nf#0e2a2c3 5.181 b0d2a2c3 5.902\ng#0g2c3d#3 5.393 e1d3g3b3 6.103\nf#1d#2a2c3 5.601 f#1e2a#2d#3 6.329\ng0f2b2d#3 5.818 d#1c#2f#2a#2 6.530\ng1f2a#2c#3 6.035 g#0b2f3g#3 6.746\ng1f2b2d#3 6.242 b0a2d#3g3 6.947\nwith aconthe\u0002rstoctave,aneandanasharp (b\u0003at)on\nthesecond octave,and\u0002nally adonthethird octave.\nWeseeinFigure 1that therepresentation givenby\nEquation (2)provides similar results fortwodifferent\nvoicings oftheCmajor chord, asde\u0002ned inLevine\n(1990). Wehavealso computed Euclidean distances be-\ntween chords induced bythisrepresentation andfound\nthattheyroughly correspond toperceptual closeness, as\nshowninTable 1.Each column givesEuclidean distances\nbetween thechord inthe\u0002rstrowandsome other chords\nthatarerepresented asdescribed here. The trained mu-\nsician should seethatthese distances roughly correspond\ntopercei vedcloseness. Forinstance, thesecond column\nisrelated toaparticular inversion oftheCminor chord\n(c1d#2a#2d3 ).Weseethat theclosest chord inthe\ndataset (c1a#2d#3g3 )isthesecond inversion ofthe\nsame chord, asdescribed inLevine (1990). Hence, we\nraise thenote d#3byoneoctaveandreplace thenote d3\nbyg3(separated byaperfect fourth). These twonotes\nshare some harmonics, leading toaclose vectorial repre-\nsentation. This distance measure could haveconsiderable\ninterest inabroad range ofcomputational generati vemod-\nelsinmusic aswell asformusic composition.\n2.2 Probabilities ofSubstitution\nAsalready pointed outinSection 2,there isnodirect way\ntorepresent Euclidean distances between discrete objects\ninthegraphical model frame work. Considering thepartic-\nularproblem ofchord progressions, itishoweverpossible\ntoconverttheEuclidean distances described insection 2.1\nintoprobabilities ofsubstitution between chords inagiven\n314C Cs D Ds E F Fs G Gs A As B−2−1.5−1−0.500.511.52c1b2e3g3\nPitch−classPerceptual\nemphasis  \nC Cs D Ds E F Fs G Gs A As B−2−1.5−1−0.500.511.522.5\nPitch−classPerceptual\nemphasis  c1e2b2d3\nFigure 1:Normalized values givenbyEquation (2)for2voicings oftheCmajor chord. Weseethatperceptual emphasis\nishigher forpitch classes present inthechord. These twochord representations havesimilar values forpitch classes that\narenotpresent ineither chords, which makestheir Euclidean distance small.\ncorpus ofchord progression.\nOne can de\u0002ne theprobability pi;jofsubstituting\nchordXiforchordXjinachord progression as\npi;j=\u001ei;jP\n1\u0014j\u0014s\u001ei;j(3)\nwith\n\u001ei;j=expf\u0000\u0015jjvi\u0000vjjj2g\nwith freeparameter 0\u0014\u0015<1.Theparameters \u0015and\n\u001a(from Equation (1))canbeoptimized byvalidation on\nanychord progression dataset provided asuitable objec-\ntivefunction. Withpossible values going from 0toarbi-\ntrary high values, theparameter \u0015allowsthesubstitution\nprobability table togofrom theuniform distrib ution with\nequal entries everywhere (such thateverychord hasthe\nsame probability ofbeing played) totheidentity matrix\n(which disallo wanychord substitution). Table 2shows\nsubstitution probabilities obtained from Equation (3)for\nchords inTable 1.\n3Graphical Model\nGraphical models (Lauritzen, 1996) areauseful frame-\nworktodescribe probability distrib utions where graphs\nareused asrepresentations foraparticular factorization\nofjoint probabilities. Vertices areassociated with random\nvariables. Iftwovertices arenotlinkedbyanedge, their\nassociated random variables areconsidered tobeuncon-\nditionally independent. Adirected edge going from the\nvertexassociated with variable Atotheonecorrespond-\ningtovariable Baccounts forthepresence oftheterm\nP(BjA)inthefactorization ofthejoint distrib ution for\nallthevariables inthemodel. The process ofcalculat-\ningprobability distrib utions forasubset ofthevariables\nofthemodel giventhejoint distrib ution ofallthevari-\nables iscalled marginalization (e.g. derivingP(A;B)fromP(A;B;C)).Thegraphical model frame workpro-\nvides ef\u0002cient algorithms formarginalization andvarious\nlearning algorithms canbeused tolearn theparameters of\namodel, givenanappropriate dataset.\nWenowpropose agraphical model forchord se-\nquences using theprobabilities ofsubstitution between\nchords described inSection 2.2.Themain assumption be-\nhind theproposed model isthatconditional dependencies\nbetween chords inatypical chord progression arestrongly\ntiedtothemetrical structure associated with it.Another\nimportant aspect ofthismodel isthatitisnotrestricted to\nlocal dependencies, likeasimpler Hidden Mark ovModel\n(HMM) (Rabiner ,1989) would be.This choice ofstruc-\nture re\u0003ects thefactthat achord progression isseen in\nthismodel asatwodimensional architecture. Everychord\ninachord progression depends both onitsposition in\nthechord structure (global dependencies) andonthesur-\nrounding chords (local dependencies). Weshowempiri-\ncally inSection 4thatconsidering both aspects leads to\nbetter generalization performance aswell asbetter gener -\nated results than byonly considering local dependencies.\nVariables inagraphical model canbeeither observ ed\ninthedataset orhidden (i.e. notpresent inthedataset but\nstillpresent inthemodel). TheExpectation-Maximization\n(EM) algorithm (Dempster etal.,1977) canbeused toes-\ntimate theconditional probabilities ofthehidden variables\nofaprobabilistic model. This algorithm proceed intwo\nsteps applied iterati velytoeach observ ations inadataset\nuntil convergence oftheparameters. First, theEstepcom-\npute theexpectation ofthehidden variables, giventhecur-\nrent parameters ofthemodel andoneobserv ation inthe\ndataset. Secondly ,theMstepupdate thevalues ofthepa-\nrameters inorder tomaximize thelikelihood ofthesame\nobserv ation combined totheexpected values forthehid-\ndenvariables.\nFigure 2showsagraphical model thatcanbeused as\nagenerati vemodel forchord progressions inthisfashion.\nAlltherandom variables inthemodel arediscrete. Nodes\n3152\n311\n2 3\n4 5 4 5\n6 7 6 7 6 7 6 7\n48 9 9 10 10 9 10 9 10 9 10 9 10 9 10 9\nFigure 2:Aprobabilistic graphical model forchord progressions, asdescribed inSection 3.Numbers inlevel1and2\nnodes indicate aparticular form ofparameter sharing thathasbeen used intheexperiments (seeSection 4.1).\ninlevel1,2and3arehidden while nodes inlevel4areob-\nserved.Everychords arerepresented asdistinct discrete\nevents. Nodes inlevel1directly model theconte xtual de-\npendencies related tothemeter .Nodes inlevel2com-\nbine thisinformation with local dependencies inorder to\nmodel smooth chord progressions. Variables inlevel1and\n2haveanarbitrary number ofpossible states optimized by\ncross-v alidation (Hastie etal.,2001). Theupper treestruc-\nturemakesitpossible forthealgorithm togenerate proper\nendings. Smooth voice leadings (e.g. small distances be-\ntween generated notes intwosuccessi vechords) aremade\npossible bythehorizontal links inlevel2.\nVariables inlevels3and4haveanumber ofpossible\nstates equal tothenumber ofchords inthedataset. Hence,\neach state isassociated with aparticular chord. Theprob-\nability table associated with theconditional dependencies\ngoing from level3to4is\u0002xedduring learning with the\nvalues givenbyEquation (3).Values inlevel3arehidden\nandrepresent intuiti velyinitial chords thatcould have\nbeen substituted bytheactual observ edchords inlevel4.\nTheroleofthe\u0002xedsubstitution matrix istoraise the\nprobability ofunseen eventsinawaythataccount forpsy-\nchoacoustical similarities. Discarding level4anddirectly\nobserving nodes inlevel3would assign extremely low\nprobabilities tounseen chords inthetraining set.Instead,\nwhen observing agivenchord onlevel4during learning,\ntheprobabilities ofevery chords ofthedataset areupdated\nwith respect totheprobabilities ofsubstitution described\ninSection 2.2.\nThe marginalization inthegraphical model canbe\nachie vedusing theJunction TreeAlgorithm (JTA)(Lau-\nritzen, 1996). Byde\u0002ning aconvenient factorization ofall\nthevariables from theonede\u0002ned bythegraph, theJTA\nallowsmarginalization ofsmall subsets ofthevariables to\nbedone ef\u0002ciently .Exact marginalization techniques are\ntractable inthismodel givenitslimited comple xity.\nManyvariations ofthisparticular model arepossible,\nsome ofwhich arecompared inSection 4.Wesaythattworandom variables aretied when theyshare thesame\nconditional probability parameters. Conditional probabil-\nitytables intheproposed model canbetied invarious\nways. Also, more horizontal links inthemodel canbe\nadded toreinforce thedependencies between higher level\nhidden variables.\nOther tree structures may bemore suitable formu-\nsichaving different meters (e.g. ternary structures for\nwaltzes). Using atree structure hastheadvantage of\nreducing thecomple xity oftheconsidered dependencies\nfrom theorder mtotheorder logm,where misthe\nlength ofagivenchord sequence. Itshould bepointed out\nthatinthispaper weonly consider musical productions\nwith \u0002xedlength. Fortunately ,thecurrent model could\nbeeasily extended tovariable length musical production\nbyadding conditional dependencies arrowsbetween many\nnormalized subtrees.\n4Experiments\n52jazz standards excerpts from Sher (1988) were\ninterpreted and recorded by the \u0002rst author in\nMIDI format onaYamaha Diskla vier piano. See\nhttp://www.idiap.ch/ \u0018paiement/chords/\nforalisting. Standard 4-note jazz piano voicings as\ndescribed inLevine (1990) were used toconvertthe\nchord symbols into musical notes. Thus, themodel\nisconsidering chord progressions astheymight be\nexpressed byatrained jazzmusician inarealistic musical\nconte xt.Thecomple xityofthechord sequences found in\nthecorpus isrepresentati veofthecomple xityofcommon\nchord progressions inmost jazz and pop music. We\nchose torecord actual voiced chords rather than symbolic\nchord names (e.g. Em7)because thesymbolic names are\nineffectiveatcapturing thespeci\u0002c voicings made bya\ntrained jazzmusician.\nEveryjazz standard excerpt was16bars long, with\na4beat meter ,and with one chord change every2\n316Table 2:Subset ofthesubstitution probability table con-\nstructed with Equation (3).Foreach column, thenumber\ninthe\u0002rst rowcorresponds totheprobability ofplaying\ntheassociated chord with nosubstitution. Thenumbers in\nthefollowing rowscorrespond totheprobability ofplay-\ningtheassociated chord instead ofthechord inthe\u0002rst\nrowofthesame column.\nc1a2e3g3 0.41395 c1d#2a#2d3 0.70621\nc1a2c3e3 0.08366 c1a#2d#3g3 0.06677\nc1a2d3g3 0.06401 c1e2a#2d#3 0.02044\nc1a1d2g2 0.02195 c1a#2e3g#3 0.00805\nc1a#2e3a3 0.01623 c1e2a#2d3 0.00582\na0c3g3b3 0.00929 a#0d#2g#2c3 0.00431\nc1e2b2d3 0.00679 a#0d2g#2c3 0.00318\nc1g2c3e3 0.00500 g#1g2c3d#3 0.00243\na0g#2c3e3 0.00363 c1e2a#2c#3 0.00176\nc1f2c3e3 0.00255 a#1g#2d3g3 0.00134\nc1d#2a#2d3 0.00156 f1a2d#3g3 0.00102\ne1e2g2c3 0.00112 d1f#2c3f3 0.00075\ng1a#2f3a3 0.00085 a0c3g3b3 0.00057\ne0g2d3f#3 0.00065 g1f2a#2c#3 0.00043\nf#0e2a2c3 0.00049 b0d2a2c3 0.00033\ng#0g2c3d#3 0.00037 e1d3g3b3 0.00025\nf#1d#2a2c3 0.00028 f#1e2a#2d#3 0.00019\ng0f2b2d#3 0.00021 d#1c#2f#2a#2 0.00015\ng1f2a#2c#3 0.00016 g#0b2f3g#3 0.00011\ng1f2b2d#3 0.00012 b0a2d#3g3 0.00008\nbeats (yielding observ edsequences oflength 32). Longer\nchords were repeated multiple times (e.g. a6beat chord\nisrepresented as3distinct 2-beat observ ations). This\nsimpli\u0002cation hasalimited impact onthequality ofthe\nmodel since generating achord progression issimply a\n\u0002rst(butveryimportant) steptowardgenerating complete\npolyphonic music, where modeling actual eventlengths\nwould bemore crucial. Thejazzstandards were carefully\nchosen toexhibit a16bars global structure. Weused the\nlast16bars ofeach standard totrain themodel. Since ev-\nerystandard ends with acadenza (i.e. amusical ending),\nthechosen excerpts exhibit strong regularities.\n4.1 Generalization\nThechosen discrete chord sequences were converted into\nsequences of12-dimensional continuous vectors asde-\nscribed inSection 2.Frequencies ranging from 30Hz to\n20kHz (MIDI notes going from thelowest note inthecor-\npustonote number 135) were considered inorder tobuild\ntherepresentation givenbyEquation (1). Itispossible\ntomeasure howwell agivenarchitecture captures condi-\ntional dependencies between sub-sequences. Inorder to\ndoso,average negativeconditional out-of-sample likeli-\nhoods ofsub-sequences oflength 8onpositions 1,9,17\nand25havebeen computed. Foreach sequence ofchords\nx=fx1;:::x32gintheappropriate validation set,we\naverage thevalues\n\u0000logP(xi;:::;xi+7jx1;:::;xi\u00001;xi+8;:::;x32):Table 3:Average negativeconditional out-of-sample log-\nlikelihoods ofsub-sequences oflength 8onpositions 1,9,\n17and25,giventherestofthesequences. These results\narecomputed using double cross-v alidation inorder toop-\ntimize thenumber ofpossible values forhidden variables\nandtheparameters \u0015and\u001a.Weseethatthetrees perform\nbetter than theHMM.\nModel (Tying inlevel1) Negativelog-lik elihood\nTree No 32.3281\nTree Yes 32.6364\nHMM 33.2527\nwithi2f1;9;17;25g.Hence, thelikelihood ofeach\nsubsequence isconditional ontherestofthesequence\n(takeninthevalidation set)from which itoriginates. Dou-\nblecross-v alidation isarecursi veapplication ofcross-\nvalidation where both theoptimization oftheparameters\nofthemodel andtheevaluation ofthegeneralization of\nthemodel arecarried outsimultaneously .This technique\nhasbeen used tooptimize thenumber ofpossible values\nofhidden variables andtheparameters \u001aand\u0015forvarious\narchitectures. Results aregiveninTable 3.\nTwoforms ofparameter tying forthetreemodel have\nbeen tested. Theconditional probability tables inlevel1\nofFigure 2canbeeither tiedasshownbythenumbers\ninside thenodes inthe\u0002gure orcanbeleftuntied. Ty-\ningforlevel2isalwaysdone asillustrated inFigure 2\nbythenumbers inside thenodes, tomodel local depen-\ndencies. Allnodes inlevel3share thesame parameters\nforalltested models. Also, recall thatparameters forthe\nconditional probabilities ofvariables inlevel4are\u0002xed\nasdescribed inSection 3.\nAsabenchmark, anHMM consisting oflevels2,3\nand4ofFigure 2hasbeen trained andevaluated onthe\nsame dataset. Theresults presented inTable 3aresimilar\ntoperple xityorprediction ability .Wechoose thispartic-\nular measure ofgeneralization toaccount forthebinary\nmetrical structure ofthechord progressions inthecorpus.\nThe factthatthese conte xtual out-of-sample likelihoods\narebetter forthetrees than fortheHMM areanindi-\ncation thattime-dependent regularities arepresent inthe\ndata. Further investig ations would benecessary inorder\ntoassess towhat extent chord structures arehierarchically\nrelated tothemeter .\n4.2 Generation\nOne can sample thejoint distrib ution learned bythe\nproposed model inorder togenerate novelchord\nprogressions. Chord progressions generated by\nthemodels presented inthis paper areavailable at\nhttp://www.idiap.ch/ \u0018paiement/chords/ .\nForinstance, Figure 3showsachord progression thathas\nbeen generated bythegraphical model showninFigure 2.\nThis chord progression hasallthecharacteristics ofa\nstandard jazz chord progression. Atrained musician may\nobserv ethat thelast8bars ofthesequence isaII-V-I4\n4Thelowest notes ared,gandc.\n317chord progression (Levine, 1990), which isverycommon.\nForcomparison Figure 4showsachord progression\ngenerated bytheHMM model. While thechords follow\noneanother inasmooth fashion, there isnoglobal co-\nherence among thechords. Forinstance, onecanseethat\nthelowest note ofthelastchord isnotac,which wasthe\ncase forallthechord sequences inthetraining set. The\nfundamental qualitati vedifference between both methods\nshould beobvious evenforthenon-musician when listen-\ningtothegenerated chord sequences.\n5Conclusion\nWehaveshownempirically that chord progressions ex-\nhibit global dependencies thatcanbebetter captured with\natreestructure related tothemeter than with asimple dy-\nnamical HMM that concentrates onlocal dependencies.\nThe importance ofconte xtual information formodeling\nchord progressions isevenmore apparent when onecom-\npares sequences ofchords sampled from both models. The\ntime-dependent hidden variables enable thetreestructure\ntogenerate coherent chord progressions both locally and\nglobally .\nHowever,thelowdifference interms ofconditional\nout-of-sample likelihood between thetreemodel andthe\nHMM, andtherelati velylownumber ofdegrees offree-\ndom foroptimal generalization (including thelowoptimal\nnumber ofpossible state forhidden variables) areagood\nindication thatincreasing thenumber ofsequences inthe\ndataset would probably benecessary infurther devel-\nopments ofprobabilistic models forchord progressions.\nAlso, abetter evaluation ofsuch models could beachie ved\nbyusing them forasupervised task. Applications where\nachord progression model could beincluded range from\nmusic transcription, music information retrie val,musical\ngenre recognition tomusic analysis applications, justto\nname afew.\nChord progressions areregular andsimple structures\nthatcondition dramatically theactual choice ofnotes in\npolyphonic tonal music. Hence, wearguethatchord mod-\nelsarecrucial inthedesign ofef\u0002cient algorithms that\ndeal with such music data. Moreo ver,generating inter-\nesting chord progressions may beoneofthemost impor -\ntantaspects ingenerating realistic polyphonic music. Our\nmodel constitutes a\u0002rststepinthatdirection.\nACKNO WLEDGEMENTS\nThe\u0002rstauthor would liketothank YvesGrandv aletand\nDavidBarber forhelpful discussions. This workwassup-\nported inpartbytheISTProgram oftheEuropean Com-\nmunity ,under thePASCAL Netw orkofExcellence, IST-\n2002-506778, funded inpartbytheSwiss Federal Of\u0002ce\nforEducation andScience (OFES) andtheSwiss NSF\nthrough theNCCR onIM2.\nRefer ences\nM.Allan andC.K.I.Williams. Harmonising chorales by\nprobabilistic inference. InAdvances inNeur alInforma-\ntionProcessing Systems ,volume 17,2004.Y.Bengio, P.Simard, andP.Frasconi. Learning long-term\ndependencies with gradient descent isdif\u0002cult. IEEE\nTransactions onNeur alNetworks ,5(2):157166, 1994.\nA.T.Cemgil. Bayesian Music Transcription .PhD thesis,\nRadboud University ofNijme gen, 2004.\nGrosv enor Cooper andLeonard B.Meyer.TheRhythmic\nStructur eofMusic .TheUniv.ofChicago Press, 1960.\nA.P.Dempster ,N.M.Laird, andD.B.Rubin. Maximum\nlikelihood from incomplete data viatheemalgorithm.\nJournal oftheRoyal Statistical Society ,39:138, 1977.\nDouglas EckandJuergenSchmidhuber .Finding tempo-\nralstructure inmusic: Blues impro visation with LSTM\nrecurrent netw orks. InH.Bourlard, editor ,Neur al\nNetworks forSignal Processing XII,Proc.2002 IEEE\nWorkshop ,pages 747756, NewYork,2002. IEEE.\nT.Fujishima. Realtime chord recognition ofmusical\nsound: asystem using common lispmusic. InProceed-\nings ofthe1999 International Computer Music Confer -\nence,pages 464467, Beijing, China, 1999.\nStephen Handel. Listening: Anintroduction tothepercep-\ntionofauditory events .MIT Press, Cambridge, Mass.,\n1993.\nT.Hastie, R.Tibshirani, andJ.Friedman. TheElements\nofStatistical Learning .Springer series instatistics.\nSpringer -Verlag, 2001.\nC.Krumhansl. Cognitive Foundations ofMusical Pitch.\nOxford University Press, Oxford, 1990.\nT.Kuusi. Set-Class and Chor d:Examining Connec-\ntionBetween Theor etical Ressemblance andPerceived\nCloseness .Number 12inStudia Musica. Sibelius\nAcademy ,2001.\nS.L.Lauritzen. Graphical Models .Oxford University\nPress, 1996.\nV.Lavrenk oandJ.Pickens. Polyphonic music modeling\nwith random \u0002elds. InProceedings ofACMMultime-\ndia,Berk eley,CA, November 2-82003.\nMark Levine. The JazzPiano Book .Sher Music\nCo./Adv ance Music, 1990.\nB.C.J. Moore. AnIntroduction tothePsychologyofHear -\ning.Academic Press, 1982.\nL.R.Rabiner .Atutorial onhidden Mark ovmodels and\nselected applications inspeech recognition. Proceed-\nings oftheIEEE ,77(2):257285, February 1989.\nC.Raphael andJ.Stoddard. Harmonic analysis with prob-\nabilistic graphical models. InProceedings ofISMIR\n2003 ,2003.\nChuck Sher,editor .TheNewReal Book ,volume 1.Sher\nMusic Co., 1988.\nV.Valimaki, J.Huopaniemi, Karjaleinen, andZ.Janosy .\nPhysical modeling ofpluck edstring instruments with\napplication toreal-time sound synthesis. J.Audio Eng.\nSociety ,44(5):331353, 1996.\nP.Vassilakis. Chords asspectra, harmon yastimbre. J.\nAcoust. Soc. Am.,106(4/2):2286, 1999. (paper pre-\nsented atthe138th meeting oftheAcoustical Society\nofAmerica).\n318\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0005\n\u0001\u0001\n\u0001\u0001\n\u0005\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0003 \u0004\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0003\nFigure 3:Achord progression generated bytheproposed model. This chord progression isverysimilar toastandard jazz\nchord progression.\u0001\u0001\u0001\u0001\n\u0005\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\n\u0001\u0001\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0005\u0005\n\u0001\u0001\u0001\u0001\n\u0005\u0003 \u0004\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0001\u0001\n\u0005\u0001\u0001\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u0003\nFigure 4:Achord progression generated bytheHMM model. While theindividual chord transitions aresmooth and\nlikely,there isnoglobal chord structure.\n319"
    },
    {
        "title": "On the Detection of Melody Notes in Polyphonic Audio.",
        "author": [
            "Rui Pedro Paiva"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417681",
        "url": "https://doi.org/10.5281/zenodo.1417681",
        "ee": "https://zenodo.org/records/1417681/files/Paiva05.pdf",
        "abstract": "This paper describes a method for melody detection in polyphonic musical signals. Our approach starts by obtaining a set of pitch candidates for each time frame, with recourse to an auditory model. Trajectories of the most salient pitches are then constructed. Next, note candidates are obtained by trajectory segmentation (in terms of frequency and pitch salience variations). Too short, lowsalience and harmonically related notes are then eliminated. Finally, the notes comprising the melody are extracted. This is the main topic of this paper. We select the melody notes by making use of note saliences and melodic smoothness. First, we select the notes with highest pitch salience at each moment. Then, by the melodic smoothness principle, we exploit the fact that tonal melodies are usually smooth. Thus, long music intervals indicate the presence of possibly erroneous notes, which are substituted by notes that smooth out the melodic contour. Finally, false positives in the extracted melody should be eliminated. To this end, we remove spurious notes that correspond to abrupt drops in note saliences or durations. Additionally, note clustering is conducted to further discriminate between true melody notes and false positives.",
        "zenodo_id": 1417681,
        "dblp_key": "conf/ismir/Paiva05",
        "keywords": [
            "melody detection",
            "polyphonic musical signals",
            "pitch candidates",
            "auditory model",
            "trajectory segmentation",
            "note candidates",
            "melodic smoothness",
            "false positives",
            "note clustering",
            "spurious notes"
        ],
        "content": "ON THE DETECTION OF MELODY  NOTES IN POLYPHONIC AUDIO\nRui Pedro Paiva Teresa Mendes Amílcar Cardoso \nCISUC – Centre for Informatics and Sy stems of the University of Coimbra \nDepartment of Informatics Engineer ing, Pólo II – Pinhal de Marrocos \nP 3030 – 290 Coimbra, Portugal \nruipedro@dei.uc.pt  tmendes@dei.uc.pt  amilcar@dei.uc.pt  \nABSTRACT \nThis paper describes a method for melody detection in \npolyphonic musical signals. Our approach starts by ob-taining a set of pitch candidates for each time frame, with recourse to an auditory model. Trajectories of the most salient pitches are then constructed. Next, note candi-dates are obtained by trajectory segmentation (in terms of frequency and pitch salience variations). Too short, low-\nsalience and harmonically related notes are then elimi-\nnated. Finally, the notes comprising the melody are ex-tracted. This is the main topic of this paper. \nWe select the melody notes by making use of note sa-\nliences and melodic smoothness. First, we select the notes with highest pitch salience at each moment. Then, by the melodic smoothness principle, we exploit the fact that tonal melodies are usually smooth. Thus, long music intervals indicate the presence of possibly erroneous notes, which are substituted by notes that smooth out the melodic contour.  \nFinally, false positives in the extracted melody should \nbe eliminated. To this end, we remove spurious notes that correspond to abrupt drops in note saliences or du-rations. Additionally, note clustering is conducted to further discriminate between true melody notes and false positives.   \nKeywords: Melody detection, melodic smoothness, fea-\nture extraction, note clustering \n1 INTRODUCTION \nQuery-by-humming (QBH) is a particularly intuitive way \nof searching for a musical piece, since melody humming is a natural habit of humans. This is an important re-search topic in an emergent and promising field called Music Information Retrieval (MIR). Several techniques have been proposed in order to attain that goal, e.g., [1]. However, this work is presently restricted to the MIDI \ndomain, which places important usability questions. In \nfact, usually we look for recorded songs, which can be obtained from CDs or are stored in audio formats such as \nmp3. Additionally, in musical pieces in the MIDI format, the melody is usually available in a separate channel. The main issues are, then, to extract the notes from the hummed query (a well-known monophonic pitch extrac-tion problem) and to match the query to the melody (an \ninformation retrieval problem).  \nOn the other hand, querying “real-world” polyphonic \nrecorded musical pieces requires the analysis of poly-phonic musical waveforms. This is a rather complex task since many types of instruments can be playing at the same time, with severe spectral interference between each other. So far, only little work has been conducted to tackle the problem of melody detection in polyphonic audio, e.g., [2, 3, 4, 5]. Additionally, most of the work is only concerned with the extraction of melodic pitch lines, rather than melody notes. \nIn our approach, we put the focus on the melody, no \nmatter what other sources are present. Thus, we base our strategy in two main assumptions that we designate as the “salience principle” and the “melodic smoothness principle”. By the salience principle, we assume that the melody notes are, in general, salient in the mixture (i.e., in terms of their intensity). As for the melodic smooth-ness principle, we exploit the fact that note frequency intervals tend, generally, to be small. Finally, false notes \npresent in the obtained melody are deleted by setting out the ones that correspond to abrupt salience or duration decreases and by performing note clustering to further separate true melody notes from false positives. \n2 MELODY DETECTION SYSTEM \nOur melody detection algorithm comprises five stages, as illustrated in Figure 1. The general strategy was de-scribed previously, e.g., [5] and, thus, only a brief pres-entation is provided here, for the sake of completeness. New improvements to the melody extraction stage are described in more detail. \nRaw Musical\nSignal Melody Notes\nMelody Detection System\nMPD MPTCTrajectory\nSegment.Note\nEliminationMelody\nExtraction\nOnset\nDetection\n \nFigure 1. Melody detection system overview. \nIn the Multi-Pitch Detection (MPD) stage, the objec-Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-mercial advan\ntage and that copies bear this notice and the full \ncitation on the first page. \n© 2005 Queen Mary, University of London \n175   \n \n tive is to capture the most salient pitch candidates, which \nconstitute the basis of possible future notes. We perform pitch detection in a frame-based analysis, with a 46.44 ms frame length and a hop size of 5.8 ms. For each ob-tained pitch, a pitch salience is computed, which is ap-proximately equal to the energy of the corresponding fundamental frequency. Our approach is based on \nSlaney and Lyon’s auditory model [6]. \nMulti-Pitch Trajectory Construction (MPTC), in the \nsecond stage, aims to\n create a set of pitch tracks, formed \nby connecting consecutive pitch candidates with similar \nfrequency values. To this end, we based ourselves on the algorithm proposed by Serra [7]. The general\n idea is to \nfind regions of stable  pitches, which  indicate the pres-\nence of musical notes. In order not to loose information on the dynamic properties of musical notes, e.g., fre-quency modulations, glissandos, we had especial care in guaranteeing that such behaviours were kept within a single track. Thus, each trajectory may contain more than one note and should, therefore, be segmented. \nThe segmentation of tracks resulting from the MPTC \nstage is performed in two phases: frequency segmenta-\ntion, aiming to separate notes with different MIDI val-ues, and salience segmentation, with the objective of dividing consecutive notes at the same MIDI note num-ber. Our trajectory segmentation algorithm is described with detail in [8]. \nIn the fourth stage, irrelevant note candidates are \neliminated, based on their saliences, durations and on the analysis of harmonic relations. We make use of per-ceptual rules of sound organization, namely “harmonic-ity” and “common fate” [9], where common frequency and amplitude modulation are exploited.  \nIn the last stage, our goal is to obtain a final set of \nnotes comprising the melody of the song under analysis. In fact, although a significant amount of irrelevant notes are eliminated in the previous stage, many notes are still present. Therefore, we have to extract the ones that con-vey the main melodic line. This is the main topic of this paper and is described in the following section. \n3 EXTRACTION OF MELODY NOTES \nThe definition of the notes comprising the melody of a \nsong under analysis, being probably the most important task of any melody detection algorithm, is also the most difficult one to carry out. In fact, many aspects of audi-tory organization influence the perception of melody by humans, for instance in terms of the role played by the pitch, timbre and intensity content of the sound signal.  \nIn order to limit the scope of our study, we focus this \nanalysis on Western tonal music, where a clear solo is \npresent, as in [9]. \nWe base our strategy on the assumptions that i) the \nmain melodic line often stands out in the mixture (sali-ence principle) and that ii) melodies are usually smooth in terms of the note frequency intervals, which tend to be small (melodic smoothness principle). In addition, we attempt to eliminate false notes in the resulting melody by removal of spurious notes and note clustering.  \n3.1 Selection of the Most Salient Notes \nIn the first step of the melody extraction stage, we select \nthe most salient notes at each time as initial melody can-didates. The used criteria for comparing the salience be-tween notes, as well as algorithmic details, were de-scribed previously, e.g., [5]. Namely, notes below MIDI number 50 (146.83 Hz) are excluded. This is motivated by the fact that the notes comprising the melody are, usu-ally, in a middle frequency range. Moreover, bass notes usually contain a lot of energy and so, if no frequency limit was set, such notes would probably be selected as part of the melody. Anyway, this restriction will be re-laxed later on, when melodic smoothness is applied.  \nIn the implemented algorithm, some of the selected \nnotes were truncated, since melody notes are not al-\nlowed to overlap in time. \nThe results of melody extraction by selecting the most \nsalient notes are illustrated in Figure 2, for an excerpt from Pachelbel’s Kanon. There, the correct notes are depicted in grey and the black continuous lines denote the obtained melody notes. The dashed lines stand for the notes that result from the note elimination stage. We can see that some erroneous notes are extracted, whereas true melody notes are excluded. Namely, some octave errors occur. As a matter of fact, one of the limitations of only taking into consideration note saliences is that the notes comprising the melody are not always the most salient ones. In this situation, wrong notes may be se-lected as belonging to the melody, whereas true notes are left out. This is particularly clear when abrupt transi-tions between notes are found, as can be seen in Figure 2. Hence, we improved our method by smoothing out the melody contour, as follows. \n0 1 2 3 4 5 6405060708090\nTime (s)MIDI note number\n \nFigure 2. Extraction of the most salient notes (ex-\ncerpt from “Pachelbel’s Kanon”). \n3.2 Melody Smoothing \nAs referred above, taking into consideration only the \nmost salient notes has the limitation that, frequently, non-melodic notes are more salient than melodic ones. As a consequence, erroneous notes are often picked up, whereas true notes are excluded. Particularly, abrupt transitions between notes give strong evidence that wrong notes were selected. In fact, small frequency tran-sitions favour melody coherence, since smaller steps in pitch hang together better [9]. In an attempt to demon-\n176   \n \n strate that musicians generally prefer to use smaller note \nsteps, the psychologist Otto Ortmann counted the num-ber of sequential intervals in several songs by classical composers, having found that the smallest ones occur more frequently and that their respective number roughly decreases in inverse pr oportion to the size of the \ninterval [9]. So being, we improved the melody extrac-\ntion stage by taking advantage of this melodic smooth-\nness principle. This is a culturally dependent principle, which is particularly relevant for Western tonal music. \nWe started to improve the initial melody by perform-\ning octave correction. In fact, in the note elimination stage not all harmonically related notes are eliminated and, thus, some octave errors occur when sub or super-harmonic notes are more salient than the right notes. In order to correct octave errors, we select all notes for which no octaves (either above or below) are found and compute their average MIDI values. Then, we analyse all notes that have octaves with common onsets: if the octave is closer to the comp uted average, the original \nnote is replaced by the corresponding octave. This sim-ple first step already improves the final melody signifi-cantly. However, some octave errors, as well as abrupt transitions, are still kept, which will be worked out in the following stages. \nIn the second step, we analyse the obtained notes and \nlook for regions of smoothness, i.e., regions where there \nare no abrupt transitions between consecutive notes. Here, we define a transition as being abrupt if the inter-vals between consecutive notes are above a fifth, i.e., seven semitones, as illustrated in Figure 3. There, the bold notes ( a\n1, a2 and a3) are marked as abrupt. In the \nsame example, four initial regions of smoothness are \ndetected (R 1, R2, R3 and R 4).  \nTimeMIDI note number74\n72\n606264666870\nR2R1\nR4\na1a2\na3R3\na2\n \nFigure 3.  Regions of smoothness. \nThen, we select the longest region as a correct region \n(region R 3, in Figure 3, filled in grey) and define the \nallowed note range for its adjacent regions (R 2 and R 4).  \nRegarding the left region, we define its allowed range \nbased on the first note of the correct region, e.g., MIDI \nvalue 70 in this example. Keeping in mind the impor-tance of the perfect fifth, the allowed range for the left \nregion is 70 ± 7, i.e., [63, 77]. As region R\n2 contains no \nnote in the allowed range, this region is a candidate for \nelimination. However, before deletion, we first check if each of its notes contains an octave in the allowed range. If so, the corresponding notes are substituted by the found octaves. If at least one octave is found, no note is deleted in this iteration. On the contrary, if no octave is found, all the notes are eliminated.  \nAs for the right region, we proceed likewise. Hence, \nwe define the allowed range based on the last note of the \ncorrect region, e.g., 69 in this example, resulting the range [62, 76]. Since region R\n4 contains notes in the \nallowed range, its first note, i.e., note a3, is marked as \nnon-abrupt. However, we still look for an octave of the \nreferred note in the allowed range. In case it is found, the abrupt note is substituted, as before.  \nIn short words, regions that correspond to sudden \nmovements to different registers are interpreted as being incoherent and are, conseque ntly, eliminated. However, \nabrupt transitions are allowed if adjacent regions are \nboth coherent in melodic terms, as happens in Figure 3 for regions R\n3 and R 4. This situation occurs in some mu-\nsical pieces as, for example, Pachelbel’s Kanon, as can \nbe seen in Figure 2 and Figure 4. \nIf no notes are substituted/eliminated for the current \nregion, the following regions are analysed in the same way, in descending length order. If no change at all is performed for all regions, the algorithm stops. Other-\nwise, whenever a change is performed, the procedure for \ndefinition of regions of smoothness, analysis of neighbours and deletion/substitution is repeated until no change is done. In the successive iterations, regions of smoothness are defined taking into consideration notes previously marked as non-abrupt, e.g., the notes in re-gion R\n4. Therefore, in a following iteration, regions R 3 \nand R4 will not be divided. \nFinally, since some regions are eliminated, their notes \nneed to be substituted by other notes that are more likely \nto belong to the melody, according to the smoothness \nprinciple. Thus, we fill each gap in the melody with the most salient note candidates that are in the allowed range for that region. In this gap filling procedure, the \nprevious restriction on the minimum allowed note no \nlonger applies: the most salient note in the allowed range is selected, no matter its MIDI value. In fact, such re-striction was imposed as a necessity to prevent the selec-tion of too many erroneous notes (particularly bass notes), which would jeopardize melody smoothing. Therefore, we kept the general assumption that melodies are contained in middle frequency ranges, but allowing now the selection of low-frequency notes, as long as the smoothness requirement is fulfilled. \n0 1 2 3 4 5 6405060708090\nTime (s)MIDI note number\n \nFigure 4.  Extracted melody after melodic \nsmoothness (excerpt from “Pachelbel’s Kanon”). \nThe results of the implemented procedures are illus-\ntrated in Figure 4, for the same excerpt from Pachelbel’ \n177   \n \n Kanon. We can see that only one erroneous note resulted \n(signalled by an ellipse), which corresponds to an octave error. This example is particularly challenging to our melody-smoothing algorithm due to the periodic abrupt transitions present. Yet, the performance was very good. \nSince our proposed melody extraction approach out-\nputs the most salient notes at each time in the allowed \nnote range, false positives may arise. Such notes may be \noutput both when pauses between melody notes are suf-ficiently long and when the solo is absent (e.g., singing has stopped and another instrument dominates for some time). Thus, spurious notes should be removed, as well as notes that are obtained when the solo is absent. \n3.3 Elimination of Spurious Notes \nAs referred, when pauses between melody notes are \nfairly long, spurious notes, resulting either from noise or background instruments, may be included in the melody. We observed that, usually, such notes have lower sali-ences and shorter durations, leading to clear minima in the pitch salience and duration contours. \nAs for the pitch salience contour, we start by comput-\ning the average pitch salience of each note in the ex-tracted melody and, then, look for deep valleys in the pitch salience sequence. Since saliences were normal-ized to the [0, 100] in the MPD stage, we defined a val-ley as being deep if it is at least 30 units below the re-\nspective left and right global maxima. Hence, notes in deep valleys of the pitch salience contour are disposed.  \nA jazz excerpt (jazz3 sample, see Table 1), where the \nsolo is often absent, was chosen to illustrate the con-ducted procedure.  \n05101520253035405060708090\nNote SequenceAverage Pitch Salience\n \nFigure 5.  Illustration of pitch salience contour \n(jazz3 excerpt). \n0 5 10 15405060708090\nTime (s)Pitch Salience\n \nFigure 6.  Illustration of elimination of spurious \nnotes based on pitch salience (jazz3 excerpt). \nFigure 5 depicts the pitch salience contour, where ‘*’ \ndenote false positives and ‘o’ represent the deleted notes. It can be seen that one true note (the last one) was, nevertheless, removed. Besides, with a lower elimination threshold, a few more false notes would have been deleted. However, best overall results were obtained with the defined threshold. \nThe extracted melody notes are visualized in Figure \n6. There, the thick lines denote true melody notes, whereas the thin ones stand for false positives. The grey lines represent deleted notes. It can be seen that, though some extra notes are disposed, some false positives re-main present in this excerpt. \nRegarding the duration cont our, we proceeded like-\nwise. However, we observed that duration variations are much more common than pitch salience variations. In this way, we decided to eliminate only isolated abrupt duration transitions, i.e., isolated notes delimited by much longer notes, where a note is too short if its dura-tion is at least 20% its neighbours’. Additionally, in or-der not to inadvertently delete short ornamental notes, a minimum difference of two semi-tones was defined. \n3.4 Note Clustering \nAs observed above, when the solo is absent, notes from \nthe dominant accompaniment are output. It can be argued that this behaviour corresponds to the way humans memorize songs: a continuous “line” that comprises both \nmelody per se and dominant accompaniments. However, since our goal is to extract the melody in a strict sense (not a predominant pitch line), the accompaniment should be eliminated. To this end, true notes and false positives are discriminated via note clustering.  \nThis work is related to the classification of musical \ninstruments in a polyphonic context. Only little work has been conducted in this field, e.g. [10], so far with limited accuracy. In fact, this is a complex task, since, in one hand, it is difficult to define acoustic invariants that are good timbre correlates and, on the other hand, the pro-posed features are difficult to measure in a polyphonic context due to spectral overlapping between sources. \nThe conducted procedures, namely feature extraction \nand selection, dimensionality reduction and clustering, are described as follows. \n3.4.1 Feature Extraction In order to acquire information on the source of each \nnote, we use a set of features that aim to capture sound \npitch, intensity and timbre content in both the attack and steady-state parts of each note. Namely, the following features were used, based on related work, e.g., [11, 12]:  - 1) Spectral centroid , which correlates well with the \nperceived sound brightness; \n- 2) Relative spectral centroid , calculated as the ratio of \nthe centroid to the fundamental frequency \n- 3) Pitch salience , which is closely related to the inten-\nsity of the sound; \n- 4) Pitch stability , measured as the frequency variation \nover successive time frames, related to aspects such as pitch jitter or modulation; \n178   \n \n - 5) Harmonic magnitude , which gives a measure of \nspectral shape; \n- 6) Relative harmonic magnitude ratio , the same as \nbefore, except that now relative values are used; \n- 7) Spectral irregularity , calculated as the average dif-\nference between the magnitude of a harmonic and its \ntwo neighbours; \n- 8) Spectral inharmonicity , computed as the sum of \ndifferences of each harmonic frequency from its theo-\nretical value; \n- 9) Spectral skewness , which is the magnitude of the \nharmonics weighed by their respective inharmonic-ities; \n- 10) Harmonic frequency , whose absolute values give \nalso information on inharmonicity; \n- 11) Relative harmonic frequency ratio , the same as \nbefore, except that relative values are used here; \n- 12) Harmonic onset time , calculated as the absolute \ntime delay of each harmonic compared to the note on-set; a measure of onset asynchrony; \n- 13) Relative onset time , the same as before, except that \nrelative timings are used here; \n- 14) Attack duration , which correlates to the type of \ncoupling between the excitation and resonant struc-tures; short attacks indicate tight coupling; \n- 15) Frequency slope in the attack , which measures the \namount of glissando before pitch stability; \n- 16) Note duration ; \nThe listed features were extracted on top of the audi-\ntory front-end used in the MPD stage. In this way, the \nharmonic frequencies and magnitudes of each pitch can-didate in each time frame are obtained directly from a correlogram frame, by using the respective correlogram columns [5]. Then, for each  column, local peaks are \ndetected and matched to the expected frequencies of each harmonic. If no peak is found in the allowed range of the frequency partial, the filter-bank channel whose centre frequency is closest to the theoretical value is selected. Hence, in this case the harmonic frequencies and magnitudes are the ones of the filter channel. This is carried out much in the same way as Martin did [11]. \nFurthermore, for partials above the sixth, several of \nthem may be mapped to the same cochlear channel [11]. In this case, some upper harmonics get the same magni-tude values. Coincidentally or not, we tested our ap-proach with different numbers of harmonics and best results were obtained with exactly six. Therefore, only six frequency partials were used. Spectral features are then computed based on the obtained harmonic frequen-cies and magnitudes in the steady-part of the signal. \nFinally, instead of storing sequences of feature val-\nues, we computed statistical summaries for each feature. Namely, mean and standard deviation were used, except for those features that have a sole value for each note (e.g., frequency slope, duration). In addition, each fea-ture vector was normalized to the [0, 1] interval, so as to avoid numerical problems resulting from the different feature ranges. \nThe computation of some of the features was prob-lematic, as a consequence of the polyphonic context we \nare working in. Namely, the frequency slope was diffi-cult to measure for notes with many missing frequency values in the beginning. Therefore, the slope was simply calculated by interpolating the first and last frequency values in the attack. Also, some harmonic magnitudes may be corrupted due to spectral collisions. Therefore, \nthose elements should be discarded and clustering \nshould be conducted following a missing feature strategy [10]. We will address this issue in future developments. \n3.4.2 Feature Selection and Dimensionality Reduction \nThe number of implemented features is very high com-\npared to the number of notes available in each song ex-cerpt. In addition, a high number of features may lead to the so-called curse of dimensionality [13]. Hence, feature selection and dimensionality reduction were performed prior to clustering. \nAs referred, it is important to select the best combina-\ntion of features to include. Since it is impractical to ana-lyse every different combination of features, forward selection was conducted [13]. In this way, starting from an empty feature set, the algorithm adds, step by step, the feature that leads to the best model accuracy. The combination of features that gives the best overall performance is then selected.  \nIn addition, the dimension of the feature space was \nreduced with recourse to Principal Component Analysis \n(PCA) [13]. This is a widely used technique whose basic idea is to project the comput ed feature matrix into an \northogonal basis that best expresses the original data set. Moreover, the resulting projected data is decorrelated.  As for the selection of the principal components, we kept the ones that retained 90% of the variance. \n3.4.3 Clustering Finally, after feature extraction, selection and dimension-\nality reduction, true notes and false positives are dis-criminated via clustering. To this end, we used Gaussian Mixture Models (GMMs) [13]. \nGMMs are extensively used for unsupervised cluster-\ning of data. Basically, Gaussian distributions are fitted to the observed data and so GMMs model the probability density of observed features by multivariate Gaussian mixture densities. \nIn order to separate false positives from true melody \nnotes, we defined only two clusters (a “melody cluster” and a “garbage cluster”), initialised with the K-means \nclustering algorithm [13].  \nNext, the parameters of the model (mean vector, co-\nvariance matrix - diagonal, in this case - and mixing co-efficients) are iteratively estim ated with recourse to the \nExpectation-Maximization algorithm [13]. The algo-rithm stops when the likelihood function stabilizes for consecutive iterations.  \nAfter that, each note is allotted to a cluster based on \nthe posterior probabilities in each: the cluster with the highest probability is selected. \nFinally, the melody is assigned to the cluster with \n179   \n \n maximum salience, where cluster salience is computed \nas the sum of the average pitch salience of each note multiplied by its duration. \nThe procedure for note clustering is illustrated in \nFigure 7, for the same jazz excerpt used before. As can be seen, all false positives were eliminated. However, three true melody notes were erroneously deleted. In \nfact, there seems to be a tr ade-off between keeping all \nthe true melody notes and removing all false positives. \n0 5 10 15405060708090\nTime (s)Pitch Salience\n \nFigure 7.  Illustration of note clustering (jazz3 ex-\ncerpt). \n3.4.4 Clustering the Whole Note Set \nWe also decided to test a different approach, where clus-\ntering was performed on the whole note set that resulted from the note elimination stage. Some constraints should be imposed on the performed clustering (e.g., no overlap between notes) [4]. However, we ignored this issue since the procedures for detection of salient notes and melody smoothing guarantee the consistency of the results. Fur-thermore, harmonically related notes may come from the same source and, thus, such constraints are problematic in this situation. \nTherefore, notes were clustered with the GMM algo-\nrithm, using now five clusters. Then, for each cluster, salient notes were detected, melody smoothing was per-formed and spurious notes were eliminated. \nFinally, the melody was assigned to the cluster with \nthe highest salience, as before. \n4 EXPERIMENTAL RESULTS \nOne difficulty regarding the evaluation of MIR systems \nresults from the absence of standard test collections and benchmark problems. This problem was partly solved through the creation of a set of evaluation databases by researchers from the Music Technology Group of Uni-versity Pompeu Fabra (MTG - UPF), Barcelona, Spain, for the ISMIR 2004 Audio Description Contest (ADC) \n[14]. Several competitions were organized as part of it. \nNaturally, we are more interested in the database created for the Melody Extraction Contest (MEC-04).  \nIn this way, we evaluated the proposed algorithms \nwith both the MEC-04 database and a test-bed we had previously created (see Table 1, where the top 11 lines correspond to our test-bed and the next 10 refer to the MEC-04 database). Both databases were defined taking into consideration its diversity and musical content. Therefore, the selected song excerpts contain a solo (vo-cal or instrumental), and accompaniment parts (guitar, \nbass, percussion, other vocals, etc.). In addition, in some excerpts the solo is absent for some time. In our test-bed, we collected excerpts of about 6 seconds from 11 songs, which were manually annotated. As for the MEC-04 database, 10 excerpts, each of around 20 seconds, were automatically annotated based on monophonic pitch estimation from multi-track recordings [14].  \nFor accuracy computation, the detected melody notes \nwere compared with the correct notes. To this end, we used two of the metrics defined in [14], with some adap-tations. In the first metric, the pitched accuracy ( PA), \ni.e., the accuracy regarding only the notes comprising the melody, was performed. In the second one, a global accuracy ( GA) was computed taking into consideration \nalso the matching of frames where the melody is absent. \nAdditionally, another frame-based metric was consid-ered, where octave errors were ignored [14]. For this one, only summary results are presented. \nIn terms of frame comparison, we defined the target \nfrequency values for each time frame as the reference frequencies of the corresponding MIDI notes. In the same way, the extracted frequencies were defined from the reference frequencies corresponding to the extracted melody notes. The accuracy was calculated as the per-centage of correctly identified frames. In the original metric defined in [14], exact frequency values were used. However, since we do not know the precise fre-quency values for the excerpts in our test-bed, reference MIDI frequencies were used for the sake of uniformity. Also, the determination of exact frequency values does not seem very relevant in a melody detection context.  \nFive evaluations were performed: i) extraction of sa-\nlient notes only ( SN); ii) note salience plus melodic \nsmoothness ( MS); iii) elimination of spurious notes ( ES); \niv) note clustering (NC); and v) note clustering in the \nwhole note set ( NCW). For each evaluation, results for \nthe two used metrics were computed. \nThe obtained results are summarized in Table 2. \nShort descriptions of the used song excerpts are pre-\nsented in Table 1. \nRegarding the MS evaluation, we can see that good \nresults were achieved. There, an average accuracy of 84.0 / 75.4% ( PA / GA, respectively) was attained. \nWithout melody smoothing, the average accuracy was 74.7 / 66.2% ( SN evaluation) and so our implementation \nof the melodic smoothness principle amounts for an av-erage improvement of 9.3 / 9.2%. A high number of octave errors was corrected, especially in the excerpts \nfrom Battlefield Band and Pachelbel’s Kanon. \nThe results from the choral sample were also interest-\ning, since four simultaneous voices are present, plus \norchestral accompaniment. Still, the algorithm could reasonably detect the melody, which we defined as cor-responding to the soprano. The use of this example con-tradicts our previous assumptions, but we were inter-ested in the results for a special situation like this one. \nAs for the MEC-04 database, the results were also \ngood, except for the opera excerpts. These samples seem \n180   \n \n ID Song Title Genre Solo Type \n1 Pachelbel’s Kanon Classical Instrumental \n2 Handel’s Hallelujah Choral Vocal \n3 Enya - Only Time Neo-Classical Vocal \n4 Dido - Thank You Pop Vocal \n5 Ricky Martin - Private Emotion Pop Vocal \n6 Avril Lavigne - Complicated Pop / Rock Vocal \n7 Claudio Roditi - Rua Dona Margarida Jazz / Easy Instrumental \n8 Mambo Kings - Bella Maria de Mi Alma Bolero Instrumental \n9 Compay Segundo - Chan Chan Son Cubano Vocal \n10 Juan Luis Guerra - Palomita Blanca Bachata Vocal \n11 Battlefield Band - Snow on the Hills Scottish Folk Instrumental \n12 daisy2 Synthesized singing voice Vocal \n13 daisy3 Synthesized singing voice Vocal \n14 jazz2 Saxophone phrases Instrumental \n15 jazz3 Saxophone phrases Instrumental \n16 midi1 MIDI synthesized Instrumental \n17 midi2 MIDI synthesized Instrumental \n18 opera_fem2 Opera singing Vocal \n19 opera_male3 Opera singing Vocal \n20 pop1 Pop singing Vocal \n21 pop4 Pop singing Vocal \nTable 1. Description of used song excerpts. \n \nSalient Notes Melody Smoothing Elim. Spurious Note Clustering Note Clust. Whole ID PA GA PA GA PA GA PA GA PA GA \n1 59.3 58.3 89.5 88.1 89.5 88.1 89.5 88.1 89.5 88.1 \n2 62.6 54.9 78.7 67.9 78.7 67.9 81.5 76.8 81.5 72.3 \n3 94.0 90.9 94.0 90.9 94.0 90.9 94.0 90.9 94.0 90.9 \n4 92.0 74.2 94.9 73.5 94.9 73.5 94.9 73.5 94.9 73.5 \n5 64.4 44.2 72.0 53.7 72.0 53.7 75.9 58.6 72.0 53.7 \n6 75.6 68.8 93.7 84.2 93.7 88.6 93.7 88.6 93.7 88.6 \n7 89.0 83.0 98.3 91.7 98.3 91.7 98.3 91.7 98.3 91.7 \n8 87.7 81.0 90.8 83.8 90.8 83.8 90.8 83.8 90.8 83.8 \n9 82.4 63.3 82.4 65.0 82.4 65.0 82.4 69.6 82.4 65.0 \n10 73.5 51.8 80.2 57.2 80.2 57.2 80.2 57.2 80.2 57.2 \n11 47.1 46.9 93.6 92.3 93.6 92.3 93.6 92.3 93.6 92.3 \n12 91.6 79.5 92.3 82.0 92.3 84.9 87.1 80.4 92.3 84.9 \n13 84.4 84.3 97.2 97.1 97.2 97.1 97.2 97.1 97.2 97.1 \n14 69.6 65.0 73.6 70.4 76.1 73.7 73.4 71.2 76.1 73.7 \n15 82.4 59.8 86.6 63.8 85.5 74.3 78.8 84.6 85.5 74.3 \n16 64.1 62.2 85.9 83.8 86.1 85.4 86.1 85.4 88.2 88.4 \n17 97.9 96.3 97.9 96.3 97.9 96.3 97.9 96.3 97.9 96.3 \n18 64.8 57.8 69.8 62.0 69.8 62.0 69.8 62.0 69.8 62.0 \n19 38.5 37.1 41.3 38.7 41.3 39.4 41.3 39.4 42.5 40.6 \n20 69.9 62.6 70.2 65.3 70.7 69.6 69.3 68.4 70.7 69.6 \n21 78.6 69.0 82.0 75.1 82.2 76.7 82.2 76.7 82.2 76.7 \nAvg 74.7 66.2 84.0 75.4 84.1 76.8 83.7 77.7 84.4 77.2 \nTable 2. Results of the melody detection system. \nto pose additional difficulties to the pitch detection al-\ngorithm, in the first stage of our system. We plan to ad-\ndress this issue in the near future. \nAnother interesting fact is that the proposed approach is almost immune to octave errors. Indeed, disregarding octave errors, the accuracy for SM raised to 85.0 / 76.2%, i.e., an improvement of only 1.0 / 0.8%. Regarding the elimination of spurious notes ( ES \nevaluation), we can see that GA improved slightly \n(1.4%). As a consequence of note elimination, the origi-nal durations of some notes were restored (recall that some of them were truncated in the note salience stage), which led to a slight improvement of PA (0.1%).  \nAs for note clustering, the global accuracy improved \n181   \n \n a bit more (0.9%, comparing to the ES evaluation and \n2.3%, regarding SM), but some excerpts still have many \nfalse positives. In fact, different songs prefer different \nfeatures combinations. For example, almost all false positives from Juan Luis Guerra’s sample were elimi-nated with a particular feature set. However, best overall results were achieved using the following features, in \norder of insertion from the forward selection algorithm: \n5, 6, 2, 8, 1, 3, 7, 10, 11, 9, 15 and 14. From the ob-tained results we can see that, while many false positives were deleted, a few true notes were also wrongly elimi-nated, leading to a 0.4% drop in the pitched accuracy. \nClustering the whole note set ( NCW evaluation) led to \nsimilar results: 84.4% for PA and 77.2% for GA. Again, \ndifferent excerpts prefer different features combinations, but best global results were attained with only these: 5, 6, 10, 11 and 1. \nThe main limitation of the note clustering stage is its \nlack of robustness. In fact, the best set of features varies from sample to sample and some particular feature com-binations simply cannot discriminate between true notes and false positives, leading to a notorious fall in melody detection accuracy. Therefore, so far, robustness cannot be guaranteed after the elimination of spurious notes. However, longer song excerpts could possibly improve the accuracy for note clustering. \nFinally, for the sake of comparison with the results \nfrom the ISMIR 2004 ADC, we also tested our approach \nwith the exact frequency values used there. As a conse-quence, the accuracy for the ES evaluation, taking into \nconsideration only the MEC-04 database, dropped from 79.9 / 75.5% to 75.0 / 69.9%, i.e., approximately 5%. In our opinion, when the goal is predominant pitch estima-tion, exact frequency values are important. However, accuracy computation in terms of MIDI reference values seems more relevant for melody detection tasks, where exact frequency values are not needed in the output. \n5 CONCLUSIONS \nWe propose a system for melody detection in polyphonic \nmusical signals. This is a main issue for MIR applica-tions, such as QBH in “real-world” music databases. The work conducted in this field is presently restricted to the MIDI domain, and so we guess we make an interesting contribution to the area, with  some encouraging results. \nAdditionally, we tackled the problem of false positives. \nAs expected, this proved to be a very difficult task, and \nso only slight improvements were achieved. \nRegarding future work, we plan to further work out \nsome of the described limitations, namely in what con-cerns the reliability of feature computation and the im-provement of the elimination of false positives.  \nACKNOWLEDGEMENTS \nThis work was partially supported by the Port uguese \nMinistry of Science and Technology, under the program \nPRAXIS XXI.  REFERENCES \n[1] Bainbridge, D., Nevill-Manning, C., Witten, I., \nSmith, L. and McNab, R. ''Towards a digital library of popular music'', Proceedings of the ACM International Conference on Digital Libraries, 1999. \n[2] Eggink, J., and Brown, G. J. \"Extracting melody \nlines from complex audio\", Proceedings of ISMIR, 2004. \n[3] Goto, M.  ''A predominant-F0 estimation method for \nCD recordings: MAP estimation using EM algorithm \nfor adaptive tone models'', Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2001. \n[4] Marolt, M. ''On finding melodic lines in audio \nrecordings'', Proceedings of the International Conference on Digital Audio Effects, 2004. \n[5] Paiva, R. P., Mendes, T., and Cardoso, A. ''An \nauditory model based approach for melody detection in polyphonic musical recordings''. In Wiil, U. K. (ed.), Computer Music Modelling and Retrieval - \nCMMR 2004, Lecture Notes in Computer Science, Vol. 3310, 2005. \n[6] Slaney, M., and Lyon, R. F. ''On the importance of \ntime - a temporal representation of sound''. In Cooke, Beet and Crawford (eds.),  Visual representations of \nspeech Signals, 1993. \n[7] Serra, X. ''Musical sound modeling with sinusoids \nplus noise''. In  Roads, C., Pope, S., Picialli, A., De \nPoli, G. (eds.), Musical signal processing, 1998. \n[8] Paiva, R. P., Mendes, T., and Cardoso, A. ''On the \ndefinition of musical notes from pitch tracks  \nfor melody detection in polyphonic recordings'', accepted for presentation at the International Conference on Digital Audio Effects, 2005. \n[9] Bregman, A. S.  Auditory scene analysis: the \nperceptual organization of sound. MIT Press, 1990. \n[10] Eggink, J., and Brown, G. J. \"Application of missing \nfeature theory to the recognition of musical instruments in polyphonic audio\", Proceedings of the International Conference on Music Information Retrieval (ISMIR), 2003. \n[11] Martin, K. D. Sound-source recognition: a theory \nand computational model. PhD Thesis, Massachusetts Institute of Technology, 1999. \n[12] Tzanetakis, G. Manipulation, Analysis and Retrieval \nSystems for Audio Signals. PhD Thesis, Princeton University, 2002. \n[13] Bishop, C. M. Neural networks for pattern \nrecognition. Oxford University Press, 1995. \n[14] MTG - UPF. ''ISMIR 2004 Audio Description \nContest''. ISMIR, 2004. http://ismir2004. ismir.net/ISMIR_Contest.html \n182"
    },
    {
        "title": "Improvements of Audio-Based Music Similarity and Genre Classificaton.",
        "author": [
            "Elias Pampalk",
            "Arthur Flexer",
            "Gerhard Widmer"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418083",
        "url": "https://doi.org/10.5281/zenodo.1418083",
        "ee": "https://zenodo.org/records/1418083/files/PampalkFW05.pdf",
        "abstract": "Audio-based music similarity measures can be applied to automatically generate playlists or recommendations. In this paper spectral similarity is combined with complementary information from fluctuation patterns including two new descriptors derived thereof. The performance is evaluated in a series of experiments on four music collections. The evaluations are based on genre classification, assuming that very similar tracks belong to the same genre. The main findings are that, (1) although the improvements are substantial on two of the four collections our extensive experiments confirm earlier findings that we are approaching the limit of how far we can get using simple audio statistics. (2) We have found that evaluating similarity through genre classification is biased by the music collection (and genre taxonomy) used. Furthermore, (3) in a cross validation no pieces from the same artist should be in both training and test set. 1",
        "zenodo_id": 1418083,
        "dblp_key": "conf/ismir/PampalkFW05",
        "keywords": [
            "audio-based music similarity measures",
            "automatic playlist generation",
            "complementary information",
            "fluctuation patterns",
            "genre classification",
            "music collections",
            "performance evaluation",
            "experiments",
            "four music collections",
            "cross validation"
        ],
        "content": "IMPROVEMENTS OF AUDIO-BASED\nMUSIC SIMILARITY AND GENRE CLASSIFICATON\nElias Pampalk1, Arthur Flexer1,2, Gerhard Widmer1,3\n1Austrian Research Institutefor Artiﬁcial Intelligence (O FAI), Freyung 6/6, 1010 Vienna\n2Instituteof Medical Cybernetics and Artiﬁcial Intelligen ce, Center for Brain Research, Medical University of Vienna\n3Department of Computational Perception, Johannes Kepler U niversity, Linz, Austria\nABSTRACT\nAudio-based music similarity measures can be applied to\nautomatically generate playlists or recommendations. In\nthis paper spectral similarity is combined with comple-\nmentary information from ﬂuctuation patterns including\ntwo new descriptors derived thereof. The performance is\nevaluated in a series of experiments on four music col-\nlections. The evaluations are based on genre classiﬁca-\ntion, assuming that very similar tracks belong to the same\ngenre. The main ﬁndings are that, (1) although the im-\nprovements are substantial on two of the four collections\nourextensiveexperimentsconﬁrmearlierﬁndingsthatwe\nareapproachingthelimitofhowfarwecangetusingsim-\npleaudiostatistics. (2)Wehavefoundthatevaluatingsim-\nilarity through genre classiﬁcation is biased by the music\ncollection (and genre taxonomy) used. Furthermore, (3)\ninacrossvalidationnopiecesfromthesameartistshould\nbe in both training and test set.\n1 INTRODUCTION\nAudio-based music similarity measures can be applied to\nplaylist generation, recommendation of unknown pieces\nor artists, organization and visualization of music collec -\ntions, or retrieval by example.\nIn general, music similarity is very complex, multi-\ndimensional, context-dependent, and ill-deﬁned. To eval-\nuate algorithms which model the perception of similarity\nwouldrequireextensivelisteningtests. Asimpleralterna -\ntive is to evaluate the performance in terms of genre clas-\nsiﬁcation. The assumption is that very similar pieces be-\nlong to the same genre. We believe this assumption holds\ninmostcasesdespitethefactthatmusicgenretaxonomies\nhave several limitations (see e.g. [1]). An obvious issue\nis that many artists have a very individual mix of several\nstyleswhichisoftendifﬁculttopigeonhole. Nevertheless ,\ngenresarewidelyusedtomanagelargemusiccollections,\nand genre labels for artists are readily available.\nInthispaperwedemonstratehowtheperformancecan\nbe improved by combining spectral similarity with com-\nplementary information. In particular, we combine spec-\ntral similarity (which describes aspects related to timbre )\nwith ﬂuctuation patterns (which describe periodic loud-\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrstpage.\nc/circlecopyrt2005 Queen Mary, University of Londonness ﬂuctuations over time) and two new descriptors de-\nrived thereof.\nThe results are evaluated using four music collections\nwith a total of almost 6000 pieces and up to 22 genres\nper collection. One of these collections was used as train-\ning set for the ISMIR’04 genre classiﬁcation contest. Us-\ning last year’s winning algorithm as baseline our ﬁndings\nshow improvements of up to 41% (12 percentage points)\non one of the collections, while the improvements on the\ncontest training set are hardly signiﬁcant. This conﬁrms\nthe ﬁndings of Aucouturier and Pachet [2] who suggest\ntheexistenceofaglassceilingwhichcannotbesurpassed\nwithout taking higher level cognitive processing into ac-\ncount.\nAnother observation is that not using different mu-\nsiccollections(withdifferentstructuresandcontents)c an\neasily lead to overﬁtting. Finally, we recommend the use\nof an artist ﬁlter which ensures that none of the artists in\nthe test set are not also present in the training set. Our re-\nsults show that the classiﬁcation accuracy is signiﬁcantly\nlower if an artist ﬁlter is used. Not using an artist ﬁlter\nmight transform the genre classiﬁcation task into an artist\nidentﬁcation task.\n2 RELATED WORK\nThere is a signiﬁcant amount of research on audio-based\ngenre classiﬁcation with one of the ﬁrst approaches pre-\nsented in [3]. More recent approaches include, for ex-\nample [4, 5]. Most of these approaches do not focus on\nsimilaritymeasures(anddonotusenearestneighborclas-\nsiﬁers to evaluate the performance). However, content-\nbased descriptors which work well for classiﬁers are also\ngood candidates to be included in a similarity measure.\nAn overview and evaluation of many of the descriptors\nused for classiﬁcation can be found in [6]. In addition,\nrecent work suggests that it is possible to automatically\nextract features [7].\nFor our work the most important ingredient is spec-\ntral similarity based on Mel Frequency Cepstrum Coefﬁ-\ncients [2, 8, 9, 10]. Similar audio frames are grouped into\nclusterswhichareusedtocomparepieces(wedescribethe\nspectral similarity in detail later on). For these similari ty\nmeasures the focus in terms of applications is mainly on\nplaylist generation and recommendation (e.g. [11, 12]).\nAlternatives include the anchor space similarity [13] and\nthe ﬂuctuation patterns [14, 15].\nComparing approaches published by different authors\nis difﬁcult. First, most implementations have not been\nmade freely available. Second, sharing the same mu-\nsic collections is infeasible due to copyright restriction s.\nThird, results on different music collections (and genre\n628taxonomies) are not comparable.\nOne solution has been to reimplement approaches by\nother authors. For example, in [16] ﬁve different ap-\nproaches were implemented. However, there is no guar-\nantee that these implementations are correct (in fact one\napproach was not implemented correctly in [16]).\nAn alternative is the collaboration of authors\n(e.g. [17]) or the use of creative commons music which\niseasilyavailable(e.g. theMagnatune collectionusedfor\ntheISMIR’04contest). Forcommerciallyinteresting(and\ntherefor copyright protected) music a solution is a cen-\ntralized evaluation system [18] as used for the ISMIR’05\nevaluation exchange (MIREX).\nRelated work on similarity measures for music in-\ncludes approaches using cultural information retrieved\nfrom the Internet such as playlists, reviews, lyrics, and\nweb pages (e.g. [19, 20, 21, 22, 23]). These web-based\napproaches can complement audio-based approaches.\n3 AUDIO-BASED MUSICSIMILARITY\nInthissectionwereviewspectralsimilarityandtheﬂuctu-\nationpatternsfromwhichweextracttwonewdescriptors,\nnamely “Focus” and “Gravity”. Furthermore we describe\nthe linear combination of these.\n3.1 Spectral Similarity\nSpectral similarity describes aspects related to timbre.\nHowever, important timbre characteristics such as the at-\ntack or decay of a sound are not modeled. Instead the\naudio signal is chopped into thousands of very short (e.g.\n20ms) frames and their order in time is ignored. Each\nframe is described by Mel Frequency Cepstrum Coefﬁ-\ncients (MFCCs). The large set of frames is summarized\nby a model obtained by clustering the frames. The dis-\ntancebetweentwopiecesiscomputedbycomparingtheir\ncluster models.\nThe ﬁrst approach was presented by Foote [9] based\non a global set of clusters for all pieces in the collection.\nThis global set was obtained froma classiﬁer.\nThe ﬁrst localized approach was presented by Logan\nand Salomon [10]. For each piece an individual set of\nclusters is used. The distances between these are com-\nputed using the Earth Movers Distance [24]. Aucouturier\nand Pachet suggested using the computationally more ex-\npensive Monte Carlo sampling instead [8].\nFortheexperimentsdescribedinthispaperweusethe\nspectral similarity implemented in the MA Toolbox [25].\nWe apply the ﬁndings of Aucouturier and Pachet in [2],\nthus we refer to it as “AP”.\nFrom the 22050Hz mono audio signals two minutes\nfrom the center are used for further analysis. The sig-\nnal is chopped into frames with a length of 512 samples\n(about 23ms) with 50% overlap. The average energy of\neach frame’s spectrum is subtracted. The 40 Mel fre-\nquency bands (in the range of 20Hz to 16kHz) are rep-\nresentedbytheﬁrst20MFCCcoefﬁcients. Forclustering\nweuseaGaussianMixtureModelwith30clusterstrained\nusing expectation maximization (after k-means initializa -\ntion). The cluster model similarity is computed withMonte Carlo sampling.\n3.2 Fluctuation Patterns\nFluctuation Patterns (FPs) describe loudness ﬂuctuations\ninfrequencybands[14,15]. Theydescribecharacteristics\noftheaudiosignalwhicharenotdescribedbythespectral\nsimilaritymeasure.\nA FP is a matrix with 20 rows (frequency bands) and\n60 columns (modulation frequencies, in the range of 0-\n10Hz). The elements of this matrix describe the ﬂuctua-\ntion strength. The distance between pieces is computed\nby interpreting the FP matrix as 1200-dimensional vector\nand computing the Euclidean distance.\nFromtheFPsweextracttwonewdescriptors. Theﬁrst\none, describes how distinctive the ﬂuctuations at speciﬁc\nfrequencies are, we call it Focus. The second one which\nwe callGravity, is related to the overall perceived tempo.\n3.2.1 Focus\nThe Focus (FP.F) describes the distribution of energy in\ntheFP.Inparticular,FP.Fislowiftheenergyisfocusedin\nsmallregionsoftheFP,andhighiftheenergyisspreadout\noverthewholeFP.TheFP.Fiscomputedasmeanvalueof\nall values in the FP matrix, after normalizing the FP such\nthat the maximum value equals 1. The distance between\ntwopiecesofmusiciscomputedastheabsolutedifference\nbetween their FP.F values.\n3.2.2 Gravity\nThe Gravity (FP.G) describes the center of gravity of the\nFP on the modulation frequency axis. Given 60 modu-\nlation frequency-bins (linearly spaced in the range from\n0-10Hz) the center usually lies between the 20thand the\n30thbin.We compute FP.G by subtracting the theoretical\nmean of the ﬂuctuation model (which is around the 31st\nband) from the center.\nLow values indicate that the piece might be perceived\nslow. However, FP.G is not intended to model the per-\nception of tempo. Effects such as vibrato or tremolo are\nalso reﬂected in the FP. The distance between two pieces\nof music is computed as the absolute difference between\ntheir FP.G values.\n3.3 Illustrations\nFigure 1 illustrates the extracted features for ﬁve songs.\nAll ﬁveclusters models have low energy in high frequen-\nciesandhighenergy(withahighvariance)inthelowfre-\nquencies. Astheclustermodelsareaverylow-levelrepre-\nsentation it is difﬁcult to guess the actual instrumentatio n\nby looking at the ﬁgures. In the FPsvertical lines indi-\ncatereoccurringperiodicbeats. ThesongSpider,byFlex,\nwhichisatypicalexampleofthegenreeurodance,hasthe\nstrongest vertical lines. The highest FP.Fvalue (0.42) is\ncomputed for Black Jesus by Everlast (belonging to the\ngenre alternative). The song has a strong focus on guitar\nchords and vocals, while the drums are hardly noticeable.\nSpider by Flex has the lowest FP.F value (0.16). Most of\nthesongsenergyisinthestrongperiodicbeats. Thehigh-\nestFP.Gvalue(-5.0)iscomputedforSpiderbyFlex. The\n629CMBrubeck et al.\nTake FiveFP\n0.28FP.F\n−6.4FP.G\nFlex\nSpider0.16 −5.0\nBeach Boys\nSurfin’ USA0.23 −6.4\nSpears\nCrazy0.32 −5.9\nEverlast\nBlack Jesus0.42 −5.8\nFigure 1: Visualization of the features. On the y-axis of the\ncluster model (CM) is the loudness (dB-SPL), on the x-axis are\ntheMelfrequencybands. Theplotsshowthe30centersandtheir\nvariances on top of each other. On the y-axis of the FP are the\nBark frequency bands, the x-axis is the modulation frequency\n(in the range from 0-10Hz). The y-axis on the FP.F histogram\nplots are the counts, on the x-axis are the values of the FP (from\n0 to 1). The mean is marked with a vertical line. The y-axis of\nthe FP.G is the sum of values per FP column, the x-axis is the\nmodulation frequency (from 0-10Hz). The center of gravity is\nmarked with a vertical line.\nArtists/Genre Tracks/Genre\nGenres Artists Tracks Min Max Min Max\nDB-S 16 63 100 2 7 4 8\nDB-L 22 103 2522 3 6 45 259\nDB-MS 6 128 729 5 40 26 320\nDB-ML 10 147 3248 2 40 22 1277\nTable 1: Statistics of the four collections.\nlowestvalue(-6.4)iscomputedforTakeFivebytheDave\nBrubeck Quartet and Surﬁn’ USA by the Beach Boys.\nFigure 2 shows the distribution of FP.F and FP.G over\ndifferent genres. The FP.Fvalues have a large deviation\nand the overlap between quite different genres is signiﬁ-\ncant. Electronic has the lowest values while punk/metal\nhas the highest. The amount of overlap is an important\nfactor for the quality of the descriptor. As we will see\nlater, in the optimal combination of all similarity mea-\nsures,FP.Fhasthesmallestcontribution. The FP.Gvalues\nhaveasmallerdeviationcomparedtoFP.Fandthereisless\noverlap between different genres. Classical and a capella\nhave the lowest values, while electronic, metal, and punk\nhave the highest values.\n3.4 Combination\nWe combine the distance matrices linearly, similar to\nthe approach used for the aligned Self-Organizing Maps\n(SOMs)[26]. Beforecombiningthedistanceswenormal-\nize the four distances such that the standard deviation of\nall pairwise distances within a music collection equals 1.\nIn contrast to the aligned-SOMs we do not rely on the\nuser to set the optimum weights for the linear combina-\ntion, instead we automatically optimize the weights for\ngenre classiﬁcation.0.10.20.30.40.5ClassicalElectronicJazz/BluesMetal/PunkPop/RockWorldFP.F\n−10−50FP.G\n(a) DB-MS\n0.10.20.30.40.5A CappellaDeath MetalElectronicJazzJazz GuitarPunkFP.F\n−10 −5 0FP.G\n(b) DB-L\nFigure 2: Boxplots showing the distribution of the descriptors\nper genre on two music collections. A description of the collec-\ntions can be found in Section 4.1. The boxes have lines at the\nlower quartile, median, and upper quartile values. The whiskers\nshowtheextentoftherestofthedata(themaximumlengthis1.5\noftheinter-quartilerange). Databeyondtheendsofthewhiskers\nare marked with plus-signs.\nDB-S alternative, blues, classic orchestra, classic piano,\ndance, eurodance, happy sound, hard pop, hip hop,\nmystera, pop, punk rock, rock, rock & roll,\nromantic dinner, talk\nDB-L a cappella, acid jazz, blues, bossa nova, celtic,\ndeath metal, DnB, downtempo, electronic,\neuro-dance, folk-rock, German hip hop, hard core\nrap, heavy metal/thrash, Italian, jazz, jazz guitar,\nmelodic metal, punk, reggae, trance, trance2\nDB-MS classical, electronic, jazz/blues, metal/punk,\npop/rock, world\nDB-ML ambient, classical, electronic, jazz, metal, new age,\npop, punk, rock, world\nTable 2: Listof genres foreach collection.\n4 GENRE CLASSIFICATION\nWe use a nearest neighbor classiﬁer and leave-one-out\ncross validation for the evaluation. The accuracies are\ncomputed as ratio of the correctly classiﬁed compared to\nthe total number of tracks (without normalizing the accu-\nracies with respect to the different class probabilities).\nIncontrasttotheISMIR’04genrecontestweapplyan\nartistﬁlterwhichensuresthatallpiecesofanartistareei -\ntherinthetrainingsetortestset. Otherwisethegenreclas -\nsiﬁcationmightbetransformedintoanartistidentiﬁcatio n\ntask since all pieces of an artist are in the same genre (in\nall of the collections we use). The resulting performance\nis signiﬁcantly worse. For example, on the ISMIR 2004\ngenreclassiﬁcationtrainingset(usingthesamealgorithm\nwe submitted last year) we obtain 79% accuracy without\nand only 64% with artist ﬁlter. On the large in-house col-\nlection(usingthesamealgorithm)weobtain71%without\nandonly27%withﬁlter. Therefor,intheresultsdescribed\nbelowwealwaysuseanartistﬁlterifnotstatedotherwise.\nIn the remainder of this section ﬁrst the four music\n630collections we use are described. Second, results using\nonly one similarity measure are presented. Third, pair-\nwise combinations with spectral similarity (AP) are eval-\nuated. Fourth, all four measures are combined. Finally,\nthe performances on all collections is evaluated to avoid\noverﬁtting.\n4.1 Data\nWe use four music collections with a total of almost 6000\npieces. Details are given in Tables 1 and 2. An important\ncharacteristic is that the collections are structured diff er-\nently and have different types of contents. This helps to\navoid overﬁtting.\n4.1.1 In-House Small (DB-S)\nThe smallest collection consists of 100 pieces. It is the\nsame used in [25]. However, we removed all classes con-\nsisting of one artist only. The categories are not strictly\ngenres (one of them is romantic dinner music). Further-\nmore,thecollectionalsoincludesonenon-musiccategory,\nnamely speech (German cabaret). This collection has a\nverygood(i.elow)ratiooftracksperartist. However,due\nto its size the results need to be treated with caution.\n4.1.2 In-House Large (DB-L)\nThe second largest collection has mainly been organized\naccording to genre/artist/album. Thus, all pieces from an\nartist (and album) are assigned to the same genre, which\nis questionable but common practice. Only two pieces\noverlap between DB-L and DB-S, namely Take Five and\nBlue Rondo by the Dave Brubeck Quartet. The genres\nare user deﬁned and inconsistent. In particular, there are\ntwo different deﬁnitions of trance. Furthermore, there are\noverlaps, for example, jazz and jazz guitar, heavy metal\nand death metal etc.\n4.1.3 Magnatune Small (DB-MS)\nThis collection was used as training set for the ISMIR’04\ngenre classiﬁcation contest. The music originates from\nMagnatune1and is licensed as creative commons. MTG2\ncompiledthecollection. AlthoughitisasubsetofDB-ML\nwe use it to compare our results to those of the ISMIR’04\nresults. However, while we report 79% accuracy for our\nlast year’s submission on the training set, the accuracy on\nthetestsetwas84%. Webelievethisisrelatedtotheartist\nﬁlter issue, as half of the pieces of each album were split\nbetween training and test set and all pieces from an artist\nbelong to the same genre.\nThe genre labels are given on the Magnatune website.\nThe collection is very unbalanced. Most pieces belong to\nthe genre classical and a large number of pieces in world\nsound like classical music. Some of the original Mag-\nnatune classes were merged by MTG due to ambiguities\nand the small number of tracks in some of the genres.\n4.1.4 Magnatune Large (DB-ML)\nThisisthelargestsetinourexperiments. DB-MSisasub-\nset of this collection. The number of artists is not much\n1http://www.magnatune.com\n2http://www.iua.upf.es/mtg29\n29\n2930\n28\n3132\n28\n3533\n25\n3630\n20\n3727\n19\n3526\n17\n3125\n17\n2923\n14\n2518\n6\n2117\n1\n15\n0102030405060708090100FP\nFP.F \nFP.G \n(a) DB-S\n27\n27\n2730\n27\n3030\n27\n2929\n25\n2830\n24\n2730\n23\n2629\n23\n2628\n22\n2526\n20\n2425\n18\n2223\n8\n8\n0102030405060708090100FP\nFP.F \nFP.G \n(b) DB-L\n64\n64\n6463\n66\n6464\n64\n6465\n63\n6465\n63\n6365\n61\n6163\n59\n6163\n58\n6162\n58\n6061\n54\n5758\n28\n42\n0102030405060708090100FP\nFP.F \nFP.G \n(c)DB-MS\n56\n56\n5657\n56\n5757\n56\n5658\n54\n5658\n54\n5557\n53\n5456\n53\n5455\n52\n5455\n52\n5352\n50\n5249\n25\n32\n0102030405060708090100FP\nFP.F \nFP.G \n(d) DB-ML\nFigure 3: Results for combining AP with one of the other mea-\nsures. All values are given in percent. The values on the x-axis\nare the mixing coefﬁcients. For example, the fourth column in\nthesecondrowistheaccuracyforcombining70%APwith30%\nof FP.F.\nhigher than in DB-MS and the genres are equally unbal-\nanced. The genres which were merged for the ISMIR’04\ncontest are separated.\n4.2 Individual Performance\nTheperformancesusingthesimilaritymeasuresindividu-\nally are given in Figure 3 in the ﬁrst (only spectral simi-\nlarity, AP) and last columns (FP, FP.F, FP.G). AP clearly\nperforms best, followed by FP. The performance of FP.F\nis extremely poor on DB-S while it is equal to FP.G\non DB-L. For DB-MS without artist ﬁlter we obtain:\nAP 79%, FP 66%, FP.F 30%, and FP.G 43% (using each\nindividually). Always guessing that a piece is classical\ngives 44% accuracy.\n4.3 Combining Two\nThe results for combining AP with one of the other mea-\nsures are given in Figure 3. The main ﬁndings are that\ncombiningAPwithFPorFP.Gperformsbetterthancom-\nbiningAPwithFP.F(exceptfor10%FP.Fand90%APin\nDB-MS). For all collections a combination can be found\nwhich improves the performance. However, the improve-\nments on the Magnatune collection are marginal. The\nsmooth changes of the accuracy with respect to the mix-\ning coefﬁcient are an indicator that the the approach is\nrelatively robust (within each collection).\n4.4 Combining All\nFigure 4 shows the accuracies obtained when all similar-\nitymeasuresarecombined. Thereareatotalof270possi-\n63129\n41\n39\n3530\n41\n39\n3633\n38\n41\n3734\n39\n41\n3939\n39\n41\n4038\n36\n38\n4138\n35\n36\n4139\n35\n35\n4139\n32\n29\n4141\n31\n21\n3741\n27\n19\n35100 95 90 85 80 75 70 65 60 55 50  \n05101520253035404550AP\nFP\nFP.F \nFP.G \n(a) DB-S\n27\n30\n31\n3230\n32\n32\n3231\n32\n32\n3232\n32\n32\n3232\n32\n31\n3132\n31\n31\n3032\n31\n30\n2932\n32\n29\n2931\n31\n28\n2932\n31\n26\n2831\n30\n23\n26100 95 90 85 80 75 70 65 60 55 50  \n05101520253035404550AP\nFP\nFP.F \nFP.G \n(b)DB-L\n64\n68\n66\n6767\n67\n68\n6868\n67\n67\n6767\n67\n67\n6767\n67\n66\n6767\n67\n66\n6667\n66\n65\n6567\n67\n65\n6567\n67\n64\n6567\n65\n64\n6467\n65\n61\n61100 95 90 85 80 75 70 65 60 55 50  \n05101520253035404550AP\nFP\nFP.F \nFP.G \n(c) DB-MS\n56\n57\n58\n5857\n58\n58\n5857\n58\n58\n5858\n58\n58\n5858\n58\n57\n5858\n58\n57\n5758\n58\n56\n5758\n58\n56\n5758\n58\n55\n5658\n57\n55\n5657\n57\n53\n54100 95 90 85 80 75 70 65 60 55 50 \n05101520253035404550AP\nFP\nFP.F \nFP.G \n(d) DB-ML\nFigure 4: Results for combining all similarity measures. A total\nof 270 combinations are summarized in each table. All values\nare given in percent. The mixing coefﬁcients for AP (the ﬁrst\nrow) are given above the table, for all other rows below. For\neach entry in the table of all possible combinations the highest\naccuracy is given. For example, the second row, third column\ndepicts the highest accuracy obtained from all possible combi-\nnations with 10% FP. The not speciﬁed 90% can have any com-\nbination of mixing coefﬁcients, e.g. 90% AP, or 80% AP and\n10% FP.G etc.\nblecombinationsusingastepsizeof5percent-pointsand\nlimitingAPtoamixingcoefﬁcientbetween100-50%and\nthe other measures to 0-50%.\nAnalogouslytothepreviousresultsFP.Fhastheweak-\nestperformanceandtheimprovementsfortheMagnatune\ncollection are hardly signiﬁcant. As in Figure 3 the\nsmooth changes of the accuracy with respect to the mix-\ning coefﬁcient are an indicator for the robustness of the\napproach (within each collection). Without the artist ﬁl-\nter the combinations on the DB-MS reach a maximum of\n81% (compared to 79% using only AP).\nIt is clearly noticeable that the results on the collec-\ntions are quite different. For example, for DB-S using as\nlittle AP as possible (highest values around 45-50%) and\na lot of FP.G (highest values around 25-40%) gives best\nresults. On the other hand, for the DB-MS collection the\nbestresultsareobtainedusing90%APandonly5%FP.G.\nThese deviations indicate overﬁtting, thus we analyze theWeights Classiﬁcation Accuracy\nRank AP FP FP.F FP.G DB-S DB-L DB-MS DB-ML Score\n165 15 5 15 38 32 67 58 1.14\n265 10 10 15 38 31 67 57 1.14\n370 10 5 15 38 31 67 58 1.14\n455 20 5 20 39 31 65 57 1.14\n560 15 10 15 38 31 66 57 1.14\n660 15 5 20 39 31 66 57 1.13\n775 10 5 10 37 31 67 58 1.13\n875 5 5 15 38 31 66 58 1.13\n965 10 5 20 38 30 66 58 1.13\n1055 5 10 30 41 29 65 56 1.13\n248100 0 0 0 29 27 64 56 1.00\n27050 0 50 0 19 23 61 53 0.85\nTable 3: Overall performance on all collections. Columns 2-4\nare the mixing coefﬁcients in percent and columns 5-8 are the\nrounded accuracies in percent.\nperformances across collections in the next section.\n4.5 Overall Performance\nTo study overﬁtting we compute the relative performance\ngain compared to the AP baseline (i.e. using only AP).\nWe compute the score (which we want to maximize) as\nthe average of these gains over the four collections. The\nresults are given in Table 3.\nTheworstcombination(using50%APand50%FP.F)\nyields a score of 0.85. (That is, in average, the accuracy\nusing this combination is 15% lower compared to the AP\nbaseline.) There are a total of 247 combinations which\nperform better than the AP baseline. Almost all of the 22\ncombinations thatfallbelow APhave alarge contribution\nof FP.F. The best score is 14% above the baseline. The\nrangesofthetop10rankedcombinationsare55-75%AP,\n5-20% FP, 5-10% FP.F, 10-30% FP.G.\nWithout artist ﬁlter, for DB-MS the top three ranked\ncombinations from Table 3 have the accuracies 1: 79%,\n2:78%,3:79%(theAPbaselineis79%,thebestpossible\ncombination yields 81%). For the DB-S collection with-\nout artist ﬁlter the AP baseline is 52% and the top three\nrankedcombinations have theaccuracies 1:63%, 2:61%,\n3: 62% (the best possible score achieved through combi-\nnation is 64%).\nFigure 5 shows the score of each combination ranked\nby their average (on all four collections) score. In several\ncases a combination performs well on one collection and\npoor on another. This indicates that there is a large po-\ntentialforoverﬁtting. Ontheotherhand,theperformance\nstaysabovethebaselineformostofthecombinationsand\nthere is a common trend. Truly reliable results would re-\nquire further testing on additional collections.\n5 CONCLUSIONS\nWepresentedanimprovementtoaudio-basedmusicsimi-\nlarityandgenreclassiﬁcation. Wecombinedspectralsim-\nilarity (in particular the approach presented by Aucou-\nturier and Pachet) with three additional similarity mea-\nsures based on ﬂuctuation patterns. We presented two\nnewdescriptorsandaseriesofexperimentsevaluatingthe\ncombinations.\n6320.811.2Average\n0.611.4DB−S\n0.711.3DB−L\n0.911.1DB−MS\n50 100 150 200 2500.911.1DB−ML\nFigure5: Score(y-axis)rankedbyaverageperformance(x-axis ).\nAlthough we obtained an average genre classiﬁcation\nperformance increase of 14%, our ﬁndings conﬁrm the\nglassceilingobservedin[2]. Inparticular,forthetraini ng\nset used for the ISMIR’04genre classiﬁcation contest our\nimprovements are hardly signiﬁcant. Furthermore, pre-\nliminary results with a larger number of descriptors indi-\ncate that the performance per collection can only be fur-\nther improved by up to 1-2 percentage points.\nOurresultsshowasigniﬁcantdifferenceintheoverall\nperformance if pieces from the same artist are in the test\nandtrainingset. Webelievethisshowsthenecessitytouse\nanartistﬁltertoevaluategenreclassiﬁcationperformanc e\n(ifallpiecesfromanartistareassignedtothesamegenre)\nand not the performance of artist identiﬁcation. However,\nsome the observed effects are partly also caused by the\nlow number of artists per genre. For example, for DB-L\ninsomecasesuptoonethirdofthepiecesfromthetarget\ngenre are removed by the artistﬁlter.\nAnother observation is that improvements on one col-\nlectionmightharmtheperformanceonanother. Thisdan-\nger of overﬁtting is imminent and a simple solution is the\nuse of different collections (with different contents and\ndifferent genre taxonomies).\nIngeneral,genreclassiﬁcationisnottheidealsolution\nto evaluate similarity. Although the assumption that the\nmost similar piece to a given piece belongs to the same\ngenre holds in many cases, a true evaluation would re-\nquire listening tests. However, a listening test where hu-\nman listeners are required to sort a complete collection\n(i.e. O(N2) comparisons) is infeasible for large collec-\ntions. Several alternatives exist and should be considered\nfor future work.\nAcknowledgements\nThis research was supported by the EU project SIMAC (FP6-\n507142). OFAI is supported by the Austrian Federal Ministries\nBMBWKandBMFIT.Theauthorswishtothanktheanonymous\nreviewers for their helpful comments.References\n[1] F. Pachet and D. Cazaly. A taxonomy of musical genres.\nInProc RIAO Content-Based Multimedia Information Ac-\ncess, 2000.\n[2] J.-J. Aucouturier and F. Pachet. Improving timbre similar-\nity: How high is the sky? Journal of Negative Results in\nSpeech and Audio Sciences , 1(1), 2004.\n[3] G. Tzanetakis, G. Essl, and P. Cook. Automatic musical\ngenreclassiﬁcationofaudiosignals. In ProcISMIR ,2001.\n[4] M. F. McKinney and J. Breebaart. Features for audio and\nmusic classiﬁcation. In Proc ISMIR ,2003.\n[5] K. West and S. Cox. Features and classiﬁers for the au-\ntomatic classiﬁcation of musical audio signals. In Proc\nISMIR,2004.\n[6] T. Pohle. Extraction of audio descriptors and their evalua-\ntion in music classiﬁcation tasks. MSc thesis, TU Kaiser-\nslautern, ¨OFAI, DFKI, 2005.\n[7] A. Zils and F. Pachet. Automatic extraction of music de-\nscriptorsfrom acoustic signals. In Proc ISMIR ,2004.\n[8] J.-J.AucouturierandF.Pachet. Musicsimilaritymeasures:\nWhat’s the use? In Proc ISMIR ,2002.\n[9] J. Foote. Content-based retrieval of music and audio. In\nMultimedia Storage and Archiving Systems II , 1997.\n[10] B. Logan and A. Salomon. A music similarity function\nbasedonsignalanalysis. In ProcIEEEIntlConfonMulti-\nmedia and Expo , 2001.\n[11] B.Logan. Musicrecommendationfromsongsets. In Proc\nISMIR,2004.\n[12] B. Logan. Content-based playlist generation: Exploratory\nexperiments. In Proc ISMIR , 2002.\n[13] A. Berenzweig, D. P.W. Ellis, and S. Lawrence. Anchor\nspaceforclassiﬁcationandsimilaritymeasurementofmu-\nsic. InProcIEEEIntlConfonMultimediaandExpo ,2003.\n[14] E. Pampalk. Islands of music: Analysis, organization, and\nvisualization of music archives. MSc thesis, Vienna Uni-\nversity of Technology, 2001.\n[15] E.Pampalk,A.Rauber,andD.Merkl. Content-basedorga-\nnizationandvisualizationofmusicarchives. In ProcACM\nMultimedia , 2002.\n[16] E. Pampalk, S. Dixon, and G. Widmer. On the evaluation\nof perceptual similarity measures for music. In Proc Intl\nConf on Digital Audio Effects , 2003.\n[17] A.Berenzweig, B.Logan,D.P.W.Ellis,andB.Whitman.\nA large-scale evaluation of acoustic and subjective music\nsimilarity measures. In Proc ISMIR , 2003.\n[18] J.S. Downie, J. Futrelle, and D. Tcheng. The international\nmusic information retrieval systems evaluation laboratory:\ngovernance, access and security In Proc ISMIR ,2004.\n[19] S. Baumann and O. Hummel. Using cultural metadata for\nartistrecommendation. In Proc WedelMusic Conf , 2003.\n[20] P.Knees,E.Pampalk,andG.Widmer. Artistclassiﬁcation\nwith web-based data. In Proc ISMIR ,2004.\n[21] B. Logan, A. Kositsky, and P. Moreno. Semantic analysis\nof song lyrics. In Proc IEEE Intl Conf on Multimedia and\nExpo 2004 , 2004.\n[22] F.Pachet,G.Westerman,andD.Laigre. Musicaldatamin-\ning for electronic music distribution. In Proc WedelMusic\nConf, 2001.\n[23] B. Whitman and S. Lawrence. Inferring descriptions and\nsimilarity for music from community metadata. In Proc\nIntl Computer Music Conf , 2002.\n[24] Y. Rubner, C. Tomasi, and L. J. Guibas. The earth movers\ndistance as a metric for image retrieval. Intl Journal of\nComputer Vision , 40(2), 2000.\n[25] E.Pampalk. AMatlabtoolboxtocomputemusicsimilarity\nfrom audio. In Proc ISMIR ,2004.\n[26] E. Pampalk, W. Goebl, and G. Widmer. Visualizing\nchanges in the structure of data for exploratory feature se-\nlection. In Proc ACM SIGKDD Intl Conf on Knowledge\nDiscovery and Data Mining , 2003.\n633"
    },
    {
        "title": "Dynamic Playlist Generation Based on Skipping Behavior.",
        "author": [
            "Elias Pampalk",
            "Tim Pohle",
            "Gerhard Widmer"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414932",
        "url": "https://doi.org/10.5281/zenodo.1414932",
        "ee": "https://zenodo.org/records/1414932/files/PampalkPW05.pdf",
        "abstract": "Common approaches to creating playlists are to randomly shuffle a collection (e.g. iPod shuffle) or manually select songs. In this paper we present and evaluate heuristics to adapt playlists automatically given a song to start with (seed song) and immediate user feedback. Instead of rich metadata we use audio-based similarity. The user gives feedback by pressing a skip button if the user dislikes the current song. Songs similar to skipped songs are removed, while songs similar to accepted ones are added to the playlist. We evaluate the heuristics with hypothetical use cases. For each use case we assume a specific user behavior (e.g. the user always skips songs by a particular artist). Our results show that using audio similarity and simple heuristics it is possible to drastically reduce the number of necessary skips. 1",
        "zenodo_id": 1414932,
        "dblp_key": "conf/ismir/PampalkPW05",
        "keywords": [
            "playlists",
            "randomly shuffle",
            "manually select songs",
            "heuristics",
            "adapt playlists",
            "audio-based similarity",
            "user feedback",
            "skip button",
            "songs similar",
            "accepted ones"
        ],
        "content": "DYNAMIC PLAYLIST GENERATION BASED ON SKIPPING BEHAVIOR\nElias Pampalk1, Tim Pohle1, Gerhard Widmer1,2\n1Austrian Research Institutefor Artiﬁcial Intelligence (O FAI), Freyung 6/6, 1010 Vienna, Austria\n2Department of Computational Perception, Johannes Kepler U niversity, Linz, Austria\nABSTRACT\nCommonapproachestocreatingplaylistsaretorandomly\nshufﬂe a collection (e.g. iPod shufﬂe) or manually select\nsongs. In this paper we present and evaluate heuristics\nto adapt playlists automatically given a song to start with\n(seed song) and immediate user feedback.\nInstead of rich metadata we use audio-based similar-\nity. The user gives feedback by pressing a skip button\nif the user dislikes the current song. Songs similar to\nskipped songs are removed, while songs similar to ac-\ncepted ones are added to the playlist. We evaluate the\nheuristics with hypothetical use cases. For each use case\nwe assume a speciﬁc user behavior (e.g. the user always\nskips songs by a particular artist). Our results show that\nusing audio similarity and simple heuristics it is possible\nto drastically reduce the number of necessary skips.\n1 INTRODUCTION\nThere are different ways to create playlists. One extreme\nis to very carefully select each piece and the order in\nwhich the pieces are played. Another extreme is to ran-\ndomly shufﬂe all pieces in a collection. While the ﬁrst\napproach is very time consuming, the second approach\nproduces useless results if the collection is very diverse.\nIn this paper we present an alternative which requires\nlittle user interaction even for very diverse collections.\nThe goal is to minimize user input and maximize satis-\nfaction. Our assumptions are (1) a seed song is given. We\ndo not tackle the problem of browsing a large collection\nto ﬁnd a song to start with. If more than one song to start\nwith is given then the task is simpliﬁed. (2) We assume\nthat a skip button is available and easily accessible to the\nuser. Forexample,thisisthecaseiftheuserrunsWinamp\nwhilebrowsingtheInternet. (3)Finally,weassumealazy\nuser who is willing to sacriﬁce quality for time. In partic-\nular,weassumealltheuseriswillingtodoispressaskip\nbutton if the song currently played is a bad choice.\nIn this paper we present simple heuristics to dynam-\nically propose the next song to be played in a playlist.\nTheapproachisbasedonaudiosimilarityandwetakethe\nuser’sskippingbehaviorintoaccount. Theideaistoavoid\nsongs similar to songs which were skipped and focus on\nsongssimilartoacceptedones. Weevaluatetheheuristics\nbased on hypothetical use cases.\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrstpage.\nc/circlecopyrt2005 Queen Mary, University of LondonPrevious work on playlist generation has partly dealt\nwith algorithms to efﬁciently ﬁnd a playlist which fulﬁlls\ngiven constraints (e.g. [1, 2]). These approaches assume\nthe existence of rich metadata. A commercial product to\ngenerate playlists from proprietary metadata is available\nfrom Gracenote.1This “dynamic” playlist generator up-\ndates playlists when the music collection is modiﬁed.\nA conceptually very similar approach to our work is\nthe internet radio station Last.FM2which creates a user\nproﬁle based on immediate user feedback. Last.FM is\nbuilt on collaborative ﬁltering and uses “Love”, “Skip”,\n“Ban” buttons as input.\nAnother approach using immediate user feedback and\nrichmetadatawaspresentedin[3]. Themaindifferenceto\nourworkisthatinourevaluationsrandomshufﬂingwould\ncompletely fail. Furthermore, our heuristics have no pa-\nrameters which need to be trained and thus we expect our\napproachtobemorerobustandrequirelessuserfeedback.\nUnlike these previous approaches we do not rely on\nmetadata or collaborative ﬁltering. Purely audio-based\nplaylist generation was proposed for example in [5] and\n[4]. In[5]theauthorshowedthatsimplyusingthe nnear-\nest songs to a seed song as a playlist performs relatively\nwell. In[4]travelingsalesmanalgorithmsareusedtoﬁnd\na path through the whole collection.\n2 METHOD\nAtthecoreofourapproachistheaudio-basedmusicsim-\nilaritymeasure. Weuseacombinationofspectralsimilar-\nity[6,7],ﬂuctuationpatterns[8],andsomeotherdescrip-\ntors. The details are described in [9].\nFrom a statistical (or machine learning) point of view\nitisproblematictousecomplexmodels(orlearners)with\na high number of parameters to learn user preferences.\nThe main reason for this is that in an ideal case the num-\nber of negative examples is extremely low (e.g. less than\n5) and even the number of positive examples is not very\nhigh (e.g. 20 tracks can ﬁll an hour of music).\nTo generate the playlists we use the following 4\nheuristics . Candidate songs are all songs in the collection\nwhich have not been played (or skipped) yet.\n(A) As suggested in [5] the nnearest neighbors to\nthe seed song are played ( n= accepted + skipped). This\nheuristic creates a static playlist and is the baseline we\nwant to improve upon.\n(B) The candidate song closest to the last song ac-\ncepted by the user is played. This is similar to heuristic\nAwiththeonlydifferencethattheseedsongisalwaysthe\nlast song accepted.\n(C) The candidate song closest to any of the accepted\nsongs is played. Using the minimum distance for recom-\n1http://www.gracenote.com/gn products/playlist.html\n2http://last.fm\n634mendations from song sets was proposed in [10].\n(D) For each candidate song, let dabe the distance\nto the nearest accepted, and let dsbe the distance to the\nnearest skipped. If da< d s, then add the candidate to\nthe set S. From Splay the song with smallest da. IfS\nis empty, then play the candidate song which has the best\n(i.e. the lowest) da/dsratio.\n3 EVALUATION\nIn the hypothetical use cases we assume that the user\nwants to listen to one hour of music which is approxi-\nmately 20 songs. The number of skips are counted until\nthese 20 songs are played. The use cases (UC) are the\nfollowing:\n(1) The user wants to listen to songs similar to the\nseed. We measure this by equating similarity with genre\nmembership. Any song outside of the seed’s genre is\nskipped.\n(2) The user wants to listen to similar music but dis-\nlikesaparticularartist(fornotmeasurablereasonssucha s\npersonaltaste). Tomeasurethisweusethesameapproach\nasforUC-1. Anunwantedartistfromtheseed’sgenre(not\nthe artist of the seed song) is randomly selected. Every\ntimeasongoutsidetheseed’sgenreorfromtheunwanted\nartist is played, skip is pressed.\n(3)Theuser’spreferenceschangeovertime. Wemea-\nsure this as follows. Let A be the genre of the seed song\nand B a related genre which the user starts to prefer. The\nﬁrst 5 songs are accepted if they are from genre A. The\nnext 10 are accepted if they are from either A or B. The\nlast 5 are accepted if they are from B. We manually se-\nlected pairs of genres for this use case. The list of pairs\ncan be found in Table 3. Unlike UC-1 and UC-2 it is pos-\nsible that in UC-3 a state is reached where none of the\ncandidate songs would be accepted although the number\nof accepted is less than 20. In such cases the remaining\nsongs in the collection are added to the skip count.\nFor UC-1 and UC-2 the evaluation is run using every\nsong in the collection as seed. For UC-3 every song in\ngenre A is used.\nOne of the biggest problems for our evaluation is that\nwe do not have enough artists per genre to implement an\nartistﬁlter. Thatis,wedonotavoidplayingseveralsongs\nfrom the same artistright after each other.\nAnother issue is that we assume only songs the user\ndislikes are skipped. However, if a song is skipped be-\ncause, e.g., the user just heard it on the radio (but likes\nit otherwise) our heuristics will be mislead. To evaluate\nthis we could have included randomly pressed skips. To\nsolve this the user could be given more feedback options.\nFor example, how long or hard the skip button is pressed\ncould indicate how dissimilar the next song should be.\n3.1 Data\nThe collection we use contains 2522 tracks from 22\ngenres (see Table 1 for further statistics). The genres\nand the number of tracks per genre are listed in Fig. 2.\nThe collection has mainly been organized according to\ngenre/artist/album. Thus, all pieces of an artist are as-\nsigned to the same genre, which is questionable but com-\nmon practice. The genres are user deﬁned and inconsis-Artists/Genre Tracks/Genre\nGenres Artists Tracks Min Max Min Max\n22 103 2522 3 6 45 259\nTable 1: Statistics of the music collection.\nHeuristic Min Median Mean Max\nUC-1 A 0 37.0 133.0 2053\nB 0 30.0 164.4 2152\nC 0 14.0 91.0 1298\nD 0 11.0 23.9 425\nUC-2 A 0 52.0 174.0 2230\nB 0 36.0 241.1 2502\nC 0 17.0 116.9 1661\nD 0 15.0 32.9 453\nTable 2: Number of skips for UC-1 and UC-2.\n15 10 15 200 5 10\nPlaylist PositionMean Skips\n(a) Heuristic A\n15 10 15 20012\nPlaylist PositionMean Skips\n(b) Heuristic D\nFigure 1: Skips per playlist position for UC-1.\ntent. In particular, there are two different deﬁnitions of\ntrance. Furthermore, there are overlaps, for example, jazz\nand jazz guitar, heavy metal and death metal etc.\n3.2 Results\nFor UC-1 using random shufﬂe to generate the playlist\nwould require more than 300 skips in half of the cases\nwhile heuristic A requires less than 37 skips in half of the\ncases. Table 2 shows the results for UC-1 and UC-2. The\nmain observation is that the performance increases from\nheuristic A to D. In general, there are a lot of outliers\nwhich is reﬂected in the large difference between mean\nand median. In a few cases almost all songs from the col-\nlection are proposed until 20 songs from the seed genre\nareintheplaylist. HeuristicDhassigniﬁcantlyfewerout-\nliers. HalfofallcasesforheuristicDinUC-1requireless\nthan 11 skips which might almost be acceptable.\nFig.1showsthatforD/UC-1thereisalargenumberof\nskipsaftertheﬁrstsong(seedsong). Oncethesystemhas\na few positive examples the number of skips decreases.\nOn the other hand, for heuristic A, the number of skips\ngradually increases with the playlist position. (Note that\nonemustbecarefulwheninterpretingthemeanbecauseit\nis strongly inﬂuenced by a few outliers.)\nFig. 2 shows that for D/UC-1 some genres work very\nwell (e.g. jazz guitar or heavy metal - trash), while others\n635Heuristic A Heuristic B Heuristic C Heuristic D\nStart Goto Median Mean Median Mean Median Mean Median Mean\nEuro-Dance Trance 69.0 171.4 36.0 64.9 41.0 69.0 20.0 28.3\nTrance Euro-Dance 66.0 149.1 24.0 79.1 6.5 44.4 4.5 8.8\nGerman Hip Hop Hard Core Rap 33.0 61.9 32.0 45.6 31.0 40.7 23.0 28.1\nHard Core Rap German Hip Hop 21.5 32.2 18.0 51.9 16.0 24.2 14.0 16.1\nHeavy Metal/Thrash Death Metal 98.5 146.4 54.0 92.5 58.0 61.1 28.0 28.4\nDeath Metal Heavy Metal/Thrash 14.0 69.2 16.0 53.7 3.0 55.5 3.0 25.7\nBossaNova Jazz Guitar 68.5 228.1 32.0 118.7 54.0 61.1 22.0 21.3\nJazz Guitar BossaNova 21.0 26.7 22.0 21.5 9.0 10.5 6.0 6.2\nJazz Guitar Jazz 116.0 111.3 53.0 75.7 45.0 74.0 18.5 27.3\nJazz Jazz Guitar 512.5 717.0 1286.0 1279.5 311.0 310.8 29.0 41.3\nA Cappella Death Metal 1235.0 1230.5 1523.0 1509.9 684.0 676.5 271.0 297\nDeath Metal A Cappella 1688.0 1647.2 1696.0 1653.9 1186.0 1187.3 350.0 309.2\nTable 3: Number of skips for UC-3.\nfail(e.g. electronicordowntempo). However,someofthe\nfailures make sense. For example, before 20 pieces from\nelectronic are played, in average almost 18 pieces from\ndowntempo are proposed.\nTable 3 gives the results for UC-3. As for the other\nuse cases the performance increases from A to D in most\ncases. We have included the pair a capella to death metal\nas an extreme to show the limitations (we do not consider\nsuch a transition to be a likely user scenario). In three of\nthe cases for heuristic D the median seems to be accept-\nably low.\nThe number of skips depends a lot on the direction of\nthe transition. For example, moving from jazz guitar to\nbossanovarequires,inhalfofthecases,lessthan6skips.\nMoving in the other direction requires almost 3 times as\nmany skips. This is also reﬂected in Fig. 2. Speciﬁcally,\njazz guitar to bossa nova works well because jazz guitar\nis mainly confused with bossa nova. On the other hand\nbossanovaisconfusedwithmanyothergenres. Thesame\ncan be observed, e.g., for the pair trance and euro-dance.\nFig.3showswhereskipsoccurforUC-3andheuristic\nD, and how often each genre was played per playlist po-\nsition. In some cases during the transition phase (where\ngenre A or B are accepted) basically only genre A is\nplayed. When the transition is forced (after the 15th song\nin the playlist) the number of skips drastically increases.\nInothercasesthetransitionworksverynicely. Anobvious\ndirection for further improvement is to include a memory\neffect to allow the system to quickly forget previous user\nchoices. However,preliminaryexperimentsweconducted\nin this direction did not show signiﬁcant improvements.\n4 CONCLUSIONS\nWe have presented an approach to dynamically create\nplaylistsbasedontheuser’sskippingbehavior. Weevalu-\nated the approach using hypothetical use cases for which\nwe assume speciﬁc behavior patterns. Compared to the\napproach suggested in [5], heuristic D reduces the num-\nberofskipsdrastically. Insomeofthecasesthenecessary\nnumber ofskipsseemslowenough forarealworldappli-\ncation.\nThemainlimitationofourevaluationisthatwedidnot\nimplement an artist ﬁlter (to avoid having a large number\nof pieces from the same artist right after each other in aplaylist) due to the small number of artist per genre.\nThe heuristic depends most of all on the similarity\nmeasure. Any improvements would lead to fewer skips.\nHowever, implementing memory effects (to forget past\ndecisionsoftheuser)orallowingthesimilaritymeasureto\nadapttotheuser’sbehaviorarealsointerestingdirection s.\nFor use cases related to changing user preferences a key\nissuemightbetotrackthedirectionofthischange. Incor-\nporating additional information such as web-based artist\nsimilarity or modeling the user’s context more accurately\n(based on data from long term usage) are other options.\nAlthough evaluations based on hypothetical use cases\nseems to be sufﬁcient for the current development state,\nexperiments with humans listeners will be necessary in\nthe long run.\nAcknowledgements\nThis research was supported by the EU project FP6-507142\n(SIMAC). OFAI is supported by the Austrian ministries\nBMBWKand BMVIT.\nReferences\n[1] M. Alghoniemy and A. Tewﬁk. A network ﬂow model for\nplaylistgeneration.In ProcIEEEIntlConfMultimediaand\nExpo, 2001.\n[2] J.-J. Aucouturier and F. Pachet. Scaling up music playlist\ngeneration. In Proc IEEE Intl Conf on Multimedia Expo ,\n2002.\n[3] S. Pauws and B. Eggen. PATS: Realization and user eval-\nuation of an automatic playlist generator. In ISMIR, 2002.\n[4] T. Pohle, E. Pampalk, and G. Widmer. Generating\nsimilarity-based playlists using traveling salesman algo-\nrithms. In Proc Intl Conf Digitial Audio Effects , 2005.\n[5] B. Logan. Content-based playlist generation: Exploratory\nexperiments. In Proc ISMIR , 2002.\n[6] J.-J. Aucouturier and F. Pachet. Improving timbre similar-\nity: How high is the sky? Journal of Negative Results in\nSpeech and Audio Sciences , 1(1), 2004.\n[7] B. Logan and A. Salomon. A music similarity function\nbasedonsignalanalysis. In ProcIEEEIntlConfonMulti-\nmedia and Expo , 2001.\n[8] E. Pampalk. Islands of music: Analysis, organization, and\nvisualization of music archives. MSc thesis, Vienna Uni-\nversity of Technology, 2001.\n[9] E. Pampalk, A. Flexer, and G. Widmer. Improvements of\naudio-based music similarity and genre classiﬁcation. In\nProc ISMIR ,2005.\n[10] B.Logan. Musicrecommendationfromsongsets. In Proc\nISMIR,2004.\n6360.3 0.1 0.4 0.2 0.2 0.6 0.3 0.2 0.5 0.4 0.2 0.2 0.5 0.1 0.1 0.2 0.2 0.1 0.3 0.1\n1.2 2.4 1.0 2.9 0.4 5.0 7.8 3.6 4.1 7.5 8.9 5.5 0.6 6.1 4.3 1.1 0.8 3.4 1.9 3.5 2.5\n0.1 2.3 0.9 2.9 0.3 0.2 0.7 0.9 0.1 3.6 1.1 1.0 0.5 13.92.7 5.4 4.0 1.2 0.1 0.1\n0.3 0.1 1.8 2.4 1.7 0.3 0.5 2.5 1.0 4.6 3.0 1.4 0.8\n0.8 1.6 0.5 0.7 0.2 0.1 1.1 0.1 0.3 0.3 3.9 0.2 0.2 1.0 1.3\n0.3 0.2 0.3 0.2 0.9 0.8 1.2 0.9 0.3 5.3 0.6 0.3 5.2 0.7 0.2 0.2 2.5 7.5 0.1 0.6 0.2\n0.3 1.2 0.2 0.1 0.1 1.0 0.3 2.7 3.2 3.1 10.00.7 1.7 1.8 0.1 1.9 2.6 1.2 0.8\n1.6 4.2 2.9 2.5 3.7 0.6 4.0 5.8 4.0 6.2 8.6 4.7 2.8 3.3 3.0 1.7 2.2 4.7 1.8 3.1 2.8\n4.0 8.5 1.6 5.0 7.9 2.3 13.0 17.9 10.2 8.5 8.5 6.3 2.5 4.3 3.6 8.5 1.9 3.1 2.0 8.0 4.2\n0.1 0.2 0.1 0.1 0.3 0.1 0.3 0.5 0.4 0.3 0.6 0.5 0.1 0.1 0.1 0.1 0.1 0.1 0.7 0.6\n0.5 0.1 0.4 0.3 0.4 0.9 0.1 0.2 0.2 0.2 0.6 0.5 2.6 1.8 0.2 0.1 2.4 4.2 0.4 0.1 0.1\n0.7 0.8 0.8 0.4 0.7 0.3 0.7 0.3 0.2 1.3 1.9 0.2 4.1 1.9 0.1 0.2 1.0 0.5 0.2 0.1\n0.4 1.6 1.2 0.5 0.8 0.2 0.3 3.2 8.3 0.1 4.9 4.0 0.2 2.7 3.2 0.1\n0.1 0.2 0.1 0.1 0.3 0.1 0.2 0.1 0.1 1.4\n0.1 0.1 0.1 0.2 0.2 0.1 0.3 0.4 0.2 0.1 0.5 0.5 0.5 0.3 0.1 0.1 0.2 1.3 0.1 0.2\n0.5 5.9 3.6 4.4 2.6 0.1 0.3 3.3 0.3 0.6 2.5 12.37.6 0.7 9.2 2.2 1.2 1.8 1.7 0.2 0.2\n0.7 0.3\n0.1 0.1 0.2 0.1 0.8 0.9 0.2 0.3 0.2 0.2 3.3 0.2 0.1 1.1 0.5 0.1 5.2 0.1 0.1\n0.1 0.1 0.1 0.5 0.1 0.2 0.1 0.3 0.1 0.1 0.1 0.2 0.1 0.1 0.1\n1.2 1.0 1.7 0.3 1.5 0.8 0.7 0.9 0.7 1.1 5.0 10.20.2 3.7 1.2 0.6 0.1 0.6 0.2 0.2\n0.7 2.6 0.5 1.3 0.9 2.3 2.5 2.9 16.14.1 3.1 1.4 1.0 2.3 0.6 0.7 2.0 4.0 0.4 4.2\n0.6 2.7 0.4 0.5 0.6 0.1 1.7 3.4 2.5 15.22.5 1.5 1.6 1.1 0.4 0.4 0.4 0.9 1.1 1.2 2.7A Cappella 112\nAcid Jazz 68\nBlues 63\nBossa Nova 72\nCeltic 132\nDeath Metal 72\nDnB 72\nDowntempo 124\nElectronic 63\nEuro−Dance 97\nFolk−Rock 238\nGerman Hip Hop 143\nHard Core Rap 164\nHeavy Metal − Thrash 242\nItalian 142\nJazz 64\nJazz Guitar 70\nMelodic Metal 169\nPunk 259\nReggae 47\nTrance 64\nTrance2 45AC AJ Blu BN Cel DM DnB DT Ele ED FR GH HC HM Ita Jaz JG MM Pun Roc Tra T2\n4.9\n74.4\n42.0\n20.5\n12.5\n28.3\n32.9\n74.4\n131.8\n5.5\n16.1\n16.3\n31.9\n2.7\n5.7\n61.1\n1.1\n13.6\n2.3\n32.1\n53.6\n41.5Sum\nFigure 2: Mean number of skips per genre for heuristic D in UC- 1. For example, the ﬁrst line shows how many songs (in\naverage, computed as the mean) from each genre were skipped f or playlists starting with an a capella seed. The number\nto the left of the table (e.g. 112) is the number of total track s in the genre. The number on the right side of the table (4.9)\nis the mean of the total number of skips.\n012Mean SkipsTrance ⇒ Euro−Dance\n15 10 15 2001\nPlaylist PositionRatio B/(A+B)0 1020Mean SkipsEuro−Dance ⇒ Trance\n15 10 15 2001\nPlaylist PositionRatio B/(A+B)\n012Mean SkipsJazz Guitar ⇒ Bossa Nova\n15 10 15 2001\nPlaylist PositionRatio B/(A+B)0246Mean SkipsBossa Nova ⇒ Jazz Guitar\n15 10 15 2001\nPlaylist PositionRatio B/(A+B)\nFigure 3: Average skips and genre ratio per playlist positio n for heuristic D in UC-3. The genre ratio is 0 if only genre\nA (the genre of the seed) is played and 1 if only genre B (destin ation genre) is played. The circle marks the last and ﬁrst\nsong which is forced to be froma speciﬁc genre.\n637"
    },
    {
        "title": "Polyphonic Musical Sequence Alignment for Database Search.",
        "author": [
            "Bryan Pardo",
            "Manan Sanghi"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417909",
        "url": "https://doi.org/10.5281/zenodo.1417909",
        "ee": "https://zenodo.org/records/1417909/files/PardoS05.pdf",
        "abstract": "Finding the best matching database target to a melodic query has been of great interest in the music IR world. The string alignment paradigm works well for this task when comparing a monophonic query to a database of monophonic pieces. However, most tonal music is polyphonic, with multiple concurrent musical lines. Such pieces are not adequately represented as strings. Moreover, users often represent polyphonic pieces in their queries by skipping from one part (the soprano) to another (the bass). Current string matching approaches are not designed to handle this situation. This paper outlines approaches to extending string alignment that allow measuring similarity between a monophonic query and a polyphonic piece. These approaches are compared using synthetic queries on a database of Bach pieces. Results indicate that when a monophonic query is drawn from multiple parts in the target, a method which explicitly takes the multi-part structure of a piece into account significantly outperforms the one that does not.",
        "zenodo_id": 1417909,
        "dblp_key": "conf/ismir/PardoS05",
        "keywords": [
            "string alignment",
            "monophonic query",
            "polyphonic piece",
            "synthetic queries",
            "Bach pieces",
            "multi-part structure",
            "similarity measurement",
            "approaches comparison",
            "performance evaluation",
            "results analysis"
        ],
        "content": "POLYPHONIC MUSICAL SEQUENCE ALIGNMENT FOR DATABASE SEARCH \nBryan Pardo and Manan Sanghi \nComputer Science Department \nNorthwestern University \n1890 Maple Ave., Evanston, IL, USA \npardo,manan@northwestern.edu  \n \nABSTRACT  \nFinding the best matching database target to a melodic \nquery has been of great interest in the music IR world. The string alignment paradigm works well for this task when comparing a monophonic query to a database of monophonic pieces . However, most tonal \nmusic is polyphonic, with mu ltiple concurrent musical \nlines. Such pieces are not adequately represented as \nstrings. Moreover, users often represent polyphonic \npieces in their queries by skipping from one  part (the \nsoprano) to another (the bass). Current string matching approaches are not designed to handle this situation. This paper outlines approaches to extending string alignment that allow measuring similarity between a monophonic query and a polyphonic piece. \nThese approaches are compared using synthetic queries on a database of B ach pieces. Results indicate \nthat when a monophonic query is drawn from multiple parts in the target, a method which explicitly takes the multi-part structure of a piece into account significantly outperforms the one that does not. \n1. INTRODUCTION \nFinding the best matching database target to a melodic \nquery has been a subject of great recent interest in the \nmusic IR world [1-4]. Query-by-humming (QBH) systems [2, 3, 5-10] are, perhaps, the most common application of current techniques to find the best match for a query melody. QBH systems let users pose queries to a database by singing or humming. \nFigure 1 shows a system diagram outlining a typical \nQBH system. Here, a sung query is transcribed into a string. Targets are indexed by one or more melodic sequences. A similarity ranker compares the transcribed query to the targets in the database and returns a ranked similarity list. Both query and target are typically represented as \na sequence of symbols drawn from a finite alphabet. Such sequences are commonly called strings. The dominant melodic query matching techniques investigated in the literature have been n-grams, edit-distance based string matching, and Markov models. \n \n  \nFigure 1. A typical Query-by-humming (QBH) system. \nPickens [11] found n-grams superior to language \nmodels, however results from n-grams were only \ngood when the full contents of a piece in the database were passed in as a query. Downie and Nelson [12] performed a systematic investigation of the best length n-gram to use, given queries and targets encoded as sequences of pitch intervals. Uitenbogerd \nand Zobel [13] compared two kinds of n-gram \nmeasures to local string alignment with a fixed match score. Local string alignment was found to beat the performance of n-gram based matching. Other researchers have investigated stochastic methods for music retrieval, particularly Markov models [2, 10, 14] and probabilistic string matchers [9]. \nWith Markov models, a monophonic query is \ntreated as an observation sequence and a theme is judged similar to the query if the associated Markov model has a high likelihood of generating the query. Markov models described in the literature are currently based on monophonic melodic themes.   \nExisting probabilistic string matchers compare \nmonophonic query strings with monophonic database targets, but take into account possible user error through the use of a match score matrix.  This matrix \ndetermines the edit distan ce between a query element \nSimilarity \nRanker Sgt. Pepper’s \nYesterday \nRanked List \nSung Query\nTranscriptionDatabase \nParts             Targets \n5\n12345678944455555\nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom  use is granted without fee \nprovided that copies are not made or distributed for profit or \ncom-mercial advantage and that copies bear this notice and the \nfull citation on the first page. \n© 2005 Queen Mary, University of London \n215and a target element by encoding the probability that \na particular substitution would be made, based on prior training examples. \nPardo and Birmingham [9] and Dannenberg, et al. \n[15] found no statistically significant difference between the precision and recall of systems based on probabilistic string matching and those based on \nMarkov models, while the string matchers \nconsistently performed on the order of 100 times faster on real-world queries.  \nGiven the speed advantage of string matching over \nMarkov models and the performance advantage of string matching over n-gram based techniques, this paper concentrates on extending probabilistic string-matching techniques to a new category of problem: alignment of monophonic queries to polyphonic targets. \n2. MULTIPLE ALIGNMENT \nPROBLEM \nStandard string matching [16-19] measures the \ndistance between two strings,  S1 and S2, as the number \nof edit operations required to turn S1 into S2, given a \nfixed set of allowable operations.  A typical set of edit operations would include deletion of a string element, insertion of an element, and matching between an element of S\n1 and an element of S2. Given a query \nstring and a set of target strings drawn from a database, the closest target is the one with the lowest cost transformation into the query . \nThis paradigm works well when comparing a \nmonophonic query to a database of monophonic pieces, as the query and the targets are all \nappropriately represented as strings. However, most \ntonal music is polyphonic, with multiple concurrent \nmusical lines. Such pieces are not adequately represented by individual strings. The Bach chorale in Figure 2 is a typical example of polyphonic music. \n \nFigure 2. To God On High A ll Glory Be – Melody: N. \nDecius, Arrangement: J. S. Bach, measures 1 through 4 \nExisting string matching methods can be used to \ncompare a monophonic queries to a polyphonic \ndatabase by keying each polyphonic piece (hereafter \nreferred to as a score ) with a collection of \nmonophonic parts, corresponding to the individual parts (such as the bass line and vocal line) in the written score. This is illustrated in Figure 1. The \nquery can then be matche d against each individual \npart in the song. The edit distance score for the best-\nmatching part is then used as the similarity estimate for the entire song. \nThis approach works well as long as the user sings \na query drawn only from a single track in the song. However, users often repr esent polyphonic pieces in \ntheir queries by skipping from one part to another. It is not unusual, for example, for a person imitating a rock song to sing a portion of the bass line, interspersed with portions of the melody. \n \nFigure 3. Query based on To  God On High All Glory Be \nConsider the query expressed as written notation \nin Figure 3. This query corresponds to the sequence \nof linked boxes in Figure 2.  Current string matching approaches are not designed to handle such a query. \nRecently, researchers have explored methods to \nmeasure the similarity between polyphonic pieces using n-grams [7]. This work has inspired us to look \nat approaches to measuring polyphonic similarity \nusing edit-distance based string matching algorithms. We now describe these approaches. \n3. DEFINITION OF TERMS \nFigure 4 shows the same passage as Figure 2 in piano-\nroll style notation. Here, each  note is indicated by a \nhorizontal line. The length of the line indicates the duration of the note. The horizontal placement indicates the onset time. The vertical placement indicates pitch. The pitch class for each note is indicated by a letter at the onset of the note.  \n \n  \n \n   \n \n  C4\n406080\nG\nDB\nGA\nCB\nG\nD\nGF#C\nGE\nEGD\nF#\nA\nDBC\nE\nC\nAF#\nDB\nGE\nEF#\nB\nDA\nE\nC\nCB\nEb\nF#\nBB\nE\nGEAF#B\nDB\nGE\nCAF#\nD\nF#BC\nGE\nEB\nDA\nCF#\nD\nACGG\nD\nBE\nC\nCBF#\nD\nA\nDG\nDB\nGG\nDB\nGA\nC\n \nFigure 4. To God On High A ll Glory Be in piano roll \nnotation \nA musical score, in its most generic form, is a \ncollection of parts , such as the Soprano and Bass \nparts in Figure 2.  Each part is a collection of notes. \nThe organization of a song as a collection of one or more (possibly polyphonic) parts mirrors that of the standard MIDI Type 1 file [20], in which songs are organized into (possibly polyphonic) tracks. A MIDI \nType 0 file contains only a single track.  \nWe define a note n\ni by an ordered 3-tuple ( si,ei,pi) \nwhere si is the start time of the note, ei is the time at \nwhich the note ends and pi is the pitch of the note. \nFor example consider the two-part musical score \nrepresented as piano-roll in Figure 5. In the figure, \nnotes in part P1 begin with a circle. Notes in part P2 \nbegin with a diamond. Start times are defined by the \norder of note onsets. Thus, a start time of 3 indicates the third note onset. The pitch is encoded as MIDI pitch number and listed above the note.  \nThe score and its two parts can be expressed as \nfollows: \nS\n = {P1, P2} \nP1 ={(1,3,38), (2,3,36), (4,6,38), (6,8,38)}  \nP2={(1,2,24), (1,3,22), (3,5,22), (6,7,24)} \n216The division of notes into parts is something that is \nroutinely available in musical scores encoded as \nFinale, Sibelius, and MIDI files. Parts captures basic underlying musical structure and we would like our matching algorithms to be sensitive to this information. In the absence of part information, the \nscore can be thought of to be consisting of just one \npart with all the notes belonging to that single part. \n \nFigure 5. A simple score in piano roll notation \nWe classify a score into three categories: \nmonophonic , homophonic  and polyphonic . A score is \nmonophonic if no new note in the score begins until \nthe current note has finished sounding. In a \nmonophonic score  S, there is a total ordering of \nnotes. For every pair of notes ni=(si,ei,pi) and     \nnj=(sj,ej,pj) in S,  si≠sj, ei≠ej and if si<sj then ei<sj.  \nIn homophonic music, notes may sound \nsimultaneously, but they must start and end at the \nsame time. For every pair of overlapping notes (i.e. \nconcurrently sounding notes) ni=(si,ei,pi) and      \nnj=(sj,ej,pj) in a homophonic score , it must hold that  \nsi=sj, and ei=ej.  \nIf a score is neither monophonic, nor homophonic, \nit is polyphonic. In this case, there are at least two \nnotes in the score that sound concurrently but either do not end or do no start concurrently, violating the constraints for both monophonic and homophonic scores. The Bach score in Figure 2 is polyphonic. \n4. FINDING TARGETS IN A \nDATABASE \nWe are interested in finding relevant targets in a database in response to a user query.  \nA query, Q, is a monophonic, one part score. We \ndefine a database , D, as set of scores { S\n1,…,S r}. We \nwill consider separately the cases when the scores in the database are monophonic, homophonic or polyphonic.  \nGiven a database of scores, D, and a query, Q, we \nassume there is a target set T⊆D that corresponds to \nthe query. This is the set of scores that the user would \nlike to access using the query. For the purpose of this \npaper, we assume that T consists of a single correct \nscore and that this score is the one which is most \nsimilar to the query. The question then becomes one of defining a similarity function whose value is maximized for the target. \nAn order may be imposed on the scores in D by \nmeasuring the similarity between each score S\n∈D \nand the query Q and then ordering the set by \nsimilarity. We now define several versions of a similarity function σ(S,Q) based on the idea of edit-\ncost. \n4.1 Monophonic Alignment \nThe typical problem formulation in the query by \nhumming literature assumes the query and all scores are monophonic. If one assumes monophonic scores \nconsist of a single part, one can represent the scores as \nstrings over a suitably defined alphabet and employ standard string matching techniques [9, 21]. \nGiven two strings, S=s\n1s2…s n and Q=q 1q2…q m, \ndrawn from alphabet ∑, string matchers find the best \nalignment between string S and string Q by finding \nthe highest-reward alignment of  S to Q in terms of \nedit operations. (insertion, deletion or matching of \ncharacters). The highest-reward is a measure of the \nsimilarity between the two strings. \nCentral to string matching algorithms is the \ndevelopment of a match score function, µ(x,y), that \ngives a numerical value corresponding  to goodness \nof the match between two characters, x and y, drawn \nfrom the alphabet ∑. \nRecent work in the music IR community [19, 21] \nhas used the log-odds approach to determining the \nmatch score. Such a match score function is shown in Equation 1.  \n \n\n\n\n=) |,() |,(log),(chanceyxPmatchyxPyxµ  (1) \nThis function returns a negative value when the \nprobability of a meaningful match is below that of a \nrandom co-occurrence. Similarly, the value is positive when a meaningful match is more likely than random chance.  \nDeveloping good estimates of the probabilities \nrequired for this function can be a challenge and is \ntask-dependent. For an example of how this may be done see [9]. \nThe best alignment of  S to Q can be found in  \nO(mn) time Here, m and n correspond to the length \nof the query and the score. This is done by applying dynamic programming. This has been used for over 30 years to align gene sequences based on a common ancestor [18].  We now describe a local edit-distance variant. \nConstruct a matrix M, of size  n+1 by m+1, which \nis indexed from 0,0 to n,m. Element M\ni,j contains the \nscore of the best alignment between the initial \nsegment s1 through si of S and the initial segment q1 \nthrough qj of Q. \nInitialize M as described in Equation 2 and \nEquation 3. \n0\n0,0=\n≤≤mjjM    (2) \n0\n00,=\n≤≤niiM   (3) \nWe define the recurrence relation for M in \nEquation 4. 24 24 \n22 22 36 38 38 38 \n217\n\n\n+− +−+\n=\n−−−−\n),(    ),(   ),(                          0\nmax\n1,11,,1\n,\nj i jii jij ji\nji\nqs Ms Mq M\nM\nµµµ\n (4) \nHere, the “-“ character is a blank, which is added \nto the alphabet of both query and score. Matching to \na blank can be thought of as skipping the element matched to the blank. The match score function value for matching a blank is set to the penalty for skipping the element matched to the blank. Skipping an element of the query assume s the query inserted an \nextra element that was not in the score. Skipping an \nelement of the score assumes that the query deleted \nthat element of the score. \nThe similarity of the best alignment of S to Q is \nthen defined as the highest valued element of M. \n) (max),(,,jijiM QS = σ   (5) \n4.2 Maximum Single-part Similarity \nIt is often the case that a user wishes to find a musical \npiece consisting of multiple parts, querying the \ndatabase with a monophonic query. Given a multi-part \nscore, as in Figure 2, the easiest way to measure \nsimilarity to the query is to use the similarity of its maximally similar part. This is shown in Equation 6.  \n)),((max),( QP QS\nSPσ σ\n∈∀=  (6) \nThis approach is simple to add to any existing \nsimilarity measure. The time complexity of this \nalgorithm is O(tr), where t is the time required to \ncalculate similarity for a single part and r is the \nnumber of parts. When applied to the monophonic \nalignment similarity measure from Section 4.1, the \ntime complexity is O(mnr). Here, we assume that the \nlength of the score is the length of the longest part. \n4.2 Homophonic Alignment \nIt is not unusual, for a query to be composed of \nsections derived from multiple parts in the desired score. The optimal alignmen t of such a query travels \nfrom part to part in the score. For example, a person may sing a bass line, interspersed with portions of the soprano.  Maximum single-part similarity does not capture this situation and may fail to produce the highest score for the correct target.  \nAs a first step to handling a query whose optimal \nalignment travels from part to part, we consider the case of a homophonic score. We can extend the dynamic programming algorithm of Section 4.1 as \nfollows. Instead of each symbol s\ni in the string \nrepresentation of the score representing a single note \nn, let it represent a note concurrency . We define a \nnote concurrency, C, as a set of notes  that share the \nsame onset and end time.   Each homophonic score in \nthe database may, thus, be represented a sequence of note concurrencies, S=C\n1C2…C l. To compare the query to the score with alignment \nalgorithm in Equation 4, one must replace the match \nscore function between two notes, µ(x,y), with a \nmatch score function, µΗ(x,C), for scoring a note n \nand a note concurrency C . Equation 7 defines this. \n)),((max),( αµ µ\nαn Cn\nCH∈∀=         (7)  \nHere, the similarity of the concurrency to the note \nis determined by the similarity of its most similar \nnote. As with the method from Section 4.1, the \nsimilarity score of the best alignment of S to Q is the \nhighest valued element of M. \n By moving the maximization into the match score \nfunction (as opposed to simply selecting the value for \nthe most-similar part), we increase the similarity \nbetween a query that travels from part to part and the \ncorrect score.  \nThe time complexity of the match score function \nin Equation 7 depends on the number of notes in the \nconcurrency, c, and is O(c). If the size of the score, n, \nis the number of concurrencies in the score and we \ntake c to be the size of the largest concurrency in the \nscore, then the time complexity is O(mnc).   \nThis alignment method may also be combined with \nthe method from Section 4.2. The resulting method \nallows comparison of a monophonic query to a score with multiple homophonic parts. \n4.4 Polyphonic Alignment \nHomophonic alignment, while an improvement over \ncombining maximum single-part similarity with monophonic alignment, has two weaknesses. It does not account for polyphonic scores, where notes begin and end independently, nor does it capture the concept of a part.  We would like the flexibility to impose a penalty on the match for skipping between parts. This lets us preferentially favor an alignment that continues on the same part over one which skips to a new part, while still allowing such skip s to take place. We start \nthe description of the polyphonic alignment algorithm by first treating the simpler homophonic case. \nLet S be a multi-part homophonic score with r \nparts P\n1,…,P r where each part has the same number \nof note concurrencies . Note that this may require \ninsertion of some silent note concurrencies, i.e. note \nconcurrencies which only contain pitch 0. Let  \nPi=pi1…p in where pij is the jth concurrency of Pi. Let \nQ=q 1q2…q m be the query sequence where qk is the kth \nnote of Q. For a sequence A=a 1a2…a l, we will use \nA[i..j]  to represent the substring aiai+1… a j. \nWe extend the dynamic programming algorithm \nfor homophonic single-part alignment by introducing \na function γ(a,b)  which returns the cost for changing \nfrom Pa to Pb. Let Sj denote a length j prefix of S \nwhich is the set of parts P1[1..j], P 2[1..j],…, P r[1..j]  \nand Qj denote the length j prefix of Q which is \nQ[1..j].  \nNext we describe the recurrence which uses γ(a,b)  \nto score alignment of a monophonic query with a \nmultiple-part homophonic scores. Instead of a two dimensional table as in Section 4.1, we will construct a three dimensional table where the third dimension \n218corresponds to the different parts in the multi-part \nscore.  \nConstruct a (m+1) × (n+1) × r matrix L, indexed \nfrom 0,0,1  to m,n,r . Here, Li,j,k is the score of the \noptimal local alignment score of Qi and Sj such that \npkj is present in the alignment.  \nInitialize L as described in Equation 8 and \nEquation 9. \n0\n1, 0,,0 =\n≤≤≤≤ rknjkjL    (8) \n0\n1, 0,0, =\n≤≤ ≤≤ rk mikiL   (9) \nWe define the recurrence relation for L in \nEquation 10. \n\n\n\n≠∀ + +≠∀ + −++−+− +\n=\n−−−−−−−\n0),(),(),(),(),(),(),(\nmax\n,1,1,1,,1,1,1,,,1\n,,\nklkl pq Lklkl p Lpq Lp Lq L\nL\njjjj\nk i ljik ljik i kjik kjii kji\nkji\nγ δγ δδδδ\n(10) \nThe similarity score of the best alignment of S to \nQ is then the highest value element of L. \n) (max),(,,,,kjikjiL QS = σ   (11) \nIn polyphonic scores, we have to deal with the \nadded complication that notes may begin and end \nindependently. Thus, it may be possible to skip between parts in the middle of a sounding note, at the point where a note in another part begins. This happens throughout the Bach example in Figure 2, starting with the very first beat. To account for this \nwe break down the notes into notebits .  \nA currently-sounding note, n, us sokut ubti twi \nbitebuts at the point where another note begins or \nends. It is, perhaps, easiest to see this in Figure 5. Here, vertical lines are placed at every note onset and ending. These lines subdivide the notes in the score \ninto notebits. For example, the first note in part P\n1 is \ndivided into two notebits. The first of these \ncorresponds to the onset  of the note. The second is a \ncontinuation of the note as a note in the other part enters. \nIn our notation, a note n is decomposed into a \nsequence of notebits, n=b\n1,b2,…,b k. Each  notebit is a \n4-tuple (s,e,p,o ) where s is the start time of the \nnotebit, e is the end time, p is the pitch and o is a \nBoolean value that defines whether the notebit \ncorresponds to the onset  of the note. The field o of a \nnotebit is true only if it is an onset notebit. The first \nnote in part P1 is n=(1,3,38) . This is decomposed as  \nn = b1,b2 = (1,2,38, true), (2,3,38, false ). \nUp to this point, we have not defined silence in a \nscore.  We do so by allowing notes with pitch 0, \ndefining 0 to mean “silence.” This allows us to divide a part into a sequence of consecutive notes, some of which may be silence Each note in a part may be further decomposed into notebits.   Given a score composed of monophonic parts, all \nparts will have the same number of notebits, since the \nnumber of notebits in any part is determined by the \nnumber of independent note onsets and endings in the \nentire piece (including a ll parts). Our algorithm \ndepends on all parts containing the same number of notebits, as it implicitly uses order in the sequences as its encoding of relativ e time. Therefore, the ith \nnotebit in part P\nj is coincident with the ith notebit in \npart Pk, for all j and k.  \nGiven a score with polyphonic parts, we break \neach part into monophonic subparts, such that each \nsubpart has no overlapping notebits and all notebits from the same note belong to the same subpart. Note \nthat each part gets divided into z subparts, where z is \nthe maximum number of simultaneously sounding \nnotes in the part. In Figure 5, z=2 for both parts. For \nthe example in Figure 5, the decomposition of notes \ninto notebits yields the following (here, we substitute \nt for true and f for false ). \nP\n11={(1,2,38,t), (2,3,38,f), (3,4,0,t), (4,5,38,t), \n(5,6,38,f),  (6,7,38,t), (7,8,38,f)};  \nP12={(1,2,0,t), (2,3,36,t), (3,4,0,t), (4,5,0,f), (5,6,0,f), \n(6,7,0,f), (7,8,0,f)}; P\n21={(1,2,24,t), (2,3,0,t), (3,4,0,f), (4,5,0,f), (5,6,0,f), \n(6,7,24,t), (7,8,0,t)}; and  P\n22={(1,2,22,t), (2,322,f), (3,4,22,t), (4,5,22,f), \n(5,6,0,t), (6,7,0,f), (7,8,0,f)}. \nNotice that the in the notebit representation, \npolyphonic scores look very similar to the \nhomophonic scores with N parts, where N is the total \nnumber of subparts in the score (for our running example, N=4). Therefore we can create a convenient \nrepresentation of the score in terms of set of N strings \nS\n1,…,S N where Si=si1…s in and sij is the jth notebit in \nsubpart Si. Let pij be the pitch corresponding to the \nnotebit  sij and let oij represent its corresponding \nboolean onset field. We set up the dynamic \nprogramming table for polyphonic alignment in a similar fashion and use the following recurrence where the change-track penalty for two subparts derived from the same original part is set to be 0. This is expressed in Equation 12. \nif  okj= false , kji kji L L,1, ,, − = ,  \nif okj = true,  \n\n\n\n≠∀ + +≠∀ + −++−+− +\n=\n−−−−−−−\n0),(),(),(),(),(),(),(\nmax\n,1,1,1,,1,1,1,,,1\n,,\nklkl pq Lklkl p Lpq Lp Lq L\nL\njjjj\nk i ljik ljik i kjik kjii kji\nkji\nγ δγ δδδδ\n(12) \nFor all the alignment algorithms described in this \npaper, we need to fill up a dynamic programming \ntable. Computing each en try in the table takes O(1) \ntime for monophonic case and O(N)  time for the \npolyphonic case. So the total time taken is the \nproduct of time taken per entry and the total number of entries. Therefore monophonic alignment takes \n219O(mn)  time and the polyphonic alignment takes \nO(mnN2) time, where m is limited by the number of \nnotes in the query, n is limited by the number of \nnotes in the score, and N is the number of subparts in \nthe score. \nWe have described a series of alignment \nalgorithms, culminating in the polyphonic alignment \nalgorithm in Equation 12. This algorithm is the most \ngeneral method we have described, allowing for \nalignment of a monophonic query that skips from part \nto part of an arbitrary polyphonic score. In Section 5, we measure the performance improvement this algorithm provides over the maximum single-part similarity measure in Section 4.2. \n5. EXPERIMENTAL SECTION \nIn order to estimate the pot ential performance gain for \nfinding the appropriate poly phonic, multi-part target \nin a database in response to a part-skipping \nmonophonic query, we constructed a small corpus of Bach Chorales and a set of synthetic queries that skip from part-to-part. We then compared the performance of a similarity measure based on the polyphonic \nalignment algorithm  from  Section  4.4 with the \nmaximum single-part similarity measure from Section \n4.2. This experiment is described in this section. \nAs our database, we chose 300 Bach four-part \nchorale harmonizations, encoded as MIDI files.  These are typical sopran to-alto-tenor-bass vocal \narrangements, and a small subset of them are alternate harmonizations of the same melody. The midi files are available at http://www.jsbchorales.net/.  The complete list of \nchorales selected for this  study is available at \nthe    http://www.cs.northwestern.edu/~pardo/researchweb page. \nWhile  three hundred chorales makes for a small \ncorpus, the point of the experiment is not to measure absolute performance of a single method, but rather relative performance improvement. For this reason, we felt a smaller database, composed of known pieces with full scores was a better choice.   \nWe are interested in creating algorithms that allow \nfor effective comparison of monophonic queries to polyphonic, multi-part scores . For this experiment, \nwe were interested in measuring the relative performance of these methods as a query is increasingly likely to skip between parts. In order to control this likelihood we constructed synthetic queries, based on targets in the corpus. \nGiven a target score in the database, T, a query \nwas constructed by selecting a subsequence of notes \nfrom T whose length was randomly selected (with an \nequal probability distribution) from the range [5,25]. \nThis length range was based on the range of typical query lengths for sung queries[9]. The initial note was selected from a randomly-chosen part (given an equal probability distribution) in the target. The start position in the selected part was also chosen randomly, so that a query might begin anywhere within the piece. Once st arted, the query was \nconstructed by adding consecutive notes from the score. The part of each additional note in the query was selected based on a fixed probability of changing parts. If the probability of changing parts was set to 0, then all notes in the query would be selected from the \nsame part. If the probability of changing part were 0.25, then there would be a 25% chance that each additional note in the query would be drawn from a different part than the previously selected note. Given a change in part from the current to the next note in the query, the new part was selected at random. The query in Figure 2 and Figure 3 is an example query \ndrawn from the score with a probability of changing notes of 0.25.  \n5.1 The Experiment \nThe maximum single-part similarity algorithm from \nSection 4.2 forms a simple baseline performance measure, as it implicitly assumes the query is drawn \nfrom a single, monophonic part. Let c be the \nprobability that a query skips from part to part. As c \nincreases, the similarity measure based on the \nmaximal single-part similarity should become \nincreasingly ineffective. Conversely, the polyphonic alignment method from section 4.4 should be unaffected by an increased amount of skipping from part to part. Accordingly, we compared these two \nmethods for determining the similarity of a query to \neach score in the database. \nTo construct the query set, we selected 150 targets \nat random from the database. For each target, we \nconstructed five queries, one with c = 0, one with c = \n0.25, one with c = 0.5, one with c = 0.75, and one \nwith c = 1.0. This created a total of 750 queries. We \nthen ranked the similarity of every target in the \ndatabase to each query, recording the rank of the correct target (hereafter called “right rank”). \n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10510152025\nPer-note probability of changing parts in queryMean rank of correct target Maximum single-part similarity\nPolyphonic alignment\n4.58.718.923.7\n2.51.6 1.41.31.7\n1.3\n \nFigure 6. Mean rank of correct target as a function of c \nFigure 6 shows the mean right rank as a function \nof c. Each point indicates the mean value for 150 \nsynthetic queries. The numbe r by each point gives the \nmean value for that point. Since there are 300 scores \nin the database, random performance would result in a mean right rank of 150. Perfect performance would result in a mean right rank of 1.  \nAs the figure shows, neither method performed \nperfectly, even when the probability of changing \nparts in a query is 0.  This is due to two factors: first, several of the targets are alternate harmonizations of \n220the same melody; second, a short query (on the order \nof 5 notes) may match multiple items in the database, if it is based on a common melodic pattern. This is the case for a number of the queries. \nAs the probability of changing parts increases, \nhowever, the difference betw een the performances of \nthe algorithms becomes clear . The performance of the \nmaximum single-part similarity measure quickly \ndegrades as the queries increasingly skip between \nparts, while the polyphonic alignment algorithm’s performance remains essentially constant, with its mean varying within a narrow range.  \n0 10 20 30 40 50 60 70 80 90 100050100Number of queriesPolyphonic alignment\n0 10 20 30 40 50 60 70 80 90 100050100Number of queriesMaximum single-part similarity\nRank of correct target (right rank) \nFigure 7. Performance of ranking methods when c = 0.5 \nFigure 7 gives more detail. The figure shows a \nhistogram of the distribution of right ranks for both \nalgorithms in the case where c, the probability of \nchanging parts at each note, is 0.5. The distribution \nreturned by the polyphonic alignment algorithm \nmuch tighter, with the vast majority of the 150 queries returning a right rank of 1. Only a third of the \nqueries do so for the maximum single-part similarity \nmethod, and the tail of right ranks extends out to the value 100.  \n6. CONCLUSIONS \nFinding the best matching database target to a melodic \nquery has been of great interest in the music IR world. Standard string alignment algorithms work well for this task when comparing a monophonic query to a database of monophonic pieces . However, most tonal \nmusic is polyphonic, with mu ltiple concurrent musical \nlines. Such pieces are not adequately represented as \nstrings. Moreover, users often represent polyphonic \npieces in their queries by skipping from part to part. \nWe described a series of algorithms designed to compare the similarity of a monophonic melodic sequence to a homophonic or polyphonic piece of \nmusic, culminating in the polyphonic alignment algorithm in Equation 12. This algorithm is the most general method we describe d, allowing for alignment \nof a monophonic query to an arbitrary polyphonic score. \nWe compared the polyphonic approach to the \nmaximum single-part similarity method for matching a polyphonic score to a monophonic query. Results using synthetic queries on the Bach database indicate that the polyphonic method significantly outperforms the maximum single-part method when a monophonic query is drawn from multiple parts in the target. This suggests that the performance of music information-retrieval systems, such as query-by-humming systems, can be improved through the use of a polyphonic-alignment algorithm. \nREFERENCES \n[1] Mazzoni, D. and R. Dannenberg. Melody \nMatching Directly From Audio. in ISMIR. 2001. Bloomington, IN. \n[2] Meek, C. and W.P. Birmingham. Johnny Can't Sing: A Comprehensive Error Model for Sung Music Queries. in ISMIR 2002. 2002. Paris, France. \n[3] Hoos, H., K. Rentz, and M. Gorg. GUIDO/MIR - \nan Experimental Musical Information Retrieval System based on GUIDO Music Notation. in International Symposium on Music Information Retrieval. 2001. Bloomington, IN. \n[4] Song, J., S.Y. Bae, and K. Yoon. Mid-Level Music Melody representation of Polyphonic Audio for Query-by-Humming System. in ISMIR 2002. 2002. Paris, France. \n[5] McNab, R.J., L.A. Smith, and e. al. Towards the digital music library: tune retrieval from acoustic input. in Digital Libraries. 1996. \n[6] Clausen, M., R. Englebrecht, and e. al. Proms: A web-based tool for searching in polyphonic music. in The International Symposium on Music Information Retrieval. 2002. \n[7] Doraisamy, S. and S. Ruger. A Comparative and Fault-tolerance Study of the Use of N-grams with Polyphonic Music. in ISMIR 2002. 2002. Paris, France. \n[8] Clarisse, L.P., et al. An  Auditory Model Based \nTranscriber of Singing Sequences. in ISMIR 2002. 2002. Paris, France. \n[9] Pardo, B., W.P. Birmingham, and J. Shifrin, Name that Tune: A Pilot Study in Finding a Melody from a Sung Query. Journal of the American Society for Information Science and Technology, 2004. 55(4): p. 283-300. \n[10] Birmingham, W.P., et al. Musart: Music Retrieval Via Aural Queries. in ISMIR 2001. 2001. Bloomington, IN. \n[11] Pickens, J. A Comparison of Language Modeling and Probabilistic Text Information Retrieval. in International Symposium on Music Information Retrieval. 2000. Plymouth, Massachusetts. \n[12] Downie, S. and M. Nelson. Evaluation of a Simple and Effective Music Information Retrieval Method. in 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. 2000. Athens, Greece. \n[13] Uitdenbogerd, A. and J. Zobel. Melodic Matching Techniques for Large Music Databases. \n221in Seventh ACM International Conference on \nMultimedia. 1999. Orlando, FL. \n[14] Durey, A.S. and M. Clements. Melody Spotting Using Hidden Markov Models. in International Symposium on Music Information Retrieval. 2001. Bloomington, IN. \n[15] Dannenberg, R., et al. The MUSART testbed for \nquery-by-humming evaluation. in ISMIR 2003, \n4th International Conference on Music Information Retrieval. 2003. Baltimore, Maryland. \n[16] Gotoh, O., An improved algorithm for matching biological sequences. Journal of Molecular Biology, 1982. 162: p. 705-708. \n[17] Gusfield, D., Algorithms on Strings, Trees, and Sequences. 1997, New York, NY: The Press Syndicate of the University of Cambridge. \n [18] Needleman, S.B. and C.D. Wunsch, A general \nmethod applicable to the s earch for similarities in \nthe amino acid sequence of two proteins. Journal of Molecular Biology, 1970. 48: p. 443-453. \n[19] Pardo, B. and W.P. Birmingham. Following a musical performance from a partially specified score. in Multimedia Technology Applications \nConference. 2001. Irvine, CA. \n[20] MIDI-Manufacturers-Associ ation, The Complete \nMIDI 1.0 Detailed Specification. 1996, Los \nAngeles, CA: The MIDI Manufacturers Association. \n[21] Hu, N., R. Dannenberg, and A. Lewis. A Probabilistic Model of Melodic Similarity. in International Computer Music Conference (ICMC). 2002. Goteborg, Sweden: The International Computer Music Association. \n \n222"
    },
    {
        "title": "Applications of Binary Classification and Adaptive Boosting to the Query-By-Humming Problem.",
        "author": [
            "Charles L. Parker"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416482",
        "url": "https://doi.org/10.5281/zenodo.1416482",
        "ee": "https://zenodo.org/records/1416482/files/Parker05.pdf",
        "abstract": "In the  query-by-humming problem, we attempt to retrieve a speci\u0002 c song from a target set based on a sung query. Recent evaluations of query-by-humming systems show that the state-of-the-art algorithm is a simple dynamic programming-based interval matching technique. Other techniques based on hidden Markov models are far more expensive computationally and do not appear to offer signi\u0002 cant increases in performance. Here, we borrow techniques from arti\u0002 cial intelligence to create an algorithm able to outperform the current state-of-the-art with only a negligible increase in running time. Keywords: melodic retrieval, sequence alignment, arti\u0002 cial intelligence 1",
        "zenodo_id": 1416482,
        "dblp_key": "conf/ismir/Parker05",
        "keywords": [
            "query-by-humming",
            "song retrieval",
            "target set",
            "sung query",
            "dynamic programming",
            "interval matching",
            "hidden Markov models",
            "performance",
            "sequence alignment",
            "artificial intelligence"
        ],
        "content": "APPLICA TIONS OFBINARYCLASSIFICA TION AND ADAPTIVE\nBOOSTING TOTHE QUER Y-BY-HUMMING PROBLEM\nCharles Parker\nOregonStateUniversity\n102Dearborn Hall\nCorvallis,OR97333\nparker@cs.orst.edu\nABSTRA CT\nInthequery-by-humming problem, weattempttore-\ntrieveaspeci\u0002csongfromatargetsetbasedonasung\nquery.Recentevaluations ofquery-by-humming systems\nshowthatthestate-of-the-art algorithm isasimpledy-\nnamicprogramming-based intervalmatching technique.\nOthertechniques basedonhiddenMarkovmodelsarefar\nmoreexpensivecomputationally anddonotappeartoof-\nfersigni\u0002cant increases inperformance. Here,weborrow\ntechniques fromarti\u0002cialintelligence tocreateanalgo-\nrithmabletooutperform thecurrentstate-of-the-art with\nonlyanegligibleincreaseinrunningtime.\nKeywords:melodicretrieval,sequence alignment, arti-\n\u0002cialintelligence\n1INTR ODUCTION\nMostmusic-related information queriesarecurrently\nbasedonmeta-data, suchasthetitleorartistofasong.\nInrecentyears,wehaveattempted tocreateasystemin\nwhichmusicalqueriescouldbeformed musically ,giving\nus,asDownie[1]hassaid,theabilitytoquerymusicon\nitsownterms.\nOnesuchnotionofmusicalqueriesisquery-by-\nhumming, whereausersings,hums,orwhistlesaquery\ntunetoacomputer.Thecomputer hasadatabaseofpos-\nsiblesongsandattemptstoreturnthesongrendered by\ntheuser,orperhapsreturnarankedlistofpossiblesongs.\nManypossibleapplications forthishavebediscussed ina\nsmallbutgrowingliterature ontheproblem, rangingfrom\nentertainment tocommercial tolegal.Unfortunately ,the\nproblemhasprovenverydif\u0002culttosolveadequately ,in-\nvolvingsubproblems likepitchdetection, notesegmenta-\ntion,polyphonic transcription, andsequence alignment.\nItisonthelastofthesethatwewillattemptto\nmakeprogressinthispaper.Recentevaluations ofvar-\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondoniousquery-by-humming systemshaveshownthatasim-\npleinterval-basednotematching approach isabletoper-\nformaswellasothermorecomplexMarkovmodel-based\nmethodswhengivenreasonable queries[2,3].Havewe\nreachedthebestattainable performance onthissubprob-\nlem?\nInthispaper,we\u0002rstbrie\u0003yoverviewquery-by-\nhumming andgeneralsequence matching problems. We\nthenreviewbinaryclassi\u0002cation andpresentamethodfor\nconstructing alignment modelsusingbinaryclassi\u0002cation.\nFinally,weuseadaptive boosting tocombine ourhand-\nbuiltmodelwithseverallearnedmodelstocreateamodel\nthatoutperforms bothonasetofcollected testqueries.\nOurconcluding remarksindicateimprovementsfortheal-\ngorithmandotherpossibleusesforthisformalism.\n2BACKGR OUND\n2.1 Query-by-humming andSequence Alignment\nInthelasttenyears,theso-called query-by-humming\nproblemhasgarneredmoderate attention inboththein-\nformation retrievalandthearti\u0002cialintelligence literature.\nBrie\u0003y,theproblemisasfollows:Ahumansingsaquery\nsongqithatcorresponds tosometargetsongtjina\ntargetsetT.IfQisthespaceofallpossiblequeries,then\nourgoalistolearnafunction fthatmapsaqueryqi2Q\ntothecorrecttargetC(qi)=tj\n8i:f(qi;T)=C(qi)=tj (1)\nOften,thisisdonebycreatingafunction F:T\u0002Q7!\n<sothatany(query,target)combination canbegivena\nscore,whichshouldbemaximized (orminimized) whena\nqueryiscombined withamatching target:\nf(q;T)=argmax\nt2TF(t;q) (2)\nDe\u0002ning thefunction Falsogivesusaconvenient\nmethodforrankingthetargets.Therankofaqueryin\nacertaintargetsetisthenumberofincorrect targetsthat\nappeartomatchthequerybetterthanthecorrecttarget,\ngivenascoringfunction. Formally,supposewehavea\nscoringfunction F,aqueryq,andatargetsetTcontain-\ningthecorrecttargetC(qi).Therankofqunder Fis:\n245rank F(q;T)=\n#T(tijF(ti;q)\u0015F(C(q);q)^C(q)6=ti)(3)\nThisde\u0002nition willbeusedinwhatfollows.\nHowdowedesignagoodscoringfunction? Although\nthereareotherapproaches thatmeritconsideration [4,5],\ntheprevalentapproach intheliterature istoconvertboth\ntargetandqueryintomonophonic sequences ofnotes1.\nTheseprocesses areoutlinedfullyin[6]and[2]sowe\nonlybrie\u0003yexplainthemhere.Figure1isasummary of\nthisprocess.\nFigure1:Apossible generalformofaQuery-by-\nHumming system.\nFortheQuery,webeginwithrawaudio(e.g.,in\nWAVformat). Wethenseparatethequeryintoanum-\nberof\u0002xed-length frames(usuallyontheorderof0.01\nsec.).Eachframeispassedtoapitchdetection algo-\nrithm[7]thatreturnsthestrongest frequencyinthatframe\nalongwithacon\u0002dence valueindicating thedominance\nofthestrongest frequency.Wethensequentially step\nthroughtheseframes,grouping framesoflikefrequency\nintonotes,usinganote segmentation algorithm.\nForthetarget,thisprocessisevenharder,usuallyin-\nvolvingthedif\u0002cultsubproblems ofpolyphonic transcrip-\ntion(transcribing arawaudiosignalthatispolyphonic),\nmelody spotting(distinguishing melodynotesfromnon-\nmelodyones),andthematic extraction(extracting coher-\nentthemesfromalongmelodysequence). Forourex-\nperiments, weuseasetofmonophonic MIDI\u0002lesina\npubliclyavailabledatabase[8]asourtargetset,andsowe\navoidthishalfoftheproblem.\n1In[3],thetargetisinstead converted toseveralmonophonic\nthemes ,each mapping back tothetarget,butthespirit isgener -\nallythesame.Oncewehavebothtargetsongsandquerysongsrep-\nresentedassequences, ourproblemcanberestatedasa\nprobleminsequence alignment. Thestandardalgorithm\nforsequence alignment istypically attributedtoSmith\nandWaterman[9]andrestatedseveralplacesinthequery-\nby-humming literature. Thisalgorithm uses editcoststo\ndetermine thecostoftransforming onesequence intoan-\nother,andusesthistoscoreatargetwithrespecttothe\nquery.\nFormally,supposewehaveatargetsequence trep-\nresented asaseriesofnotes t1;t2;:::;tnandaquery\nqrepresented asaseriesofnotes q1;q2;:::;qm.Sup-\nposefurtherthanwehaveacost model,K,whichgives\nusKi(x),thecostofinserting notexintothequeryse-\nquence, Kd(x),thecostofdeletingnotexfromthetarget\nsequence, andKm(x;y)isthecostofmatching thenote\nxinthetargettothenoteyinthequery.Wecanthen\u0002nd\nthelowestcostalignment2withthefollowingrecursion:\nalign(i;j;t;q)=\nmin8\n<\n:Km(ti;qj)+align(i+1;j+1;t;q)\nKi(qj)+align(i;j+1;t;q)\nKd(ti)+align(i+1;j;t;q)(4)\ninwhichthebasecaseistheendofoneorbothse-\nquences. Withdynamicprogramming, wecanconstruct a\nstraightforw ard,ef\u0002cientsolution. Anexample3oftwo\nalignedsequences isshowninFigure2.Wenotethat\nKcompletely speci\u0002esthefreeparameters forthisalgo-\nrithm.Fornotational convenience,wede\u0002ne alignK(t;q)\ntobethebestalignment oftandqwiththecostmodel\nK.Wealsode\u0002ne K(at;q)tobethecostofsomegiven\nalignment aoftandq(asinFigure2).Finally,wede\u0002ne\nK(t;q)asthelowestcostalignment oftandqgiventhe\ncostmodel K.Toillustrate thisnotation, considerthat,if\nweuseKasthecostmodelinEquation 4:\nK(t;q)=align(1;1;t;q)=K(alignK(t;q))(5)\nThismayseemlikemanydifferentwaysofsayingthe\nsamething,butthesenotational conveniences willgain\nmoreutilityinthefollowingsections.\niidm dm m\n--CABIN\nDR-A-IN\nFigure2:ALexicographical Example ofSequential\nAlignment.\nWithafewmodi\u0002cations tothisrecursion, weareable\ntoignorepre\u0002xesorsuf\u0002xesaswesee\u0002t.Therearemany\nvariationsonthisbasictheme[10,11,12],butallarede-\npendenton\u0002ndingacostmodelthatcaneffectivelydistin-\nguishthecorrecttargetfromamongsttheincorrect ones.\n2Notice thatwehaveswitched from \u0002nding high scoring\ntargets tolowcost targets. This contradicts Equation 2.The\nreader willplease forgivetheinconsistenc y\n3Withthanks to[10].\n246Although parameters canoftenbeestimated byhand,\nitwouldbemoreusefulandpractical toestimatethese\nparameters fromdata.Onecommon methodistocon-\nstructatablecontaining eachpossiblenotevalueforKi\nandKdandatablecontaining thecrossproductofallpos-\nsiblenotevaluesforKm.Ifwearethengivenaseriesof\ntrainingalignments (oftheformofFigure2).Thenwe\ncansimplycountthenumberoftimesanoteisinserted,\ndeleted,orreplacedwithanothernote,usethesecounts\ntodetermine probabilities, andusethelogprobabilities as\neditcosts[13].\nTheproblemhere,asmentioned in[2],isthatthese\ntablesaresolargeinthisdomainthatlearningthemfrom\ndatabecomesintractable. Infact,sinceboththepitchand\nthedurationofanotearereal-valued,itseemsweshould\nusealearningalgorithm thatisabletohandlesucharepre-\nsentation. Atable-based estimate, inotherwords,maybe\nthewronghypothesis spaceforthefunctionwearetrying\ntoapproximate.\nArethereotherformsofmachinelearningthatcanbe\nusedinthiscontext?Weturnourattention tothisinthe\nnextsection.\n2.2 Binary Classi\u0002cation\nTohelpuslearncostmodels, wewillutilizelearn-\ningalgorithms forbinaryclassi\u0002cation. Abinaryclas-\nsi\u0002cation problem canbestatedasaseriesofduples\nf(x1;y1);(x2;y2);:::;(xn;yn)g.Ingeneral, xiisa\nvectordrawnfromsomeinputspace X,andyi2\nf+1;\u00001gorfpositive;negativegistheclass labelofxi.\nThegoalistolearnafunction thatmapsallpossi-\nblevectorsintheinputspace,eventheonesnotseen\nbythelearning algorithm, totheircorrectclassla-\nbel.Formally,ifListhelearning algorithm ,and\nT=f(x1;y1);(x2;y2);:::;(xn;yn)gisthetraining\ndatathenL(T)outputsahypothesis h:X7!f+1;\u00001g.\nAlternatively,wecanmodifymostlearningalgorithms\nsothatwegetaprobability oftheclasslabelratherthan\nthelabelasoutput.Ourhypotheses willnowbeofthe\nformh:X7![0;1],whereavalueapproaching zeroin-\ndicateswithhighprobability thattheclassisnegativeand\navalueapproaching oneindicates thesamefortheposi-\ntiveclass.Thisisthevariantwewilluseinthesections\nbelow.\nTherearemanychoicesforL.Amongthemareneural\nnetworklearners, decisiontreelearners, andperceptron\nlearners. Thisisanextremely well-studied problemand\ncanbefoundinmanyplacesintheliterature [14].Itis\nhopedthatwecanbringsomeofthisknowledgetobear\nonourcurrentproblem.\n3ALIGNMENT EVALU ATION VIA\nBINARYCLASSIFIERS\nWenowdealwithsomeofthedif\u0002cultiesoflearningin\nthiscontext.Recentattemptsatthisproblem[15]haveen-\njoyedsuccessbyformulating theproblem discrimatively\nasopposedtogeneratively.Inourcontext,thismeansto\nlearnthecostswithboththecorrectandincorrect targets\ninmind.Thefollowingsubsections outlinethemethodformally.\n3.1 The Algorithm\nOurdiscriminati veapproach isasfollows:Wetakeas\ntrainingdataanumberofalignments, someofwhichalign\naquerytoacorrecttargetsongandsomeofwhichalign\naquerytoanlow-cost,incorrect targetsong(thatis,the\nbest-scoring incorrect target).Eacheventinboththecor-\nrectandincorrect alignments (insert,delete,ormatch)is\nplacedintoasetcontaining alloftheeventsofthattype,\nsothatallofthetrainingdatais\u0002nallyinthreesets,one\nforeachtypeofevent.Eacheventislabeledaspositiveif\nitcamefromanalignment withacorrecttargetandnega-\ntiveifitcamefromanincorr ecttarget.\nIneachofthesesets,wehavethenanumberofpos-\nitiveandnegativeexamples. Thisisthenabinaryclas-\nsi\u0002cation problem. Theclassi\u0002er learnedfromeachof\nthesesetswilloutputtheprobability thatagivenevent\ncamefromanalignment ofaquerywiththecorrecttar-\nget.Thisexactlyde\u0002nesourlearnedcostmodel KLwith\nthethreelearnedclassi\u0002ers asthefunctions KL\ni,KL\nd,and\nKL\nm.Aschematic ofthealgorithm isshowninFigure3\nandpsudeocode isgivenasAlgorithm 1.\nQuery = “1 4 4”\nCorrect target (p)\n4 3 2 1 -\n1 -4 -4Incorrect Target (n)\n1 2 3 4\n1 -4 4\n1 1    p\n3 4    p\n4 4    n\n4 1    n\n2 4    n\n“Match”Cost\nFunctionEvent          Class\n4   n 2p\n3n\n1   nEvent    Class Event     Class\n“Insert”Cost\nFunction“Delete”Cost\nFunction1 1    p\n3 4    p\n4 4    n\n4 1    n\n2 4    nEvent          Class\n4   nEvent     Class\n2p\n3n\n1   nEvent    Class\nLearn Classifier\nLearn ClassifierLearn Classifier\nFigure3:Aschematic ofourlearningprocess.The'class'\ncolumndenoteswhethertheeventcamefromacorrect(p)\norincorrect (n)alignment.\nTheastutereader,observing Algorithm 1,willnote\nthatweuseahand-builtmodeltoconstruct thealignment\noftargetandquerybothintrainingandaftertraining.\nWhynotjustusethelearnedmodeltoconstruct thealign-\nment?Thereasonsherearetwo-fold:\n1.Toreduce running time.Ifwehaveatargetof\nlength mnotesandaqueryoflength nnotes,ittakes\nO(mn)callstothemodeltoalignthetwotargets\nandatmostO(m+n)callstothemodeltoeval-\nuateagivenalignment. Thehand-builtmodelwe\nusehererequiresonlyatinybitofcomputation per\ncall,whereasthelearnedmodelrequiresvaluestobe\npropagatedthroughalearnedmodel(e.g.,aneural\nnetwork),whichtakesconsiderably longer.Using\nthesimplemodelforalignment andthecomplexone\n247Algorithm 1TheBinaryClassi\u0002cation Alignment Algo-\nrithm\n1:Given:AquerysetQ,atargetsetT,acostmodel\nK0,andalearningalgorithm L\n2:Initialize trainingsetsI=D=M=fg\n3:forallqi2Qdo\n4:ap alignK0(C(qi);qi)\n5:tn argmaxt6=C(qi)K0(t;qi)\n6:an alignK0(tn;qi)\n7: foralleventsai2apdo\n8:Label aipositive\n9:Insert aiintoI,D,orMifitisaninsertion,\ndeletion,ormatchevent,respectively.\n10: endfor\n11: foralleventsaj2ando\n12:Label ajnegative\n13:Insert ajintoI,D,orMifitisaninsertion,\ndeletion,ormatchevent,respectively.\n14: endfor\n15:endfor\n16:KL\ni L(I)\n17:KL\nd L(D)\n18:KL\nm L(M)\nThe\u0002nallearnedscoringfunction Fis:\nF(t;q)=KL(alignK0(t;q))\nforevaluationconsiderably reducesrunningtime,es-\npeciallyifwehavemultiplemodelsaswewillsee\nbelow.\n2.Toreduce thedif\u0002culty ofthelearning problem.\nLearning inthisdomainremainsdif\u0002cult,buttrain-\ningandtestingononlyhighscoringalignments con-\nstructedbyareasonable modeltrimsthesizeofthe\nspace,soweonlyneedfocusonlearningusefulcon-\nceptsthatthebasemodelhasmissed,ratherthanon\naligningsequences ingeneral.\nIncaseswherethelearningalgorithm isablelearna\nmodelthatwillaligncorrectly andquickly,thehand-built\nmodelmaybeomitted,andrandomalignments maybe\nusedfortraining. Inthisapplication, however,usingthe\nhand-builtmodeltoalignthesequences helpsagreatdeal\nwithperformance andspeed.\n3.2 Particularizing theAlgorithm for\nQuery-by-humming\nAnobviousadvantageofthisalgorithm overtable-based\nmethodsisthatwearenotconstrained bythenumberof\ndiscretevaluesthatournotescantake.Thisishandybe-\ncausetospecifyasongintermsofitsnotes,oneneedsat\nleasttworealvaluespernote,apitchandaduration. This\nmeansourcharacters orsymbols inthesequence will\nbevectorsofrealvaluesratherthandiscretecharacters\nasinFigure3.Learners forbinaryclassi\u0002cation prob-\nlemsarebyandlargecomfortable withreal-values,unlike\nthetable-based methoddescribed above,whererealval-\nuesmustingeneralberoundedorbinned.Inwhatfollowsbelow,weassumethatnotesare\nvector-valued,containing acomponent forboththepitch,\nsp,andtheduration,sd,ofanevents,soasongsisrep-\nresentedbyaseriesofduples:\ns=f(sp\n1;sd\n1);(sp\n2;sd\n2);:::;(sp\njsj;sd\njsj)g\nFurthermore, thepitchandduration ofthestarting\neventisoftenconsidered immaterial solongastheproper\nrelativepitchesanddurations aremaintained4:.Thus,we\ninsteadrepresent thesongusing pitchdifferencesanddu-\nration ratios:\ns=f(s\u000e\n1;sr\n1);(s\u000e\n2;sr\n2);:::;(s\u000e\njsj;sr\njsj)g(6)\nwhere\ns\u000e\ni=sp\ni+1\u0000sp\niandsr\ni=sd\ni+1\nsd\ni(7)\nWemaketwo\u0002nalmodi\u0002cations tothedatabefore\npassingittothelearningalgorithm: Wetakethelogof\nthedurationratio,asrecommended in[17]andformatch\nevents,ratherthanpassinginthetargetandquerysymbols\ndirectly,wepassinthequerysymbolandthecomponent-\nwisedifferencebetweenthetwo.Table1summarizes the\nfeaturesusedintrainingthelearnedmodel.\nt\u000e-Pitchdifferencefortargetevent\nlog(tr)-Logdurationratiofortargetevent\nq\u000e-Pitchdifferenceforqueryevent\nlog(qr)-Logdurationratioforqueryevent\n\u0001\u000e-jq\u000e\u0000t\u000ej\n\u0001r-jqr\u0000trj\nTrainingsetforKm=fq\u000e;log(qr);\u0001\u000e;\u0001rg\nTrainingsetforKi=fq\u000e;log(qr)g\nTrainingsetforKd=ft\u000e;log(tr)g\nTable1:TableofFeaturesusedinTrainingfortheQuery-\nby-humming Problem\n4BOOSTING FOR ACCURA TE\nSELECTION\nWehaveseenthen,thatifwehaveageneralbasemodel\nforsequence alignment, thenwecanuseittolearnanew\nmodel.However,aswewillseeintheresultssection,\nthelearnedmodelperforms slightlyworsethanthebase\nmodel.Aquickscanoftheoutputfromthesemodelsre-\nvealsthattheyarenotmakingthesamemistakes,andthat\nwemayreapbene\u0002tsifwearesomehowabletocombine\nthetwo.\nFurthermore, onecouldviewthealgorithm discussed\nintheprevioussectionasaniterativeone.Thatis,weuse\nthebasemodeltogeneratethetrainingdataforthelearn-\ningprocess,learnanewmodel,thenusethelearnedmodel\ntogeneratenewtrainingdataforthenextiteration. The\ngoalistotrainanumberofweak models,eachtrainedon\nthemistakesofmodelsinpreviousiterations. Thehopeis\nthatthesemodelswillcombinetogiveusaccuracygreater\nthananysinglemodelalone.\n4See[16] foraninteresting dissenting opinion.\n248Bothoftheseintuitions, iterativetrainingandweak\nmodelcombination, arecaptured inanalgorithm known\nasadaboost [18],theadaptiveboosting algorithm. Its\ngeneralformisgiveninAlgorithm 2,andweseethegen-\neralideasexpressed above.We\u0002rstlearnamodel,then\nweight themodelbasedonitsperformance onthetrain-\ningdata.Wewouldliketheweight, \u000btoincreaseasthe\nmodeldoesbetter.Afterlearningthemodelweupdatea\ndistributionoverthetrainingdata:Trainingexamplesthat\naremisclassi\u0002ed getgreaterweightinthenextiteration.\nTheseweightsareusedtoforcethelearningalgorithm to\nfocusitsattention onthemistakesofpreviousiterations.\nWethenlearnanewmodelandrepeattheprocess.\nAlgorithm 2Thegeneralformoftheadaptiveboosting\nalgorithm.\n1:Given:AtrainingsetS=f(x1;y1);:::;(xn;yn)g\n2:Initialize trainingsetweights D1(j)=1\nn\n3:Selectlearningalgorithm, L\n4:fori=1;:::;Tdo\n5:Getweakmodel hi L(S)usingweights Di\n6:Geterror\u000ftofhtonS\n7:Set\u000b=1\n2ln1\u0000\u000ft\n\u000ft8:Updatethedistribution:\nDi+1(j)=Di(j)exp(\u0000\u000biyjhi(xj))\nZt\nwhere Ztnormalizes Dtobeadistribution\n9:endfor\nOutputthe\u0002nalhypothesis, H:\nH(x)=sign TX\ni=1\u000bihi(x)!\nTheversionofadaboost giveninAlgorithm 2isused\nforsimplebinaryclassi\u0002cation problems with0-1loss.In\nthequery-by-humming problem, ifwehaveadatabaseof\ntargetsofsizen,wehaveann-classproblem. Inaddition,\nthelossfunctionisnot0-1.Ourmodelsareabletopro-\nducearankingofthetargetsbasedonthequery,andwe\nwouldliketopenalizemodelsbasedonhowlowthecor-\nrecttargetisranked.Foracostmodel, K,wethende\u0002ne\nthereasonable lossfunction 1\u00001\nrank K(q;T).\nUsingtheseguidelines, wemodifytheadaptiveboost-\ningalgorithm, givenasAlgorithm 3.Thechoiceof\u000bin\ntheoriginalalgorithm isusefulonlyforbinaryclassi\u0002ca-\ntionproblems andwillnotworkforushere5.Wechoose\n\u000btobethemean recipr ocal rankorMRRasusedin[3].\nThisiscloselyrelatedtothelossfunctionde\u0002nedabove.\nTheupdateforthedistributionoverthetrainingsetisalso\ntailoredtobinaryproblems. Weagainmakeamodi\u0002ca-\ntionsothatourlossfunctionisre\u0003ected intheupdate.\nThe\u0002nalmodelissimplyaweighted combination ofthe\nmodelslearnedineachiteration.\n5This isbecause ituses thefactthat aconsistently wrong\nmodel canbegivenanegativeweight andthewrong predictions\nmade toberight. Inourapplication, there isnosuch notion ofa\nmirror image modelAlgorithm 3Theadaptiveboostingalgorithm appliedto\nlearningcostmodelsforalignment\n1:Given:AquerysetQ=fq1;:::;qng,atargetsetT,\nandabasemodel K0\n2:Selectlearningalgorithm, L\n3:Initialize trainingsetweights D1(j)=1\nn\n4:fori=1;:::;Tdo\n5:Learnweakcostmodel KifromTandQusing\nAlgorithm 1,basemodel K0,andweights Di\n6:Set\u000b=1\nnP\nqk2Q1\nrank Ki(qk;T)\n7:Updatethedistribution:\nDi+1(j)=Di(j)exp\u0010\n\u000bi\n2\u0000\u000bi\nrank Ki(qk;T)\u0011\nZt\nwhere Ztnormalizes Dtobeadistribution.\n8:endfor\nOutputtheboostedcostmodel, K\u0003:\nK\u0003(t;q)=TX\ni=0\u000biKi(alignK0(t;q))\nWiththesechanges,weshouldbeabletocombinethe\nweakmodelslearnedintoasingle,strongermodel.This\nwewillattempttoverifyexperimentally .\n5EXPERIMENT ALSETUP\nAsdataforourexperiment, weuseabodyofsungqueries\ncollected bytheauthor.Thereareatotalof50singersand\n12differentsongs.Thesingerswerewequeryoverato-\ntalof12songs,witheachsingerchoosing thefourthey\nweremostfamiliarwithandsingingasmall,prede\u0002ned\nexcerptfromeachone.Fortrainingwesplitthedataon\nbothsingerandsong,sothealgorithm istestedonsingers\nandsongsithasneverheardbefore.Thisresultsina\ntrainingsetof100queriesover6songsand15singers\nandatestsetof321queriesover6songs35singers.The\nsingersintheexperiments weregenerally amateursingers\nwithexperience inacollegeorchurchchoirbutnotpro-\nfessionally .Weimaginethatthisdemographic willbethe\nmostlikelyusersofa\u0002nishedquery-by-humming system.\nForthetargetset,wealsosplitthedata,usinga421\nsongdatabase fortraininganda2000songdatabase for\ntesting.Weusethe2000songsfromthetestdatabaseto\nsimulatedatabases ofsmallertargetsetsbydrawingran-\ndomlyfromthese2000songs.Foreachsimulated size\nwedraw10randomdatabases fromourbasesetof2000\nandaveragetheresults.The2421targetsongsaswellas\nthe12querysongsaretakenfromtheDigital Tradition\n[8]databaseofmonophonic folkmelodies.\nOurbase,hand-builtmodelistheintervalmatching\nmodelconstructed in[3]andshowntobestate-of-the-art.\nThelearnedandboostedmodelsareconstructed usingthe\nmethodsoutlinedinprevioussections. Thebinaryclas-\nsi\u0002erweuseisaneuralnetworkasimplemented in[19]\nwiththedefaultparameters. Notuningwasdone.The\ntypeofalignment usedhereisthelocaltypede\u0002nedin\n249[10],whereweignorepre\u0002xesandsuf\u0002xesinthetarget\nbutnotthequery.Thatis,thealgorithm assumesthatthe\nqueryiscontained wholeinthecorrecttarget.\n6RESUL TS\nToevaluateourmodelsweplotMRRasthesizeofthe\ntargetsetsizeincreases inFigure4andthepercentage\nofsongsranked\u0002rstastargetsetsizeincreases inFigure\n5.Wewouldexpect,asthetargetsetgrows,thatboth\nofthesemeasures willdecrease onallmodels,butthat\nbettermodelswillshowlessofadecrease. Forreference,\nconsult[3].\n0 200 400 600 800 1000 1200 1400 1600 1800 20000.50.550.60.650.70.750.80.850.90.95\nTarget Set SizeMRRHand−built Model\nLearned Model\nBoosted Model (4 Iterations)\nFigure4:Meanreciprocal rankversusincreasing target\nsetsize.\n0 200 400 600 800 1000 1200 1400 1600 1800 20000.40.450.50.550.60.650.70.750.80.850.9\nTarget Set SizeProbability of Correct Target Ranked FirstHand−built Model\nLearned Model\nBoosted Model (4 Iterations)\nFigure5:Percentage ofqueriesreturning thecorrecttarget\nranked\u0002rstagainstincreasing targetsetsize.\nAswecanseefromtheplots,thehand-builtinterval\nmatching modeloutperforms themodellearnedinthe\u0002rst\niteration, buttheboostedmodelisabletooutperform both\nbyasigni\u0002cant margin,returning thecorrecttargetmore\nthan10%moreoftenthanthehand-builtmodel.Wealsonotethattherearevastdifferencesinperfor-\nmancebetweenthistestofthestandardintervalmodeland\nprevioustests.Inparticular ,thistestgivesamuchbet-\nterMRRthanin[3].Weechothecomments madethere\naboutresultsvaryingwildlywiththequerysetused.We\nimaginethisisduetothefactthatthesubjectsforthistest\nweretoldexactlywhattosing,andwerereminded ofthe\ntunebeforesinging.Thus,thequerieswerelikelybetter\nrenderings ofthetargetthanwasthecasein[3].\nInTable2weshowtheMRRforatargetsetsizeof\n2000forthehand-builtmodelandforeachiterationofthe\nboosting algorithm. Weseethattheboosting algorithm\nconvergesveryquicklywithourtrainingdata.\nModel MRR\nBaseModel 0.598\nLearnedModel 0.510\nIteration10.698\nIteration20.683\nIteration30.705\nIteration40.698\nTable2:MRRatatargetsetsizeof2000forallmodels\nFinally,recentcomparisons tothehand-builtmodel\nusedhere[3,2,6]ofteninvolvemethodsthattakesub-\nstantially morecomputation withoutshowingsubstantial\nimprovement.Wereportanecdotally thatthedifferencein\nrunningtimebetweenouralgorithm andthestandardin-\ntervalmatching modelislessthanafactoroftwowithno\noptimizations.\n7CONCLUSIONS AND FUTURE WORK\nWehavehereoutlinedamethodforconstructing aquery-\nby-humming systemthatsubstantially outperforms cur-\nrentstate-of-the-art methods withanegligibleincrease\ninrunningtime.Toaccomplish this,wehaveuseda\nweakmodelandastandard binaryclassi\u0002cation algo-\nrithm,alongwithaversionofadaptiveboosting tailored\ntothisparticular problem.\nSomeinteresting phenomena wereobservedasweex-\nperimented withvarioustrainingdata.First,asonemight\nexpect,ifwetrainthesystemonpoorqueries(wherethe\ntargetissungincorrectly), thenitdoesnotperformvery\nwell6.Ifwetrainonamixofgoodqueriesandpoorones,\nthentheboosting algorithm tendstoring,alternately\nweighting thegoodandthepoorqueriesheavily,andthus\nalternately learninggoodandpoormodels.\nWealsoconcedethatwehaveabusedtheboostingfor-\nmalismhereinsomesense.Wehaveusedourhand-built\nmodel, h0asoneofourweakmodels.Theboostingalgo-\nrithmisprovablyconvergentinthelimitofin\u0002niteitera-\ntions,butifwecontinuetoiteratethealgorithm, wewill\neventuallylosetheeffectofh0,becauseitisnotgener-\natedfromthedata.Hencewemustbecarefulofover-\niterating thealgorithm andselectanumberofiterations\nthatcompromises betweentheusefulness oflearningad-\nditionalmodelsandtheusefulness ofthebasemodel.In\n6One might makethecomparison toachild with tone deaf\nparents ifonewere soinclined.\n250thisinstance, thebasemodelturnsouttobeuseful,cap-\nturingsomethingsthatthelearningprocessdoesnot.One\ncanimagineaninstanceinwhichthelearningprocesscap-\nturesalloftheusefulness ofthebasemodel.Inthiscase\nwewouldonlyusethebasemodelforconstructing align-\nmentsandwouldbefreetoiterateuntilconvergence.\nSowearenottiedtothenotionofusingahand-built\nmodelaspartofthe\u0002nalmodel.Infact,wearetieddown\ninrelativelyfewways.Thenotionofboostingcanbeap-\npliedtoanyalgorithm thatisabletolearnamodelfor\nselecting sequences basedonasetoftarget-query align-\nments,andtherehasbeenmuchrecentworkinmachine\nlearningonthissubject[20,21,15].Ifweusethebinary\nclassi\u0002cation formalism, wecanchooseanyrepresenta-\ntionfornotesthatsuitsus,usingabsolutevaluesofpitch\nandtime,asin[16]orvaluesforpitchandtimerelativeto\notherelements asin[2].Inaddition, wecanimagineus-\ningsubsequences ofnotesasthesymbolsofthesequence,\nratherthansinglenotes.Thiscorresponds totheideaof\nsliding window classi\u0002cation asoutlinedin[22].Withpa-\nrametertuning,itishighlyprobablethatperformance will\nincreasefurtherstill.Preliminary experiments indicateas\nmuch.\nFinally,wenotethatthisformalism isnottiedtothe\nquery-by-humming problem. Thismethodmaybeuseful\nforretrievalofstructures otherthansequences ofnotes\n(sequences ofothertypesofelements, trees,graphs,etc.).\nFutureworkmayentailexpanding theideasoutlinedin\nthispapertootherdomains.\nREFERENCES\n[1]StephenDownieandProf.MichaelNelson. Eval-\nuationofasimpleandeffectivemusicinformation\nretrivalmethod. InProc.23rdInternational ACM\nSIGIR confer ence onResear chandDevelopment in\nInformation Retrie val,2000.\n[2]BryanPardo,WilliamBirmingham, andJonah\nShifrin. Namethattune:Apilotstudyin\u0002nding\namelodyfromasungquery.Journal oftheAmeri-\ncanSociety forInformation Science andTechnolo gy,\n55(4),2004.\n[3]RogerB.Dannenber g,WilliamP.Birmingham,\nGeorgeTzanetakis, ColinMeek,NingHu,and\nBryanPardo.Themusarttestbedforquery-by-\nhumming evaluation. InProc.4thInternational\nSymposium onMusic Information Retrie val,2003.\n[4]NaokoKosugiandYuichiNishihara. Apracti-\ncalquery-by-humming systemforalargemusic\ndatabase. InProc.8thACMMultimedia Confer ence,\n2000.\n[5]Dominic Mazzoni andRogerB.Dannenber g.\nMelodymatching directlyfromaudio.InProc.2nd\nAnnual International Symposium onMusic Informa-\ntionRetrie val,2001.\n[6]ColinMeekandWilliamBirmingham. Johnnycan't\nsing:Acomprehensi veerrormodelforsungmusic\nqueries. InProc.3rdInternational Symposium on\nMusic Information Retrie val,2002.[7]PaulBoersma. Accurate short-term analysisofthe\nfundamental frequencyandtheharmonics-to-noise\nratioofasampledsound. Proceedings oftheInstitute\nofPhonetic Sciences,17:97110, 1993.\n[8]TheDigitalTradition FolkMusicDatabase.\n[9]M.S.SmithandT.F.Waterman. Identi\u0002cation of\ncommonmolecular subsequence. Journal ofMolec-\nular Biolo gy,147:195197, 1981.\n[10]ColinMeek. Modelling errorinquery-by-humming\napplications .PhDthesis,TheUniversityofMichi-\ngan,2004.\n[11]JieWei.Markoveditdistance. IEEE Transac-\ntions onPattern Analysis andMachine Intellig ence,\n26(3):311321, March2004.\n[12]MarcelMongeau andDavidSankoff.Comparison of\nmusicalsequences. Computer sandtheHumanities ,\n24:161175, 1990.\n[13]RichardDurbin,SeanEddy,AndersKrogh,and\nGraemeMitchison. Biolo gical Sequence Analysis.\nCambridge UniversityPress,1998.\n[14]StuartRussellandPeterNorvig. Arti\u0002cial Intelli-\ngence: AModern Appr oach,chapter20.Prentice\nHall,secondedition,2003.\n[15]IoannisTsochantaridis, ThomanHofmann, Thorsten\nJoachims, andYaseminAltun.Supportvectorma-\nchinelearningforinterdependent andstructured out-\nputspaces.InProc.21st International Confer ence\nonMachine Learning,2004.\n[16]ColinMeekandWilliamP.Birmingham. Thedan-\ngersofparsimonyinquery-by-humming applica-\ntions.InProc.4thInternational Symposium onMu-\nsicInformation Retrie val,2003.\n[17]BryanPardoandWilliamBirmingham. Encoding\ntiminginformation formusicalquerymatching. In\nProc.3rdInternational Symposium onMusic Infor -\nmation Retrie val,2002.\n[18]RobertE.Schapire. Theboosting approach toma-\nchinelearning: Anoverview.InMSRI Workshop on\nNonlinear Estimation andClassi\u0002cation ,2002.\n[19]IanH.WittenandEibeFrank. Data Mining: Practi-\ncalMachine Learning Tools with JavaImplementa-\ntions.MorganKaufmann, SanFrancisco, 2000.\n[20]JohnLafferty,AndrewMcCallum, andFernando\nPereira. Conditional random\u0002elds:Probabilistic\nmodelsforsegmenting andlebelingsequence data.\nInICML,2001.\n[21]BenTaskar,CarlosGuestrin, andDaphane Koller.\nMaxmarginmarkovnetworks.InNIPS,2004.\n[22]ThomasG.Dietterich. Machinelearningforsequen-\ntialdata:Areview.Structur al,Syntactic, andSta-\ntistical Pattern Reco gnition; Lectur eNotes inCom-\nputer Science,2396:1530, 2002.\n251"
    },
    {
        "title": "User Evaluation of a New Interactive Playlist Generation Concept.",
        "author": [
            "Steffen Pauws",
            "Sander van de Wijdeven"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415180",
        "url": "https://doi.org/10.5281/zenodo.1415180",
        "ee": "https://zenodo.org/records/1415180/files/PauwsW05.pdf",
        "abstract": "Selecting the ‘right’ songs and putting them in the ‘right’ order are key to a great music listening or dance experience. ‘SatisFly’ is an interactive playlist generation system in which the user can tell what kind of songs should be contained in what order in the playlist, while she navigates through the music collection. The system uses constraint satisfaction to generate a playlist that meets all user wishes. In a user evaluation, it was found that users created high-quality playlists in a swift way and with little effort using the system, while still having complete control on their music choices. The novel interactive way of creating a playlist, while browsing through the music collection, was highly appreciated. Ease of navigation through a music collection is still an issue that needs further attention. Keywords: playlist generation, user evaluation, constraint satisfaction. 1",
        "zenodo_id": 1415180,
        "dblp_key": "conf/ismir/PauwsW05",
        "keywords": [
            "playlist generation",
            "user evaluation",
            "constraint satisfaction",
            "interactive playlist",
            "music listening",
            "dance experience",
            "user control",
            "music collection",
            "ease of navigation",
            "novel interactive way"
        ],
        "content": "USER EVALU ATION OFANEW INTERA CTIVE PLA YLIST\nGENERA TION CONCEPT\nSteffen Pauws\nPhilipsResearch\nProf.Holstlaan 4\n5656AAEindhoven,theNetherlands\nsteffen.pauws@philips.comSander vandeWijdeven\nPhilipsResearch\nProf.Holstlaan 4\n5656AAEindhoven,theNetherlands\nsander.van.de.wijdeven@philips.com\nABSTRA CT\nSelecting the`right'songsandputtingtheminthe`right'\norderarekeytoagreatmusiclistening ordanceexperi-\nence.`SatisFly' isaninteractiveplaylistgeneration sys-\nteminwhichtheusercantellwhatkindofsongsshould\nbecontained inwhatorderintheplaylist,whileshenavi-\ngatesthroughthemusiccollection. Thesystemusescon-\nstraintsatisfactiontogenerateaplaylistthatmeetsalluser\nwishes.Inauserevaluation, itwasfoundthatuserscre-\natedhigh-quality playlistsinaswiftwayandwithlittleef-\nfortusingthesystem,whilestillhavingcomplete control\nontheirmusicchoices.Thenovelinteractivewayofcre-\natingaplaylist,whilebrowsingthroughthemusiccollec-\ntion,washighlyappreciated. Easeofnavigationthrough\namusiccollection isstillanissuethatneedsfurtheratten-\ntion.\nKeywords:playlistgeneration, userevaluation, con-\nstraintsatisfaction.\n1INTR ODUCTION\nPlaylistcreationrangesfromthelaborious variantofhav-\ningtoselecteachsongone-by-one totheeaseofran-\ndom/shuf \u0003eplayandone-click playlistgeneration. Inthe\nlattermethod,auseronlyhastoindicateasingle`seed'\nsongandgetsacomplete playlistwithadditional songs\ninreturnbyasinglebuttonpress.Inthispaper,wede-\nscribetheworkingandevaluationofthe`SatisFly' system\nthatallowsausertoselectsongsone-by-one, toaskfor\nadditional `similar' songsbasedonareferentsong,and\ntospecifyadditional requirements thattheplaylistshould\nmeet.\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\n\u00002005 Queen Mary ,University ofLondon2SATISFL YSYSTEM\n`SatisFly' isasoftwaresystemforautomatic playlistgen-\neration.Auserspeci\u0002esrequirements whatkindofsongs\nshouldbeintheplaylistandwhatkindofsongsshould\nnot.Thesystemthenusesconstraint satisfactiontoarrive\natalistofsongsthatmeetstheserequirements. Theinput\nmodality ofthesystemisaconventionalremotecontrol\nusingthecursorkeys,the`ok'button,andthecolorkeys.\nItsoutputmodality isavisualdisplay.\nFigure1:SatisFlyplaylistgeneration system.\nUserscanbrowsethroughthemusiccollection usinga\ntwo-panelvisualisation, asshowninFigure1.Movement\nthroughthepanelsisdonebyusingthecursorkeyson\ntheremotecontrol.Theleft-hand panelprovidesthecur-\nrentchoicesfortheuser.Theright-hand paneldisplays\ntheconsequences ofachoice.Selecting anitemcanbe\ndonebypressingthe`ok'button.Colorkeyscanbeused\ntoinvokefunctions; inFigure1,theusercaninvokea\n`reset',a`clear'anda`generate' function. Whilenavigat-\ningandlistening, songscanbeaddedtoorremovedfrom\naplaylist.Inaddition, ausercanselectplaylistrequire-\nmentson,forinstance,\u0001thenumberofsongsordurationofaplaylist,\u0001thevarietyingenres,artists,andalbums,\u0001tempoandperiodofreleaseofthesongs,and\u0001thesimilarity ofsongs.\n638Tothisend,thesystemusesadatabasewithattribute\ninformation abouteachsongincluding songtitle,artist,\nalbum,genre,duration, yearofrelease,andtempo.\nUserscanselectandalteralmostanycombination of\nrequirements. Bypressingasinglebutton,thesystemgen-\neratesaplaylistsatisfying thecurrentsetofrequirements.\nAtalltimes,thecontentandsongorderoftheplaylist\ncanbechangedmanually,leavingcomplete controlinthe\nhandsoftheuser.\n2.1 Constraint satisfaction\nInaconstraint satisfactionapproach (Tsang,1993),the\nplaylistrequirements aremodelled aslogicalconstraints\nofsongattributes(e.g.,artistname,genre,tempo)de\u0002ned\noverplaylistpositions. Eachconstraint limitsthecom-\nbinations ofsongsthatareallowedintheplaylist. Con-\nstraintscanbedistinguished bythenumberofplaylist\npositions onwhichtheyarede\u0002ned: unary,binary,and\nglobalconstraints. Unaryconstraints restrictthesongs\nthatareallowedtooccuratasingleplaylistposition. An\nexampleisthatthe\u0002rstsongshouldbeofaparticular\nartist.Binaryconstraints represent abinaryrelationthat\nhastobemetbetweensongsattwo(successive)playlist\npositions. Forinstance, twosuccessivesongsshouldhave\nthesametempo.Finally,globalconstraints arede\u0002nedon\nanynumberofpositions; theycanrepresent asetofunary\norbinaryconstraints. Forinstance, ifwewanttobound\nthenumberofoccurrences ofparticular attributevaluesin\naplaylist,wecaninstantiate acounting constraints. Us-\ningthisconstraint, wecandeclarethatwewant,say,4to\n6Rocksongsinaplaylistof10songs,oratmost,say,3\nsongsof`Prince'or`Michael Jackson'. Inthesamevein,\nconstraints arede\u0002nedforsortingsongsinaplaylist,for\nensuringthesimilarity ofsuccessivesongs,totalduration\noftheplaylist,etc.\nWhengenerating aplaylist, songsareassigned to\nplaylistpositions inaconstructi vesearchmethodwhile\nguaranteeing thatallconstraints willbesatis\u0002ed. The\nsearchmethodconsistsofconstr aint propagation,con-\nstruction,andbacktrackingthatareapplieduntileithera\ncomplete andconsistent playlisthasbeenfoundoritis\naborted.\nConstraint propagation isthesetoftechniques aimed\natreducing thesearchspacebyeliminating songsfrom\nwhichitcanbedetermined thattheycannotbepartof\naplaylistthatmeetsallconstraints. Forinstance, ifwe\nknowthatweonlywantRocksongswithagiventempo\nrangetoappearinapartoftheplaylist,wecanleaveout\nallsongsthatdonot\u0002tthisdescription fromfurthercon-\nsideration.\nForconstruction, playlistpositions areaddressed one-\nby-oneusingthefail-\u0002r stprinciple:positions aread-\ndressed\u0002rstforwhichthesmallestnumberofsongsis\navailable.Foreachconsidered position,songsarechosen\nbasedonaconstr aint voting principle:onlythosesongs\naretriedfromwhichitcanbecomputed thatmostcon-\nstraintswillbesatis\u0002ed.\nIfnosongscanbefoundforthecurrentpositionwith-\noutviolating anyoftheconstraints, adeadendinthe\nsearchspacehasbeenreached. Abacktracking proce-dureisthenrequiredthatchangesthesongassignment\natapreviousposition; weuseachronological variantof\nbacktracking. Ifnobacktrack ispossible, allpossible\nsongassignments havebeenevaluatedwithoutsuccess.In\notherwords,thereisnocomplete andconsistent solution.\nFortunately ,itispossibletokeeptrackofthebestpossi-\nblepartialsolution, thatis,thelongestpartialplaylistfor\nwhichallconstraints arestillsatis\u0002ed. Asanexpedient\nforcompleting aplaylisttoitsrequiredlength,weaddran-\ndomlyselectedsongsthathavenotbeeneliminated during\npreviousconstraint propagation steps.\nOneoftheadvantagesofconstraint satisfactionisthat\nitstrivesforexactsolutions byconstructi vesearchand,\nhence,providesmeanstodetectthatnoplaylistexistwhile\nperforming thesearchprocess.\n2.2 Related work\nWithoutproof,westatethatautomatic playlistgeneration\ninthecurrentde\u0002nition isaNP-hardproblem. Itisthus\nunlikelythatapolynomial algorithm existsthatcomputes\naplaylistthatmeetsanygivensetofconstraints.\nLiterature presentsseveralapproaches fortheauto-\nmaticplaylistgeneration problem. Alghoniemy andTew-\n\u0002k(2001)formulated theproblemasanintegerlinearpro-\ngramming (ILP)problemandusedastandardILPsolver.\nThisisnotatime-ef\u0002cientmethodandhencenotpracti-\ncal.Constraint satisfactiontechniques havebeenalsoused\nbyothers(Pachet,Roy,andCazaly,2000),whichareless\ninef\u0002cientthanintegerprogramming. Forfurtherscala-\nbility,localsearchhasbeenproposed andrealized(Au-\ncouturier andPachet,2002).Unfortunately ,themethods\nwerenotpairedwithathorough evaluationandapplica-\ntiontoprospectiveuserstoassesstheperformance and\nuserbene\u0002tsofthemethods.\n3USER TEST\nTheusertestassessed usertaskperformance, perceived\nease-of-use andusefulness, anduserpreference ofthe\n`SatisFly' playlistcreationsystemincomparison witha\ncontrolsystem.Incontrastto`SatisFly', Thecontrolsys-\ntemdidnotconstructi velymeettherequirements related\ntowhatgenres,artists,andalbumsshouldbepresentin\ntheplaylist;itmadeuseofarandomselection process\nforaddressing theserequirements. However,itdidmeet\nallotherwishes,forinstancethoserelatedtotimepe-\nriod/tempo rangeselection andordering. Notethatthe\nuserinterfaceofbothsystemswere\u0002xed.Testpartici-\npantswereaskedtocreateaplaylistusingbothsystems\ntwice(i.e.,attwotrials)fora\u0002xed,personally imagined\nmusiclisteningsituation.\n3.1 Hypotheses\nParticipants aregivenampletimeformakingapreferred\nplaylist;itisexpectedthatthequalityoftheplaylistdoes\nnotdifferundervariousexperimental conditions. How-\never,weexpectthatlesstimeandfeweractionsarere-\nquiredtomakeaplaylistwhenusingthe`SatisFly' sys-\ntemthanwhenusingthecontrolsystem.Inaddition, we\n639Figure2:Panels(a)and(b)showrespectivelymean time ontaskandmean number ofactionsacrosssystemsandtrials.\nCross-bars represent standarderror.\nexpectlesstimeandfeweractionsarerequiredtomakea\nplaylistatthesecondtrial.\nBecauseofitstime-andeffort-saving,weexpectthat\nthe`SatisFly' systemwillbevaluedasmoreusefuland\nmoreusable,andhencemorepreferred, thanthecontrol\nsystem.\n3.2 Participants\nTwenty-four persons(14m,10f,avgage:29yrs)partici-\npatedvoluntarily duringnormalworkingtime.Theywere\nallcolleagues orstudentsoftheresearchlaboratory .All\nparticipants werefrequentlistenerstopopularrockmusic.\nAllparticipants hadcompleted highervocational educa-\ntion.\n3.3 Design\nAfactorialwithin-subject designwithtwoindependent\nvariables,named systemandtrial,wasused.Thevariable\nsystemreferredtothe`SatisFly' andthecontrolsystem.\nTrialreferredtothetwotasktrials,intended tomeasure\nchangesinperformance, userperception, andpreference\nasaresultofexperience. Tocompensate foranyorderef-\nfects,participants wererandomly assigned tooneofthe\nsixpossiblepermutations ofadmissions tothetwosys-\ntems.\n3.4 Testequipment andmaterial\nAmusiccollection comprising 2248popularmusic\nrecordings from169CDalbumsfrom111differentartists\ncovering7differentmusicalgenresreleasedintheperiod\nfrom1963to2001inMP3formatservedastestmaterial.\nThetestequipment consisted ofaPConwhichthesys-\ntemwasrunning. ThedisplaywasdirectedtoaPhilips\nMatchLine televisionset.Theremotecontrolwastapped\ntocontrolboththetelevisionsetandthePC.Theaudio\nwasdirectedtoamid-range audioampli\u0002er andapairof\nhi-\u0002loudspeakers.Participants wereseatedinacomfort-\nablechairinfrontofthetelevisionsetandaudioampli\u0002-\ncationsystem.\n3.5 Procedur e\nParticipants wererandomly assigned toanorderofsys-\ntemadmission inthetest.Theyreceivedampleinstruc-tion,practice,andtimetomasterthesystemunderstudy\nwithoutneedforoutsidehelp.Foreachtrial,thesystem\nwaspresented withadifferentcolourforallowingrefer-\nenceinthequestionnaires. Obviously,participants were\nnottoldaboutthenatureofthesystems.Different10-song\nplaylistshastobecreatedoverfourtrials,butrepresent-\ningintentions forthesamelisteningsituation. Qualityof\ntheplaylistwaspresented asthesoleoptimisation crite-\nrion.Aftereachtrial,participants completed aquestion-\nnaire.Attheendofthetest,participants rankedthesys-\ntemsaccording totheirpreference ofuse.Subsequently ,\ntheywereaskedtoratetheplaylistona0-10scaleandto\nindicatewhatsongsintheplaylistdidnot\u0002ttheintended\nlisteningsituation, aftersecondlistening.\n3.6 Measur es\n3.6.1 Playlist quality\nPlaylistqualitywasmeasured byprecisionandarating\nscore.Precisionwasde\u0002nedastheproportion ofpartic-\nipantindicated preferred songsinaplaylistof10songs.\nTherating scorewasaparticipant' sratingonascalerang-\ningfrom0to10(0=extremely bad,\u0002\u0003\u0002\u0004\u0002,10=excellent).\n3.6.2 Taskperformance\nTaskperformance wasmeasured bytime ontaskandnum-\nberofactions.Timeontaskmeasured thetimeelapsed\nfromtheparticipant performing the\u0002rstbuttonpressto\ntheparticipant performing thelastbuttonpress. Number\nofactionsmeasured thenumberofbuttonpressesonthe\nremotecontrolthatwereperformed bytheparticipant.\n3.6.3 Perceived ease ofuseandperceived usefulness\nAn(adapted) versionoftheTechnology Acceptance\nModel(TAM)questionnaire (Davis,1989)assessedper-\nceivedeaseofuseandperceivedusefulness. Participants\nresponded bystatingtowhatextenttheyagreedwitha\nstatement inthequestionnaire ona7-pointscale.\nStatements assessing perceivedeaseofusewerethe\nfollowing:\nQ1.I\u0002ndlearning howtousethesystem easy.\nQ2.I\u0002nditeasy togetthesystem todowhat Iwantittodo.\nQ3.I\u0002nditeasy tobecome skilful atusing thesystem.\nQ4.I\u0002ndthesystem easy touse.\n640Statements assessing perceivedusefulness werethe\nfollowing:\nQ5.I\u0002ndthatbyusing thesystem Icanmakegood playlists.\nQ6.I\u0002ndthatbyusing thesystem Iamable tocreate aplaylist\nrapidly .\nQ7. I\u0002nd thatbyusing thesystem Ienjoythemaking ofa\nplaylist.\nQ8.I\u0002ndthissystem useful athome.\n3.6.4 Orderofpreference\nOrderofpreference ofthesystemswasassessedbyhav-\ningparticipants thesystemsrankfrom1to4according\ntotheirpreference. Rankvalue1wasassigned tothe\nmostpreferred system.Indecisions resulting intotiesin\ntherankingweretreatedasequalpreference forthesys-\ntemsinvolved;theirjointrankvaluewasthemeanofrank\nvaluesthattheywouldbeassignedto.\n3.7 Results\nAllanalysesofvariance(MANOVA)wereconducted with\nrepeated measures andwith systemandtrialaswithin-\nsubjectindependent variables.\n3.7.1 Playlist quality\nWithprecisionasdependent variable,amaineffectfor\ntrialwasfoundtobesigni\u0002cant (F(1,23)=5.24,p¡0.05).\nOnaverage,playlistscreatedatthesecondtrialcontained\nhalfapreferred songmorethantheplaylistscreatedatthe\n\u0002rsttrial(mean precision:0.83(trial1),0.88(trial2)).\nWithrating scoreasdependent variable,noeffectswere\nfoundtobesigni\u0002cant. Participants ratedtheirplaylists\nconsistently .Themean rating scoreforaplaylistwas7.5.\n3.7.2 Taskperformance\nTheresultsontime ontaskareshownintheleft-hand\npanel(a)ofFigure2.\nWithtime ontaskasdependent variable,amaineffect\nforsystemwasfoundtobesigni\u0002cant (F(1,23)=13.78,p¡\n0.001).Makingaplaylistwiththe`SatisFly' systemtook\n505seconds(8:25),onaverage,whichwasfasterthan\nwiththecontrolsystemwhichtook646seconds(10:45).\nAmaineffectfortrialwasfoundtobesigni\u0002cant\n(F(1,23)=15.46,p¡0.001).Makingaplaylistforthe\n\u0002rsttime,whichwas658seconds(10:58),tookmoretime\nthanforthesecondtime,whichwas491seconds(8:11).\nNoothereffectswerefoundtobesigni\u0002cant.\nTheresultsonnumber ofactionsareshowninthe\nright-hand panel(b)ofFigure2.Amaineffectforsys-\ntemwasfoundtobesigni\u0002cant (F(1,23)=4.59,p¡0.05).\nParticipants performed 258actions,onaverage,whenus-\ningthe`SatisFly' system,whichwasafewernumberthan\nwhenusingthecontrolsystem(302actions).\nAmaineffectfortrialwasfoundtobesigni\u0002cant\n(F(1,23)=9.87,p¡0.01).Participants performed more\nactionswhenworkingwiththesystemsforthe\u0002rsttime\n(322actions)thanforthesecondtime(239actions). No\nothereffectswerefoundtobesigni\u0002cant.3.7.3 Perceived ease ofuseandusability\nResponses totheadaptedTAMquestionnaire weresub-\njectedtoatwo-dimensional non-linear principal compo-\nnentanalysis. Theeightitemsinthequestionnaire were\ntreatedasactivevariablesandthetwodifferentsystems\novertwotrialsweretreatedaspassivevariablestolabel\ntheplot(i.e.,SatisFly1,SatisFly2,control1,control2).\nTheresponses weretreatedasordinalcategories.\nThevisualisation ofthePCAsolutionoftheTAM\nquestionnaire isshowninFigure3.7.3.Itdisplaysthe\nmeantransformed itemresponses relatedtothetwodif-\nferentsystemsovertwotrials.Also,themeanscores\ntotheeightquestionnaire items(i.e.,Q1toQ8)aredis-\nplayed.Thedashedlinesgothroughtheoriginandthe\nmeanscoresofeachgroupofitems.Theselinesrepre-\nsentthe`mean'axesalongwhichthetransformed ordinal\nresponse categoriesoftheitems(i.e.,the7-pointscaleof\nthequestionnaire) arelocated.\nA\u0002rstobservationofFigure3.7.3tellsusthatthe`Sat-\nisFly'systemsandthecontrolsystemsarepositioned at\neithersideoftheorigin.However,aregressiontothe\nmean(i.e.,theorigin)overtrialsisclearlyvisible.Itemre-\nsponsesweremorediscriminatory afterworkingwiththe\ntwodifferentsystemsforthe\u0002rsttime(i.e.,SatisFly1,\ncontrol1),butresponses weremoresimilarafterworking\nwiththetwodifferentsystemsforthesecondtime(i.e.,\nSatisFly2,control2).\nThescoresfortheitemsQ1,Q2,Q3,andQ4are\nhighlycorrelated aswellasthescoresfortheitemsQ5,\nQ6,Q7,andQ8,thoughitemsQ2andQ6arespeculative.\nNevertheless, thehighcorrelations meanthatthetwosets\noffouritemsloadondifferentfactorsthatcanbelabelled\nas`perceivedeaseofuse'and`perceivedusefulness'. As\nshowninFigure3.7.3,bothgroupsofcorrelated itemsare\ndisplayed asclusterswhiletheirresponse categoriesare\nbestdisplayed astwoalmostorthogonal axes(thedashed\nlines).Inthisway,theupperright-hand cornerandthe\n\u0002rstquadrant represents high`perceivedeaseofuse',and\nthelowerright-hand cornerofthefourthquadrant repre-\nsentshigh`perceivedusefulness'.\nThevisualisation oftheTAMsolutionsuggests that\nworkingforthe\u0002rsttimewiththe`SatisFly' systempro-\nvidedthehighestperceivedusefulness. Thisdropped\nwhenworkingforthesecondtimewithit.Theuseful-\nnessofbothcontrolsystemswasperceivedlowerthan\nboth`SatisFly' systemsbutitdidnotchangeovertrials.\nItalsosuggests thatworkingforthe\u0002rsttimewiththe\ncontrolsystemprovidedthelowestperceivedeaseofuse.\nThisimprovedwhenworkingforthesecondtimewithit.\nBoth`SatisFly' systemswereperceivedaseasiertouse\nthanthecontrolsystems.\n3.7.4 Orderofpreference\nParticipants wereaskedtorankthecombination ofasys-\nteminatrialaccording totheirpreference. Rankvalue\n1wasassigned tothemostpreferred combination; simi-\nlarrankingofcombinations wasallowed.Fifteen(outof\n24)participants rankeda`SatisFly' astheirmostpreferred\nsystem.Fiveparticipants rankedacontrolsystemastheir\nmostpreferred system.Fourparticipants rankedeithera\n`SatisFly' oracontrolsystemastheirpreferred one.\n641Figure3:Thenon-linear principal component solutionfortheeightitemsoftheTAMquestionnaire loadedontheterms\n`perceivedease-of-use' and`perceivedusefulness'.\nForamoredetailedanalysis,therankingdatacanalso\nbeusedtoindicatecomparati vejudgements ofallpairs\nofsystems. Thetaskofrankingfoursystemsrequires, in\nessence,thecomparisons ofsixpairsofsystemstotell\ntheirrelativepreferences. Tiesintherankingweretreated\nasequalpreference ofthesystemsinvolved.Usingthis\nmodeofthought,wecandetermine theproportion ofthe\ntimethatasystemismorepreferred thananyothersystem.\nTheseproportions areshowninTable1.\nTable1:Proportion oftimesthatasysteminatrialatthe\ntopwaschosenoverasysteminatrialattheside.\nSatisFly1SatisFly2control1\nSatisFly214/24\ncontrol120/2415.5/24\ncontrol218/2415/248.5/24\nThestandardwaytoanalysepair-comparison datais\nbasedonThurstone' slawofcomparati vejudgment. In\nourcontext,thislawassumesthatameanpsychological\nvalueisattachedtoeachsystembyusers.Now,theextent\ntowhichonesystemisjudgedtobemorepreferred than\nanotherisrelatedtothedifferenceinthesevaluesofthe\ncompared systems. Werefertothesepsychological val-\nuesasscalevalues.Togofromproportional datatoscale\nvalueinaleast-squares problemsense,werefertoGuil-\nford(1954).\nBysettingthescalevalueofsystem`control1'tozero\n(whichhappened tobetheleastpreferred system), the\nleast-squares solutionoftheover-determined setofequa-\ntionsyieldsthescalevalueestimates asshowninTable2.Thestandarderroroftheestimates was\u0005\u0007\u0006\t\b\u0003\n.Thecorre-\nlationbetweentheobservedz-scoresandthepredicted z-\nscore(fromtheleast-squares solution) ishigh(r=0.941)\nwhichmeansthat88.7%ofthevarianceisexplained.\nTable2:Scalevaluesofthefoursystems.\nSatisFly10.90control20.30\nSatisFly20.56control10.00\nThescalevaluesinTable2showsthatthecombina-\ntionsofsystemsandtrialscanbeorderedaccording to\ntheirpreference. Participants hadanoverallpreference for\nthe`SatisFly' systemusedatthe\u0002rsttrial(i.e.,SatisFly\n1),followedbythesamesystematthesecondtrial(i.e.,\nSatisFly2).Theyhadtheleastpreference forthecontrol\nsystemthattheyhadusedinthe\u0002rsttrial(i.e.,control1).\n4DISCUSSION\nWhenusingthe'SatisFly' system,participants needed2\nminutesand20secondslesstimeand44feweractionsto\ncreateaplaylistthanusingthecontrolsystem.Thiswas\nalldonewithoutanydecreaseinqualityoftheplaylistbe-\ningcreated.Thus,`SatisFly' enabledparticipants tocreate\ntheirpreferred playlistinlesstimeandfeweractions.\nThetestfoundoutthatparticipants neededalmost3\nminutes(167seconds) lesstimeand83feweractionsto\nmakeaplaylistatthesecondtrial.Thiswasalldonewith-\noutanydecrease inqualityoftheplaylistbeingcreated.\nThus,learnability ofthesystemswaslessofaissue;in\n642shorttime,participants becameskilfulincreatingtheir\npreferred playlist.\nTheTAMquestionnaire indicated thatthe`SatisFly'\nsystemwasperceivedmostuseful,especially whenused\nforthe\u0002rsttime,andthatthecontrolsystemwasper-\nceivedleasteasy-to-use, especially whenusedforthe\u0002rst\ntime.\nThetasktoorderthesystemsonpreference foundout\nthatthe`SatisFly' systemwaschosenoverthecontrolsys-\nteminbothtrials.Itwasremarkable thatparticipants were\nconsistent in\u0002ndingthatworkingwithonesystemforthe\n\u0002rsttimeisdifferentfromworkingwiththesamesystem\nforthesecondtime.\n5CONCLUSION\nEasy-to-use toolstopickoutthe'right'songsandtoput\ntheminthe'right'orderfromadaunting volumeofmu-\nsicareattractivefeaturesofmusicplayers.Mostpartic-\nipants(16/24)statedexplicitlytheirappreciation ofthe\nnovelwayofplaylistcreationbyselection andgeneration\nasdemonstrated bythe`SatisFly' concept.\nThe`SatisFly' systemenablesuserstocreatehigh-\nqualityplaylistsinaswiftwayandwithlittleeffort,while\nhavingstillcomplete controlontheirmusicchoices. In\nthetest,participants neededmorethan8minutesand\nabout250buttonpressesontheremotecontroltocreatea\nplaylist.Weexpectthatuserswillspendlessthan8min-\nuteswhenselecting musicfromtheirpersonalmusiccol-\nlection.Thelargenumberofactionsrequiredisde\u0002nitely\nanissue,asitismainlycausedbyrepetitivenavigation\nbehavioursuchasgoingthroughlonglistsorswitching\nbetweenpanels.Ingeneral,designsofnavigationstruc-\nturesshouldfocusonminimization ofnumberofactions\nrequired.\nObservationsduringthetestmadeclearthatthebe-\nhaviourofusersisnotuniform. Usersdifferindegree;\nsomeneedmorethan20minutestoselect10songs.Oth-\nersspendonly2minutes. Usersalsodifferinessence;\nsomearepreciseinformulating theirmusicpreferences.\nOthersjustpicksomesongsorletthesystemgenerate\nsomerandomsongs.Yetothersselectallsongsone-by-\noneandusethesystemonlytoorderthesesongsontempo\noryear.Itmightbeobviousthatthewayinwhichusers\nuseaninteractivesystemdetermines howtheywillappre-\nciatethesystem.\nConstraint satisfactiontriesto\u0002ndanexactsolution\nbymeetingallconstraints inaplaylistgeneration prob-\nlem.Anotherwayisto\u0002ndonlyanapproximate solution,\nwhichsolvesissuesonfeasibility ,scalability andrunning\ntimewithrespecttolongerplaylistsandlargermusiccol-\nlections.Weusedasimulated annealing approach tosolve\ntheproblemapproximately .Findings inauserevaluation\nmadeclearthatplaylistsgenerated bythisapproximating\nmethodbetterre\u0003ectasetofconstraints thantheplaylists\ngenerated byconstraint satisfaction.\nACKNO WLEDGEMENTS\nWethankourcolleagues VincentBuil,GerardHollemans,\nFabioVignoli,andallparticipants inthetest.Refer ences\nAlghoniemy ,M.,andTew\u0002k,A.H.(2001).Anetwork\n\u0003owmodelforplaylistgeneration. Proc. ICME 2001\nJapan, Aug. 2001.\nAucouturier ,M.,andPachet,F.(2002).Scalingupmusic\nplaylistgeneration. In:Proceedings ICME 2002 -IEEE\nInternational Confer ence onMultimedia andExpo,26-\n29August2002,SwissFederalInstituteofTechnology ,\nLausanne, Switzerland.\nDavis,F.D.(1989).Perceivedusefulness, perceivedease\nofuse,anduseracceptance ofinformation technology .\nManagement Information Science Quarterly ,18,189-\n211.\nGuilford, J.P.(1954). Psyc hometric methods, Second edi-\ntion.NewYork:McGraw-Hill.\nPachet,F,Roy.P.,andCazaly,D.(2000).Acombinato-\nrialapproach tocontent-based musicselection, IEEE\nMultimedia, 7,1,March2000,44-51.\nTsang,E.(1993). Foundations ofconstr aintsatisfaction.\nAcademic Press,1993.\n643"
    },
    {
        "title": "Rhythm Classification Using Spectral Rhythm Patterns.",
        "author": [
            "Geoffroy Peeters"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417495",
        "url": "https://doi.org/10.5281/zenodo.1417495",
        "ee": "https://zenodo.org/records/1417495/files/Peeters05.pdf",
        "abstract": "In this paper, we study the use of spectral patterns to represent the characteristics of the rhythm of an audio signal. A function representing the position of onsets over time is first extracted from the audio signal. From this function we compute at each time a vector which represents the characteristics of the local rhythm. Three feature sets are studied for this vector. They are derived from the amplitude of the Discrete Fourier Transform, the AutoCorrelation Function and the product of the DFT and of a Frequency-Mapped ACF. The vectors are then sampled at some specific frequencies, which represents various ratios of the local tempo. The ability of the three feature sets to represent the rhythm characteristics of an audio item is evaluated through a classification task. We show that using such simple spectral representations allows obtaining results comparable to the state of the art. Keywords: rhythm representation, classification 1",
        "zenodo_id": 1417495,
        "dblp_key": "conf/ismir/Peeters05",
        "keywords": [
            "spectral patterns",
            "audio signal",
            "onsets",
            "vector representation",
            "Discrete Fourier Transform",
            "AutoCorrelation Function",
            "product of DFT",
            "sampling frequencies",
            "classification task",
            "rhythm characteristics"
        ],
        "content": "RHYTHM CLASSIFICATION USING SPECTRAL RHYTHM PATTERNS\nGeoffroy Peeters\nIRCAM - Sound Analysis/Synthesis Team\n1, pl. Igor Stravinsky\n75004 Paris - France\npeeters@ircam.fr\nABSTRACT\nIn this paper, we study the use of spectral patterns to rep-\nresent the characteristics of the rhythm of an audio sig-\nnal. A function representing the position of onsets over\ntime is ﬁrst extracted from the audio signal. From this\nfunction we compute at each time a vector which repre-\nsents the characteristics of the local rhythm. Three featur e\nsets are studied for this vector. They are derived from the\namplitude of the Discrete Fourier Transform, the Auto-\nCorrelation Function and the product of the DFT and of a\nFrequency-Mapped ACF. The vectors are then sampled at\nsome speciﬁc frequencies, which represents various ratios\nof the local tempo. The ability of the three feature sets\nto represent the rhythm characteristics of an audio item is\nevaluated through a classiﬁcation task. We show that us-\ning such simple spectral representations allows obtaining\nresults comparable to the state of the art.\nKeywords: rhythm representation, classiﬁcation\n1 INTRODUCTION\nAutomatic music description from signal analysis has be-\ncome one of the major research ﬁelds in the last decade.\nMusic description is often achieved by combining three\ndifferent points of view [1]: melody/harmony, timbre\n(which is related roughly to the orchestration of the mu-\nsic), and tempo/rhythm. This last point raises questions\nabout the representation of time into a compact and gener-\nalizable form that is suitable for task such as classiﬁcatio n,\nsearch by similarity or visualization.\nFor this representation, several proposals have been\nmade so far. The main differences between them are the\ntype of information being represented (representation of\nevent positions, of the acoustical characteristics of the\nevents or both) and the way they are represented (se-\nquence of events, histogram, proﬁles, evolution, ...). [2]\nproposes the use of a beat spectrum (obtained by sum-\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonming the signal similarity matrix along diagonals at spe-\nciﬁc lags) to visualize the temporal structure of a song\n(beat, measure and small structure). [1] proposes the use\nof a beat histogram obtained by collecting over time the\ncontribution of the dominant peaks of an enhanced auto-\ncorrelation. Various features are derived from this his-\ntogram and used, in combination with timbre and pitch\ncontent features, for music genre classiﬁcation. [3] pro-\nposes to model the rhythm characteristics as a sequence\nof audio features (loudness, spectral centroid, ...) along\ntime. A Dynamic Time Warping algorithm is then used\nto align time and allows the comparison of two sequences\nof different lengths. Gouyon’s work is also based on au-\ndio features. [4] tests a set of 73 features to character-\nize the rhythm. These include features derived from the\ntempo, from a periodicity histogram and from the Inter-\nOnset-Interval Histogram (IOIH). These features are used\nfor the classiﬁcation of 8 music genres from the “ballroom\ndancer” database. The authors report 90.1% correct recog-\nnition using the correct tempo (78.9% using the estimated\ntempo). Another study made by Gouyon [5] considers\ntempo estimation errors as part of the estimation process.\nThey use 28 pair-wise classiﬁers and obtain 67.6% correct\nrecognition. A recent study by Dixon [6] proposes to add\nto Gouyon set of features, a representation of the temporal\nrhythmic patterns derived from the energy evolution of the\nsignal inside each bar. This pattern represents the tempo-\nral position of the events. Various other features are also\nused (meter, syncopation, swing factor, ...). The perfor-\nmances are also tested on the “ballroom dancer” database.\nThe authors report 50% correct recognition using only the\npattern, and up to 96% using this pattern and all features\nwith an AdaBoost classiﬁer.\nIn this paper, we study the use of three simple spectral\npatterns to characterize the rhythm. The paper is orga-\nnized as follows. In part 2.1, we give a quick overview\nof our global tempo estimation system. In part 2.2., we\npropose the three spectral rhythm patterns. In part 3, we\ncompare the use of these representations in a task of music\ngenre classiﬁcation and compare our results with the state\nof the art.\n2 PROPOSED METHOD\nIn [7] we have proposed a system for the estimation of the\ntempo and beat positions of a piece of music. This system\nis the basis for our spectral rhythm representation.\n6442.1 Tempo estimation\nOnset estimation: An onset-energy function is ﬁrst ex-\ntracted from the audio signal. In order to allow a ro-\nbust detection of onsets even in case of music with non-\npercussive instruments we propose the use of a reassigned\nspectral energy ﬂux obtained from the reassigned spec-\ntrum [8].\nPeriodicities estimation: The onset-energy function is\nthen used to estimate the dominant periodicities at a given\ntime. This could be done using either Discrete Fourier\nTransform (DFT) or AutoCorrelation Function (ACF),\nbut we propose the use of a combination of DFT and\nFrequency-Mapped ACF. Why using both the DFT and\nthe ACF? The DFT of a periodic signal is a set of har-\nmonically related frequencies. Depending on their rela-\ntive amplitude it can be difﬁcult to decide which one of\nthe harmonics corresponds to the tempo frequency. This\nambiguity can lead to octave errors which are especially\ndetrimental in the case of triple or compound meter (in\nthese cases octave errors can lead to musically insignif-\nicant frequencies). The same occurs for the ACF but in\nthe time domain. Because the octave uncertainty of the\nDFT and ACF occur in inverse domain (frequency do-\nmain for the DFT, lag domain or inverse frequency do-\nmain for the ACF), we use this property to construct a\nproduct function that reduces these ambiguities. Calcu-\nlation: •At each frame ti, the DFT F(ωk,ti)and the\nACFA(l,ti)are computed on the same signal frame1.\n•The value at lag lof the ACF represents the amount\nof periodicity at the lag l/sr (where sris the sampling\nrate) or at the frequency ωl=sr/l ∀l > 0. Each lag\nlis therefore “mapped” in the frequency domain. •In\norder to get the same linearly spaced frequencies ωkas\nfor the DFT, we interpolate A(l,ti)and sample it at the\nlagsl=sr/ω k.•We now have two measures (the DFT\nand the FM-ACF) of periodicity at the same frequencies\nωk. We ﬁnally combined the functions by computing the\nproduct of the DFT and the FM-ACF at each frequency\nωk:Y(ωk,ti) =F(ωk,ti)·A(ωk,ti).\nIn Figure 1 we illustrate the interesting properties for\nrhythm characterization of this product function for two\nsignals at 120 bpm: - a simple meter in 4/4 (each beat is\ndivided into 8thnote), - a compound meter in 4/4 (each\nbeat is divided into 8thnote triplet). We represent the\nmean value over time of the DFT, the ACF and the prod-\nuct function2. The product function allows to better em-\nphasize the 4thnote / 8thnote frequencies for the simple\nmeter, the 4thnote / 8thnote triplet frequencies for the\ncompound meter therefore reducing octave ambiguities.\nSpurious peaks however exist due to the spectral leakage\nand the frequency resampling process of the ACF.\nTempo estimation: In [7], we have derived the most\nlikely tempo path over time ωbpm(ti)fromY(ωk,ti)us-\ning a Viterbi decoding algorithm. However in this pa-\nper, we are only interested in the discriminative power of\nY(ωk,ti)for rhythm characterization, apart from the pre-\ncision of the tempo estimation. Because of that, in the rest\nof the paper we will use a ground-truth tempo.\n1We use a window length of 6 s. and a hop size of 0.5 s. For\na simple meter in 4/4 at 120bpm, this allows the observation of\n3 measures of 4 beats with a good spectral resolutions.\n2The window length was set to 3 s. and the hop size to 0.5 s.0 100 200 300 400 500 60000.5144-4\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19DFT\n0 100 200 300 400 500 60000.51\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19FM-ACF\n0 100 200 300 400 500 60000.51\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19Pr o du ct\n0 100 200 300 400 500 60000.5144-3\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19DFT\n0 100 200 300 400 500 60000.51\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19FM-ACF\n0 100 200 300 400 500 60000.51\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19Pr o du ct\nFigure 1: Comparison of DFT, FM-ACF and product\nfunction for [top three] simple meter in 4/4 [bottom three]\ncompound meter in 4/4; x-axis: frequency, y-axis: ampli-\ntude, dotted lines denote the frequency of symbolic dura-\ntions (4: 4thnote, 8: 8thnote, 12: 8thnote triplet, ...)\n2.2 Spectral rhythm patterns\nRhythm can be roughly deﬁned by the tempo, the position\nand duration of the events and their acoustical characteris -\ntics. Instead of the position of the events, we usually pre-\nfer to work on the sequence of event’s duration (or the suc-\ncessive Inter-Onset-Intervals). In this paper, we are only\ninterested in the representation of the sequence of duratio n\nnot on the acoustical characteristics of the events. We seek\na representation of the rhythm that is •sensitive to the se-\nquence (order) of duration (but robust to small changes) •\nindependent of the tempo (i.e. the speed of reading of the\nsequence) •compact.\nSensitiveness to the sequence (order) of duration:\nAmong the representation mentioned in part 1, neither the\nIOI histogram, nor the beat histogram are sensitive to the\nsequence of duration but only to the relative frequency\nof the duration. This was noticed by [6]. The authors\ntake the example of a ChaChaCha pattern (which contains\nthe following successive events ♩♩♩/eigthnote/eigthnote ) and a Rumba pat-\ntern (♩/eigthnote♩♩/eigthnote ). They have different rhythm patterns but the\nsame distribution of IOI and therefore the same IOI his-\ntogram (IOIH). This is illustrated in the top part of Figure\n2 (♩=0.5 s. and /eigthnote=0.25 s.). However, the amplitude of\nthe DFT is sensitive to the sequence of duration through\nthe phase relations3. Since the ACF is the inverse of the\npower spectrum, it is also sensitive to the sequence of du-\nration, and therefore also the product DFT/FM-ACF. This\nis illustrated in the remaining part of Figure 2. While\nthe two IOIHs are identical, the ACFs, DFTs and prod-\nuct DFT/FM-ACF of both patterns differ. Considering\n3A simple mathematical demonstration of this can be made\nby representing the above-mentioned signals as the summation\nof pulses trains of period 4T(Tbeing the tempo period) with\nvarious time-shifts. Considering that these time-shifts ( ∆) intro-\nduce phase modiﬁcations in the complex spectrum ( e−jω∆), and\ntherefore inﬂuence the addition of the components in the com-\nplex domain (components in phase, in phase opposition, ...).\n6450 2 4 6 8012\nTime [s.]Signal\n0 1 2 3 4012\nTime [s.]IOI Histogram\n0 1 2 300.51\nLag [s.]ACF\n0 2 4 6 8 1002040\nFrequency (Hz)DFT\n0 2 4 6 8 1002040\nFrequency (Hz)DFT / FM−ACF0 2 4 6 8012\nTime [s.]Signal\n0 1 2 3 4012\nTime [s.]IOI Histogram\n0 1 2 300.51\nLag [s.]ACF\n0 2 4 6 8 1002040\nFrequency (Hz)DFT\n0 2 4 6 8 1002040\nFrequency (Hz)DFT / FM−ACF\nFigure 2: [Left column] ChaChaCha pattern and [right\ncolumn] Rumba pattern represented by [from top to bot-\ntom] Temporal pattern, IOI Histogram, ACF, DFT and\nproduct DFT/FM-ACF\nthat, we propose these three functions as candidates fea-\nture vectors for rhythm characterization.\nIndependence of the tempo: We note Y(ωk,ti)either\nthe DFT, the ACF or the product DFT/FM-ACF vector\nat the frequency ωkand time ti.•For each track, we\nextract the series of vectors Y(ωk,ti).•In order to make\nthe feature sets independent of the tempo, we normalize\nthe frequencies of Y(ωk,ti)by the local tempo frequency\nωbpm(ti):ω′\nk=ωk\nωbpm(ti).•We then compute the mean of\nY(ω′\nk,ti)over time ti.•Finally, the vector is normalized\nto unit sum. Each track is now represented by a single\nvector ¯Yn(ω′\nk).\nCompactness: We ony retain from ¯Yn(ω′\nk)a reduced\nset of normalized frequencies selected to correspond to\nmusically meaningfull frequency:1\n4,1\n3,1\n2,2\n3,3\n4,1,1.25,\n1.5,1.75,2,2.25,2.5,2.75,3,3.25,3.5,3.75,4. The\nlower components ( <1) represent measure subdivision\ncharacteristics. The upper components ( >1) represent\nbeat subdivision characteristics. The reduced vector is\nnotedZ(k)and is called a spectral rhythm pattern . It is a\npattern which represent the amount of energy at musically\nmeaningfull frequencies.\n3 MUSIC GENRE CLASSIFICATION\nIn this part we compare the use of the DFT, the ACF and\nthe product DFT/FM-ACF functions for the task of music\ngenre classiﬁcation.\nData: As in [5] and [6], we use the “ballroom dancer”\ndatabase [9] because this database contains music genres\nfor which there is a close link between the music genre\nand the rhythm genre. The “ballroom dancer” database\nis composed of 698 tracks, each of 30 sec long, repre-\nsenting the following music genre: ChaChaCha (111 in-\nstances), Jive (60), QuickStep (82), Rumba (98), Samba\n(86), Tango (86), Viennese Waltz (65), Waltz (110).\nFeatures: In the following we compare the three fea-\nture sets derived from the DFT, the ACF and the product\nDFT/FM-ACF functions. In each case, we consider the\nuse of each feature set alone and the use of it combined\nchachacha\n1 2 3 4 520\n40\n60\n80\n100\njive\n1 2 3 4 520\n40\n60\nquickstep\n1 2 3 4 520\n40\n60\n80\nrumba\n1 2 3 4 520\n40\n60\n80\nsamba\n1 2 3 4 520\n40\n60\n80\ntango\n1 2 3 4 520\n40\n60\n80\nviennesewaltz\n1 2 3 4 520\n40\n60\nwaltz\n1 2 3 4 520\n40\n60\n80\n100\nFigure 3: Spectral rhythm patterns ¯Yn(ω′\nk)(using product\nDFT/FM-ACF) for the various music genres of the “ball-\nroom dancer” database (x-axis: normalized frequencies,\ny-axis: item’s number on each category).\n50 100 150 200 25000.10.20.30.40.50.60.7\nTempoChaChaCha\nJive\nQuickStep\nRumba\nSamba\nTango\nVienneseWaltz\nWaltz\nFigure 4: Tempo distribution for the eight musical genres\nof the “ballroom-dancer” database.\nwith the tempo information. The tempo we consider here\nhas been manually entered for each track. We haven’t con-\nsidered the use of estimated tempo as [5] did.\nIn Figure 3, we represent the spectral rhythm pattern\n¯Yn(ω′\nk)in the case of the product DFT/FM-ACF for all\nthe songs belonging to each music genre category of the\n“ballroom dancer” database. Some characteristics of mu-\nsic genre appears immediately on this representation: Vi-\nenneseWaltz and Waltz are the only genres having a com-\nponent at ω′= 1/3(3/4 meter), but VienneseWaltz has no\n(a weak) component at ω′= 2(8thnote) while Walz has,\nSamba is the only genre having a component at ω′= 4\n(16thnote), Jive and QuickStep have no component at\nω′= 2. In Figure 4, we represent the tempo distribution\nfor the eight musical genres of the database.\nClassiﬁcation algorithm We study the ability of each\nof the three feature sets to correctly classiﬁed the audio\nitem of the “ballroom dancer” database into the 8 above-\nmentioned classes. Each audio item is represented by\na single 18-elements feature vector Z(k). We consider\nthree different classiﬁcation algorithms: 1) the widely\nused C4.5. decision tree algorithm, 2) the Partial Deci-\nsion Tree algorithm, 3) the Classiﬁcation using regression\nmethods. We have used the J48, PART, and Classiﬁcation-\nViaRegression implementations of Weka [10]. In order to\nallow to compare our results with the ones obtained by [4],\n646J48 PART ClassViaReg\nDFT 75,64 73,78 80,8\nACF 69,34 70,34 76,64\nDFT/FM-ACF 65,9 65,32 75,5\nDFT + tempo 90,4 88,96 90,4\nACF + tempo 86,67 86,67 90,25\nDFT/FM-ACF + tempo 86,38 86,24 90,25\ntempo 77,79 77,36 77,93\nFigure 5: Recognition rates obtained using various feature\nsets and classiﬁers (with and without tempo information)\nclassified as --> \nC\nJ Q\nR\nS T\nVW WChaChaCha 87,4% 4,5% 0,9% 7,2%\nJive 86,7% 1,7% 6,7% 5,0%\nQuickstep 1,2% 97,6% 1,2%\nRumba 2,0% 79,6% 2,0% 3,1% 13,3%\nSamba 1,2% 7,0% 89,5% 1,2% 1,2%\nTango 3,5% 1,2% 1,2% 94,2%\nViennese Waltz 3,1% 1,5% 95,4%\nWaltz 0,9% 4,5% 94,5%\nFigure 6: Confusion matrix using DFT + tempo feature\nset and a Classiﬁcation Via Regression algorithm\n[5] and [6], we evaluate the performances using a 10-fold\ncross validation method.\nResults: The recognition rates obtained using the three\nfeature sets (with and without tempo information) with the\nvarious classiﬁcation algorithms are indicated in Figure 5 .\nIn almost all cases, the best classiﬁers is the Classiﬁcatio n-\nViaRegression. The tempo alone achieves up to 78% cor-\nrect recognition. Without the tempo information, the DFT\nis the best feature sets (81%), then the ACF (77%) and the\nproduct DFT/FM-ACF (75.5%). With the tempo infor-\nmation, all feature sets have very close recognition rates;\nhowever the DFT set performs slightly better (90.4%). In\ncomparison, [4] report 90.1% recognition using a large set\nof features with the correct tempo (78.9% with the esti-\nmated tempo, 79.6% without the tempo), [6] report 50%\nusing only the temporal rhythmic pattern, and 96% us-\ning this pattern, the whole set of features of Gouyon and\nthe tempo. Note however that our representation does not\nuse any acoustical feature. The confusion matrix is in-\ndicated in Figure 6. The larger confusion occurs between\nChaChaCha/ Rumba/ Tango, Samba/ Rumba, Jive/ Tango/\nVienneseWaltz and Rumba/ Waltz. These larger confu-\nsions can be explained either by their close tempi (see Fig-\nure 4) or their close spectral rhythm patterns (see Figure\n3). In the opposite, in our study, the confusion between Vi-\nenneseWaltz and Waltz remains low. Best features: In or-\nder to better understand, the discriminative power of each\nelement kofZ(k), we have applied an automatic feature\nselection algorithm (the Correlation Feature Selection of\nWeka) The ﬁrst selected features are:1\n3,2\n3(importance of\nternary metrics), 1(importance of the 4thnote) 2(impor-\ntance of the 8thnote) 3(presence of 8thnote triplet), 3.75\n(?) and 4(importance of the 16th note). This corresponds\nto the intuition we get from Figure 3. Reducing Z(k)to\nthe 7 above-mentioned features, only slightly decreases\nthe results: from 80.8% to 75.5% without tempo informa-\ntion, and from 90.4% to 89.54% with tempo information\n(using the DFT feature type and a ClassiﬁcationViaRe-\ngression algorithm).4 CONCLUSION\nIn this paper, we have studied the use of three spectral\npatterns to represent the rhythm characteristics of an au-\ndio item. For a task of music genre classiﬁcation, we have\nshown that the use of these simple spectral patterns al-\nlows to achieve a high recognition rate (close to the results\nobtained with more complex methods proposed so far).\nAmong the three proposes spectral patterns, the use of a\npattern derived from the DFT allows to achieve the highest\nrecognition rate (90.4% with tempo, 81% without tempo).\nThis result is surprising considering we though that the\nproduct DFT/FM-ACF would allow to better differentiate\nthe various characteristics of rhythm. This is possibly due\nto the frequency mapping process of the FM-ACF, which\ndecreases the overall frequency resolution. Future works\nwill concentrate on evaluating the performances of this\nmethod when using the estimated tempo (instead of us-\ning the ground-truth tempo), and when applied to a larger\nset of music genre.\nACKNOWLEDGEMENTS\nPart of this work was conducted in the context of the Euro-\npean IST project Semantic HIFI (http://shf.ircam.fr) [11] .\nREFERENCES\n[1] G. Tzanetakis and P. Cook. Musical genre classiﬁ-\ncation of audio signals. IEEE Trans. on Speech and\nAudio Processing , 10(5):293–302, 2002.\n[2] J. Foote and S. Uchihashi. The beat spectrum: A new\napproach to rhythm analysis. In ICME . Pal Xerox\nFXPAL-PR-01-022, 2001.\n[3] J. Paulus and A. Klapuri. Measuring the similarity\nof rhythmic patterns. In ISMIR , Paris, France, 2002.\n[4] F. Gouyon, S. Dixon, E. Pampalk, and G. Widmer.\nEvaluating rhythmic descriptors for musical genre\nclassiﬁcation. In AES 25th Int. Conf. , 2004.\n[5] F. Gouyon and S. Dixon. Dance music classiﬁcation:\na tempo-based approach. In ISMIR , 2004.\n[6] S. Dixon, F. Gouyon, and G. Widmer. Towards char-\nacterisation of music via rhythmic patterns. In IS-\nMIR, Barcelona, Spain, 2004.\n[7] G. Peeters. Time variable tempo detection and beat\nmarking. In ICMC , Barcelona, Spain, 2005.\n[8] P. Flandrin. Time-Frequency/Time-Scale Analysis .\nAcademic Press, San Diego, California, 1999.\n[9] Ballroom-Dancers.com.\n[10] I. Witten and E. Frank. Data Mining: Practical\nMachine Learning Tools and Techniques with Java\nImpl. Morgan Kaufmann, San Fransisco, CA, 1999.\n[11] H. Vinet. The semantic hiﬁ project. In ICMC ,\nBarcelona, Spain, 2005.\n647"
    },
    {
        "title": "Classifier Combination for Capturing Musical Variation.",
        "author": [
            "Jeremy Pickens"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418219",
        "url": "https://doi.org/10.5281/zenodo.1418219",
        "ee": "https://zenodo.org/records/1418219/files/Pickens05.pdf",
        "abstract": "At its heart, music information retrieval is characterized by the need to find the similarity between pieces of music. However, “similar” does not mean “the same”. Therefore, techniques for approximate matching are crucial to the development of good music information retrieval systems. Yet as one increases the level of approximation, one finds not only additional similar, relevant music, but also a larger number of not-as-similar, non-relevant music. The purpose of this work is to show that if two different retrieval systems do approximate matching in different manners, and both give decent results, they can be combined to give results better than either system individually. One need not sacrifice accuracy for the sake of flexibility. Keywords: Classifier Combination, Approximate Matching 1",
        "zenodo_id": 1418219,
        "dblp_key": "conf/ismir/Pickens05",
        "keywords": [
            "music information retrieval",
            "finding similarity",
            "approximate matching",
            "retrieval systems",
            "classifier combination",
            "accuracy",
            "flexibility",
            "approximate matching",
            "decent results",
            "better than either system individually"
        ],
        "content": "CLASSIFIER COMBIN ATION FOR CAPTURING MUSICAL VARIA TION\nJeremy Pick ens\nDepartment ofComputer Science\nKing' sColle geLondon\u0003\nLondon WC2R 2LS, England\njeremy@dcs.kcl.ac.uk\nABSTRA CT\nAtitsheart, music information retrie valischaracterized\nbytheneed to\u0002ndthesimilarity between pieces ofmusic.\nHowever,similar does notmean the same. There-\nfore, techniques forapproximate matching arecrucial to\nthedevelopment ofgood music information retrie valsys-\ntems. Yetasoneincreases thelevelofapproximation, one\n\u0002nds notonly additional similar ,relevantmusic, butalsoa\nlargernumber ofnot-as-similar ,non-rele vantmusic. The\npurpose ofthisworkistoshowthatiftwodifferent re-\ntrievalsystems doapproximate matching indifferent man-\nners, andboth givedecent results, theycanbecombined\ntogiveresults better than either system individually .One\nneed notsacri\u0002ce accurac yforthesakeof\u0003exibility .\nKeywords: Classi\u0002er Combination, Approximate\nMatching\n1INTR ODUCTION\nItisawell-kno wnresult, due toworksuch asScha-\npire (1990) and Tumer and Ghosh (1999) that ifmul-\ntiple classi\u0002ers each givesresults better than random, one\ncanachie veresults better than each classi\u0002er individually\nbycombining their classi\u0002cation hypotheses. Inthispa-\nper,wefocus onrankedlistclassi\u0002ers, orclassi\u0002ers that\nmakesome sortofjudgement about howrelevantornon-\nrelevantapiece ofmusic istoaquery andthen rank by\nthisjudgement.\nInmusic information retrie val,wearelooking notfor\nexact matches, butforsimilarity .Asaresult, music in-\nformation retrie valsystems need tobeapproximate in\ntheir search formatches. However,asoneincreases the\nlevelofapproximation, onenotonly \u0002nds more relevant\nmusic pieces, butmore non-rele vantones aswell.\n\u0003This workissupported byEPSRC grant GR/N63123/01\nPermission tomakedigital orhard copies ofallorpart of\nthisworkforpersonal orclassroom useisgranted without fee\nprovided that copies arenotmade ordistrib uted forpro\u0002t or\ncommercial advantage andthatcopies bear thisnotice andthe\nfullcitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonThere aretwomain approaches toapproximate match-\ning. One candoexact matching onfuzzy data, orfuzzy\nmatching onexact data Wiggins (2005). Wepresent two\nfuzzy music retrie valsystems: Mark ovRandom Fields\nmodels andHarmonic models. Each ofthese systems does\nitsapproximation inaslightly different manner ,\u0002nding\nmanynon-rele vantpieces alongside therelevantones. We\nshowthatbycombining theresults givenbyeach system,\nwecanimpro veupon theresults available through either\nsystem.\nAsaresult ofthiscombination, weshowthatoneneed\nnotsacri\u0002ce precision toobtain better recall. Thus, one\ncancon\u0002dently buildnewsystems thataremore \u0003exible\nintheir approximations, knowing that through classi\u0002er\ncombination thevariations oneisseeking canbesuccess-\nfully captured.\n2MUSIC REPRESENT ATION\nForthese experiments, weusea12-pitch class, octave-\ninvariant, event-based symbolic representation. Forex-\nample:\nThis isapolyphonic sequence ofnotes, with time\nalong thex-axis andpitch along they-axis. Allthenotes\nthatstart atthesame time arearranged intothesame ver-\ntical slice. Forthepurpose ofthispaper ,durations ofnotes\naswell astime between notes, isignored.\n3MARK OVRANDOM FIELD MODEL\nAMark ovRandom Field isamodel thatwillallowusto\npredict thevalue ofacurrent noteni;tfrom thevalues\nofthesurrounding variables (notes). Inother words, itis\nanestimated probability distrib utionP(ni;tjHi;t),where\nHi;tisthesetofallvariables within some \u0002xeddistance\nprevious totimet,ornotes thatoccur attimet,buthave\nanindexlowerthani.\nIngeneral, arandom \u0002eld frame workallowsarbitrary\ndependencies (orfeatur es)between thetargetni;tandits\nneighborhood Hi;t.Inthiswork, wedeliberately restrict\nallowed dependencies tobinary questions oftheform:\nwas notejplayed attimesbefor et?.Wealso allow\n648Figure 1:Examples ofmusical features thatmay bein-\nduced topredict theprobability ofnote 2being played\nattimet.Black circles represent notes that arepart of\nthefeature function. Boxedblack circle denotes thenote\nn2;t.Boxedarea represents thehistory H2;t.From leftto\nright, thefeatures are:fn2;tn1;tg,fn2;tn2;t\u00001n2;t\u00002g,\nfn2;tn1;tn3;t\u00001n3;t\u00002g,fn2;tn0;tn2;t\u00002n0;t\u00002g\ngeneralizations where aquestion isaskedabout some sub-\nsetSofthenotes intheallowed history Hi;t.Theanswer\ntoaquestion ofthisform willbecalled thefeature func-\ntionfSandSwillbereferred toasthesupport off.Fora\ngivensupport S2Hi;t,thefeature function fSisde\u0002ned\nastheconjunction ofanswers about theindividual notes in\nnj;s2S:\nfS(ni;t;Hi;t)=ni;tY\nnj;s2Snj;s (1)\nDe\u0002ned inthismanner ,ourfeature functions areal-\nwaysboolean, andequal to1ifallthenotes de\u0002ned byS\nwere played before thetargetnoteni;t.Features arealso\ntime-in variant (does notmatter inwhich onset time they\noccur) butarenotindexinvariant (arenottransposable up\nordownanysemitones). Asanillustration, Figure 1con-\ntains some examples offeatures thatcould haveanimpact\nonnote 2attimet.\nThe parametric form that characterizes therandom\n\u0002eld model isamember oftheexponential (orlog-linear)\nfamily ,expressed as:\n^P(ni;tjHi;t)=1\nZi;texp8\n<\n:X\nf2F\u0015ff(ni;t;Hi;t)9\n=\n;(2)\nInequation (2),thesetofscalars \u0003=f\u0015f:f2Fg\nisthesetofLagrange multipliers forthesetofstructural\nconstraints F.Intuiti vely,theparameter \u0015fensures that\nourmodel predicts feature fasoften asitshould occur\ninreality .Zi;tisthenormalization constant thatensures\nthatourdistrib ution sums tounity overallpossible values\nofni;t.Instatistical physics, itisknownasapartition\nfunction andisde\u0002ned asfollows:\nZi;t=X\nnexp8\n<\n:X\nf2F\u0015ff(n;Hi;t)9\n=\n;(3)\nTheexactmanner inwhich thefeatures f2F,scalars\n\u00152\u0003andZi;tarechosen isbeyond thescope ofthis\nposter ,butiscovered inPickensandIliopoulos (2005).\nHowever,thebasic idea isthatthere isatargetempirical\ndistrib ution ~P(njH)givenbyapiece ofmusic, andthe\ngoal isto\u0002tthemodel ^P(njH)tothistarget.\nMusic retrie valusing these models isdone byestim-\nating amodel using agivenquery asatargetdistrib ution,andthen observing howwell thatquery model predicts the\nnotes ineach document inthecollection.\nX\nH~PD(H)X\nn~PD(njH)log^PQ(njH) (4)\nInother words, oursimilarity measure istheex-\npected cross-entr opybetween theempirical distrib ution\n~PD(njH)ofthedocument andtheestimate ^PQ(njH)pro-\nduced bythequery model, asgivenbyequation 4.\n4HARMONIC MODEL\nWhile therandom \u0002eld models operate onnote conjunc-\ntion features, theharmonic models developed byPick-\nensand Crawford (2002) workbymapping each 12-\ndimensional note onset vectorsonto a24-dimensional\nchord vector ofthe12major and12minor triads. This\nadhocmapping takesinto account notonly thesize of\ntheoverlap between thenote andthechord, butalso the\ntotal number ofnotes inthesimultaneity ,andtheKrum-\nhansl andShepard (1979) perceptual distance between the\nchords inthelexicon:\nContext (s;c)=js\\cj\njsjX\nc02lexiconjs\\c0j\n(jsj\u0003Krum(c0;c))+1\n(5)\nThis conte xtscore iscomputed foreverychord cin\nthelexicon. Additionally ,inter-vector smoothing isper-\nformed, whereby neighboring vectors areallowed tocon-\ntributetothepartial observ ation ofthecurrent vector .A\nvector ofpartial observ ations isthen obtained bynormal-\nizing bythesum total:\nPartialObs(s;c)=Context (s;c)P\nc02lexicon Context (s;c0)(6)\nThis vector ofpartial observ ations overthechord lex-\nicon isthen used astherawfeature setformodel estim-\nation. Forexample, suppose wehavealexicon ofthree\nchords, P,Q,andR.Asequence ofpartial observ ation\nvectors might appear asfollows:\nPartial observ ation vectors\nChord 1 2 3 4 5\nP 0.2 0.1 0.7 0.5 0\nQ 0.5 0.1 0.1 0.5 0.1\nR 0.3 0.8 0.2 0 0.9\nTotal 1.0 1.0 1.0 1.0 1.0\nWesimply count thenumber oflength msequences\nthrough apiece ofmusic, each count weighted bythefrac-\ntional observ ation amount. Continuing ourexample, sup-\nposem=2.Webeginwith thewindo wfrom timestep 1\ntotimestep 2.Thesequence P;Pisobserv edinpropor -\ntion totheamount inwhich weobserv ePattimestep 1\nandalso observ ePattimestep 2(0.2 *0.1=0.02). The\nsequence Q;Risobserv edinproportion totheamount in\nwhich weobserv eQattimestep 1andthen Rattimestep 2\n(0.5*0.8=0.4), andsoon.\nWenextdivide these chains intotwoparts, theprevi-\nousstate, orhistory ,andthecurrent state. Wede\u0002ne\n649\n649thehistory Hasthe\u0002rstm-1chords inthesequence, and\nthecurrent statecasthe\u0002nal chord inthesequence. For\nexample, with anm=2chain P;Q,thehistory isthe\nstate Pandthecurrent state isQ. Withanm=3chain\nP;Q;P,thehistory isthestate P;Qandthecur-\nrentstate isP\nWeobtain parameters fortheconditional probability\ndistrib ution ^P(cjH)bydoing maximum likelihood estim-\nation using thecomplete setofextracted chains, where\njH;cjisthenumber oftimes thesequence with history H\nfollowed bychord cisobserv ed.\n^P(cjH)=jH;cjP\nHijHi;cj(7)\nPrior toretrie val,atindexing time, weestimate\n^P(cjH)foreverypiece ofmusic inthecollection. Atre-\ntrievaltime, when presented with aquery ,weestimate a\nmodel forthequery intheexact same manner .Similarity\niscalculated between thequery model andeverydocu-\nment model inthecollection using theKullback-Leibler\n(KL) divergence measure, alsoknownasrelati veentrop y.\nPieces arerankedbyincreasing dissimilarity tothequery .\n5CLASSIFIER COMBIN ATION\nOurapproach isamong thesimplest possible. Combining\ntheactual similarity score givenbyeach system isdif\u0002-\ncult, because thisscore means something completely dif-\nferent depending onthesystem. Instead, wecombine the\nranks thateach system gives,bygiving each piece ofmu-\nsicanewscore equal totheaverage ofthetworanks given\nbyeach system. Pieces arethen rerank edbythisaverage\nscore. Ties,ifany,arebrokenbyrandomly ordering all\npieces with thatsame score.\nIfboth systems rank apiece highly ,itwillcontinue to\nberankedhighly inthecombined ranking. Ifonesystem\ngivesahigh rank andanother alowrank, itwillnotfareas\nwell. Theintuition isthat, because thetwosystems differ\nintheir approximation methods, non-rele vantpieces that\nhappen tobegivenahigh rank under thepeculiarities of\nonesystem will notfareaswell under theother .Pieces\nwhich truly arerelevantwill recei vedecent, though not\nperfect, rankings under both methods, andwilltherefore\npercolate tothetop,inthecombination.\n6EXPERIMENTS\nForourproject, wehavefour collections. The \u0002rst isa\nsetofapproximately 3000 polyphonic music pieces from\ntheCCARH atStanford. These aremostly baroque and\nclassical pieces from Bach, Beetho ven,Corelli, Handel,\nHaydn, Mozart andVivaldi. Ourremaining three setsof\nmusic, ontheother hand, arepieces which were inten-\ntionally composed asvariations onsome theme. These\nare26variations onthetune knowntoEnglish speak-\nersas`Twinkle, twinkle, little star', 75versions ofJohn\nDowland' s`Lachrimae Pavan'from different 16th and\n17th-century sources, and50variations byfour different\ncomposers onthewell-kno wnbaroque tune `Les Folies\nd'Espagne'.Forretrie val,weselect apiece from thethree setsof\nvariations andusethatasthequery .Allother pieces from\nthatsame variation setarejudged relevanttothequery ,\nandtherestofthecollection isjudged non-rele vant.This\nprocess isrepeated forallpieces inallthree setsofvari-\nations, foratotal of151queries.\nWede\u0002ne \u0002MRFastheretrie valsystem based onran-\ndom \u0002elds. \u0002HARM=2denotes aretrie valsystem based\nonharmonic models with thechord sequence length setto\n2(and asmall smoothing windo w),while\u0002HARM=3isa\nlength 3chord sequence andalargersmoothing windo w.\nFinally ,\u0002MERGE=2and\u0002MERGE=3denote thecom-\nbined rankedlistsof\u0002MRFand\u0002HARM,with thechain\nsetto2or3,respecti vely.Figure 2showstheresults. Per-\ncentage impro vements aregivenforthe\u0002MERGElists\novereach oftheother twolists. Anasterisk indicates stat-\nistical signi\u0002cance (t-test ata0.05 level).\n7ANALYSIS\nThe \u0002rst thing wenote isthattheclassi\u0002er combination\nworks. Forboth chain lengths, thecombination yields\nanywhere from a20% toa40% ormore average impro ve-\nment overeither theharmonic model ortherandom \u0002eld\nmodel alone, andtheimpro vement isstatistically signi\u0002c-\nantateverylevelofrecall. Totalnumber ofrelevantdoc-\numents inthetop1000 decreases slightly (\u00195%) when\ncompared against\u0002MRF,butitstillan\u001920% impro ve-\nment overthe\u0002HARMmodels.\nButwhydoes itwork? MRF model estimation in-\nvolvesadding thousands offeatures tothemodel, each\nofwhich pinpoints some exact subset ofnotes andgivesa\nweight totherelati veimportance ofthatsubset. Theidea\nisthatifavariation hasafewnotes missing, hundreds of\nthese feature functions will nolonger beactivated. Yet\nthousands more features willstillbeactivated, along with\ntheir weights, andthus theoverall prediction accurac yof\nthecurrent note will stillbegood. However,thesystem\nstillmakesmistak esinthatspurious occurances ofnotes\nsometimes activatehighly-weighted feature functions, and\nthus pullinnon-rele vantpieces.\nHarmonic models, ontheother hand, smear outnote\nobserv ations into chords. Relevantvariations which do\nnotusetheexact same notes might usenotes from theex-\nactsame chord. Byusing triads asfeatures, rather than\nthenotes themselv es,onecan\u0002ndthese variations. How-\never,thesystem stillmakesmistak esinthatthere arenon-\nrelevantpieces thatareharmonically similar ,without ac-\ntually being atrue variation onthequery .\nEach ofthese systems makesits relevance-\nclassi\u0002cation decision using adifferent technique.\nBoth doagood jobof\u0002nding relevantvariations, but\nalso makemistak es.However,theymakemistak esfor\ndifferent reasons. Ifthemistak eswere asconsistent as\nthesuccesses, i.e.ifthesame non-rele vantpieces were\nalwaysrankedhighly byboth systems, wewould not\nexpect toseetheconsistent impro vement inprecision\noverboth systems atalllevelsofrecall thatthemerged\nlistprovides.\n650 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n 0  0.2  0.4  0.6  0.8  1Precision\nRecall11-pt Recall/Precision graph\nMRF\nHARM=2\nMERGE=2\n 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n 0  0.2  0.4  0.6  0.8  1Precision\nRecall11-pt Recall/Precision graph\nMRF\nHARM=3\nMERGE=3\n\u0002MRF\u0002HARM=2\u0002HARM=3 \u0002MERGE=2 \u0002MERGE=3\n%Change %Change %Change %Change\n(\u0002MRF) (\u0002HARM=2) (\u0002MRF) (\u0002HARM=3)\nRetrie ved: 151000 151000 151000 151000 151000\nRelevant: 8801 8801 8801 8801 8801\nReljret: 7239 5818 5650 6884 -4.90* +18.32* 6832 -5.62* +20.92*\nInterpolated Recall -Precision\nat0.00 0.9781 1.0000 1.0000 0.9967 +1.9* -0.3 1.0000 +2.2* 0.0\nat0.10 0.4990 0.6166 0.6484 0.6818 +36.6* +10.6* 0.7215 +44.6* +11.3*\nat0.20 0.3488 0.4352 0.4638 0.5078 +45.6* +16.7* 0.5610 +60.8* +20.9*\nat0.30 0.2597 0.3363 0.3553 0.4015 +54.6* +19.4* 0.4387 +69.0* +23.5*\nat0.40 0.1923 0.2803 0.2945 0.3344 +73.9* +19.3* 0.3589 +86.6* +21.9*\nat0.50 0.1510 0.1398 0.1226 0.2072 +37.3* +48.2* 0.2109 +39.7* +72.0*\nat0.60 0.1174 0.1073 0.0861 0.1555 +32.4* +45.0* 0.1531 +30.3* +77.8*\nat0.70 0.0857 0.0730 0.0523 0.1140 +33.0* +56.1* 0.1116 +30.2* +113.4*\nat0.80 0.0575 0.0480 0.0362 0.0763 +32.6* +58.8* 0.0698 +21.5 +92.7*\nat0.90 0.0210 0.0328 0.0239 0.0445 +111.6* +35.8* 0.0397 +88.7* +65.8*\nat1.00 0.0029 0.0106 0.0053 0.0147 +401.5* +37.8* 0.0130 +343.4* +146.6*\nAverage precision (non-interpolated) overallreldocs\n0.2054 0.2360 0.2476 0.2859 +39.22* +21.7* 0.3061 +49.05* +23.61*\nFigure 2:Recall-Precision results. Inthe\u0002MERGEcolumns, results aregivenforthecombination ofthe\u0002MRFand\ncorresponding-sized \u0002HARMmodel, followed bythepercentage impro vement overboth individual systems.\n8CONCLUSION\nWehaveshowninthispaper thattwogood polyphonic\nretrie valsystems canbecombined tobecome evenbetter .\nThough thesystems evaluated were forsymbolic data, this\nhasserious implications formuch ofmusic retrie val.Mu-\nsicretrie valsystems typically donotworkby\u0002nding exact\nmatches, nomatter ifthedata issymbolic oraudio. Music\nis\u0003uid andchanging, andthere arenohard rules about\nwhat constitutes avariation. Some degree ofapproxim-\nation isalwaysgoing tobeneeded.\nInthefuture wehope totestanassortment ofother\nboosting andclassi\u0002er combination techniques, notjust\nrankedlistaveraging. Wealso hope totestanumber of\nother matching systems beyond random \u0002elds andhar-\nmonic models. However,thepoint ofthisshort paper is\nnotthespeci\u0002cs ofanyoneboosting technique, oreven\nanyoneretrie valtechnique. Itissimply toshowthatwe\nneed notbuild all-encompassing retrie valsystems todo\napproximate matching orvariation \u0002nding. Bybuilding a\nhost ofsystems, each ofwhich tackles thevariation prob-\nlemfrom aslightly different angle, andthen combining\nthem, future music retrie valsystems will beable togain\nthe\u0003exibility of\u0002nding more pieces without sacri\u0002cing\naccurac y,andthus better capture musical variation.REFERENCES\nC.L.Krumhansl andR.N.Shepard. Quanti\u0002cation ofthe\nheirarch yoftonal functions within adiatonic conte xt.\nJournal ofExperimental Psychology:Human Percep-\ntionandPerformance ,5:579594, 1979.\nJ.PickensandT.Crawford. Harmonic models forpoly-\nphonic music retrie val. InProceedings oftheACM\nConfer ence inInformation Knowledg eand Mana ge-\nment (CIKM) ,McLean, Virginia, November 2002.\nJ.PickensandC.Iliopoulos. Mark ovrandom \u0002elds and\nmaximum entrop yformusic information retrie val.In\n6thAnnual International Confer ence onMusic Inform-\nation Retrie val(ISMIR) ,September 2005.\nR.E.Schapire. The strength ofweak learnability .Ma-\nchine Learning ,5(2):197227, 1990.\nK.Tumer andJ.Ghosh. Linear andorder statistics com-\nbiners forpattern classi\u0002cation. InA.Shark ey,ed-\nitor,Combining Arti\u0002cial Neur alNets,pages 127162.\nSpringer Verlag, 1999.\nG.Wiggins. Personal conversation, London SE14, Janu-\nary,2005.\n651\n651"
    },
    {
        "title": "Markov Random Fields and Maximum Entropy Modeling for Music Information Retrieval.",
        "author": [
            "Jeremy Pickens",
            "Costas S. Iliopoulos"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414716",
        "url": "https://doi.org/10.5281/zenodo.1414716",
        "ee": "https://zenodo.org/records/1414716/files/PickensI05.pdf",
        "abstract": "Music information retrieval is characterized by a number of various user information needs. Systems are being developed that allow searchers to find melodies, rhythms, genres, and singers or artists, to name but a few. At the heart of all these systems is the need to find models or measures that answer the question “how similar are two given pieces of music”. However, similarity has a variety of meanings depending on the nature of the system being developed. More importantly, the features extracted from a music source are often either single-dimensional (i.e.: only pitch, or only rhythm, or only timbre) or else assumed to be orthogonal. In this paper we present a framework for developing systems which combine a wide variety of non-independent features without having to make the independence assumption. As evidence of effectiveness, we evaluate the system on the polyphonic theme similarity task over symbolic data. Nevertheless, we emphasize that the framework is general, and can handle a range of music information retrieval tasks. Keywords: Random Fields, Music Modeling 1",
        "zenodo_id": 1414716,
        "dblp_key": "conf/ismir/PickensI05",
        "keywords": [
            "Music information retrieval",
            "Markov random fields",
            "Maximum entropy modeling",
            "Similarity measurement",
            "Polyphonic theme similarity",
            "Non-independent features",
            "Feature combination",
            "Music modeling",
            "Rhythms",
            "Genres",
            "Pitch",
            "Timbre",
            "Symbolic data"
        ],
        "content": "MARK OVRANDOM FIELDS AND MAXIMUM ENTR OPY MODELING\nFOR MUSIC INFORMA TION RETRIEV AL\nJeremy Pick ensandCostas Iliopoulos\nDepartment ofComputer Science\nKing' sColle geLondon\u0003\nLondon WC2R 2LS, England\njeremy,csi@dcs.kcl.ac.uk\nABSTRA CT\nMusic information retrie valischaracterized byanumber\nofvarious user information needs. Systems arebeing de-\nveloped that allowsearchers to\u0002nd melodies, rhythms,\ngenres, andsingers orartists, toname butafew.Atthe\nheart ofallthese systems istheneed to\u0002nd models or\nmeasures thatanswer thequestion howsimilar aretwo\ngivenpieces ofmusic. However,similarity hasavariety\nofmeanings depending onthenature ofthesystem be-\ningdeveloped. More importantly ,thefeatures extracted\nfrom amusic source areoften either single-dimensional\n(i.e.: only pitch, oronly rhythm, oronly timbre) orelseas-\nsumed tobeorthogonal. Inthispaper wepresent aframe-\nworkfordeveloping systems which combine awide vari-\netyofnon-independent features without having tomake\ntheindependence assumption. Asevidence ofeffective-\nness, weevaluate thesystem onthepolyphonic theme\nsimilarity task oversymbolic data. Nevertheless, weem-\nphasize thattheframe workisgeneral, andcanhandle a\nrange ofmusic information retrie valtasks.\nKeywords: Random Fields, Music Modeling\n1INTR ODUCTION\nRecent interest inthearea ofmusic information retrie val\nandrelated technologies isexploding. Digital music col-\nlections arebecoming available locally (computer hard\ndisks, mp3 players) andremotely (online music stores, di-\ngitial libraries). However,fewoftheexisting techniques\ntakeadvantage ofrecent developments instatistical mod-\neling. Inthispaper wediscuss anapplication ofMark ov\nRandom Fields totheproblem ofcreating accurate yet\n\u0003exible statistical models ofmusic. Withsuch models\ninhand thechallenges ofdeveloping effectivesearch-\n\u0003This workissupported byEPSRC grant GR/N63123/01\nPermission tomakedigital orhard copies ofallorpart of\nthisworkforpersonal orclassroom useisgranted without fee\nprovided that copies arenotmade ordistrib uted forpro\u0002t or\ncommercial advantage andthatcopies bear thisnotice andthe\nfullcitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondoning,browsing andorganization techniques forthegrowing\nbodies ofmusic collections may besuccessfully met.\nRandom Fields areageneralization ofMark ovchains\ntomulti-dimensional spatial processes. Theyareincred-\nibly \u0003exible, allowing ustomodel arbitrary interactions\nbetween variables. Theyhavebeen verypopular inmod-\neling ofphysical systems, andrecently havedemonstrated\nsuperior performance inanumber oflanguage-related ap-\nplications such asBergeretal.(1996), Della Pietra etal.\n(1997) andLafferty etal.(2001). Finally ,andperhaps\nmost importantly ,random \u0002elds areextremely attracti ve\nfrom atheoretical standpoint. Probability distrib utions\noverthe\u0002elds haveexponential form, consistent with the\nmaximum-entrop yframe workforinference. Theobject-\nivefunction isglobally\\-convexwith respect toparamet-\nersofthemodel, andsoparameters canbelearned effect-\nivelythrough iterati vemethods. Furthermore, there exists\naprincipled wayofinducing thestructure ofthe\u0002eld that\nguarantees impro vement intheobjecti vefunction, andin\nsome cases allowsclosed-form solutions.\n2MUSIC REPRESENT ATION\nMusic hasseveralpossible representations. Initsmost un-\nstructured form, music canberepresented asasequence of\naudio signal samples, asforexample inawavormp3 \u0002le.\nOntheother endofthespectrum, music may berepres-\nented asinstructions toaperformer ,asinsheet music. In\nthemiddle arerepresentations such asMIDI, inwhich the\nonset, duration, andpitch ofeverynote inapiece ofmu-\nsicisknownbutnoother information isnecessarily avail-\nable. Nottobeoverlook ed,music may alsoberepresented\nusing metadata, such assong title, artist, genre, year,in-\nstrumentation andsoon.Fortheevaluation ofourcurrent\nmodel, wewillfocus onsymbolic pitch information. But\nweemphasize thatthemodel ingeneral canaccomodate\nfeatures from anyrepresentation, including both content-\nandmetadata-related features, simultaneously ,aslong as\nthefeature istheanswer toabinary question. This willbe\ncovered inmore detail attheendofthepaper .\nInpolyphonic music, wemay think ofthenotes as\natwo-dimensional graph, with ticks (time) along thex-\naxis, andMIDI note number (1to128) along they-axis.\nAtanypoint along they-axis, notes turn on, remain on\nforaparticular duration, andthen turn back offagain.\nAsaquick example, seethe\u0002gures below.Black circles\n207represent notes being on. White circles represent notes\nbeing off.\nFigure 1:Example music sequence, preprocessed\nForourcurrent purposes (though notnecesssary in\ngeneral) wemakethefollowing simpli\u0002cations. Webegin\nbyselecting only theonset times ofeach newpitch inthe\nsequence, ignoring theduration ofthenote. Next,were-\nmoveallonset times which contain nopitches. (Thus, we\narethrowing awaynotonly theduration ofthenotes them-\nselves,buttheduration between notes, including rests) Fi-\nnally ,wereduce the128-note y-axis toa12-note octave-\ninvariant pitch setbytaking themod-12 value ofevery\nsymbolic pitch number .Ourexample abovebecomes:\nFigure 2:Example music sequence, postprocessed\n3MARK OVCHAIN RETRIEV AL\nMODEL\nIntheprevious section weshowed howpolyphonic mu-\nsiccanberepresented asatemporal progression of12-\ndimensional binary vectors. Inthissection weintroduce\nasabaseline approach asimple modeling andretrie val\nsystem based onextracting multiple, sometimes overlap-\nping Mark ovchains from thissequence.\nAMark ovchain model proceeds inthree stages. In\nthe\u0002rststage, weextract thechains (orfeatur es).Inthe\nsecond stage, weusetherelati vefrequenc ycounts ofthese\nextracted chains toestimate astate transition distrib ution.\nInthe\u0002nal stage, wecompare models estimated from both\namusic query andacollection document andrank collec-\ntiondocuments bythissimilarity score.\nForfeature extraction, wesimply count thenumber of\nlength msequences through apiece ofmusic. This ap-\nproach isbroadly similar tothatofDoraisamy andR¨uger\n(2001). Forexample, ifweextract chains oflength m=2\nfrom themusic inFigure 2,weendupwith thefollowing\nsetofsequences:f0;2,2;1,1;0,1;2,0;1,2;1g.\nInthesecond stage, wedivide these chains into two\nparts, theprevious state, orhistory ,andthecurrent\nstate. Wede\u0002ne thehistory Hasthe\u0002rstm-1notes in\nthesequence, andthecurrent statenasthelastnote inthe\nsequence. Forexample, with anm=2chain 1;2,the\nhistory isthestate 1andthecurrent state is2. With\nanm=3chain 1;2;1,thehistory isthestate 1;2\nandthecurrent state is1\nWenextobtain parameters fortheconditional probab-\nility distrib ution ^P(njH)bydoing maximum likelihood\nestimation onthecomplete setofextracted chains, wherejH;njisthenumber oftimes thechain with history H\nfollowed bynotenisobserv ed.\n^P(njH)=jH;njP\nHijHi;nj(1)\nBasic Dirichlet smoothing isused toovercome the\nzero frequenc yproblem (Zhai and Lafferty). Figure 3\nshowsan(unsmoothed) example forasample 3-state\nmodel trained with thedata from Figure 2.\nCounts\n012\n0 011\n1 101\n2 020^P(njH)\n0 1 2\n0 0 0.5 0.5\n1 0.5 0 0.5\n2 0 1.0 0\nFigure 3:Example 1st-order (m=2)state transition\ncounts (left)andstate transition distrib ution (right )\nPrior toretrie val,atindexing time, weestimate\n^P(njH)foreverypiece ofmusic inthecollection. At\nretrie valtime, when presented with aquery ,weestimate\namodel forthequery intheexact same manner .Similar -\nityiscalculated between thequery andeverydocument in\nthecollection using theKullback-Leibler (KL) divergence\nmeasure. Documents arerankedbyincreasing dissimilar -\nitytothequery .\n4MARK OVRANDOM FIELD\nRETRIEV ALMODEL\nIntheprevious section, wemodeled polyphonic music as\nasequence ofMark ovchains across neighboring 12-bit\nbinary note vectors. However,there isonemain prob-\nlemwith thismethod: Since themusic ispolyphonic, the\nextracted Mark ovchain features overlap. This overlap\ncauses themodel tocount certain notes more than once,\nwhich hastheeffectofoverestimating certain transitions.\nUsing Mark ovchains makestheassumption thatnotes oc-\ncuring atthesame point intime areindependent ofeach\nother; thisisclearly nottrue. Therefore, weturn toa\nframe workthatallowsustoselecti velymodel theinter-\nactions (dependencies) between theindividual notes.\n4.1 Natur eoftheField\nSuppose wearegivenamusical piece represented byT\nsimultaneities (binary vectors oflength 12). Withthis\npiece wewillassociate alattice ofbinary variablesfni;tg,\nindexedbythetimet=1:::T,and bynote index\ni=0:::11.Each variablefni;tgcanbeeither 0or1,\nindicating whether anoteiisonoroffattimet.Fig-\nure4illustrates thelattice. Ourgoal istodevelop amodel\nthatwillallowustopredict thevalueni;tfrom thevalues\nofthesurrounding variables. Inother words, wewould\nliketodevelop anestimate fortheprobability distrib ution\nP(ni;tjfnj;s:j6=iors6=tg).Itisimportant tostress that\nwedonotwanttoassume independence among thecondi-\ntioning variables, orrestrict theconditioning totheimme-\ndiate neighbors ofni;t.Onthecontrary ,webelie vethat\n208n0;1n0;2n0;3n0;4:::n0;t:::\nn1;1n1;2n1;3n1;4:::n1;t:::\nn2;1n2;2n2;3n2;4:::n2;t:::\n:::\nn11;1n11;2n11;3n11;4:::n11;t:::\nFigure 4:Variables ofaMusical Mark ovRandom Field\nthevalue ofni;tisstrongly in\u0003uenced byboth itsshort-\nrange andlong-range neighbors inthelattice. However,\nforthescope ofthispaper wewillimpose severallimita-\ntions onwhat kind ofdependencies may existinour\u0002eld.\nThe\u0002rstlimitation weimpose concerns thetemporal\nnature ofmusic. Inthemost general formulation ofaran-\ndom \u0002eld, thevalue ofthenoteni;tmay bein\u0003uenced by\nboth thenotes thatprecede itfnj;s\u0014tg,andnotes thatfol-\nlowitfnj;s>tg.Forcomparison with thechain model, in\nourinitial random \u0002eld model wewillrestrict thedepend-\nencies toonly those notes thatprecede thetargetnote in\nthesequence. Foreverynoteni;twede\u0002ne theconcept\nofhistory orneighborhood Hi;ttoinclude thenotes that\neither occur before timet,ornotes thatoccur attimet,but\nhaveanindexlowerthani:\nHi;t=fnj;s:s<tg[fnj;s:s=t;j<ig (2)\nNotes inHi;taretheones thatcanbeexamined when\nwearemaking theprediction regarding ni;t.Inother\nwords, weassume thattheprobability ofnoteiplaying at\ntimetiscompletely determined byHi;tinourmodel. We\nreiterate thatwestillallowarbitrary dependencies within\nthesubset de\u0002ned bytheneighborhood Hi;t.\nIngeneral, weneed notconstrain ourneighborhood\ninthismanner .Webelie vethatbetter models could be\ncreated byconsidering notes that will beplayed inthe\nfuture, notjustinthepast. However,wewish todo\ntwothings with thispaper: (1)introduce random \u0002elds\nasamodeling frame worksuitable formusic information\nretrie val,and(2)evaluate thisframe workbycomparing\nitwith asimilar model (Mark ovchains). Mark ovchains\nonly consider thepast. Ifwewere tointroduce aversion of\ntherandom \u0002eld thatconsidered thefuture aswell asthe\npast, then itwould beunclear astowhether performance\ngains of\u0002elds overchains were duetoinherent modeling\nsuperiority ,ortothefactthatthe\u0002elds haduseofmore\ndata. Thus, weconstrain the\u0002eld neighborhood insuch\namanner astomakeitfairandanalogous aspossible to\nthechain models. Future work(nopunintended) need not\nadhere tothisconstraint.\n4.2 Conjuncti veFeatur es\nThesecond limitation weimpose ontheconditional prob-\nability P(ni;tjHi;t)concerns thenature ofdependencies\nthat will bemodeled bya\u0002eld. Ingeneral, arandom\n\u0002eld frame workallowsarbitrary dependencies (orfea-\ntures)between thetargetni;tanditsneighborhood Hi;t.\nForexample, ni;tmay depend ontheanswer tothefol-\nlowing question: what isthetotal number oftimes note\niwas played inthehistory Hi;t?.Forthesakeofsim-\nplicity andelegance wewilldeliberately restrict allowedFigure 5:Examples ofmusical features thatmay bein-\nduced topredict theprobability ofnote 2being played\nattimet.Black circles represent notes that arepart of\nthefeature function. Boxedblack circle denotes thenote\nn2;t.Boxedarea represents thehistory H2;t.From leftto\nright, thefeatures are:fn2;tn1;tg,fn2;tn2;t\u00001n2;t\u00002g,\nfn2;tn1;tn3;t\u00001n3;t\u00002g,fn2;tn0;tn2;t\u00002n0;t\u00002g\ndependencies tobinary questions oftheform: wasnotej\nplayed attimesbefor et?.Wewillalsoallowgeneraliza-\ntions where aquestion isaskedabout some subset Softhe\nnotes intheallowed history Hi;t.Theanswer toaques-\ntion ofthisform will becalled thefeature function fS,\nandSwillbereferred toasthesupport off.Foragiven\nsupport S2Hi;t,thefeature function fSisde\u0002ned as\ntheconjunction ofanswers about theindividual notes in\nnj;s2S:\nfS(ni;t;Hi;t)=ni;tY\nnj;s2Snj;s (3)\nDe\u0002ned inthismanner ,ourfeature functions areal-\nwaysboolean, andequal to1ifallthenotes de\u0002ned byS\nwere played before thetargetnoteni;t.Afeature function\nalwaysincludes thetargetnoteni;t.This isnotafallacy\ninthemodel, sinceni;twillneveractually beconsidered\napart ofitsownhistory .Presence ofni;tinthefeature\nservesonly totietheoccurrences ofnotes inStotheoc-\ncurrence ofni;t.Ifthefeature isconsidered likely,that\nisevidence infavorofpredicting ni;t=1.Ifthefeature\ndoes notoccur ,itsuggests thatni;tislikelytobezero.\nOne \u0002nal comment: wechoose tomakefeatures time-\ninvariant, butnotindexinvariant. This means thatafea-\ntureisexpected tocharacterize thesame kind ofdepend-\nency,regardless ofthetime indextofthetargetni;t,but\nthatthefeature isnot(pitch) transposition invariant. Con-\nsequently ,wewillindexthetime component ofthenotes\ninSnotinabsolute values butrelati vetothetimet.We\ndonotdothesame forthenote indexi,sothese indices\nwillremain absolute. Asanillustration, Figure 5contains\nsome examples offeatures thatcould haveanimpact on\nnote 2attimet.\n4.3 Exponential Form\nAtthispoint weareready toselect theparametric form\nthat wewill beusing forcomputing theprobabilities\nP(ni;tjHi;t).There areanumber ofdifferent forms we\ncould choose, butitturns outthatforrandom \u0002elds there\nisanatural formulation ofthedistrib ution thatisgivenby\nthemaximum-entrop yframe work. Suppose wearegiven\nasetFoffeature functions that de\u0002ne thestructure of\nthe\u0002eld. Themaximum-entrop yprinciple states thatwe\nshould select theparametric form that is:(i)consistent\nwith thestructure imposed byFand(ii)makestheleast\n209amount ofunwarranted assumptions thatisthemost\nuniform ofalldistrib utions consistent withF.Thefamily\noffunctions thatsatis\u0002es these twocriteria istheexponen-\ntial(orlog-linear) family ,expressed as:\n^P(ni;tjHi;t)=1\nZi;texp8\n<\n:X\nf2F\u0015ff(ni;t;Hi;t)9\n=\n;(4)\nInequation (4),thesetofscalars \u0003=f\u0015f:f2Fg\nisthesetofLagrange multipliers forthesetofstructural\nconstraintsF.Intuiti vely,theparameter \u0015fensures that\nourmodel predicts feature fasoften asitshould occur\ninreality .Zi;tisthenormalization constant thatensures\nthatourdistrib ution sums tounity overallpossible values\nofni;t.Instatistical physics, itisknownasapartition\nfunction andisde\u0002ned asfollows:\nZi;t=X\nnexp8\n<\n:X\nf2F\u0015ff(n;Hi;t)9\n=\n;(5)\nForageneral random \u0002eld, thepartition function Zi;t\nisexceptionally hard tocompute, since itinvolvessum-\nmation overallpossible states ofthesystem. Inatypical\nsystem thenumber ofstates isexponential inthenumber\nof\u0002eld variables, anddirect computation ofequation (5)\nisnotfeasible.\nInour case, our assumption ofnodependencies\nbetween thecurrent state notesni;tmakescomputation\nofthepartition function extremely simple. Recall that\nallunderlying \u0002eld variables arebinary ,soequation (5)\nonly needs tobecomputed fortwocases: ni;t=0and\nni;t=1.Wecanfurther simplify theproblem ifwere-\ncallthateveryfeature function fisabinary conjunction,\nandeveryfincludes ni;tinitssupport. Asadirect con-\nsequence, f(ni;t;Hi;t)isnon-zero only ifni;t=1.The\nassertion holds forallfeature functions f2F,which im-\nplies thatthesummation inside theexponent inequations\n(4)and(5)iszero whene verni;t=0.These observ a-\ntions allowustore-write equation (4)inaform thatallows\nveryrapid calculations:\n^P(ni;t=1jHi;t)=\u001b8\n<\n:X\nf2F\u0015ff(1;Hi;t)9\n=\n;\n^P(ni;t=0jHi;t)=1\u0000P(ni;t=1jHi;t) (6)\nHere \u001bisthe sigmoid function, de\u0002ned as:\n\u001b(x)=ex\n1+ex.Wehavestated equation (6)asaspecial\ncase applicable toourparticular setting. Intheremaining\narguments wewillusetheform givenbyequations (4)and\n(5)toensure generality .\n4.4 Objecti veFunction\nThe ultimate goal ofthisproject istodevelop aprobab-\nility distrib ution ^P(ni;tjHi;t)thatwillaccurately predict\nthenotesni;tinpolyphonic music. There existanumber\nofdifferent measures thatcould indicate thequality ofpre-\ndiction. Wechoose oneofthesimplest log-lik elihood\nofthetraining data. Givenatraining setTconsisting of\nTsimultaneities with 12notes each, thelog-lik elihood issimply theaverage logarithm oftheprobability ofprodu-\ncing everynote inT:\nL^P=1\n12TlogTY\nt=111Y\ni=0^P(ni;tjHi;t) (7)\n=X\nH~P(H)X\nn~P(njH)log^P(njH)\nInthesecond step inequation (7)were-expressed\nthelog-lik elihood interms oftheexpected cross-entr opy\nbetween thetargetdistrib ution ~P(njH)andtheestimate\n^P(njH)produced byour\u0002eld. Thetargetempirical dis-\ntribution~P(njH)canbecomputed directly from thetrain-\ningsetT,itisjusttherelati vefrequenc yofobserving a\nnotentogether with thehistory Hacross allthepositions\n(i;t)inthe\u0002eld:\n~P(njH)=1\n12TTX\nt=111X\ni=0\u000e(n;ni;t)\u000e(H;Hi;t)(8)\nHere\u000erefers totheKroneck erdelta function, de\u0002ned\nas:\n\u000e(x;y)=\u001a\n1ifx=y\n0otherwise(9)\nReturning ourattention toequation (7),westress that\ntheexpectationP\nH[:::]isperformed overallpossible\nvalues that ahistory Hofanote might take.This set\nisexponentially large,andadirect computation would be\ninfeasible. However,forcomputation wealwaysusethe\n\u0002rst part (top) ofequation (7),whereas thesecond part\n(bottom) comes veryhandy inthealgebraic derivations of\nthe\u0002eld induction algorithm.\n4.5 Maximum Entr opy\nTosummarize, intheprevious twosubsections were-\nstricted ourselv estotheexponential (Gibbs) form ofthe\nprobability distrib ution ^P(njH),and declared that our\nobjecti veistomaximize thelikelihood ofthetraining\ndata within that parametric form. Itisimportant to\nnote thatthere isadifferent statement ofobjecti vesthat\nprovably leads toexactly thesame exponential solution\n^P(njH).Rather than focus onmaximum-lik elihood, we\ncould search forthemost uniform distrib ution ^P(njH)\nthatisconsistent with thestructure imposed byF.Tocla-\nrifywhat wemean bythestructure consistenc y,suppose\nf2Fisafeature ofthe\u0002eld. Let~E[f]denote theem-\npirical ortargetexpected value off,which issimply how\noften thefeature actually occurs inthetraining dataT:\n~E[f]=X\nH~P(H)X\nn~P(njH)f(n;H)(10)\n=1\n12TTX\nt=111X\ni=0f(ni;t;Hi;t)\nSimilarly ,ourestimate ^P(njH)givesrisetothepre-\ndicted expectation ^E[f]forthefunction f.Predicted ex-\npected value issimply howoften ourmodel thinks that\nfshould occur inthetraining set:\n210^E[f]=X\nH~P(H)X\nn^P(njH)f(n;H) (11)\n=1\n12TTX\nt=111X\ni=0X\nn^P(njHi;t)f(n;Hi;t)\nThekeydifference between ^E[f]and~E[f]isthatwe\ndonotlook attheactual valueni;twhen wecompute\n^E[f],instead wepredict itfrom ourmodel ^P(njH).\nGiventhetwoexpectations inequations (10) and(11) it\nisnatural tostrivethattheybeequal, thatiswe'dliketo\narrange ourmodel insuch awaythatpredicted frequenc y\n^E[f]ofanyfeature fmatches itsactual frequenc yofoc-\ncurrence ~E[f].Furthermore, ifthere aremultiple distrib u-\ntions^P(njH)thathonor theconstraint that^E[f]=~E[f],\nthemaximum-entrop yprinciple would guide ustopick\nthedistrib ution thatmakestheleast amount ofassump-\ntions about thedata, orequivalently ,maximizes itsown\nexpected entrop y:\nEnt^P=X\nH~P(H)X\nn^P(njH)log^P(njH) (12)\nCuriously ,maximizing theentrop ysubject tothecon-\nstraint that~E[f]=^E[f]foreveryfeature fturns out\ntobeequivalent toassuming anexponential form forour\nprobability distrib ution ^P(njH)andmaximizing thelike-\nlihood givenbyequation (7).\n4.6 Featur eInduction\nIntheprevious sections weoutlined thegeneral structure\nofarandom \u0002eld overpolyphonic music andstated our\nobjecti ve:tolearn theprobability distrib ution ^P(njH)\nthatmaximizes thelikelihood ofthetraining data (equa-\ntion(7)). Recall thatweselected theexponential form for\n^P(njH).Ifweexamine equation (4)wenote thatthere\naretwothings themodel depends on. The \u0002rst andthe\nmost important inouropinion isthestructure ofthe\u0002eld\nF,represented asasetofconstraints orfeature functions\nf2F.These constraints represent most signi\u0002cant de-\npendencies between thevariables ofthe\u0002eld. Thesecond\nthing welearn isthesetofweights \u0003=f\u0015fg,onefor\neach feature f2F.Weknowthat\u0003andFareintimately\nintertwined andweneed tolearn them simultaneously ,\nbutforthesakeofclarity wesplit thediscussion intwo\nsections. This section will describe howwecanincre-\nmentally induce thestructureFofthe\u0002eld, starting with\navery\u0003at, meaningless structure (primiti ve,atomic fea-\ntures) generalizing tomore interesting comple xrelation-\nships.\nThe \u0002eld induction procedure closely followstheal-\ngorithm described inDella Pietra etal.(1997), theprimary\ndifference being that wearedealing with aconditional\n\u0002eld, whereas Della Pietra uses ajoint model. Westart\nwith a\u0002eld thatcontains only individual notes, without\nanydependencies:F0=fni;t:i=0:::11g.Wewill\nincrementally update thestructureFbyadding thefea-\nturesgthatresult inthegreatest impro vement intheob-\njectivefunction. SupposeFk=ffSgisthecurrent \u0002eldstructure. Also assume thatthecorresponding weights \u0003k\nareoptimized with respect toFk.Wewould liketoaddto\nFkanewfeature gthatwillallowustofurther increase\nthelikelihood ofthetraining data. Inorder todothatwe\n\u0002rstneed toform asetofcandidate featuresGthatcould\nbeadded. Wede\u0002neGtobethesetofallone-note exten-\nsions ofthecurrent structureF:\nG=\u001a\nfS\u0001nj;s:fS2Fandthere exists\nnj0;s02Ssuch thatjs\u0000s0j\u00142\u001b\n(13)\nInother words, weform newcandidate features gtak-\ninganexisting feature fandattaching asingle notenj;s\nthatisnottoofarfromfintime (inourcase, notmore\nthan bytwosimultaneities). Naturally ,wedonotinclude\nascandidates anyfeatures thatarealready members ofF.\nNow,following thereasoning ofDella Pietra, wewould\nliketopick acandidate g2Gthatwill result inthemax-\nimum impro vement intheobjecti vefunction. Suppose\nthatprevious log-lik elihood based only onFkwasL^P.\nNow,ifweaddafeature gweighted bythemultiplier \u000b,\nthenewlikelihood ofthetraining data would be:\nL^P+f\u000bgg=L^P+\u000b~E[g]\u0000log^E[e\u000bg] (14)\nWhen weaddanewfeature gtothe\u0002eld, wewould\nliketoadd itwith areasonable weight \u000b,preferably\ntheweight that maximizes thecontrib ution of\u000b.We\ncanachie vethatbydifferentiating thenewlog-lik elihood\nL^P+f\u000bggwith respect to\u000band\u0002nding therootofthede-\nrivative:\n0=@L^P+f\u000bgg\n@\u000b()\u000b=log\"~E[g](1\u0000^E[g])\n^E[g](1\u0000~E[g])#\n(15)\nAnimportant observ ation tomakeisthatwearrived\nataclosed-form solution fortheoptimal weight \u000btobe\nassigned tothenewfeature g.Theclosed-form solution is\naspecial property ofbinary feature functions, andgreatly\nsimpli\u0002es theprocess ofinducing \u0002eld structure. Know-\ningtheoptimal valueof\u000binclosed form allowsustocom-\npute theresulting impro vement, orgain,inlog-lik elihood,\nalsoinclosed form:\nGain =~E[g]log~E[g]\n^E[g]+(1\u0000~E[g])log1\u0000~E[g]\n1\u0000^E[g](16)\nThe \u0002nal form isparticularly interesting, since it\nrepresents theKullback-Leibler divergence between two\nBernoulli distrib utions with expected values ~E[g]and\n^E[g]respecti vely.\n4.7 Parameter Estimation\nIntheprevious section wedescribed howwecanauto-\nmatically induce thestructure ofarandom \u0002eld byin-\ncrementally adding themost promising candidate feature\ng2G.Wealso presented theclosed form equations that\nallowustodetermine theimpro vement inlog-lik elihood\nthat would result from adding gtothe\u0002eld, and the\noptimal weight \u000bthat would lead tothat impro vement.\nWhat wedidnotdiscuss istheeffectofadding gonthe\n211weights ofother features already inthe\u0002eld. Since the\nfeatures f2Farenotindependent ofeach other ,adding\nanewfeature willaffectthebalance ofexisting features.\nFrom equation (16) weknowthatthenewlog-lik elihood\nL^P+f\u000bggisalwaysgoing tobebetter than theoldoneL^P\n(unless the\u0002eld issaturated andcannot beimpro vedany-\nmore). However,thisdoes notguarantee thatthecurrent\nsetofweights \u0003isoptimal forthenewstructure. Wemay\nbeable tofurther impro vetheobjecti vebyre-optimizing\ntheweights forallfunctions thatarenowinthe\u0002eld.\nAssume nowthatthestructureFcontains allthede-\nsired features. Wewould liketoadjust thesetofweights\n\u0003,sothattheobjecti vefunctionL^Pismaximized. This\nisaccomplished bycomputing thepartial derivativesof\nL^Pwith respect toeach weight \u0015f0,with theintention of\ndriving these derivativestozero:\n@L^P\n@\u0015f0=~E[f0]\u0000^E[f0] (17)\nUnfortunately ,there isnoclosed-form solution that\nwould allowustosettheweights totheir optimal values.\nInstead, weutilize aniterati veprocedure thatwill drive\ntheweights towards theoptimum. There areanumber\nofalgorithms foradjusting theweights inanexponential\nmodel, themost widely knownbeing theGeneralized It-\nerativeScaling (GIS) algorithm proposed byDarroch and\nRatclif f(1972). However,iterati vescaling isextremely\nslow;much faster convergence canbeachie vedbyusing\nvariations ofgradient descent. Giventhecurrent value of\ntheweight vector\u0003,wewillupdate itbyasmall stepinthe\ndirection ofthesteepest increase ofthelikelihood, given\nbythevector ofpartial derivatives:\n\u0015k+1\nf \u0000\u0015k\nf+\f@L^P\n@\u0015f=\u0015k\nf+\f\u0010\n~E[f0]\u0000^E[f0]\u0011\n(18)\nEquation (18)willbeapplied iterati vely,forallf2F,\nuntil thechange inlikelihood issmaller than some pre-\nselected threshold. Note thatwhile ~E[f]iscomputed only\nonce foreach feature f,wewill havetore-compute the\nvalue^E[f]after everyupdate. This makesthelearning\nprocedure quite expensi ve.However,thelearning proced-\nureisguaranteed toconvergetotheglobal optimum. Con-\nvergence isensured bythefactthattheobjecti vefunction\nL^Pis\\-convexwith respect totheweights \u0015f.One may\nverify thisbycomputing thesecond-order derivativeof\nL^Pandobserving thatitiseverywhere negative.\n4.8 Field Induction Algorithm\nWeare\u0002nally ready tobring together theresults ofthe\nprevious subsections intoonealgorithm forautomatic in-\nduction ofrandom \u0002elds models forpolyphonic music:\n1.Initialization\n(a)Letthefeature setF0bethesetofsingle-note\nfeatures:F0=fni;t:i=0:::11g\n(b)Settheinitial features weights \u0015f=1forall\nf2F02.Weight Update\n(a)Set\u0015k+1\nf \u0015k\nf+\f\u0010\n~E[f]\u0000^E[f]\u0011\nforeach fea-\nturef2F\n(b)Ifthere isnoticeable change inlikelihood, repeat\nstep(2a)\n3.Featur eInduction\n(a)Enumerate thesetofcandidate features\n(b)Foreverycandidate feature g2Gcompute the\noptimal weight \u000bg=logh~E[g](1\u0000^E[g])\n^E[g](1\u0000~E[g])i\n(c)Foreveryg2Gcompute expected impro vement\n(gain)from adding gtothestructureF\n(d)Pick thecandidate gthatpromises thehighest im-\nprovement, addittothestructureF,andset\u0015g=\u000bg\n(e)Ifthere isnoticeable change inlikelihood, goto\nstep (2),otherwise returnFand\u0003astheinduced\n\u0002eld\n4.9 Discussion onMark ovChains\nAtthispoint weshould note that ourrandom \u0002eld ap-\nproach insome sense encompasses theMark ovchain ap-\nproach. Instead ofinducing thefeatures ofthe\u0002eld,\nonecould easily preselect asfeatures allpossible one-\ndimensional chains under acertain \u0002xedlength andthen\nlearn theweights ofthose features directly .One advant-\nagetoourcurrent approach, however,isthatbyselecti vely\nadding only thebestfeatures tothemodel, notonly isthe\n\u0002nal number ofparameters much smaller ,butthefeatures\nthemselv esgrowinthedirection ofthedata. Inourex-\nperiments, forsome songs welearned features covering\n6simultaneities (onsets) within the\u0002rstthousand features\ninduced. AMark ovchain would require 126\u00192:99mil-\nlionparameters tocoverthissame onset range.\n4.10 Music Retrie valusing Random Fields\nNowthat wehaveaframe workforcreating aMark ov\nRandom Field model ofapiece ofmusic wewish to\nusethismodel forretrie val.Wedothisbyestimating\namodel from aquery andthen observing howwell that\nquery model predicts thenotes ineach document inthe\ncollection. Inother words, ourmeasure ofsimilarity is\ntheexpected cross-entr opybetween theempirical distrib u-\ntion~PD(njH)ofthedocument andtheestimate ^PQ(njH)\nproduced bythequery model. This measure isessentially\nequation (7)from above,with thedocument rather than\nthequery asthetargetdistrib ution.\nIfthemodel estimated from thequery does well atpre-\ndicting thenotes inapiece ofmusic, then thequery and\nthatpiece could havebeen drawnfrom thesame underly-\ningdistrib ution. Therefore weregardthem assimilar.\nThis process isrepeated forallpieces inthecollection,\nandpieces arethen rankedinorder ofincreasing cross-\nentrop y(dissimilarity).\n212 0 0.2 0.4 0.6 0.8 1\n 0  0.2  0.4  0.6  0.8  1Precision\nRecall11-pt Recall/Precision graph\nMRF\nMC=2\nMC=3\nMC=4\n\u0002MRF \u0002MC=2 \u0002MC=3 \u0002MC=4\n%Change %Change %Change\nRetrie ved: 151000 151000 151000 151000\nRelevant: 8801 8801 8801 8801\nReljret: 7291 4508 -38.17* 6805 -6.67* 6037 -17.20*\nInterpolated Recall -Precision\nat0.00 1.0000 1.0000 0.0 1.0000 0.0 1.0000 0.0\nat0.10 0.5316 0.3171 -40.3* 0.2176 -59.1* 0.1319 -75.2*\nat0.20 0.3802 0.2388 -37.2* 0.1522 -60.0* 0.0970 -74.5*\nat0.30 0.2787 0.1404 -49.6* 0.1316 -52.8* 0.0931 -66.6*\nat0.40 0.2063 0.0848 -58.9* 0.0873 -57.7* 0.0691 -66.5*\nat0.50 0.1598 0.0492 -69.2* 0.0754 -52.8* 0.0550 -65.6*\nat0.60 0.1209 0.0307 -74.6* 0.0656 -45.7* 0.0498 -58.8*\nat0.70 0.0880 0.0134 -84.8* 0.0530 -39.8* 0.0406 -53.9*\nat0.80 0.0571 0.0021 -96.2* 0.0407 -28.7* 0.0404 -29.2*\nat0.90 0.0218 0.0000 -100.0* 0.0116 -46.8* 0.0372 +70.5*\nat1.00 0.0028 0.0000 -100.0* 0.0002 -91.2* 0.0197 +603.4*\nAverage precision (non-interpolated) overallreldocs\n0.2175 0.1245 -42.76* 0.1021 -53.05* 0.0790 -63.68*\nFigure 6:Recall-Precision results forMark ovRandom Field versus Mark ovChain models\n5EXPERIMENTS AND ANALYSIS\nForevaluation, wehaveassembled four collections. The\n\u0002rst isasetofapproximately 3000 polyphonic music\npieces from theCCARH atStanford. These aremostly\nbaroque andclassical pieces from Bach, Beetho ven,Co-\nrelli, Handel, Haydn, Mozart andVivaldi. Longer scores\nhavesometimes been brokenupintotheir various move-\nments, butotherwise each piece isunique. Ourremaining\nthree sets ofmusic arepieces which were intentionally\ncomposed asvariations onsome theme:\nTwinkle 26individual variations onthetune knownto\nEnglish speak ersas`Twinkle, twinkle, little star' (in\nfactamixture ofmostly polyphonic andafewmono-\nphonic versions);\nLachrimae 75versions ofJohn Dowland' s`Lachrimae\nPavan'from different 16th and17th-century sources,\nsometimes varying inquality (numbers of`wrong'\nnotes, omissions andother inaccuracies), andinscor-\ning(forsolo lute, keyboard or\u0002ve-part instrumental\nensemble);\nFolia 50variations byfour different composers onthe\nwell-kno wnbaroque tune `Les Folies d'Espagne'.Forretrie val,weselect apiece from thethree setsof\nvariations andusethatasthequery .Allother pieces from\nthatsame variation setarejudged relevanttothequery ,\nandtherestofthecollection isjudged non-rele vant.This\nprocess isrepeated forallpieces inallthree setsofvari-\nations, foratotal of151queries.\nLetusde\u0002ne \u0002MCastheretrie valsystem based on\nMark ovChain models, and\u0002MRFastheretrie valsystem\nbased onMark ovRandom Field models. Figure 6shows\ntherecall-precision results fortherankedlistsproduced by\neach ofthevarious modeling approaches, \u0002MRFaswell\nas\u0002MCwith thelength ofthechain setto2,3,and4(1st-\n,2nd-,and3rd-order models, respecti vely). Inthetable,\n\u0002MRFisshown\u0002rst, andeach ofthe\u0002MCsystems are\nshownincomparison, with percentage change (whether\npositi veornegative)andanasterisk toindicate statistical\nsigni\u0002cance (t-test ata0.05 level).\nNomatter thechain length, therandom \u0002eld approach\noutperforms theMark ovchain approach onjust about\neverylevelofprecision-recall. Onaverage, the\u0002MCsys-\ntems arefrom 42% to63% worse. These results showthe\nvalue oftherandom \u0002eld approach.\nWebelie vewhat ishappening isthattherandom \u0002elds\nareless sensiti vetothenoise that appears with vari-\n213ations. Forexample, suppose there isaninsertion ofa\nsingle note inoneofthevariations. The Mark ovchain\napproach counts allpossible paths through apolyphonic\nsequence. Thenumber ofthese paths isexponential inthe\nlength ofthechain. Asingle note insertion therefore dis-\nproportionately increases thenumber (and character) of\npaths being counted. This analysis isborne outbythe\nfacthigher -order Mark ovchains actually doprogressi vely\nworse (see Figure 6).The longer thechain, themore a\nsingle insertion affects themodel. Twonote insertions\ncreate evenmore paths.\nOntheother hand, random \u0002elds takeintoaccount the\ndependencies between thefeatures. Theyworkbycalcu-\nlating feature expectations overthedata (seesection 4.5),\nandadjusting the\u0015weights sothat thecontrib ution of\neach feature towardtheprediction ofanote label (on\noroff)isbalanced. Asingle note insertion may activ-\nateafewadditional feature functions, butthecontrib ution\nofthese additional features donotthrowofftheoverall\ncorrectness ofthenote probability estimate because their\n\u0015weights arelearned initially with non-independence in\nmind. Random \u0002elds aremore robustwhen itcomes to\ndetecting variations.\n6CONCLUSION\nInthispaper wedeveloped aretrie valsystem based on\nautomatically-induced random \u0002elds andshowthesuperi-\nority ofthese models overMark ovchains. Central toour\napproach isthenotion ofabinary feature function, acon-\njunction ofnotes positioned at\u0002xedpitches andlocations.\nYetthese features arenottheonly ones possible. Re-\ncallfrom section 4.2thatfeatures arejustfunctions that\nreturn aboolean value. What happens inside thefunction\nisaslimitless asoneneed imagine. Forexample, onecan\ncreate models ofrhythmic patterns bychoosing onset con-\njunction features ofthesort:f1=wasthere anonset 100\nticks prior tothecurrent onset, andanother onset 300ticks\nprior tothecurrent onset? (Wearecurrently developing\nsuch models.)\nOne isnotlimited torhythm alone, anymore than\noneislimited topitch alone. Alongside pitch-only and\nrhythm-only features, oursetofactivefeature functionsF\ncancontain features thatareamixture ofboth pitch and\nduration. E.g.: Letf3=the previous note wasanEandit\nlasted for200ms. Notonly does thisfeature contain both\npitch andduration information, butifthemodel already\ncontains thepitch-only feature f2=theprevious note was\nanE,wemay addf3without worry.Thetraining thatoc-\ncurs aspartofweight updating (section 4.7) insures that\nthe\u0015values giventoallfeature arebalanced, toautomat-\nically takeinto account statistical dependencies between\nfeatures. (Itshould beclear thatf2andf3arenotin-\ndependent.) Early workbyDoraisamy andR¨uger (2001)\nandLemstr ¨ometal.(1998) experimented with features of\nthisnature, butwere forced bylack offrame worktomake\ntheindependence assumption. Withrandom \u0002elds, this\nassumption isnolonger necessary .\nFinally ,itgoes without saying thatinaddition topitch\nandrhythm, aslong asonecancraft abinary wrapper (fea-\nture function) onecanuseanyother type ofmusical in-formation onehasavailable, including metadata anddata\nobtained from semantic analysis ofaudio. Features could\ninclude functions such as:f4=my timbre classi\u0002er gives\nmecon\u0002dence >0.9thatthere isatrombone playing at\nthispoint inthisaudio recording, andmybeat track eres-\ntimates thecurrent tempo forthissong atbetween 80and\n100bpm, andthere wasanonset around 300ticks ago, and\nthemetadata tells methissong wasrecorded inthe1970s\nThis feature function may (when properly weighted and\ncombined with therestoff2F)beastrong positi ve\nindication thatthenote C#ison.\nRandom \u0002elds areaframe workforsequential mu-\nsicmodeling inwhich combination ofmultiple, non-\nindependent sources andtypes ofdata may beexplored.\nMark ovrandom \u0002elds aremore robustthan Mark ov\nchains. Theyareaccurate, without over\u0002tting, aswecan\nseefrom theretrie valresults above.Theyalso offera\nmethod forattaching relati veimportance tovarious fea-\ntures without having tomakeindependence assumptions\nbetween thefeatures used. Inshort, theyareanimportant\nframe workfordeveloping thekinds ofmodels needed for\nmusic information retrie valapplications.\n7ACKNO WLEDGEMENTS\nManythanks toVictor Lavrenk o,whose early collabora-\ntionmade retrie valsystems ofthissortpossible. Wealso\nwish tothank theconference reviewers fortheir invaluable\ncomments.\nREFERENCES\nA.L.Berger,S.A.Della Pietra, andV.J.Della Pietra.\nAmaximum entrop yapproach tonatural language\nprocessing. Computational Linguistics ,22(1):3971,\n1996.\nJ.Darroch andD.Ratclif f.Generalized iterati vescal-\ningforlog-linear models. InAnn. Math. Statistics, 43,\npages 14701480, 1972.\nS.Della Pietra, V.Della Pietra, andJ.Lafferty.Inducing\nfeatures ofrandom \u0002elds. InIEEE Transactions onPat-\nternAnalysis andMachine Intellig ence,19,pages 380\n393, 1997.\nS.Doraisamy andS.M.R¨uger.Anapproach towarda\npolyphonic music retrie valsystem. InJ.S.Downie and\nD.Bainbridge, editors, Proceedings ofthe2ndAnnual\nISMIR ,pages 187193, Indiana University ,Blooming-\nton,Indiana, October 2001.\nJ.Lafferty,A.McCallum, andF.Pereira. Conditional ran-\ndom \u0002elds: Probabilistic models forsegmenting and\nlabeling sequence data. InProc.18th International\nConf .onMachine Learning ,pages 282289. Morgan\nKaufmann, SanFrancisco, CA, 2001.\nK.Lemstr ¨om,A.Haapaniemi, andE.Ukkonen. Retrie v-\ningmusic -toindexornottoindex.InTheSixth ACM\nInternational Multimedia Confer ence (MM '98),pages\n6465, Bristol, UK, September 13-16 1998.\nC.Zhai and J.Lafferty. Dual role ofsmoothing\ninthelanguage modeling approach. URLcite-\nseer.ist.psu.edu/zhai01dual.html .\n214"
    },
    {
        "title": "A Novel HMM Approach to Melody Spotting in Raw Audio Recordings.",
        "author": [
            "Aggelos Pikrakis",
            "Sergios Theodoridis"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414886",
        "url": "https://doi.org/10.5281/zenodo.1414886",
        "ee": "https://zenodo.org/records/1414886/files/PikrakisT05.pdf",
        "abstract": "This paper presents a melody spotting system based on Variable Duration Hidden Markov Models (VDHMM’s), capable of locating monophonic melodies in a database of raw audio recordings. The audio recordings may either contain a single instrument performing in solo mode, or an ensemble of instruments where one of the instruments has a leading role. The melody to be spotted is presented to the system as a sequence of note durations and music intervals. In the sequel, this sequence is treated as a pattern prototype and based on it, a VDHMM is constructed. The probabilities of the associated VDHMM are determined according to a set of rules that account (a) for the allowable note duration flexibility and (b) with possible structural deviations from the prototype pattern. In addition, for each raw audio recording in the database, a sequence of note durations and music intervals is extracted by means of a multi pitch tracking algorithm. These sequences are subsequently fed as input to the constructed VDHMM that models the pattern to be located. The VDHMM employs an enhanced Viterbi algorithm, previously introduced by the authors, in order to account for pitch tracking errors and performance improvisations of the instrument players. For each audio recording in the database, the best-state sequence generated by the enhanced Viterbi algorithm is further post-processed in order to locate occurrences of the melody which is searched. Our method has been successfully tested with a variety of cello recordings in the context of Western Classical music, as well as with Greek traditional multi-instrument recordings, in which clarinet has a leading role. Keywords: Melody Spotting, Variable Duration Hidden Markov Models. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. c⃝2005 Queen Mary, University of London 1",
        "zenodo_id": 1414886,
        "dblp_key": "conf/ismir/PikrakisT05",
        "keywords": [
            "melody spotting system",
            "Variable Duration Hidden Markov Models (VDHMM’s)",
            "monophonic melodies",
            "raw audio recordings",
            "note durations",
            "music intervals",
            "sequence of note durations",
            "multi pitch tracking algorithm",
            "enhanced Viterbi algorithm",
            "performance improvisations"
        ],
        "content": "A NOVEL HMM APPROACH TO MELODY SPOTTING IN RAW AUDIO\nRECORDINGS\nAggelos Pikrakis and Sergios Theodoridis\nDept. of Informatics and Telecommunications\nUniversity of Athens\nPanepistimioupolis, TYPA Buildings\n15784, Athens, Greece\nphone: + (30) 2107275363\nfax: + (30) 2107275337\n{pikrakis, stheodor }@di.uoa.gr\nABSTRACT\nThis paper presents a melody spotting system based on\nVariable Duration Hidden Markov Models (VDHMM’s),\ncapable of locating monophonic melodies in a database\nof raw audio recordings. The audio recordings may ei-\nther contain a single instrument performing in solo mode,\nor an ensemble of instruments where one of the instru-\nmentshasaleadingrole. Themelodytobespottedispre-\nsented to the system as a sequence of note durations and\nmusic intervals. In the sequel, this sequence is treated as\na pattern prototype and based on it, a VDHMM is con-\nstructed. The probabilities of the associated VDHMM\nare determined according to a set of rules that account\n(a) for the allowable note duration ﬂexibility and (b) with\npossible structural deviations from the prototype pattern .\nIn addition, for each raw audio recording in the data-\nbase, a sequence of note durations and music intervals is\nextracted by means of a multi pitch tracking algorithm.\nThesesequencesaresubsequentlyfedasinputtothecon-\nstructed VDHMM that models the pattern to be located.\nThe VDHMM employs an enhanced Viterbi algorithm,\npreviously introduced by the authors, in order to account\nfor pitch tracking errors and performance improvisations\nof the instrument players. For each audio recording in\nthe database, the best-state sequence generated by the en-\nhanced Viterbi algorithm is further post-processed in or-\ndertolocateoccurrencesofthemelodywhichissearched.\nOur method has been successfully testedwitha variety of\ncellorecordingsinthecontextofWesternClassicalmusic,\naswellaswithGreektraditionalmulti-instrumentrecord-\nings, in which clarinet has a leading role.\nKeywords: MelodySpotting,VariableDurationHidden\nMarkov Models.\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrstpage.\nc/circlecopyrt2005 Queen Mary, University of London1 INTRODUCTION\nMelody spotting can be deﬁned as the problem of loca-\nting occurrences of a given melody in a database of mu-\nsic recordings. Depending on the origin and representa-\ntion of the melody to be spotted, as well as the nature of\nthe music recordings to be searched, several variations of\nthe melody spotting problem can be encountered in prac-\ntice.Mostresearchefforthasfocusedoncomparingsung\n(or hummed) queries to MIDI data [1,2,3,4,5]\nin the context of the so-called “Query-by-Humming”\nsystems. Such systems mainly employ Dynamic Time\nWarping techniques (variations of the Edit Distance) for\nmelodymatching,inordertoaccountforpitchandtempo\nerrors that are usually inherent in any real hummed tune.\nInanefforttocircumventtheneedforMIDImetadata\nin the database, certain researchers have proposed using\nstandardHiddenMarkovModelsforlocatingmonophonic\nmelodies in databases consisting of raw audio data. In [6]\nand [7] the database consists of recordings of a single in-\nstrumentperforminginsolomode,whereasin[8]thecase\nof studio recordings of operas, that contain a leading vo-\ncalist,istreated. In[6–8],theinputtothesystemis\nassumedtobeasymbolicrepresentationofthemelody\nto be searched (e.g., a MIDI-like representation). This\nassumption leads to a different melody matching philo-\nsophy, when compared with “Query-by-Humming” sys-\ntems. The term “Query-by-Melody” is often used in or-\ndertodescribethefunctionalityofsystemslikethosepro-\nposedin[6–8].\nIn our approach, the melody to be spotted is also as-\nsumed to be available in a symbolic format, e.g., a MIDI\nlike representation. This type of representation makes\nit possible to convert the melody to be searched to a\nsequence of note durations and music intervals (time -\nmusic interval representation) . This sequence is subse-\nquently treated as a patternand a Variable Duration Hid-\nden Markov Model (VDHMM) is built in order to model\nit. UsingVDHMM’smakesitpossibletoaccountforvari-\nability of note durations and also permits to model varia-\ntions of the pattern’s sequence of music intervals. The\nresulting VDHMM is then fed with (feature) sequences\nof note durations and music intervals that have been ex-\ntracted from the raw audio recordings by means of a\nmulti-pitch tracking analysis model. We have focused on\nmulti-pitch tracking algorithms because we want to treat,\ninauniﬁedmanner,bothsingle-instrumentrecordingsand\n652multi-instrument recordings in which one of the instru-\nments has a leading role. For each feature sequence, the\nVDHMM generates a best-state sequence by means of an\nenhancedViterbialgorithm, whichhasbeenpreviouslyin-\ntroduced by the authors [9]. The enhanced Viterbi algo-\nrithm is able to deal with pitch tracking errors stemming\nfrom the application of the multi-pitch algorithm to the\nraw audio recordings. Once a best-state sequence is ge-\nnerated, it can be further processed by a simple parser in\norder to locate instances of the musical pattern. For each\ndetected occurrence of the melody in question, a recogni-\ntion probability is also returned, thus allowing for sortin g\nthe listof results.\nThenoveltyofourapproachconsistsofthefollowing:\na) a VDHMM is being employed to such problem for the\nﬁrst time, providing a noticeably enhanced performance\ninthesystem. ThisisbecauseVDHMMallowstheuseof\na robust, non-standard cost function for the Viterbi algo-\nrithm it presents.\nb) A uniﬁed treatment of both monophonic and non-\nmonophonic raw audio data, provided that in the non-\nmonophonic case, an instrument has a leading role.\nSection 2 presents the pitch tracking procedure that is\napplied to the raw audio recordings. Section 3 describes\nthemethodologywithwhichtheVDHMMisbuiltinorder\ntomodelthemelodytobespotted. Section4describesthe\nenhanced Viterbi algorithm and the post-processing stage\nthatisappliedonthebest-statesequence. Implementation\nand experiment details are given in Section 5 and ﬁnally\nconclusions are drawn in Section 6.\n2 FEATUREEXTRACTION FROM RAW\nAUDIO RECORDINGS\nThe goal of this stage is to convert each raw audio recor-\ndinginthedatabasetoasequenceofmusicintervalswith-\nout discarding note durations. The use of music intervals\nensuresinvariancetotranspositionofmelodies,whilenot e\ndurations preserve information related to rhythm. This\ntypeofintervalicrepresentationisanoptionbetweenothe r\nstandard music representation approaches (e.g. [10]).\nAt ﬁrst, a sequence of fundamental frequencies is ex-\ntracted from the audio recording using Tolonen’s multi-\npitch analysis model [11]. Tolonen’s method splits the\naudio recording into a number of frames by means of a\nmoving window technique and extracts a set of pitch can-\ndidates from each frame. In our experiments, we always\nchoose the strongest pitch candidate as the fundamental\nfrequency of the frame. For single instrument recordings,\nthis is the obvious choice, however for audio recordings,\nconsisting of an ensemble of instruments, where one of\nthe instruments has a leading role, this choice does not\nguarantee that the extracted fundamental frequency coin-\ncides with the pitch of the leading instrument. Although\nthis can distort the extracted sequence of fundamentals,\nsuch errors can be efﬁciently dealt with by the enhanced\nViterbi algorithm of Section 4.\nWithout loss of generality, let F={f1,f2,...,f N},\nbe the sequence of extracted fundamentals, where Nis\nthe number of frames into which the audio recording is\nsplit. Each fundamental frequency is in turn quantizedto the closest half-tone frequency on a logarithmic fre-\nquency axis and, ﬁnally, the difference of the quantized\nsequence is calculated. The frequency resolution adopted\nat the quantization step can be considered as a parameter\ntoourmethod,i.e.,itisalsopossibletoadoptquarter-ton e\nresolution, depending on the nature of the signals to be\nclassiﬁed. For micro-tonal music, as is the case of Greek\nTraditional Music, quarter-tone resolution is a more rea-\nsonable choice.\nEachfiis then mapped to a positive number, say k,\nequal to the distance of fifromfs(the lowest fundamen-\ntalfrequencyofinterest, A1= 55Hzinourexperiments).\nFor half-tone resolution, k=round (12 log2fi\nfs), where\nround (·)denotes the roundoff operation. As a result, F\nis mapped to sequence L={li;i= 1...N}, where\nli∈[0,lmax]. It is now straightforward to compute D,\nthe sequence of music intervals and note durations, from\nL. Thisisachievedbycalculatingthedifferenceof L,i.e.,\nD={di=li+1−li;i= 1...N −1}. We assume that\ndi∈[−G,G ], whereGis the maximum allowable music\ninterval. In the rest of this paper, we will refer to di’s as\n“symbols” and to Das the “symbol sequence”.\nItisworthnoticingthat,mostofthetime, li+1isequal\ntoli, since each note in an audio recording is very likely\nto span more than one consecutive frames. Therefore, we\ncan rewrite Das\nD={0z1,m1,0z2,m2,...,0zN−1,mN−1,0zN}(1)\nwhere0zkstands forzksuccessive zeros and each miis\na non-zerodi. As a result, Dconsists of subsequences of\nzerosseparated by non-zero values (themi’s), with each\nmidenoting a music interval, i.e., the beginning of a new\nnote. The physical meaning of a subsequence of zeros is\nthatit represents the duration of a musical note.\n3 MODELING THE MELODY TO BE\nSPOTTED BY MEANS OF A VDHMM\nWe now turn our attention to the representation of the\nmelody to be spotted. Following the notation adopted in\nequation(1),themelodywillalsoﬁrstberepresentedasa\nsequence of music intervals and note durations. Without\nloss of generality, let\nMp={(fr1,t1),(fr2,t2),..., (frM,tM)}\nbe a melody consisting of Mnotes, where for each pair\n(fri,ti),friis the pitch of the i−thnote (measured\nin Hz) and tiis the respective note duration (measured\nin seconds). This time-frequency representation is not re-\nstrictive, as it can be computed in a straightforward man-\nnerfromdatastoredinsymbolicformat(e.g.,MIDI).Fol-\nlowing the approach adopted in Section 2, each frican\nalso be quantized to the closest half-tone frequency, say\nlri. As a result, Mpis mapped to Lp={(lri,ti);i=\n1...M}, wherelri∈[0,lmax]andtiis still measured\nin seconds. The i−thnote duration is mapped to a se-\nquence ofzizeros, say Ozi, wherezi=round (ti/step ),\nwithstepbeing thestepofthemoving window technique\nthatwasalsousedfortherawaudiorecordings(measured\n653in seconds). Mpcan now be written as\nDp={0z1,mr1,0z2,mr2,...,0zM−1,mr M−1,0zM}\n(2)\nwheremri=lri+1−lri. Taking equation (2) as a star-\nting point, a VDHMM can now be built for the melody\nto be spotted. Before proceeding, it has to be noted that,\nwith the exception of the ﬁrst note of the melody (which\nhas been mapped to a sequence of zeros), each note cor-\nresponds to a non-zero symbol followed by a sequence of\nzeros. The VDHMM is thus built according to the fol-\nlowing set of rules:\n(I)Onestateiscreatedforeachsubsequenceofzeros Ozk,\nk= 1...M. These are the Z-states, Z1...,Z M. Each\nZ-state only emits zeros with probability equal to one.\nTherefore, each note duration is modeled by a Z-state.\n(II)The state duration for each Z-state is modeled by a\nGaussian probability density function, namely, pZi(τ) =\nG(τ,µZi,σ2\nZi). The values of µZiandσZidepend on the\nallowabletempoﬂuctuationandtimeelasticity,duetoper-\nformance variations of the instrument players. By adopt-\ning different zero-states, we allow a different state dura-\ntionmodelforeachnote,somethingthatisdictatedbythe\nnature of real world signals.\n(III)For eachmri,i= 1...M −1, marking the begin-\nning of a note, a separate state is created. These are the\nS-states,S1,...,S M−1. Each S-state only emits the re-\nspectivemriwith probability equal to one.\n(IV)This is a left-to-right model, where each Z-state, Zi,\nis followed by an S-state, Si, and eachSiis deﬁnitely\nfollowed by Zi+1.It must pointed out that, according\nto this approach, each note of the melody corresponds\nto a pair of states, namely a non-zero state followed by\na zero-state, with the exception, of course, of the ﬁrst\nnote (ﬁgure 1). In addition, for a melody consisting of\na sequence of Mnotes, the respective HMM consists of\nS= 2 +M+M−1 = 2M+ 1states.\nZ1 S1 Z2 SN-1.. ZN\noZMoZ1oZ2 mr1 mrM-1\n2nd□note{1st□note{\nMth□note{\nFigure 1: Mapping melody to a VDHMM\n(V)A third type of state is added, both in the beginning\nandintheendoftheVDHMMofﬁgure(1),whichwecall\ntheend-state. Eachend-stateisallowedtoemitanymusic\ninterval(symbol),aswellaszeros,withequalprobability .\nIf the end states are named E1andE2, the successor to\nE1can be either Z1orE2andE2is now the rightmost\nstate of the model. As a result, the following state tran-\nsitions are allowed to take place: E1→Z1,E1→E2\nandE2→E1. The state duration for the end states is\nmodeled by a uniform probability density function with a\nmaximum state duration equal to ≃1seconds. This com-\npletes a basic version of the VDHMM (shown in ﬁgure\n2).\nWe have now reached the point where this basic ver-\nsion of the VDHMM can be used as a melody spotter .This is because, if the sequence of music intervals, that\nhasbeenextractedfromtherawaudiorecording(equation\n(1)), is fed as input to this VDHMM and the Viterbi algo-\nrithmisusedforthecalculationofthebest-statesequence ,\ntheVDHMMisexpectedtoiteratebetweentheend-states,\nE1andE2, until the melody is encountered. Then, the\nVDHMMwillgothroughthesequenceofZ-statesandS-\nstates modeling the music intervals of the melody, until\nit jumps toE2and will start again iterating between the\nend-states,untilonemoreoccurrenceofthemelodyisen-\ncountered or the end of the feature sequence is reached.\nZ1 S1 Z2 SM-1.. ZM E1 E2\nFigure 2: Basic version of the VDHMM\nAfter the whole feature sequence of the raw audio\nrecording is processed, a simple parser can post-process\nthe best-state sequence and any state subsequences cor-\nresponding to occurrences of the melody can be easily\nlocated. This is because, whenever an instance of the\nmelody is detected, the VDHMM will go through a se-\nquence of states consisting only Z-states and S-states.\nIt is therefore straightforward to locate such sequences\nof states with a simple parser (like in a simple string-\nmatching situation).\nThe VDHMM described sofarisonly suitable for ex-\nact matches of the melody to be spotted in the raw audio\nrecording,i.e.,onlynotedurationsareallowedtovaryac-\ncording to the Gaussian pdf’s that model the state dura-\ntion. However, if certain state transitions are added, the\nVDHMM of ﬁgure (2) can also deal with the cases of\nmissingnotesandrepeatingsub-patterns,byextendingthe\naforementioned set of rules. Speciﬁcally:\n(VI)Missing notes can be accounted for, if certain addi-\ntional state transitions are permitted. For example, if the\ni-th note is expected to be absent, then a transition from\nZi−1toSi, denoted as Zi−1→Si, should also be made\npossible. This is because the i-th note corresponds to the\npair of states {Si−1,Zi}and similarly, the (i+1)-th note\nstarts at state Si, whereas the (i-1)-th note ends at state\nZi−1(ﬁgure 3).\n(VII)In the same manner, accounting for successive rep-\netitions of a sub-pattern of the prototype, leads to per-\nmitting backward state transitions to take place. For in-\nstance,ifnotes {i,i+ 1,...,i +K}areexpectedtoform\na repeating pattern, then clearly, the backward transition\nZi+K→Si−1must be added. This is again because the\n(i+K)-th note ends at state Zi+K, whereas the i-th note\nstartsat state Si−1(ﬁgure 3).\nMissing notes and repeated sub-patterns are particu-\nlarlyusefultomodel,whendealingwithmusicwhereim-\nprovisation of the instrument players is a common phe-\nnomenon, like in the case of Greek Traditional Clarinet\nperforming a leading role while accompanied by an en-\nsemble of instruments.\n654Si-2 Si Zi+1Zi-1 Si-1 Zi......\ni-th□note{(i-1)-th□note{(i+1)-th□note{Si+k-1 Zi+k\n(i+k)-th□note{\nFigure 3:Zi−1→Siaccounts for a possibly missing i-th\nnote.Zi+K→Si−1accounts for a repeating sub-pattern\nofk+ 1notes\nFurthermore, it is also possible to relax the constraint\nthat each S-state emits only one symbol, if one is unsure\nof the exact score of the melody to be searched, or if one\nwishes to locate variations of the melody with a single\nsearch. For example, state Sicould also be allowed to\nemit symbols mri+1 ormri-1.\n4 THE ENHANCED VITERBI\nALGORITHM\nTranslated in the HMM terminology, let\nH={π,A,B, G}be the resulting VDHMM, where\nπSx1is the vector of initial probabilities, AS×Sis the\nstate transition matrix and B(2G+1)×Sis the symbol\nprobability matrix ( Gis the maximum allowed music\ninterval). Regarding the GS×2matrix, the ﬁrst element of\nthe i-th row is equal to the mean value of the Gaussian\nfunction modeling the duration of the i-th state and the\nsecond element is the standard deviation of the respective\nGaussian. For the VDHMM of ﬁgure (2):\n(a)BothZ1andE1can be the ﬁrst state, suggesting that\nπ(1) =π(2) = 0.5andπ(i) = 0,i= 3...S.\n(b)Ais upper triangular with each element of the ﬁrst\ndiagonalbeingequaltoone. Allotherelementsof Ahave\nzero values, unless backward transitions are possible, as\nis the case when modeling repeating sub-patterns.\n(c)For the Z-states, each column of Bhas only one\nelement with value equal to 1, BZi(ds= 0) = 1 (and\nall other elements are zero valued) and similarly, for\neach S-state, BSi(ds=mri) = 1and all other elements\nare zero valued, unless of course, a S-state is allowed to\nemit more than one music intervals (in which case all\nallowable emissions can be set to be equiprobable).\nIn practice, sequence D, which has been extracted\nfrom a raw audio recording, suffers from a number of\npitch-trackingerrors. Sucherrorsaremorefrequentwhen\ndealing with multi-instrument recordings, where one of\nthe instruments has a leading role. This can be seen in\nﬁgure (4), where pitch-tracking errors have been marked\nin circles. In the feature sequence of the audio recording,\nsuch errors are likely to appear as subsequences of sym-\nbols whose sum is equal to zero or to a mriof the pattern\ntobelocated(forastudyofpitch-trackingerrorssee[12]) .\nIfHemploys a standard Viterbi algorithm for the cal-\nculationofthebest-statesequence,amelodyspottingfail -\nure will result, as Hwill only iterate between the end-\nstates. ThiscanbeaccommodatediftheenhancedViterbi\nalgorithmthathasbeenintroducedbytheauthorsin[9]is\nadopted. In this paper, we will only summarize the equa-Figure 4: Pitch tracking results from an audio recording\nwhere a cello instrument performs in solo mode. Errors\nhave been marked in circles\ntions for the calculation of the best-statesequence.\nBasically, the essence of this algorithm is to be able\ntoaccountforallpossiblepitch-trackingerrors(e.g. pit ch\ndoubling errors) by incorporating them in the cost func-\ntion of the Viterbi algorithm.\nAs an example, consider the feature sequence Dt=\n{0z1,+1,0z2,+1,0z3,+1,0z4,+1,0z5,+2,0z6,+1,\n0z7,+1,0z8,−1,0z9,+2,0z10}of ﬁgure (4), which\ncan be considered as a variation of the prototype Dp=\n{0zp1,+2,0zp2,+2,0zp3,+2,0zp4,+1,0zp5,+2,0zp6}.\nIfDtis given as input to a VDHMM built for Dp,\na melody spotting failure will occur, which is clearly\nundesirable.\nOn the other hand, careful observation of Dtreveals\nthat,m7(the 7th music interval), which is equal to 1\nandm8, which is equal to −1, cancel out. In addition,\nm1+m2= 2,whichistherespectivemusicintervalofthe\nprototype pattern that is modeled by the VDHMM . Simi-\nlarly,m3+m4= 2(which is again the respective music\ninterval of the prototype ).\nTheseobservationsleadustotheideathatonecan en-\nhance the performance of the VDHMM, by inserting in\nthe model a mechanism capable of deciding which sym-\nbol cancellation/summations are desired . For example,\nregarding sequence Dt:\n(a)if+1and−1are canceled out, the subsequence\n{0z7,1,0z8,−1,0z9}can be replaced by a single subse-\nquenceofzeros, 0z7+z8+z9+2. This,inturn,suggeststhat\nifamodiﬁedversionof Dt,say ˆDt,isgeneratedbytaking\ninto account the aforementioned symbol cancellation, ˆDt\nwould possess a structurecloser to the prototype Dp.\n(b)Concerning symbols m1andm2, which sum to +2,\nit is desirable to treat subsequence {+1,0z2,+1}as one\nsymbol, equal to +2. Similarly, concerning symbols m3\nandm4, which sum to +2, it is desirable to treat subse-\nquence {+1,0z4,+1}as one symbol equal to +2.\nIfthesetransformationsareappliedtotheoriginalfea-\nture sequence Dt, the new sequence ˆDtbecomes\nˆDt={0z1,+2,0z3,+2,0z5,+2,0z6,+1,0z7+z8+z9+2,\n+ 2,0z10},whichislikelytobedifferentfrom Dponlyin\n655the number of zeros separating the non-zero valued sym-\nbols (depending on the observed tempo ﬂuctuation).\nIn order to present in brief the equations for the en-\nhanced Viterbi algorithm, certain deﬁnitions must ﬁrst be\ngiven. For an observation sequence D={d1d2...d N}\nand a discrete observation VDHMM H, let us deﬁne the\nforward variable at(j)as in[13], i.e.,\nat(j) =P(d1d2...d t, statej endsatt |H),j= 1...S\n(3)\nthatisat(j)standsfortheprobabilitythatthemodelﬁnds\nitself in the j-th state after the ﬁrst tsymbols have been\nemitted. It can be shown that ([13]),\nat(j) = max\n1≤τ≤T,1≤i≤S,i/negationslash=j[δt(i,τ,j )](4)\nδt(i,τ,j ) =at−τ(i)Aijpj(τ)t/productdisplay\ns=t−τ+1Bj(ds)(5)\nwhereτis the time duration variable, Tis its maxi-\nmumallowablevalue withinany state, Sisthetotalnum-\nber of states, Ais the state transition matrix, pjis the\nduration probability distribution at state jandBis the\nsymbol probability matrix. Equations (4) and (5) sug-\ngest that there exist ( S×T−T) candidate arguments,\nδt(i,τ,j ), for the maximization of each quantity at(j).\nIn order to retrieve the best state sequence, i.e., for back-\ntracking purposes, the state that corresponds to the ar-\ngument that maximizes equation (4), is stored in a two-\ndimensional array ψ, asψ(j,t). Therefore, ψ(j,t) =\narg max[δt(i,τ,j )],1≤τ≤T,1≤i≤S,i/ne}ationslash=jIn\naddition, the number of symbols spent on state jis stored\nin a two-dimensional matrix c, asc(j,t).\nIt is important to notice that, if/summationtextt\ns=t−τ+1ds= 0,\nthisindicatesapossiblepitchtrackingerrorcancellatio n.\nThus, one must also take into consideration that the sym-\nbols{dt,dt−1,...,d t−τ+1}could be the result of a pitch\ntrackingerror,andmustbereplacedbyazerothatlastsfor\nτsuccessive time instances. This is quantiﬁed by consi-\ndering,fortheZ-states,( SxT−T)additional ˆδarguments\nto augment equation (4), namely\nˆδt(i,τ,j ) =at−τ(i)Aijpj(τ)t/productdisplay\ns=t−τ+1Bj(ds= 0)(6)\nThus,maximization is now computed over all δand ˆδ\nquantities . If maximization occurs for a ˆδargument, say\nˆδt(i,τ,j ), then the number of symbols spent at state jis\nequal toτ, as is the case with the standard VDHMM. If,\nintheend,itturnsoutthatforsomestatesofthebest-state\nsequence, a symbol cancellation took place, it is useful to\nstore this information in a separate two-dimensional ma-\ntrix,s, by setting the respective s(j,t)element equal to\n“1”.\nIfat(j)refers to an S-state, then a symbol summation\nis desirable, if the sum/summationtextt\ns=t−τ+1dsis equal to the ac-\ntual music interval associated with the respective S-state\nof the VDHMM . If this holds true, the whole subsequence\nofsymbolsistreatedasonesymbolequaltotherespective\nsum and again, for each S-state, ( SxT−T) additional ˆδarguments must be computed for at(j), according to the\nfollowing equation:\nˆδt(i,τ,j ) =at−τ(i)Aijpj(τ)Bj(t/summationdisplay\ns=t−τ+1ds)(7)\nSimilar to the previous case, maximization is again com-\nputed over all δandˆδquantities. The need to account\nfor possible symbol summations reveals the fact that, al-\nthough in the ﬁrst place the HMM was expected to spend\noneframeateachS-state,itturnsoutthataGaussianprob-\nability density function, namely pSi(τ) =G(τ,µSi,σ2\nSi),\nmust also be associated with each S-state.\nAfter the whole feature sequence of the raw audio\nrecording is processed, a simple parser can post-process\nthe best-state sequence and any state subsequences cor-\nresponding to occurrences of the melody can be easily\nlocated. This is because, whenever an instance of the\nmelody is detected, the VDHMM will go through a se-\nquence of states consisting only of Z-states and S-states.\nIt is therefore straightforward to locate such sequences\nof states with a simple parser (like in a simple string-\nmatching situation).\n4.1 Computational cost related issues\nThe proposed enhanced Viterbi algorithm leads to in-\ncreased recognition accuracy to the expense of increasing\nthe computational cost, due to the fact that the ˆδt(i,τ,j )\nargumentsneedalsobecomputed. However,itispossible\ntoreducethecomputationalcost,ifthefollowingassump-\ntions are adopted:\n(a)A Z-state may only emit sequences of symbols ( di’s)\nthatstartandendwithazero-valued di.Thissuggeststhat\nfor the Z-states, the emitted symbol sequence must be of\ntheform {0zk,mk,...,m l−1,0zl},l>=k. Ifl=kthen\nonlyone zero-valued subsequence has been emitted. Asa\nresult, for the Z-states, the respective equations need onl y\nbe computed when the following hold: dt= 0,dt+1/ne}ationslash= 0,\ndt−τ+1= 0anddt−τ/ne}ationslash= 0\n(b)In a similar manner, a S-state may only emit se-\nquences of symbols ( di’s) that start and end with a non-\nzerodi. Equivalently, for the S-states, the emitted sym-\nbol sequence must be of the form {mk,0zk+1,...,m l},\nl>=k. Ifl=kthenonlyonenon-zero dihasbeenemit-\nted. As a result, for the S-states, the respective equations\nneed only be computed when the following hold: dt/ne}ationslash= 0,\ndt+1= 0,dt−τ+1/ne}ationslash= 0anddt−τ= 0.\n5 EXPERIMENTS\nAs it has already been mentioned, Tolonen’s multipitch\nanalysismodel[11]wasadoptedasapitchtrackerforour\nexperiments and the following parameter tuning was de-\ncided: the moving window length was set equal to 50ms\n(each window was multiplied by a Hamming function)\nanda 5msstepwasadoptedbetweensuccessivewindows.\nThis small step ensures that rapid changes in the signal\nare captured effectively by the pitch tracker, to the ex-\npense of increasing the length of the feature sequence.\n656The pre-processing stage involving a pre-whitening ﬁl-\nter was omitted. For the two channel ﬁlter bank, we\nused butterworth bandpass ﬁlters with frequency ranges\n70Hz−1000Hzand1000Hz−10KHz. The parame-\nterwhichcontrolsfrequencydomaincompressionwasset\nequal to 0.7. From each frame, the strongest candidate\nfrequency returned by the model, was chosen as the fun-\ndamental frequency of the frame.\nOur method was tested on two raw audio data sets:\ntheﬁrstsetconsistedof commerciallyavailablesoloCello\nrecordings of J.S Bach’s Six Suites for Cello (BWV 1007-\n1012), performed by seven different artists (namely Boris\nPergamenschikow, Yo-Yo Ma, Anner Byslma, Ralph Kir-\nshbaum, Roel Dieltiens, Peter Bruns and Paolo Beschi).\nThe printed scores of these Cello Suites served as the ba-\nsis to deﬁne (with the help of musicologists) a total of\n≃50melodiesconsistingof 3to16notes. Thesemelodies\nwere manually converted to sequences of note durations\nand music intervals, following the representation adopted\nin Section 3. For the quantization step, half-tone reso-\nlution was adopted and an alphabet of 121discrete sym-\nbols was used, implying music intervals in the range of\n−60...+ 60half-tones, i.e., G= 60. The duration of\ntheZ-statesoftheresultingVDHMM’swastunedbyper-\nmitting a 20%tempo ﬂuctuation, in order to account for\nperformance variations. The maximum state duration for\ntheS-stateswassetequalto 40ms. Dependingonthepat-\ntern,e.g.,formovingbassmelodies,certainS-stateswere\nallowedtoemitmorethanonemusicintervals,inorderto\nbe able to locate pattern variations. The proposed method\nsucceeded in locating approximately 95%of the pattern\noccurrences.\nThe second raw audio data set consisted of ≃140\ncommercially available recordings of Greek Traditional\nmusic performed by an ensemble of instruments where\nGreek Traditional Clarinet has a leading role. A de-\ntailed description of the music corpus can be accessed at\nhttp://www.di.uoa.gr/pikrakis/melody spotter.html. Due\nto the fact that Greek Traditional Music is micro-tonal,\nquarter-tone resolution was adopted. Although printed\nscores are not available for this type of music, follow-\ning musicological advice, we focused on locating twelve\ntypes of patterns that have been shaped and categorized\nin practice over the years in the context of Greek Tradi-\ntional Music (a description of the patterns can be found\nin [12]). These patterns exhibit signiﬁcant time elastic-\nityduetoimprovisationsintheperformanceofmusicians\nand it was therefore considered appropriate to permit a\n50%tempo ﬂuctuation, when modeling the Z-states. In\nthis set of raw audio data, our method successfully spot-\nted83%of the pattern occurrences. This performance\nis mainly due to the fact, that, despite the application of\nan enhanced Viterbi algorithm, the leading instrument’s\nmelodic contour can often be severely distorted in the ex-\ntracted feature sequence of an audio recording, due to the\npresence of the accompanying instrument ensemble. A\nprototype of our melody spotting system was initially de-\nveloped in MATLAB and was subsequently ported to a\nC-development framework.6 CONCLUSIONS\nIn this paper we presented a system capable of spotting\nmonophonic melodies in a database of raw audio record-\nings. Both monophonic and non-monophonic raw audio\ndata have been treated in a uniﬁed manner. A VDHMM\nhasbeenemployedfortheﬁrsttimeasamodelforthepat-\nterns to be spotted. Pitch tracking errors have been dealt\nwith an enhanced Viterbi algorithm that results in notice-\nably enhanced performance.\nREFERENCES\n[1] Ning Hu and Roger B. Dannenberg, “A Comparison\nof Melodic Database Retrieval Techniques using Sung\nQueries”, Proceedings of the Joint Conference on Digital\nLibraries(JCDL’02) ,pp. 301-307, July13-17, 2002, Port-\nland, Oregon, USA.\n[2] Ning Hu, Roger B. Dannenberg and Ann L. Lewis,\n“A Probabilistic Model of Melodic Similarity”, Proceed-\nings of the International Computer Music Conference\n(ICMC’02) ,Gotheborg, Sweden, September 2002.\n[3] Yongwei Zhu and Mohan Kankanhali, “Music Scale Mod-\neling for Melody Matching”, Proceedings of the ACM\nMM’03, November 2-8, Berkeley, California, USA.\n[4] V. Lavrenko and J. Pickens, “Polyphonic Music Modeling\nwith Random Fields”, Proceedings of the ACM MM’03 ,\nNovember 2-8, Berkeley, California, USA.\n[5] N.Kosugi et al, “SoundCompass: A practical Query-by-\nHumming System”, Proceedings of the ACM SIGMOD\n2004, June 13-18, 2004, Paris France.\n[6] A. S. Durey and M. A. Clements, “Features for Melody\nSpotting Using Hidden Markov Models,” Proceedings of\nICASSP2002 , May 13-17, 2002, Orlando, Florida.\n[7] A.S.DureyandM.A.Clements,“MelodySpottingUsing\nHidden Markov Models,” Proceedings of ISMIR 2001 , pp.\n109-117, Bloomington, IN, October 2001.\n[8] S.S.Shwartzetal,“RobustTemporalandSpectralModel-\ning for Query By Melody”, Proceedings of SIGIR’02 , Au-\ngust 11-15, 2002, Tampere, Finland.\n[9] A. Pikrakis, S. Theodoridis and D. Kamarotos, “Classiﬁ-\ncation of Musical Patterns using Variable Duration Hid-\nden Markov Models”, Proceedings of the 12th European\nSignal Processing Conference (EUSIPCO-2004), Vienna,\nAustria, September 2004.\n[10] E. Cambouropoulos, “A General Pitch Interval Represen-\ntation: Theory and Applications”, Journal of New Music\nResearch, vol. 25(3), September 1996.\n[11] T. Tolonen and M. Karjalainen, “A Computationally Efﬁ-\ncient Multipitch Analysis Model”, IEEE Transactions on\nSpeech and Audio Processing , Vol. 8(6), 2000.\n[12] A. Pikrakis, S. Theodoridis, D. Kamarotos, “Recogni-\ntion of Isolated Musical Patterns using Hidden Markov\nModels”, LNCS/LNAI2445 , Springer Verlag, pp. 133-143,\n2002.\n[13] L.R. Rabiner, “A Tutorial on Hidden Markov Models and\nSelected Applications in Speech Recognition”, Proceed-\nings of the IEEE ,Vol. 77, No. 2, 1989.\n657"
    },
    {
        "title": "A Classification Approach to Melody Transcription.",
        "author": [
            "Graham E. Poliner",
            "Daniel P. W. Ellis"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414796",
        "url": "https://doi.org/10.5281/zenodo.1414796",
        "ee": "https://zenodo.org/records/1414796/files/PolinerE05.pdf",
        "abstract": "Melodies provide an important conceptual summarization of polyphonic audio. The extraction of melodic content has practical applications ranging from content-based audio retrieval to the analysis of musical structure. In contrast to previous transcription systems based on a model of the harmonic (or periodic) structure of musical pitches, we present a classification-based system for performing automatic melody transcription that makes no assumptions beyond what is learned from its training data. We evaluate the success of our algorithm by predicting the melody of the ISMIR 2004 Melody Competition evaluation set and on newly-generated test data. We show that a Support Vector Machine melodic classifier produces results comparable to state of the art model-based transcription systems. Keywords: Melody Transcription, Classification 1",
        "zenodo_id": 1414796,
        "dblp_key": "conf/ismir/PolinerE05",
        "keywords": [
            "Melodies",
            "conceptual",
            "summarization",
            "polyphonic",
            "audio",
            "transcription",
            "practical",
            "applications",
            "harmonic",
            "structure"
        ],
        "content": "A CLASSIFICATION APPROACH TO MELODY TRANSCRIPTION\nGraham E. Poliner and Daniel P.W. Ellis\nLabROSA, Dept. of Electrical Engineering\nColumbia University, New York NY 10027 USA\n{graham,dpwe }@ee.columbia.edu\nABSTRACT\nMelodiesprovideanimportantconceptualsummarization\nof polyphonic audio. The extraction of melodic content\nhas practical applications ranging from content-based au-\ndio retrieval to the analysis of musical structure. In con-\ntrasttoprevioustranscriptionsystemsbasedonamodelof\ntheharmonic(orperiodic)structureofmusicalpitches,we\npresent a classiﬁcation-based system for performing au-\ntomatic melody transcription that makes no assumptions\nbeyond what is learned from its training data. We evalu-\nate the success of our algorithm by predicting the melody\nof the ISMIR 2004 Melody Competition evaluation set\nand on newly-generated test data. We show that a Sup-\nport Vector Machine melodic classiﬁer produces results\ncomparable to state of the art model-based transcription\nsystems.\nKeywords: Melody Transcription, Classiﬁcation\n1 INTRODUCTION\nMelodyprovidesaconciseandnaturaldescriptionofmu-\nsic. Even for complex, polyphonic signals, the perceived\npredominant melody is the most convenient and memo-\nrable description, and can be used as an intuitive basis\nfor communication and retrieval e.g. through query-by-\nhumming. However, to deploy large-scale music organi-\nzation and retrieval systems based on melody, we need\nmechanisms to automatically extract this melody from\nrecordedmusicaudio. Suchtranscriptionalsohasvaluein\nmusicological analysis and various potential signal trans-\nformation applications. As a result, a signiﬁcant amount\nofresearchhasrecentlytakenplaceintheareaofpredom-\ninant melody detection (Goto, 2004; Eggink and Brown,\n2004; Marolt, 2004; Paiva et al., 2004; Li and Wang,\n2005).\nPrevious methods, however, all rely on a core of\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonrule-based analysis that assumes a speciﬁc audio struc-\nture, namely that a musical pitch is realized as a set of\nharmonics of a particular fundamental. This assump-\ntion is strongly grounded in musical acoustics, but it is\nnot strictly necessary: in many ﬁelds (such as automatic\nspeech recognition) it is possible to build classiﬁers for\nparticulareventswithoutanypriorknowledgeofhowthey\nare represented in the features.\nIn this paper, we pursue this insight by investigating\na machine learning system to generate automatic melody\ntranscriptions. Weproposeasystemthatlearnstoinferthe\ncorrect melody label based only on training with labeled\nexamples. Our algorithm performs dominant melodic\nnote classiﬁcation via a Support Vector Machine classi-\nﬁer trained directly from audio feature data. As a result,\nthe proposed system may be easily generalized to learn\nmanymelodicstructuresortrainedspeciﬁcallyforagiven\ngenre.\n2 SYSTEM DESCRIPTION\nThe basic ﬂow of our transcription system is as follows:\nFirst, the input audio waveform is transformed into a fea-\nturerepresentationassomekindofnormalizedshort-time\nmagnitude spectrum. A Support Vector Machine (SVM)\ntrained on real multi-instrument recordings and synthe-\nsizedMIDIaudioclassiﬁeseachframeashavingapartic-\nulardominantpitch,quantizedtothesemitonelevel. Each\nof these steps is described in more detail below:\n2.1 Acoustic Features\nThe original music recordings are combined to one chan-\nnel (mono) and downsampled to 8 kHz. This wave-\nform x[n]isconvertedtotheshort-timeFouriertransform\n(STFT),\nXSTFT [k, n] =N−1/summationdisplay\nm=0x[n−m]∗w[m]∗e−j2πkm/N(1)\nusing an N= 1024point Discrete Fourier Transforms\n(i.e. 128 ms), an N-point Hanning window w[n], and a\n944pointoverlapofadjacentwindows(fora10msgrid).\nIn most cases, only the bins corresponding to frequencies\nbelow 2 kHz (i.e. the ﬁrst 256 bins) were used. To im-\nprove generalization across different instrument timbres\n161and contexts, a variety of normalizations were applied to\nthe STFT, as described in section 3.\n2.2 Support Vector Machines\nLabeled audio feature vectors are used to train an SVM\nwith a class label for each note distinguished by the sys-\ntem. The SVM is a supervised classiﬁcation system that\nuses a hypothesis space of linear functions in a high di-\nmensional feature space in order to learn separating hy-\nperplanesthataremaximallydistantfromalltrainingpat-\nterns. As such, SVM classiﬁcation attempts to generalize\nan optimal decision boundary between classes. Labeled\ntraining data in a given space are separated by a maxi-\nmum margin hyperplane through SVM classiﬁcation. In\nthe case of N-way multi-class discrimination, a majority\nvote is taken from the output of N(N−1)/2pairwise\ndiscriminant functions. In order to classify the dominant\nmelodic note for each frame, we assume the melody note\nat a given instant to be solely dependent on the normal-\nizedfrequencydatabelow2kHz. Wefurtherassumeeach\nframe to be independent of all other frames.\n2.3 Training Data\nA supervised classiﬁer requires a corpus of pairs of fea-\nture vectors along with their ground truth labels in order\nto be trained. In general, greater amounts and variety\nof training data will give rise to more accurate and suc-\ncessful classiﬁers. In the classiﬁcation-based approach to\ntranscription, then, the biggest problem becomes collect-\ning suitable training data. Although the number of digital\nscoresalignedtorealaudioisverylimited,thereareafew\ndirections that facilitate the generation of labeled audio.\nIn this experiment, we investigate multi-track recordings\nand MIDI audio ﬁles as sources of training data.\n2.3.1 Multi-track Recordings\nPopularmusicrecordingsareusuallycreatedbylayeringa\nnumber of independently-recorded audio tracks. In some\ncases, artists (or their record companies) may make avail-\nableseparatevocalandinstrumentaltracksaspartofaCD\nor 12” vinyl single release. The ‘acapella’ vocal record-\nings can be used as a source for ground truth in the full\nensemble music since they will generally be amenable to\npitch tracking with standard tools. As long as we can\nkeeptrackofwhattimeswithinthevocalrecordingcorre-\nspondtowhattimesinthecomplete(vocalplusaccompa-\nniment) music, we can automatically provide the ground\ntruth. Note that the acapella recordings are only used to\ngenerate ground truth; the classiﬁer is not trained on iso-\nlatedvoices(sincewedonotexpecttouseitonsuchdata).\nA set of 30 multi-track recordings was obtained from\ngenres such as jazz, pop, R&B, and rock. The digital\nrecordings were read from CD, then downsampled into\nmono ﬁles at a sampling rate of 8 kHz. The 12” vinyl\nrecordings were converted from analog to digital mono\nﬁles at a sampling rate of 8kHz.\nFor each song, the fundamental frequency of the\nmelody track was estimated using the YIN fundamen-\ntal frequency estimator (de Cheveigne and Kawahara,2002). Fundamental frequency predictions were calcu-\nlated at 10 ms steps and limited to the range of 100 to\n1000 Hz. YIN deﬁnes a periodicity measure,\nP eriodicity =PPERIODIC\nPTOT(2)\nwhere PPERIODIC is the energy accounted for by the\nharmonics of the detected periodicity, and PTOTis the\ntotalenergyofaframe;Onlyframeswithperiodicityofat\nleast95%(correspondingtoclearly-pitchedvoicednotes)\nwere used as training examples.\nTo align the acapella recordings to the full ensemble\nrecordings, we performed Dynamic Time Warp (DTW)\nalignment between STFT representations of each signal,\nalong the lines of the procedure described in Turetsky\nand Ellis (2003). This time alignment was smoothed and\nlinearly interpolated to achieve a frame-by-frame corre-\nspondence. The alignments were manually veriﬁed and\ncorrected in order to ensure the integrity of the training\ndata. Target labels were assigned by calculating the clos-\nestMIDInotenumbertothemonophonicpredictionatthe\ntimes corresponding to the STFT frames.\n2.3.2 MIDI Files\nThe MIDI medium enables users to synthesize audio and\ncreateadigitalmusicscoresimultaneously. Extensivecol-\nlections of MIDI ﬁles exist consisting of numerous ren-\nditions from eclectic genres. Our MIDI training data is\ncomposed of 30 frequently downloaded pop songs from\nwww.ﬁndmidis.com.\nThe training ﬁles were converted from the standard\nMIDI ﬁle format to mono audio ﬁles (.WAV) with a sam-\nplingrateof8kHzusingtheMIDIsynthesizerinApple’s\niTunes.\nToﬁndthecorrespondinggroundtruth,theMIDIﬁles\nwere parsed into data structures containing the relevant\naudio information (i.e. tracks, channels numbers, note\nevents,etc). Themelodywasisolatedandextractedbyex-\nploitingMIDIconventionsforrepresentingtheleadvoice.\nCommonly,theleadvoiceinpopMIDIﬁlesisrepresented\nby a monophonic track on an isolated channel. In the\ncase of multiple simultaneous notes in the lead track, the\nmelodywasassumedtobethehighestnotepresent. Target\nlabelsweredeterminedbysamplingtheMIDItranscriptat\ntheprecisetimescorrespondingtoeachSTFTframeinthe\nanalysis of the synthesized audio.\n2.3.3 Resampled Audio\nIn the case when the availability of a representative train-\ning set is limited, the quantity and diversity of the train-\ning data may be extended by re-sampling the recordings\nto effect a global pitch shift. The multi-track and MIDI\nrecordingswerere-sampledatratescorrespondingtosym-\nmetric,semitonefrequencyshiftsoverthechromaticscale\n(i.e.±1,2, . . . 6semitones). Thegroundtruthlabelswere\nscaledaccordinglyandlinearlyinterpolatedinordertoad-\njust for time alignment. This approach created a more\nGaussian training distribution and reduced bias toward\nspeciﬁc keys present in the training set.\n1620 20 40 60 80 100010203040506070\nPercentage of Training DataPercent Error RateMulti Track Classification\n0 20 40 60 80 100\nPercentage of Training DataMIDI Classification Training\nTesting\nValidationFigure 1: Variation of classiﬁer frame error rate as a function of the amount of training data used, for training on real\nrecordings (left) and MIDI syntheses (right). 100% of the training data corresponds to 30,000 frames or 300 s of audio.\nCurves show the accuracy on the training and test sets, as well as on the separate ISMIR 2004 set (see text).\n3 EXPERIMENTS\nThe WEKA implementation of Platt’s Polynomial Se-\nquential Minimal Optimization (SMO) SVM algorithm\nwas used to map the frequency domain audio features to\nthe MIDI note-number classes (Witten and Frank, 2000;\nPlatt, 1998). The default learning parameter values (C =\n1, epsilon = 10−12, tolerance parameter = 103) were used\nto train the classiﬁers. Each audio frame was represented\nby a 256-element input vector, with sixty potential output\nclassesspanningtheﬁve-octaverangefromG2toF#7for\nN-way classiﬁcation, and twelve potential output classes\nrepresentingaoneoctavechromascaleforN-binaryclas-\nsiﬁcation. Thirty multi-track recordings and thirty MIDI\nﬁleswithaclearlydeﬁneddominantmelodywereselected\nfor our experiments; for each ﬁle, 1000 frames in which\nthe dominant melody was present (10 s of audio data)\nwererandomlyselectedtobeusedastrainingframes. Ten\nmulti-trackrecordingsandtenMIDIﬁlesweredesignated\nas the test set, and the ISMIR 2004 Melody Competi-\ntion test set was used as a validation set (Gomez et al.,\n2004). This was an international evaluation for predomi-\nnant melody extraction, the ﬁrst of its kind, conducted in\nthe summer of 2004. The evaluation data (which has now\nbeen released) consisted of 20 excerpts, four from each\nof 5 styles, covering a wide range of musical genres, and\neachconsistingofabout30sofaudio. Followingthecon-\nventionsofthatevaluation,tocalculateaccuracywequan-\ntize the ground-truth frequencies for every pitched frame\nto the nearest semitone (i.e. to its MIDI number), and\ncountanerrorforeachframewhereourclassiﬁerpredicts\na different note (or in some cases a different chroma i.e.\nforgivingoctaveerrors). Wedonot,inthiswork,consider\nthe problem of detecting frames that do not contain any\n‘foreground’ melody and thus for which no note should\nbe transcribed.3.1 N-way Classiﬁcation\nWe trained separate N-way SVM classiﬁers using seven\ndifferent audio feature normalizations. Three normaliza-\ntions use the STFT, and four normalizations use Mel-\nfrequencycepstralcoefﬁcients(MFCCs). Intheﬁrstcase,\nwe simply used the magnitude of the STFT normalized\nsuch that the maximum energy frame in each song had\na value equal to one. For the second case, the magni-\ntude of the STFT is normalized within each time frame\nto achieve zero mean and unit variance over a 51-frame\nlocal frequency window, the idea being to remove some\nof the inﬂuence due to different instrument timbres and\ncontexts in train and test data. The third normalization\nscheme applied cube-root compression to the STFT mag-\nnitude, to make larger spectral magnitudes appear more\nsimilar; cube-root compression is commonly used as an\napproximation to the loudness sensitivity of the ear.\nA fourth feature conﬁguration calculated the autocor-\nrelation of the audio signal calculated by taking the in-\nverse Fourier transform (IFT) of the magnitude of the\nSTFT. Taking the IFT of the log-STFT-magnitude gives\nthe cepstrum, which comprised our ﬁfth feature type. Be-\ncause overall gain and broad spectral shape are contained\nin the ﬁrst few cepstral bins, whereas periodicity appears\nat higher indexes, this feature also performs a kind of\ntimbral normalization. We also tried normalizing these\nautocorrelation-based features by local mean and vari-\nance equalization as applied to the spectra, and by lif-\ntering (scaling the higher-order cepstra by an exponential\nweight).\nFor all normalization schemes, we compared SVM\nclassiﬁers trained on the multi-track training set, MIDI\ntraining set, and both sets combined. An example learn-\ning curve (based on the locally-normalized spectral data)\nisshowninﬁgure1. Theclassiﬁcationerrordatawasgen-\nerated by training on randomly selected portions of the\ntraining set for cross validation, testing, and validation.\nThe classiﬁcation error for the testing and validation sets\n163daisy jazz midi opera pop020406080100frame accuracy % - chroma020406080100frame accuracy % - raw\nSVM\nPaivaFigure2: Variationintranscriptionframeaccuracyacross\nthe 20 excerpts of the ISMIR 2004 evaluation set. Solid\nlineshowstheclassiﬁcation-basedtranscriber;dashedline\nshows the results of the best-performing system from the\n2004 evaluation. Top pane is raw pitch accuracy; bottom\npanefoldsallresultstoasingleoctaveof12chromabins,\nto ignore octave errors.\nreaches an asymptote after approximately 100 seconds of\nrandomly-selected training audio. Although the classiﬁer\ntrained on MIDI data alone generalizes well to the IS-\nMIR validation set, the variance within the MIDI ﬁles is\nso great the classiﬁer generalizes poorly to the MIDI test\nset.\nTable 1 compares the accuracy of classiﬁers trained\non each of the different normalization schemes. Here we\nshow separate results for the classiﬁers trained on multi-\ntrack audio alone, MIDI syntheses alone, or both data\nsources combined. The frame accuracy results are for the\nISMIR 2004 melody evaluation set and correspond to f0\ntranscription to the nearestsemitone.\nA weakness of any classiﬁcation based approach is\nthat the classiﬁer will perform unpredictably on test data\nthat does not resemble the training data, and a particu-\nlar weakness of our approach of deliberately ignoring our\nprior knowledge of the relationship between spectra and\nnotes is that our system cannot generalize from the notes\nithasseentodifferentpitches. Forexample,thehighest f0\nvalues for the female opera samples in the ISMIR test set\nTable 1: Frame accuracy percentages on the ISMIR 2004\nset for each of the normalization schemes considered,\ntrained on either multi-track audio alone, MIDI syntheses\nalone, or both data sets combined.\nNormalization Multi-track MIDIALL\nSTFT 54.5 45.859.0\n51-pt norm 52.7 51.362.7\nCube root 55.1 47.162.4\nAutocorr 53.6 51.959.0\nCepstrum 48.5 44.752.1\nNormAutoco 40.8 38.544.6\nLiftCeps 53.4 48.660.3\n0 1 2 3 4 5 66065707580\ntransposition limit / semitonesframe accuracy %raw\nchromaFigure 3: Effect of including transposed versions of the\ntraining data. As the training data is duplicated at all\nsemitone transpositions out to ±6semitones, transposi-\ntion frame accuracy improves by about 5% absolute for\nraw transcripts, and about 2% absolute for the chroma\n(octave-equivalent) transcription.\nexceed the maximum pitch in all our training data. In ad-\ndition, the ISMIR set contains stylistic genre differences\n(such as opera) that do not match our pop music corpora.\nHowever, if the desired output states are mapped into the\nrange of one octave, a signiﬁcant number of these errors\nare reduced. Neglecting octave errors yields an average\npitched frame accuracy in excess of 70% on the ISMIR\ntest set.\nWe trained six additional classiﬁers in order to dis-\nplay the effects of re-sampled audio on classiﬁcation suc-\ncess rate. All of the multi-track and MIDI ﬁles were re-\nsampled to plus and minus one to six semitones, and ad-\nditional classiﬁers trained on the resampled audio were\ntested on the ISMIR 2004 test set using the best perform-\ning normalization scheme. Figure 3 displays the classiﬁ-\ncation success rate as the amount of re-sampled training\ndata is varied from ±1. . .6semitones.\nTheinclusionofthere-sampledtrainingdataimproves\nclassiﬁcation accuracy over 5%. In Figure 2, the pitched\nframe transcription success rates are displayed for the\nSVM classiﬁer trained using the resampled audio com-\npared with best-performing system from the 20 test sam-\nples from the 2004 evaluation, where the pitch estimates\nhave been time shifted in order to maximize transcription\naccuracy (Paiva et al., 2004).\n3.2 N Binary Classiﬁers\nInadditiontotheN-waymelodyclassiﬁcation,wetrained\n12 binary SVM classiﬁers representing one octave of the\nnotesofawesternscale(the‘chroma’classes). Theclassi-\nﬁers were trained on all occurrences of the given chroma\nand an equal number of randomly selected negative in-\nstances. We took the distance-to-classiﬁer-boundary hy-\nperplane margins as a rough equivalent to a log-posterior\nprobability for each of these classes; Figure 4 shows an\nexample ‘posteriorgram’, showing the variation in the ac-\ntivation of these 12 different classiﬁers as a function of\ntime for two examples; the ground truth labels are over-\nlaid on top. For the simple melody in the top pane, we\ncan see that the system is performing well; for the female\noperaexampleinthelowerpane,oursystem’sunfamiliar-\nity with the data is very apparent.\n1645 10 15 20 25\n -10 -50510daisy4\ntime / sec\nopera_fem2\ntime / secA#\nCDEF#G#AB\nC#D#FGA#\nCDEF#G#AB\nC#D#FGFigure 4: ‘Posteriorgram’ showing the temporal variation in distance-to-classiﬁer boundary for 12 classiﬁers trained on\nthe different notes of the octave. Ground-truth labels are plotted with dots. Top pane is a well-performing simple melody\nexample. Bottom pane is a poorly-performing female opera excerpt.\n4 DISCUSSION AND CONCLUSIONS\nLooking ﬁrst at table 1, the most obvious result is that\nallthefeatures,withtheexceptionof‘NormAutoco’,per-\nform much the same, with a slight edge for the 51-point\nacross-frequency local-mean-and-variance normalization.\nIn a sense this is not surprising since they all contain\nlargely equivalent information, but it also raises the ques-\ntion as to how effective our normalization (and hence\nthe system generalization) has been (although note that\nthe biggest difference between ‘Multi-Track’ and ‘MIDI’\ndata,whichissomemeasureofgeneralizationfailure,oc-\ncurs for the ﬁrst row, the STFT features normalized only\nbyglobalmaximum). Itmaybethatabetternormalization\nscheme remains to be discovered.\nLooking across the columns in the table, we see that\nthemorerealisticmulti-trackdatadoesformabettertrain-\ning set than the MIDI syntheses, which have much lower\nacousticsimilaritytomostoftheevaluationexcerpts. Us-\ning both, and hence a more diverse training set, always\ngives a signiﬁcant accuracy boost – up to 10% absolute\nimprovement, seen for the best-performing 51-point nor-\nmalized features. We can assume that training on addi-\ntional diverse data (particularly, say, opera) would furtherimprove performance on this evaluation set.\nAs shown in ﬁgure 2, our classiﬁer-based system is\ncompetitive with the best-performing system from the\n2004 evaluation, and is a few percent better on average.\nThisresultmustalsobeconsideredinlightofthefactthat\nthereisnopost-processingappliedinthissystem. Instead,\nthe performance represents scoring the raw, independent\nclassiﬁcation of each audio frame. Various smoothing,\ncleaning-up,andoutlierremovaltechniques,rangingfrom\nsimple median ﬁltering through to sophisticated models\nofmusicalexpectancy,aretypicallyemployedtoimprove\nupon raw pitch estimates from the underlying acoustic\nmodel.\nThis is the basis for our interest in the multiple par-\nallel classiﬁers as illustrated in ﬁgure 4. By representing\nthe outcome of the acoustic model as a probabilistic dis-\ntribution across different notes, this front end can be ef-\nﬁciently integrated with a back-end based on probabilis-\ntic inference. In particular, we are investigating trained\nmodels of likely note sequences, starting from melodies\nextracted from the plentiful MIDI ﬁles mentioned above.\nWearefurtherinterestedinhidden-modemodelsthatcan,\nfor instance, learn and recognize the importance of latent\n165constraintssuchasthelocalkeyor‘mode’impliedbythe\nmelody, and automatically incorporate these constraints\ninto melody, just as is done explicitly in Ryyn ¨anen and\nKlapuri (2004).\nWe note that our worst performance was on the\n“opera” samples, particularly the female opera, where,\nas noted above, some of the notes were outside the\nrange covered by our training set (and thus could never\nbe reported by our classiﬁer). While this highlights a\nstrength of model-based transcription in comparison with\nour example-based classiﬁer (since they directly general-\nize across pitch), there is a natural compromise possible:\nby resampling our training audio by factors correspond-\ning to plus or minus a few semitones, and using these\n‘transposed’ versions as additional training data (with the\nground-truth labels suitably offset), we can ‘teach’ our\nclassiﬁer that a simple spectral shift of a single spectrum\ncorresponds to a note change, just as is implicit in model-\nbased systems.\nBy the same token, we may ask what the trained clas-\nsiﬁer might learn beyond what a model-based system al-\nready knows, as it were. By training on all examples of\na particular note in situ, the classiﬁer transcriber can ob-\nserve not only the prominent harmonics in the spectrum\n(or autocorrelation) corresponding to the target pitch, but\nany statistical regularities in the accompaniment (such as\nthemostlikelyaccompanyingnotes). Lookingatﬁgure4,\nforexampleattheﬁnalnoteofthetoppane,weseethatal-\nthough the actual note was a B, the classiﬁer is confusing\nit with a G – presumably because there were a number of\ntraining instances where a melody G included strong har-\nmonics from an accompanying B, which could in some\ncircumstances be a useful regularity to have learned. In-\ndeed, noting that our current classiﬁers seem to saturate\nwith only a few seconds of training material, we might\nconsider a way to train a more complex classiﬁer by in-\ncluding richer conditioning inputs; the inferred ‘mode’\nhidden state suggested aboveis an obvious contender.\nThe full melody competition involved not only decid-\ningthenoteofframeswherethemainmelodywasdeemed\nto be active, but also discriminating between melody and\nnon-melody (accompaniment) frames, on the face of it a\nvery difﬁcult problem. It is, however, a natural ﬁt for a\nclassiﬁer: once we have our labeled ground truth, we can\ntrain a separate classiﬁer (or a new output in our existing\nclassiﬁer)toindicatewhenbackgroundisdetectedandno\nmelodynoteshouldbeemitted;differentfeatures(includ-\ning overall energy) and different normalization schemes\nare appropriate for this decision.\nIn summary, we have shown that the novel approach\nto melody transcription in which essentially everything\nis left to the learning algorithm and no substantial prior\nknowledge of the structure of musical pitch is hard-coded\nin, is feasible, competitive, and straightforward to imple-\nment. Thebiggestchallengeisobtainingthetrainingdata,\nalthough in our conﬁguration the amount of data required\nwasnotexcessive. Westressthatthisisonlytheﬁrststage\nof a more complete music transcription system, one that\nwe aim to build at each level on the principle of learning\nfrom examples of music rather than through coded-in ex-\npert knowledge.ACKNOWLEDGEMENTS\nMany thanks to Emilia G ´omez, Beesuan Ong, and Sebas-\ntian Streich for organizing the 2004 ISMIR Melody Con-\ntest, and for making the results available. This work was\nsupported by the Columbia Academic Quality Fund, and\nby the National Science Foundation (NSF) under Grant\nNo. IIS-0238301. Anyopinions,ﬁndingsandconclusions\nor recommendations expressed in this material are those\nof the authors and do not necessarily reﬂect the views of\nthe NSF.\nREFERENCES\nA. de Cheveigne and H. Kawahara. Yin, a fundamen-\ntal frequency estimator for speech and music. Journal\nAcoustic Society of America , 111(4):1917–1930, 2002.\nJ. Eggink and G. J. Brown. Extracting melody lines from\ncomplex audio. In Proc. Int. Conf. on Music Info. Re-\ntrieval ISMIR-03 , pages 84–91, 2004.\nE. Gomez, B. Ong, and S. Streich. Ismir 2004 melody\nextraction competition contest deﬁnition page, 2004.\nhttp://ismir2004.ismir.net/melody_\ncontest/results.html .\nM. Goto. A predominant-f0 estimation method for poly-\nphonic musical audio signals. In 18th International\nCongress on Acoustics , pages 1085–1088, 2004.\nY. Li and D. Wang. Detecting pitch of singing voice in\npolyphonic audio. In IEEE International Conference\non Acoustics, Speech, and Signal Processing , pages\nIII.17–21, 2005.\nM. Marolt. On ﬁnding melodic lines in audio recordings.\nInDAFx, 2004.\nR. P. Paiva, T. Mendes, and A. Cardoso. A methodology\nfordetectionofmelodyinpolyphonicmusicsignals. In\n116th AES Convention , 2004.\nJ.Platt. Fasttrainingofsupportvectormachinesusingse-\nquentialminimaloptimization.InB.Scholkopf,C.J.C.\nBurges, and A. J. Smola, editors, Advances in Kernel\nMethods – Support Vector Learning , pages 185–208.\nMIT Press, Cambridge, MA,1998.\nM. P. Ryyn ¨anen and A. P. Klapuri. Modelling of\nnote events for singing transcription. In Proc.\nISCA Tutorial and Research Workshop on Statisti-\ncal and Perceptual Audio Processing , Jeju, Korea,\nOctober 2004. URL http://www.cs.tut.fi/\n˜mryynane/mryynane_final_sapa04.pdf .\nR. J. Turetsky and D. P. Ellis. Ground-truth transcrip-\ntions of real music from force-aligned midi syntheses.\nInProc. Int. Conf. on Music Info. Retrieval ISMIR-03 ,\n2003.\nI. H. Witten and E. Frank. Data Mining: Practical ma-\nchine learning tools with Java implementations . Mor-\ngan Kaufmann, San Francisco, CA, USA, 2000. ISBN\n1-55860-552-5.\n166"
    },
    {
        "title": "A Graphical Model for Recognizing Sung Melodies.",
        "author": [
            "Christopher Raphael"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417052",
        "url": "https://doi.org/10.5281/zenodo.1417052",
        "ee": "https://zenodo.org/records/1417052/files/Raphael05.pdf",
        "abstract": "A method is presented for automatic transcription of sung melodic fragments to score-like representation, including metric values and pitch. A joint model for pitch, rhythm, segmentation, and tempo is defined for a sung fragment. We then discuss the identification of the globally optimal musical transcription, given the observed audio data. A post process estimates the location of the tonic, so the transcription can be presented into they key of C. Experimental results are presented for a small test collection. Keywords: monophonic music recognition, graphical models 1",
        "zenodo_id": 1417052,
        "dblp_key": "conf/ismir/Raphael05",
        "keywords": [
            "Sung melody recognition",
            "Automatic transcription",
            "Musical score representation",
            "Pitch",
            "Rhythm",
            "Segmentation",
            "Tempo",
            "Graphical models",
            "Monophonic music recognition",
            "Musical transcription",
            "Tonic estimation",
            "Audio data"
        ],
        "content": "AGraphical Model forRecognizing Sung Melodies\nChristopher Raphael\nSchool ofInformatics\nIndiana Univ.\nBloomington, IN47408\ncraphael@indiana.edu\nABSTRA CT\nAmethod ispresented forautomatic transcription ofsung\nmelodic fragments toscore-lik erepresentation, including\nmetric values andpitch. Ajoint model forpitch, rhythm,\nsegmentation, andtempo isde\u0002ned forasung fragment.\nWethen discuss theidenti\u0002cation oftheglobally optimal\nmusical transcription, giventheobserv edaudio data. A\npost process estimates thelocation ofthetonic, sothe\ntranscription canbepresented intotheykeyofC.Experi-\nmental results arepresented forasmall testcollection.\nKeywords: monophonic music recognition, graphical\nmodels\n1INTR ODUCTION\nThe problem ofautomatic transcription ofsung melodic\nfragments needs little justi\u0002cation ormotivation within\nthemusic information retrie valcommunity ,since some\nform ofthisproblem isthe\u0002rst step inanyquery-by-\nhumming-type system. This community contains quite a\nfewefforts thatdescribe thisrecognition problem invar-\nious levelsofdetail including McNab etal.(1996), Haus\nand Pollastri (2001), Meek and Birhmingham (2002),\nPauws (2002), Song etal.(2002), Clarisse etal.(2002),\nPardo etal.(2002). Singing recognition hasother appli-\ncations such asthepreserv ation ofunnotated vocal mu-\nsictraditions andforspeech-recognition-lik einterf aces to\nmusic notation softw are. Wealso \u0002nd signi\u0002cant intel-\nlectual merit inthisproblem, independent ofanyapplica-\ntions, with itsdeep tiestohuman cognition andtheasso-\nciated modeling andcomputational challenges.\nMusic isanunusually organized andrule-bound do-\nmain when compared toother recognition domains such\nasspeech orvision. Insuch adomain weareparticularly\ninclined touseOckham' srazor asaguiding principle \ngiventwohypotheses thatexplain thedata equally well,\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonwebelie vethesimpler onetobemore likely.Wefeel\nthiscriterion isparticularly appropriate formusic since, it\nseems tobeconsistent with human perception ofmusic,\nwhile itisoften straightforw ardtoformalize thenotion of\nsimplicity formusical hypotheses. Theidea ofOckham' s\nrazor isthoroughly embedded inmuch literature onrecog-\nnition, including that inthemusic information retrie val\ncommunity ,and isoften implemented through explicit\npenalty terms inoptimization formulations orthrough the\nuseprior distrib utions inprobabilistic models. Exam-\nples ofexplicit penalties within theMIR community are\nDixon (2001), Scheirer (1998) andGoto (2004) while ex-\namples ofmodel-based penalties areRaphael andStod-\ndard (2003), Cemgil andKappen (2003), andAbdallah\nandPluble y(2004).\nSome notions ofsimplicity canbedescribed without\nanyknowledge ofthedeeper structure ofmusic. For\ninstance, asung fragment ispresumably composed of\nnotes having fundamental frequencies that, givenatun-\ningreference, arepitches inthechromatic scale. Weex-\npect comparati velyfewnotes inasung fragment, soahy-\npothesis thatexplains each frame ofaudio astheclosest\nchromatic pitch isapttoexplain theobserv edaudio data\nwell, butproduce anunrealistically comple xhypothesis.\nOntheother hand, ahypothesis thatgroups contiguous\nregions ofsimilar frames intonotes willproduce simpler\nhypotheses andisjusti\u0002able, evenifthenotes aresome-\nwhat further from theactual audio data.\nAllpractitioners ofmachine recognition arelikelyto\nagree with thisanalysis sofar,buttheartofmodeling lies,\ninlargemeasure, indeciding howfartoextend theidea.\nContinuing with thesame example, thesegmentation of\nthedata intonotes canbeaccomplished more accurately\nwhen thereference tuning isgiven,since then thepossible\nnote frequencies arenolonger acontinuum, butrather a\nsmall number ofdistinct andwell-separated possibilities.\nSo,clearly wearemuch better offifthetuning isknown,\nbutdoes thisjustify simultaneously estimating thetuning\naswell asthepartitioning intonotes?\nThis same question appears overandoverintherecog-\nnition ofmelodic segments. Forinstance, ifweknow\nthekeyofthefragment, thelikelihoods ofvarious pitches\nchanges dramatically ,strongly favoring notes inthescale\nofthatkey.Does thisjustify simultaneously estimating\nthekey?\nThehuman' spartitioning ofaudio data intonotes usu-\n658allyoccurs within arhythmic frame workinwhich inter-\nonset times aresimple proportions ofoneanother .While\nitispossible topartition audio data into notes justusing\npitch information, understanding theaverage length ofthe\nbasic time unit, saybeat ormeasure, allowsustocapital-\nizeonthebasic rhythmic structure ofmusic. Does this\nadded knowledge justify thesimultaneous estimation of\nbeat length ortempo? Aswith pitch, there isconsider -\nably more rhythmic structure tomusic than thenotion of\nsimple proportions. Typically ,music exists within ameter\nimplying rather strong assumptions about howmeasures\ndivide intonotes. Should weincur thecomputational bur-\ndenofsimultaneous estimation ofmeter toincrease the\ndiscriminating powerofthemodel? Certainly there are\nother examples ofthisbasic question.\nInsome conte xts,thegoal ofrecognition might beto\nlearn these higher levelconstructs such askey,tempo, and\nmeter .Inthese cases, itseems wehavenochoice other\nthan including theconstructs into themodel. However,\nevenifweonly desire asegmentation intonotes, webe-\nlievethere issigni\u0002cant bene\u0002t tomodeling these nui-\nsance parameters. People tend tobequite categorical in\ntheir perceptions ofmusic: Interv alsareheard distinctly as\nmajor thirds, octaves,etc.,evenwhen thefrequencies are\nnotcompletely consistent. Similarly wetend hear rhyth-\nmicrelations with de\u0002niteness evenwhen notcompletely\nsupported bytheliteral data. Forinstance, thisnote lies\nonthedownbeat andthisother istwice aslong asthe\u0002rst.\nWebelie veitisthesimultaneous existence oftempo, me-\nter,key,harmon y,phrase structure, motivicstructure, and\ntheir interrelations, such asharmonic rhythm, thatbrings\nabout thiscategorical perception. That is,within thecon-\ntextofthese higher levelconstructs, thehuman will be-\nlievenoother data interpretation mak essense. Forthis\nreason, webelie vethatmodels including deeper levelsof\nstructure such askey,meter ,andharmonic analysis, (even\ninmonophonic fragments) havemuch greater power to\ndiscriminate accurately ,evenwhen thehigher levelcon-\nstructs arenotofinterest.\nWehavesuggested abovethat simultaneous estima-\ntionofthese higher levelconstructs istheonly alternati ve\ntosimply forgetting about them, and, ofcourse, thisisnot\nthecase. Our bias forsimultaneous estimation isthatit\ncircumv entsthechick enandeggproblem. Forinstance,\nonecan'treally estimate note value (quarter ,eighth, etc.)\naccurately without having anotion oftempo andvice-\nversa. Ingeneral, simultaneous estimation ispreferable\nwhen thejoint knowledge ofparameters leads toamuch\nmore de\u0002nite data model than either parameter inisola-\ntion. Forinstance, scale degree andtuning standard com-\nbine togiveade\u0002nite expectation ofobserv edfrequenc y\nthatcan'tberealized without both parameters. Insome\ncases itmight bepossible tobootstrap one'swayup,\nadding more sophisticated structure toourinterpretation\nwith aseries ofsuccessi verecognition passes. When there\nisnochick en-and-e ggproblem, weareinfavorofthisap-\nproach, inspite ofitsmessiness, andgiveanexample in\nthispaper .\nThis workshould beviewed, inpart, asanexploration\nofthese ideas. Wetrytoformulate themaximum amount\nofmusically relevantinformation intoourmodel thatcanbehandled insimultaneous estimation. After thefact,we\ntrytodisambiguate further byestimating more structure.\nWearenottrying tobuild afront endforanyparticular\nQuery-by-Humming system. While weviewtheexperi-\nmental results aspromising, webelie vethatevendeeper\nstructure willlead tostillbetter recognition asdiscussed\nlater.Our approach differs signi\u0002cantly from thework\ncited aboveinitsattempt tomodel themusic atasigni\u0002-\ncantly deeper level.Webelie vetheinformal results, while\nfarfrom perfect, support thisgeneral lineofresearch.\nSpeci\u0002cally ,theproblem weaddress isasfollows.We\ntreat sung musical fragments with knowntime signature\nandmode: 3/4time andmajor mode with ade\u0002ned listof\npossible measure positions inourexperiments. Wesimul-\ntaneously estimate thepartition ofaudio data into notes,\nandthelabeling ofthenotes with pitches andrhythmic\nvalues that makesense within themetric conte xt. We\nalso simultaneously estimate a(potentially) time-v arying\ntempo process. The scheme wepropose iscapable of\nidentifying theglobally most likelycon\u0002guration ofthese\nparameters, giventheaudio data. Inapost-processing\nphase wefurther estimate thefrequenc yofthetonic and\nrelabel therecognized pitches within thisconte xt.This\n\u0002xessome pitch errors andallowsustopresent allofthe\nrecognized results automatically transposed tothekeyof\nCmajor .\n2THE MODEL\nWeassume thattheaudio fragment toberecognized hasa\nknowntime signature. While thisassumption iscertainly\nunrealistic forsome examples, theaudio recognition prob-\nlem isdif\u0002cult enough towarrant some simplifying as-\nsumptions. Wefurther assume thepossible rhythm posi-\ntions areenumerated inasetRandmodel thesequence\nofnote onset positions asaMark ovchain.\nTobemore speci\u0002c, suppose thefragment isin3/4\ntime andthat only note onsets beginning ateighth-note\npositions aredeemed possible. Then thepossible onset\npositions aredescribed bytheset\nR=fstart;0\n6;1\n6;2\n6;3\n6;4\n6;5\n6;tie;endg\nWemodel thesequence ofmeasure positions byaMark ov\nChain, R0;R1;:::;RKwhere Rk2Rthatmust beginin\nthestart state andendintheendstate. Thus weassume\naninitial distrib utionP(R0=start) =1andtransition\nprobability matrix\nP(Rk+1=rk+1jRk=rk)=Q(rk;rk+1)\nThetieelement corresponds toanote that istied over\nfrom thecurrent measure tothebeginning ofthenext\nmeasure andcanthus beconsidered another version of\nthebarlineposition. Adding thiselement tooursetof\npossible states allowsustomodel arbitrarily long notes\nwithout signi\u0002cantly increasing thesizeofthestate space.\nWeconstrain thetransitions sothatQ(start;start)=\nQ(start;tie)=0andQ(rk;rk+1)=0whenJ(rk+1)<\nI(rk)where\nJ(r)=\u001a\n1ifr=tie\nrotherwise\n659I(r)=\u001a\n0ifr=tie\nrotherwise\nandrk;rk+162fstart;endg.This simply states that a\nnote cannot cross thebarlinewithout using thetiestate,\nasinusual musical notation. The chain generates mea-\nsure positions Rkuntil wereach theendstate; wewrite\nRK=endsothatKistheindexofthe\u0002nal state. The\nmodeling allowstherhythm tobeunambiguously recon-\nstructed from thesequence ofstates. Forinstance, the\nsequence start;2\n6;tie;3\n6;0\n1;endcorresponds toarhythm\nbeginning onthe2nd quarter ofthemeasure which is\ntied overtoadotted quarter inthenextmeasure fol-\nlowed byanother dotted quarter andending with anote\nonthedownbeat ofthefollowing measure. Wewillwrite\nR=(R0;:::;RK)andr=(r0;:::;rK),andsimilarly\nforother vectors, forthecollection ofallrhythm variables\nandtheir actual values. Due totheMark ovassumption,\nP(R=r)factors as\nP(R=r)=KY\nk=1Q(rk;rk+1)\nforsequences rstarting inthestart position and end-\ningintheendposition. Each transition, notincluding\nthestart andend states hasanunambiguous amount of\nmusical time, inmeasures, ittraverses, which wedenote\nl(rk;rk+1)=J(rk+1)\u0000I(rk).\nAssociated with each measure position Rkisapitch\nvariable Pk2P=frest;plo:::;phiggiving either arest\northeMIDI pitch ofthenote thatissung during Rkto\nRk+1.Without akeyasreference itisdif\u0002cult togive\naprobability distrib ution forthepitches. However,ifwe\nknewthetonic, wecould design areasonably informati ve\ndistrib ution onpitches. Inour\u0002rststage ofrecognition we\nassume auniform distrib ution onpitches. Inalater re\u0002ne-\nment wewillestimate thelocation ofthetonic andusea\nmore re\u0002ned pitch model. Inboth cases weuseabag of\nnotes model, meaning thepitches areindependent draws\nfrom some \u0002xedpitch distrib ution. WewriteB(pk)for\nthepitch distrib ution.\nUnlik ethemodel forthemeasure positions and\npitches, which arediscrete, wemodel thesequence ofon-\nsettimes forthenotes asaGaussian process. Forsimplic-\nityofnotation, weprefer tomeasure time interms ofanal-\nysisframes ,which are\u0001-second-long sequences ofaudio\nsamples onwhich wecompute theFFT.LetS1;:::;SK\u00001\nbethelocal tempo variables, giveninframes permeasure,\nandde\u0002ne T1;:::;TK\u00001tobethesequence ofactual note\nonset times, inframes. Wemodel these variables jointly\nby\nSk=Sk\u00001+\u001bk (1)\nTk=Tk\u00001+l(Rk\u00001;Rk)Sk+\u001ck (2)\nfork=2;:::;K\u00001.Thef\u001bk;\u001ckgvariables are0-mean\nandGaussian sotheSprocess canbeseen tobearan-\ndom walk. This model hasbeen used inRaphael (2004)\nandCemgil (2004). Ifthef\u001ckgvariables were 0then the\nnote onset times would evolveexactly aspredicted bythe\nnote lengths andtempo. The addition ofthe\u001cvariables\nadds robustness tothemodel byallowing small deviationsfrom what ispredicted bythetempo andnote length. The\nrhythm-conditional density forthetempo andonset vari-\nables isthen\np(s;tjr)=N(s1;\u0016S1;\u001b2\nS1)N(t1j0;\u001b2\nT1) (3)\n\u0002KY\nk=2N(sk;sk\u00001;\u001b2\nSk) (4)\n\u0002KY\nk=2N(tk;tk\u00001+l(rk\u00001;rk)sk;\u001b2\nTk)(5)\nwhere N(\u0001;\u0016;\u001b2)isthenormal density function with\nmean \u0016andvariance \u001b2.The variances f\u001b2\nSk;\u001b2\nTkgcan\nbeallowed todepend ontheamount ofmusical time tra-\nversed bythetransitions, since, presumably ,longer notes\nallowforlargerincrements intempo andgreater devia-\ntions from theexpected length.\n0 2 4 6 8 100 5 10 15 20 25\nfreqspectral energy\nFigure 1:The distrib ution which generates thespectral\nbits.\nFinally ,letY1;:::;YNdenote theframes ofaudio data\neach accounting for\u0001seconds. Ifthenote onsets are\u0002xed\n(T=t)then these frames arepartitioned into contigu-\noussegments corresponding tothenotes ofthefragment.\nInparticular ,each frame, n,liesinsegment k(n)where\ntk(n)\u0014n<tk(n)+1.Weconnect ourhidden variables to\nthedatabyassuming thattheY1;:::;YNareconditionally\nindependent givenT=tandP=psothat\np(yjt;p)=NY\nn=1p(ynj\u0019(t;p;n))\nwhere \u0019(t;p;n)isthepitch being sung atframe n.That\nis\u0019(t;p;n)=pk(n).\nTobespeci\u0002c, if\u0019=\u0019(t;p;n)isthepitch being sung,\nwede\u0002ne theidealized powerspectrum, f\u0019,asasuperpo-\nsition ofpeaks centered attheharmonics ofpitch\u0019asin\nFigure 1.f\u0019isassumed tobenormalized tosum tounity .\nInde\u0002ning ourdata model wetreat theobserv edpower\n660spectrum inframe n,ynasahistogram ofasample from\ntheprobability distrib utionf\u0019.That is\np(ynj\u0019)=cY\n!f\u0019(!)yn(!):\nInthecase inwhich thepitch isarest, wetakef\u0019=rest\ntobeauniform model\nPutting thisalltogether givesafactorization ofour\nmodel as\np(r;p;t;s;y)=p(r)p(p)p(sjr)p(tjr;s)p(yjp;t)(6)\n=KY\nk=1Q(rk;rk+1)B(rk) (7)\n\u0002N(s1;\u0016S1;\u001b2\nS1)N(t1j0;\u001b2\nT1)\n\u0002KY\nk=2N(tk;tk\u00001+l(rk\u00001;rk)sk;\u001b2\nTk)\n\u0002KY\nk=2N(sk;sk\u00001;\u001b2\nSk)\n\u0002NY\nn=1p(ynj\u0019(t;p;n))\nyp\nr\ns\nt\npi...\n...\nFigure 2:Description ofthemodel asadirected acyclic\ngraph. Thetopsection ofthemodel represents, from top\ntobottom, pitch (p),rhythm (r),tempo (s),andonset\ntimes (t).Thebottom section ofthemodel givesthecondi-\ntional distrib ution oftheaudio data (y),giventhelabeling\noftheframes (\u0019).Since thelabeling \u0019canbedeterminis-\ntically derivedfromt;p,wede\u0002ne amodel p(r;p;t;s;y)\nAgraphical depiction ofthemodel isgiveninFigure\n2.\n3FINDING THE GLOB ALMAP\nCONFIGURA TION\nArather surprising factisthat, givenourspectrogram\ndatay,theglobally optimal con\u0002guration ofther;p;t;s,\n(rhythm, pitch, onset times, tempo) sequences canbecomputed using avariant ofdynamic programming, under\nreasonable assumptions. Wediscuss here thecomputation\nofthisglobal optimum\n(^r;^p;^t;^s)=argmax\nr;p;t;sp(r;p;t;sjy)\n=argmax\nr;p;t;sp(r;p;t;s;y)\nOur approach istoconstruct atreethat, inprinciple,\naccounts forallpossible con\u0002gurations ofther;p;t;sse-\nquences. Inconstructing thistreethecontinuously-v alued\nnote onset times, t,areonly considered toonly takeinte-\ngralvaluestk2f0;1;:::;(N\u00001)g.Amore fastidious\ndescription ofthemodel oftheprevious section would\nhavenoted thattheonset variables ofEqns. 4and5are\nnotactually normal, butrather adiscrete approximation\nofnormal evaluated only attheintegers andfurther con-\nstrained sothat0\u0014t1<t2<:::;<tK\u0014N\u00001.\nA\u0002rstobserv ation isthat, since there isnodependence\namong ourpitch variables, p1;:::;pK\u00001,then givena\ncon\u0002guration ofonset timest1;:::;tK\u00001,themost likely\ncon\u0002guration ofpitches issimple tocompute. Forin-\nstance, thevaluest1;t2specify thatthere isanote that\nbegins atframe t1andends att2(aslong asr16=tie).\nThus theoptimal pitch associated with thisregion must be\n^p1=argmax\n\u00192Pt2\u00001Y\nn=t1p(ynj\u0019)\nThus \u0002xing note boundaries automatically \u0002xestheopti-\nmalchoice ofpitches, sowewillleavethepitch variables\noutoftheconstruction ofourtreesince theycanbein-\nferred from theonset frames. Thecomputation thatasso-\nciates everypossible sequence offrames with anoptimal\npitch canbeperformed before webegintheconstruction\nofourtree.\n0/60/6\n0/6\n0/6 5/6\n5/65/65/6\n0/6−−\n−−\n−−−−\n... ... ......\nFigure 3:Thetreedescribing thepossible evolution ofall\nrhythm sequences andpartitions oftheaudio data.\nThetreeisconstructed byspecifying therhythm vari-\nable fromRforeach frame ofaudio data. The\u0002rstframe\n661islabeled with thevaluestart Ateach lowerlevelinthe\ntreewecaneither remain inthecurrent note, theupper\nbranch inFigure 3,orwecanmoveontoanewnote and\nchoose anewvaluefromR,thelowerbranches inthetree.\nItisimportant toobserv ethat, while thetreeonly speci\u0002es\nthepossible sequences ofR,other information isimplic-\nitlyspeci\u0002ed. First ofall,apartial path inthistree\u0002xes\ntheframes atwhich rhythm transitions takeplace, there-\nfore \u0002xing the\u0002rst severalvalues ofT.Furthermore, as\nnoted above,\u0002xing thenote transition frames implies \u0002x-\ningtheoptimal choices ofthepitch variables P.Thus,\ngivenouraudio dataY,theonly variables that arenot\n\u0002xedbychoosing atreebranch arethelocal tempo vari-\nables, S.\nSuppose weconsider abranch ofthetreeatdepth n,\ntherefore apossible explanation ofthe\u0002rstnframes of\ndatayn\n1=y1;:::;yn.Suppose that inthisbranch the\nkthnote begins onthenthframe. Thus theaudio datayn\n1\naccounts forthevariables rk\n1;pk\u00001\n1;tk\n1;sk\n1.Examination\nofEqn. 7showsthatp(rk\n1;pk\u00001\n1;tk\n1;sk\n1;yn\n1)isaproduct\nofconstants andGaussian density functions. Thus this\nprobability canbeexpressed astheexponential ofsome\nquadratic function ofthetk\n1andsk\n1variables. Itiswell-\nknownthat ifonemaximizes aquadratic function over\nseveralofthevariables, theresult isquadratic inthere-\nmaining variables. Thus\nmax\ntk\n1;sk\u00001\n1p(rk\n1;pk\n1;tk\n1;sk\n1;yn\n1)=he\u00001\n2(sk\u0000m)2=v\ndef=K(sk;h;m;v)\nThe details ofhowthismaximization areperformed are\nsome what involvedandcanpotentially distract onefrom\nthesimple observ ation thatthecomputation canbeper-\nformed inclosed form. Details arediscussed inRaphael\n(2002) forasimilar problem andmodel.\nTheabovemaximization givestheoptimal probability\nofthebranch asafunction ofthecurrent tempo. Wewill\nstore thisfunction ateverybranch. Infact,itisrelati vely\neasy tocompute thefunction recursi velyfrom theparent\nbranch. Inparticular if^pb(s)istheoptimal probability of\nthecurrent branch basafunction ofthecurrent tempo s,\nThen forachild branch b0,wehave\n^pb0(s)=^pb(s)\nwhen nonote transition takesplace between bandb0.Oth-\nerwise, ifanote transition takesplace atlevelnofthetree,\nwemovefrom rhythm position rtor0,from thelastnote\nonset timettothecurrent timet0=n,andfrom thelast\n(unkno wn)tempo, stothecurrent tempo s0by\n^pb0(s)=max\ns^pb(s)Q(r;r0)B(^\u0019)\n\u0002p(s0js)\n\u0002p(t0js0;t)\n\u0002t0Y\n\u0017=tp(y\u0017j^\u0019)\nwhere ^\u0019istheoptimal pitch fortheinterv al(t;t0).\nAtthispoint weseem tobefaced with anexponen-\ntially growing tree, making theaboveprocess impossibleFigure 4:Left: Thefunctions f^p\f(s)gbefore Right: The\nreduced collection offunctions after thinning.\ntocontinue formore than afewlevelsofthetree. The\nsurprising factisthatthetreecanbepruned toatinyfrac-\ntion ofitsoriginal size with noloss ofoptimality ,using\ndynamic programming.\nSuppose wedenote thecollection ofbranches thatbe-\nginanewnote\u001a2RatlevelnofthetreebyB(\u001a;n).If\nforoneofthese paths, b2B(\u001a;n),\n^pb(s)\u0014max\n\f2B(\u001a;n)^p\f(s)\nforalls,there isnohope ofbbeing apre\u0002x totheoptimal\npath, since forallvalues ofthecurrent state(\u001a;n;s)some\nother path hasahigher optimal probability .Thus wecan\nprune bwith nolossofoptimality .Werefer tothisopera-\ntionasthethinning operation, graphically depicted inFig-\nure4andwriteThin(B(\u001a;n))forthesurvi ving branches.\nItiseasy toshowthatthinning canbeperformed with a\ncomputational comple xitythatisquadratic inthenumber\noforiginal branches.\nWecontinue theconstruction ofthistreewith thinning\nuntil wereach levelN.atthispoint itiseasy to\u0002ndthe\nsurvi ving branch with thebestoptimal probability ending\nwith\u001a=end.This willbetheglobally optimal path and\nwecantrace itshistory back totheroot.\n4EXPERIMENTS\nWenowdescribe experiments with theanalysis method\ndescribed above.Ourgoal inconducting thisresearch is\ntoexamine theproblem ofmonophonic recognition from\nadeeper structural levelthan haspreviously done. Inpar-\nticular ,wewish toseeiftheimposition ofbasic musi-\ncalknowledge canbeanaidtotherecognition process,\nrather than todevelop thebest front end toaQuery-\nby-Humming system. Thus, theexperiments serveasa\ncourse check, rather than aformal evaluation, andare\nwell-suited totheexploratory nature ofthiswork.\nWecollected asmall testsetofsimple melodies in\n3/4time, allinmajor mode, sung bymale voices. The\nmelodies were sung byanon-random subset oftheau-\nthor' snetw orkofacquaintances. Severaloftheexamples\narechoruses ofmale voices. The testsetcontained a\ntotal of15sound \u0002les. Ourintention wastorestrict ourat-\ntention tothecases inwhich themusical content isunam-\nbiguous tothehuman listener .Webelie vethese cleaner\nexamples constitute themost interesting subset since the\nhuman isrelati velycertain ofthecorrect hypothesis, while\n662theexamples stillpose considerable problems forrecog-\nnition. Thus these examples arewell-suited forastudy of\ntherelation between knowledge representation andrecog-\nnition results.\nOne impro vement overthebasic model wepursued\nconcerns theroleofthekeyoftheexcerpt. Inthe\u0002rstpass\nofouralgorithm weuseapitch model that givesequal\nprobability toallchromatic pitches assuming anarbitrary\nchoice oftuning. Notknowing thekeyleavesreally no\nother reasonable choice. Evenwith what must beanocca-\nsionally inaccurate choice oftuning, ouralgorithm often\ndoes areasonable jobofsegmenting thedata into notes\nandascribing rhythm. Ina\u0002nal phase, wecorrect the\npitches bythefollowing method.\nWebeginwith amodel forpitch distrib ution assuming\nthekeyofCmajor .This model isnotestimated from data,\nbutsimply assumes thatthenotes inthetonic triad arethe\nmost likely,thenotes inthescale arethe2ndmost likely,\nandtheremaining black notes aretheleast likely.We\nconsider thedata likelihood, assuming thegivennote seg-\nmentation, using 24quarter -steps candidates forthetonic.\nForeach tonic location welabel each pitch with thechoice\nthatmaximizes thepitch likelihood times thedata likeli-\nhood. This hastheeffectofnudging ambiguous pitches\ntowardplausible notes inthekey.Wechoose thetonic lo-\ncation thatmaximizes thislikelihood overallofthedata,\nandcallthetonicC.Thus allexamples areautomatically\ntransposed toCmajor ,nomatter where theyaresung.\nThis method provesquite effectiveandidenti\u0002es thecor-\nrectkeyinallcases butthe1stexample ofItCame upon\naMidnight Clear .Asithappens, the\u0002rst phrase ofthe\ncarol does notcontain the4thscale degree, thus making\nFareasonable (oratleast reasonably scoring) choice for\nthetonic. Inaddition tosupplying useful information, the\nestimation ofthetonic helps tocorrect notes whose actual\nfrequencies areambiguously placed. This isanexample\nofhowmodeling ofdeeper musical structure canimpro ve\nrecognition results.\nAnumber oftherecognized examples incorrectly es-\ntimated thetempo byafactor ortwoorthree. The for-\nmercase amounts torepresenting themusic in6/8rather\nthan 3/4with a6/8measure account fortwo3/4measures.\nThis error isnearly inevitable atourcurrent stage, since\nthedistinction between these twometers requires avery\ndeep musical understanding which goes beyond thatrep-\nresented inourcurrent model. Theoneexample, Daisy ,\nwhose tempo wasoffbyafactor ofthree ismore puz-\nzling. Wesuspect that early intherecognition process\nbranches were mistak enly pruned thataccount forthecor-\nrecttempo.\nSeveral oftheexamples, Happ yBirthday ,God\nSavetheQueen, andSilv erBells were recognized as\nshifted versions ofthecorrect one. Thedistinction be-\ntween these metrical shifts isalso asubtle one, butitis\ndemonstrably onethatourmodel makescorrectly most of\nthetime.\nThe audio \u0002les as well as\nthe transcriptions are available at\nhttp://xa vier.informatics .indiana.edu/cr aphael/ismir05 .Refer ences\nS.Abdallah andM.Pluble y.Polyphonic music transcrip-\ntion bynon-ne gativesparse coding ofpower spectra.\nProceedings ofthe5thInternational Confer ence inMu-\nsicInformatics Retrie val,2004.\nA.T.Cemgil. Bayesian Music Transcription .PhD thesis,\nRadboud University ofNijme gen, 2004.\nA.T.Cemgil andH.J.Kappen. Monte carlo methods\nfortempo tracking andrhythm quantization. Journal of\nArti\u0002cial Intellig ence Resear ch,18,2003.\nL.P.Clarisse, L.P.Martens, M.Lesaf fre, B.DeBaets,\nH.DeMe yer,andM.Leman. Anauditory model based\ntranscriber ofsinging sequences. Proceedings ofthe\nThirdInternational Confer ence inMusic Informatics\nRetrie val,2002.\nS.Dixon. Automatic extraction oftempo andbeat from\nexpressi veperformances. Journal ofNewMusic Re-\nsearch,30(1), 2001.\nM.Goto. Anaudio-based real-time beat tracking sustem\nformusic with orwithout drum sounds. Journal ofNew\nMusic Resear ch,30(2):159171, 2004.\nG.Haus andE.Pollastri. Anaudio front endforquery-\nby-humming systems. Proceedings ofSecond Annual\nSymposium onMusic Informatics Retrie val,2001.\nJ.McNab, I.H.Witten, C.L.Henderson, andS.J.Cun-\nningham. Towards thedigital music library: Tunere-\ntreivalfrom acoustic input. Digital Libraries ,1996.\nC.Meek andW.Birhmingham. Johnn ycan'tsing: Acom-\nprehensi veerror model forsung music queries. Pro-\nceedings oftheThirdInternational Confer ence inMu-\nsicInformatics Retrie val,2002.\nB.Pardo, W.Birmingham, andJ.Shifrin. Name thattune:\nApilot study in\u0002nding amelody from asung query .\nJorunal oftheAmerican Socity forInformation Science\nandTechnolo gy,55,2002.\nS.Pauws. Cubyhum: Afully operational query-by-\nhumming system. Proceedings oftheThirdInterna-\ntional Confer ence inMusic Informatics Retrie val,2002.\nC.Raphael. Ahybrid graphical model forrhythmic pars-\ning. Arti\u0002cial Intellig ence,137(1):217238, 2002.\nC.Raphael. Ahybrid graphical model foraligning poly-\nphonic audio with musical scores. Proceedings ofthe\n5thInternational Confer ence inMusic Informatics Re-\ntrieval,2004.\nC.Raphael and J.Stoddard. Harmonic analysis with\nprobabilistic graphical models. Proceedings ofthe\nFourth International Confer ence inMusic Informatics\nRetrie val,2003.\nE.Scheirer .Tempo andbeat analysis ofacoustic musical\nsignals. J.Acoust. Soc. Am,103(1), 1998.\nJ.Song, S.Y.Bae, andK.Yoon. Mid-le velmelody rep-\nresentation ofpolyphonic audio forquery-by-humming\nsystem. Proceedings oftheThirdInternational Confer -\nence inMusic Informatics Retrie val,2002.\n663"
    },
    {
        "title": "Exploiting Musical Connections: A Proposal for Support of Work Relationships in a Digital Music Library.",
        "author": [
            "Jenn Riley"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415660",
        "url": "https://doi.org/10.5281/zenodo.1415660",
        "ee": "https://zenodo.org/records/1415660/files/Riley05.pdf",
        "abstract": "Musical works in the Western art music tradition exist in a complex, inter-related web. Works that are derivative or part of another work are common; however, most music information retrieval systems, including traditional library catalogs, don’t use these essential relationships to improve search results or provide information about them to end-users. As part of the NSF-funded Variations2 Digital Music Library project at Indiana University, we have developed a set of functional requirements defining how derivative and whole/part relationships between musical works should be acted upon in search results, and how these results should be displayed. This paper describes recent research into these relationships, provides examples why they are important in Western art music, outlines how Variations2 or any other music information retrieval system could use these relationships in matching user queries, and describes optimal displays of these relationships to end-users.",
        "zenodo_id": 1415660,
        "dblp_key": "conf/ismir/Riley05",
        "keywords": [
            "Musical works",
            "Western art music tradition",
            "derivative or part of another work",
            "music information retrieval systems",
            "traditional library catalogs",
            "essential relationships",
            "search results",
            "information about them",
            "end-users",
            "Variations2 Digital Music Library project"
        ],
        "content": "EXPLOITING MUSICAL CONNECTIONS: A PROPOSAL FOR SUPPORT \nOF WORK RELATIONSHIPS IN A DIGITAL MUSIC LIBRARY \n Jenn Riley  \n Indiana University Digital Library Program  \n1320 E 10th St, E170 \nBloom ington, IN 47404 \njenlrile@indiana.edu   \nABSTRACT \nMusical works in the W estern art m usic trad ition exist \nin a com plex, inter-related web. W orks that are de-\nrivative or part  of anot her work are com mon; how-\never, m ost m usic in formation retriev al system s, in-\ncluding traditional library catalogs, don’t  use these \nessen tial relatio nships to improve search  resu lts o r \nprovi de inform ation about  them to end-users. As part  \nof the NSF-funded Vari ations2 Di gital Music Library  \nproject  at Indi ana Uni versity, we have devel oped a set \nof funct ional requi rements defi ning how derivative \nand whol e/part relationshi ps bet ween m usical works \nshoul d be act ed upon i n search resul ts, and how these \nresul ts shoul d be di splayed. Thi s paper descri bes re-\ncent research into these relationships, provides exam -\nples why they are im portant in W estern art m usic, \noutlines how Variatio ns2 or any other music in forma-\ntion retrieval system could use t hese rel ationshi ps in \nmatching user queri es, and descri bes opt imal displays \nof these rel ationshi ps to end-users. \n  \nKeyw ords: Digital m usic lib raries, m etadata, b iblio-\ngraphi c relationshi ps.  \n1 DIGITAL MUSIC LIBRARIES IN THE \nACADEMIC ENVIRONMENT \nThe goals of digital m usic libraries in the academ ic \nenvironm ent are frequent ly different  than those of \ncommercial system s. Academ ic libraries focus heavily \non meetin g the needs of the educatio nal institutions of \nwhich they are a part. To meet these needs, MIR sys-\ntems in academ ic libraries m ust support highly spe-\ncific queri es for known i tems for users i n search of \nmaterials for perform ance and research. At the sam e \ntime, they must provi de m echani sms for exploration, \nallowing users t o discover music previ ously unknown \nto them but relevant to their perform ance or schol arly \ninterests. \n Librari es have l ong creat ed descri ptive metadata \nfor m usical m aterials. Toda y, library catalogs serve \ntwo prim ary functions: access for patrons and inven-\ntory cont rol. For bot h purposes, l ibrary catalog re-\ncords have at  their core descri bed an “i tem,” defi ned \nby the Anglo-American Cataloging Rules, 2nd edition \n(AAC R2), as “A docum ent or set  of docum ents in \nany physical form , publ ished, issued, or treated as an \nentity, an d as su ch forming the basis for a single bib-\nliographic descrip tion.” [1] This focus implicitly as-\nsumes that each bibliogra phic item  contains one and \nonly one nam ed work of i nterest to users. For musical \nmaterials, th is focus on the physical item  over its in-\ntellectual content has im peded access for end-users, \nboth those l ooking for speci fic musical works and \nthose wi th more expl oratory intentions. \nLibrary catalog records in  the MARC form at are \ncreated according to rules from  a num ber of sources, \nmost notably AACR2. These rules prescribe when to \nprovide explicit access to nam ed works appearing in \na bibliographic item , and when to omit this informa-\ntion or rel egate it to more unstructured (and therefore \nless useful  for ret rieval) areas of t he bibliographi c \nrecord. Musical m aterials, such as com pact discs, \nfrequent ly cont ain a l arge num ber of m usical works \non the same bibliographi c item. In the MARC envi -\nronm ent, there is a serious “lack of established rela-\ntionships between fields a ssociated with each work,” \n[2] resulting in less th an ideal retriev al of music in  \nlibrary catalo gs. Sim ilarly, o ther artifacts of library \ncataloging codes and record form ats pose significant \nbarri ers t o discovery  of l ibrary materials, including \ninconsi stent indication of i nstrumentation [3]  and \ndifficulty different iating bet ween rol es of cont ributors \nwhen searching. [4]  \n2 RESEARCH ON BIBLIOGRAPHIC \nRELATIONSHIPS AND ABSTRACT \nWORKS \nDue to increasing recogniti on of the problem s created \nby a focus on the bibliographi c item, the library com -\nmunity has begun t o invest igate methods of i ncreasi ng \naccess to intellectual works rather than the items on \nwhich they are contained. Research in this area has \nfocused largely on two areas:  first, on underst anding \nrelationshi ps bet ween bi bliographi c items; and sec-\nond, on defi ning the “work” as separat e from  the bib-\nliographi c item on whi ch it resides. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies  bear this  notice and the full \ncitation on the first page. \n© 2005 Queen Mary , University  of London \n The fi rst major t axonom y of bibliographi c rela-\ntionships was co mpiled by Barb ara Tillett, today at \n123   \n \n the Library of Congress, in her 1987 Ph.D. disserta-\ntion. The relationships Tillett defined are as follows:  \n Equivalence relationships, “which hold between \nexact copies of the same manifestation of a work, or between an original item and repro-ductions of it…” \n Derivative relationships, “which hold between a \nbibliographic item and a modification based on that item” \n Descriptive relationships, “which hold between \na bibliographic item or work and a description, criticism, evaluation, or review of that work…” \n Whole-part relationships, “which hold between \na component part of a bibliographic item or work and its whole…” \n Accompanying relationships, “which hold be-\ntween a bibliographic item and the biblio-graphic item it accompanies,  such that the two \nitems augment each other equally or one item augments the other principal or predominant item” \n Sequential relationships, “which hold between \nbibliographic items that continue or precede one another…” \n Shared characteristic relationships, “which hold \nbetween a bibliographic item and other biblio-graphic items [sic] that is not otherwise related but coincidentally has a common author, title, subject, or other characteristic used as an ac-cess point in a catalog…” [5] \nRichard Smiraglia performed research further sub-\ndividing derivative relationships, similar to the sec-ond relationship Tillett defined, into the following categories: \n Simultaneous derivations, “works that are pub-\nlished in two editions simultaneously or nearly simultaneously…” \n Successive derivations, “works that are revised \none or more times…works that are issued suc-cessively with new authors, as well as works that are issued successively without statements identifying the derivation” \n Translations, “including those that also include \nthe original text” \n Amplifications, “including illustrated texts, mu-\nsical settings, and criticisms, concordances and commentaries that include the original text” \n Extractions, “including abridgements, conden-\nsations and excerpts” \n Adaptations, “including simplifications, screen-\nplays, librettos, arrangements of musical works, and other modifications” \n Performances, “including sound or visual (i.e., \nfilm or video) recordings\" [6] \nNote, however, that although Tillett defines the de-\nrivative relationship as between a bibliographic item and a modification based on it, Smiraglia’s derivative relationship categories describe derivatives of ab-\nstract  works rather than bibliographic items. \nSherry Vellucci applied the bibliographic relation-\nships defined by Tillett and Smiraglia to music by studying their occurrence in records from a major music library catalog. Vellucci found that the whole-part relationship appeared most frequently, in 86.6% of records sampled. “The high percentage of whole-part relationships appearing in [her] study is expected \nwhen the nature of musical performance is consid-\nered. Performance creates a need for performance parts and performing editions of discreet performable units. Both of these conditions contribute to the exis-tence of whole-part relationships.” [7] Vellucci simi-larly found that the derivative relationship, using a definition similar to Smiraglia’s, is extremely com-mon in music cataloging, present in 85.4% of records in her sample. She found derivative relationships in the following categories: performances, derivative editions, amplifications, arrangements, forms of mu-sical presentation, adaptations, translations, and nota-tional transcripts. [8] \nRichard Smiraglia’s research towards defining “a \nwork” has been influential to bibliographic control. \nSmiraglia defines a work as “the set of ideas created \nprobably by an author or perhaps a composer, or other artist, set into a document using text, with the intention of being communicat ed to a receiver (proba-\nbly a reader or listener).” [9 ] Smiraglia uses the term \n“text” in its most generic sense, to mean the embodi-\nment of a work into some form. \nA major milestone in the move from pure research \nto potential implementations focusing on the intellec-\ntual work was the publication of the 1998 report Functional Requirements for Bibliographic Records  \n(FRBR) [10] from the International Federation of Library Associations and Institutions. The FRBR report employs entity-relationship analysis to “isolate \nthe key objects that are of in terest to users of infor-\nmation….” [11] “The [FRBR] entities defined as \nwork (a distinct intellectual or artistic creation) and expression  (the intellectual or artistic realization of a \nwork) reflect intellectual or artistic content. The enti-ties defined as manifestations  (the physical embodi-\nment of an expression of a work) and item  (a single \nexemplar of a manifestation ), on the other hand, re-\nflect physical form.” [12] While the FRBR report has \nbeen influential in stimulating thinking about next-\ngeneration library catalogs, adoption in production systems is still almost exclusively in theoretical stages. \n3 THE VARIATIONS2 DIGITAL MUSIC \nLIBRARY SYSTEM \nVariations2 is a digital music library system currently \nunder development at Indiana University, building on the ground-breaking Variations digital audio delivery system. [13] In support of instruction in the world-\n124   \n \nrenowned Indi ana Uni versity School  of M usic, the \nVariations2 system provi des searchi ng of m usical \nworks and t he cont ainers on whi ch they reside; deliv-\nery of digital audio, scanned score i mages, and en-\ncoded scores;  and advanced t ools for usi ng di gital \nobjects in  the system  in an instructional environ-\nment.The Vari ations2 search sy stem operat es on a \nwork-based metadata model sim ilar in  many ways to \nFRBR. Th e Variatio ns2 system  focuses o n “classical” \nmusic, the canon of W estern art m usic traditionally \nstudied and performed at institutions of higher educa-\ntion, though this tradition is  currently expanding in \nschool s of m usic around t he worl d. As seen in Figure \n1, the Vari ations2 m etadata model is centered on a \n“Work” entity, wh ich “rep resen ts the abstract co ncept \nof a musical piece or set of pieces.” [14] The m odel is \nstructured with the “Work” entity at its co re, o n the \nassum ption that the m usical work is in m ost cases \nmore important to users searchi ng for t his type of mu-\nsic than any given bi bliographi c item cont aining that \nwork. For exam ple, the m odel assum es that a user \nwould more likely be looking for a decent perform -\nance of t he Bizet aria “Au fond du t emple saint” (the \nfamous duet  from  the opera Les Pêcheurs de perles  \n[The Pearl Fishers ]), rath er th an specifically the CD \n“Bryn Terfel  Sings Favori tes,” on which that aria hap-\npens t o appear. Thi s focus st ands i n stark cont rast to \nthe trad itional focus on the bibliographic item  in li-\nbrary catalogs. \nThe Vari ations2 m odel also provi des for users who \nare searching for specific perform ances or printed \neditions of musical works. Onl y individuals responsi -\nble for the abstract work, su ch as com posers and lyri-\ncists, are recorded as creat ors of the work. Each work in the Vari ations2 m etadata model then appears on a \nrecordi ng or i n score form  as an “Inst antiation.” Per-\nform ers, conduct ors, edi tors, and ot her i ndividuals \nassociated with a specific m anifestation of a work are \nrecorded as contributing to  the Instantiation, and are \nsearchable as such. A “Cont ainer” i n the Variations2 \nmetadata model is the bibliographi c item (recordi ng \nor score) on whi ch instantiations appear. The con-\ntainer is analogous to the item  traditionally described \nin library catalogs. “Media  Objects” in the Varia-\ntions2 metadata m odel are d igital files rep resenting \nthe content of a container that can be delivered to \nend-users. C urrent ly, the Vari ations2 sy stem delivers \nonly cont ainers avai lable in digital form at; physical \nitems on the shel f of t he library are not  included. \nThe current  implementation of the Variations2 \ndata model includes onl y the most rudimentary meth-\nods for speci fying rel ationshi ps bet ween work ent i-\nties, although rel ationshi ps bet ween works are fre-\nquent  in the genres of m usic on whi ch the Vari ations2 \nsystem  focuses. Currently, any work m ay be specified \nhaving an “is versi on of,” “has versi ons,” “i s part  of,” \nor “has part ” relationshi p to anot her work. These re-\nlationships are not recip rocal; th ey m ust be explicitly \nspeci fied for bot h works i nvolved in the relationshi p. \nFurtherm ore, t he relationshi ps indicated in the current  \nVariations2 sy stem don’t  do anything; their existence \nor absence has no effect  on searchi ng or resul ts dis-\nplay, and any  relationshi p inform ation added to work \nrecords by  Vari ations2 cat alogers i s not visible to \nend-users on defaul t resul ts screens;  rather, it is only \navailable if the user clicks an icon to receive a more \ndetailed  work reco rd. \n \n \n \nFigure 1 . Vari ations2 M etadata Modelis represented by  \nMEDIA OBJECT represents a piece of digital m edia \ncontent (e.g., sound fi le, score i m-\nage) is enclosed in \nCONTAINER represents the physical item  or set of \nitems on whi ch one or more instan-\ntiations of works can be found (e.g., \nCD, sco re) is manifested in \nINSTANTIATIONrepresent s a manifestation of a work \nas a recorded perform ance or a \nscore WORK represents the abstract concept \nof a m usical com position or  \nset of com positions \nis created by\nCONTRIBUTOR \nrepresent s peopl e or groups \nthat contribute to a work, \ninstantiation, or cont ainer \n \n125   \n \n \n \nFigure 2 . A Partial W ork Stru cture in the Varia-\ntions2 Adm inistrative Interface \n \nThe Vari ations2 sy stem also has an additional im-\nplicit implementation of t he whol e/part relationshi p, \nthrough the use of searchabl e work st ructure nodes. \nEach work record can have  a hierarchical structure \ndocum enting m eaningful  divisions of t hat work, as seen \nin Figure 2. Each work st ructure node can be gi ven a \nlabel, wh ich is th en added to the title in dex in the Varia- tions2 sy stem. When a user’s search m atches one of \nthese work st ructure nodes, t he node label is displayed \ntogether with  its p arent work in the search  resu lts, as \nseen in Figure 3. This cap ability h as proved useful in the \ninitial Variatio ns2 implementation, but it is in adequate in \ndepth, flexibility, and robustness for a full next-\ngenerat ion, user-cent ered product ion di gital music li-\nbrary system . \n4 FUNCTIONAL REQUIREMENTS FOR \nWORK RELATIONSHIPS \nThe current  implementation of work rel ationshi ps in the \nVariatio ns2 system  is inadequate to  meet th e complex \nretrieval needs of m usicians and m usic researchers in the \nuniversi ty environm ent. Based on research i nto biblio-\ngraphi c relationshi ps in the library and i nform ation sci -\nence literature and ongoing studi es of user searching \nbehavi or by  the Vari ations2 project  team and Indiana \nUniversity Digital Library Pro gram staff, th e metadata \nteam for the Vari ations2 project  has devel oped a set  of \nfunct ional requi rements for t he implementation of de-\nrivative and whol e/part relationshi ps. These two rela-\ntionship types were chosen b ecause they m ost frequently \nappear in m usic catalogs, a nd because their handling was \nparticularly in adequate in  the cu rrent Variatio ns2 im-\nplementation.  \nOur requi rements descri be mechanism s for creating \nthe relatio nships in the Variatio ns2 administrativ e inter-\nface, using the relationship data to im prove search re-\nsults, and displaying rel ationshi p inform ation in mean-\ningful  way s to end-users. These requi rements woul d be \nimplemented on t op of t he existing Variations2 search \nmechani sm, descri bed i n detail in Scherl e and Byrd \n[15], although som e additional fields would be indexed \n(e.g., instrum entation). \n \n \nFigure 3 . Vari ations2 Di splay of Query  Match to a W ork St ructure Node\n \n126   \n \n4.1 Derivative relationships \n4.1.1 Defin ing the rela tionship \nIn Vari ations2, a deri vative relationshi p is that between a \nsource work and a deri vative work based i n som e way  \non the source work. Deri vative rel ationshi ps for m usic \ninclude arrangem ents, versi ons, m edleys, and free inter-\npretations of source works. The rel ationshi p can be very  \nstrong, as is the case when a work originally written for \none i nstrument is arranged for anot her. It  can al so be \nmuch weaker, as is the case when a derivative work is a \nfree in terpretatio n of a so urce wo rk. Both Smiraglia and \nVelluci distinguish the strengt h of deri vative relation-\nships through t he sub-cat egori es they define. Derivative \nrelationships are extrem ely com mon in m usic; however, \nfew digital m usic lib raries u se them to improve retriev al \nfor end-users.  \nDeriv ative relatio nships by their n ature are fully re-\nciprocal ; for any  work t hat is a source of a second work, \nthe second work is necessarily  a derivative work of the \nfirst. Any  system implementing derivative relationshi ps \nshoul d propagat e changes m ade to the relationshi p in a \nsingle place to both the sour ce and derivative works. \nThere shoul d be no arbi trary restriction on t he dept h \nof derivative relationshi ps. Any  work t hat is itself a de-\nrivative can have i ts own deri vatives. A source work \nmay have any number of deri vative works, and any  de-\nrivative wo rk may be related  to one or more  source \nworks. Loops where a work has as a derivative a work \nthat is its source at  som e hierarchi cal level shoul d be \nprevented. Works particip ating in a derivative relatio n-\nship may also part icipate in a whol e/part relationshi p. \nA digital music library system might choose t o record \nthe strengt h of t he deri vative relationshi p; descri bing in \nsome way how close, m usically, the derivative work is \nto the source. In  the Variatio ns2 implementation re-\nquirements, we have chosen not  to implement this op-\ntion. We similarly defi ne onl y deri vative rel ationshi ps, \nand not  any sub-cat egori zation of t hem. One reason for \nthis decisio n is that th e stren gth of the derivative rela-\ntionshi p is extremely subjec tive. W e chose instead to \nspeci fy the recordi ng of derivative relationshi ps be-\ntween works whenever t hey are known, and allow end-\nuser to decide for them selves  if they wish  to explore \nthese rel ationshi ps. \n4.1.2 Query mat ches and di splay  \nOur speci fications provi de ret rieval behavi or for deri va-\ntive rel ationshi ps to meet two classes of user needs. In \nthe first category, a user m ay be interested  primarily in  a \nspeci fic versi on of a work and onl y marginally interested \nin others. Arrangem ents, frequent ly appeari ng in West-\nern art m usic, ten d to fall in to this categ ory. A user in \nthis case i s probabl y looking for a score or recordi ng \nwith certain instrum entation for perform ance or study . In \nthis situ ation, a user is lik ely to include a specific in-\nstrumentation in their search , which in turn is lik ely to  \nmatch one or m ore but  not all deri vatives of a source work. Here our speci fications requi re that the source \nwork be di splayed together wi th matched deri vative \nworks, allo wing the user to  select in stantiatio ns of any \ndisplayed work for l istening or vi ewing. The user there-\nfore can include instrum entation information in a search  \nand be provi ded i n the first search resul t screen wi th the \nspecific instrum entation they were seeking, as seen in \nFigure 4. A sy stem might addi tionally provi de visual \ngroupi ngs of deri vative works when several  of them, but \nnot all, match the query . \n \n \nFigure 4 . Sam ple Resu lts Disp lay fo r a Qu ery In -\ncluding Inst rumentation \nThe second class of user needs rel ated to work rel a-\ntionshi ps is those where t he user has som e sort  of re-\nsearch interest for discoveri ng al l known deri vations of \nsource works. For all works t hat have rel ated deri vative \nworks, when a search m atches a source work only, or \nboth a source work and all its derivatives, our specifica-\ntions requi re that the source work be di splayed in the \nresul t set, along wi th a m echani sm that allows the user \nto display all derivative wo rks for that so urce. A sam ple \ndisplay o f this case m ay be seen in Figure 5, where a \nparticular Bach  sonata, o riginally written  for violin, \nmight be avai lable in arrangem ents for piano, lute, and \nmarimba. Since m any musical works are bet ter known \nin a popul ar arrangem ent than in their original form , the \nuser t herefore can be exposed t o and choose among all \nversions of a work available in the system .  \n \n \nFigure 5 . Sam ple Resu lts Disp lay fo r a Qu ery \nMatch ing a W ork with  Deriv atives Work title: \nView der ivative wor ks (3) Com poser : \nInstrumentation: Sonaten und Partiten, violin, BW V \n1001-1006. Sonat a, no. 1;  arr.  \nBach,  Johann Sebastian  1685- 1750 \nViolinQuery:  bach and sonata and 1001 Work title: \nDerived fr om: Com poser : \nInstrumentation: Salón México; arr.  \nCopland,  Aaron 1900- 1990 \nPiano \nSalón M éxico \nCopland,  Aaron 1900- 1990 \nOrchestra Query:  copland and m exico and piano \n \n127   \n \n4.2 Whole/part relationships (parent/child relation-\nships) \n4.2.1 Defin ing the rela tionship \nAs descri bed above, a whol e/part relationshi p is that \nbetween a parent  work and a child work that is com -\npletely enclosed in the parent . In m usic, child works are \nfrequently performable units in  their own right. In tradi-\ntional library catalo ging, Uniform Titles can  be used to \nindicate this relationshi p to som e degree, but  few if any \nsystem s with  uniform title d ata use the wh ole/part struc-\nture to improve searchi ng. [16]   \nLike the deri vative relationshi p, the whol e/part rela-\ntionshi p is fully reciprocal . For any  work t hat is a parent  \nof a second work, the second wo rk is necessarily a child \nwork of the first. Any  system implementing whol e/part \nrelationshi ps shoul d propagat e changes m ade to the re-\nlationship in a single place to  both the parent and child \nworks. \nThere shoul d be no arbi trary restriction on t he dept h \nof whol e/part relationshi ps. Any  work t hat is itself a \nchild can have i ts own chi ldren. A parent  work m ay \nhave any  num ber of chi ld works. In t he Vari ations2 \nproject , we have seen no evi dence that leads us to be-\nlieve a work woul d need t o be descri bed as t he chi ld of \nmultiple parents. Therefore we are proceeding for the \ntime bei ng under t he assum ption that a system imple-\nmenting whol e/part work rel ationshi ps coul d assum e \nany child work can have one parent  but no more. Loops \nwhere a work has as a child a work that is its p arent at \nsome hierarchi cal level shoul d be prevent ed. W orks \nparticipating in a whol e/part relationshi p may also par-\nticipate in a deri vative relationshi p. A sy stem shoul d be \nable to handle cases where two child works of the sam e \nparent  are deri vatives of di fferent  source works. \nIn the Vari ations2 sy stem, where the work structure \nas it ex ists to day is essen tial as stru ctural metadata for \nlinking search results to the appropriate places in re-\ncording or score media objects, we will h ave additional \nrequi rements for integrating whol e/part work rel ation-\nship support  into the current  environm ent. The concept  \nof a work st ructure i s not equi valent to that of a par-\nent/child relatio nship between  works. A ch ild work \nmust be a perform able unit of m usic, one t hat is now or \nis expect ed to be a work i n its own ri ght that woul d ap-\npear as an instantiation in the Vari ations2 sy stem. Work \nstructure nodes are not necessarily perform able units; \ninstead, t hey are navi gation poi nts for listeners of audio \nand viewers of scores, poi nts at whi ch the Vari ations2 \nsystem  connects the abstract st ructure of a work to that \npoint in a part icular recordi ng or score. In our imple-\nmentation, all ch ild works will ap pear as work structure \nnodes, but not all work structure nodes will be child \nworks. \n4.2.2 Query mat ches and di splay \nQuery  matching and di splay requi rements were designed \nto support  a user i nterested in a speci fic perform able unit \nof a m usical work, whet her or not  this work has parent  works. If t his user were l ooking for a score of an opera \naria from  which to perform  at a recital, her need woul d \nbe m et by a col lection of ari as for t his user’s voice type \nor a com plete vocal  score of t he opera. If she were look-\ning for recordings of perform ances for study , her need \nwoul d be m et by a CD of a famous perform er’s favori te \narias, o r by a reco rding of the complete o pera. In this \ncase the user is lik ely to  include in her query information \nspeci fic to one (or som e) but  not all children of a parent  \nwork. Our requirem ents ther efore specify the system  \nshoul d ret urn as search resul ts the chi ldren m atched by \nthe query  plus their immedi ate parent works, as seen in \nFigure 6.  \n \n \nFigure 6 . Sam ple Resu lts Disp lay fo r a Qu ery \nMatching a C hild Work and i ts Parent  \nMechani sms shoul d then be avai lable for t he user to \nview the complete hierarchy  of parent s and chi ldren of \nany works in the search resu lts not already  displayed. \nTo al low users t o further expl ore m usic represent ed in \nthe system, any  parent  or chi ld shoul d be available for \nselection, to view the containers on whi ch the work at  \nthat or a higher hierarch ical lev el resid es.  \nThese requi rements for query  matches and di splay of \nwhol e/part relationshi ps al low users t o discover more \nmaterials relev ant to their queries th an traditional music \ncatalogs. The goal is to provide the user with a work at a \nlevel most relevant to her query , yet allow exploration \nup and down t he work hi erarchy  once a work has been \nretrieved. These requi rements do, however, requi re that \nmetadata for these works m eets certain m inimal expec-\ntations i n order t o funct ion. Si nce t he work matching \nrequi rements operat e by knowi ng which levels of a hi er-\narchy  of works match a gi ven query , the search engi ne \ncannot  simply follow links t o parent  and child works to \ndeterm ine if the group m atches a query. As titles of par-\nent works are not used to determ ine if a query m atches a \nwork, Variatio ns2 metadata reco rds will co ntain a Uni-\nform  Title, a practice from  traditional library catalogu-\ning practice. Th e Un iform Title o f a work with parent \nworks will g enerally co ntain the titles of all parent \nworks. To t ake the exam ple shown i n Figure 6, Wag-\nner’s aria Nothung!  Nothung!  Neidliches Schw ert! will \nhave the Un iform Title Ring des N ibelungen. Si egfried. \nNothung!  Nothung!  Neidliches Schw ert! This aria will \ntherefore be consi dered a m atch to a Vari ations2 query  \nincluding t he words “ri ng,” “ni belungen,” or “sieg-\nfried.” Nothung! Not hung! \nNeidliches Schwert ! Query:  wagner and siegfried and nothung \nCom poser : \nPart of : Wagner, Richard  1813- 1883 Work title: \nRing des Nibelungen.  Siegfr ied.  \nWagner, Richard  1813- 1883 \n \n128   \n \n5 NEXT STEPS \nDue t o com peting dem ands for devel opment time on the \nVariatio ns2 project, we will n ot be able to implement the \nrequi rements descri bed here , although we hope t o be \nable to address these issues  in a fol low-on project  for \nwhich we are current ly request ing fundi ng. As we begi n \nto implement these req uirements, we will continue to \niteratively refi ne them. Although t hese speci fications \nwere devel oped t hrough expert ise in the canon of West-\nern art  music, devel oping underst anding of user search \nbehavior in music an d other domains, and research  lit-\nerature i n these areas, st udies exposi ng these rules to \nactual users and their expectations will necessarily in-\nform  us of changes t hat need t o be m ade. Si milarly, \nthese speci fications have been devel oped based on \nknowledge of m usic literature and selected sam ple \nworks. As we appl y these pri nciples to more works, up-\ndates to the requi rements may emerge. \nThe work rel ationshi ps current ly planned for i mple-\nmentation in Vari ations2 are onl y a few of t he possi ble \nrelationshi ps that coul d be i mplemented in a digital mu-\nsic library. W e are currently considering a “version of” \nrelatio nship that wo uld exist between  two works related  \nto one anot her but  that do not have a source-deri vative \nrelationshi p. In addi tion, current ly, the Vari ations2 sy s-\ntem is limited in scope to reco rdings and scores of mu-\nsical materials. If the sco pe were to  expand in the future \nto include t he myriad of ot her m aterials hel d by music \nlibraries, work rel ationshi ps descri bing m usico-dram atic \nsettings of textual works, t ranslations, and t exts anal yz-\ning m usical works woul d be needed. \nThe Vari ations2 sy stem current ly operat es on search-\ning of metadata to  retriev e music in  the system . The \ndesign of t he system, however, i s modular, and can \nmake use of pl ugged-i n versi ons of content-based \nsearching mechanism s. The rules for work m atching and \ndisplay d escrib ed here co uld apply to initial matches \nmade by a content-based search al gorithm, if the work \nrelatio nships were sp ecified  in the system  metadata. \nAnot her activity we are pl anning is an anal ysis of the \neffect iveness of a work-based m etadata model for popu-\nlar and world music, where there is less of a dependence \non a canoni cal musical “work.” The application of the \nwork relatio nship principles outlined here to other types \nof music will h elp us evaluate the appropriaten ess of the \nVariations2 m etadata model to those st yles of m usic. \nREFERENCES \n[1] Angl o-Am erican C ataloging R ules (AAC R2), 2nd \ned., 2002 revision. C hicago:  Am erican Li brary  \nAssoci ation; Ottawa: Canadian Li brary  Associ ation; \nLondon: Chartered Institute of Library and \nInform ation Professi onals, 2002, gl ossary  entry for \n“item .” \n[2] Hem masi, H. Why not MARC?  Proceedings of the \n3rd Int ernational Conference on M usic Inform ation Retrieval, Pari s, France, 13-17 Oct ober 2002. Pari s: \nIRCAM—Centre Pom pidou, 2002, 242-248, p. 244. \n[3] Notess, M., Riley, J., and Hem masi, H. From  \nAbstract to  Virtu al En tities: Im plementation of \nWork-Based  Search ing in a Multimedia Digital \nLibrary . Research and Advanced Technol ogy for \nDigital Libraries: Proceedi ngs of the 8th European \nConference, EC DL 2004, B ath, UK, September 12-\n17, 2004, edited by  Rachel  Heery  and Li z Ly on, \n157-167. Hei delberg:  Spri nger-Verl ag, 2005, p. \n160. \n[4] Hem masi, p. 243. \n[5] Tillett, B. Bibliographic Relatio nships: To ward  a \nConcept ual Structure of B ibliographi c Inform ation \nused i n Cataloging. Ph.D. Di ss., Graduat e School  of \nLibrary & Information Scien ce, Un iversity o f \nCalifornia, Los Angel es, 1987, p. 24-25. \n[6] Smiraglia, R. Au thority Co ntrol and the Extent of \nDerivative B ibliographi c R elationshi ps. Ph.D. \nThesi s, Uni versity of C hicago, 1992, p. 28. \n[7] Vellu cci, S. Bibliographi c Relationshi ps in Musi c \nCatalogs. Lanham , Md. & London:  Scarecrow \nPress, 1997, p. 92. \n[8] Vellucci, p. 103 \n[9] Smiraglia, R. The Nature of “A Wo rk”: Imp lications \nfor the Organi zation of  Knowledge. Lanham , Md. & \nLondon:  Scarecrow Press, 2001, pp. 3-4 \n[10] IFLA St udy Group on t he Funct ional Requirements \nfor B ibliographi c Records. Funct ional Requi rement s \nfor Bi bliographi c Records . Munich: K.G. Saur, \n1998. http://www.ifla.org/VII/s13/frbr/frbr.pdf \n[11] IFLA, p. 9 \n[12] IFLA, p. 12 \n[13] For a sum mary and overvi ew of t he goals and \nimplementation of the Vari ations and Vari ations2 \nproject s, see:  Dunn, J, Davi dson, M . W., Holloway, \nJ. R., and B ernbom , G. \"The Vari ations and \nVariations2 Digital Music Li brary  Project s at  \nIndiana University.\"  In Digital Lib raries: Po licy, \nPlanning and Pract ice, Andrews, J. and Law, D., \neds., Ashgat e Publ ishing, 2004, pp. 189-211. \n[14] DML Dat a Model Speci fication for Versi on 2, \nhttp://variatio ns2.indiana.edu/pdf/DML-DataMo del-\nV2.pdf, p. 2. \n[15] Scherl e, R., and B yrd, D. The Anat omy of a \nBibliographic Search Sy stem  for M usic. 5th \nInternational Conference on M usic Inform ation \nRetrieval, Barcelona, Spai n, 10-14 Oct ober 2004. \n[16] Notess, R iley and Hem masi, p. 161. \n \n129"
    },
    {
        "title": "Exploiting the Tradeoff Between Precision and Cpu-Time to Speed Up Nearest Neighbor Search.",
        "author": [
            "Pierre Roy",
            "Jean-Julien Aucouturier",
            "François Pachet",
            "Anthony Beurivé"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417453",
        "url": "https://doi.org/10.5281/zenodo.1417453",
        "ee": "https://zenodo.org/records/1417453/files/RoyAPB05.pdf",
        "abstract": "We describe an incremental filtering algorithm to quickly compute the N nearest neighbors according to a similarity measure in a metric space. The algorithm exploits an intrinsic property of a large class of similarity measures for which some parameter p has a positive influence both on the precision and the cpu cost (precision-cputime tradeoff). The algorithm uses successive approximations of the measure to compute first cheap distances on the whole set of possible items, then more and more expensive measures on smaller and smaller sets. We illustrate the algorithm on the case of a timbre similarity algorithm, which compares gaussian mixture models using a Monte Carlo approximation of the Kullback-Leibler distance, where p is the number of points drawn from the distributions. We describe several Monte Carlo algorithmic variants, which improve the convergence speed of the approximation. On this problem, the algorithm performs more than 30 times faster than the naive approach. Keywords: Nearest Neighbor, Similarity Measure, Timbre, Large Databases. 1",
        "zenodo_id": 1417453,
        "dblp_key": "conf/ismir/RoyAPB05",
        "keywords": [
            "incremental filtering",
            "metric space",
            "N nearest neighbors",
            "similarity measure",
            "precision-cputime tradeoff",
            "Monte Carlo approximation",
            "Kullback-Leibler distance",
            "gaussian mixture models",
            "Timbre similarity algorithm",
            "large databases"
        ],
        "content": "Exploiting the Tradeoff BetweenPrecision andCpu-time\ntoSpeed UpNearest Neighbor Search\nPierreRoy,Jean-JulienAucouturier,Franc ¸oisPachetand AnthonyBeuriv ´e\nSONY ComputerScienceLaboratoryParis\n6,rueAmyot75005Paris,France.\n{roy,jj,pachet,beurive }@csl.sony.fr\nABSTRACT\nWe describe an incrementalﬁltering algorithm to quickly\ncomputetheNnearestneighborsaccordingtoasimilarity\nmeasure in a metric space. The algorithm exploits an in-\ntrinsic propertyof a large class of similarity measuresfor\nwhich some parameter phas a positive inﬂuence both on\nthe precision and the cpu cost ( precision-cputime trade-\noff). Thealgorithmusessuccessiveapproximationsofthe\nmeasuretocomputeﬁrst cheapdistancesonthewholeset\nofpossibleitems,thenmoreandmoreexpensivemeasures\non smaller and smaller sets. We illustrate the algorithm\non the case of a timbre similarity algorithm, which com-\npares gaussian mixture models using a Monte Carlo ap-\nproximation of the Kullback-Leibler distance, where pis\nthe number of points drawn from the distributions. We\ndescribe several Monte Carlo algorithmicvariants, which\nimprovethe convergencespeed of the approximation. On\nthis problem, the algorithm performs more than 30 times\nfaster thanthe naiveapproach.\nKeywords: Nearest Neighbor, Similarity Measure,\nTimbre,LargeDatabases.\n1 INTRODUCTION\nAlgorithms for fast nearest neighbor (NN) searching in\ngeneral metric spaces are of considerable interest for\ncontent-basedretrievalin largemusicdatabases. Answer-\ning NN queries requires computing the relative distance\nbetween complex data objects, such as songs in audio or\nsymbolic format, which is typically a very costly opera-\ntion.\n1.1 Metric SpaceIndex Structures\nOne approach for speeding-up NN search is to use pre-\nbuilt index structures. Traditional index structures, suc h\nPermission to make digital or hard copies of all or part of thi s\nwork for personal or classroom use is granted without fee pro -\nvided that copies are not made or distributed for proﬁt or com -\nmercial advantage and that copies bear this notice and the fu ll\ncitationon the ﬁrstpage.\nc/circlecopyrt2005 Queen Mary, University ofLondonas B+-trees or KD-trees (Samet, 1989), only work for\ndatasets which can be represented in a suitable vector\n(euclidean) space, i.e. for which there exists an ordering\nof the data that preserves relative similarities. This hold s\nfor a number of simple similarity functions, e.g. those\nbased on the euclidean distance between feature vectors\ncomputed from audio (Wold and Blum, 1996). In Reiss\net al. (2001), we reviewed a number of such indexing\ntechniquesin the context of Music InformationRetrieval.\nHowever, a large number of similarity algorithms are not\nsuitabletosuchvector-spaceindexstructures,astheyonl y\nprovide a distance function or metric to measure the dis-\nsimilaritybetweendatapoints. Thisisnotablytrueofsim-\nilarity functions based on statistical pattern recognitio n,\nsuch as timbre similarity (Aucouturier and Pachet, 2004)\nwhere songs are modeled with statistical models and the\nmodels compared with distribution comparison measures\nsuchasKullback-Leiblerortransportationdistances.\nThere has been a number of proposals for metric-\nspaceindexstructures,mostofwhichexploitthetriangle-\ninequality property of the distance function to prune dis-\ntance calculations during searching. Obviously, one may\nﬁrst have to verify that the distance measure indeed ver-\niﬁes the triangle inequality, as e.g. Vidal et al. (1988)\nfor the edit-distance. The Approximating and Eliminat-\ningSearchAlgorithm (AESA)(Juanetal.,1998)performs\nNN searchin approximatelyconstantaverage-time,at the\nexpenseofpre-processingthematrixofpairwisedistances\nbetweenobjects. Thisistypicallywellsuitedformatching\nincomingobjectsagainsta small (a fewthousand)dataset\nof pre-computed prototypes, e.g. isolated word recogni-\ntion tasks. However, as the space requirementsof typical\nmusic databaseskeep increasing(forinstance, the SONY\nConnect service1offers more than 700,000 tracks as of\n2004), computing the whole N2similarity matrix is sim-\nply not feasible. The M-tree (Ciacca et al., 1997)and the\nMulti-vantage point (MVP) tree (Bozkaya and Ozsoyo-\nglu, 1999) are radius-based indexing methods that do not\nrequire the computation of the whole similarity matrix,\nwhilestill preservingfastaccesstime. Inthese structure s,\nthedataishierarchicallyorganizedinclustersdeﬁnedbya\ncenterandaradius(themaximumdistancefromthecenter\nto any point in the cluster). If a query is too far from the\ncenter of a cluster, by virtue of the triangle inequality, al l\nthepointswithinthe clustercanbepruned,andthecorre-\n1http://www.connect.com\n230sponding distances calculations can be spared. Miranker\netal.(2003)haveused,comparedandimprovedsuchtech-\nniquesinthecontextoflargeimageandbiologicalprotein\nstructuresdatabases. Arecentapplicationofvantagepoin t\nindexing to melodic similarity in music databases can be\nfoundinTypkeetal. (2003).\n1.2 Exploitingthe TradeoffBetweenPrecisionand\nCputime\nIn this paper, we proposea genericalgorithm for fast NN\nsearch in metric spaces which relies neither on an index\nstructure2, nor on the veriﬁcation of the triangle inequal-\nity by the distance measure. The algorithm exploits an\nintrinsicpropertyofalargeclassofsimilarityalgorithm s,\nwhich exhibit a precision-cputime tradeoff for some pa-\nrameter p(tradeoff parameter ), i.e. for which both the\nprecisionandthecputimeincreasewith p.\nMany music similarity measures proposed in the lit-\nerature exhibit this precision-cputime tradeoff. This is\nnotably true for pattern recognition distance measures,\nsuch as Foote (1997); Welsh et al. (1999); Pampalk et al.\n(2003);AucouturierandPachet(2004). Insuchmeasures,\nthe distributions of each song’s frame-based feature vec-\ntors (e.g. Mel-FrequencyCepstrum CoefﬁcientsMFCCs)\naremodeled(e.g. withGaussianMixtureModelsGMMs)\nthen compared (e.g. using Kullback-Leibler). Many can-\ndidatesexist forthetradeoffparameter p:\n•pmay be the size of the featurevector. For instance,\nthe number of MFCCs typically inﬂuences the pre-\ncision of the measure (as illustrated e.g. in Aucou-\nturier and Pachet (2004)), but also the dimension of\nthe model, hence the cpu time both for learning and\ncomparing.\n•pmay also be the size of the model, e.g. the number\nofgaussiancomponentsinaGMM,orthebinsizeof\na histogram. Themorecomplexthemodel,themore\nprecisethemeasure3,butalsothemoreexpensivethe\nlearningandthecomparison.\n•pcan also be found at the model comparison stage.\nIn Section 3, we consider a Monte-Carlo sampling\napproximation of the Kullback Leibler distance be-\ntween GMMs: the more samples are drawn from\nthe GMMs, the more precise is the approximation\nby virtue of the central limit theorem, but also the\nmore expensive are both the sampling and the dis-\ntancecomputation.\nWe propose to exploit the precision-cputime tradeoff\nof such distance algorithms Ato efﬁciently calculate the\nresult of NN queries. We use nsuccessive reﬁnementsof\nAto compute ﬁrst cheap, unprecise distances (i.e. A(p)\nforpsmall) on the whole set of possible items, then\nmoreandmoreexpensiveandprecisedistances(i.e. A(p)\nforpbig) on smaller and smaller sets. If the precision\nPREC (p)of the distance measure increases faster4than\n2therefore, it is compatible and complementary with the\nabove-described metricindex structures\n3this is not taking into account the curse of dimensionality,\nsee Bishop(1995)\n4ina sense tobe deﬁned inSection2the cputime CPU (p), then we will show that the cumu-\nlatedcputimeofthesuccessivestepsusing A(p0),A(p1),\n...,A(pn−1)may be a lot smaller than the direct compu-\ntation ofthe most precisedistance A(pn−1)onthe whole\nset ofitems.\nThis approach can be viewed and implemented as a\nplanningwrap-uparoundanexistingdistancemeasure,to\nspeeduptheassociatednearestneighborsearch. Weshow\nthatdramaticspeed-upcanbeachievedwithoutmodifying\ntheimplementationoftheunderlyingdistancemeasure.\nSection 2 presents a general formulation of the algo-\nrithm, discusses the required condition on the distance\nmeasure, and explains the optimization process to ﬁnd\nthe optimal sequence of steps that yields the smallest to-\ntal cputime. In Section 3, we illustrate the algorithm on\nthe case of NN queries, using a timbre similarity algo-\nrithm (Aucouturier and Pachet, 2004), which compares\nGaussian mixture models using a Monte Carlo approxi-\nmationof the Kullback-Leiblerdistance,where the trade-\noff parameter pis the number of points drawn from the\ndistributions. We describe several Monte Carlo algorith-\nmicvariants,whichimprovetheconvergencespeedofthe\napproximation, and report speed improvement factors as\nhighas30.\n2 INCREMENTAL FILTERING\nInSection1.2,analgorithmforfastNNsearchinginmet-\nricspaceswassketchedout. Thisalgorithmcanbeseenas\na particularillustration ofa moregeneralapproachwhich\nwepresentinthissection.\nInthismoregeneralcontext,weareinterestedincom-\nputingtheelementsofaset Sthatsatisfyagivencriterion\nc, called the target criterion. Note that computingthe NN\nof a given item with respect to a given measure is a par-\nticular instance of this schema. The approach we present\nappliestoanytargetcriterionthatcanbeapproximatedby\na series of criteria with the followingproperty: roughap-\nproximationsofthetargetcriterionareeasy(fast)tocom-\npute,whereasgood(precise)approximationsofthe target\ncriteriontakelonger. Moreover,approximationsarefaste r\ntocomputethanthetargetcriterionitself.\nThe standardapproachto computingthe set Nof ele-\nmentsof Sthatsatisfy cistoevaluate cagainsteveryitem\ninS, retaining only those items that satisfy c. Roughly\nspeaking,ourapproachconsistsinstartingwithaﬁrstcri-\nterion that can be evaluated quickly, to eliminate irrele-\nvantitems,andthen,toprogressivelyevaluatecriteriath at\nare better approximationsof the target criterion, ﬁnishin g\nwiththetargetcriterionitself,toachievethetask. Theid ea\nbehind this strategy is that if the precision of the succes-\nsive criteria increases faster than their computation cost ,\nwe can save a substantial amount of computation time,\nbecausecriteriathatareexpensivetoevaluatewillbeeval -\nuatedagainstfeweritems.\n2.1 Deﬁnitionsand Assumptions\nLet usﬁrst introducesome necessarydeﬁnitionsandcon-\nventions:\n• Sisa ﬁniteset.\n231S=N 0 N \nS=N 0 N1 N2 Nn N c0 c1 cn = cc\nFigure 1: Instead of computing Ndirectly by applying c,\nwe iterativelycomputethe Nifori= 0,1, ..., n\n•cisa criteriondeﬁnedover S\nc:S→ {true, false } (1)\nWe call cthetargetcriterion.\n•c0,c1, ...,cnarecriteriadeﬁnedover Sthat approxi-\nmatecwithincreasingprecision,withtheconvention\nthatcn=c.\n•Nis the subset of Scontaining those elements that\nsatisfy c. Thegoalofthealgorithmistocompute N.\n•Similarly, Niis the subset of Sthat contains those\nelementsthat satisfy ci−1. By convention,we deﬁne\nN0=S.\n•t(ci)< t(ci+1)∀i∈[0, n−1]where t(ci)denotesthe\ncpu time needed to compute ci(x)for any element\nx∈ S.\nNote that the two following properties are a formal-\nization of the type of algorithms described in Section 1.2\n(precision-cputimetradeoff)\nProperty1 c0,c1, ...,cnapproximate cwith increasing\nprecision\nProperty2 Thecostofcomputing ciincreaseswith i,i.e.\nt(ci+1)> t(ci)\nThealgorithmcanbedescribedbyasimpleidea,illus-\ntratedinFigure1: insteadofcomputing Ndirectlybyap-\nplying c,weiterativelycomputethe Nifori= 0,1, ..., n.\nProperty3 Nn⊆Nn−1⊆...⊆N1⊆N0=S(i.e.\nci+1⇒ci)\nWhenProperty3holdsonthe Nisets,itisstraightforward\nto show that ci(Ni) =ci(S) =Ni+1. In other words,\none can compute Ni+1by applying citoNiinstead of\napplying citoS,thussavingtimesince Niissmallerthan\nS.\nFigure 2 illustrates the algorithm. In this ﬁgure, we\nassume that S=N0={x1, x2, ..., x p}and that the xi\nare ordered so that N={x1, x2, ..., x k}and more gen-\nerally Ni={x1, x2, ..., x ki}. This reordering is made\npossiblebytheinclusionrelationshipbetweenthe Nisets\nassumption. The top part, with the horizontal arrow la-\nbeledc=cn, represents the standard way of computing\nN, i.e. evaluate con every element of S, and retain only\nthe itemsthat satisfy c. Thecost ofthisapproachis:\nt(c)|S|=t(c)|N0| (2)\nwhere t(c)is the time it takes to evaluate function c on\none item and |S|is the cardinality of S. The rest of the\nﬁgure illustrates our approach, reading from left to right.\nThe leftmost column of the ﬁgure, labeled “ S” is an enu-\nmeration of S. The nearest column,labeled“ N1”, can be\nunderstood as follows: we evaluate c0on every item inS, which yields N1, the set of items that satisfy c0. This\nis represented by the oblique arrow labeled “ c0”.N1is\nenumeratedverticallyinthiscolumn. Thecostofthisstep\nis:\nt(c0)|S|=t(c0)|N0| (3)\nReadingFigure2fromlefttorightillustratesthatwe iter-\natively apply c0,c1, ...,cntoN0,N1, ...,Nn. Eventually,\ncn=c,thetargetcriterion,isevaluatedagainst Nn,yield-\ningN. Theoverallcost of thisapproachis the sumof the\ncostofeachstep:\nn/summationdisplay\ni=0t(ci)|Ni| (4)\nOurapproachisinterestingonlyinthosesituationswhere:\nn/summationdisplay\ni=0t(ci)|Ni|< t(c)|N0|=t(c).|S| (5)\nOnFigure2,thesuccessivesetscomputedarerepresented\nvertically,andthesuccessivecriterionevaluationsarer ep-\nresentedhorizontally. Thecostscan bevisualizedgraphi-\ncally if we assume that the proportionsare respected, i.e.\nthat the height of a set is proportional to its cardinality\nand that the width of a column is proportionalto the cost\nofthecorrespondingcriterionevaluation. Theoverallcos t\nof ourapproachcorrespondsto the light graysurface(the\nupper-left “triangle”), while the cost of the standard ap-\nproach is the hashed surface. With this graphical repre-\nsentation, it appears that if the Ni(the heights) decrease\nfastenoughandthatthe t(ci)(thewidths)simultaneously\nincrease fast enoughwith increasing i, the light gray sur-\nfacewill besubstantiallysmaller thanthe hashedsurface.\nThisiswhatwe discussinthenextsection.\n2.2 EfﬁciencyoftheApproach\nOur approach is interesting when it saves time, i.e. when\nequation 5 holds. This gives us a set of necessary con-\nditions for the method to run faster than the standard ap-\nproach. We will construct them recursivelyon n, starting\nwiththecase n= 1. Forn= 1,equation5becomes:\nt(c0)|N0|+t(c1)|N1|< t(c).|S| (6)\n⇒t(c0)< t(c)|N0|−|N1|\n|N0|=t(c)|S|−| N1|\n|S|(7)\nForn= 2,equation5becomes:\nt(c0).|N0|+t(c1).|N1|+t(c2).|N2|< t(c).|S|(8)\nwhere c2=c. Ifwe assume that equation7 holds, we get\nthesufﬁcientcondition:\nt(c1)< t(c)|N1| − |N2|\n|N1|(9)\nand so on. Finally, we have the following sufﬁcient con-\nditionsforourapproachtobeinterestingintermsofcom-\nputationtime:\nt(ci)< t(c)|Ni| − |Ni+1|\n|Ni|,∀i∈[0, n−1](10)\n232Figure2: Illustrationofthe algorithm. Thecumulatedcost ofthe successivestepsappearsasthe lightgrayarea,where as\nthe costofthedirectNN calculationappearsasthestripped area.\n|Ni|is related to the precision with which ciapprox-\nimates the target criterion c5: the less precise is ci, the\nlargeristhesmallerset ofitemsthatsatisfy ciwhichcon-\ntainsallitemsthatsatisfy c. Equation10thusrequiresthat\nateachstep i,theprecisionofthe ci’sincreasesfasterthan\ntheircomplexity.\n2.3 Implementing the Approach\nFor a given problem, one thus needs to ﬁnd a sequence\nof steps (the successive ci’s and Ni’s) that both veriﬁes\nproperties P1,P2, andP3and equation 10. Equation 10\nholdsonthecardinalitiesofthesuccessiveresultsets (th e\nNisets). Therefore, our approach is worth applying to\nproblem for which the cardinalities of the result sets can\nbe computed or estimated easily. Section3 illustrates a\ncase where the cardinalities are estimated once, even at a\nhighcost,forawholefamilyofcriteria.\nForagivenset Sandagivencriterion c,ourapproach\nis based on the existence of a series (ci)ithat satisﬁes\nproperties P1,P2, andP3. Such a series can easily be\nfound for the class of criteria that possess a tradeoff pa-\nrameter p. Let us assume that ptakes value in a ﬁnite\nsetP={0, ..., n}(using quantization if needed). In\nthiscontext,theseries (ci)i∈Pdoesnotnecessarilysatisfy\nequation 10, and if it does, there may exist sub-series of\n(ci)i∈Pthat allow a more efﬁcient implementationof our\napproach. Moreprecisely,givenset Sandcriterionseries\n(ci)i∈P, there exist 2nsub-series (c′\ni)i∈P′⊆Pof(ci)i∈P,\n5In Section 3, we will show that in terms of information re-\ntrieval,|Ni|is relatedtothe precision at recall1corresponding to different steps of the approach. (Note\nthat if (c′\ni)iis a sub-series of (ci)i, an item c′\njis one of\ntheciwithj≤i, and similarly, N′\nj=Ni.) The cost of\nthe approach for (c′\ni)iis/summationtext\ni∈P′t(c′\ni)|N′\ni|. Among those\nsub-series, at least one of them is optimal, i.e. there is\nat least one for which minimizing/summationtext\ni∈P′t(c′\ni)|N′\ni|. Note\nthat when the optimal sub-series contains only the target\ncriterion c,ourapproachequalsthestandardapproach.\nTo implement the approach optimally, one needs to\ncompute the optimal (c′\ni)i. In general, one cannot com-\nputethecost ofevery 2nsub-series. However,thiscanbe\nachievedveryefﬁciently using dynamicprogramming,as\nillustratedbythe followingalgorithm:\nbestSubSeries(n)\nif memValue(n)already computed\nreturn memValue(n)\nmin←+∞\nfor p←0ton−1\ntmp←bestSubSeries(p)\nc←cost(tmp∪ {n})\nif c <min\nresult←tmp∪ {n}\nmin←c\nend if\nend for\nmemValue(n)←result\nreturn result\nend bestSubSeries\ncost(listOfIndices)/summationtext\nt(c′\ni)|N′\ni|foriin listOfIndices\nend cost\n2333 TIMBRE SIMILARITY\nEXPERIMENTS\nIn this section, we apply the algorithm described in Sec-\ntion 2 to the practical task of calculating the n nearest\nneighborofasongaccordingtothetimbresimilaritymea-\nsure presentedin AucouturierandPachet(2004).\n3.1 The Precision-CputimeTradeoff\nWe sum up here the timbre similarity algorithm as pre-\nsented in Aucouturier and Pachet (2004). The signal is\nﬁrstcutintoframes. Foreachframe,weestimatethespec-\ntral envelope by computing a set of Mel Frequency Cep-\nstrum Coefﬁcients(MFCCs) (Rabinerand Juang (1993)).\nWe then model the distribution of the MFCCs over all\nframesusingaGaussianMixtureModel(GMM).AGMM\nestimatesaprobabilitydensityastheweightedsumof M\nsimplerGaussiandensities,calledcomponentsorstatesof\nthe mixture. (Bishop(1995)):\np(xt) =m=M/summationdisplay\nm=1πmN(xt, µm,Σm)(11)\nwhere xtis the feature vector observed at time t,Nis\na Gaussian pdf with mean µm, covariance matrix Σm,\nandπmis a mixture coefﬁcient (also called state prior\nprobability). The parameters of the GMM are learned\nwith theclassic E-Malgorithm(Bishop(1995)).\nWe then comparethe GMM modelsto matchthe tim-\nbre of different songs, which gives a similarity measure\nbased on the audiocontentof the music. We use a Monte\nCarlo approximation of the Kullback-Leibler (KL) dis-\ntance between each duple of models A and B. The KL-\ndistancebetween2GMMprobabilitydistributions pAand\npB(asdeﬁnedin(11))isdeﬁnedby:\nd(A, B ) =/integraldisplay\npA(x) logpB(x)\npA(x)dx (12)\nTheKLdistancecanthusbeapproximatedbytheempiri-\ncal mean:\n/tildewidestd(A, B ) =1\nnn/summationdisplay\ni=1logpB(xi)\npA(xi)(13)\n(where nis the numberofsamples xidrawnaccordingto\npA) byvirtueofthecentrallimittheorem:\nlim\nn→∞(1\nnn/summationdisplay\ni=1Xi− E(X)) =1√nN(0, σ2)(14)\nwhere Xis the random variable logpB(x)\npA(x),Xia realiza-\ntion of X,E(X)the mean of XandN(0, σ2)a normal\ndistributionofmean 0andvariance σ2,thevarianceof X.\nThe precision of the approximation is clearly depen-\ndent on the number of samples ndrawn from the distrib-\nutions,whichwecallDistanceSampleRate( dsr). Figure\n3 shows the inﬂuence of dsron the precision of the mea-\nsure, as deﬁned in Aucouturier and Pachet (2004) (on a\ncpu time\nFigure3: Inﬂuenceofthedistancesamplerateonthepre-\ncisionandcputime ofthe timbresimilarityalgorithm\nsemi-logarithmic scale). We see that the DSR has a pos-\nitive inﬂuence on the precision when it increases from 1\nto 2000, and that further increase has little if any inﬂu-\nence. Figure 3 also shows the (rescaled)cpu time proﬁle,\nwhich is a linear function of dsr. It appearsthat ﬁrst, the\nalgorithmexhibitsa precision-cputimetradeoff(usingth e\ndsras tradeoff parameter p), and second that, for small\ndsr’s, the precision of the measure increases faster than\nits cpu time, which makesit a good candidatefor the NN\nalgorithmpresentedin Section2.\n3.2 FormulationoftheProblem\nWe applythe algorithmdescribed in Section 2 to the task\nof computing the 100 nearest neighbors of an arbitrary\nseed song in a database containing 15,554 music ﬁles,\nwith respect to the target distance d. In our problem, d\nis the timbredistance describedaboveusing dsr= 2000,\nwhichis consideredtobean idealsetting.\nThe distance algorithm has a tradeoff parameter p=\ndsrwhich takes its integer values in P={1, ...,2000},\nandwe referto the instancesof the distance which uses p\nasdp. Notably, d=d2000. The cost of computing dpis\nlinearin p,andtheprecisionof dpincreaseswith p.\nThisproblemﬁtsintotheschemepresentedinSection\n2if onestatesit asfollows:\n•Sisthe collectionofmusicﬁles\n•dpis the Monte Carlo approximation of the KL dis-\ntancewith psamplingpoints\n•sisanelementof S\n•Np(s)is the set of the 100 nearest neighbors of s\nwrtdp. In particular, what we want to compute is\nN2000(s), the set of the 100 nearest neighbors of s\nwrtd=d2000\nGiven sinS,∀i∈ {1, ...,2000},we deﬁnethe result sets\nNi⊆Sas follows: ∀i∈ {1, ...,2000},Niisthe smallest\nsubsetof Ssuchthat:\n∀x∈N2000,∀y∈S, di(x, s)≥di(y, s)⇒y∈Ni\n(15)\nIn terms of information retrieval, if we deﬁne the set of\nrelevantdocumentsas N2000(s), we canobservethat\n• |Ni|isthenumberofdocumentsretrievedby diwhen\nrecall6= 1, i.e. when we have retrieved all the rele-\nvantdocuments.\n6Recall is the ratio of the number of relevant documents re-\ntrievedtothetotalnumberofrelevantdocumentsinthedata base.\n234• |Ni|isinverselyrelatedtotheprecision7ofthemea-\nsurediat recall1.\nprecision (di) =|N2000(s)|\n|Ni|=100\n|Ni|(16)\nWe cannowdeﬁne ciby:\nci(x) =true⇔x∈Ni(s) (17)\nLetusdemonstratethatproperties P1,P2andP3hold\nforthe cithusdeﬁned:\n•P1is satisﬁed since the cost of computing dpis lin-\near in p\n•P2andP3are satisﬁed statistically, since the preci-\nsionof dpincreaseswith pandbyconstructionofthe\nNiresultsets.\nTherefore, one can apply our approach to the problem of\ncomputing N2000forseedsong s.\n3.3 PracticalImplementation\nIn order to ﬁnd the optimal series of (ci)ithat minimizes\nthe total cputime of our approach for a given query on\nN2000(s), we needto estimate the |Ni|fora (large)set of\ni∈ {1, ...,2000}. One way to estimate |Ni|is to actually\ncomputetheset Ni,i.e.\n•applydi−1onN0=Sinordertosortthesongsin S\nbydistanceto saccordingto di−1\n•ﬁnd the maximum rank over all songs in N2000. It\ncorresponds to the rank after which all the items of\nN2000(s)havebeenretrieved,i.e. |Ni|\nHowever,thisdirectapproachhastwo majorproblems.\n•The set of |Ni|depends on the seed song, so in the-\nory, we have to apply this procedure for each seed\nsong before being able to ﬁnd the optimal sequence\nofsteps. Thisisunpractical,asestimatingthe |Ni(s)|\nfor a given sis itself longer than the direct calcula-\ntion of N2000(s)with the standard approach. More-\nover, it’s a chicken and egg problem, as computing\nthe|Ni(s)|requirestoknow N2000(s).\n•ThePdistances diare stochastic algorithms based\non Monte Carlo, which never return the same dis-\ntancedi(s, t)between 2 given songs sandttwice\n(although the variance on the results obviously de-\ncreases as dsrincreases). Hence, for a given seed\nsongs, the|Ni(s)|’s themselves should be averaged\noverseveralrunsofthe aboveprocedure.\nTo overcome these limitations, we propose to estimate a\nuniquesetof/tildewidest|Ni|forthewholedatabase,byapplyingthe\nabove procedure to a few random songs in the database\nand averaging the results. This has the drawback that the\nsuccessive inclusion property (Property P4) is only sta-\ntistically veriﬁed for the estimated/tildewidest|Ni|, and we have no\n7The precision is the ratio of the number of relevant docu-\nments retrievedtothe totalnumber of documents retrieved020004000600080001000012000140001600018000\n0 10 100 1000 log(dsr)N(dsr)\nFigure4: Convergenceproﬁleofthe Ni,averagedover50\nNNtimbrequeries.\ninsurance that, for a given seed song s, at a given step\ni, the set of items /tildewiderNiactually contains all the items in\nN2000(s). It followsthattheﬁnal set ofitemsreturnedby\nthe algorithm after a given series of steps (ci)iis only an\nestimate/tildewidestN2000(s)of the actual set N2000(s), associated\nwitha precision\np((ci)i, s) =|/tildewidestN2000(s)∩N2000(s)|\n|N2000(s)|(18)\nFigure 4 shows the estimated/tildewidest|Ni|fori= 1, ...,2000,\ncomputedonthetestdatabasebyaveragingthe Ni(s)over\nns= 50randomsongs. Thedarkestcurvecorrespondsto\nthe average m=1\nns/summationtextns\nk=1|Ni(sk)|, the medium curve\ncorresponds to the sum m+σof the average mand the\nstandarddeviation σofthe|Ni(sk)|,andthelightestcurve\ntom+ 2σ.\n3.4 Results\nWe apply our algorithmto the task of calculating the 100\nnearest neighbors of a given seed song according to the\ntimbre similarity described above. Table 1 shows the op-\ntimal sequence of steps (ci)iobtained with dynamic pro-\ngramming(see Section 2.3), andthe associated cost mea-\nsured by/summationtext\ni|Ni|t(ci). We compare the results using the\n3 sets of estimated/tildewidest|Ni|in Figure 4 and an additional set\nobtained by downsizing the |Ni|by 25%. For dynamic\nprogramming, we make the assumption that the cputime\nis linear t(ci) =α.i+β, with α= 1andβ= 0. It\nappearsthattheoptimalsequencesdifferslightlywhether\nwe consider the/tildewidest|Ni|with or without standard deviation.\nThe optimal sequence yields an algorithm which is theo-\nretically more than 30 times faster than the standard ap-\nproach.\nTable 2 shows the measured performance (cputime\nand precision) of the actual implementation of the algo-\nrithm for the same sequences of steps. Overall, the cpu\nperformanceisverygood(weachievespeedimprovement\nfactorsgreaterthan 30)while still preservingnear perfec t\nprecision (we retrieve 98% of the 100 true nearest neigh-\nbors). We observe that as the |Ni|increase, the precision\noftheresultsincreases(wearelesssubjectedtoaccidentl y\npruningrelevantnearest neighbors)but also the cpu time.\nWemayobservethattheachievedcputimeratesarelower\n235Table 1: Optimalsequencesaspredictedbydynamicprogramm ing\nStrategy Steps( |Ni|,i) cost(% standard)\nstandard {15554,2000 } 31,080k(100%)\nbest (mean) {15554,6 },{4501,20 },{2710,60 },{652,200 },{290,400 },{218,2000 }1,028k(3.3%)\nbest (mean-25%) {15554,6 },{3375,20 },{2032,60 },{489,200 },{217,400 },{163,2000 }793k(2.6%)\nbest (mean+ σ){15554,6 },{4090,60 },{894,200 },{374,400 },{264,2000 } 1,195k(3.9%)\nbest (mean+ 2 σ){15554,6 },{6819,60 },{1136,200 },{458,400 },{310,2000 } 1,532k(4.9%)\nthan thetheoreticalpredictions(about1% absolute). This\ncanbe explainedbythefollowingpoints:\n•The optimal sequence found by dynamic program-\nming and its expected performance were computed\nusing a very simple cpu time model t(ci) =i. This\ndoesn’tincludee.g. theoverheadcost of ﬁle I/O (re-\ntrievingtheGMMsfromthedatabase,writingthere-\nsults, etc.)\n•The distance algorithm was not reimplemented to\nsupport our recursive approach, i.e. the same exe-\ncutableisrunforthesuccessivevaluesof dsr. While\nthis makes the algorithm generic (no need to re-\nprogram the distance algorithm it uses), this has an\nunnecessary cost: each step adds the overhead of its\nownsystem call (theexecutableiscalledfromJava),\ninitialization, ﬁle I/O (all the needed GMM ﬁles are\nre-opened at each step, while |Ni+1| − |Ni|ﬁles\narecommonbetweeneachsuccessivecall),Gaussian\nsampling(ateachstep, dsripointsaresampledfrom\nthe Gaussians, while only dsri+1−dsrinew points\nare needed). Most of these overhead costs are not\naccountedforin thetheoreticalpredictions.\nTable 2: Measured cputime and precision of several se-\nquencesofsteps (ci)i\nSeries cpu-time(% stand.) precision\nstandard 663.75(100%) 100%\nbest (mean) 27.07(4.0%) 98.2%\nbest (mean-25%) 20.98(3.1%) 94.0%\nbest (mean+ σ)33.91(5.1%) 98.9%\nbest (mean+ 2 σ)39.19(6.0%) 99.0%\n3.5 Monte-CarloImprovements\nThe previous results show that the speed of convergence\noftheprecisionofthesuccessiveapproximationswith in-\ncreasing dsris an important factor to ensure both an im-\nportant speed improvement and a precise result set. We\npresenthereseveralvariantsoftheMonteCarlosampling\nmeant to improve the convergence of the approximation.\nNote that these variants don’t improve the overall preci-\nsionofthedistancealgorithm,butratherenablefastercal -\nculationusingthe algorithmdescribedinthispaper.3.5.1 Semi-DeterministicGaussiansampling\nFor very small dsr(1-10), it may be appropriate to max-\nimize the prototypicality of the drawn samples from a\nGMM by not drawing them at random (i.e. ﬁrst drawing\na Gaussian component according to the a priori distribu-\ntion,andthendrawingapointfromthechosenGaussian),\nbut rather by deterministically choosing the centers of\nthe Gaussian components. More precisely, we can use 3\nstrategiestosamplefromthe GMMs.\n1. successively pick the center of the largest Gaussian\ncomponents by decreasing importance (ﬁrst the\nlargestcomponent,thensecondlargest,etc.)\n2. pick the center of a randomly drawn Gaussian com-\nponent(accordingto thea prioridistribution)\n3. normal sampling : random Gaussian, random point\nintheGaussian.\nThe ﬁrst strategy is more deterministic than the second,\nwhich itself is more than the third, hence we can explore\nthe whole space of such variants using 2 cut points, cut1\nandcut2. Tosample dsr=npointsfromaGaussian,ap-\nply the ﬁrst strategy for the ﬁrst cut1points, then switch\nto the second strategy for cut2−cut1points, and ﬁ-\nnally use the third strategy for the remaining n−cut2\npoints. Figure 5 shows an exploration of the space de-\nﬁned by (cut1, cut 2), where cut1andcut2take values\nin{0,1,5,10,20,50}. We estimate the precision con-\nvergence by computing a=/summationtext2000\ni=1i.|Ni|, which corre-\nsponds to the area of the light gray curve in Figure 2 in\nthecasewhere t(ci)islinear. Thesmallerthe avalue,the\nfaster the convergence. We compute aby averaging over\n50 nearest neighbor queries on randomly drawn items in\nthe test database. Figure 5 showsthat an hybridsampling\nstrategy which consists in ﬁrst drawing the center of the\nlargest Gaussian ( cut1= 1), then drawing the centers\nof 9 randomly drawn Gaussians ( cut2= 10), and ﬁnish\nsampling with the standard strategy, is more than twice\nas effective than the standard strategy all through (which\ncorrespondsto {cut1= 0, cut 2= 0}.\n3.5.2 Antitheticvariantmethod\nThe antithetic variant method is a simple improvement\nmethodof Monte Carlo’s convergence,which is indepen-\ndent of the distribution type. It simply generates an extra\nrandomnumber yforeverygeneratednumber xbychang-\ningitssign y=−x. Thismakestheempiricalmeanofthe\nsequence tend to 0 with a signiﬁcant increase in conver-\ngence. The samples are then shifted and scaled in match\nthetargetdistribution’smeanandvariance.\n236Figure 5: Explorationofthe samplingvariantsdeﬁnedby\n(cut1, cut 2). The optimal is a hybrid sampling strategy\nwithcut1= 1andcut2= 10.\nFigure6: Comparisonofsamplingstrategies\nFigure 6 showsthe decreasing |Ni|in functionof dsr\nfor the normal sampling strategy (black curve), sampling\nwith antithetic variant method (gray curve) and a com-\nbination of antithetic variant and the semi-deterministic\nsampling(whitecurve). Thisshowsthatasigniﬁcantcon-\nvergenceincreasecanbeachievedusingthesemethods.\n4 Conclusion\nWe described a non-intrusive algorithm to quickly com-\npute the N nearest neighbors according to arbitrary sim-\nilarity measures which present a tradeoff between preci-\nsion and cputime. The algorithmuses successive approx-\nimations of the measure to compute more and more ex-\npensivemeasuresonsmallerandsmallersets. Weachieve\nspeed improvement factors as high as 30, while still pre-\nservingmorethan98%precision,whichpavesthewayfor\nreal-worldsized musicdatabases.\nACKNOWLEDGEMENTS\nThis work was partially foundedby the SemanticHiﬁ Eu-\nropeanproject(http://shf.ircam.fr/).References\nJ.-J.AucouturierandF.Pachet. Improvingtimbresimilar-\nity: How high’s the sky ? Journal of Negative Results\ninSpeechandAudioSciences ,1(1),2004.\nC. M. Bishop. Neural Networks for Pattern Recognition .\nOxfordPress, 1995.\nT. Bozkaya and M. Ozsoyoglu. Indexing large metric\nspacesforsimilaritysearchqueries. ACMTransactions\nonDatabaseSystems ,pages1–34,1999.\nP. Ciacca, M. Patella, and P. Zezula. M-tree: an efﬁcient\naccess method for similarity search in metrix spaces.\nInProceedingsofthe23rdInternationalConferenceon\nVeryLargeDatabases ,1997.\nJ. T. Foote. Content-based retrieval of music and audio.\nInMultimedia Storage and Archiving Systems II, Proc.\nofSPIE,1997. pages138-147.\nA.Juan,E.Vidal,andP.Aibar. Fastk-nearest-neighbours\nsearchingthroughextendedversionsoftheapproximat-\ning and eliminating search algorithm (aesa). In Pro-\nceedingsoftheFourteenthINternationalConferenceon\nPatternRecognition ,Brisbane,Australia,August1998.\nD. Miranker, W. Xu, and R. Mao. Mobios: a metric-\nspacedbmstosupportbiologicaldiscovery.In Proceed-\nings Of the InternationalConference On Scientiﬁc and\nStatistical Database Management Systems (SSDBM) ,\n2003.\nE. Pampalk, S. Dixon, and G. Widmer. On the evalua-\ntion of perceptual similarity measures for music. In\nProceedings of the Sixth International Conference on\nDigital Audio Effects DAFX, London (UK) , September\n2003.\nL. R. Rabiner and B. H. Juang. Fundamentals of speech\nrecognition . Prentice-Hall,1993.\nJ.Reiss,J.-J.Aucouturier,andM.Sandler.Efﬁcientmulti -\ndimensionalsearchingroutinesformir. In Proceedings\nofthe2ndInternationalSymposiumonMusicInforma-\ntion Retrieval (ISMIR), Bloomington, Indiana (USA) ,\nOctober2001.\nH. Samet. Applications of Spatial Data Structures .\nAddison-Wesley,1989.\nR. Typke, P. Giannopoulos, R. C. Veltkamp, F. Wiering,\nand R. van Oostrum. Using transportation distances\nfor measuring melodic similarity. In Proceedings of\ntheInternationalConferenceonMusicInformationRe-\ntrieval,October2003.\nE. Vidal, F. Casacuberta, J.M. Benedi, J. Lloret, and\nH. Rulot. On the veriﬁcation of triangle inequality by\ndynamic time-warping dissimilarity measures. Speech\nCommunication ,7(1):67–79,1988.\nM.Welsh,N.Borisov,J.Hill,R.vonBehren,andA.Woo.\nQuerying large collections of music for similarity.\nTechnicalReportTechnicalReportUCB/CSD00-1096,\nU.C.BerkeleyComputerScienceDivision,1999.\nE.WoldandT.Blum. Contentbasedclassiﬁcation,search\nandretrievalofaudio. IEEEMultimedia ,3(3),1996.\n237"
    },
    {
        "title": "Specmurt Analysis of Multi-Pitch Music Signals with Adaptive Estimation of Common Harmonic Structure .",
        "author": [
            "Shoichiro Saito",
            "Hirokazu Kameoka",
            "Takuya Nishimoto",
            "Shigeki Sagayama"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417707",
        "url": "https://doi.org/10.5281/zenodo.1417707",
        "ee": "https://zenodo.org/records/1417707/files/SaitoKNS05.pdf",
        "abstract": "This paper describes a multi-pitch analysis method using specmurt analysis with iterative estimation of the quasioptimal common harmonic structure function. Specmurt analysis (Sagayama et al., 2004) is based upon the idea that superimposed harmonic structure pattern can be expressed as a convolution of two components, a fundamental frequency distribution and a ‘common harmonic structure’ function if each underlying tone component has similar harmonic structure pattern. As proved in our previous work (Sagayama et al., 2004) inappropriate common structure function leads to inaccurate analysis results. The iterative algorithm proposed in this paper automatically chooses a proper structure, which results in finding concurrent multiple fundamental frequencies and reduces the dependency on heuristically chosen initial common harmonic structure. The experimental evaluation showed promising results. Keywords: audio feature extraction, specmurt analysis, visualization of the fundamental frequency. 1",
        "zenodo_id": 1417707,
        "dblp_key": "conf/ismir/SaitoKNS05",
        "keywords": [
            "multi-pitch",
            "analysis",
            "specmurt",
            "iteration",
            "common",
            "harmonic",
            "structure",
            "function",
            "fundamental",
            "frequency"
        ],
        "content": "SPECMURTANALYSISOF MULTI-PITCHMUSIC SIGNALSWITH\nADAPTIVEESTIMATIONOF COMMON HARMONIC STRUCTURE\nShoichiroSaito, HirokazuKameoka, TakuyaNishimoto and Shigeki Sagayama\nGraduate School of Information Science and Technology\nThe Universityof Tokyo\n7-3-1,Hongo, Bunkyo-ku,Tokyo,113-8656, Japan\nE-mail: fsaito,kameoka,nishi,sagayama g@hil.t.u-tokyo.ac.jp\nABSTRACT\nThis paper describes a multi-pitch analysis method using\nspecmurt analysis with iterative estimation of the quasi-\noptimal common harmonic structure function. Specmurt\nanalysis (Sagayama et al., 2004) is based upon the idea\nthat superimposed harmonic structure pattern can be ex-\npressed as a convolution of two components, a funda-\nmental frequency distribution and a ‘common harmonic\nstructure’functionifeachunderlyingtonecomponenthas\nsimilar harmonic structure pattern. As proved in our pre-\nvious work (Sagayama et al., 2004) inappropriate com-\nmonstructurefunctionleadstoinaccurateanalysisresults.\nThe iterative algorithm proposed in this paper automati-\ncally chooses a proper structure, which results in ﬁnding\nconcurrent multiple fundamental frequencies and reduces\nthe dependency on heuristically chosen initial common\nharmonic structure. The experimental evaluation showed\npromising results.\nKeywords: audio feature extraction, specmurt analysis,\nvisualization of the fundamental frequency.\n1 INTRODUCTION\nIn audio feature extraction, the fundamental frequency\nis one of the useful features characterizing music struc-\nture. Accurately extracted fundamental frequencies can\nbe translated into a musical score by applying some re-\ncently developed rhythm recognition techniques as post-\nprocessing. What makes it difﬁcult to extract funda-\nmental frequencies from multi-pitch signals is the exis-\ntence of harmonics. In general, harmonic structure pat-\nterns differ among instruments or fundamental frequen-\ncies, and also vary along time. Top-down approaches us-\ningGraphicalmodelssuchasBayesiannetworksandHid-\nden Markov Models (HMM) for detecting pitch class of\nnoteevent(formusictranscriptionuse)wererecentlypro-\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc°2005Queen Mary,Universityof Londonposed by Kashino et al. (1995), Raphael (2002), Cemgil\net al. (2003). Feder and Weinstein (1988), Goto (2004),\nGodsill and Davy (2002) and Kameoka et al. (2005) pro-\nposed parametric harmonic structure pattern models, that\nallow ﬂexible estimation. Such parametric models have\nstrong advantages especially for the purpose of precisely\nanalyzing real-world audio signals. Meanwhile, we have\nproposed specmurt analysis , another simple, yet efﬁcient\napproach to multi-pitch audio signals(Sagayama et al.,\n2004).\nIn this method, a common harmonic structure is as-\nsumedandfundamentalfrequencydistributionisobtained\nby deconvolution of the power spectrum in log-frequency\ndomainbythecommonharmonicstructure. Thisisanew\nmethod in that fundamental frequency distribution can be\nobtained as an analytic solution, but at the same time it\nhas the drawback that the accuracy of the calculated fun-\ndamentalfrequencydependsontheinitialenvelopeofthe\ncommonharmonic structure.\nThis paper proposes an adaptive estimation of the\ncommonharmonicstructurepatternforeachframewhich\nmaximizes the resolution between signiﬁcant fundamen-\ntals and other unnecessary components in specmurt anal-\nysisthrough iterativenon-linear mapping.\nThe rest of the paper is organized as follows. After\nreviewing specmurt analysis in section 2, we discuss the\nalgorithm for quasi-optimizing the harmonic structure in\nsection 3 followed by evaluation of the accuracy of the\nmethodin section 4 and conclusion in section 5.\n2 SPECMURTANALYSIS\nHarmonicsignalssuchassingletonesinmusiccontainan\nenergy component of fundamental frequency, f1, as well\nasmultipleovertoneswhosefrequenciesareintegralnum-\nber multiples, nf1; n= 2;3;4; : : :, of the fundamental\nfrequency. In linear frequency scale f, the distance be-\ntween fundamental frequency and the n-th harmonic fre-\nquency is (n¡1)f1and depends on the fundamental fre-\nquency. In logarithmic frequency (log-frequency) scale\nx= log f, on the other hand, the harmonic frequencies\nare located log2, log3,: : :, lognaway from the funda-\nmental log-frequency, x1= log f1; the relative positions\nremainconstantnomatterhowthefundamentalfrequency\nﬂuctuatesandis an overallparallel shift dependingon the\ndegreeof ﬂuctuation (see Fig. 1).\n84We now deﬁne a commonharmonic structure pattern,\nh(x), as a function of log-frequency, x, choosing the ori-\nginx= 0at the fundamental frequency, and h(0) = 1.\nWe assume that the relative powers at harmonic frequen-\ncies are constant and independent of the fundamental fre-\nquency. If h(x)is shifted by ¢xto the direction in which\nxincreases, this pattern represents the harmonic structure\npatternwhosefundamentalfrequencyis ¢x. Therefore,if\npower spectrum is additive1, the multi-pitch power spec-\ntrum, v(x), is represented by the convolution of the com-\nmon harmonic structure, h(x), and the power distribution\nfunction, u(x),representingthepowerofthefundamental\nfrequencycomponent at log-frequency x,i.e.,\nv(x) =h(x)¤u(x): (1)\nConversely,ifamulti-pitchspectrum ~v(x)isobserved,we\ncan calculate the distribution of fundamental frequencies,\nu(x),by deconvolution:\nu(x) =h(x)¡1¤~v(x): (2)\nAccording to the convolution theorem, convolution be-\ncomes multiplication in the frequency domain by Fourier\ntransform. Suppose that U(y),H(y)and ~V(y)are the\ninverse Fourier-transformed function of u(x),h(x)and\n~v(x), respectively. U(y)is obtained by dividing ~V(y)by\nH(y):\nU(y) =~V(y)\nH(y); (3)\nwhere ymeansFouriertransformedlog-frequency. There-\nfore, we can calculate u(x)as the Fourier transform of\nU(y):\nu(x) =F[U(y) ] (4)\nIn this way, we can estimate the fundamental fre-\nquency distribution from the multi-pitch spectrum. We\ncall this method “ specmurt analysis ” (Sagayama et al.,\n2004).\nTheillustrationofthisprocessisbrieﬂyshowninFig.\n2, 3. The process is done over every short-time analy-\nsis frame and thus we ﬁnally have a time series of funda-\nmentalfrequencycomponents,i.e.,apiano-roll-likevisual\nrepresentation, with a small amount of computation.\nIn short-time analysis, spectrum does not look like\nan impulse (Dirac’s delta-function) sequence, but has\nsome broadening coming from the inﬂuence of window-\nfunction,etc : : :. Whenweusespecmurtmethod,itisnec-\nessarytoevenupthewidthofbroadeningofthespectrum\nat every (log) frequency. This problem is solved by us-\ning wavelet transform and constant-Q ﬁlter in calculating\nv(x).\nWecreatedtheword“specmurt”fortheFouriertrans-\nform of linear-scaled spectrum along log-frequency axis\nby reversing the last four letters in “spectrum” following\n1Strictly speaking, this assumption is true only in the expec-\ntation sense. The power of the sum of sinusoids of identical\nfrequency is not always equal to the sum of the powers of the\nsinusoids, butdepends on the phases of the sinusoids.\nDw 2Dw 3Dw 4Dww\nwLinear frequency domain\nxLog-frequency domain\nxDFT spectrum Wavelet spectrum\nDx Dx DxDx\nFigure1: Linear-and log-scaled harmonic structures\nxh(x)u(x) v(x)\nfundamental frequencypatterncommon  harmonic structure pattern\ngenerated multi-pitch spectrum\nxx1 2xx1 2\nFigure2: Conceptual diagram of convolutionwith h(x)\nFT\nv(x)observed spectrum\nx\nV(y)xh(x)common  harmonic structure pattern\n?\nH(y) U(y)u(x)fundamental frequencydistribution\nIFT IFT\nspecmurt domainx~\n~\nFigure3: Outlineof specmurtanalysis formulti-pitchsig-\nnals\ntheterminologyof cepstrum whichwascreatedbyrevers-\ning the ﬁrst four letters of “spectrum”and represents the\nFourier transform of log-scaled spectrum with linear fre-\nquency. It should be noted that spectrum logarithmically\nscaled both in frequency and in magnitude is identical to\nBode diagram often used in the automatic control theory.\nIts Fourier transform has no speciﬁc name, while it is es-\nsentially similar to mel-scaled frequency cepstrum coefﬁ-\ncients (MFCC) and is very often used in the feature anal-\nysisin speech recognition.\n3 OPTIMIZATIONOF HARMONIC\nSTRUCTUREPATTERN\n3.1 The Role of Common Harmonic Structure\nThe common harmonic structure pattern, which is de-\nscribed in Section 2, is based on the assumption that the\npattern is constant regardless of the sound source, but ac-\ntuallytheharmonicstructurepatterndependsonthesound\nsource and the fundamental frequency. In addition, there\nis a quite low possibility that the default harmonic struc-\nture pattern corresponds with the optimal harmonic struc-\nture pattern of the sound source, and it is unrealistic to\n85 0 10000 20000 30000 40000\n 0  100  200  300  400  500  600Power spectrum\nSample number of log-frequency (resolution: 100points=1200cent)C4\nE4\nFigure4: Powerspectrum(linearscale)ofmulti-pitchau-\ndiosignalofviolin’sC4andE4withlog-scaledfrequency.\n-10000 0 10000 20000 30000 40000\n 0  100  200  300  400  500  600Fundamental frequency distribution\nSample number of log-frequency (resolution: 100points=1200cent)\n(a)u(x)withf¡1:5chosenfor h(x)\n-10000 0 10000 20000 30000 40000\n 0  100  200  300  400  500  600Fundamental frequency distribution\nSample number of log-frequency (resolution: 100points=1200cent)\n(b)u(x)withf¡0:5chosenfor h(x)\nFigure 5: Estimated fundamental frequency distribution,\nu(x), obtained by specmurt analysis. Figs. (a) and (b)\nchose f¡1:5andf¡0:5, respectively, along linear fre-\nquency fas the envelope of the common harmonic struc-\ntureh(x).\nchange the default pattern little by little until ﬁnding the\noptimal pattern.\nFig. 4 shows the multi-pitch spectrum obtained by\nadding the real audio signal of violin’s C4 and E4. Hori-\nzontal axis is in log-frequency domain. The fundamental\nfrequency distribution u(x), calculated from v(x)of Fig.\n4, is shown in Fig. 5. In case of (a), the envelope of the\ncommonharmonicstructurepattern h(x)isassumedtobe\nf¡1:5(then-thharmoniccomponenthasapowerratioof\n1=p\nn3relativetothefundamentalfrequencycomponent)\nand in the case of (b) it is assumed to be f¡0:5. In Fig. 5\n(a), the ﬁrst harmonic overtone still has a large power be-\ncause f¡1:5envelope has small effect for the attenuation\nof harmonic overtones. On the other hand, in Fig. 5(b),\nthoughthesecondharmonicovertoneisrelativelyreduced\nin power, there arise heavily ﬂuctuating power and many\nunwanted components as well as negative components in\nthe entire range of frequency.\nb u(x)u(x)\nf (u(x))Identity mapping\nFigure6: Concept of mapping from u(x)to¹u(x)\nFor the purpose of solving this problem, we propose\nan adaptive estimation algorithm. This algorithm consists\nof two steps and generates quasi-optimal h(x)through it-\nerations. First, in Section 3.2 (step I), we renew u(x)cal-\nculatedbyspecmurtanalysistoamorepreferable(ormore\naccurate)distribution ¹u(x),andinSection3.3(stepII)we\nparameterize h(x)in¹h(x;£)andestimate £tooptimize\n¹h(x;£). This estimated harmonic structure pattern ¹h(x)\nisquasi-optimal,andwecangenerateamoreaccuratefun-\ndamentalfrequencydistribution u(x)byapplying ¹h(x)to\nspecmurt analysis. The following describes these steps in\ndetail.\n3.2 Step I: Non-Linear Mapping of Fundamental\nFrequencyDistribution\nIt is difﬁcult to distinguish deﬁnitely between true fun-\ndamental frequency components and unwanted frequency\ncomponents in u(x)obtained through specmurt analysis.\nIn consideration of this problem, we introduce a non-\nlinear mapping function to update fundamental frequency\ndistribution, which has a fuzziness parameter ®, and a\nthreshold magnitude parameter ¯.¯means the value un-\nder which frequency components are assumed to be un-\nwanted, and ®represents the degree of fuzziness of the\nboundary( ® > 0).\nAsanexampleofthisfunction,weproposeamapping\nbasedon the sigmoid function as follows:\n¹u(x) =u(x)\n1 + exp f¡®(u(x)¡¯)g(5)\nThis mapping returns almost the same value when\nu(x)is signiﬁcantly larger than ¯, and a much smaller\nvalue when u(x)is smaller than or near ¯. Fig. 6 shows\na sketch of mapping from u(x)to¹u(x).¹u(x)is 50% of\nu(x)when u(x) =¯, and for other values of u(x), the\nsuppression effect of this mapping depends on ®. When\n®is small, the suppression effect is small and suppres-\nsion range is wide. If ®is large enough, the mapping is\nsimilar to a threshold model. In addition, if ¯is a large\nnegativevalue,thismappingisapproximatelyequaltothe\nidentity mapping. Fig. 7 shows four typical mappings of\nu(x)to¹u(x)and Fig. 8 shows the experimental results\nof applying mappings in Fig. 7 to fundamental frequency\ndistributionof Fig. 5 (a) ( f¡1:5envelope).\n86-0.2 0 0.2 0.4 0.6 0.8 1\n-0.2  0  0.2  0.4  0.6  0.8  1u(x)u±(x)\n(a)®= 15000 ; ¯=¡10000\n-0.2 0 0.2 0.4 0.6 0.8 1\n-0.2  0  0.2  0.4  0.6  0.8  1u(x)u±(x)\n(b)®= 15000 ; ¯= 0\n-0.2 0 0.2 0.4 0.6 0.8 1\n-0.2  0  0.2  0.4  0.6  0.8  1u(x)u±(x)\n(c)®= 15 ; ¯= 0:5\n-0.2 0 0.2 0.4 0.6 0.8 1\n-0.2  0  0.2  0.4  0.6  0.8  1u(x)u±(x)\n(d)®= 15000 ; ¯= 0:5\nFigure7: Fourtypical mapping from u(x)to¹u(x).\n-10000 0 10000 20000 30000 40000\n 0  100  200  300  400  500  600Fundamental frequency distribution\nSample number of log-frequency (resolution: 100points=1200cent)\n(a)®= 15000 ; ¯=¡10000\n-10000 0 10000 20000 30000 40000\n 0  100  200  300  400  500  600Fundamental frequency distribution\nSample number of log-frequency (resolution: 100points=1200cent)\n(b)®= 15000 ; ¯= 0\n-10000 0 10000 20000 30000 40000\n 0  100  200  300  400  500  600Fundamental frequency distribution\nSample number of log-frequency (resolution: 100points=1200cent)\n(c)®= 15 ; ¯= 0:5\n-10000 0 10000 20000 30000 40000\n 0  100  200  300  400  500  600Fundamental frequency distribution\nSample number of log-frequency (resolution: 100points=1200cent)\n(d)®= 15000 ; ¯= 0:5\nFigure8: Sigmoid-mapped fundamental frequencydistributionof Fig. 5(a).\nBy using this mapping, the components of u(x)that\nhave small or negative power are brought close to zero,\nand the middle power components are still left place for\nremaining as major fundamental frequency components,\nunlikea simple threshold mapping.\n3.3 Step II: Optimization of the Common Harmonic\nStructurePattern\n¹u(x)generatedinstepIismoreaccuratethan u(x)inthat\ntheunwantedcomponentsaresuppressed,thereforeasup-\nposedfunction ¹h(x),whichisgeneratedbydeconvolution\nof observed multi-pitch spectrum ~v(x)with ¹u(x), will be\ncloser to the actual harmonic structure pattern.\nWe now deﬁne ¹h(x;£), a function parameterized by\n£n, which is the amplitude ratio of the n-th harmonic\novertonerelativetofundamentalfrequency(showninFig.\n9). Thus ¹h(x;£)isdescribed as follows:\n¹h(x;£) = £ 0±(x¡­o) +¢ ¢ ¢+ £ N±(x¡­N)\n=NX\nn=0£n±(x¡­n) (6)\nwhere ­nis the x-coordinate of the n-th harmonic over-\ntone on log-frequency scale, and so £0= 1and­0= 0.\nThen an ideal multi-pitch spectrum ¹v(x), generated by\nconvolution of ¹h(x;£)and¹u(x)calculated in step I, is\nalso parameterized by £, and we write it ¹v(x;£). Ac-\ntually, ¹v(x;£)cannot be completely matched observed\nspectrum ~v(x), hence we want to know the parameter\n¹£=f¹£1;¢ ¢ ¢;¹£Ngthat minimizes the integral square\nerrorbetween ~v(x)and¹v(x;£). Theintegralsquareerror\nis\nZ1\n¡1f~v(x)¡¹v(x;£)g2dx ; (7)whichis rewrittenin discrete calculation by:\nI¡1X\ni=0½\n~v(xi)¡¹v(xi;£)¾2\n(8)\nwhere Iindicates the number of log-frequency samples.\nDifferentiating eq.8 partially in £and making it equal to\nzero,the equation belowis obtained:\n@\n@£I¡1X\ni=0(NX\nn=0£nu(xi¡­n))2\n= 2@\n@£I¡1X\ni=0~v(xi)(NX\nn=0£nu(xi¡­n))\n(9)\nHence, we obtain the following system of linear equa-\ntions:\n0\nBBBBBB@A1;1¢ ¢ ¢A1;n¢ ¢ ¢A1;N\n.........\nAn;1¢ ¢ ¢An;n¢ ¢ ¢An;N\n.........\nAN;1¢ ¢ ¢AN;n¢ ¢ ¢AN;N1\nCCCCCCA0\nBBBBBB@£1\n...\n£n\n...\n£N1\nCCCCCCA=0\nBBBBBB@B1\n...\nBn\n...\nBN1\nCCCCCCA;\n(10)\nwhere\nAj;k =I¡1X\ni=0u(xi¡­j)u(xi¡­k);(11)\nBj=I¡1X\ni=0½\n~v(xi)¡u(xi)¾\nu(xi¡­j):(12)\nNow we can work out the system by calculating the\ninverseofthematrixontheleftside,forexamplewiththe\nuse of LU decomposition. Then updating h(x)using the\n87 0 0.2 0.4 0.6 0.8 1Relative amplitude of harmonic component\nLog-frequencyQ0\nQ1\nQ2\nQ3Q4\nQ5Q6\nQ7Q8\nW0 W1 W2 W3W4W5W6W7W8\nFigure 9: Parameterized harmonic structure pattern\n¹h(x;£)\n-10000 0 10000 20000 30000 40000\n 0  100  200  300  400  500  600Fundamental frequency distribution\nSample number of log-frequency (resolution: 100points=1200cent)\n(a)Starting from f¡1:5forh(x)\n-10000 0 10000 20000 30000 40000\n 0  100  200  300  400  500  600Fundamental frequency distribution\nSample number of log-frequency (resolution: 100points=1200cent)\n(b) Starting from f¡0:5forh(x)\nFigure 10: Improved fundamental frequency distribution\nofFig. 5after5iterations( ®= 15; ¯= 0:5)startingfrom\nFig. 5(a) and (b), respectively.\noptimalparameter ¹£andapplyingittospecmurtanalysis,\nwe obtain the fundamental frequency distribution again.\nWeget back to step I, and iterate.\n3.4 Results of Iteration on a Certain Frame\nFig. 10 shows the fundamental frequency distributions\nwhicharegeneratedfromthedistributionofFig. 5(a)and\n(b) through 5 iterations of proposed algorithm. In both\ndistribution C4 and E4 are properly emphasized and un-\nwantedcomponents are strongly suppressed.\n4 EXPERIMENTALEVALUATION\n4.1 Visualization\nAnotheraspectofspecmurtanalysisisthatthefundamen-\ntal frequency distribution resulting from the process can\nbe easily visualized. That is, unlike the estimation of var-\nious parameters, the result is much comprehensible for\nFrame numberSample number of log-frequency\n0100200300400500\n50010001500\n0Intensity\nFigure11: 3D-viewoffundamentalfrequencydistribution\nof data8 (piano) obtained by specmurt analysis through 5\niterations( ®= 15; ¯= 0:5).\nusers. An example is shown in Fig. 11. The ﬁgure is gen-\nerated from the u(x)of data8 in Table 2 after 5 iterations\nwhere ®=15and ¯=0.5, In this ﬁgure,time-frequency-\nintensity space is considered. Note that intensity does not\nmeanwhetherafrequencyisfundamentalornot,butgives\na hint about the possibility that a frequency is fundamen-\ntal. By looking at this ﬁgure the user can understand the\ndistribution intuitively and if estimation error occurs, for\nexample of the tone at a certain pitch actually sounds but\nspecmurt analysis drop it out, they can look for the next\ncandidateeasily.\nThe 2-dimensional views of the fundamental fre-\nquencydistributionareshowninFig. 12–15. Fig. 12dis-\nplays the contrast density of the power spectrum at each\nshort frame. Fig. 14 shows the fundamental frequency\ndistribution u(x)without using iteration and Fig. 15 with\n5iterations. LookingatFig. 12,onecanseethatthereare\na lot of overtones, but in Fig. 14 many of overtones dis-\nappear or strongly attenuated, and In Fig. 15 much more\novertones are attenuated. By means of iterative estima-\ntion, Fig. 15 comes closer to Fig. 13, the correct funda-\nmentalfrequencydistribution.\nHere, let us havea closer look ontwosegmentsof the\nFig. 15, which are between the frame approximately 470\n–560 and approximately 1200 – 1300. In the ﬁrst seg-\nment, a double-F0 error is made in estimation. However,\ndouble-F0 is also much stronger in Fig. 12. That is to\nsay, the harmonic structure in this segment has a kind of\nmissing fundamental feature, and specmurt analysis can-\nnot estimate it correctly in this case in principle. In the\nsecond segment, the fundamental frequency near sample\nnumber 300 disappears, while in Fig. 14 it exists. This\nis because the frequency at 300 is the fourth harmonic of\nthe frequency at 100 whose amplitude is largest in these\nframes. As estimation is iterated, the harmonic structure\nof the lower and stronger fundamental frequency takes in\nthe harmonic structure of the higher and weaker funda-\nmental frequency and it seems to be optimal. These are\nweakpointsofspecmurtanalysisandweneedtoexamine\n88Figure 12: Observed spectrogram of data8 (piano)\nwhere overlapping harmonics make it difﬁcult to fol-\nlowmultiplefundamentals. (Usedasinputtospecmurt\nanalysis)\n 0 100 200 300 400 500\n 0  200  400  600  800  1000  1200  1400Sample number of log-frequency\nFrame number\nFigure 13: Handcrafted MIDI reference of funda-\nmental frequency time pattern (piano-roll display) for\ndata8, each red line indicating a single note event acti-\nvation.\nFigure 14: Fundamental frequency of data8 obtained\nthrough specmurt analysis with a ﬁxed common har-\nmonicstructure.\nFigure 15: Estimated fundamental frequency of data8\nobtained through specmurt analysis with adaptive es-\ntimation of common harmonic structure after 5 itera-\ntions.\nthe issues.\n4.2 Preparation\nIn this section, we applied the specmurt analysis with it-\nerative algorithm to several music signals to obtain their\nfundamentalfrequencydistributionsandevaluatetheirac-\ncuracy. Specmurt analysis is not a method to estimate\ndiscrete parameters or states but, so to speak, to empha-\nsize the (continuous) fundamental frequency distribution.\nTherefore, there is no obvious criterion for evaluation of\nthe accuracy, but as one strategy, we considered a certain\nfrequency as being in “ON” state if the amplitude was\nover a certain threshold intensity. To evaluate the accu-\nracy, we ﬁrst divided the fundamental frequency distribu-\ntionateachtimeandfrequencyintotwostates(“ON”and\n“OFF”) by comparing the peak of distribution with the\nthreshold. After that we compared the obtained two-state\ntable with correct two-state table and calculated the accu-\nracy.\nTo compare two tables, we used DP matching algo-\nrithmforeachnotenumber. Matchingperfectlyinallnote\nnumbers the accuracy was 100%. If there were idele-\ntion and jinsertion errors, the accuracy was deﬁned as\n100¤(N¡i¡j)=N,where Nisthenumberof“ON”inTable1: Experimental conditions for specmurt analysis.\nanalysis\nsamplingrate\n16kHz (monaural)\nframeshift\n16 ms\nwaveletfunction\nGabor function\n°\n50\nfrequencyresolution\n12 cent\nfrequencybandwidth\n60 – 7626.95 Hz\nh(x)\nnumberof harmonics\n8\nsigmoidmapping\n®= 15; ¯= 0:5\nnumber of iterations\n5\nthereference table.\nThe strategy to determine threshold amplitude was as\nfollows. First, counting the positive amplitude of u(x)at\nevery time and frequency and making an amplitude his-\ntogram, we chose as a threshold candidate the amplitude\nsuch that xpercent largest amplitudes where higher than\nit. In other words, this threshold turned only the xper-\ncent largest amplitudes into “ON” state. This time, we\nadopted largest 8 threshold amplitudes, or xis 1, 2, : : :,\n8. Note that this strategy is temporary and arbitrary, and\notherpolicies can be employed.\n89\u0000\n\u0001\u0003\u0002\n\u0002\n\u0001\u0003\u0002\u0004\u0003\u0002\n\u0005\u0006\u0002\n\u0007\u0003\u0002\n\b\t\u0002\u0003\u0002\n\u0003\u000b\r\f \u000b\u000f\u000e\u0010\n\u0003\u000b\r\f \u000b\u0006\u0011\u0012\n\u0003\u000b\r\f \u000b\u0003\u0013\u0014\n\u0003\u000b\r\f \u000b\u0003\u0015\u0016\n\u0003\u000b\r\f \u000b\u0018\u0017\u0019\n\u0003\u000b\r\f \u000b\u0006\u001a\u0016\n\u0003\u000b\r\f \u000b\u0006\u001b\u0016\n\u0003\u000b\r\f \u000b\u001d\u001c\n\u001e \u001f\u001f !\"\u001f#\n$ %\n&\n'\u0006(*) +-,/. 0*1 '\u0006(*) +324. ,51 '\u0003(*)\t+627. 051 '\u0006(*) +989. ,-1\n(a)Results from initial common harmonic structure (no iteration)\n\u0000\n\u0001\n\u0000\n\u0002\n\u0000\n\u0003\n\u0000\n\u0004\n\u0000\n\u0005\u0006\u0000\u0007\u0000\b\u0006\t\u000b\n \t\u0007\f\r\b\u0006\t\u000b\n \t\u000f\u000e\u0010\b\u000f\t\u000f\n \t\u0006\u0011\u0012\b\u000f\t\u000b\n \t\u000f\u0013\u0010\b\u000f\t\u000b\n \t\u0015\u0014\u0016\b\u0015\t\u0017\n \t\u0006\u0018\u0019\b\u0015\t\u0017\n \t\u000f\u001a\u0012\b\u0006\t\u000b\n \t\u0006\u001b\n\u001c \u001d\u001d\u001e\u001f \u001d!\n\" #\n$\n%\u000f&(' )(*,+ -(. %\u000f&(' )0/1+ *(. %\u000f&(' )0/1+ -(. %\u0006&(' )323+ *3.\n(b) 5 iterations to optimize the common harmonic structure\nFigure 16: Multi-pitch accuracy of specmurt analysis evaluated over 8 music pieces with errorbars showing the range\nbetween the highest and the lowestaccuracies. (a) Results with no iteration. (b) Results with 5 iterations.\n4.3 Evaluationof Effectivenessof Adaptive\nEstimation\nIn this experiment, we used 8 pieces of real music perfor-\nmance data excerpted from RWC music database (listed\nin Table 2). 23s long WAV ﬁles were used in specmurt\nanalysis and MIDI ﬁles were used to make correct “ON”\ntable. The power spectrum in log-frequency domain ~v(x)\nwas calculated using Gabor wavelet transform. Other ex-\nperiment conditions are shown in Table 1. We calculated\ntheaccuracyonbothconditionsthatiterationalgorithmis\nnot applied and that it is applied 5 times while changing\nthe initial envelope of h(x)among four types from f¡0:5\ntof¡2:0.\nThe results are shown in Fig. 16. In Fig. 16(a) iter-\nation algorithm was not used, and in Fig. 16(b) used 5\ntimes. The height of a box means the average of the ac-\ncuracy values of 8 thresholds. The bottom of an errorbar\nmeans the minimum value among the 8 accuracy values\nand the top means the maximum value.\nWhen no iterative estimation was used, the accuracy\nwas dependent on the initial envelope of the harmonic\nstructure and, especially in case of f¡0:5initial envelope,\nthe accuracy was rather low. On the other hand, in Fig.\n16(b), the dependency on the initial envelope disappearsand all of the accuracy values were as high as or higher\nthanthe maximum accuracyvalueof Fig. 16(a).\n4.4 Evaluationof Sigmoid Mapping Parameter\nIn this experiment, we evaluated the accuracy depending\non different sigmoid parameters, ®and¯. As shown in\nFig. 7, we selected four typical mappings. In addition,\nwe prepared 12 types of mapping, by changing the value\nof¯. The initial envelope of h(x)wasf¡1:5, and other\nexperiment conditions and used music database were the\nsame as in the previous experiment. But unlike the previ-\nous experiment, because the average accuracy and whole\ntendency show little difference, we decided to use in the\nevaluation the maximum accuracy of 8 data obtained by\nchangingthe threshold from 1% to 8%.\nThe results are shown in Table 3. In each ﬁeld, we\nshow the maximum accuracy for each data and set of pa-\nrameters.\nThere were small differences between the results, but\ntheaccuracytendedtobehigherwhenusingthetype(c)of\nsigmoidmapping. Itwouldbeunwisetoconcludethatthis\ntendency is general, but one can think that if you choose\ntype (c) as the sigmoid mapping, the accuracy will not be\nsmall. This needs to be analyzed in more details.\n90Table2: List of The Experimental Data Excerpted from RWCMusic Database \nSymbol\nTitle(Genre)\nCatalog number\nComposer/Player\nInstruments\ndata1\nCrescentSerenade (Jazz)\nRWC-MDB-J-2001No.9\nS. Yamamoto\nGuitar\ndata2\nForTwo(Jazz)\nRWC-MDB-J-2001No.7\nH. Chubachi\nGuitar\ndata3\nJive(Jazz)\nRWC-MDB-J-2001No.1\nM. Nakamura\nPiano\ndata4\nLoungeAway(Jazz)\nRWC-MDB-J-2001No.8\nS. Yamamoto\nGuitar\ndata5\nForTwo(Jazz)\nRWC-MDB-J-2001No.2\nM. Nakamura\nPiano\ndata6\nJive(Jazz)\nRWC-MDB-J-2001No.6\nH. Chubachi\nGuitar\ndata7\nThreeGimnopedies no. 1 (Classic)\nRWC-MDB-C-2001No.35\nE. Satie\nPiano\ndata8\nNocturneno.2, op.9-2(Classic)\nRWC-MDB-C-2001No.30\nF.F.Chopin\nPiano\nTable3: Maximum accuracyfor each sigmoid parameter\n®\n¯\ndata1\ndata2\ndata3\ndata4\ndata5\ndata6\ndata7\ndata8\n15000\n-10000\n90.2%\n78.9%\n73.8%\n74.1%\n49.0%\n77.5%\n70.8%\n65.0%\n15000\n0\n88.6%\n76.5%\n71.3%\n73.9%\n47.3%\n73.3%\n66.5%\n62.0%\n15\n0.2\n87.9%\n79.1%\n72.9%\n76.1%\n49.1%\n76.3%\n68.6%\n65.2%\n15\n0.3\n88.3%\n79.7%\n73.6%\n74.3%\n49.4%\n77.5%\n69.3%\n65.6%\n15\n0.4\n88.6%\n80.9%\n74.1%\n74.8%\n49.4%\n78.6%\n72.1%\n67.2%\n15\n0.5\n87.9%\n80.7%\n74.3%\n73.5%\n50.9%\n78.9%\n72.2%\n68.2%\n15\n0.6\n88.1%\n79.3%\n75.4%\n75.6%\n48.8%\n78.4%\n71.9%\n66.6%\n15000\n0.2\n87.5%\n78.8%\n73.4%\n75.3%\n49.1%\n75.4%\n67.8%\n64.6%\n15000\n0.3\n88.1%\n79.6%\n73.3%\n75.4%\n49.3%\n76.2%\n71.2%\n65.4%\n15000\n0.4\n88.5%\n79.5%\n73.9%\n75.7%\n49.3%\n77.4%\n71.4%\n67.1%\n15000\n0.5\n88.8%\n80.4%\n74.5%\n74.8%\n49.2%\n78.2%\n72.0%\n67.0%\n15000\n0.6\n87.9%\n80.3%\n74.1%\n73.4%\n50.0%\n78.7%\n71.5%\n68.6%\n5 CONCLUSION AND FUTURE WORK\nInthispaper,weproposedamethodofiterativeestimation\nof quasi-optimal harmonic structure pattern in specmurt\nanalysis. We have discussed the two steps of the algo-\nrithm and the analytical calculation of the quasi-optimal\nharmonic structure. This method gives a more accurate\nfundamentaldistributionwhichdependslessontheinitial\ncommonharmonicstructurepattern. Inaddition,wecould\nvisualize the fundamental frequency distribution. Spec-\nmurt analysis is a user-friendly method and the proposed\niteration algorithm makes specmurt analysis more accu-\nrate and robust.\nThere is, however, still a room for improving the\nmethod. InstepIoftheiteration,thereisnomathematical\nguarantee that the mapping from u(x)to¹u(x)takes u(x)\ncloser to optimum (and this is why we say “quasi” opti-\nmizing). This mapping strategy is based on the assump-\ntionthatthepoweroffundamentalfrequencycomponents\nshouldnottakenegativeorlowvalues. Thoughthisrarely\nharmstheconvergenceofiterativeestimationofharmonic\nstructurepattern(actuallyobservedinafewframesoutof\nover 1000 frames), we wish to solve the optimality and\nstability problems of the iterativeestimation.\nReferences\nA. T. Cemgil, B. Kappen, and D. Barber. Generative\nmodel based polyphonic music transcription. In Proc.\nWASPAA2003 ,2003.M. Feder and E. Weinstein. Parameter estimation of su-\nperimposed signals using the em algorithm. ASSP, 36\n(4):477–489,1988.\nS. Godsill and Manuel Davy. Baysian harmonic mod-\nels for musical pitch estimation and analysis. In\nProc.IEEE, International Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP2002) , pages\n1769–1772,2002.\nM. Goto. A real-time music-scene-description system:\npredominant-f0 estimation for detecting melody and\nbass lines in real-world audio signals. ISCA Journal ,\n43(4):311–329,2004.\nH. Kameoka, T. Nishimoto, and S. Sagayama. Minimum\nbic estimate of harmonic kernel regression model for\nmulti-pitchanalysis. Trans.IEEE.(submitted) , 2005.\nK. Kashino, K. Nakadai, and H. Tanaka. Organization\nof hierarchical perceptual sounds: Music scene analy-\nsis with autonomous processing modules and a quan-\ntitive information integration mechanism. In Proc. IJ-\nCAI,volume1, pages 158–164, 1995.\nC. Raphael. Automatic transcription of piano music. In\nProc.International Conference on Music Information\nRetrieval(ISMIR2002) , pages 15–19, 2002.\nS. Sagayama, K. Takahashi, H. Kameoka, and T. Nishi-\nmoto. ”specmurt analysis: A piano-roll-visualization\nof polyphonic music signal by deconvolution of log-\nfrequency spectrum”. In SAPA2004 , pages in CD–\nROM,2004.\n91"
    },
    {
        "title": "Online Database of Scores in the Humdrum File Format.",
        "author": [
            "Craig Stuart Sapp"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417281",
        "url": "https://doi.org/10.5281/zenodo.1417281",
        "ee": "https://zenodo.org/records/1417281/files/Sapp05.pdf",
        "abstract": "KernScores, an online library of musical data currently consisting of over 5 million notes, has been created to assist projects dealing with the computational analysis of musical scores.  The online scores are in a format suitable for processing with the Humdrum Toolkit for Music Research, but the website also provides automatic translations into several other popular data formats for digital musical scores.",
        "zenodo_id": 1417281,
        "dblp_key": "conf/ismir/Sapp05",
        "keywords": [
            "KernScores",
            "online library",
            "musical data",
            "over 5 million notes",
            "computational analysis",
            "Humdrum Toolkit",
            "Music Research",
            "automatic translations",
            "popular data formats",
            "digital musical scores"
        ],
        "content": "Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full \ncitation on the first page. \n© 2005 Queen Mary, University of London ONLINE DATABASE OF SCORES IN THE HUMDRUM FILE FORMAT\nCraig Stuart Sapp \nCenter for Computer Assisted Research in the Humanities \nStanford University \nCentre for the History and Analysis of Recorded Music \nRoyal Holloway, University of London \ncraig@ccrma.stanford.edu \nABSTRACT \nKernScores, an online library of musical data currently \nconsisting of over 5 million notes, has been created to \nassist projects dealing with the computational analysis of \nmusical scores.  The online scores are in a format suit-\nable for processing with the Humdrum Toolkit for Music \nResearch, but the website also provides automatic trans-\nlations into several other popular data formats for digital \nmusical scores. \n \nKeywords: Musical Data, Digital Scores, Humdrum \nToolkit, Computational Music Analysis. \n1 INTRODUCTION \nThe KernScores website (http://kern.humdrum.org) was \ndeveloped to organize musical scores in the Humdrum \n**kern data format originating from a variety of musical \nsources.  The website was designed to allow easy access \nto the data for students doing projects and coursework in \nCCARH courses about musical information and compu-\ntational music analysis at Stanford University (http:// \nccarh.org/courses).  Students work on projects during \nthe second quarter in these classes, primarily facilitated \nby the Humdrum Toolkit for Music Research [1]. \n   The Humdrum file format is text-based, allowing for \neasy viewing and editing in any standard text editor.  \nEach staff/part in a work with more than one instrument \nforms a column similar to cells in a spreadsheet with \nmusical time progressing by successive rows, and with \nall elements on the same row occurring at the same per-\nformance time.  In addition to musical data, other forms \nof data can be included in their own columns (called \nspines in Humdrum terminology) that facilitate analytic \nmark-up of the musical data.  Here is an example meas-\nure of music in the Humdrum file format: \n \n   **kern  **kern  **kern  **kern  **harm \n  4BB     4G      8d      4g      IVb \n  .       .       8c#     .       . \n  4AA     4A      4d      4f#     Ic \n  4GG     4B      4d      4e      ii7b \n  4AA     8E      4c#     4a      V \n  .       8A      .       .       . \n    *-      *-      *-      *-      *- \n \n \n \n \n \n \n \n \n \n \n \n  \nIn this particular example, there is an extra column in \nthe data for harmonic analysis labels. This example \nmeasure of Humdrum data represents the following \ngraphical musical information: \n \n \n \n2 DATA ORGANIZATION \nMusical scores on the website are accessible either by \nsearching for bibliographic information such as com-\nposer and title on the main page, or by browsing through \nthe database holdings in a hierarchical tree structure \nbased on individual collections’ organization.  In addi-\ntion, a list of most composers and genres available at the \nwebsite are found on the main webpage.  No mono-\nphonic or polyphonic searching of the actual musical \ndata has yet been implemented. \n   Online access to music is controlled by a CGI inter-\nface to the data.  Music currently under copyright re-\nstrictions is available only to users with login privileges.  \nAccess to all public-domain music is available to any \nwebsite user without the need to log in. \n   Each data file represents one movement in multi-\nmovement pieces, or an entire composition for single-\nmovement works.  Every data file has an information \npage that contains three sections: (1) bibliographic in-\nformation stored as reference records in the data file, (2) \nautomatic data translation links, and (3) basic online \nanalysis links.  These data pages are accessible directly \nfrom search results pages, or from the browsing pages \nthat list either single works, or larger collections of \nworks. \n      Browsing and search-results pages usually display \nthree icons to the left of each work/movement title for \nquick data access.  Clicking on the “H” icon yields the \nactual Humdrum data file; clicking on the “M” icon \nyields an automatic conversion of the Humdrum data \ninto a MIDI file, and clicking on the “S” icon will \ndownload a PDF file with a graphical score for the work \n(when available).  Clicking on the work titles takes a \n664   \n \n user to the work information page. For power users, all \nbrowsing pages have a “Z” icon near the top of the page \nthat allows download of the ZIP-compressed data con-\ntents of each page, and certain pages also allow \ndownloading of a zipped file containing the recursive \ncontents of data from all sub-pages. \n3 DATA ENTRY \nDuring the past year, input of music into the Humdrum \ndata format has been greatly facilitated by the SharpEye \nprogram for optical music recognition [2].  SharpEye \n(only available for the Microsoft Windows operating-\nsystem) allows for output into the MusicXML data for-\nmat, which is then translated into the Humdrum file for-\nmat with a program, named xml2hum, written by the au-\nthor1. \nDuring the past year, over 800 works/movements \nconsisting of approximately one million notes have been \nprocessed by the author from scanned images of musical \nscores using SharpEye.  Most music input using the \nSharpEye program has been for piano solo or string \nquartet. Using SharpEye as data input for orchestral \nscores is currently being evaluated at Stanford Univer-\nsity and the Ohio State University. \nMusic entered into the Finale music typesetting pro-\ngram can be converted into MusicXML files using the \nDolet plug-in2 (for both Windows and Macintosh OS X) \nwhich can also be converted into Humdrum with \nxml2hum. \n4 DATA TRANSLATIONS \nThe native storage format for musical scores in Kern-\nScores is the Humdrum file format; however, six other \ndigital musical formats can be converted on demand \nfrom the source format in real-time: \n1. Standard MIDI Files – useful for listening to \nthe musical data and for inputting into analy-\nsis programs that require only basic musical \ndescriptions of scores.  \n2. MusicXML – An XML-based musical score \ninterchange format that is used in many \npopular commercial programs such as Fi-\nnale, SharpEye, and ScoreMaker3 [3]. \n3. Guido Music Notation – A data format \nwhich can be used to generate graphical mu-\nsic at http://noteserver.org and can be proc-\nessed with other programs listed at \nhttp://www.salieri.org/guido. \n4. Note lists for the Melisma Music Analyzer \nwhich is a set of programs for harmonic \nanalysis [4]. \n5. Input data for Director Musices which is a \nprogram for computer-performance of digi-\ntal scores using performance rules [5]. \n                                                           \n1 http://extras.humdrum.org/man/xml2hum \n2 http://store.recordare.com/doletfin2.html \n3 http://www.kawai.co.jp/cmusic/products/scomwin \n 6. SKINI – A text-based format that is similar \nto MIDI and used for physical modeling \nsoftware synthesis in the Synthesis Toolkit \n[6]. \n \nIf available, a printable score for the work is also \navailable in the data translation section of each work \ndescription page.  If the music was scanned from out-of-\ncopyright musical scores, then the scanned music is of-\nten available in PDF format for reference, and is also \nuseful for identifying and correcting errors in the Hum-\ndrum data file derived from the scanned source. \n5 ONLINE ANALYSES \nA few basic online analyses are available at the bottom \nof each data file description page.  These links provide \nCGI interfaces to Humdrum Toolkit programs as well as \nother programs written by the author that are run in real-\ntime on the Humdrum data files when a user accesses an \nanalysis hyperlink. \nFor example, here is the opening of Chopin’s Prelude \nOp. 24, No. 10 in C# minor in piano-roll notation avail-\nable from the analyses section: \n \n \n \nThe analyses section for the prelude (and all other \nwork pages) also has links to programs that will count \npitch-classes in the file, generate MIDI files of the prel-\nude with mixed-up pitch content, and allows for an on-\ndemand harmonic analysis of the prelude using the Mel-\nisma Music Analyzer. \nREFERENCES \n[1] Huron, David.   The Humdrum Toolkit: Software for \nMusic Research.  http://dactyl.som.ohio-\nstate.edu/Humdrum. \n[2] Jones, Graham. SharpEye Music Reader. \nhttp://www.visiv.co.uk. \n[3] Good, Michael.  MusicXML Definition. \nhttp://www.recordare.com/xml.html. \n[4] Sleator, Daniel and Davy Temperley.  The Melisma \nMusic Analyzer. http://www.link.cs.cmu.edu/music-\nanalysis. \n[5] Friberg, Anders. Director Musices.  \nhttp://www.speech.kth.se/music/performance/downlo\nad. \n[6] Cook, Perry and Gary Scavone.  The Synthesis \nToolkit in C++ (STK). \nhttp://ccrma.stanford.edu/software/stk. \n665"
    },
    {
        "title": "On the Modeling of Time Information for Automatic Genre Recognition Systems in Audio Signals.",
        "author": [
            "Nicolas Scaringella",
            "Giorgio Zoia"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416064",
        "url": "https://doi.org/10.5281/zenodo.1416064",
        "ee": "https://zenodo.org/records/1416064/files/ScaringellaZ05.pdf",
        "abstract": "The creation of huge databases coming from both restoration of existing analogue archives and new content is demanding fast and more and more reliable tools for content analysis and description, to be used for searches, content queries and interactive access. In that context, musical genres are crucial descriptors since they have been widely used for years to organize music catalogues, libraries and shops. Despite their use musical genres remain poorly defined concepts which make of the automatic classification problem a non-trivial task. Most automatic genre classification models rely on the same pattern recognition architecture: extracting features from chunks of audio signal and classifying features independently. In this paper, we focus instead on the low-level temporal relationships between chunks when classifying audio signals in terms of genre; in other words, we investigate means to model short-term time structures from context information in music segments to consolidate classification consistency by reducing ambiguities. A detailed comparative analysis of five different time modelling schemes is provided and classification results are reported for a database of 1400 songs evenly distributed over 7 genres.",
        "zenodo_id": 1416064,
        "dblp_key": "conf/ismir/ScaringellaZ05",
        "keywords": [
            "huge databases",
            "content analysis",
            "reliable tools",
            "musical genres",
            "automatic classification",
            "pattern recognition architecture",
            "audio signals",
            "genre classification",
            "time modelling schemes",
            "classification results"
        ],
        "content": "ON THE MODELING OF TIME INFORMATION FOR AUTOMATIC \nGENRE RECOGNITION SYSTEMS IN AUDIO SIGNALS\nNicolas Scaringella Giorgio Zoia \nSignal Processing Institute (ITS-LTS3) \nÉcole Polytechnique Fé dérale de Lausanne \nEPFL, Lausanne, CH-1015 Switzerland \nnicolas.scaringella@epfl.ch  Signal Processing Institute (ITS-LTS3) \nÉcole Polytechnique Fé dérale de Lausanne \nEPFL, Lausanne, CH-1015 Switzerland \ngiorgio.zoia@epfl.ch  \nABSTRACT \nThe creation of huge databases coming from both resto-\nration of existing analogue archives and new content is demanding fast and more and more reliable tools for \ncontent analysis and description, to be used for searches, \ncontent queries and interactive access. In that context, musical genres are crucial descriptors since they have been widely used for years to organize music cata-logues, libraries and shops. Despite their use musical \ngenres remain poorly defined concepts which make of the automatic classification problem a non-trivial task. Most automatic genre classification models rely on the \nsame pattern recognition architecture: extracting fea-\ntures from chunks of audio signal and classifying fea-tures independently. In this paper, we focus instead on \nthe low-level temporal relationships between chunks \nwhen classifying audio signals in terms of genre; in other words, we investigate means to model short-term time structures from context information in music seg-ments to consolidate classification consistency by reduc-ing ambiguities. A detailed comparative analysis of five different time modelling schemes is provided and classi-fication results are reported for a database of 1400 songs \nevenly distributed over 7 genres.  \n \nKeywords: musical genres, content analysis and index-\ning, machine learning, features extraction.  \n1 INTRODUCTION \nMusical genres are the main top-level descriptors used \nby music dealers and librarians to organize their music \ncollections. Though they may represent a simplification \nof one artist’s musical discourse, they are of a great in-terest as summaries of some shared characteristics in \nmusic pieces. \nWith Electronic Music Distribution (EMD), music \ncatalogues tend to become huge; in that context, associ-ating a genre to a musical piece is crucial to help users in \nfinding what they are looking for. In fact, the amount of digital music data urges for new means of automatic \nannotation since manual labeling would be too time-consuming. \nAt the same time, even if terms such as jazz, rock or \npop are widely used, they remain poorly defined con-\ncepts so that the problem of automatic genre classifica-\ntion becomes a non-trivial task. \nIn this paper, we assume that genre taxonomy is \ngiven and we focus only on the ways to uniquely and automatically associate a song to a genre. More specifi-cally, to improve results so far reported in literature spe-cial attention will be paid on different approaches to model the inner temporal structure of music; these ap-proaches will be described and compared, and their im-pact on classification results will be evaluated. In this sense, the paper analyzes five different machine-\nlearning algorithms that “encode” more or less explicitly \nrelationships between successive audio chunks. These relationships represent an attempt to identify some tem-poral structural patterns inside music excerpts; even if these patterns are hidden and not appearing at the sur-face as a set of clear rules, nevertheless they emerge from music during machine training processes and allow obtaining robust results on rather general and non con-strained data base. \nThe paper is organized as follows: section 2 will \nbriefly review the state of the art in genre classification. \nSection 3 will describe the extraction of features charac-\nterizing the audio signal. Section 4 will present the clas-sification schemes evaluated in this work while section \n5 will be devoted to the discussion of results obtained on a database of 1400 songs evenly distributed over 7 gen-res. The last section will reach some conclusions. \n2 RELATED WORK \nThough unsupervised clustering of music collections \nbased on similarity measures is gaining more and more interest in the music information retrieval community \n(see [1] and [2]), most works related to classification of \nmusic titles into genres are based on supervised tech-\nniques. These methods suppose that a taxonomy of gen-\nres is given and they try to map a database of songs into \nit by machine learning algorithms. \nSoltau et al. [3] have compared a Hidden Markov \nModel to one new classification architecture, the ETM-\nNN (Explicit Time Modelling with Neural Networks) in a classification experiment involving 360 songs distrib-uted over 4 genres. \nTzanetakis and Cook [4] and Li et al. [5] have \nworked on a database of 1000 songs over 10 genres and Permission to make digital or hard copies of all or part of this  \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-mercial advantage and that copies bear this notice and the full\n \ncitation on the first page. \n© 2005 Queen Mary, University of London \n666   \n \n have compared the use of different audio features (tim-\nbral features, rhythmic features, pitch features, wavelet features) and different classifier (Support Vector Ma-chines, Gaussian Mixtures, Linear Discriminant Analy-sis, K-nearest neighbours) on time-independent chunks. \nBurred and Lerch [6] have proposed a hierarchical \nclassification scheme evaluated on a database of 850 \nsongs over 17 classes (including some speech classes and background noise),  \nWest and Cox [7] have used a Maximal Classifica-\ntion Binary Tree along with Linear Discriminant Analy-sis to classify a set of 900 songs into 6 genres. \nDixon et al. [8] extracted the main rhythmical pattern \nof a song and classify it according to features derived from the pattern, the tempo plus other timbral features. They evaluate their system on a database of 698 files in 8 subgenres of Standard and Latin ballroom dance mu-sic. \n3 FEATURE EXTRACTION \nThe first step of analysis systems consists of extracting \nsome features  from the audio data to manipulate more \nmeaningful information and to reduce the further proc-\nessing of the classification task. \nIn this paper we focus on classification strategies tak-\ning into account as far as possible the time structure of a \nmusical piece. Consequently, we select a fixed set of \nlow-level features to characterize analyzed audio signals and to allow for direct comparison between classification \nschemes. However, it is clear that some particular fea-\ntures are more discriminative when trying to isolate one particular genre from another. As a matter of fact, fea-\nture selection techniques should be systematically used \nwhen trying to build a robust classifier. \n3.1 Segmentation into analysis frames \nThe audio content used in our experiments is sampled at \n44100 Hz and converted to a mono signal. The resulting \nsignal is then analyzed through sliding windows of 20 ms overlapped by 50%. Each analysis frame is multi-plied by a hamming-window and zero-padded to a power \nof two. \n3.2 Timbre features \nMel-frequency Cepstral Coefficients (MFCC) are then \ncomputed from the analysis frames. Each analysis frame is parameterized with 6 MFCCs. MFCCs have proved to be successful in speech recognition applications and have since then been widely used in music genre classi-fication ([3],[4],[6],[7]). They are a good choice in our case, as we did not try yet to select the features charac-terizing optimally a given genre. \n3.3 Rhythmic features \nIn [9], Klapuri et al . introduce a beat, measure bars \nand tempo tracker for audio signals. It induces tempo \nfrom a so-called periodicity function, which summarizes the strength of different periodicities in the region of \npulse sensation . Simple statistical descriptors including \nmean, standard deviation, skewness, kurtosis and maxi-\nmum of the periodicity function are computed from this periodicity function to describe its shape, which charac-terizes the strength of the different periodicities and their relations. \n3.4 Texture window \nBoth our timbre and rhythmic features may be computed \nat the analysis frame rate (i.e. every 10 ms). Yet the information contained at this time scale is not sufficient \nand too many variations occur. A solution is to average \nfeatures at the frame rate over texture windows  so that \ngreater portions of the signal are considered. \nIn the following experiments, we compare three types \nof window – one vector per 30 seconds, one vector per 1 \nsecond and a more musical  modelling: one vector cen-\ntred on each beat averaging frames over the local beat \nperiod (with beats and beat rate extracted through the system proposed in [9]). \nMean, standard deviation, skewness and mean of the \nabsolute value of each timbre feature are computed over \nthe size of the considered window. Periodicity function \nis averaged over the considered window and its mean, \nstandard deviation, skewness and maximum are evalu-ated. A vector of 28 features thus characterizes each texture window. \n4 CLASSIFICATION SCHEMES \nOnce audio signals are parameterized in terms of feature \nvectors, the genre classification problem is reduced to a typical pattern classification task. Our goal being to as-\nsociate an audio signal to a class of an a-priori defined \nset, we use a supervised approach. \nIn most reviewed papers in literature, genre classifi-\ncation is obtained by statically analysing an excerpt fea-ture vector or subdividing the excerpt into chunks, \nwhich are considered as time-uncorrelated. However, in similar problems like e.g. speech recognition and natural language analysis, temporal relationships from one pho-neme or word to the following provide important hooks to improve recognition and obtain more robust results. \nMoving from a similar approach, special attention is \npaid in this paper to ways to represent temporal progres-sion and contextual information in the classification \nprocess. Five classification schemes are evaluated with \nthat purpose in mind: a Support Vector Machine (SVM), Support Vector Machines with delayed inputs, a recur-rent neural network (the Elman network), an Explicit Time Modelling Neural Network (ETM-NN), and a Hidden Markov Model (HMM). \n4.1 Support Vector Machines \nThe underlying idea of SVM classification [10] is to \nproject data in a high dimensional space in which it is easier to separate into classes. A simple linear discrimi-\n667   \n \n nant function which maximizes the margin between \nclasses is then found in that high-dimension space. \nThough SVMs have proved to be very efficient for \nclassification tasks, they are not able to handle temporal sequences. As a matter of fact, they can only classify statically one vector into a given class. \nA first solution to consider temporal evolution of mu-\nsical signals is to consider texture windows shifted \nalong time. The features associated to each texture win-dow are classified independently and a majority vote is used to decide the class of the complete excerpt (as done in [4], [5], [6] and others in literature). \nThree SVM classifiers with radial basis kernels were \nbuilt. Multi-class classification is achieved by using \nerror-correcting codes [11]. One classifier is trained \nwith 30 seconds windows (it is referred to as SVM-30s ). \nAnother one is trained with 1 second windows ( SVM-\n1s) and the last one with windows centred on beats with \nlength the beat rate ( SVM-beats ). The feature vector \nassociated with a window is classified independently and the final decision for a song is taken as the most represented class in the song. SVM-30s  actually stands \nfor the case where a single feature vector is used for the complete song as we have been working with excerpts of 30 seconds. \n4.2 Support Vector Machines with delayed inputs \nAs it was already said, SVMs (as well as neural net-\nworks and a number of other machine learning algo-rithms) can only process static patterns. \nA solution to handle temporal sequences is to build a \nspatial representation out of it and to use it as input of the classifier. By using a tapped delay line, one can pre-sent to the classifier a sequence of feature vectors. \nNotice however that this scheme suffers from a num-\nber of weaknesses: \n1. the delay must be large enough to contain a signifi-cant sequence: this implies that the number of pa-rameters of the classifier will be larger and thus a very large number of examples is needed for the training. \n2. the classifier is not invariant to time-shifting i.e. a very large number of examples is needed for every output class and every position in the delay line. \n3. the classifier is sensitive to time-variation i.e. it re-quires the delays to precisely match the input time intervals (this may be corrected by having feature \nvectors synchronized to the beats of the musical \nsignal). \nExperiments were conducted using a delay line of 3 \nfeature vectors so that each pattern presented to the SVM is the concatenation of 3 feature vectors corre-sponding to 3 adjacent texture windows. We denote by SVM-delay-1s  the SVM trained with feature vectors \ncorresponding to 1 second texture windows and by \nSVM-delay-beats  the SVM with texture windows cor-\nresponding to beats with length equal to the beat rate.  4.3 Recurrent neural networks \nThe most widely used artificial neural network is the \nmulti-layer perceptron. Neural networks suffer from the same weakness as SVMs, i.e. they can only process static patterns (SVMs have proved however to be more suited to classification tasks than neural networks). \nNeural networks may be used in the same manner as \nthe previously presented SVMs (with texture windows \nand with delayed inputs). An alternative is to use a recur-rent or partially recurrent network (i.e. all or some of the layers of the network have their output connected to their \ninput). More specifically, we evaluate the performance \nof the Elman network [12], which is typically a two-layer network with feedback from the first layer output to the first layer input. The feedback connections allow taking the close past into account when classifying a new feature vector. \nWe use fully connected networks with 100 neurons. \nTwo cases are considered: ELM-1s  refers to the Elman \nnetwork fed with vectors corresponding to 1 second windows while ELM-beats  refers to the Elman network \nwith vectors corresponding to windows centred on beats with length equal to the beat rate. The final decision for \nthe complete song is obtained by integrating over time \neach output and selecting the maximal integrated output as the correct class. \n4.4 Explicit Time Modelling with Neural Networks \nSoltau et al [3] have introduced in the context of recogni-\ntion of music genres an original method for explicit time \nmodelling of temporal structure of music. This new ar-\nchitecture is referred to as ETM-NN (Explicit Time Modelling with Neural Networks). \nIn this architecture, a multi-layer perceptron is trained \nto recognize given input feature vectors that are 0.4 sec-onds long. The main idea is to use the hidden layer of the perceptron and not its output, which is supposed to give the genre. As a matter of fact, it is known that the first \nhalf of a feed-forward network performs a specific non-\nlinear transformation of the input data into a sp ace in \nwhich the discrimination should be simpler. \nThe activation of these hidden neurons corresponds to \nthe use of a compact representation of the input feature vector. Each hidden neuron can be understood as an ab-stract musical event – not necessarily related to an actual musical meaning. An abstract event e\ni occurs if the hid-\nden unit i has the highest activation of all hidden units. \nThe sequence of abstract events over time is then ana-\nlysed to build one single feature. More specifically, the \nnumber of events ei (unigram), the number of pairs ei ej \n(bigram) and the number of triplets ei ej ek (trigram) are \nevaluated as well as the event durations, the mean, the \nmaximum and the variance of event activations. All of \nthese features, normalized over the length of the se-\nquence are combined into a single vector which is given to another neural network; this latter implements the final decision about the genre of the musical piece.  \nWe consider two cases: ETMNN-1s  with evaluation \nof abstract events for each 1 second windows and \n668   \n \n ETMNN-beats with evaluation of abstract events for \neach beat. All the considered neural networks have a \nsingle hidden layer with a number of neurons equal to the number of genres considered. \n4.5 Hidden Markov models \nHidden Markov models have been extensively used in \nspeech recognition [13] because of their capacity to handle time series data. HMMs may be understood as a doubly embedded stochastic process: one process is not observable (hidden) and can only be observed through another stochastic process (observable) that produces \nthe time set of observations. \nA HMM is defined by its number of states, the transi-\ntion probability between its states, the initial state distri-\nbution and the observation symbol probability distribu-tion. \nOne HMM was trained per musical genre using mix-\ntures of 3 Gaussians to model the state probability den-sities. Each HMM has 4 states and an ergodic transition model. Other topologies have been briefly explored but some additional work would be needed to find the best topology depending on the genre. \nHere again were consider two cases: HMM-1s  for \nthe sequence of 1 second windows and HMM-beats  for \nthe sequence of windows centred on beats. \n5 EXPERIMENTAL RESULTS \nExperimentations were run to compare the performances \nof the five main classification schemes and the type of bag of frames. \n5.1 Dataset \nThe dataset used contain 1400 songs over 7 genres \n(i.e. 200 songs per genre). For each s ong, 30 seconds \nafter the initial 30 seconds were used. The used genres \nare: Blues, Classical, Electronica, Jazz, R&B/Soul, Rap and Rock. Songs were assigned a genre label according \nto the AllMusic\n1 guide. \n5.2 Experiment setup \nThe reported results are obtained by 10-cross validation. \nNamely, for each classification scheme, experiments are run 50 times: each time is characterized by a different random split of the database (90 % of training data and 10 % of testing data). Reported results give the average \nof the 50 runs. \n5.3 Results \nTable 1 shows the accuracy of the different classifica-\ntion schemes. Recognition rates are given in percentage \nfor the 7 genres and for the complete set (the best result \namong all classifiers for specific genres are in bold). \nAs a comparison, it should be remembered that the \naccuracy of ra ndom guess on this dataset is 14%. \n                                                            \n \n1 http://www.allmusic.com Moreover the human performance for genre classifica-\ntion has been studied in [14]: college students were able to classify songs correctly in 70% of the cases after lis-tening to 3 seconds of the songs (on a database where chance would give 10%). This result should not be un-derstood as an upper boundary to automatic classifica-\ntion accuracy but rather as an expected accuracy. \n5.4 Analysis of the different classifiers \nThe SVM with delayed inputs and texture windows syn-\nchronised on beats gives the best overall results (69.98% of correct classification). Yet some methods show sig-\nnificantly better results on some specific genres. As a \nmatter of fact, likewise some features are more suitable than others when classifying into a given set of subgen-res, some classification schemes may be more suited to a particular genre. Let us discuss more in depth the pros and cons of each classification scheme. \n5.4.1 Results of SVMs Support vector machines are known to give excellent \nresults in many classification tasks including musical genre classification (SVMs are successfully used in [5]).  \nIt is interesting to notice how in our experiments the \nsize of the texture window influences the results. In the case of Electronica, Jazz and R&B/Soul, better results are obtained when considering a window of all frames rather than smaller chunks of data. It may be understood in the case of Electronica as smaller chunks may be lo-cally misclassified because of the use of samples from other genres (mostly jazz, soul and funk). \nThe use of beat-synchronised texture windows im-\nproves results only for Rock when used with SVMs. This may be explained of course by the fact that beats \nare not extracted perfectly. Another reason is that the \nfaster the detected tempo, the smaller is the texture win-dow. This may be harmful as Tzanetakis and Cook re-port in [4]: when the window is too small, the classifica-tion accuracy drops. \n5.4.2 Results of SVMs with delayed inputs SVMs with delayed inputs give the best classification \nresults. The inherent weaknesses of such architecture (see section 4.2) are probably overcome by the large amount of data used in our experiments (200 songs per \ngenre). \nThe fact that SVMs with delayed inputs beat simple \nSVMs in the genre classification task is an interesting \nresult: it confirms the importance of time structure in \nmusical genre understanding. Indeed SVMs with de-\nlayed inputs encode a simple (non-explicit) representa-tion of time structure by considering adjacent texture windows (with a delay of 3 windows, unigrams, bigrams \nand trigrams are encoded). \nHowever the training time of such a model is consid-\nerably larger than the training of a SVM with single texture windows. \n669   \n \n  \n Blues Classical Electronica Jazz R&B/Soul Rap Rock All genres \nSVM-30s 72.12 89.89 49.43 67.42 54.02 70.32 62.18 66.48 \nSVM-1s 78.07 91.76 38.90 61.51 43.31 79.03 66.95 65.65 \nSVM-beats 76.48 90.77 29.92 60.25 39.19 77.20 69.36 63.31 \nSVM-delay-1s 84.95 92.33 52.35 65.49 51.01 75.43 64.27 69.40 \nSVM-delay-beats 86.74 89.50 45.58 71.50 52.75 76.29 67.52 69.98 \nELM-1s 66.90 89.56 30.60 56.99 41.25 70.92 61.30 59.65 \nELM-beats 66.77 87.92 33.18 60.13 39.33 72.25 64.34 60.56 \nETMNN-1s 69.18 90.12 30.42 57.67 41.76 71.23 62.20 60.37 \nETMNN-beats 68.95 89.60 32.15 61.18 39.78 73.02 64.79 61.35 \nHMM-1s 66.41 91.53 27.60 58.99 36.93 84.72 72.56 62.68 \nHMM-beats 61.54 92.98 30.37 64.08 33.56 82.91 74.65 62.87 \nTable 1. Recognition rate for the different genres and classifiers \n \n5.4.3 Results of Elman networks \nElman networks give the worst overall results. Yet their \nmodelling of time structure, though it may be too simple, is comparable to the modelling of networks with delayed \ninputs. \nIndeed, each texture window is classified according \nto its feature vector plus a feedback of the previous state of the hidden units of the network. In the recurrence, the hidden units are decreased by a multiplicative constant, which determines the memory depth of the network. Thus the network models some local structure by taking \ninto account adjacent windows with a decaying integra-\ntion factor; this is similar to the case of networks with delayed inputs, considering that this time there is no integration factor and the memory depth is fixed by the size of the delay. \nIn other words, neural networks with delayed inputs \nmay be compared to Finite Impulse Response filters while recurrent networks may be compared to Infinite Impulse Response filters: the two architectures are able to model the same problems as long as their parameters are properly estimated. Recurrent networks may indeed be as efficient as networks or SVMs with delayed in-\nputs. Yet a general weakness of systems with feedback \nloops is their tendency to become instable and this also applies to recurrent networks. \nAs a matter of fact, because of possible instability \nproblems, Elman networks are particularly tricky to train properly. In our experiments, Elman networks were sometimes overspecialized for certain classes while being weak for other classes (though on average it is not noticeable in the presented results). \n5.4.4 Results of ETMNNs The results obtained with ETMNNs are a little disap-\npointing compared to those reported by Soltau et al. [3]: he reports that his architecture significantly outperforms HMMs while this does not occur in our case. In fact, our implementation of the ETMNN differs \nslightly from the one initially proposed by Soltau: we \nuse the same feature vectors as in our previous experi-ments (vectors of dimension 28) while he uses vectors composed of the concatenation of the first 5 cepstral \ncoefficients of 10 adjacent frames of 50 ms (vectors of dimension 50). In any case, our results in terms of ETMNN and HMM performances are sometimes diffi-cult to compare to those reported by Soltau as he used a database of 360 songs over 4 genres (rock, pop, techno and classical) while we used a database of 1400 songs \nover 7 genres.  \nIn any case, the ETMNN architecture is not so differ-\nent from the HMM architecture. As a matter of fact, the \nfirst network is selecting an abstract event  in the termi-\nnology of the ETMNN: such event correspond to an HMM state .(notice by the way, that neural networks can \nbe used to model the probability densities of the HMM states).  Moreover the use of events’ unigrams and bi-grams correspond to the connections between states in a standard HMM. To model trigrams, one has to consider second order relations between states of an HMM which is usually not the case with HMMs (in most HMMs set-\ntings, the so-called first-order Markovian assumption  is \nsupposed, i.e. the probability of being in a state depends \nsolely on the previous state).  \n5.4.5 Results of HMMs \nHMMs are more suited to model time sequences than \nany other experimented model. Yet they are outper-formed by SVMs for the overall performance. As a mat-ter of fact, a number of hypotheses, which make it pos-sible to optimize these models, limit their generality and are at the root of some of their weaknesses. \nHMMs have indeed a low discriminative power be-\ncause they are usually trained with a maximum likeli-hood criterion rather than with an optimal maximum a-\nposteriori  criterion. In other words, during the training, \nthe likelihood that the model of a genre did produce an \n670   \n \n observation is maximised but the likelihood of the other \nmodels is not minimized. A number of alternative solu-tions for the training of HMMs have been proposed to ensure them a better discrimination power [15]. \nMoreover, in our experiments we have modelled the \nemission probability of the different states by mixtures of three Gaussians. This implies a strong assumption on \nthe distribution of the emission probability. This as-sumption may be relaxed by modelling emission prob-\nabilities with neural networks (see [16]). \n6 CONCLUSION \nWe have compared 5 different methods taking low-\nlevel, short-term time relationships into account to clas-sify audio excerpts into musical genres. SVMs with de-\nlayed inputs proved to give the best results with a simple \nmodelling of time structures. However, we do not claim overall that SVMs perform better than other classifiers such as multi-layer perceptrons or linear discriminant analysis: instead, the main outcome is that a simple \nmodel (using context and synchronisation on musical \nrhythm) somehow improves musical genre classification results in many cases. \nReported results may now be greatly improved by \nconsidering hierarchical classification techniques to model the underlying genre taxonomy. Feature selection \nat each node of the hierarchy would allow optimization \nof the classification task to a specific set of genres. Since some classification algorithms seem more suitable to some particular genres, one could also consider using different classifiers for each node of the hierarchy or \nusing multiple classifiers and combining their results \nlike in architectures based on a mixture of experts. \nREFERENCES \n[1] A .  R a u b e r ,  E .  P a m p a l k ,  D .  M e r k l ,  “ U s i n g  \npsycho-acoustic models and self-organizing maps to create a hierarchical structuring of \nmusic by sound similarity”, in Proc. 3\nrd Int. \nConf. on Music Information Retrieval (ISMIR), \nParis, France, 2002. \n[2] X. Shao, C. Xu. M. Kankanhalli, “Unsupervised \nclassification of musical genre using hidden Markov model”, in I EEE Int. Conf. of \nMultimedia Explore (ICME), Taibei, Taiwan, \nChina, 2004. \n[3] H. Soltau, T. Schultz, M. Westphal, A. Waibel, “Recognition of music types”, in Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), Seattle , USA, 1998. \n[4] G. Tzanetakis, P. Cook, “Musical genre classification of audio signals”, in IEEE Transactions on Speech and Audio Processing, \nVol. 10, No. 5, July 2002. \n[5] T. Li, M. Ogihara, Q. Li, “A comparative study \non content-based music genre classification”, in Proc. Of the 26\nth Annual Int. ACM SIGIR Conf. on Research and Development in Information \nRetrieval, Toronto, Canada, 2003. \n[6] J.J. Burred, A. Lerch, “A hierarchical approach to automatic musical genre classification”, in Proc. Of the 6\nth Int. Conf. on Digital Audio \nEffects (DAFx), London, UK, 2003. \n[7] K. West, S. Cox, “Features and classifiers for \nthe automatic classification of musical audio \nsignals”, in Proc. of the 5th Int. Conf. on Music \nInformation Retrieval (ISMIR), Barcelona, \nSpain, 2004.  \n[8] S. Dixon, F. Gouyon, G. Widmer, “Towards \ncharacterization of music via rhythmic patterns”, in Proc. Of the 5\nth Int. Conf. on \nMusic Information Retrieval (ISMIR), \nBarcelona, Spain, 2004. \n[9] A. Klapuri, A. Eronen, J. Astola, “Analysis of \nthe meter of acoustic musical signals”, in IEEE Trans. Speech and Audio Proc., 2004. \n[10] C. Burges, “A tutorial on support vector \nmachines for pattern recognition”, in Data \nMining and Knowledge Discovery, 2, 121-167, \n1998. \n[11] T.K. Huang, R.C. Weng, C.J. Lin, “A generalized Bradley-Terry model: from group competition to individual skill”, in 18\nth Int. \nConf. on Neural Information Processing \nSystems, 2004. \n[12] J.L. Elman, “Finding structure in time”, in \nCognitive Science, vol. 14, pp. 179-211, 1990. \n[13] L. Rabiner, “A tutorial on Hidden Markov \nModels and selected applications in speech \nrecognition”, in Proc. of the IEEE, 77:2 (257-\n286), 1989.   \n[14] D. Perrot, R. Gjerdigen, “Scanning the dial: an \nexploration of factors in identification of \nmusical style”, in Proc. Society for Music \nPerception and Cognition, Evanston, IL, USA, 1999. \n[15] S .  K a t a g i r i ,  C . H .  L e e ,  B . H .  J u a n g ,  “ N e w  \ndiscriminative training algorithms based on the generalized probabilistic descent method”, in IEEE Proc. Works hop on Neural Networks for \nSignal Processing, pp. 299-308, 1991. \n[16] N. Morgan, H. Bourlard, “Continuous speech recognition: an introduction to the hybrid \nHMM/connectionist approach”, in IEEE Signal \nProcessing Magazine, vol. 12, n°3, pp. 25-42, 1995.  \n671"
    },
    {
        "title": "Discovering and Visualizing Prototypical Artists by Web-Based Co-Occurrence Analysis.",
        "author": [
            "Markus Schedl",
            "Peter Knees",
            "Gerhard Widmer"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418315",
        "url": "https://doi.org/10.5281/zenodo.1418315",
        "ee": "https://zenodo.org/records/1418315/files/SchedlKW05.pdf",
        "abstract": "Detecting artists that can be considered as prototypes for particular genres or styles of music is an interesting task. In this paper, we present an approach that ranks artists according to their prototypicality. To calculate such a ranking, we use asymmetric similarity matrices obtained via co-occurrence analysis of artist names on web pages. We demonstrate our approach on a data set containing 224 artists from 14 genres and evaluate the results using the rank correlation between the prototypicality ranking and a ranking obtained by page counts of search queries to Google that contain artist and genre. High positive rank correlations are achieved for nearly all genres of the data set. Furthermore, we elaborate a visualization method that illustrates similarities between artists using the prototypes of all genres as reference points. On the whole, we show how to create a prototypicality ranking and use it, together with a similarity matrix, to visualize a music repository. Keywords: prototypical artist detection, visualization, asymmetric artist similarity, web mining, co-occurrence analysis 1",
        "zenodo_id": 1418315,
        "dblp_key": "conf/ismir/SchedlKW05",
        "keywords": [
            "prototypical artist detection",
            "visualization",
            "asymmetric artist similarity",
            "web mining",
            "co-occurrence analysis",
            "music repository",
            "genre",
            "artist names",
            "Google search queries",
            "rank correlation"
        ],
        "content": "DISCO VERING AND VISU ALIZING PROTOTYPICAL ARTISTS BY\nWEB-B ASED CO-OCCURRENCE ANALYSIS\nMarkus Schedl1;2\nmarkus.schedl@jku.atPeter Knees1\npeter.knees@jku.at\n1Department ofComputational Perception\nJohannes Kepler University (JKU)\nA-4040 Linz, Austria\n2Austrian Research Institute forArti\u0002cial Intelligence (ÖFAI)\nA-1010 Vienna, AustriaGerhard Widmer1;2\ngerhard.widmer@jku.at\nABSTRA CT\nDetecting artists thatcanbeconsidered asprototypes for\nparticular genres orstyles ofmusic isaninteresting task.\nInthispaper ,wepresent anapproach thatranks artists ac-\ncording totheir prototypicality .Tocalculate such arank-\ning, weuseasymmetric similarity matrices obtained via\nco-occurrence analysis ofartist names onweb pages. We\ndemonstrate ourapproach onadata setcontaining 224\nartists from 14genres andevaluate theresults using the\nrank correlation between theprototypicality ranking and\naranking obtained bypage counts ofsearch queries to\nGoogle thatcontain artist andgenre. High positi verank\ncorrelations areachie vedfornearly allgenres ofthedata\nset.Furthermore, weelaborate avisualization method that\nillustrates similarities between artists using theprototypes\nofallgenres asreference points. Onthewhole, weshow\nhowtocreate aprototypicality ranking anduseit,together\nwith asimilarity matrix, tovisualize amusic repository .\nKeywords: prototypical artist detection, visualization,\nasymmetric artist similarity ,web mining, co-occurrence\nanalysis\n1INTR ODUCTION\nFinding artists thatde\u0002ne amusic genre orstyle, oratleast\nareverytypical forit,isachallenging andinteresting task.\nInformation onprototypical artists may beused invarious\nareas ofapplication. Forexample, music information sys-\ntems liketheAllMusic Guide1ortheDesdichado Mu-\nsicInformation System2aswell asonline music stores,\ne.g.Amazon3,could bene\u0002t considerably .Forinstance,\ninformation onprototypes could beexploited tosupport\ntheir users in\u0002nding music more ef\u0002ciently .\n1http://www .allmusic.com\n2http://www .music-i-s.com\n3http://www .amazon.com\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonFurthermore, prototypical artists areveryuseful forvi-\nsualizing music repositories since theyareusually well-\nknown.Thus, alsounexperienced music listeners areable\ntoassign them toaparticular genre orstyle ofmusic and\ncanusethem asreference points todisco versimilar but\nlessknownartists. One possible wayofvisualizing proto-\ntypical artists andtherelations totheir most similar neigh-\nbors willbeshowninthispaper .\nThe approach presented here canbeused tode\u0002ne\ncomplete rankings based ontheprototypicality ofartists.\nSuch rankings enable further applications. Forexample,\ntogether with genre information, theycanserveasamea-\nsure ofthedegree ofartist membership inaparticular\ngenre, thusde\u0002ning towhich extent anartist produces mu-\nsicofacertain style orgenre.\nPrototypicality isstrongly related tothetopic ofsim-\nilarity measurement. Infact,weexploit information on\nco-occurrences ofartist names onweb pages toestimate\nconditional probabilities foranartist tobefound onweb\npages ofother artists. These probabilities giveanasym-\nmetric similarity matrix which isused forthecalculation\nofaprototypicality ranking.\nUsing theWorld WideWebforinformation retrie val\nand data mining offers theadvantage ofincorporating\ntheknowledge and opinions ofalargenumber ofdif-\nferent people. Thus, theInternet re\u0003ects akind ofcul-\ntural knowledge that weextract anduseforestimating\nartist similarity and, subsequently ,forprototype detec-\ntion. However,web-based information retrie valanddata\nmining techniques also facesome problems: First, they\nobviously depend ontheexistence ofweb pages dealing\nwith therequested topic. Ifsuch web pages cannot be\nfound, e.g. because thequery forthesearch engine can-\nnotbede\u0002ned adequately orcomprises ambiguous words,\nweb-based data mining does notyield valuable results.\nForexample, asearch formusic-related web pages that\nofferinformation about artists likeBush, Kiss, or\nPorn willmost probably result inalargenumber ofweb\npages notdealing with these artists.4Nevertheless, weal-\nready showed thatweb-based co-occurrence analysis can\nbeused successfully forartist similarity measurement and\nartist-to-genre classi\u0002cation (Schedl etal.,2005). Inthis\npaper ,wemakeuseoftheasymmetric similarity measure\ngivenbytheprobability estimation andshowhowtouse\n4Toovercome thisproblem, werestrict thesearch byadding\nmusic-related keywords tothequeries.\n21itforde\u0002ning anartist prototypicality ranking.\nThe remainder ofthispaper isorganized asfollo ws.\nRelated workisbrie\u0003y summarized inSection 2.InSec-\ntion3,wepresent ourapproach toprototype detection and\ntheperformed evaluation, andwediscuss theresults. Sec-\ntion4describes ourContinuous Similarity Ring (CSR)\nvisualization that isused toillustrate relations between\nprototypical artists andtheir most similar neighbors. Fi-\nnally ,inSection 5,wesummarize thework,drawconclu-\nsions, andpoint outpossible future research directions.\n2RELA TED WORK\nWhile wecould not\u0002ndprevious workonprototype de-\ntection formusic artists, there hasbeen some workonco-\noccurrence analysis inmusic information retrie val.One of\nthe\u0002rstpublication onMIR-related co-occurrence analy-\nsisis(Pachet etal.,2001), where playlists ofradio sta-\ntions anddatabases ofcompilation CDs areused tode-\ntectco-occurrences between titles andbetween artists. In\n(Ellis etal.,2002; Whitman andLawrence, 2002), \u0002rst\nattempts toexploit thecultural knowledge offered bythe\nWorld WideWebcanbefound. User collections ofthe\nmusic sharing service OpenNap areanalyzed togain a\nsimilarity measure based oncommunity metadata. The\nartist co-occurrences extracted from these collections are\nevaluated bycomparison with direct subjecti vesimilarity\njudgments obtained viaaweb-based survey.Incontrast\ntothissurveyofnon-professionals, Cano andKoppen-\nberger(2004) useexpert opinions takenfrom theAllMu-\nsicGuide tocreate asimilarity netw ork. Tothisend, the\nsimilar artists links of400artists aregathered. Further -\nmore, co-occurrences onplaylists from The Artofthe\nMix5areextracted andvisualized asanetw orkcontain-\ningmore than 48.000 artists.\nZadel and Fujinaga (2004) also investigate co-\noccurrences ofartist names onweb pages. Incontrast\ntoourwork, Zadel andFujinaga (2004) focus ontheus-\nageofweb services forcreating clusters ofsimilar music\nartists. Starting with aseed artist, theAmazon web ser-\nvice Listmania! isused toobtain alistofpotentially\nrelated artists. Based onthislist,co-occurrences arede-\nrivedbyquerying Google. Thereafter ,therelatedness of\neach Listmania!-artist totheseed artist iscalculated as\ntheratio between thecombined page count andthemini-\nmum ofthesingle page counts forboth artists. Incontrast\ntoourco-occurrence approach, theoneused in(Zadel and\nFujinaga, 2004) does notyield complete similarity matri-\nces.\nInthispaper ,weusethesame technique asdescribed\nin(Schedl etal.,2005) toobtain asimilarity matrix based\nonco-occurrences. Wequery Google forcombinations of\nartist names andusetheresulting page counts toestimate\nconditional probabilities thatgiveanasymmetric similar -\nitymatrix.\nSimilarity measures based onsubjecti veorcultural\nopinions areingeneral asymmetric. Forexample, itis\nmore natural tosaythat theFinnish heavymetal band\nSentenced sounds likethewell-kno wnpioneers Metal-\nlica than vice versa. This canbeexplained bythefact\n5http://www .artofthemix.or gthatMetallica servesasaprototype forthegenre heavy\nmetal. Ellis etal.(2002) regard thisasymmetry asaprob-\nlemsince itundermines aEuclidean model ofsimilarity .\nInfact,nearly allofthecited publications dealing with\nco-occurrence-based similarity measurement consider the\nasymmetry ashortcoming andperform operations tosym-\nmetrize thesimilarity matrices. Tocontrast, inthispaper ,\nwedescribe aprototype detection approach thatcapital-\nizesonasymmetric similarity matrices.\n3PROTOTYPE DETECTION\nInthefollo wing, wesketch howweuseco-occurrence\nanalysis tode\u0002ne anasymmetric similarity measure. To\nthisend, weapply thesame technique asin(Schedl etal.,\n2005). Based onthissimilarity measure, wethen elabo-\nrateournovelmethod forcalculating theprototypicality\nranking.\n3.1 Methodology\n3.1.1 Co-Occurr ence Analysis\nGivenalistofartist names, weuseGoogle toestimate the\nnumber ofweb pages containing each artist andeach pair\nofartists. Since wearenotinterested inthecontent ofthe\nfound web pages, butonly intheir number ,thesearch is\nrestricted todisplay only thetop-rank edpage. Infact,the\nonly information weuseisthepage count thatisreturned\nbyGoogle. This raises performance andlimits webtraf\u0002c.\nAddressing theissue of\u0002nding only music-related\nweb pages, we add additional keywords tothe\nGoogle search query .More precisely ,weuse the\nscheme artist1 [artist2] +music+r eview toform\nqueries. This scheme, already used in(Whitman and\nLawrence, 2002), provedtoyield good results for\nclassi\u0002cation tasks (Knees etal.,2004; Schedl etal.,\n2005). Furthermore, itperformed slightly better than\nartist1 [artist2] +music+genr e+style in\u0002rst experi-\nments ofprototype detection.\nTheoutcome ofthequerying procedure isasymmet-\nricmatrix C,where element cijgivesthenumber ofweb\npages containing theartist with indexitogether with the\noneindexedbyj.The values ofthediagonal elements\nciishowthetotal number ofweb pages containing artist\ni.Based onthepage count matrix C,wethen userela-\ntivefrequencies tocalculate aconditional probability ma-\ntrixPasfollo ws.Giventwoeventsai(artist with index\niismentioned onweb page) andaj(artist with indexj\nismentioned onweb page), weestimate theconditional\nprobability pij(theprobability forartistjtobefound on\naweb page thatisknowntocontain artisti)asshownin\nFormula 1.\np(ai^ajjai)=cij\ncii(1)\nPgivesasimilarity matrix thatisobviously notsym-\nmetric. Itcanbesymmetrized andused, forexample, for\nclassifying newartists into agivengenre taxonomy ,e.g.\n(Schedl etal.,2005), forgenerating playlists with simi-\nlarpieces ofmusic, e.g. (Aucouturier andPachet, 2002;\nLogan, 2002), orforvisualizing music repositories, e.g.\n22(Pampalk etal.,2003; Schedl, 2003). Incontrast, wecan\nalsobene\u0002t from theasymmetry ofPanduseitforproto-\ntype detection asdescribed inthefollo wing.\n3.1.2 Prototype Detection using Backlink/F orwar dLink\nRatios\nWeregard theprototypicality ofamusic artist asbeing\nstrongly related tohowoften music-related web pages re-\nfertotheartist andbuildamodel upon thisconsideration.\nOur approach isbased onanidea similar tothe\nPageRank Citation Ranking (Pageetal.,1998) used by\nGoogle. LikePageetal.(1998), weuseinformation about\nthenumber ofbacklinks andforwar dlinks ofaweb page.\nPageetal.(1998) de\u0002ne aforw ardlinkofaweb pagewas\nalinkthatisplaced onwandlinks toanother web page.\nAbacklink ofaweb pagew,incontrast, isde\u0002ned asa\nlinkonanyweb page other thanwthatlinks tow.\nSince weinvestigate co-occurrences rather than links,\nweslightly modify theabovede\u0002nitions. Inourmodel for\nprototypicality ranking, wecalculate thenumber ofback-\nlinks ofanartist ofinterest abyfocusing aandcount-\ninghowmanyweb pages thatareknowntomention an-\nother artist also mention artista.Thus, wecallanyco-\noccurrence ofartistaandartistb(unequal toa)onaweb\npage thatisknowntocontain artistbabacklink ofa(from\nb).Aforwar dlinkofanartist ofinterest atoanother artist\nb,incontrast, isgivenbyanyoccurrence ofartistbona\nweb page which isknowntomention artista.\nUsing these de\u0002nitions, wecreate amodel forproto-\ntypicality ranking. Toobtain theranking ofanartist ofin-\nterestai,weinvestigate, foreach artist tuple(ai;aj;j6=i),\nwhether thenumber ofbacklinks ofaifromajexceeds\nthenumber offorw ardlinks ofaitoaj:Wecount, for\nhowmanyoftheartists ajfrom thesame genre asaithis\nisthecase. Thelargerthiscount, themore often artistai\nismentioned intheconte xtofanother artist from thesame\ngenre andthus, thehigher theprototypicality ofaiforthe\nrespecti vegenre.\nMore formally ,using thesimilarity matrix P,wede-\n\u0002nearanking function rforeach artistai(iistheindexof\ntheartist inP)asshowninFormula 2.Here,nisthetotal\nnumber ofartists andbwl(i;j)andfwl(i;j)arefunc-\ntions thatreturn boolean values, cf.Formulas 3and4re-\nspecti vely.These functions usetheestimated conditional\nprobabilities asalready de\u0002ned inFormula 1.\nr(ai)=Pn;j6=i\nj=1bwl(i;j)\nPn;j6=i\nj=1fwl(i;j)(2)\nbwl(i;j)=\u001a\n1ifpij<pji\n0otherwise(3)\nfwl(i;j)=\u001a\n1ifpij\u0015pji\n0otherwise(4)\nMore precisely ,bwl(i;j)givesthevalue1ifartist\naihasmore backlinks from artistaj(relati vetothetotal\nnumber ofweb pages mentioning artistai)than forw ard\nlinks toartistaj(relati vetothetotal number ofweb pages\nmentioning artistaj).Analogously ,fwl(i;j)returns the\nvalue1ifartistaihasmore forw ardlinks toartistaj(rela-\ntivetothetotal number ofweb pages mentioning artistai)than backlinks from artistaj(relati vetothetotal number\nofweb pages mentioning artistaj).\nWecallr(ai)thebacklink/forwar dlink(bl/\u0003) ratioof\nartistaisince itcounts howoften therelati vefrequenc yof\nbacklinks foraiexceeds therelati vefrequenc yofitsfor-\nwardlinks andrelates these twocounts. Ourassumption\nisthatr(ai)measures theprototypicality ofartistaisince\nitsvalue isthehigher themore web pages ofother artists\nmention artistai.\n3.2 Evaluation\nWeapplied themethods described abovetoobtain an\nintra-genre-similarity ranking foratestcollection of224\nartists in14quite general genres. Theresults canbefound\ninTable 1andwillbediscussed inthenextsection.\nEvaluating thequality oftheresults isaverydif\u0002cult\ntask since theprototypicality ofanartist foragenre can-\nnotbede\u0002ned easily andisalsoaffected bysubjecti veand\ncultural opinions. Forthelatter reason, wedecided touse\ntheWorld WideWebagain forevaluation. Wetried to\ncreate aranking ofthedegree ofartist membership and\npopularity forallartists ofagenre toestimate aprototyp-\nicality ranking. Tothisend, weused thequery scheme\nartist +genre +music+r eview andretrie vedthepage\ncount foreach artist. Wethen rankedtheartists ofagenre\naccording tothese page counts andcomputed theSpear -\nman' srank correlation, e.g.(Hogg etal.,2004), between\nthisranking andtheonegivenbythebl/\u0003 ratios. The\nSpearman' srank correlation coef\u0002cients foreach genre\ncanbefound inTable 2.\nUsing the page counts obtained bythe queries\nartist +genre +music+r eview forevaluation works\nwell ifataxonomy ofwell-de\u0002ned genres isgiven.How-\never,applying thebl/\u0003 approach toanartist setwhich is\nstructured according toanother taxonomy (e.g. mood, na-\ntionality oftheartist, orpersonal attrib utes auser may\nutilize) would probably need another evaluation method\nsince theused query scheme may notgiveauseful rank-\ning.\n3.3 Results andDiscussion\nTable 1showstheartist prototypicality ranking foreach\ngenre ofthetestcollection. Taking acloser look reveals\nthatthetop-rank edartists areusually knowntobevery\nfamous andtypical fortherespecti vegenre, whereas the\nartists atthelowerendoftheranking seem tobelesstypi-\ncal,atleast insome cases. However,wehavetoadmit that\ntheused artist collection hasoriginally been composed for\nadifferent purpose andtherefore barely contains artists\nwhich areunkno wntotheinterested music listener .Thus,\nnone oftheartists ofthetestcollection isreally untypi-\ncalfortherespecti vegenre. Further evaluations onatest\nsetcomprising more than 950artists with highly varying\npopularities arecurrently inpreparation (cf.Section 5).\nInterestingly ,artist names which arealso used in\neveryday speech arealways top-rank ed,forexample,\nKiss from thegenre Hea vyMetal/Hard Rock, Bush,\nHole, andNirv ana from Alternati veRock/Indie, or\nPrince andMadonna from Pop. Thereason forthis\nisthat such common speech words occur veryoften on\n23Country Folk Jazz Blues\nartist ranking bl/\u0003 artist ranking bl/\u0003 artist ranking bl/\u0003 artist ranking bl/\u0003\nJohnn yCash 15:0 Bob Dylan 15:0 Miles Davis 15:0 BBKing 15:0\nWillieNelson 14:1 Dono van 14:1 DukeEllington 14:1 TajMahal 14:1\nTimMcGra w 12:3 Leonard Cohen 13:2 Louis Armstrong 13:2 Muddy Waters 13:2\nDixie Chicks 12:3 Joni Mitchell 12:3 Count Basie 12:3 EttaJames 11:4\nHank Williams 10:5 CatStevens 11:4 John Coltrane 10:5 Howlin' Wolf 10:5\nDolly Parton 10:5 John Denver 10:5 EllaFitzgerald 10:5 John Mayall 10:5\nFaithHill 9:6 Joan Baez 9:6 Billie Holiday 9:6 John LeeHook er 10:5\nKennyChesne y 8:7 TracyChapman 8:7 NatKing Cole 7:8 Albert King 8:7\nKennyRogers 7:8 Pete Seeger 7:8 Herbie Hancock 6:9 T-Bone Walker 7:8\nGarth Brooks 7:8 Don McLean 6:9 Thelonious Monk 5:10 WillieDixon 6:9\nKris Kristof ferson 6:9 Townes vanZandt 5:10 Charlie Parker 5:10 Lightnin' Hopkins 5:10\nRoger Miller 4:11 Suzanne Vega 4:11 Nina Simone 5:10 Mississippi John Hurt 4:11\nJimReeves 2:13 Crosby Stills &Nash 3:12 Glenn Miller 5:10 Blind Lemon Jefferson 4:11\nBrooks andDunn 2:13 TimBuckle y 2:13 Django Reinhardt 2:13 Otis Rush 2:13\nHank Snow 2:13 Steele yeSpan 1:14 DaveBrubeck 2:13 BigBillBroonzy 1:14\nLeeHazle wood 0:15 Woodie Guthrie 0:15 Cannonball Adderle y 0:15 Blind WillieMcT ell 0:15\nRnB/Soul Heavy Metal/Hard Rock Alternativ eRock/Indie Punk\nartist ranking bl/\u0003 artist ranking bl/\u0003 artist ranking bl/\u0003 artist ranking bl/\u0003\nAlicia Keys 15:0 Kiss 15:0 Bush 15:0 Green Day 15:0\nJames Brown 14:1 Metallica 14:1 Hole 14:1 Ramones 14:1\nMarvin Gaye 13:2 Slayer 13:2 Nirvana 13:2 Blink 182 13:2\nJillScott 11:4 AC/DC 12:3 Beck 12:3 TheClash 12:3\nTheTemptations 11:4 Iron Maiden 11:4 Radiohead 11:4 Sum 41 11:4\nAretha Franklin 11:4 Anthrax 10:5 Sonic Youth 10:5 SexPistols 10:5\nAlGreen 9:6 Black Sabbath 9:6 Pearl Jam 9:6 Rancid 8:7\nTheSupremes 8:7 DefLeppard 8:7 Weezer 8:7 NoFX 8:7\nErykah Badu 7:8 Deep Purple 7:8 Smashing Pumpkins 7:8 BadReligion 7:8\nOtis Redding 6:9 Megadeth 6:9 Depeche Mode 6:9 Penn ywise 6:9\nIsaac Hayes 5:10 Pantera 5:10 FooFighters 5:10 Dead Kennedys 5:10\nSam Cook e 4:11 Alice Cooper 4:11 TheSmiths 3:12 Buzzcocks 4:11\nIndia Arie 3:12 Judas Priest 3:12 Alice inChains 3:12 PattiSmith 4:11\nFatsDomino 2:13 Sepultura 2:13 Belle andSebastian 3:12 TheMis\u0002ts 2:13\nSolomon Burk e 1:14 Skid Row 1:14 Jane' sAddiction 1:14 SidVicious 1:14\nTheDrifters 0:15 Queensryche 0:15 Echo andtheBunn ymen 0:15 Screeching Weasel 0:15\nRap/Hip-Hop Electronica Reggae Rock'n'Roll\nartist ranking bl/\u0003 artist ranking bl/\u0003 artist ranking bl/\u0003 artist ranking bl/\u0003\nEminem 15:0 Moby 15:0 Bob Marle y 15:0 TheWho 15:0\nJay-Z 14:1 Underw orld 13:2 Peter Tosh 14:1 TheAnimals 14:1\nSnoop Dogg 13:2 Prodigy 13:2 Inner Circle 13:2 Elvis Presle y 13:2\n50Cent 12:3 Chemical Brothers 12:3 Shaggy 12:3 TheFaces 12:3\nDMX 11:4 FatboySlim 11:4 Sean Paul 11:4 TheRolling Stones 11:4\n2Pac 10:5 Kraftwerk 10:5 UB40 10:5 Buddy Holly 9:6\nDr.Dre 9:6 Massi veAttack 9:6 Jimmy Cliff 9:6 Chuck Berry 9:6\nIceCube 8:7 Aphe xTwin 8:7 Ziggy Marle y 8:7 TheKinks 9:6\nPublic Enemy 7:8 PaulOakenfold 6:9 Desmond Dekk er 7:8 Jerry LeeLewis 7:8\nLLCool J 6:9 Basement Jaxx 6:9 Bounty Killer 6:9 Little Richard 6:9\nCypress Hill 5:10 Daft Punk 6:9 Black Uhuru 5:10 BoDiddle y 5:10\nBusta Rhymes 4:11 Mouse onMars 4:11 Capleton 4:11 TheYardbirds 4:11\nRun DMC 3:12 Molok o 3:12 Shabba Ranks 3:12 Carl Perkins 2:13\nMissy Elliot 2:13 Carl Cox 3:12 Maxi Priest 2:13 Chubby Check er 2:13\nMystikal 1:14 Armand vanHelden 1:14 Alpha Blondy 1:14 Gene Vincent 1:14\nGrandmaster Flash 0:15 Jimi Tenor 0:15 Eddy Grant 0:15 BillHaley 1:14\nPop Classical\nartist ranking bl/\u0003 artist ranking bl/\u0003\nPrince 15:0 Johann Sebastian Bach 14:1\nMadonna 14:1 Tchaik ovsky 14:1\nBritne ySpears 13:2 Ludwig vanBeetho ven 13:2\nMichael Jackson 12:3 Wolfgang Amadeus Mozart 12:3\nAvrilLavigne 10:5 Richard Wagner 11:4\nJanet Jackson 10:5 Johannes Brahms 10:5\nJennifer Lopez 9:6 Franz Schubert 9:6\nChristina Aguilera 8:7 Giuseppe Verdi 8:7\nRobbie Williams 7:8 Antonio Vivaldi 7:8\nABB A 6:9 Gusta vMahler 6:9\nJustin Timberlak e 4:11 Joseph Haydn 5:10\nN'Sync 4:11 Herbert vonKarajan 4:11\nShakira 3:12 Yehudi Menuhin 3:12\nSpice Girls 3:12 Antonin Dvorak 3:12\nO-Town 1:14 Frederic Chopin 1:14\nNelly Furtado 1:14 Geor gFriedrich Haendel 0:15\nTable 1:Artist ranking according toprototypicality foreach genre. Furthermore, thebacklink/forwar dlink(bl/\u0003) ratiois\nshownforeveryartist.\n24genr e rank correlation\nCountry 0.96\nFolk 0.89\nJazz 0.92\nBlues 0.96\nRnB/Soul 0.67\nHeavyMetal/Hard Rock 0.57\nAlternati veRock/Indie 0.57\nPunk 0.96\nRap/Hip-Hop 0.55\nElectronica 0.76\nReggae 0.81\nRock 'n'Roll 0.76\nPop 0.95\nClassical 0.69\nmean 0.79\nTable 2:Spearman' sRank Correlations between ranking\nofartist names according tobacklink/forwar dlinkproto-\ntypicality andartist-g enre-pagecounts foreach genre.\nartists' web pages andtherefore produce alotofbacklinks\nfortherespecti veartist with thesame name. However,\ntheyusually donotrefer totheartist, butsimply denote\nthecommon speech word. Togiveanexample, \u0002nding\nmanyco-occurrences ofBush andMichael Jackson\ndoes notnecessarily mean thatthese artists create simi-\nlarmusic. Itcould also mean thatMichael Jackson hada\nmeeting with thecurrent president oftheUSorthatMr.\nJackson likesbushes onhisNeverland-ranch.\nSuch misleading co-occurrences areashortcoming of\nweb-based information retrie valmethods andcould also\ndistort theprototypicality ranking. Forexample, theau-\nthors would notattest Bush ahigher prototypicality than\nNirv ana forthegenre Alternati veRock/Indie. How-\never,wecould turn thetables anduseourprototype de-\ntection approach to\u0002nd artist names thatequal common\nspeech words byinvestigating theterms with outstand-\ningly high bl/\u0003 ratios. Tothisend, thebl/\u0003 ratios should\nbecalculated onthecomplete artist setrather than foreach\ngenre separately since common speech words appear on\nartists' web pages independently oftheir genre. Indeed,\nperforming these computations onourtestcollection re-\nvealsthattheartists which showbyfarthehighest bl/\u0003ra-\ntiosareBush (223:0), Prince (222:1), Kiss (221:2),\nMadonna (220:3), andNirv ana (218:5).\nAsfortheresults oftheevaluation, Table 2shows\ntheSpearman' srank correlation coef\u0002cients foreach of\nthe14genres ofthetestcollection. The prototypicality\nranking givenbythebl/\u0003 ratios andtheevaluation rank-\ningobtained bytheartist-genre-page counts showstrong\norevenverystrong positi vecorrelations. Especially for\nthegenres Country, Blues, Punk, andPop, the\nprototypicality ranking nearly perfectly correlates with the\nevaluation ranking. Incontrast, theresults forthegenres\nHea vyMetal/Hard Rock, Alternati veRock/Indie, and\nRap/Hip-Hop aresituated attheother endoftheper-\nformance scale with correlation coef\u0002cients ofabout 0.55\nwhich isnevertheless anindication forstrongly correlat-\ningrankings.4VISU ALIZA TION\nInorder tovisualize theprototypical artists that were\nidenti\u0002ed (together with similar artists), wedeveloped a\nnovelmethod which wecall theContinuous Similar -\nityRing (CSR) .Asample screenshot takenfrom the\nmusic information retrie valandvisualization frame work\nCoMIR VA6which wearedeveloping atourdepartment\ncanbefound inFigure 1.\nThe basic idea istodisplay theprototypes onefor\neach genre intheform ofacircle. Since similar orre-\nlated prototypes andthegenres theyrepresent should be\nplaced close toeach other ,weformulate aTraveling Sales-\nman Problem (TSP) ,e.g. (Lawler etal.,1985; Skiena,\n1997), andapply asimple heuristic algorithm. Tothisend,\nweuseasymmetrized version Psofthesimilarity ma-\ntrixP(cf.Formula 1)which weobtain bycalculating the\narithmetical mean ofpijandpjiforeverypair ofartists\niandj.Subsequently ,weconvertPsintoadistance ma-\ntrix. TheTSP-algorithm then tries to\u0002ndtheshortest path\nbetween allprototypes. Thus, itgivesatour thatpasses\nallprototypes andminimizes theoverall distance. There-\nsulting tour de\u0002nes thearrangement oftheartists within\nthecircle ofprototypes.\nSince wealso wanttoshowwhich artists aresimilar\ntowhich prototypes, weagain usethesymmetrized sim-\nilarity matrix Psandselect, foreach prototype r,a\u0002xed\nnumber kofartists with minimal distance tor.These\nkneighbors arechosen from thecomplete artist setre-\ngardless oftheir genre assignment, which enables theuser\ntoeasily detect artists that areinspired bymusicians of\ndifferent genres. Hence, unlik etheprototype detection\napproach, theCSR-visualization technique does notrely\nongenre information, provided thatalistofprototypes is\navailable.\nGiventhesetofnearest neighbors Nrforeach proto-\ntyper,weinvestigate which artists areneighbor ofonly\noneprototype (inserted intoartist setO),andwhich neigh-\nbormore than one(inserted intoartist setI).Thegoal is\ntopoint outartists which cannot beclassi\u0002ed exactly into\nonegenre andthus neighbor severalprototypes. Forvisu-\nalizing, weusetheregion outside ofthecircle ofproto-\ntypes todisplay theartists contained inOsince theyneed\ntobeconnected only totheir single prototype. Artists\nofthesetIaremapped tothearea inside ofthecircle\nofprototypes andareconnected toallprototypes rcon-\ntaining them inNr.Forthese artists, special handling\nisnecessary since wewanttopreserv etheoriginal dis-\ntances between theartists andtheir prototypes asgiven\nbyPs.Furthermore, thelength oftheedges connecting\nprototypes andneighbors should beminimized inorder\ntoavoidoverloading ofthevisualization. Thus, weusea\nheuristic cost-minimizing algorithm toposition theartists\nofsetI.The costscnforanartistn2Iarecalculated\nasshowninFormula 5,where Pnisthesetofprototypes\nthatareconnected toartistn,origDist(r;n)istheorigi-\nnaldistance between prototype randneighbor naccord-\ningtothesimilarity matrix, origDistSumisthesum of\ntheoriginal distances between nandallelements ofPn,\nscreenDist(r;n)isthedistance onthescreen between\n6http://www .cp.jku.at/comirva\n25thevertexrepresenting prototype randthevertexrepre-\nsenting neighbor n,andscreenDistSumisthesum of\nthescreen distances between thevertexrepresenting nand\nallvertices thatrepresent anelement ofPn.\ncn=X\nr2Pn\u0012origDist(r;n)\norigDistSum\u0000screenDist(r;n)\nscreenDistSum\u0013\n(5)\nThealgorithm forpositioning thevertexofaneighbor\nn2Icomprises three steps which areperformed itera-\ntively(5000 iterations seemed tobeagood choice forthe\nused artist set).\n1.Thevertexofthecurrent neighbor n2Iisinitially\npositioned inthecenter ofthescreen.\n2.This position isthen randomly modi\u0002ed byasmall\namount (werestricted themovement toamaximum\nof10pixelsineach direction).\n3.Thecosts forthisnewposition arecalculated andthe\nvertexismovedtothenewposition ifanimpro ve-\nment incosts andinthescreenDistSum(formini-\nmizing thelength oftheedges) canbeachie ved.\nFigure 1showsascreenshot ofaCSR-visualization which\nisbased ontheprototypes oftheused artist collection.\nThethree nearest neighbors ofeach prototype aredepicted\nandedges connecting these neighbors with therespecti ve\nprototypes aredrawn. Varying thickness andcolor ofthe\nedges revealinformation about thesimilarity values ofthe\nartists theyconnect. Thick anddark edges connect very\nsimilar artists, whereas thinner andlighter edges connect\nartists with lowersimilarity values. Regarding Figure 1,it\ncanbeseen thattheonly prototype whose neighbors are\nnotconnected toanyother prototype isJohann Sebastian\nBach. Thus, wecanstate thatclassical artists arevery\nwell distinguishable from artists ofother genres. Wealso\nseethatNirv ana isoneofthethree nearest neighbors\nofGreen Day, Kiss, andJohnn yCash which does\nmakesense tosome extent since Alternati veRock/Indie\nisrelated toPunk andalsoHea vyMetal/Hard Rock is\nnotthatfaraway.Unfortunately ,artists whose name equal\ncommon speech words dominate theregion inside ofthe\ncircle ofprototypes. However,thisproblem arises only for\nsmall values ofk(number ofdisplayed neighbors foreach\nprototype). Using k=5,forexample, revealsmore inter-\nesting relations. Due tolimited space inandresolution\nofthispaper weunfortunately cannot depict such ade-\ntailed CSR-visualization. Theinterested reader isinvited\ntoexperiment with theCoMIR VA-frame workandcreate\nhis/her ownCSR.\nApossible application scenario for the CSR-\nvisualization technique could beitsusage inonline music\nstores. Prototypical artists according toasetofgenres, or\nanyother useful taxonomy (e.g. mood), canbeseen as\nreference points fortheuser since theyareusually well-\nknown.Starting atthese prototypes, theuser could utilize\ntheCSR-visualization toexplore similar butlessknown\nartists. Moreo ver,focusing anartist which hasbeen se-\nlected arbitrarily bytheuser,thein\u0003uence ofdifferent\nprototypical artists andtheir genres (orother attrib utesac-\ncording totheused taxonomy) ontheartist under con-sideration could easily bemade outwhen using aCSR-\nvisualization.\nAlso (music) search engines could apply theprototyp-\nicality ranking technique (maybe together with theCSR-\nvisualization approach) tosupport their users indisco v-\nering lessknownartists based onanentered orselected\nprototype. Ontheother hand, iftheuser entered aless\nprototypical artist, thesystem could provide alistofartists\nthatmay havein\u0003uenced theentered one.\n5CONCLUSIONS AND FUTURE WORK\nInthispaper ,weshowed howtouseco-occurrences of\nartist names onwebpages tocalculate anasymmetric sim-\nilarity matrix. Based onthissimilarity matrix, weesti-\nmate aprototypicality ranking fortheartists using back-\nlink/forwar dlink ratios .Weevaluated ourapproach on\natestcollection containing 224artists of14genres. Us-\ningthepage counts obtained bysearch queries thatcom-\nprise artist name andgenre, wecalculated therank corre-\nlation andshowed thattheprototypicality ranking given\nbythebl/\u0003ratios correlates well with theevaluation rank-\ninggivenbytheartist-genre-page counts. Furthermore,\nwepresented avisualization approach called Continu-\nousSimilarity Ring (CSR) thatmakesuseoftheextracted\nprototypes ofeach genre.\nAshortcoming oftheused testcollection isthatmost\nofitsartists arequite popular andtypical oftheir genre.\nAddressing thisissue, itisplanned toevaluate ourap-\nproach onalargerartist setcontaining 953 artists from\n15genres. Tocreate thisartist set,weused theartist\ndatabase oftheAllMusic Guide. Wechose tenvery\ngeneral and \u0002vequite speci\u0002c genres and selected the\nartists assigned thehighest andthelowest tierinthegenre-\nspeci\u0002c artist listoftheAllMusic Guide. This pro-\nvides amix ofverywell-kno wnartists andartists which\narenotthat popular .Unfortunately ,calculating theco-\noccurrences ofsuch alargeartist setwould include raising\nmore than 450.000 queries. Since theGoogle Web-API7\nallowsonly 1.000 queries perday,using itisoutofthe\nquestion. Thus, wehavetoelaborate other approaches,\nwhich will bedone inthenear future. Forexample, we\ncould usetheGoogle Web-API toretrie vetheURLs of\nthetop-rank edweb pages foreach artist. Thecontent of\ntheweb pages addressed bythese URLs may then beex-\ntracted andscanned forthenames ofallother artists inthe\nartist set. Storing therelati vefrequecies ofartist names\nappearing onother artists top-rank edweb pages would\ngiveusaco-occurrence matrix which could beused to\nestimate similarities again.\nAnother interesting issue would betheevaluation of\nthebl/\u0003 prototypicalities onaranking created bymusical\nexperts, ideally from different cultures. Since thiswould\nbehardly feasible, aweb-based surveyofmusic lovers\nfrom allovertheworldcould beconducted instead.\nACKNO WLEDGEMENTS\nThis research issupported bytheAustrian Fonds zur\nFörderung derWissenschaftlichen Forschung (FWF) un-\n7http://www .google.com/apis\n26Figure 1:ACSR-visualization based ontheprototypes ofthetestcollection andthesimilarity matrix. Everyprototype is\nconnected toitsthree nearest neighbors.\nderproject numbers L112-N04 andY99-ST ART,andby\ntheEU6thFPproject SIMA C(project number 507142).\nTheAustrian Research Institute forArti\u0002cial Intelligence\nissupported bytheAustrian Federal Ministry forEdu-\ncation, Science, andCulture andbytheAustrian Federal\nMinistry forTransport, Innovation, andTechnology .\nREFERENCES\nJ.-J. Aucouturier andF.Pachet. Scaling upmusic playlist\ngeneration. InProceedings ofIEEE International Con-\nference onMultimedia and Expo (ICME) .Lausanne,\nSwitzerland, August 2002.\nP.Cano andM.Koppenber ger.TheEmer gence ofCom-\nplexNetw orkPatterns inMusic Artist Netw orks. In\nProceedings ofthe5thInternational Symposium on\nMusic Information Retrie val(ISMIR'04) ,Barcelona,\nSpain, October 2004.\nD.P.W.Ellis, B.Withman, A.Berenzweig, and\nS.Lawrence. The Quest forGround Truth inMusical\nArtist Similarity .InProceedings ofthe3rdInterna-\ntional Symposium onMusic Information Retrie val(IS-\nMIR'02) ,Paris, France, 2002.\nR.V.Hogg, A.Craig, andJ.W.McK ean. IntroductiontoMathematical Statistics .Prentice Hall, 6thedition,\nJune 2004.\nP.Knees, E.Pampalk, andG.Widmer .Artist Classi\u0002ca-\ntionwith Web-based Data. InProceedings ofthe5thIn-\nternational Symposium onMusic Information Retrie val\n(ISMIR'04) ,pages 517524, Barcelona, Spain, October\n2004.\nE.L.Lawler,J.K.Lenstra, A.H.G.R.Kan, andD.B.\nShmo ys.TheTraveling Salesman Problem: AGuided\nTourofCombinatorial Optimization .WileySeries in\nDiscrete Mathematics and Optimization. John Wiley\nandSons, September 1985.\nB.Logan. Content-based Playlist Generation: Ex-\nploratory Experiments. InProceedings ofthe3rdIn-\nternational Symposium onMusic Information Retrie val\n(ISMIR'02) ,Paris, France, October 2002.\nF.Pachet, G.Westerman, andD.Laigre. Musical Data\nMining forElectronic Music Distrib ution. InProceed-\nings ofthe1stWedelMusic Confer ence,2001.\nL.Page, S.Brin, R.Motw ani, andT.Winograd. The\nPageRank Citation Ranking: Bringing Order tothe\nWeb. InProceedings oftheAnnual Meeting ofthe\nAmerican Society forInformation Science (ASIS'98) ,\npages 161172, January 1998.\n27E.Pampalk, S.Dixon, andG.Widmer .Exploring Mu-\nsicCollections byBrowsing Different Views. InPro-\nceedings ofthe4thInternational Confer ence onMusic\nInformation Retrie val(ISMIR'03) ,Washington, D.C.,\nUSA, October 2003.\nM.Schedl. AnExplorati ve,Hierarchical User Interf aceto\nStructured Music Repositories. Master' sthesis, Vienna\nUniversity ofTechnology ,Austria, December 2003.\nM.Schedl, P.Knees, and G.Widmer .AWeb-Based\nApproach toAssessing Artist Similarity using Co-\nOccurrences. InProceedings oftheFourth Interna-\ntional Workshop onContent-Based Multimedia Index-\ning(CBMI'05) ,Riga, Latvia, June 2005.\nS.S.Skiena. TheAlgorithm Design Manual .Springer ,\nNovember 1997.\nB.Whitman andS.Lawrence. Inferring Descriptions and\nSimilarity forMusic from Community Metadata. In\nProceedings ofthe2002 International Computer Mu-\nsicConfer ence,pages 591598, Goetebor g,Sweden,\nSeptember 2002.\nM.Zadel andI.Fujinaga. WebServices forMusic Infor -\nmation Retrie val.InProceedings ofthe5thInterna-\ntional Symposium onMusic Information Retrie val(IS-\nMIR'04) ,Barcelona, Spain, October 2004.\n28"
    },
    {
        "title": "Beatbox Classification Using ACE.",
        "author": [
            "Elliot Sinyor",
            "Cory McKay",
            "Rebecca Fiebrink",
            "Daniel McEnnis",
            "Ichiro Fujinaga"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414920",
        "url": "https://doi.org/10.5281/zenodo.1414920",
        "ee": "https://zenodo.org/records/1414920/files/SinyorMFMF05.pdf",
        "abstract": "This paper describes the use of the Autonomous Classification Engine (ACE) to classify beatboxing (vocal percussion) sounds. A set of unvoiced percussion sounds belonging to five classes (bass drum, open hihat, closed hihat and two types of snare drum) were recorded and manually segmented. ACE was used to compare various classification techniques, both with and without feature selection. The best result was 95.55% accuracy using AdaBoost with C4.5 decision tress. Keywords: ACE, beatboxing, classification, feature selection. 1 INTRODUCTION Besides tapping one’s fingers, vocalizing percussion is perhaps the most intuitive way for musicians and nonmusicians alike to express a rhythm. The range of sounds that can be made by one’s mouth, however, is far greater than that of fingers alone. The act of vocalizing percussive sounds is as old as music itself, and almost every culture has its own approach. A notable example is Indian Tabla players’ use of bols, a set of vocal sounds used to express rhythmic phrases. In North American culture, two examples immediately come to mind: 50’s doo-wop, and more recently, beatboxing. Both originated in African-American music. The term “beatboxing” originally referred to the mimicking of early 80’s drum-machines, also known as beatboxes. While beatboxing was first used as a backing rhythm for rap performance, it has been developed into an art form in and of itself by performers like Biz Markie and Rahzel. In MIR research, the main interest in beatboxing has been in using it as a means of querying stored drum data, but other possibilities exist. Reliable recognition of different drum sounds could serve as a starting point for developing an intuitive rhythm-performance interface. Similarly, it could be used as an input to a metrical analysis system. The methods described here can also be used to develop a more general mouth-based control channel, as the sounds require very little effort to produce and do not necessarily need to be mapped to drum sounds, or even sounds for that matter. This project centres around an attempt to reliably classify five different drum sounds: bass or kick drum, closed hihat, open hihat and two types of snare drum. We began by collecting a set of vocal percussion samples, from both expert beatboxers as well as nonbeatboxers. The recording of beatboxers was carried out primarily as a study on common ways to vocally express drum sounds. For classification, we ran our collected data through the Autonomous Classification Engine (ACE) described in [1]. ACE combines several approaches to classification and can be used to determine which classifiers and features are effective at classifying a given data set. Furthermore, we used a k-nearest neighbour classifier coupled with genetic algorithm (GA) based feature selection (described in [2]) as a baseline to compare with ACE’s performance. 2 RELATED WORK There are numerous publications dealing with classification of instrument sounds and several dealing specifically with drum sounds (e.g., [3]). This problem also closely resembles speech-recognition problems. Since the audio signals in question are for the most part unvoiced and extremely short (20–100ms), pitch-based analysis tools are generally not successful. This rules out many phoneme-based techniques. Approaches based on plosives and fricatives [4–6], however, are relevant. Generally, previous attempts have used a number of timbral features and statistical classifiers. In [7], the authors describe a system to retrieve a MIDI drum loop from a bank of recorded drum loops by means of a user beatboxing into a microphone. This system attempts to classify the drum sounds into one of three categories, namely bass drum, snare, and hihat. A 97.3% accuracy is reported using zero-crossing rate as the sole feature. While this result is impressive, simply using zero-crossing rate is unlikely to yield similarly high results for more classes or for a larger number of subjects. In [8], the author describes a variety of spectral and temporal features to classify beatboxing samples into four classes: bass drum, snare drum, closed hihat and open hihat. A success rate of 86% for the training set is Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or com-mercial advantage and that copies bear this notice and the full citation on the first page. © 2005 Queen Mary, University of London 672 achieved using 24 features and a C4.5 classifier with boosting. Interestingly, the author reports 90% using the same classifier for a previously unseen test set. Somewhat different, but worth mentioning, is the system described in [9], which uses complete syllables (e.g., “don”, “ta”, “zur”) to represent drum sounds. Each syllable is subdivided into consonants, vowels, and nasal sounds. 3 DATA COLLECTION",
        "zenodo_id": 1414920,
        "dblp_key": "conf/ismir/SinyorMFMF05",
        "keywords": [
            "beatboxing",
            "vocal percussion",
            "classification",
            "feature selection",
            "unvoiced percussion sounds",
            "five classes",
            "bass drum",
            "open hihat",
            "closed hihat",
            "snare drum"
        ],
        "content": "BEATBOX CLASSIFICATION USING ACE\nElliot Sinyor Cory McKay Rebecca\nFiebrinkDaniel McEnnis Ichiro Fujinaga\nMusic Technology\nMcGill University\nMontreal, Quebec\n{elliot.sinyor, cory.mckay, rebecca.fiebrink, daniel.mcennis}@mail.mcgill.ca\nich@music.mcgill.ca\nABSTRACT\nThis paper describes the use of the Autonomous Classifi-\ncation Engine (ACE) to classify beatboxing (vocal per-\ncussion) sounds. A set of unvoiced percussion sounds\nbelonging to five classes (bass drum, open hihat, closed\nhihat and two types of snare drum) were recorded and\nmanually segmented. ACE was used to compare various\nclassification techniques, both with and without feature\nselection. The best result was 95.55% accuracy using\nAdaBoost with C4.5 decision tress.\nKeywords: ACE, beatboxing, classification, feature\nselection.\n1 INTRODUCTION\nBesides tapping one’s fingers, vocalizing percussion is\nperhaps the most intuitive way for musicians and non-\nmusicians alike to express a rhythm. The range of sounds\nthat can be made by one’s mouth, however, is far greater\nthan that of fingers alone. The act of vocalizing percus-\nsive sounds is as old as music itself, and almost every\nculture has its own approach. A notable example is In-\ndian Tabla players’ use of bols, a set of vocal sounds\nused to express rhythmic phrases.\nIn North American culture, two examples immedi-\nately come to mind: 50’s doo-wop, and more recently,\nbeatboxing. Both originated in African-American music.\nThe term “beatboxing” originally referred to the mim-\nicking of early 80’s drum-machines, also known as\nbeatboxes. While beatboxing was first used as a backing\nrhythm for rap performance, it has been developed into\nan art form in and of itself by performers like Biz\nMarkie and Rahzel.\nIn MIR research, the main interest in beatboxing has\nbeen in using it as a means of querying stored drum\ndata, but other possibilities exist. Reliable recognition\nof different drum sounds could serve as a starting point\nfor developing an intuitive rhythm-performance inter-face. Similarly, it could be used as an input to a metri-\ncal analysis system. The methods described here can\nalso be used to develop a more general mouth-based\ncontrol channel, as the sounds require very little effort\nto produce and do not necessarily need to be mapped to\ndrum sounds, or even sounds for that matter.\nThis project centres around an attempt to reliably\nclassify five different drum sounds: bass or kick drum,\nclosed hihat, open hihat and two types of snare drum.\nWe began by collecting a set of vocal percussion sam-\nples, from both expert beatboxers as well as non-\nbeatboxers. The recording of beatboxers was carried out\nprimarily as a study on common ways to vocally ex-\npress drum sounds.\nFor classification, we ran our collected data through the\nAutonomous Classification Engine (ACE) described in\n[1]. ACE combines several approaches to classification\nand can be used to determine which classifiers and fea-\ntures are effective at classifying a given data set. Fur-\nthermore, we used a k-nearest neighbour classifier cou-\npled with genetic algorithm (GA) based feature selection\n(described in [2]) as a baseline to compare with ACE’s\nperformance.\n2 RELATED WORK\nThere are numerous publications dealing with classifica-\ntion of instrument sounds and several dealing specifi-\ncally with drum sounds (e.g., [3]). This problem also\nclosely resembles speech-recognition problems. Since\nthe audio signals in question are for the most part un-\nvoiced and extremely short (20–100ms), pitch-based\nanalysis tools are generally not successful. This rules\nout many phoneme-based techniques. Approaches based\non plosives and fricatives [4–6], however, are relevant.\nGenerally, previous attempts have used a number of\ntimbral features and statistical classifiers.\nIn [7], the authors describe a system to retrieve a\nMIDI drum loop from a bank of recorded drum loops by\nmeans of a user beatboxing into a microphone. This\nsystem attempts to classify the drum sounds into one of\nthree categories, namely bass drum, snare, and hihat. A\n97.3% accuracy is reported using zero-crossing rate as\nthe sole feature. While this result is impressive, simply\nusing zero-crossing rate is unlikely to yield similarly\nhigh results for more classes or for a larger number of\nsubjects.\nIn [8], the author describes a variety of spectral and\ntemporal features to classify beatboxing samples into\nfour classes: bass drum, snare drum, closed hihat and\nopen hihat. A success rate of 86% for the training set isPermission to make digital or hard copies of all or part of\nthis work for personal or classroom use is granted without\nfee provided that copies are not made or distributed for\nprofit or com-mercial advantage and that copies bear this\nnotice and the full citation on the first page.\n© 2005 Queen Mary, University of London\n672achieved using 24 features and a C4.5 classifier with\nboosting. Interestingly, the author reports 90% using\nthe same classifier for a previously unseen test set.\nSomewhat different, but worth mentioning, is the sys-\ntem described in [9], which uses complete syllables\n(e.g., “don”, “ta”, “zur”) to represent drum sounds. Each\nsyllable is subdivided into consonants, vowels, and na-\nsal sounds.\n3 DATA COLLECTION\n3.1 Recording\nThe data collection involved three accomplished1 beat-\nboxers and three non-beatboxers. The rationale behind\nusing  beatboxers was to observe the range of sounds\nthey made and how they opted to mimic the four initial\nclasses we required. While we could have done the same\nwith non-beatboxers, they are less likely to have found\nand refined a particular way to make a certain drum\nsound. These preliminary observations proved useful, as\nwe discovered two common ways to make a snare sound,\nwhich led us to change our taxonomy.\nEach subject recorded individual drum hit sounds\nboth separately and as part of beat patterns. Subjects\nwere instructed to imitate a kick drum (also known as a\nbass drum), a snare drum, a closed hihat and an open\nhihat. They were provided with an example of each but\ntold to vocalize as was usual for them. Also, they were\ninstructed to make the sounds unvoiced, or non-pitched.\nThe recordings took place in two slightly different\nacoustic environments. Half were done in a small office\nwith linoleum flooring and painted concrete walls, and\nthe remainder were done in an office-like environment\nwith carpeting and acoustic-tiled ceilings. The rooms\nlikely had only a small effect on the recordings, since\nalmost all subjects held the microphone extremely close\nto their mouths, often touching it to their lips. In fact,\ntwo of the beatboxers made use of their free hand to cup\nthe microphone while doing bass drum hits.\nThe recording was done using ProTools with a\nDigidesign MBox audio interface and a Shure SM58\ndynamic vocal microphone. The microphone input lev-\nels were kept the same for all recordings, but no normal-\nizing was done on any of the audio data, as some vari-\nance in level was desired to account for the difference in\nbeatboxing style from subject to subject.\nThe recording was split into two parts. First, subjects\nwere told to simply make beats as they pleased using the\nabove drum sounds. Next they were told to record each\ndrum hit in sets of 10, and then repeat this process three\nmore times, yielding 40 samples of each type. They\nwere instructed to try to keep the hits similar to each\nother.\n3.2 Segmentation\nThe segmentation was done manually using Audacity,\nan open-source and multi-platform audio editor. Special\n                                                \n1 All three perform as part of a cappella  groups and two participate in\nbeatboxing competitions.effort was made to include little to no silence, as the re-\nsulting low-amplitude background noise could bias cer-\ntain features, most notably ones based on the zero-\ncrossing rate. Once segmented and labelled, all drum hits\nwere exported as numbered WAV files, all mono with a\nbit rate of 16 bits and a sampling rate of 44.1 kHz.\nWhile manual hit segmentation was sufficient for the\nneeds of this project, larger-scale projects would require\nautomatic segmentation. Several experiments on our\ndata showed that the relative difference function, as dis-\ncussed in [10], is particularly useful for onset detection.\nThe entire sample set consisted of 1206 drum hits.\nAfter auditioning the sample set, 12 samples were re-\nmoved due to audible clicks or very low level input.\nSince the goal was to have a realistic data set, some\nquestionable samples were included and only obviously\nflawed ones were removed.\nIn general, each drum hit sounded qualitatively similar\nacross subjects. This may have been partly due to the\nfact that the subjects were given model examples. The\nkick drums most closely resembled the unvoiced bilabial\nplosive (/p/), the closed hihat resembled the unvoiced\nalveolar plosive (/t/) and the open hihat most resembled\nthe unvoiced alveolar fricative (/s/).\nThe snare hits were vocalized slightly differently by\ndifferent subjects. Two of the subjects imitated the snare\ndrum by combining the bilabial plosive and alveolar\nfricative to make a short and explosive “pss” sound. It\ncan be thought of as a combination of the /p/ sound\nwith the /s/ sound, as the waveform in Figure 4 shows.\nThe remaining beatboxer imitated the snare drum by\nmaking an unvoiced velar plosive, or /k/ sound. Two of\nthe non-beatboxers also did this, and the remaining one\nwas unsure of what to do, so he did both. The snare\ncategory was thus subdivided into two: p-snare and k-\nsnare. The figures below provide illustrations of the\nwaveforms from the five classes.\n \nFigure 1. Kick drum (beatboxer, non-beatboxer)\n \nFigure 2. Closed-hihat (beatboxer, non-\nbeatboxer)\n673 \nFigure 3. Open-hihat (beatboxer, non-\nbeatboxer)\nFigure 4. p-snare (beatboxer)\nFigure 5. k-snare (non-beatboxer)\n4 CLASSIFICATION\nThe final dataset, totalling 1192 samples, is divided into\nfive classes: 311 kick drum, 298 closed hihat, 290 open\nhihat, 137 p-snare and 156 k-snare.\n4.1 Features\nThe following features were extracted using ACE’s\njAudio feature extractor component [11]:\nSp_Centroid_Overall_Avg false\nSp_Centroid_Overall_Std_Dev \nfalse\nSp_Rolloff_Point_Overall_Avg \ntrue\nSpectral_Rolloff_Point_Ovl_Std_Dev true\nSp_Flux_Overall_Avg false\nSpectral_Flux_Overall_Std_Dev\nfalse\nCompactness_Overall_Avg true\nCompactness_Overall_Std_Dev  true\n                                                                             \n2 All three perform as part of a cappella  groups and two participate in\nbeatboxing competitions.Spectral_Variability_Overall_Avg true\nSpectral_Variability_Overall_Std_Dev true\nRMS_Overall_Avg false\nRMS_Overall_Std_Dev true\nRMS_Derivative_Overall_Avg true\nRMS_Derivative_Overall_Std_Dev true\nZC_Overall_Avg true\nZC_Overall_Std_Dev  true\nZC_Derivative_Overall_Avg true\nZC_Derivative_Overall_Std_Dev\nfalse\nStrongest_Freq_Via_ZC_Overall_Avg false\nStrongest_Freq_Via_ZC_Overall_Std_Dev false\nStrongest_Freq_Via_SC_Overall_Avg \ntrue\nStrongest_Freq_Via_SC_Overall_Std_Dev false\nStrongest_Freq_Via_FFT_Max_Overall_Avg false\nStrongest_Freq_Via_FFT_Max_Ov_Std_Dev    true\nFigure 6. Features used for classification.\nIn the above list, RMS refers to Root Mean Sqaure, ZC\nrefers to zero-crossing, and SC refers to spectral cen-\ntroid. Strongest_Freq_Via_ZC  refers to the strongest\nfrequency in Hz that corresponds to the ZC rate. Like-\nwise, Strongest_Freq_Via_FFT_Maxds refers to the\nfrequency corresponding to the highest peak of the FFT.\nFlux is a measure of the difference between two succes-\nsive FFT windows. Compactness is used to measure the\ndegree of noise in a signal and is measured as follows:\n€ log(M[n])−log(M[n−1])+log(M[n])+log(M[n+1])\n3n=1N−1\n∑\nwhere M[n] is the nth bin of the magnitude spectrum.\nFor FFT-based features, a Hanning window of 512\nsamples with no overlap was used.\n4.2 Feature Selection\nIn addition to the ACE-based experiment, a second ex-\nperiment was performed using a genetic algorithm fea-\nture-selection system described in [2].  The chosen fea-\ntures are labelled as “true” in Figure 6. These features\nwere chosen using an initial population of 50 chromo-\nsomes, and it took 14 generations to achieve conver-\ngence. Since the GA assigns fitness to chromosomes\nbased on the performance of a classifier that is trained\nwith the corresponding feature set, the chosen features\ncan be quite classifier-dependent. In our case, a k-NN\nclassifier was used,  for which the presence of redundant\nfeatures can impair classification. Removing redundancies\nhas a positive effect on classification rate and thus chro-\nmosomes that exclude redundant features will have higher\nfitness scores. What this means is that the selected fea-\ntures are more likely to be discriminant, however the\nrejected features are not necessarily useless.\n4.3 Results using ACE\nWithout feature selection, the best accuracy rate achieved\nwas 95.55% using AdaBoost with C4.5 decision trees as\nbase learners. Other successful approaches used by ACE\nincluded a backpropagation neural network, which yielded\n674an accuracy rate of 93.37%, and a support vector ma-\nchine, which yielded a rate of 83.8%. ACE also per-\nformed naïve Bayes and k-NN classification, with k rang-\ning from 1 to 12. Also, as a means of comparing our\nsystem with the one described in [1], we reduced the\nnumber of classes to three (bass, snare, hihat) and ob-\ntained an accuracy rate of 98.15%.\nUsing the features selected by the GA system with a 1-\nNN classifier, an accuracy of 94.55% was achieved. This\ncan be compared to a rate of 89.36% when ACE used a 1-\nNN classifier and all 24 features. In all cases, 10-fold\ncross validation was used.\nTable 1. Confusion matrix for best classification\nresult, AdaBoost with C4.5\nClassified as:\na b c d e Actual:\n309 0 1 0 1 a = kick\n0 278 12 0 0 b = open\n0 15 273 6 4 c = closed\n0 1 5 149 1 d = k-snare\n2 1 1 3 130 e = p-snare\nTable 2. Recent attempts at beatbox classifica-\ntion. Only best accuracy rates are shown.\nAuthor # of\nclasses# of\nsamples# of\nfeat.Classifier Acc.\nKapur 3 75 1 ANN 97.3%\nHazan 4 242 28 C4.5 w/\nboosting86%\nSinyor 5 1192 24 C4.5 w/\nboosting95.55%\nSinyor 3 1192 24 C4.5 w/\nboosting98.15%\n5 CONCLUSION AND FUTURE WORK\nThe results described above are promising, and the high\nclassification rate shows this work to be a good starting\npoint for future vocalized music query systems and other\nvoice-control applications. The GA-based feature selec-\ntion approach identified particular features that would\nserve as a focus for future work with this type of data. It\nshould be noted that some sort of automatic segmenta-\ntion would be required in order for this approach to be\nused as part of a larger system.\n6 ACKNOWLEDGEMENTS\nThe authors would like to thank beatboxers Benjamin\nHammond, Jason Levine, and Kweku Sam Kwofie aswell as non-beatboxers Ansel Brandt and Joseph Malloch\nfor their time and beats.\nREFERENCES\n[1] McKay, C., Fiebrink, R., McEnnis, D., Li, B., and\nFujinaga, I. “ACE: A framework for optimizing\nmusic classification,” International Conference on\nMusic Information Retrieval , 2005.\n[2] Fiebrink, R., McKay, C., and Fujinaga, I.,\n“Combining D2K and JGAP for efficient feature\nweighting for classification tasks in music\ninformation retrieval,” International Conference on\nMusic Information Retrieval , 2005.\n[3] Tindale, A., Kapur, A., Tzanetakis, G., and\nFujinaga, I. “Retrieval of percussion gestures using\ntimbre classification techniques,” Proceedings of\nthe International Conference on Music\nInformation Retrieval , 2004.\n[4] Chan, C., and Ng, K. “Separation of fricatives from\naspirated plosives by means of temporal spectral\nvariation,” IEEE Transactions on Acoustics, Speech,\nand Signal Processing , 33(15) (Oct. 1985),\n1130–1137.\n[5] Molho, L. “Automatic acoustic-phonetic analysis of\nfricatives and plosives,” Proceedings of the IEEE\nInternational Conference on Acoustics, Speech, and\nSignal Processing , 1976.\n[6] Demichelis, P., De Mori, R., Laface, P., and\nO’Kane, M. “Computer recognition of stop\nconsonants,” Proceedings of the IEEE International\nConference on Acoustics, Speech, and Signal\nProcessing, 1979.\n[7] Kapur, A., Benning, M., and Tzanetakis, G.\n“Query- by-beat-boxing: Music retrieval for the DJ,”\nProceedings of the International Conference on\nMusic Information Retrieval , 2004.\n[8] Hazan, A. “Towards automatic transcription of\nexpressive oral percussive performances,”\nProceedings of the International Conference on\nIntelligent User Interfaces , 2005.\n[9] Nakano, T., Ogata, J., Goto, M., and Hiraga, Y.\n“A drum pattern retrieval method by voice\npercussion,” Proceedings of the International\nConference on Music Information Retrieval ,\n2004.\n[10] Klapuri, A. “Sound onset detection by applying\npsychoacoustic knowledge,” Proceedings of the\nIEEE International Conference on Acoustics,\nSpeech, and Signal Processing , 1999.\n[11] McEnnis, D., McKay, C., Fujinaga, I., and Depalle,\nP. “JAudio: A feature extraction library,”\nInternational Conference on Music Information\nRetrieval , 2005.\n675"
    },
    {
        "title": "Improving Content-Based Similarity Measures by Training a Collaborative Model.",
        "author": [
            "Richard Stenzel",
            "Thomas Kamps"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416090",
        "url": "https://doi.org/10.5281/zenodo.1416090",
        "ee": "https://zenodo.org/records/1416090/files/StenzelK05.pdf",
        "abstract": "We observed that for multimedia data – especially music collaborative similarity measures perform much better than similarity measures derived from content-based sound features. Our observation is based on a large scale evaluation with >250,000,000 collaborative data points crawled from the web and >190,000 songs annotated with content-based sound feature sets. A song mentioned in a playlist is regarded as one collaborative data point. In this paper we present a novel approach to bridging the performance gap between collaborative and contentbased similarity measures. In the initial training phase a model vector for each song is computed, based on collaborative data. Each vector consists of 200 overlapping unlabelled 'genres' or song clusters. Instead of using explicit numerical voting, we use implicit user profile data as collaborative data source, which is, for example, available as purchase histories in many large scale ecommerce applications. After the training phase, we used support vector machines based on content-based sound features to predict the collaborative model vectors. These predicted model vectors are finally used to compute the similarity between songs. We show that combining collaborative and content-based similarity measures can help to overcome the new item problem in e-commerce applications that offer a collaborative similarity recommender as service to their customers.",
        "zenodo_id": 1416090,
        "dblp_key": "conf/ismir/StenzelK05",
        "keywords": [
            "multimedia",
            "music",
            "collaborative",
            "similarity",
            "measures",
            "content-based",
            "sound",
            "features",
            "evaluation",
            "ecommerce"
        ],
        "content": "IMPROVING CONTENT-BASED SIMILARITY MEASURES BY \nTRAINING A COLLABORATIVE MODEL\n Richard Stenzel, Thomas Kamps  \n Fraunhofer IPSI \nDolivostr. 15 \nD-64293 Darmstadt \nstenzel@ipsi.fraunhofer.de\nkamps@ipsi.fraunhofer.de \n  \nABSTRACT \nWe observed that for multimedia data – especially music \n- collaborative similarity measures perform much better than similarity measures derived from content-based sound features. Our observation is based on a large scale evaluation with >250,000,000 collaborative data points crawled from the web and >190,000 songs annotated with content-based sound feature sets. A song mentioned \nin a playlist is regarded as one collaborative data point. \nIn this paper we present a novel approach to bridging the performance gap between collaborative and content-based similarity measures. In the initial training phase a model vector for each song is computed, based on col-laborative data. Each vector consists of 200 overlapping unlabelled 'genres' or song clusters. Instead of using ex-plicit numerical voting, we use implicit user profile data as collaborative data source, which is, for example, available as purchase histories in many large scale e-commerce applications. After the training phase, we used support vector machines based on content-based sound features to predict the collaborative model vec-tors. These predicted model v ectors are finally used to \ncompute the similarity between songs. We show that combining collaborative and content-based similarity measures can help to overcome the new item problem in e-commerce applications that  offer a collaborative simi-\nlarity recommender as service to their customers. \n \nKeywords: collaborative metadata, content based sound \nfeature similarity measures , music similarity, machine \nlearning, acoustic measures, evaluation, recommender \n1 INTRODUCTION \nFrom the point of view of e-commerce, organizing and \nrecommending items is an important but challenging task. Collaborative metadata constitute an important ba-sis to this end: Web server logs of our customers in the area of newspaper publishing showed that 60 to 70% of the links of a page are followed due to similarity rec-\nommendations like \"people who lik ed this, also liked ...\".  \nIn today’s e-commerce shops for music, two domi-\nnant approaches to generating similarity recommenda-tion links can be found: (1) a genre-based approach, where each song is assigned to one or more musical genres and (2) a collaborative similarity approach, where similarity is computed based on purchase histo-ries.  \nThe genre-based approach needs to label each song \nby hand with some high level genre information like jazz, blues, classical etc. Most shops (e.g. Amazon.com, \nmp3.com, buy.com, Musicload.de, Musicline.de) pro-vide a genre-based naviga tion schema. Because a shop \nmay sell more than 800,000 songs, a small number of high level categories would result in huge song classes and are therefore mostly not su fficient. In order to create \nsmaller classes, some shops use more fine grained cate-gories. Musicline.de, for example, uses about 220 genre classes for categorisation. Su ch a system has some se-\nvere drawbacks. First, all songs have to be labelled manually. Second, even with 220 genres and a perfectly balanced distribution of the 800,000 songs across all genres, each genre still would contain an average of 3,636 songs, which is far too large an amount for a user to navigate. As a further drawback it has to be consid-ered that effective access by fine-grained genres is only \npossible for a musicologist. Finally, genres are highly subjective. As [1] has shown, given 20 genres, only \n70% percent of all songs are correctly classified by test users.  \nCurrently much research is being done in assigning \ngenres to songs automatically (e.g. [1,2,3]). Based on our experiments with the genre information of the Ger-man Amazon product catalogue, we come to the conclu-sion that their genre classification is not well suited for similarity search in large music databases. Our evalua-\ntion (Section 4) shows, that wh en tested against real life \nmusic listening habits, Amazon-genre based similarity is outperformed by pure content-based sound-feature simi-larity. \nThe second area of research is similarity recommen-\ndation using collaborative data. Amazon, for example, \nprovides a list of recommendations for each music CD. \nThis item-to-item recommendation is based on data mining in the history of purchases [4]. Even if the re-sults of this collaborative similarity recommender are considered to be good, this approach has the drawback of the new item problem. It is not possible to recom-\nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full citation on the first page. \n© 2005 Queen Mary, University of London \n264   \n \n mend new music CDs, because shortly after their release \nthey do not occur in enough (or any) purchase histories yet.  \n In this paper we present a novel approach which at-\ntacks the new item problem of collaborative similarity recommendations, i.e. a methodology to compute a list of recommendations of songs given only one song with-\nout purchase histories. The methodology described in \nthis paper is a first step towards a CF/CN (collaborative filtering/content-based) meta-level recommender ac-cording to Burke [5].  \nBecause of the lack of la rge purchase histories, we \nhave crawled the web for collaborative data. In the do-main of music, collaborative data can be found when searching for personal music playlists, similar to [6] and [7]. Our work is therefore based a large scale evaluation \nwith >250.000.000 collaborative data points, each data point representing one song mentioned in a playlist and >190.000 sound feature sets, each set representing the music of one song. From analysis of the collaborative data points, we created 200 unlabelled and overlapping genres. Our fully automated approach generates as a first step a collaborative model. The principal idea is that this collaborative model consists of one model vec-tor per song. A model vector itself consists of 200 com-ponents, each reflecting one of our predefined unla-\nbelled and overlapping genres. Each component of such a model vector contains the binary information, whether a song belongs to a specific unlabelled genre or not. This is decided according to th e analysis of the playlists. \nEach song is assigned to a minimum of 4 and a maxi-mum of 33 genres. Similarity between two songs is de-fined as the similarity between their model vectors. The major advantage of the generated model, besides its predictive quality, is the ease of prediction given only content-based sound features. \nHaving both the collaborative model vectors and the \ncontent-based sound features for a song, it is possible to train supervised classifiers (support vector machines), which predict for any given sound feature set the 'gen-res' to which it belongs. Even though we currently use only 30 sound features generated by the MARSYAS framework [1], our experiments show that our method outperforms the Amazon genre-based similarity recom-mender as well as similarity recommenders based on pure sound feature data. Due to the fact that a super-vised machine learning approach is used, all possible sound feature extractors can be used to improve our system. Even hand labelled genre data can be added to create better recommendations.  \n2 RELATED WORK \nThe methodology presented in this paper is an attempt \nto combine two groups of similarity recommenders. One group aims to create similarity measures based on con-tent-based sound features. The other group aims to cal-culate similarities from collaborative metadata. \nOur work heavily depends on the generation of de-\nscriptive content-based sound features created from audio signals. First attempts to create similarity meas-\nures based on raw audio signal analysis have been un-dertaken by [8]. The system used a FFT (Fast Fourier Transformation) to compute windowed MFCCs (Mel-Frequency Cepstrum Coefficients) which were quan-tized and accumulated in histograms. These histograms were used to compute similarities, using the cosine of two histogram vectors. Later on, more complex sound features were used. Welsh et al.'s approach [9], for ex-ample, used 1248 sound features per song which, in addition to frequency and amplitude, derived higher \nlevel concepts such as temporal data. Another system [10] aims at generating playlists based on content-based features. \nInstead of creating similarity measures, some re-\nsearch focuses on automatic genre classification based \non raw audio data. Most systems (e.g. [1,2,3]) use a two step approach. First of all, sound features from raw au-dio data are extracted. In a second step, supervised clas-sification is used to categor ize feature vectors into gen-\nres. From the viewpoint of si milarity search, the classi-\nfication into genres is an ideal test bed for evaluation of the descriptiveness of sound features. \nThe use of collaborative metadata is a much investi-\ngated area. Most research uses explicit votes on Likert-scales to predict future user voting (e.g. [11]). This might be due to the existence of the MovieLens Dataset, which acts as a perfect test bed for evaluations. In this \npaper we focus on music similarities, an application area where a gold standard for evaluation does not exist [12]. In [6] several similarity m easures are compared. One of \ntheir conclusions is, that similarity measures derived from subjective data (co-occu rrence analysis of play-\nlists) is the most useful gold standard. Accordingly to this, we use subjective collaborative data as gold stan-dard for our evaluation. \nAnother interesting collaborative approach, which we \ndo not consider in our work, is the use of textual meta-data from the web for artist classification [13,14]. \n3 METHOD \nAn item-to-item recommender needs to measure similar-\nity between individual songs. On e possibility is to derive \na similarity measure from collaborative metadata like purchase histories [4] or playlists. This method produces good results but has the \"new item problem\". Another option is to compute a similarity measure calculated from content-based features like rhythm and spectral \nfeatures [9]. Content-based similarity measures do not \nhave the \"new item problem\" but do not produce as good results as collaborative approaches [6]. \nIn our basic approach we first build a collaborative \nmodel from playlists we crawled from the web. In order to produce good results and to avoid the \"new item problem\" of collaborative r ecommenders, we then feed \ncontent-based sound features (generated by Marsyas [1]) into an auto-learning pr ocess that trains our col-\nlaborative model. This training is carried out by a ma-chine learning process that maps content-based sound \n265   \n \n features to collaborative mode l vectors. The results are \nclearly improved content-based similarity measures. \n3.1   A Collaborative Item-to-Item Model \nGiven a set of songs S = { s 1, s2 … sK } and a set of play-\nlists or purchase histories S = { S i | S i \n S} we compute a \ncollaborative model C = { c 1, c2, … ck } which assigns a \ncollaborative model vector c i  to each song s i. \nMany researchers use supervised classification to \npredict a musical genre for each song ([1,2,3]). An ex-\ntension to this approach is to use multiple classifiers which classify each song to  multiple overlapping 'gen-\nres'. Two songs which share the most 'genres' should be the most similar. This leads to the idea that our collabo-\nrative model consists of multiple overlapping 'genres', \ni.e. a collaborative model vector C \n C is assigned to \neach song and each component C i of C is related to one \n'genre'. If a song belongs to a genre, the corresponding \ncomponent C i is set to 1, otherwise to 0. \n In order to assign a collaborative model vector         \nci \n  C to each song s i \n S, we proceed in two steps. In \nstep (a) unlabelled 'genres' are generated by unsuper-vised clustering of playlists. In step (b), based on the occurrence of songs in playlists which occur in 'genre' \nclusters, each song is assigned to a set of 'genres'. \nThe unsupervised clustering algorithm in step (a) \ncomputes a projection \nS Æ PS  which assigns each play-\nlist S i \n S to an unlabelled 'genre' cluster P j using an \nunsupervised k-means clustering algorithm [15]. \n \n \n \nWhile clustering, we treat each playlist S i \n S as a \nvector v \n  RK with each component v k = |{s k| sk\nSi}|. \nSimilarities and distances ar e computed according to the \nEuclidean norm. Because of the sparseness of the play-\nlist data (our vectors contain 99.988% zeros) an iterative approach has been used.   \n 1) Randomly pick one playlist vector S\ni\nS for each \nplaylist cluster P j as initial centroid \n2) Compute the centroid P j,center  for each playlist \ncluster P j \n∑=\niS ii\njcenterjSS\nPP||||||||1:,r                       (2) \n3) Assign each playlist S i\nS to the closest centroid \nPj using the cosine as similarity function. \n||||*||||), cos(\nj ij i\nj iS SSSSS vvvvvv ⋅=                      (3) \n4)  Goto 2) until no more changes occur \n \nIn step (b) of the generation of the collaborative item-\nto-item model, a model vector D is computed for each \nsong s s \n S. Given a set of clusters P S of playlists con-\ntaining songs, each component D k of D consists of the \nnumber of playlists in cluster P k \n PS containing the song \nss \n S.  \nDk := |{ S i | ss \n Si \n Si \n Pk }| with 1 \n  k \n |PS| (4) \n \nDue to the fact that binary  classification is more ac-\ncurate than prediction of real numbers, all nonzero components in D\nk could be set to 1, i.e. whenever a \nsong occurs in a playlist cluster P k, the corresponding \nvector component D k could be set to 1, otherwise to 0. \nThis discretisation leads to bad results, because playlists are highly noisy, which means that some songs occur in too many clusters only once or twice. Therefore, we have used a threshold, i.e. we compute a vector C \n C \nfor each song where all components C k are set to 1 if   \nDk > threshold value and set to 0 otherwise.  \n)5( 1\n001.0 1\nSk\nk P k with\notherwiseDDifC ≤≤\n⎪⎩⎪⎨⎧<=      \nThe threshold value of 0.01 has been determined em-\npirically by maximizing in our evaluation results (Sec-\ntion 4). The evaluation will show that each step de-scribed above reduces the predictive quality of the col-laborative model when using the cosine as similarity \nmeasure. The collaborative model nonetheless makes \nbetter predictions than a pure sound-feature-based re-commender. \n3.2   Predicting a Model Vector from Raw Sound \nData \nIn the previous section we generated a collaborative \nmodel \nC = {c 1 … ck} which assigns each song s i \n S a \ncollaborative model vector c i \n C. In this section we use \nmachine learning techniques to learn a mapping from \nraw sound data of song s i \n S to a collaborative model \nvector c i \n C. Having learned this mapping, it is possible \nto predict collaborative model vectors for songs, which are not in the initial training set S, i.e. once the mapping is learned it is possible to compute 'collaborative' simi-\nlarities between songs using the collaborative model \neven for songs which do not occur in playlists S\ni \n S.  \nThere are two steps to learning a mapping from raw \nsound data of songs s i \n S to model vectors c i \n C. First \nwe have to extract content-based sound features f i \n F \nfrom each song s i \n S. In a second step |PS| classifiers \nare trained to map feature vectors f i \n F  to components \nof model vectors c i \n C. \nThe first step, content-based feature extraction, is \ncurrently based on the Marsyas Framework [1], which extracts 30 real valued features from each sound file \ns\ni\nS, resulting in a feature vector f i\nF for each song. \nThese 30 real valued features, which are called 'genre-\nfeatures' in Marsysas, include  averaged timbral texture \nfeatures (e.g. spectral centr oid), rhythmic content fea-\ntures (e.g. beats per minute) and pitch content features. In principle, other kinds of features can be added, re-gardless of whether they are real valued or binary (e.g. \nexisting hand labelled classifications).  \n266   \n \n In the second step we train |PS| binary support vector \nmachine (SVM) classifiers ( RN=30 Æ [0,1]), one for each \ncomponent C i of C \n  C using the WEKA toolkit [16]. \nGood parameters for the SVM RBF kernel (support \nvector machine with radial basis function kernel), are \nidentified by means of a grid search. \nAfter the training phase, similarities between two \nnew songs s' 1 and s' 2 which do not occur in any playlist \nSi \n S can be calculated using the collaborative model. \nThis is done in three steps.   \n1. (\nS Æ F') Extraction of the sound features  \nf'1,f'2 \n F ' from songs s' 1 and s' 2, based on the \naudio-signal \n2. (F Æ Ci    with F \n F ' \n C \n C ') Prediction of \nthe two model vectors c' 1,c'2 \n C ' by applying \nthe |PS| trained SVMs to the sound features vec-\ntors f' 1 and f' 2, i.e. each component of c' 1,i  and \nc'2,i of c' 1 and c' 2 is predicted independently. \n3. Computation of the similarity between c' 1 and c' 2, \nwhich is defined as the cosine of c' 1 and c' 2. \n \n \nFigure 1:  Overall Block Diagram of our Method \nBy predicting the collaborative model vectors from \nsound features, collaborative recommenders can be \nseamlessly extended with content-based similarity to deal with the new item problem. Our method can be employed for completely new songs. For songs which have a large purchase history, a purely collaborative approach can be used. For songs which have not been sold very often, a combination of collaborative and con-tent-based methods could be used. The combination could be arrived at, for example via a weighted sum of two model vectors C \n C, where one vector is based on \nour method, the other one on collaborative data.  The use of binary components in C and the inde-\npendent training of SVMs  for each component C i of C \n  \nC is not optimal. This is due to the fact that the compo-\nnents D i (4) of our collaborative model vectors D are \nnot mutually independent, b ecause each component of a \nvector D is based on a cluster of playlists. Playlist clus-\nters themselves have similar ities to each other, i.e. it is \nlikely that if a song occurs in one playlist cluster it will \noccur in the most similar clusters too. This means that if one component D\ni (=cluster) in D is set to a specific \nvalue, the most similar components of D will have simi-lar values. It would have been nice to train a mapping from sound features F\nF to D without any discretiza-\ntions (DÆC) using an algorithm, which learns vector-\nvalued functions ( RNÆR|PS|). Unfortunately, research on \nlearning vector-valued functions is still at a rather early stage [17]. A simpler approach  is to neglect the mutual \ndependence of components D\ni of D and train independ-\nent classifiers to predict binary classes of C i.  \n4 EXPERIMENTS \nIn our experiments we build and evaluate four types of similarity measures, varying in their data sources. These similarity measures are based on data from \n• Collaborative sources \n• Content-based sound features \n• Amazon's genre data \n• Collaborative sources and content-based sound features (our method) \nBased on the similarity measures a song recom-\nmender for each measure is created. Each recommender \ncomputes for each given song s\ni \n S the N most similar \nsongs TopN = {t 1…tN | ti \n S}. In order to evaluate the \npredictive performance of each recommender, we fol-\nlow the observations of Berenzweig [6] that collabora-tive data sources are the best gold standard. This means that our evaluation metric is based on collaborative playlist data \nStest = { S i,test | S i,test \n S} which is not used  \nin the training phase. Precisi on, Recall and F1 are calcu-\nlated for each song s song \n S and averaged over all \nsongs. \n ∑∩\n∈=\ntestStesti\ntesti song testiTopNTopN S\nS s Sprecision,\n, , } | {1  (6) \n \n ∑∩\n∈=\ntestS testitesti\ntesti song testi STopN S\nS s Srecall\n,,\n, , } | {1    (7)    \n \nrecall precisionrecall precisionF+=**21                      (8) \nF1 is the classic information retrieval measure for over-all retrieval performance. In our evaluation metrics \"pre-\ncision\" reflects the probab ility given one song by the \nuser that this user would add the recommended song in \na playlist together with the given one.  \n Collaborative Pla ylist Data  S \nCluster Pla ylists:  S ÆPS \nCompute C \n C  for each Song ss \n S \nby Thresholding D k := |{ S i | ss \n Si \n Si \n  Pk }| \nCollaborative model C = {c 1 … ck} \nFor each Son g si:     c i = [1 0 0 … 1 0] \n R|PS| \nTrain/Use | PS | SVM-Classifiers RN Æ [0,1] \nF Æ Ci   with F \n F   \n C \n C \nFeatures F = {f1…fK |  fi  \nRN }   \nSound Feature Extraction  S Æ F \nRaw Sound Data of Songs S = {s1 … sK} Similarity := cos(c i, ck) \nSection: Predicting a Model Vector from Raw Sound Data  Section: A Collaborative Item-to-Item model  \n267   \n \n In the following subsection we show that it is possi-\nble to create collaborative sim ilarity measures by crawl-\ning music playlists from the web. We show in a large \nscale evaluation that the quantity of the crawled data if a key factor regarding the predictive performance of this approach. In Section 4.2 we create and evaluate a simi-\nlarity measure like proposed in Section 3.1, which is \nbased on crawled collaborative data. As a testing base-\nline we create and evaluate similarity measures based on pure content-based sound features and based on Ama-zon's genre-data in Section 4.3. Finally in Section 4.4 we train and predict a collaborative model based on content-based sound features like proposed in section 3.2.  \n4.1  Crawling the Web for Music Playlists \nIn this section a song item-to-item recommender based \non 250,000,000 collaborative data points crawled from the web is built and evaluated. We show that 'size mat-ters', i.e. the quality of recommendation increases with \nthe quantity of collaborative data or, more formally, that the averaged F1 measure of the overall system is propor-tional to log(|Data|). \nA collaborative model of music similarity which can \nbe tested against real world data (e.g. user playlists, ra-\ndio programme histories) should cover a large portion of current music. [7], for example, used a 2800 album dataset which was used by a closed community. Be-cause of the lack of a free dataset, we choose to crawl the web for real world playlists.  \nSome programs (e.g. WinAmp, XMMS) produce \nHTML pages with playlists. Due to the low cost of pub-lishing web pages, many people put \"the playlist of our last party\" online. These pages have simple structures and can be found using common search engines (e.g. altavista, google).  The idea is to search for specific words within the auto generated HTML page (e.g. for WinAmp +\"WinAmp generated playlist\") in combina-tion with artist names (e.g. +\"Eric Clapton\").  Artist names can be taken from a music catalogue, such as Amazon. Unfortunately, not all playlists contain valu-able information, e.g. some contain \"all tracks I have on my harddisk\". To decide, if a playlist should be used, we employed a rule based filter. The rules are: a playlist must contain at least three di fferent artists, have at most \n300 entries and tracks may not be sorted alphabetically. \nDue to limitations of search engines, such as a re-\nstriction on queries per day and computer, the crawler has to run in a distributed manner and should automati-cally crawl the immediate area of a match. After two years of crawling we have gathered >250,000,000 play-list member datasets.  \nThe listed songs in playlists do not have globally \nidentifiable keys. Usually a playlist entry contains only \na string like \"track01.mp3\" or \"(eagles)-the long run-04 the disco strangler.ogg\". Our first attempt to match the text strings (>17.000.000) with the free CDDB failed, because the CDDB is not itself normalized (e.g. \"Track01\" is a common track name in the CDDB). In order to produce high quality results, we have used the \nAmazon product catalogue as a dictionary to match against. Even if Amazon sells only a small portion of existing music, and not all CDs in the catalogue contain track information, we were able to match about 7.5% of our playlist entries against this reliable source (com-pared to 39% using the CDDB, where \"Track01\" is in-\ncluded). \nThe result of crawling and matching is a data set with \n176,930 songs, 880,985 playlists and 18,769,469 play-\nlist entries. Compared to ot her research systems, our \napproach (>8,000,000 playlists before tagging and fil-tering) is an extremely larg e scale approach. E.g. [6], \nwho stated that the similar ity measures created with \nplaylist data, is the \"most useful ground truth\" (gold standard), used only 29,000 playlists from \"The Art of the Mix\" (www.artofthemix.org), which is a small sub-set of our data (about 0.3% of our data set).  \nTo evaluate the improveme nt in prediction accuracy \nwith the increase of data, we have produced a simple artist similarity recommender. The similarity between two artists is defined by the cosine between their play-list occurrence vectors. Test data was taken to compute an averaged F1 measure. The evaluation results are \nshown in Figure 2. The F1 measure is approximately proportional to log(|Data|). \n \n Figure 2:  Each line represents a collaborative \nartist similarity recommender, evaluated against \nplaylists. The lowest line is built from 1% of all \navailable playlist data, the highest from 100%. \nThe F1 measure (Y-Axis) increases roughly with log(|Data|). The number of recommended Items \n(X-Axis) influences F1. \nDue to the fact, that we did not know whether the re-\nsults are specific to the cosi ne algorithm, we used an \nassociation rules algorithm and some variants. The dif-\nferent algorithms did not change the result in principle. The normalisation rules, such as the rule that every artist occurs only once in a playlist, have more influence on the results than alternative algorithms have. \nIn addition to the automated evaluation against \ncrawled playlists, we ran a web based test, where three different recommender engines were randomly pre-sented to test users. The test users had to express their satisfaction with the recommended results. The three recommender engines only differ in the amount of data used to train them (1%, 8% and 100% of all playlist \n268   \n \n data). Interestingly, even if the test users did not know \nwhich engine was presented, their ranking corresponded with the F1 measures of our automated evaluation. \n4.2 Creating a Collaborative Model \nWith 30 content-based features per song provided by the \nMarsyas framework we cannot realistically train high dimensional collaborative models. Thus we need to re-duce the dimension of the collaborative model. To inves-tigate the effect of reducing dimensions we have per-formed the following experiment:  \nUsing the playlist data set described above, it is pos-\nsible to build a collaborative model described in Section 3.1. In the creation process, there are four steps in which \na similarity measure between songs can be computed and evaluated against real life playlist data. In each step, a feature vector is assigned to each song. Similarity be-tween two songs can be evaluated using the cosine measure between the feature vectors. \nIn the first step we used the raw playlist data to create \na similarity measure. Even if  the cosine measure is not \nthe first choice for computing similarity, we use it to be compatible with the other steps. Each component of a song's feature vector in this step is a playlist. If the song occurs in the playlist, the component is set to 1, other-wise to 0. The evaluation of the resulting similarity \nmeasure is the highest line (best performance) in figure \n3, marked as 'collaborative'.  \nThe second line from the top, marked as 'clustered \nsongs', shows the performance of the second step, in \nwhich the feature vector D is used to create a similarity \nmeasure, i.e. at this point all playlists are clustered in \n|PS| = 200 clusters using the k-means algorithm de-\nscribed in section 3.1. Each vector D for a song s song \n  S \nhas 200 components D i and each of these components \ncontains the number of occurrences of song s song in play-\nlist cluster P i. \nOur experiments show that if we create less than 200 \nclusters, the predictive precision decreases. Creating \nmore than 200 clusters does not increase the precision. The difference in the results  of precision when creating \n200 or 400 playlist clusters is about 4±0.5 percent. \n \nFigure 3:  Evaluation of reduction steps described \nin Section 3.1 against a test set of playlists. The third step and third line in figure 3 uses D/||D|| as \nfeature vector with all components <0.01 set to 0. And \nfinally, the bottom line in figure 3 uses the binary fea-ture vector C \n C as source for the similarity measure, \nwhere the feature vector from step 3 is taken and all \ncomponents >0 are set to 1. \nIt is obvious that each step  decreases the predictive \nperformance of the similarity measure. On the other hand, even the bottom line of the collaborative model performs far better than purely content-based similarity \nmeasures (Section 4.3). \n4.3  Comparing a Content-based and a Genre-based \nMusic Recommender \nIn this section we set up a testing baseline by generating \nand evaluating three similarity measures based on three different data sources. One similarity measure is based on 30 dimensional content-based sound feature data pro-vided by Marsyas [1], the second one is based on Ama-zon's genre-data and the third is based on a 30 dimen-sional random vector.   \nIn order to build a content-based music recom-\nmender, it is necessary to ha ve a large database of sound \nfeatures. Due to the lack of  a common set of free test \nsongs, or a free large scale collection of sound features, we created a sound feature extraction utility based on Marsyas [1] which extracts a 30 dimensional feature vector per song. The extraction utility, which sends ex-tracted metadata via http to a central server, runs on our \nstudents' private music collections. Unfortunately, the \ncollection is not completely balanced across genres, \nwhich means that nearly 70% percent of all data is re-lated to the genres Pop / Rock / Independent. \nIn total we were able to extract >190,000 files. After \neliminating duplicates and matching against the Ama-zon (Germany) product catalogue, we obtained 41,816 tagged feature sets. \nFor the purpose of performance comparison, we \nbuild a simple sound-feature-based music recommender. Similarity between two songs or two feature vectors is defined as the cosine of the two feature vectors. Before the cosine calculation, each feature was linearly scaled \nto the range [-1...1]. The underlying assumption of this straightforward implementation is that all features con-\ntribute equally to the subjective perception of music similarity and all features are equally noisy across all types of music. This assumption makes it hard to add new features from different frameworks and even if the \ncurrent features are carefu lly selected the assumption \nmay not hold. \nIn order to compare the basic sound feature recom-\nmender to an alternative a pproach we build a music \nsimilarity recommender based on hand labelled genre information. Because we matched all music titles against the product catalogue of Amazon (Germany), it seems reasonable to use their genre information. The highest level of genre information at Amazon.de divides all music into twelve top level genres, but most songs belong to more than one genre, e.g. \"The Beatles – All \n269   \n \n You Need is Love\" belongs to the genres \"Jazz&Blues\", \n\"Pop\", \"Rock\" and \"Classics\". Of course, this is not a musicological categorisation but a real life example.  \nTo create a similarity recommender based on genre \ninformation, we represent the genre information as bi-nary feature information, i.e. each genre is one dimen-sion in a song's feature vector. If the song belongs to the \ngenre, that component is set to 1, otherwise to 0. Just as \nfor the sound-feature-based music similarity recom-mender, similarity between two songs or two feature vectors is defined as the cosi ne of the feature vectors. \nAs a testing baseline we employed a similarity en-\ngine, with 30 real valued random features per song, with random numbers in the range [0…1]. Again, similarity between two songs or two random-feature vectors is defined as the cosine of the two feature vectors. Even if \nin theory the predictive precision of random-features should be constant in an equally distributed test set, the real world test bed shows a small increase in precision with an increase in the number of recommended items. \n \nFigure 4 : Comparison of three music similarity \nrecommendation engines based on sound fea-\ntures, Amazon (Germany) genres seen as binary \nfeatures and random features. \nAll three similarity engines were tested against \n654,506 human generated playlists. The results (preci-\nsion against number of recommendations) are shown in \nFigure 4. It can be seen, that the sound-feature-based similarity engine as well as the Amazon genre-based engine produce far better results than the random-feature based engine. More interesting is that the sound-feature-based similarity recommender outperforms the hand labelled Amazon genre-based recommender. \n4.4  Predicting the Collaborative Model \nIn this section we create a similarity measure with our \nmethod described in Section 3.2. In an evaluation we \ncompare our approach with a content-based approach. \nIn the previous sections we created a collaborative \nmodel which represents each song as a binary feature vector C \n C, in our case with 200 binary dimensions. \nAdditionally, we have used the MARSYAS framework \nto create for each song a content-based feature vector with 30 dimensions. On this basis, we can use a super-\nvised learning approach to learn a mapping from fea-\ntures \nF to the collaborative model C. After the one time training, for each new song a collaborative feature vec-\ntor C \n  C ' can be predicted using the learned mapping \nfrom F to C. \nWe have collaborative data for 176,930 songs and \n41,816 tagged content-based feature sets. The intersec-\ntion of both sets contains 36,916 songs. \nAs stated in Section 3, even if the dimensions of each \ncollaborative model vector C \n  C are not independent, \nwe have to assume that they  are independent because of \nthe lack of appropriate vector-valued learning methods. This means that we trained 200 binary support vector \nmachines using a radial basis function kernel (SVM \nwith RBF kernel) to predict each binary component C\ni \nof C \n  C independently, based on the feature vector      \nF \n F provided by MARSYAS.  \nTo evaluate our method we predicted, based on the \nsound feature vectors F, for each song of our test set data a collaborative model vector C \n C '. After this \nprediction similarities of the predicted vectors C are computed using the cosine as similarity function. The resulting similarities are tested against 301,227 human \ngenerated playlists. \n \nFigure 5 : Comparison of three music similarity \nengines: our method (top line), sound features \n(middle) and random features (bottom line).  \nOf course, it would have been preferable to present a \nten-fold cross validation instead of only one run, but the training of one SVM takes (incl. automated optimization \nof the RBF-kernel parameters) about 3.4 hours which results in 28 days for 200 SVMs. \nThe evaluation results in figure 5 show that our \nmethod (top-line) nearly doubles the precision for the 'most similar' item (TopN=1), i.e. given one song by the user, the probability that this user would add the rec-ommended song in a playlist with the given one is dou-bled by our method compared to a pure content-based approach and more than doubled compared to a recom-mender based on Amazon's genre-data (Section 4.3). \n5 CONCLUSIONS AND FUTURE WORK \nIn this paper we have presented an approach to improv-\ning purely content-based similarity measures by training a collaborative model with content-based sound features. With our approach we were able to almost double the precision of similarity-based recommendations com-\n270   \n \n pared to a content-based appro ach when tested in a real-\nlife test bed against human generated playlists. In addi-\ntion to that, our approach is open to any progress in the area of content-based feature extraction. The more de-scriptive new sound features are, the better the result should be. \nOn the collaborative side, we created a collaborative \nrecommender from a large scale dataset (18,769,469 \ntagged data points). This recommender and two ver-sions, trained with fewer data, have been tested by users on the web. The users ranked their satisfaction with the recommenders according to th e offline predicted F1 \nmeasure and precision of the recommender, which means that the test bed seems to reflect reality. \nWe conducted an experiment to build a similarity \nmeasure based on the genre data of Amazon (Germany). The evaluation results of the Amazon genre-based simi-larity engine were outperformed by the similarities gained through the pure sound-feature-based approach. It emerges that an approach based on Amazon’s genres is not suitable.  \n \nFigure 6:  Performance comparison of 3 music \nsimilarity recommenders: pure collaborative ap-\nproach (top line), our approach (middle), sound-\nfeature-based (bottom line). \nFig. 6 shows that our method has much potential for \nimprovement. The theoretical maximum is the top line \nin figure 6. One way to improve the performance of our \nmethod could be to use more descriptive sound features, another one is to build a better collaborative model. A third way to improve the prediction rate is to find ma-chine learning approaches which exploit the interde-\npendence of collaborative model vector components. \nSimulations showed that if we could raise our current \naverage SVM prediction rate  from 73.1% correctly pre-\ndicted items to 95%, we could raise the precision for the 'most similar item' (one recommendation) in our method from 0.0108 to 0.04, compared to 0.0054 with a purely sound feature-based approach.  \nREFERENCES \n[1] George Tzanetakis and Perry Cook: \"Musical Genre \nClassification of Audio Si gnals\" IEEE Transactions \non Speech and Audio Processing, 10(5),  2002 \n[2] D. Pye, \"Content-Based Methods for Managing Electronic Music\". In  Proc ICASSP 2000, 2000 [3] T. Lambrou, P. Kudumakis, M. Sandler, R. Speller \nand A. Linney, \"Classification of Audio Signals using Statistical Features on Time and Wavelet Transform Domains\". In Proc ICASSP 1998, 1998 \n[4] G. Linden, B. Smith, \"Amazon.com Recommendations: Item-to-Item Collaborative Filtering\", in IEEE Internet Computing, pp. 76-80, Jeremy York Publication, January 2003  \n[5] R. Burke, \"Hybrid Recommender Systems: Survey and Experiment\", User M odeling and User-Adapted \nInteraction 12(4): 331-370; Nov 2002 \n[6] A. Berenzweig, B. Logan, D. Ellis, B. Whitman, \"A large-scale evaluation of acoustic and subjective music similarity measures\", Proc. ISMIR 2003, 2003  \n[7] W. W. Cohen, W. Fan, \"Web-collaborative filtering: recommending music by crawling the web\". WWW9 / Computer Networks, 33(1-6):685–698, 2000.  \n[8] J. T. Foote. \"Content-based retrieval of music and audio.\"In C.-C. J. Kuo et al., editor, Multimedia Storage and Archiving Systems II, Proc. of SPIE, Vol. 3229, pp. 138-147, 1997 \n[9] Matt Welsh, Nikita Borisov, Jason Hill, Rob von Behren, and Alec Woo: \"Querying Large Collections of Music for Similarity\",  UC Berkeley Technical \nReport UCB/CSD-00-1096, 1999 \n[10] B. Logan, \"Content-based playlist generation: \nExploratory experiments'', Proc. ISMIR 2002, pp. 295-296 (2002) \n[11] N. Good, J. Schafer, J. Konstan, A. Borchers, B. \nSarwar, J. Herlocker and J. Riedl, \"Combining collaborative Filtering with personal agents for better recommendations\" In Proceed ings of the Sixteenth \nNational Conference on Artificial Intelligence, 1999 \n[12] D. P. Ellis, B. Whitman, A. Berenzweig and S, \nLawrence, \"The quest for ground truth in musical artist similarity\", Proc. ISMIR 2002, 2002 \n[13] P. Knees, E. Pampalk, and G. Widmer, \"Artist \nClassification with Web-based Data\", Proc. ISMIR 2004, pp 517-524, (2004) \n[14] B. Whitman, S. Lawrence \"Inferring descriptions \nand similarity for music from community metadata\", in Proc. ICMC 2002, 2002 \n[15] R. Duda, P. Hart, and D. Storck. \"Pattern \nclassification\" John Wiley & Sons, New York, 2000.  \n[16] I. H. Witten, E. Frank: \"Data Mining: Practical \nMachine Learning Tools and Techniques with Java \nImplementations\", Morgan Kaufmann, 1999 \n[17] C. A. Micchelli, M. Pon til, \"On Learning Vector-\nValued Functions\" in Neural Computation, MIT Press, vol. 17, no. 1, pp. 177-204(28), 2005 \n \n271"
    },
    {
        "title": "Collecting Ground Truth Annotations for Drum Detection in Polyphonic Music.",
        "author": [
            "Koen Tanghe",
            "Micheline Lesaffre",
            "Sven Degroeve",
            "Marc Leman",
            "Bernard De Baets",
            "Jean-Pierre Martens"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417715",
        "url": "https://doi.org/10.5281/zenodo.1417715",
        "ee": "https://zenodo.org/records/1417715/files/TangheLDLBM05.pdf",
        "abstract": "In order to train and test algorithms that can automatically detect drum events in polyphonic music, ground truth data is needed. This paper describes a setup used for gathering manual annotations for 49 real-world music fragments containing different drum event types. Apart from the drum events, the beat was also annotated. The annotators were experienced drummers or percussionists. This paper is primarily aimed towards other drum detection researchers, but might also be of interest to others dealing with automatic music analysis, manual annotation and data gathering. Its purpose is threefold: providing annotation data for algorithm training and evaluation, describing a practical way of setting up a drum annotation task, and reporting issues that came up during the annotation sessions while at the same time providing some thoughts on important points that could be taken into account when setting up similar tasks in the future.",
        "zenodo_id": 1417715,
        "dblp_key": "conf/ismir/TangheLDLBM05",
        "keywords": [
            "ground truth data",
            "manual annotations",
            "polyphonic music",
            "drum events",
            "beat annotation",
            "experienced drummers",
            "automatic music analysis",
            "data gathering",
            "algorithm training",
            "evaluation"
        ],
        "content": "COLLECTING GROUND TRUTH ANNOTATIONS FOR DRUM \nDETECTION IN POLYPHONIC MUSIC\nKoen Tanghe1, Micheline Lesaffre1, Sven Degroeve2,  \nMarc Leman1, Bernard De Baets2 and Jean-Pierre Martens4 \n1 IPEM, Department of Musicology, Ghent University, Blandijnberg 2, 9000 Ghent, Belgium \nKoen.Tanghe@UGent.be \n2 Department of Applied Mathematics, Biometrics and Process Control, Ghent University \n4 Department of Electronics and Information Systems, Ghent University \n \nABSTRACT \nIn order to train and test algorithms that can automati-\ncally detect drum events in polyphonic music, ground truth data is needed. This paper describes a setup used for gathering manual annotations for 49 real-world mu-sic fragments containing different drum event types. Apart from the drum events, the beat was also anno-tated. The annotators were experienced drummers or \npercussionists. This paper is primarily aimed towards \nother drum detection researchers, but might also be of interest to others dealing with automatic music analysis, manual annotation and data gathering. Its purpose is threefold: providing annotation data for algorithm train-ing and evaluation, describing a practical way of setting up a drum annotation task, and reporting issues that came up during the annotation sessions while at the same time providing some thoughts on important points that could be taken into account when setting up similar \ntasks in the future. \n \nKeywords: drum detection, annotation, data gathering  \n1 INTRODUCTION \nDrum events provide important clues about the \nrhythmical organisation of a musical piece. For many \nmusic genres nowadays, rhythmic structures have be-\ncome (at least) equally important as melodic or tonal structures. In the same way that melody lines can be seen as a representation of one aspect of the musical content of a piece of music, drum sequences can be seen \nas another type of musical content representation, but \nmore related to rhythm. This information could be used to allow people to search for a particular drum sequence or a typical drum pattern, for automatically classifying music pieces into different ge nres or subgenres, or for \nobtaining information about tempo and metrical struc-ture. Research on drum detection is relatively new (com-\npared to melody-related research) and there is a real need for reference material to  train and test drum detec-\ntion algorithms. One of the interesting initiatives for which this type of reference material is important is the MIREX “contest” track [1] of the annual ISMIR confer-ence [2]. The goal of this “c ontest” is to compare state-\nof-the-art algorithms and systems relevant for Music Information Retrieval (MIR), and ground truth data is needed in order to perform objective evaluations. Some efforts towards setting up rea listic databases of refer-\nence music and annotations have already been made (e.g. the Real World Com puting (RWC) Music Data-\nbase [3]), and some people have also started developing specialized annotation tools to ease the cumbersome task of performing manual annotations (e.g. the Sound Onset Labelizer in [4] or the semi-automatic beat anno-tation tool in [5]). In contra st to the speech analysis re-\nsearch field, however, the music analysis field in gen-\neral does not (yet?) have flexible music annotation tools or large sets of real-world annotated data that can serve as reference material and thus  as a catalyst for faster and \nhigher-quality algorithm development. We believe that research projects dedicated solely to data gathering and preparation will be needed in  the near future. We hope \nthat the results of our little annotation task may be a valuable contribution in that sense. \n2 CONTEXT \nIn the context of the Musical Audio Mining (MAMI) \nproject [6], research is being done on extracting drum events from polyphonic music. Algorithms have been designed and software has been developed to localize and label drum events, based on a model consisting of three main parts: onset detection, feature extraction and feature vector classification. \nThere are a few reasons why we need realistic \nground truth data for this drum detection system. The \nfirst reason is that it contains a machine learning algo-rithm for the classification part  that needs to be trained \nin a supervised way on data that represents the true task of drum detection before it can actually do anything useful. So, the labelled data is used to train the drum type classification models. \nAnother reason why we need the ground truth data is \nparameter optimization. The components of the system typically have a certain number of parameters that all have some influence on the performance of the overall system. In order to obtain a parameter combination that \nPermission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for profit or com-\nmercial advantage and that copies bear this notice and the full citation on the first page. \n© 2005 Queen Mary, University of London \n50   \n \n leads to a good overall performance of the system, it is \nnecessary to optimize the various parameters over a set of realistic music examples. This optimization process requires minimizing some cost function that indicates how bad the system performs. And the best way to ob-tain a “badness indication” is  to compare the detected \ndrum events with the “correct” drum events (ground \ntruth data) and to calculate a quantitative error measure \nfrom the comparison results. \nFinally, as already stated in the introduction, ground \ntruth data is very useful for comparing various drum detection systems against each  other in a systematic \nway. That gives us an objective indication of how well our trained and optimized system performs compared to other systems, which is interesting in se, but it also tells us (and the designers of the other systems) something about the strong and weak points of the used algorithms. \n3 DATA AND PEOPLE \n3.1 Music \nOur goal is to make the drum detection system work on \n“real music”. We are not interested in making it work for \na few carefully selected test  examples in a laboratory \nenvironment. Of course, work ing with special cases or \nselected examples can be useful for evaluating some \nspecific aspects of the various  parts of the system. Also, \nworking with MIDI files can be an interesting first step \nwhen quickly trying out and comparing various algo-rithms. But the end goal is still drum detection on fully produced music (both recorded and sequenced) from different popular genres. \nWe collected a set of 52 music fragments digitally \nextracted from various commercial music CD’s. The fragments are 30 seconds long and in 16 bit 44100 Hz stereo PCM WAV format. In selecting the music, we have tried to make a compromise between diversity and annotator preferences. Having as much different styles and genres as possible is important to evaluate our algo-rithms for robustness and flexibility, but making sure that our annotators are familia r with the music is also \nimportant because it usually leads to more reliable anno-tations. Therefore, prior to actually setting up the anno-tation task, we had asked the annotation candidates to answer a short list of questions about (amongst others) the genres and styles of music they are most familiar with. Since most of our candidates also took part in a separate online inquiry aime d at recruiting a large group \nof subjects willing to partic ipate in diverse annotation \nexperiments [7], we have also taken into account the music they had then specified as being their “favourite”. A full list of the music we have used can be found at the end of this paper in Table 3. \nApart from this set of “real music” fragments, we \nalso added three other fragments: a very simple, self-made reference file containing clear drums of various types and no music, and two recordings of MIDI files with drums and music. These fragments were meant as “reference” material to get an idea about the annotation \nquality. Since we generated these audio fragments from a symbolic representation, it is possible to compare the manual annotations with the true events. \n3.2 Annotations \nThe goal of the annotation task was to come up with \nreliable ground truth data that represents the positions in the sound files where  drum events occur, together with \nlabels specifying which  types of drum events are occur-\nring at these locations. Unlike monophonic melody lines, drum events can overlap in time, so for a specific posi-\ntion, multiple drum events can occur at the same time. The 18 types of drum events we have considered are \nlisted in Table 1. While we initially planned to use only 6 types of drum events (BD, SD, HH, CY, TM and “other”), feedback from the annotators during prelimi-nary tests showed that some other drum types were con-sidered as being important nonetheless. And since these other types would make no big difference in the annota-tion task itself, we decided to go for a more elaborated list of drum types. For our current algorithms, we can still reduce these full annotations by remapping or omit-ting some drum types to only a set of very basic drum types. For future versions that might be able to deal with more drum types, we can then use the full annotations. The annotations were stored in a MIDI file where a MIDI “note on” message per annotated drum event en-\ncodes both the position and drum type. \nSince we are also interested  in tempo detection for a \nlater stage of our research, we asked the annotators to \ntap along with the beat of each music fragment. This information is again stored in a MIDI file where each \nbeat is represented by a MIDI “note on” message with MIDI note number 76. \nTable 1.  Overview of the annotated drum types \nwith their labels and MIDI note numbers. \nFull name Label Note \nbass drum BD 36 \nsnare drum SD 40 \nopen hi-hat OH 46 \nclosed hi-hat CH 42 \nride cymbal RC 59 \ncrash cymbal CC 57 \nlow tom LT 45 \nmid tom MT 47 \nhigh tom HT 50 \nclaps CP 39 \nrim shot RS 37 \nsplash cymbal SC 55 \nshaker SH 70 \ntambourine TB 54 \nwood block WB 77 \nlow conga LC 64 \nhigh conga HC 63 \ncow bell CB 56 \nother drum -D 75 \n51   \n \n 3.3 Annotators \nAnnotation of the different fragments was performed by \na team of 10 experienced male1 drummers and percus-\nsionists varying in age from 23 to 57 (half of them were 25 or younger though). Most of the annotators were selected from the pool of participants in the online in-quiry mentioned in section 3.1. We chose the ones who indicated that they play a percussion instrument and who thought of themselves as having a high level of \nmusicality. Some were also recruited directly through \nother connections because  they were known as good \ndrummers. They were all volunteers but were nonethe-less financially compensated for their time and efforts. We did not want to “use” st udents for the task because \nwe think it requires a more than average familiarity with drum sounds and a lot of true motivation to perform the task rigorously. \nAs already mentioned, the annotators had been asked \nin advance to answer a short list of questions about the music styles/genres they are most familiar with. Other questions were related to their acquaintance with se-quencer software/hardware, the (in their opinion) rela-tive importance of various drum sound types and the appropriateness of different methods for entering a drum sequence into a comput er. All these answers have \nbeen taken into account wh ile setting up the annotation \ntask. \nFinally, all annotators signed a statement saying that \nthey give us permission to freely use the gathered data for research purposes. \n4 ANNOTATION METHOD \n4.1 Setup \nAfter a short evaluation of a few available candidate \nannotation tools, we decided to use a standard music \nproduction software package with multi-track audio and \nMIDI sequencing capabilities, namely Cakewalk Sonar2. \nA summary of the advantages and disadvantages of the considered programs is given in Table 2. \nEach annotator was presented with a Sonar multi-\ntrack project consisting of 1 audio track for the music fragments, 1 MIDI drum track for the drum annotations, and 1 MIDI drum track for the beat annotations (see \nFigure 1). The music fragments were placed after each \nother on the audio track (usually between 4 and 8 frag-ments per annotation session) with a gap of 5 seconds of silence in between. The top ha lf of the screen showed \nthe three tracks and standard  controls, and the bottom \nhalf showed a drum grid view of either the drum track or the beat track, depending on which one was selected. Navigation was done by the mouse and by special key-board shortcuts (e.g. for jumping to the start of a frag-ment). For starting and stopping playback and re-\n                                                          \n \n1 This was not a deliberate choice. We just didn’t find female drummers \nor percussionists who wanted to pa rticipate in the annotation task. \n2 Sonar is a registered trademark of Twelve Tone Systems, Inc. cording, the standard transport buttons (and the corre-\nsponding keyboard shortcuts) could be used. Snapping to a grid and quantization were turned off completely, and time was displayed in seconds (we didn’t want to force the tempo track of the project to match the tempo of the fragments to avoid biased timings). \nTable 2.  Advantages and disadvantages of a few \ncandidate annotation tools \nCakewalk Sonar \n+ easy-to-use interface \n+ allows you to listen to your annotations and also to \na mix of the original and your annotations \n+ many musicians already know how to use this type \nof program \n+ possibility of using a MIDI input device \n+ synchronization between audio and annotations \n(MIDI) is automatic \n+ avoids annotation errors ( no values or data entering \nin a difficult GUI as in Praat) \n- no possibility to see audi o and annota tions (audio \nand MIDI track) in the same edit window \nPraat \n+ audio and annotations in the same window \n+ easier to add complicated annotations (reliability \nscores etc…) compared to using MIDI CC’s \n- no possibility to listen to your annotations (!) \n- unknown by annotators, not focused on music \n- user interface is not so eas y to use, editing is a bit \ndifficult for music annotations \n- potential for making typing errors, when typing in \nthe occurring drum types is needed \nAudacity \n+ has potential for the future: open source C++ code, \npossibility to extend with own code \n- mark tracks are not fully worked out (almost no \nediting possibilities) \n The MIDI tracks were connected to a virtual sound \nmodule, which means that the annotators could actually listen to their annotated events by playing back the an-notation track through the sound module. Of course, this could also be done while the original music was playing back at the same time. Volu me controls for each track \ncould then be used to setup the right balance between \nboth signals, and stereo panning could be used to play-back the original sound in one channel (left) and the sound of the annotated events in the other (right). The audio track was also supplied with a filter that could be switched to low-pass, band-pass or high-pass mode in order to make it easier to listen to specific spectral re-gions of the music (e.g. low-pass filtering to ease the annotation of the bass drum events). \nWe also attached a “tweak ed” MIDI keyboard to the \nannotation computer as an alternative to entering all the drum events one by one into the drum grid view. The MIDI keyboard was “tweaked ” in the sense that we \nsticked drum labels on a range of white MIDI keys and inserted a strip of insulation material underneath these \n52   \n \n \n \nFigure 1.  A multi-track sequencer drum and beat annotation setup in Cakewalk Sonar 2.2 \n \nkeys to limit their downward movement and make it \neasier to play fast drum sequences. \nOf course, a combination of visual editing and MIDI \nkeyboard recording was possible too. Usually, the \ndrummer would start the annotation task by recording a few drum parts played on the MIDI keyboard while listening to the original music. This recording process would then be repeated for a few other drum parts, until the most important drums were registered. Then, the drummer would start adjusting or extending the re-corded annotations by moving, adding or deleting drum events in the drum grid view using the graphical inter-face. At any time it was possi ble to playback the anno-\ntated events together with the original music in order to verify the accuracy of the annotations. \nAs for the physical setup: the annotation task was set \nup in a standard office room with an old DX7 MIDI keyboard connected through a Midisport2x2 USB MIDI I/O device to a WindowsXP PC running Cakewalk So-nar 2.2. The used soundcard was a low-latency M-Audio Delta, the sound module for playing back the drums was an Edirol Virtual Sound Canvas DXi shipped with Sonar, and the annotators were wearing medium-quality headphones. 4.2 Guidelines \nIn order to make sure that all annotators fully understood the purpose of the task and how the offered setup could be used to complete it, we  prepared a small document \nwith guidelines that was verbally presented to them by one of the organizers and was left at their disposal in the annotation room as a reference. \nAt the one hand, this document served as a practical \nuser guide for the annotation setup: it included a screen-shot of the Sonar user interface with a description of the most important views and controls and how to interact with them. Of course, one of the organizers was always available in case there were questions or problems. \nOn the other hand and maybe more importantly, \ngiven the fact that we were working with different anno-tators, these guidelines also contained a few more se-mantics-related thoughts on how to annotate the music in a consistent way. For example, it was stressed that although the specific sound  of the played back drum \nsamples may differ from the ones found in the original music fragment, it is nonetheless very important to choose the correct drum type. Also, if the music frag-\n53   \n \n ment contains a drum type that is not contained in the \ngiven list of drum types, and the annotator thinks it is nonetheless very important, he should annotate the drum as “other drum”. However, the annotators were asked to use the given list of drum types as much as possible. For example: if a sound fragment contains two types of crash cymbals, they should annotate both types with the \nsame label for “crash cymbal”, instead of using “other \ndrum” for one of the two types. \nAnother point we wanted to emphasize, is that both \nthe timing (exact location) and the labelling (choice of the drum types) are equally important. We prefer to col-lect fewer annotations of  good quality than more anno-\ntations that are hardly usable as ground truth. \nFinally, we encouraged the annotators to write down \nany thoughts, remarks, suggestions and problems that came up during the annotation session. \n5 RESULTS AND DISCUSSION \n5.1 Revision of initial goals \nInitially, we built up each annot ation setup out of 3 ref-\nerence fragments and 4 real music fragments, but we \ndecided to omit one of the reference fragments so that \nthere was more time left for the real fragments. An an-notation session lasted roughly one morning or after-noon (about 4 hours, depending on the difficulty of the fragments).  Annotators who came back for a second or \nthird annotation session could immediately start with the real music fragments. \nWe also intended to have each fragment cross-\nannotated by three different annotators so that we could check the inter-subject consistency of the annotations, but since obtaining a single reliable annotation for each \nfragment was already hard enough to accomplish, this idea had to be dropped in the end. Only the self-made reference file and one of the two recordings of the MIDI reference files have been cross-annotated by 9 different annotators. At the time of writing, however, a detailed analysis of the inter-annotator agreement for these ref-erence fragments has not been performed yet. \n5.2 Difficult drum types and articulations \nFrom the remarks and suggestions of the annotators, it \nwas clear that some drum types and articulations are very difficult to annotate. Brushes for example have a typical “dragged” sound which is hard to annotate as a single percussive event. In this case most annotators chose to register the accents of the brush sounds. Snare rolls do consist of a series of discernable percussive onsets, but it’s very hard to annotate the many fast strokes accurately. The same is true for “flammed” drums (typically the snare dr um) where two hits of the \nsame drum type are deliberat ely played almost (but not \nquite) at the same time, leading to the sensation of a ghost note occurring slightly before a main note. The difficulties with hi-hats on the other hand have more to do with the different ways in which the two metal discs of this instrument can be controlled while playing. After evaluating the answers to the questions we had asked the annotators in advance, we had already decided to include both closed and open hi-hats, but a few annota-tors reported that half-open hi-hats and the typical sound of an open hi-hat closed by the foot pedal should also be included in our list of drum types. One annotator reported that he found it difficult at some point to decide whether he should annotate a series of drum sounds as hi-hat or as shaker events, while another one had some doubts about a particular sequence where it was unclear to him whether the percussive events he was hearing were generated by a strummed rhythm guitar or by a hi-\nhat. A similar type of confusion was also reported for fragments that were heavily post-processed with audio effects. Reverb typically smears out sudden events in the audio and filtering can alter the original timbre of an instrument to a degree wher e it becomes very difficult \nto keep recognizing it. Also, dynamics processing on the bass frequencies of a song can sometimes make it hard to perceive the bass drum and the bass lead as separate entities. \n5.3 Use of multi-track software \nApart from the recurring request to provide a count-in \nsound at the start of the fragments, most annotators got around surprisingly well with our sequencer-based \nsetup, even those who were not really acquainted with \nmulti-track sequencer software. There were nonetheless a few types of mistakes that were caused by non-optimal use of the software. There were a couple of double events, probably due to re-recording a drum se-quence played on the keyboard without deleting the already registered events, and some events had been \nrecorded on both the drum track and the beat track, which is typically caused by forgetting to switch on or off the “arm for recording” button for the appropriate MIDI track. \nOnly one annotator clearly had troubles understand-\ning how to use the setup. Apart from the sporadic errors already mentioned above, he had somehow shifted all audio fragments in time which led to big synchroniza-tion problems with his annotated events. He had also clearly copied/pasted parts of his annotations which resulted in many incorrectly annotated events, and he did not annotate the beat at all. Furthermore, he had reported that “there was too much info on the screen for \na beginner”, which kind of makes sense, but funnily \nenough he had also asked if it was possible to “remove some instruments from the audio mix so that it would be   easier to hear the drum sounds one by one”, which is of course precisely what we are trying to do with our automatic drum detection research. This person also complained that the drum grid view did not have any subdivision in measures or beats, which was a deliberate choice to avoid biased timings. All of this shows that the clearly explained purpose of the annotation task was not fully understood. These annotations were totally \n54   \n \n unusable as ground truth, so we had to ask another an-\nnotator to come back and annotate these files correctly. \n5.4 Use of a MIDI keyboard interface \nOur setup with a tweaked MI DI keyboard seemed to \nwork sufficiently well for most drummers, although \nsome problems did exist. In particular, we had a request to use more than one single MIDI key for each drum event, as this would facilitate the annotation of fast drum sound sequences by alternately pressing the two MIDI keys. There were also a few remarks about it be-ing too hard to annotate complex sequences using a MIDI keyboard. This came up when drummers wanted to play in more than 2 drum parts using the keyboard. \nIt may be useful at this point to mention that we had \nalso done some experiments using an electronic drum kit with MIDI output, in order to stay as close as possi-ble to the natural way of drumming, but this didn’t work out as expected. Many drum parts can not be played on a standard electronic drum kit alone, and require a broader range of drum types. Also, it’s harder (physi-cally) to switch between live playing on the drums and visual editing on the computer, which is very important in order to be able to evaluate, correct and overdub the played drum sequences easily. We also noticed that when playing live on an el ectronic kit, the drummer \ntends to start playing along with the music without actu-ally making sure to play exactly what is being played in the music, which is of course very important for this task. Apart from that, there we re also technical reasons \nwhy we decided not to continue with an electronic drum kit: sometimes drum events were “double triggered” or there was cross-talk between the different drum brain sensors (e.g. a tom sensor being triggered slightly when a hard snare is played). At some point, we had to com-pletely disconnect a foot pedal because, although not \nexplicitly being played at all, it was still being triggered by the arm and body movements propagating through the legs of the drummer while playing. These technical issues introduced ghost notes, which required manual ad hoc checking and correction, so it turned out that our intentions to use an electronic drum kit to avoid these manual post-processing efforts afterwards were not very realistic. Together with the higher setup cost and the disadvantages already mentioned above, we eventually felt that using an electronic drum kit would not really help us much for this task. \n5.5 Evaluation and corrections \nAfter all annotations had been gathered, we manually \nwent through each and every one  of them to check their \nquality and select the best ones. Occurring mistakes ranged from double events (usually due to overdubs) or \nsingle missing events, to spurious events caused by copy/paste actions without checking if they are really accurate at the pasted loca tion. The beginning and end \nof a fragment were most sus ceptible to mistakes. At the \nbeginning of a fragment timing was sometimes a bit jittery because it is difficult to start with the correct \ntempo immediately (hence the suggestion of adding a \ncount-in at the start), whereas near the end, annotators may start feeling overconfident that the same is being repeated all the time, which may make them miss subtle changes that do occur and act ually indicate semantically \ninteresting moments (like the end of a measure or a \nshort pause). This is somehow related to the already \nmentioned danger of starting to play along instead of focussing on doing an accurate transcription. There were also a few cases where similar drum event types were annotated with different labels (like two different snare drums) or where incorrect drum types had been used in order to make a difference between different “other” drum types (like udu, tabla and djembe). This “trying to make a distinction by using another (inappro-priate) label” mistake also occurred for a few pitched drum types (two woodblocks with a different pitch, or more than 3 types of toms). As for the beat, a recurring problem was the missing or jittery beats near the start of a fragment, which is related to the “settling time” we need in order to lock on to a periodicity in the beat-range. Most drummers handled this by extrapolating the beat towards the start of the fragment, but the first beats \nare usually a bit jittery nonethel ess. The beat events also \nsometimes ended up on the drum events track instead of the beat track. This has to do with forgetting to toggle the recording status of the different tracks, and might be avoided in the future by just using a single MIDI track and a special note number for the beat, although this may introduce a bit more work afterwards to split up the track in drum and beat events . In any case, only the very \nobvious mistakes were corrected (like a clearly missed cymbal, an extra bass drum where there is none, exces-sive timing jitter, events on wrong tracks, …) in order to keep the annotations as “true” as possible. \nFinally, we should mention that we had to eliminate 3 \nfragments from the original  52 because the annotators \n(remember: experienced drummers and percussionists) considered them impossible or too time-consuming to annotate. One of these fragments was an electronic mu-sic fragment with dense layers of drums in a complex rhythmical structure; anot her one was a jazz fragment \nwith hard to follow drum sequences with lots of in-between hits and lush cymbals and hi-hats, and the third one was a fragment in which the hi-hats were very diffi-cult to hear and in which lots of lower dynamics drum rolls occurred. \n6 CONCLUSIONS \nIn this paper, we presented a method for collecting \nground truth data for drum detection in polyphonic mu-\nsic. This method has been used by a group of 10 experi-enced drummers and percussionists for locating and labelling 18 types of drum events in 49 polyphonic mu-sic fragments from different popular genres. The beat for these fragments has been annotated as well. The \nobtained data can be found on the MAMI web site un-\n55   \n \n der “Test collections and annotation material” in the \npublic section of the site [6]. \nBeing able to see the events and the audio at the same \ntime, together with the possibility to actually hear the annotated events proved to be essential for the annota-tion process. Although we have managed to use a stan-dard multi-track software p ackage for this annotation \ntask, we do believe there is a need for easy-to-use and \nflexible tools dedicated to music annotation. Apart from the already mentioned aural a nd visual feedback, things \nlike multi-layer and hierarch ically structured annota-\ntions, connectivity to extern al user interfaces, slowdown \nof fast audio sequences, flexible input and output for-mats are all features that were found to be important for gathering reliable and well-structured music annotations in a fast and comfortable way. \nFinally, the choice of audio fragments should be \ncarefully considered. For this annotation task, we had decided to use “real music” from commercial CD’s, but the problem with that is that we cannot distribute the music itself along with the annotation data due to copy-right restrictions: we can only provide a documentation file with all the info needed to obtain the exact same fragment we used for our annotations. In the future, we will consider using music released under a more open license like the Creative Commons “attribution, non-commercial, share a like” license [8]. \nACKNOWLEDGEMENTS \nThis work was done in the context of the “Musical \nAudio Mining” (MAMI) project, which is funded by the \nFlemish Institute for the Promotion of Scientific and Technological Research in Industry. \nThe authors wish to thank Liesbeth De Voogdt and \nDirk Van Steelant for their help with the annotator guidelines, music selection and practical organization. REFERENCES \n[1] Music Information Retrieval Evaluation eXchange (MIREX), http://www.music-ir.org/mirexwiki  \n[2] International Conference on Music Information Retrieval (ISMIR), http://www.ismir.net  \n[3] Goto, M. “Developmen t of the RWC Music \nDatabase”, Proceedings of the 18\nth International \nCongress on Acoustics (ICA 2004), pp. I-553-556, April 2004 \n[4] Leveau P., Daudet L. and Richard G., “Methodology and Tools for the evaluation of automatic onset detection algorithms in music”, Proceedings of the 5\nth International Symposium on Music Information \nRetrieval (ISMIR 2004), pp. 72-75, Barcelona, Spain, 2004  \n[5] Gouyon F., Wack N. and Dixon S., “An open source tool for semi-automatic rhythmic annotation”, Proceedings of the 7\nth International Conference on \nDigital Audio Effects (DAFx 2004), Naples, Italy, 2004  \n[6] Musical Audio Mining (MAMI), Ghent University, Belgium, http://www.ipem.ugent.be/MAMI  \n[7] Lesaffre M., Leman M., De Baets B. and Martens J.-P., “Methodological considerations concerning \nmanual annotation of musical audio in function of algorithm development”, Proceedings of the 5\nth \nInternational Conference on Music Information \nRetrieval (ISMIR 2004), pp. 64-71, Barcelona, Spain, 2004 \n[8] Creative Commons, http://creativecommons.org  \n \n \nTable 3.  Overview of the music we used for the annotation task. For the fragments marked with an asterisk, no \nreliable annotations could be obtaine d. More detailed information (incl uding CD identification numbers and \nstart and end times of the fragments) can be found on the MAMI project web site. \nTitle Performer \nAchterbank De nieuwe Snaar \nAfro-Left Leftfield \nAhmad's Blues Ahmad Jamal Trio \nAngel Massive Attack \nBard Dance Enya \nBillie Jean Michael Jackson \nBoom Boom John Lee Hooker \nBoth Sides of the Story Phil Collins \nBusiness Eminem \nBusted * Johnny Cash \nCaroline Hard-Core Ecstasy Frank Zappa, Captain B eefheart, George Duke, Napoleon Murphy Brock, \nBruce Fowler,… \nChicken Walk The Jon Spencer Blues Explosion \nCold Water Tom Waits \n56   \n \n Dans La Spirale Starflam \nDejamer ser mujer Axelle Red \nDromen zijn bedrog Marco Borsato \nHave a cigar Pink Floyd \nHaw 16 Horsepower \nHighway to Hell AC/DC \nIn Bloom Nirvana \nJoin Hands Laurent Garnier \nJumbo Underworld \nLand of… St Germain \nLeave Home Chemical Brothers \nLinks 2,3,4 Rammstein \nLooking Through the Eye of a Pig Cypress Hill \nMalegria Manu Chao \nMarilou Reggae Serge Gainsbourg \nMarket Daze Nitin Sawhney \nMeisjes Raymond van het Groenewoud \nMolten Universe Kuyss \nMy World Metallica \nNooit met krijt Kadril \nPoofter's Froth Wyoming Plans \nAhead Frank Zappa, Captain Beefheart, Geor ge Duke, Napoleon Murphy Brock, \nBruce Fowler,… \nPrison Shoe Romp 16 Horsepower \nMiserlou Dick Dale & His Del-Tones \nQueremoz Paz Gotan Project \nRocket 88 The Jimmy Cotton Blues Quartet \nSay What You Say Eminem \nSink To The Bottom Fountains of Wayne \nSorte Gal Costa, Gaëtano Velosa \nSt. Anger Metallica \nStinkfist Tool \nSunday Bloody Sunday U2 \nThe Box * Orbital \nThe Time Is Now Moloko \nThe Watcher Dr. Dre \nTriptico Gotan Project \nWaterloo ABBA \nWe Speak * Booker Litttle, Eric Dolphy, Julian Priester, Max Roach, … \nWhen It Sings Elvis Costello \nYellow Coldplay \n \n57"
    },
    {
        "title": "Classification of Musical Metre with Autocorrelation and Discriminant Functions.",
        "author": [
            "Petri Toiviainen",
            "Tuomas Eerola"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416040",
        "url": "https://doi.org/10.5281/zenodo.1416040",
        "ee": "https://zenodo.org/records/1416040/files/ToiviainenE05.pdf",
        "abstract": "The performance of autocorrelation-based metre induction was tested with two large collections of folk melodies, consisting of approximately 13,000 melodies in MIDI file format, for which the correct metres were available. The analysis included a number of melodic accents assumed to contribute to metric structure. The performance was measured by the proportion of melodies whose metre was correctly classified by Multiple Discriminant Analysis. Overall, the method predicted notated metre with an accuracy of 75 % for classification into nine categories of metre. The most frequent confusions were made within the groups of duple and triple/compound metres, whereas confusions across these groups where significantly less frequent. In addition to note onset locations and note durations, Thomassen's melodic accent was found to be an important predictor of notated metre. Keywords: Metre, classification, autocorrelation 1 INTRODUCTION Most music is organized to contain temporal periodicities that evoke a percept of regularly occurring pulses, or beats. The period of the most salient pulse is typically within the range of 400 to 900 ms [1-3]. The perceived pulses are often hierarchically organized and consist of at least two simultaneous levels, whose periods have an integer ratio. This gives rise to a percept of regularly alternating strong and weak beats, a phenomenon referred to as metre [4,5]. In Western music, the ratio of the pulse lengths is usually limited to 1:2 (duple metre) and 1:3 (triple metre).  Metre in which each beat has three subdivisions, such as 6/8 or 9/8, is referred to as compound metre. A number of computational models have been developed for the extraction of the basic pulse from music. Modelling of metre perception has, however, received less attention. Large and Kolen [6] presented a model of metre perception based on resonating oscillators. Toiviainen [7] presented a model of competing subharmonic oscillators for determining the metre (duple vs. triple) from an acoustical representation of music. Brown [8] proposed a method for determining the metre of musical scores by applying autocorrelation to a temporal function consisting of impulses at each tone onset whose heights are weighted by the respective tone durations. A shortcoming of Brown's study [8] is that it fails to provide any explicit criteria for the determination of metre from the autocorrelation function. Frieler [9] presents a model based on autocorrelation of gaussified onsets for the determination of metre from performed MIDI files. Pikrakis, Antonopoulos, and Theodoridis [10] present a method for the extraction of music metre and tempo from raw polyphonic audio recordings based on selfsimilarity analysis of mel-frequency cepstral coefficients. When tested with a corpus of 300 recordings, the method achieved a 95 % correct classification rate. Temperley and Sleator [11] present a preference-rule model of metre-finding. An overview of models of metrical structure is provided in [12]. Although there is evidence that the pitch information present in music may affect the perception of pulse and metre [13-15], most models of pulse and metre finding developed to date rely only on note onset times and durations. Dixon and Cambouropoulos [16], however, proposed a multi-agent model for beat tracking that makes use of pitch and amplitude information. They found that including this information when determining the salience of notes significantly improved the performance of their model. Vos, van Dijk, and Schomaker [17] applied autocorrelation to the determination of metre in predominantly isochronous music. They utilized a method similar to that proposed in [8], except for using the melodic intervals between subsequent notes to represent the accent of each note. In a previous study [18], we applied discriminant function analysis to autocorrelation functions calculated from Brown's [8] impulse functions for classification of folk melodies into duple vs. triple/compound metre. Using two large folk song collections with a total of 12,368 melodies, we obtained a correct classification rate of 92 %. Furthermore, we examined whether the inclusion of different melodic accent types would improve the classification performance. By determining the components of the autocorrelation functions that were significant in the classification, we found that periodicity in note onset locations above the measure level was the most important cue for the determination of metre. Of the melodic accents included, Thomassen's [14] melodic accent provided the most reliable cues for Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2005 Queen Mary, University of London 351",
        "zenodo_id": 1416040,
        "dblp_key": "conf/ismir/ToiviainenE05",
        "keywords": [
            "metre",
            "classification",
            "autocorrelation",
            "melodic accents",
            "performance",
            "multiple discriminant analysis",
            "folk melodies",
            "midi file format",
            "duple and triple/compound metres",
            "compound metre"
        ]
    },
    {
        "title": "Query-By-Example Technique for Retrieving Cover Versions of Popular Songs with Similar Melodies.",
        "author": [
            "Wei-Ho Tsai",
            "Hung-Ming Yu",
            "Hsin-Min Wang"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415200",
        "url": "https://doi.org/10.5281/zenodo.1415200",
        "ee": "https://zenodo.org/records/1415200/files/TsaiYW05.pdf",
        "abstract": "Retrieving audio material based on audio queries is an important and challenging issue in the research field of content-based access to popular music. As part of this research field, we present a preliminary investigation into retrieving cover versions of songs specified by users. The technique enables users to listen to songs with an identical tune, but performed by different singers, in different languages, genres, and so on. The proposed system is built on a query-by-example framework, which takes a fragment of the song submitted by the user as input, and returns songs similar to the query in terms of the main melody as output. To handle the likely discrepancies, e.g., tempos, transpositions, and accompaniments between cover versions and the original song, methods are presented to remove the non-vocal portions of the song, extract the sung notes from the accompanied vocals, and compare the similarities between the sung note sequences.",
        "zenodo_id": 1415200,
        "dblp_key": "conf/ismir/TsaiYW05",
        "keywords": [
            "audio queries",
            "content-based access",
            "popular music",
            "cover versions",
            "song retrieval",
            "user input",
            "song similarity",
            "main melody",
            "non-vocal portions",
            "sung notes"
        ],
        "content": "A QUERY-BY-EXAMPLE TECHNIQUE FOR RETRIEVING COVER \nVERSIONS OF POPULAR SONGS WITH SIMILAR MELODIES\nWei-Ho Tsai Hung-Ming Yu Hsin-Min Wang \nInstitute of  Information Science, Academ ia Sinica  \nTaipei, Taiwan, Republic of China \nwesley,donny,whm@iis.sinica.edu.tw \nABSTRACT \nRetrieving audio material based on audi o queri es is an \nimportant and challenging issue i n the research fi eld of \ncontent-based access to popular music. As part of this \nresearch field, we present a preliminary in vestigation \ninto retrieving cover versi ons of songs speci fied by  users. \nThe techni que enabl es users t o listen to songs wi th an \nidentical tune, but  perform ed by  different  singers, i n \ndifferent  languages, genres, and so on. The proposed \nsystem  is built on a query-by-example fram ework, which \ntakes a fragm ent of the song subm itted by the user as \ninput, and returns songs si milar to the query  in terms of \nthe m ain melody as out put. To handl e the likely \ndiscrepanci es, e.g., t empos, t ransposi tions, and \naccom panim ents between cover versions and the original \nsong, methods are present ed to rem ove t he non-vocal  \nportions of t he song, ext ract the sung not es from  the \naccom panied vocals, and com pare the sim ilarities \nbetween t he sung not e sequences. \n \nKeyw ords: cover versi on, m ain melody, query -by-\nexam ple, accom panim ents.  \n1 INTRODUCTION \nRapid advances i n Internet connect ivity and si gnal \nprocessi ng technol ogies have l ed to a dram atic and \nunprecedented increase in th e availability of m usic \nmaterial in recent years. Ir onically, it has becom e more \nand more difficu lt to  locate d esired  item s from the \ninnum erabl e options. Thus, t echni ques t hat coul d enabl e \nusers t o quickly acqui re the music they want are being \nextensively explored to keep pace with the rapid \nproliferation of m usic material. Am ong such techni ques, \nretrieving audi o material based on audi o queri es is of \nparticular interest in the dom ain of accessing popular \nmusic. Its root  concept  of query -by-humming or query -\nby-song continues t o motivate the devel opment of m any \nprom ising sol utions for ret rieving m usic beyond the \nconvent ional text-processi ng paradi gm, such as \nallowing users t o retrieve a song by  hum ming a catchy tune without needi ng to nam e the song [1-9] , or hel ping \nusers find songs perform ed by  their favori te singers [10] , \ngenre [11] , mood [12] , etc., by  playing an excerpt  of the \nmusic as a query . In t andem  with the above solutions, \nthis stu dy presents our preliminary investigation of the \nretrieval of cover recordi ngs, whi ch aims to find songs \nwith melodies si milar to the melody of a user’s song \nquery .   \nA cover version of a song refers to a new rendition of \na song t hat was ori ginally recorded and made popul ar by \nanother artist. It is often used as a m eans to attract \naudiences who like a fam iliar song, or to increase the \npopul arity of an art ist by adapt ing a proven hi t. \nSometimes pop m usicians gai n publ icity by recordi ng a \ncover versi on that cont rasts with the ori ginal recordi ng. \nOver several  years, t housa nds upon t housands of cover \nversi ons have been recorded, som e of which are virtually \nidentical to  the original version, while some are radically \ndifferent . The onl y feature that is alm ost invariant in the \ndifferent  recordi ngs i s the main melody of t he vocal s. \nUsual ly, the most frequent  difference between a cover \nsong and t he ori ginal versi on is that they are perform ed \nby different  singers. In such cases the associated tem pos, \nornam ents, accom panim ents, etc., m ay be changed to \ncater to the taste of contem porary audiences, or to fit the \nthem e of an album . Thus, in a m usic retrieval system , it \nwoul d be useful  if a search funct ion for a si ngle song \nrendered by  different  singers  or bel onging to different  \ngenres coul d be provi ded. \nOther com mon di fferences bet ween cover versi ons \nand t he ori ginal song are t hat they may have different  \nlyrics and titles, or they ar e sung in different languages. \nIn particular, a hit song can oft en be t ranslated into \ndifferent  languages, t hereby  making it more popul ar \nworldwide. Since a translatio n is u sually n ot literal, \ncover-versi on retrieval based on the main melody woul d \nbe more feasib le th an text-based retriev al for those \nwishing to listen to a song rendered i n a di fferent  \nlanguage. In addition, it is com monplace for live \nperform ances to be recorded and then released as \nauthorized cover songs. The m ethod of cover-song \nretrieval coul d thus be appl ied to index and classify such \nundocum ented live recordi ngs. Thi s woul d also hel p \ncopy right holders det ect unaut horized or boot leg concert  \nrecordings. Permission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or\ncommercial advantage and that copies  bear this notice and the\nfull citation on the first page. \n© 2005 Queen Mary , University  of London In this work, we address t he probl em of cover-versi on \nretriev al by investigating how to determine if o ne or \nmore m usic collections cont ain sim ilar m elodies to a \nspeci fied song query . Thi s task bel ongs to the probl em \nof ret rieving polyphoni c music docum ents based on \n183   \n \n polyphoni c music queri es. In cont rast to monophoni c \nmusic, in which at most one not e is played at any given \ntime, polyphoni c music oft en cont ains m any notes that \nare p layed  simultaneously. Th us, it is d ifficu lt to extract \nthe main melody automatically from  polyphoni c music \n[13]. Due to this difficulty, a large num ber of current  \nquery -by-humming system s [1-4]  work within the \nmonophoni c dom ain, whi ch convert s a monophoni c \naudio query  into a sy mbolic form at to match a \nmonophoni c symbolic collection. Som e studies [14,15]  \nfocus on locating the m ajor them es from  a piece of \npolyphoni c sy mbolic music, i n whi ch the note \ninform ation is given as a pri ori. However, very  few \nsystems operat e in the mode of m onophoni c audi o \nqueri es on a pol yphoni c audi o collection [5,6] , or \nentirely polyphoni c audi o queri es on a pol yphoni c audi o \ncollectio n [7-9]. Th is wo rk further differs from  the \nabove systems by the need t o com pare the main melody \npresent  in the vocal s of pol yphoni c music. Thus, the \nproposed m ethods, t hough drawn from  the query -by-\nhumming paradigm, are sp ecifically tailo red to solve the \nprobl em of cover-versi on ret rieval.    \n2 METHOD OVERVIEW \nOur goal is to desi gn a sy stem that takes as i nput an \naudio query  from  a fragm ent of a song, and produces as \noutput a ranked l ist of songs t hat are si milar to the query  \nin terms of the main melody. Songs ranked hi gh are \nthen consi dered as t he cover or original versi ons of the \nsong request ed by the user. Ho wever, as cover versi ons \nmay differ si gnificantly from  the ori ginal song i n the \nway that the accom panim ents are introduced, an \narbitrary audio query  coul d cont ain non-vocal  \n(accom panim ent-only) segm ents whose melody patterns \nare not  present  in the songs request ed by  the user, or \nvice versa. To  simplify th e problem during this initial \ndevel opment stage, we assum e that a user’s query  does \nnot cont ain salient non-vocal  segm ents. \nIn general , the structure of a popul ar song can be \ndivided i nto five sect ions: 1) intro, usual ly the first 5-20 \nseconds of t he song, whi ch is simply an instrumental \nstatement of the subsequent  sect ions; 2) verse , which \ntypically co mprises th e m ain theme of the story \nrepresent ed in the song’s l yrics; 3) chorus , wh ich is \noften the heart  of a song, where t he most recogni zable \nmelody is present  and repeat ed; 4) bridge, which com es \nroughl y two-thirds into a song, where a key change, \ntempo change or new l yric is usual ly introduced to \ncreat e a sensat ion of som ething new coming next; 5) \noutro,  whi ch is often a fadi ng versi on of the chorus or \nan instrum ental restatem ent of some earlier sectio ns to \nbring the song t o a concl usion. In essence, t he verse and \nchorus cont ain the vocal s sung by  the lead si nger, while \nthe intro, bridge, and outro  are largely accom panim ents. \nSince a vast majority of popul ar songs fol low the \nstructure of “i ntro-verse-chorus-verse-chorus-bri dge-\nchorus-out ro”, we furt her assum e that a user woul d \nsubm it a fragm ent of the regi on bet ween t he intro and \nthe bridge (i f at all) of a song t o the system. Figure 1 shows a bl ock diagram  of our cover-\nversi on retrieval system, whi ch operat es in two phases:  \nindexi ng and searchi ng. The i ndexi ng phase generat es \nthe m elody description for each  of the songs (docum ents) \nin the collectio n.  It co mmences with  removal of the \nnon-vocal  segm ents longer t han two seconds1, which \nvery likely belong to the intro, bri dge, or out ro. Then, \nmain m elody extraction proceeds by converting each \nsong from  the waveform  sam ples into a sequence of \nmusical note sym bols. In the searching phase, the task is \nto det ermine whi ch of t he songs (docum ents) are \nrelevant to a m usic query . Thi s phase begi ns wi th the \nmain melody extraction, whi ch convert s the audi o query  \ninto a sequence of m usical not e sy mbols, and i s \nfollowed by compariso n of the sim ilarities b etween  the \nquery’s note sequence and each docum ent’s note \nsequence. The more si milar the docum ent’s not e \nsequence, the more relev ant the document will b e to the \nsong request ed by  the user. Then, a ranked list of the \nsimilarities b etween  the query’s sequence and the \ndocum ent’s sequence i s present ed to the user.  \n \nSong 1Non-vocal\nRemovalMain Melody\nExtractionNote\nSequence 1\nSimilarity\nComputation\n& Ranking\nRanked ListSong 2Main Melody\nExtractionNote\nSequence 2\nSong MMain Melody\nExtractionNote\nSequence MMain Melody\nExtractionAudio QueryIndexing\nPhaseSearching\nPhase\nNon-vocal\nRemoval\nNon-vocal\nRemoval\nFigure 1 . The proposed cover-versi on ret rieval system. \n3 NON-VOCAL SEGMENT REMOVAL \nAlthough i t woul d be desi rable if all the non-vocal  \nregions wi thin a m usic recordi ng coul d be located \nautom atically, the task of  accurately distinguishing \nbetween t he segm ents with and wi thout singing is rather \ndifficult. Our previ ous work [16]  on this probl em found \nthat a vocal  segm ent tends t o be cl assified as non-vocal  \nif it is m ixed with loud background accom panim ent. \nAlthough discarding a low “vocal-to-accom panim ent-\nratio” seg ment is alm ost harmless in some applicatio ns, \nsuch as singer clustering [16], it coul d resul t in a very  \nfragm ented and unnat ural melody pattern being \nextracted from  a song. Thus, i nstead of l ocating all the \nvocal  and non-vocal  boundari es of a song docum ent, we \nonly try to detect the non-vocal  segm ents that are l onger \nthan two seconds. \nThe basi c strategy appl ied here is adapt ed from  our \nprevi ous work [16] , in whi ch a st ochast ic classifier is \n                                                           \n1 This cor responds to a whole rest , if 120 BPM  is assum ed. \n184   \n \nconst ructed to distinguish vocal  from  non-vocal  regions. \nAs shown i n Figure 2, t he classifier consi sts of a front -\nend signal processor t hat convert s waveform  sam ples to \ncepstral-based feature vector s, fol lowed by  a backend \nstatistical processor that perform s m odeling and \nmatching. In m odeling the acoust ic charact eristics of the \nvocal  and non-vocal  classes, t wo Gaussi an m ixture \nmodels (GM Ms), λV and λN, are created using the \nrespective feature vectors of the manually-segm ented \nvocal  and non-vocal  part s of t he music dat a collected \nbeforehand. W hen an unknown song is received, the \nclassifier takes as i nput the T-length feat ure vect ors X = \n{x1, x2, ..., xT} extracted from  that song, and produces as \noutput the fram e likelihoods p(xt|λV) and  p(xt|λN). \n \nUnknown\nRecordingVocal/Non-vocal\nSegmentsVocal/Non-vocal\nModels,     and\nModel\nAdaptation\nAdapted\nModels\nand       \nN VN V\nλ′  λ′λ  λ\n and byandReplacing\nVλ′Nλ′Cepstral\nFeature\nExtractionStochastic\nMatching\n& DecisionCepstral\nFeature\nExtractionGaussian\nMixture\nModelingManually\nAnnotated\nData\n  N V λ      λ \n \nFigure 2 . Vocal /non-vocal  classification. \n \nSince si nging tends t o cont inue for several  fram es, \nclassification can be m ade in a segm ent-by-segm ent \nmanner. Specifically, a W-length segm ent is classified as \neither vocal  or non-vocal  using \n, )λ| ( log )λ| ( log\nvocal-nonvocal\n1 1η≤>∑ ∑\n=+\n=+ −W\niN i sWW\niV i sW p p x x   (1) \nwhere s is the segm ent index. However, t o avoi d the risk \nthat a large W might cross m ultiple vocal/non-vocal \nchange boundari es, the classification is only valid for t he \nsegm ents where the classifi cation resul ts obtained wi th \nW and W/2 are co nsisten t. \nIn addition, recognizing that the accuracy of \nclassificatio n crucially depends on the reliab ility o f the \nvocal/non-vocal m odels, it seem s necessary to use \ntraining data that exhaust ively covers t he vocal /non-\nvocal characteristics of various m usic styles. However, \nacqui ring such a l arge am ount of training dat a is usual ly \ncost prohibitive, sin ce it req uires co nsiderable effo rt to \nmanually label the music. To ci rcum vent this probl em, \nwe tailor vocal/non-vocal m odels for each of the \nindividual test music recordi ngs, i nstead of designing \nmodels that can cover the universal  vocal /non-vocal  \ncharacteristics.  \nSimilar to [17] and [18], the idea is to  refin e the \nvocal /non-vocal  models by means of t he classification \nresults. It is assu med that the acoustic characteristics of \nthe true vocal/non-vocal se gments within each m usic recording can be inferred la rgely from  the classified \nvocal /non-vocal  segm ents. Thus, t he classified segm ents \ncan be used to refine the m odels, so that the classifier \nwith the refi ned m odels then repeat s the likelihood \ncomputation and deci sion-m aking, whi ch shoul d \nimprove recogni tion. There are a number of ways to \nperform  model refi nement. Thi s study uses a m odel \nadapt ation techni que based on m aximum a post eriori \nestimation [19] . The procedure of classification and \nmodel ad aptation is p erformed iterativ ely, until the \nresul ting vocal /non-vocal  boundari es do not change \nfurther. Fi nally, non-vocal  segm ents longer t han 2 \nseconds are l ocated and rem oved from  the recordi ng.  \n4 MAIN MELODY EXTRACTION \n4.1 Note sequence generati on \nGiven a music recordi ng, t he aim of m ain melody \nextraction is to find the sequence of m usical notes \nproduced by  the singing part  of t he recordi ng. Let  e1, \ne2,…, eN be the invent ory of possi ble not es perform ed \nby a singer. The task , therefo re, is to  determine wh ich \namong N possible notes is m ost likely sung at each \ninstant. To  do this, th e music sig nal is first divided into \nfram es by using a fi xed-l ength sliding wi ndow. Every  \nfram e is then convol ved wi th a Ham ming wi ndow and \nundergoes a fast  Fouri er transform  (FFT) with size J. \nSince m usical notes diffe r from  each other by the \nfundam ental frequenci es (F0s) t hey present , we may \ndeterm ine if a certain note is sung in each fram e by \nanalyzin g the spectral in tensity in  the freq uency reg ion \nwhere the F0 of the note is located.  \nLet xt,j denote the signal’s energy with respect to FFT \nindex j in fram e t, where 1 ≤ j ≤ J. If we use the MIDI \nnote num ber to represent  e1, e2,…, eN, and m ap the FFT \nindices into MIDI note num bers according to the F0 of \neach note, the signal’s energy on note en in fram e t can \nbe estim ated by  \n,   maxarg, ,\n)( ,jt\nnnt x y\nejUj = ∀=    (2) \nand \n, 5.69440)(log12)(2 ⎥⎦⎥\n⎢⎣⎢+⎟\n⎠⎞⎜\n⎝⎛⋅ =jFjU   (3) \nwhere ⎣⎦ is a floor operat or, F(j) is the correspondi ng \nfrequency  of FFT i ndex j, and U(⋅) represents a \nconversi on bet ween t he FFT i ndices and t he MIDI note \nnumbers. \nIdeally, if n ote en is sung i n fram e t, the resu lting \nenergy, yt,n, shoul d be t he maximum among yt,1, yt,2,…, \nyt,N. However, due t o the exi stence of harm onics, the \nnote numbers that are several  octaves hi gher t han the \nsung note can also receive a large proportion of the \nsignal’s energy. Som etimes the energy on a harm onic \nnote num ber can be even larger than the energy on the \ntrue sung note number; hence, t he not e num ber \nreceiving the largest energy is  not necessarily what is \nsung. To determine the sung not e more rel iably, this \n \n185   \n \nstudy adapt s Sub-Harm onic Sum mation (SHS) [20]  to \nthis probl em.  \nThe pri nciple appl ied here i s to com pute a value for \nthe “strength” of each possible note by sum ming up the \nsignal’s energy  on a note and i ts harm onic not e num bers. \nSpeci fically, the strengt h of not e en in fram e t is \ncomputed usi ng  \n,  \n012 , ,∑\n=+ =C\ncc ntc\nnt yh z   (4) \nwhere C is the num ber of harm onics that are taken into \naccount , and h is a positive value less than 1 to discount \nthe cont ribution of hi gher harm onics. The resul t of this \nsummation is that the not e num ber correspondi ng to the \nsignal’s F0 will receive the largest am ount of energy \nfrom  its harm onic not es. Thus, t he sung not e in fram e t \ncould be det ermined by  choosi ng t he not e num ber \nassociated with the largest value of t he strengt h, i.e., \n .  (5)    maxarg,\n1nt t z o\nNn≤≤=\nHowever, si nce m ost popul ar m usic contains \nbackground accom panim ent during m ost or all vocal \npassages, the note number associated with the largest \nvalue of t he strengt h may not be produced by  a singer, \nbut by the concurrent  instruments instead. To al leviate \nthe interference of the b ackground accom panim ent, we \npropose suppressi ng the strengt h pert aining to the notes \nthat are likely produced by  the in struments. The \nproposed method is motivated by  an observat ion m ade \nin popular m usic that the principal accom panim ents \noften cont ain a peri odically-repeated note, com pared to \nthe vocals. Figure 3 shows an exam ple of a fragm ent of \na pop song, i n which the tune i s convert ed into a MIDI \nfile. It is shown by software Cakewal kTM for ease of \nillustratio n. We can see fro m Figure 3 that the melody \nproduced by the principal accom panim ent tends to be \nrepeated in the ad jacent m easures, com pared to the m ain \nmelody produced by singing. Therefore, i t can be \nassum ed that a note num ber associated with the \nconst antly-large val ue of t he strengt h within and across \nadjacent measures is likely pr oduced by the instrum ents. \nIn response to this assum ption, we m odify the \ncomputation of st rengt h in Eq. (4) by    \n, )1 (21 ~2\n11\n2, ,\n1 2, , ⎟⎟\n⎠⎞\n⎜⎜\n⎝⎛\n++−− = ∑ ∑\n=+−\n−=+L\nLlnltL\nLlnlt nt nt z zL Lz z (6) \n \nOne Measure\nSinging Accompaniment\n \nFigure 3 . A fragm ent of the pop song “Let  It Be” \nby The Bea tles, in whi ch the tune i s convert ed \nmanually in to a MIDI file. where L1 and L2 specify the regions [ t−L2, t−L1 ] and [ t + \nL1, t + L2 ], in which an average st rengt h of not e en is \ncomputed. Implicit in Eq. (6) is th at the stren gth of note \nen in fram e t will be largely suppre ssed, if the average \nstrengt h of not e en computed from  the surroundi ng \nfram es is large. Accordi ngly, the sung not e in fram e t is \ndetermined by   \n.~  maxarg,\n1nt\nNnt z o\n≤≤=    (7) \n \n4.2 Note sequence rectification \nThe above fram e-based generat ion of not e sequences \nmay be i mproved by  expl oiting the underl ying relation \nor const raints bet ween fram es. The m ost visible \nconst raint between fram es is that the length of a note is \nusual ly several  times longer t han a fram e; hence, t here \nshould not be a drastic change like jitter between \nadjacent fram es. To rem ove the j itters in a note \nsequence, we apply m edian filtering, which replaces \neach note of the fram e with the local m edian of its \nneighbori ng fram es.  \nIn addi tion t o the short -term const raint between \nadjacent fram es, we further exploit a long-term  \nconst raint to rect ify a not e sequence. Thi s const raint is \nbased on t he fact  that the not es sung i n a music \nrecordi ng usual ly vary  far l ess than the range of all \npossi ble sung not es. Furt herm ore, t he range of t he notes \nsung wi thin a verse or chorus sect ion can be even \nnarrower. Figure 4 shows a segm ent of a pop song, i n \nwhich the singing part  is convert ed into a MIDI file. It is \nclear that the range of the not es within the verse can be \ndistinguished from  that of the chorus, m ainly because \nthe sung notes within a sect ion do not  spread over al l the \npossi ble notes, but  are onl y distributed over t heir own \nnarrower range. An i nform al survey  using 50 pop songs \nshows that the range of sung not es wi thin a whol e song \nand within a verse or chorus sect ion is around 24 and 22 \nsemitones, respectiv ely. Fig ure 5 details o ur statistic \nresul ts. The range of sung not es serves as a long-t erm \nconst raint to rectify a not e sequence. \nThe basic idea of rectificati on is to  locate incorrectly \nestimated not es that resul t in a not e sequence bey ond t he \nnorm al range. Since the accom panim ent is often played \nseveral  oct aves above or bel ow t he vocal s, the \nincorrectly estim ated notes ar e likely th e octave of their \ntrue notes. Therefore, we m ay adjust som e suspect notes \nby moving them several  octaves up or down, so t hat the \nrange of notes within an adjust ed sequence conform s to \nthe norm al range. To be speci fic, let o = {o1, o2,…, oT} \ndenot e a not e sequence est imated usi ng Eq. (7). An \nadjusted note sequence o′ = {o′1, o′2,…, o′T } is obtained \nby \n⎪⎪⎪\n⎩⎪⎪⎪\n⎨⎧\n−<−⎥⎦⎥\n⎢⎣⎢ −−×−>−⎥⎦⎥\n⎢⎣⎢ +−×−≤−\n=′ ,\n)2/(   if, 122/12   )2/(   if, 122/12)2/(| |if    ,                              \nR ooRoooR ooRoooR oo o\no\ntt\nttt\ntt t\nt (8) \n \n186   \n \nwhere R is the norm al range of t he sung not es in a \nsequence, say  24, and o is the m ean note com puted by \naveraging all the notes in o. In Eq. (8), a note, ot, is \nconsidered incorrect and needs to be adjusted if it is too \nfar away  from  o, i.e., | ot −o| > R/2. The adjustm ent is \nperform ed by moving t he incorrect  not e ⎣(ot−o+ \nR/2)/12⎦ or ⎣(ot −o− R/2)/12⎦ octaves.   \n \nVerse Chorus\n \nFigure 4 . A fragm ent of t he pop song \n“Yesterday” by The Bea tles, in which the singing \nis convert ed into a MIDI fi le. \n \n101112131415161718192021222324\nR (Se mitones)020406080100 Percentage of Songs\n \n(a) The range of sung not es wi thin a whol e song. \n10 11 12 1314 151617 18 19 20 21 22\nR (Se mitones)020406080100 Percentage of Songs\n \n(b) The range of sung not es wi thin a verse or chorus.  \nFigure 5 . Statistics of t he range of sung not es in \n50 pop songs, i n whi ch the percent age of songs \nwhose range of sung notes less than R sem itones \nis shown. \n5 SIMILARITY COMPUTATION \nAfter represent ing m usic dat a as a sequence of note \nnumbers, cover-versi on ret rieval can be convert ed into a \nprobl em of com paring the similarity between a query ’s \nsequence and each of the docum ents’ sequences. Since \ncover versi ons are oft en different  from  the original song \nin terms of key , tempo, ornam ent, etc., it is virtually \nimpossi ble to find a docum ent sequence t hat matches the \nquery  sequence exact ly. M oreover, m ain melody \nextraction is known t o be frequent ly imperfect , whi ch \nfurther introduces erro rs of substitution, deletion, and \ninsertion into the not e sequences. For rel iable melody similarity comparison, an approxi mate matching m ethod \ntolerable to occasional note e rrors is therefore needed. \nLet q = {q1, q2,…, qT}, and u = {u1, u2,…, uL} be \nthe note sequences extracted from  a user’s query and a \nparticular music docum ent to be com pared, respectively. \nThe m ost apparent problem  we face is that the lengths of \nq and u are usual ly unequal . Thus, i t is necessary  to \ntemporally alig n q and u before com puting t heir \nsimilarity. Fo r this reaso n, we ap ply Dynamic Time \nWarping2 (DTW ) to find the m apping between each qt \nand ul, 1 ≤ t ≤ T, 1 ≤ l ≤ L. DTW  operat es by  \nconst ructing a T×L distance m atrix D = [ D(t,l)]T × L, \nwhere D(t,l) is the distance between note sequences {q1, \nq2,…,qt} and { u1, u2,…, ul}. It is com puted by  \n,  \n),( )2,1(),( )1,1(),( 2)1,2(\nmin),(\n⎪⎩⎪⎨⎧\n+− −− +− −×+− −\n=\nl ll ll l\nl\ntd tDtd tDtd tD\ntD ε      (9) \nand  \nd(t,l) = | qt − ul| ,   (10) \nwhere ε is a small const ant that favors t he mapping \nbetween not es qt and ul, given t he distance bet ween note \nsequences { q1, q2,…,qt-1} and { u1, u2,…, ul-1}. The \nboundary conditions for the above recursion are defined \nby \n.  \n4, )2,()2,2( 2)1,1( )2,3()2,2( )1,1( )3,2()2,2( )1,1( )2,2(2, )1,()1,1( )1,1(\n⎪⎪⎪⎪\n⎩⎪⎪⎪⎪\n⎨⎧\n≤≤ ∞=×+ =+ =− + =≤≤ ∞==\nTt tDd d Dd d Dd d DTt tDd D\nε              (11) \nAfter th e distance m atrix D is co nstructed, the sim ilarity \nbetween q and u can be eval uated by  \n[ ]\n,\n 2/  if   ,                                  2/  if   , ),(/1 max\n),(),2 min( 2/\n⎩⎨⎧\n< ∞≥\n=≤≤\nTLTL TD\nSLT Tl\nl uq (12) \nwhere we assum e that the end of a query’s sequence \nshoul d be al igned t o a certain fram e between T/2 and \nmin(2T,L) of the docum ent’s sequence, and assum e that \na docum ent whose l ength of sequence i s less than T/2 is \nnot a rel evant docum ent to the query . \nSince a song query  may be perform ed in a different  \nkey or regi ster than the target music docum ent, i.e., the \nso-called  transposition, the resul ting not e sequences of \nthe query  and t he docum ent coul d be rat her different . To \ndeal with this probl em, the dynamic range of a query ’s \nnote sequence needs t o be adjusted to that of the \ndocum ent to be compared. This can be done by  moving \nthe query ’s note sequence up or down several  semitones, \nso that the m ean of the note sequence i s equal  to that of \nthe docum ent to be com pared. Briefly, a query’s note \nsequence is adjusted by \n) ( qu q qt t − + ← ,  (13) \n                                                           \n2 Similar work can be found in [3, 4,21]. \n \n187   \n \nwhere q and u are the m eans of the query’s note \nsequence and t he docum ent’s not e sequence, \nrespect ively. However, our experi ments find that the \nabove adjust ment can not  ful ly overcom e the \ntransposi tion probl em, since t he val ue of (q−u) can \nonly reflect a gl obal difference of key  between a query  \nand a docum ent, but  cannot  charact erize the part ial \ntransposi tion or key  change over t he course of a query . \nTo handl e this probl em better, we furt her m odify the \nDTW  similarity co mpariso n by considering the key \nshifts of a query ’s not e sequence. Speci fically, a query  \nsequence  q is sh ifted with ±1, ±2,..., ±K sem itones to \nspan a set  of not e sequences {q(1), q(-1), q(2), q(-2),…, q(K), \nq(-K)}. For a docum ent sequence u, the sim ilarity S(q, u) \nis then det ermined by  choosi ng the one among {q(0), q(1),   \nq(-1), q(2),  q(-2), …, q(K), q(-K)} that is m ost sim ilar to  u, \ni.e.,  \n),,( max),()(u q uqk\nKkKS S\n≤≤−=        (14) \nwhere q(0) = q. \n6 EXPERIMENTS \n6.1 Musi c data \nThe music dat abase used i n this study consi sted of 794 \ntracks3 from  pop music CDs, whi ch mainly com prised \nfive genres: soundtrack, country , folk, jazz, and rock. It \nwas divided into three sub-set s. The fi rst sub-set , \ndenot ed as DB-1, cont ained 47 pai rs of t racks i nvolving \ncover/ original songs. In t his sub-set , the difference \nbetween a cover versi on and t he ori ginal song was \ncharact erized by  the fol lowing fact ors: L: language \n(including Engl ish, M andari n, and Japanese);  S: singer;  \nA: principal accom panim ents; T: tem po; and N: non-\nvocal melodies. A sum mary of the characteristic \ndifferences within each pair of tracks is given in Table 1. \n \nTable 1. A sum mary of the characteristic \ndifference within each cover/original pair of \ntrack s in sub-set DB-1 . \nType of wi thin-pair difference No. of pai rs \nL 8 \nL + S 7 \nL + T 3 \nL + S + T 7 \nL + T + N 6 \nL + S + T + N 4 \nL + A + T + N 2 \nL + S + A + T + N 10 \n \nThe second sub-set , denot ed as DB -2, cont ained 500 \ntracks, none of whi ch was a cover versi on of any  track \nin DB-1. The t hird sub-set , denot ed as DB-3, contained \n200 tracks, perform ed by  13 fem ale and 8 m ale singers, \nnone of whom  appeared i n DB-1 and DB -2. The sub-\nsets DB-1 and DB-2 were us ed to evaluate the cover-\n                                                           \n3 The database did not contain the 50 pop songs used for  analy zing the \nrange of sung notes as descr ibed in Sec.  4.2. version retriev al system , while DB-3  was u sed to create \nthe vocal  and non-vocal  models. Manual annot ation of \nvocal /non-vocal  boundari es was onl y perform ed on DB-\n1 and DB -3. The waveform  signals were down-sam pled \nfrom  a CD sam pling rat e of 44.1 kHz t o 22.05 kHz, t o \nexclude the high frequency  com ponent s that usual ly \ncontain sparse vocal  inform ation.  \n6.2 Experimental results \nOur fi rst experi ment was conduct ed usi ng DB -1 as t est \ndata. It was run i n a leave-one-out  manner, whi ch used \none track  at a tim e in DB-1  as a q uery trial to  retriev e \nthe rem aining 93 t racks, and t hen rot ated through al l the \n94 tracks. To roughly reflect a real-use scenario, each \nquery  was only a verse or chorus obt ained wi th manual \nsegm entation. The l ength of query  ranged from  31 to 54 \nseconds. Perform ance of t he song ret rieval was \nevaluated on the basis of retrieval accuracy: \n100%.queries #first ranked are songs  target  whose queries #×  \nWe also com puted the Top-N accuracy defined as the \npercent age of the queri es whose t arget songs are am ong \nTop-N. \nTable 2 shows the retrieval results for different \nconfi gurat ions used i n main melody extraction. In this \nexperim ent, each of the docum ents was a track with \nnon-vocal  segm ents rem oved m anually. The invent ory \nof possi ble sung notes consi sted of t he MIDI num bers \nfrom  41 t o 83, whi ch corresponds t o the frequency  \nrange of 87 t o 987 Hz. In FFT computation, the fram e \nlength and the overl ap bet ween fram es were set  to be \n2048 and 1704, respectively. In addition, in m elody \nsimilarity co mpariso n, we u sed K = 2 i n Eq. (14) t o \nhandl e the transposi tion probl em. We can see from  \nTable 2 that the retrieval perform ance obt ained by  using \nEq. (5) was t he worst  of t he three m ethods compared, \nmainly because this m ethod determ ines the sung notes \nbased on t he strengt h com puted from  the observed \nsignal, whi ch is vul nerabl e to the interference of \nbackground accom panim ents. It is clear from  Table 2 \nthat a better estimation of t he not e strengt h can be \nobtained by  using Eq. (7), which discount s the note \nnumbers associ ated wi th the const antly-large val ues of \nthe strength within and across adjacent m easures. W e \ncan also see from  Table 2 that  melody extraction can be \nfurther improved by usi ng t he not e sequence \nrectificatio n of Eq. (8). \nTable 3 shows the retrieval results for different \nconfi gurat ions used i n melody similarity com parison. In \nthis experi ment, main melody extraction was perform ed \nusing the method of Eqs. (7) and (8) wi th R = 24, i .e., \nthe best resul ts shown in Tabl e 2. W e can see from  \nTable 3 that the retrieval perform ance improves as the \nvalue of K increases. This indicates that the m ore the \npossi ble changes of key  are t aken i nto account , the \ngreater the chance th at a q uery’s seq uence will m atch \nthe correct docum ent’s sequence. However, increasing \nthe value of K substantially in creases computational \ncosts, because the sim ilarity com parison requires two \n \n188   \n \nextra DTW  operations whenever the value of K is \nincreased by  one. An econom ic value of K = 2 was t hus \nchosen t hroughout  our experi ments. \n \nTable 2. Perform ance of cover-versi on ret rieval \nfor different  confi gurat ions used i n main melody \nextraction, in which each m ethod operates \ntogether with five-fram e median filtering. \nAccuracy (%) Main  melody  \nextraction m ethod Top 1 Top 3 Top 10\nEq. (5)  60.64 71.28 78.72 \nEq. (7) \n(L1 = 64, and L2 = 192) 70.21 73.40 80.85 \nR = 22 65.96 72.34 74.47 \nR = 24 76.60 78.72 87.23 \nR = 26 74.47 77.66 86.17 Eqs. (7) \nand (8)  \nR = 28  70.21 77.66 85.11 \n \nTable 3. Perform ance of cover-versi on ret rieval \nfor different  confi gurat ions used i n melody \nsimilarity co mpariso n. \nAccuracy (%) Value of K in Eq. (14)  Top 1 Top 3 Top 10\n0 64.89 67.02 77.66 \n1 73.40 75.53 80.85 \n2 76.60 78.72 87.23 \n3 76.60 79.79 88.30 \nNext, we exam ined the perform ance of cover version \nretrieval based on the aut omatic rem oval of t he non-\nvocal segm ents of each docum ent. The num ber of \nGaussian densities used in the vocal and non-vocal \nmodels was empirically determined to be 64. The l ength \nof segm ent, W, in Eq. (1) was set  to be 200. Table 4 \nshows the experimental resu lts, in  which the results of \n“Manual  removal” correspond t o the resul ts of “ K = 2” \nin Tabl e 3. W e can see from  Tabl e 4 that although t here \nis a significant perform ance gap between the m anual \nand aut omatic rem oval of t he non-vocal  segm ents, the \nperform ance obt ained wi th automatic non-vocal  \nremoval is much bet ter than that obtained wi thout non-\nvocal  removal. \nExperi ments were furt her conduct ed to evaluate the \nretrieval perform ance of our sy stem  for a larger \ncollection of songs. W e used each of the 94 queries \nonce at  a time to retrieve the 593 t racks i n DB-1 and \nDB-2. Since no m anual annot ation of vocal /non-vocal  \nboundari es was perform ed on DB -2, the experi ment was \nrun on t he basi s of aut omatically rem oving the non-\nvocal segm ents of each docum ent. Table 5 shows the \nexperim ental results. As expected, the increased num ber \nof non-t arget songs i nevitably reduced the retrieval \naccuracy. By com paring Table 5 with Table 4, we can \nfind that the retrieval accur acy deteriorates sharply \nwhen the system operat es on a l arger col lection of songs \nwithout rem oving the non-vocal  segm ents. Once again, \nthis indicates the necessi ty of non-vocal  region rem oval. \nFigure 6 details th e retriev al resu lts fo r the 94 query \ntrials, in which each point indicates the rank of each query ’s target song am ong t he 593 docum ents. We can \nsee from  Figure 6 t hat almost all the target songs of \nqueri es belonging to “L” and “L + T” were ranked \namong t he Top 3, whereas a l arge proport ion of the \ntarget songs of queri es belonging to “L + S + A + T + \nN” were ranked out side the Top 10. This reflects the \nfact that the greater the difference between the cover \nversi on and t he ori ginal song, t he more difficult it is to \nretrieve one song by  using another song as a query . \nAlthough t he overal l perform ance l eaves m uch room  for \nfurther improvement, our system  shows the feasib ility \nof ret rieving pol yphoni c cover recordi ngs i n a query -by-\nexam ple fram ework. \n \nTable 4. Perform ance of cover-versi on ret rieval \nobtained wi th and wi thout rem oving the non-\nvocal segm ents of each docum ent. \nAccuracy (%) Non-vocal  segm ent \nremoval method Top 1 Top 3 Top 10\nManual removal 76.60 78.72 87.23 \nAutomatic removal 65.96 69.15 72.34 \nWithout removal  54.26 59.57 64.89 \n \nTable 5. R esults of cover-versi on ret rieval for a \ncollection of 594 t racks i n DB-1 and DB -2. \nAccuracy (%) Non-vocal  segm ent \nremoval method Top 1 Top 3 Top 10\nAutomatic removal 63.83 65.96 72.34 \nWithout removal  47.87 54.26 60.64 \n \nL\nL+S\nL+S+A+T+NL+T\nL+S+T\nL+T+N\nL+A+T+NL+S+T+N94 Queries\n110100Rank of the tar get song500\nFigure 6 . The ranks of t he 94 queri es’ target songs. \n7 CONCLUSIONS \nIn this study, we h ave examined the feasib ility o f \nretrieving cover versi ons of a song speci fied by  a user. \nA query -by-exam ple fram ework has been proposed t o \ndetermine which among a col lection of songs cont ain \nsimilar main melodies to a user’s song query . In \nparticular, to exclude factors that are irrelev ant to the \nmain melody of a song, we have proposed rem oving the \nnon-vocal  segm ents that are l onger t han a whol e rest . In \naddition, to alleviate the interference of background \n \n189   \n \naccom panim ents during the estim ation of the sung note \nat each instant, we have pr oposed avoiding that a certain \nnote number is regarded as a sung not e if the strengt h of \nthis note is continually larg e within and across adjacent \nmeasures. We have al so proposed correct ing t he \nestim ated sung note sequence by lim iting the range of \nsung not es in a sequence t o 24 semitones. Furtherm ore, \nwe have st udied t he m ethod of comparing the \nsimilarities between a query’s note sequence and each \nof the docum ents’ not e sequences. Thi s method has \nproven capabl e of handl ing the discrepanci es in tempo \nand t ransposi tion bet ween cover versi ons and t he \noriginal songs.   \nDespi te their potential, the methods proposed i n this \nstudy can only be basel ine sol utions t o the cover- \nversi on ret rieval probl em. Anal ogous t o other research \non retrieving pol yphoni c docum ents based on \npolyphonic queries, m ore work is still needed to \nimprove m elody ext raction and m elody similarity \ncomparison. In addi tion, t o furt her expl ore t he cover- \nversion retriev al problem, the essen tial wo rk is to  scale \nup the music dat abase, whi ch covers a wider variety of \nmusic styles, genres, si ngers, l anguages, and so on. \n8 ACKNOWLEDGEMENT \nThis work was support ed in part by the National Science \nCounci l, Taiwan, under Grant  NSC 93-2422-H-001-0004. \nREFERENCES \n[1] Ghias, A., Logan, H., Cham berlin, D., and Smith, B. \nC. “Query  by  hum ming: musical inform ation \nretrieval in an audio database”, Proceedings of the \nACM International Conference on Multim edia, 1995. \n[2] Kosugi, N., Nishihara, T. Sakata, S., Yam amuro, M., \nand Kushi ma, K. “A pract ical query -by-huming \nsystem  for a large m usic database”, Proceedings of \nthe ACM Conference on Multim edia, 2000. \n[3] Hu, N. and Dannenberg, R . B. “A com parison of \nmelodic dat abase ret rieval techni ques using sung \nqueries”, Proceedings of the ACM/IEEE-CS Joint \nConference on Di gital Librari es, 2002. \n[4] Pauws, S. “CubyHum : A ful ly operat ional query  by \nhumming system ”, Proceedings of the International \nConference on M usic Inform ation R etrieval, 2002. \n[5] Nishimura, T., Hashi guchi , H. Taki ta, J.  Zhang, J. \nX., Got o, M., and Oka, R . “Music signal spotting \nretrieval by a hum ming query  using st art fram e \nfeature dependent  cont inuous dynamic \nprogram ming”, Proceedings of the International \nSymposium on M usic Inform ation R etrieval, 2001. \n[6] Song, J., Bae, S. Y., Yoon, K. “M id-level music \nmelody represent ation of pol yphoni c audio for \nquery-by-hum ming system ”, Proceedings of the \nInternational Conference on Music Inform ation \nRetrieval, 2002. \n[7] Dorai samy, S., and R uger, S. M . “An approach \ntowards a pol yphoni c m usic retrieval system”, Proceedings of the International Sym posium  on \nMusic Inform ation Retrieval , 2001. \n[8]   Pickens, J., Bello, J. P., Monti, G., Crawford, T., \nDovey , M., Sandl er, M . and Byrd, D. “Polyphoni c \nscore ret rieval using pol yphoni c audi o queri es: A \nharm onic modelling approach”, Proceedings of the \nInternational Conference on M usic Inform ation \nRetrieval, 2002. \n[9]   Foote, J. “ARTHUR: Retrieving orchestral music \nby long-term  structure”, Proceedings of the \nInternational Symposium on M usic Inform ation \nRetrieval, 2000. \n[10] Tsai, W. H., and W ang, H. M . “A query -by-\nexam ple fram ework to retrieve m usic docum ents \nby singer”, Proceedings of the IEEE Conference on \nMultim edia and Expo, 2004. \n[11] Tzanet akis, G., and C ook, P. Musical genre \nclassification of audio si gnals. IEEE Transactions \non Speech and Audio Processing, 10 (5), 2002. \n[12] Feng, Y., Zhuang, Y., and Pan, Y. “Popul ar music \nretrieval by detecting m ood”, Proceedings of the \nACM Conference on Research and Developm ent in \nInform ation R etrieval, 2003. \n[13] Eggl ink, J., and B rown, G. J. “Extracting melody \nlines from  com plex audio”, Proceedings of the \nInternational Conference on M usic Inform ation \nRetrieval, 2004. \n[14] Meek, C ., and B irmingham , W. P. Automatic \nthematic ex tracto r. Jo urnal of Intellig ent \nInform ation Sy stems, 21 (1), 2003, 9–33. \n[15] Typke, R ., Vel tkamp, R . C., and Wiering, F. \n“Searchi ng notated pol yphoni c m usic usi ng \ntransportation distances”, Proceedings of the ACM \nConference on Multim edia, 2004. \n[16] Tsai , W. H., W ang, H. M ., and R odgers D. Blind \nclustering of popul ar m usic recordi ngs based on \nsinger voice characteristics. Com puter Music \nJournal , 28 (3), 2004, 68–78. \n[17] Nwe, T. L., and W ang, Y. “Aut omatic detection of \nvocal segm ents in popular songs”, Proceedings of \nthe International Conference on Music Inform ation \nRetrieval, 2004. \n[18] Tzanet akis, G. “Song-speci fic boot strappi ng of \nsinging voice structure”, Proceedings of the IEEE \nConference on Multim edia and Expo, 2004. \n[19] Reynolds, D. A., Quat ieri, T. F., and Dunn, R. B. \nSpeaker veri fication usi ng adapt ed Gaussi an \nmixture m odels. Digital Signal Processi ng, 10, 2000. \n[20] Piszczalski, M., and Ga ller, B. A. “Predicting \nmusical pitch from  com ponent  frequency  ratios”, \nJournal  of the Acoust ical Soci ety of America, 66(3), \n1979, 710–720. \n[21] Pardo, B ., Birmingham , W. P., and Shi frin, J. Nam e \nthat tune: a pi lot study in finding a melody from  a \nsung query . Journal  of t he Am erican Soci ety for \nInform ation Sci ence and Technol ogy 55(4), 2004. \n \n190"
    },
    {
        "title": "A Survey of Music Information Retrieval Systems.",
        "author": [
            "Rainer Typke",
            "Frans Wiering",
            "Remco C. Veltkamp"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417383",
        "url": "https://doi.org/10.5281/zenodo.1417383",
        "ee": "https://zenodo.org/records/1417383/files/TypkeWV05.pdf",
        "abstract": "This survey paper provides an overview of content-based music information retrieval systems, both for audio and for symbolic music notation. Matching algorithms and indexing methods are briefly presented. The need for a TREC-like comparison of matching algorithms such as MIREX at ISMIR becomes clear from the high number of quite different methods which so far only have been used on different data collections. We placed the systems on a map showing the tasks and users for which they are suitable, and we find that existing content-based retrieval systems fail to cover a gap between the very general and the very specific retrieval tasks. Keywords: MIR, matching, indexing. 1",
        "zenodo_id": 1417383,
        "dblp_key": "conf/ismir/TypkeWV05",
        "keywords": [
            "survey paper",
            "content-based music information retrieval systems",
            "audio",
            "symbolic music notation",
            "matching algorithms",
            "indexing methods",
            "TREC-like comparison",
            "MIREX",
            "ISMIR",
            "tasks and users"
        ],
        "content": "A SURVEY OF MUSIC INFORMATION RETRIEVAL SYSTEMS\nRainer Typke, Frans Wiering, Remco C. Veltkamp\nUniversiteit Utrecht\nPadualaan 14, De Uithof\n3584CH Utrecht, The Netherlands\nrainer.typke,frans.wiering,remco.veltkamp@cs.uu.nl\nABSTRACT\nThis survey paper provides an overview of content-based\nmusic information retrieval systems, both for audio and\nfor symbolic music notation. Matching algorithms and\nindexing methods are brieﬂy presented. The need for a\nTREC-like comparison of matching algorithms such as\nMIREX at ISMIR becomes clear from the high number\nof quite different methods which so far only have been\nused on different data collections. We placed the systems\non a map showing the tasks and users for which they are\nsuitable, and we ﬁnd that existing content-based retrieval\nsystems fail to cover a gap between the very general and\nthe very speciﬁc retrieval tasks.\nKeywords: MIR, matching, indexing.\n1 INTRODUCTION\nThis paper gives an overview of Music Information Re-\ntrieval (MIR) systems for content-based music search-\ning, preceded by a brief overview of the methods com-\nmonly used by these systems. Unlike the existing liter-\nature (Downie,2003;Birmingham et al. ,2003), we try\nto place the systems on a two-dimensional map of re-\ntrievaltasksandtargetedusers. Informationaboutthesys-\ntems was collected with the help of a website ( http://\nmirsystems.info )withaquestionnairewheredevel-\nopers of MIR systems can enter descriptions of their sys-\ntems,includingpublications,matchingmethods,features,\nindexing method, and collection size. Most of the infor-\nmation in this paper, however, comes from the publica-\ntions containing the developers’ own evaluations of their\nsystems.\nTwo main groups of MIR systems for content-based\nsearching can be distinguished, systems for searching au-\ndio data and systems for searching notated music. There\nare also hybrid systems that ﬁrst convert audio signal into\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonasymbolicdescriptionofnotesandthensearchadatabase\nof notated music.\nContent-based music search engines can be useful for\na variety of purposes and audiences:\n•Query-by-Humming: in record stores, it is not un-\ncommon for customers to only know a tune from a\nrecord they would like to buy, but not the title of the\nwork, composer, or performers. Salespeople with a\nvastknowledgeofmusicwhoarewillingandableto\nidentify tunes hummed by customers are scarce, and\nitcouldbeinterestingtohaveacomputerdothetask\nof identifying melodies andsuggesting records.\n•A search engine that ﬁnds musical scores similar to\na given query can help musicologists ﬁnd out how\ncomposersinﬂuencedoneanotherorhowtheirworks\nare related to earlier works of their own or by other\ncomposers. This task has been done manually by\nmusicologists over the past centuries. If computers\ncould perform this task reasonably well, more inter-\nesting insights could be gained faster and with less\neffort.\n•Copyrightissuescouldberesolved,avoidedorraised\nmore easily if composers could easily ﬁnd out if\nsomeone is plagiarizing them or if a new work ex-\nposesthemtotheriskofbeingaccusedofplagiarism.\nContent-based search mechanisms that work speciﬁcally\nfor audio recordings can be useful for the following pur-\nposes:\n•It is possible to identify music played, for example,\nontheradioorinabarbypointingacellularphoneat\nthespeakersforafewsecondsandusinganaudioﬁn-\ngerprinting system for identifying the exact record-\ning that is being played.\n•Recordings made by surveillance equipment can be\nsearched for suspicious sounds.\n•Content-based video retrieval can be made more\npowerful by analyzing audiocontent.\n•Theaters, ﬁlm makers, and radio or television sta-\ntions might ﬁnd a search engine useful that can ﬁnd\nsoundeffectssimilartoagivenqueryoraccordingto\na given description in a vast library of audio record-\nings.\nAlthough MIR is a rather young ﬁeld, and the problems\nofMIRarechallenging ( ByrdandCrawford ,2002),there\n153arealreadycommercialapplicationsofMIRsystems. The\nautomatic identiﬁcation of recordings via cellular phones\nusing audio ﬁngerprinting, for example, is offered by\nShazam1, a UK-based service that charges its customers\nfor identifying tunes and also offers matching ringtones\nand CDs.\n2 SEARCHING SYMBOLIC DATA\n2.1 String-based methodsfor monophonic melodies\nMonophonic music can be represented by one-\ndimensional strings of characters, where each charac-\nter describes one note or one pair of consecutive notes.\nStrings can represent interval sequences, gross contour,\nsequences of pitches and the like, and well-known string\nmatching algorithms such as algorithms for calculating\nediting distances, ﬁnding the longest common subse-\nquence, or ﬁnding occurrences of one string in another\nhave been applied, sometimes with certain adaptations to\nmake them suitable for matching melodies.\n2.1.1 Distance Measures\nSomeMIRsystemsonlycheckforexactmatchesorcases\nwhere the search string is a substring of database entries.\nFor such tasks, standard string searching algorithms like\nKnuth-Morris-Pratt and Boyer-Moore can be used. The-\nmeﬁnder (see Section 4.17) searches the database for en-\ntries matching regular expressions. In this case, there is\nstill no notion of distance, but different strings can match\nthe same regular expression.\nForapproximatematching,itcanbeusefultocompute\nan editing distance with dynamic programming. Musi-\npediaisanexampleofasystemthatdoesthis(seeSection\n4.7). Simplycomputinganeditingdistancebetweenquery\nstrings and the data in the database is not good enough,\nhowever, because these strings might represent pieces of\nmusic with different lenghts. Therefore, it can be nec-\nessary to choose suitable substrings before calculating an\nediting distance.\n2.1.2 Indexing\nFor ﬁnding substrings that match exactly, the standard\nmethods for indexing text can be used (for example, in-\nverted ﬁles, B-trees, etc.). The lack of the equivalent of\nwords in music can be overcome by just cutting melodies\ninto n-grams ( Downie,1999) and indexing those.\nFormosteditingdistancesthatareactuallyuseful,the\ntriangle inequality holds2. Therefore, the vantage index-\ning method described in Typke et al. (2003) can be used\nfor those, but other methods like metric trees or vantage\npoint trees are also possible.\n2.2 Set-based methods forpolyphonic music\nUnlike string-based methods, set-based methods do not\nassume that the notes are ordered. Music is viewed as aset of events with properties like onset time, pitch, and\nduration.\n2.2.1 Distance Measures\nClausenetal. (2000)proposedasearchmethodthatviews\nscores and queries as sets of notes. Notes are deﬁned by\nnoteonsettime,pitch,andduration. Exactmatchesaresu-\npersets of queries, and approximate matching is done by\nﬁnding supersets of subsets of the query or by allowing\nalternative sets.\nTypkeetal. (2003)alsoviewscoresandqueriesassets\nof notes, but instead of ﬁnding supersets, they use trans-\nportationdistancessuchastheEarthMover’sDistancefor\ncomparing sets (see 4.9).\n2.2.2 Indexing\nBy quantizing onset times and by segmenting the music\ninto measures, Clausen et al. (2000) make it possible to\nuse inverted ﬁles. Typke et al. (2003) exploit the triangle\ninequality for indexing, which avoids the need for quan-\ntizing. Distances to a ﬁxed set of vantage objects are pre-\ncalculatedforeachdatabaseentry. Queriesthenonlyneed\ntobecomparedtoentrieswithsimilardistancestothevan-\ntage objects.\n2.3 Probabilistic Matching\nThe aim of probabilitstic matching methods is to deter-\nmineprobabilisticpropertiesofcandidatepiecesandcom-\npare them with corresponding properties of queries. For\nexample, the GUIDO system (see Section 4.5) calculates\nMarkovmodelsdescribingtheprobabilitiesofstatetransi-\ntionsinpiecesandthencomparesmatriceswhichdescribe\ntransition probabilities.\n2.3.1 Distance Measures\nFeatures of melodies such as interval sequences, pitch\nsequences, or rhythm can be used to calculate Markov\nchains. In these Markov chains, states can correspond\nwith features like a certain pitch, interval, or note dura-\ntion,andthetransitionprobabilitiesreﬂectthenumbersof\noccurrences of different subsequent states. The similar-\nity between a query and a candidate piece in the database\ncan be determined by calculating the product of the tran-\nsition probabilities, based on the transition matrix of the\ncandidate piece, for each pair of consecutive states in the\nquery. See Section 4.5for an example of a MIR system\nwith probabilistic matching.\n2.3.2 Indexing: Hierarchical Clustering\nTransitionmatricescanbeorganizedasatree. Theleaves\nare the transition matrices of the pieces in the database,\nwhile inner nodes are the transition matrices describing\ntheconcatenationofthepiecesinthesubtree. SeeSection\n4.5orHoos et al. (2001) for a more detailed description.\n1http://www.shazam.com , not to be confused with http://www.shazam.co.uk\n2An example for a not very useful editing distance would be one where any character can be replaced with one special character\nat no cost. That way, the detour via a string consisting only of that special character would always yield the distance zero for unequal\nstrings of the same length.\n1543 SEARCHING AUDIO DATA\n3.1 Extracting perceptionally relevant features\nA natural way of comparing audio recordings in a mean-\ningful way is to extract an abstract description of the au-\ndio signal which reﬂects the perceptionally relevant as-\npects of the recording, followed by the application of a\ndistance function to the extracted information. An audio\nrecording is usually segmented into short, possibly over-\nlappingframeswhichlastshortenoughsuchthatthereare\nnotmultipledistinguishableeventscoveredbyoneframe.\nWold et al. (1996) list some features that are commonly\nextracted from audio frames with a duration between 25\nand 40 milliseconds:\n•Loudness: can be approximated by the square root\nof the energy of the signal computed from the short-\ntime Fourier transform, in decibels.\n•Pitch:theFouriertransformationofaframedelivers\naspectrum,fromwhichafundamentalfrequencycan\nbe computed with an approximate greatest common\ndivisor algorithm.\n•Tone (brightness and bandwidth): Brightness is a\nmeasure of the higher-frequency content of the sig-\nnal. Bandwidth can be computed as the magnitude-\nweighted average of the differences between the\nspectral components and the centroid of the short-\ntime Fourier transform. It is zero for a single sine\nwave, while ideal white noise has an inﬁnite band-\nwidth.\n•Mel-ﬁltered Cepstral Coefﬁcients (often abbrevi-\nated as MFCCs) can be computed by applying a\nmel-spaced set of triangular ﬁlters to the short-time\nFourier transform, followed by a discrete cosine\ntransform. The word “cepstrum” is a play on the\nword “spectrum” and is meant to convey that it is\na transformation of the spectrum into something that\nbetter describes the sound characteristics as they are\nperceived by a human listener. A mel is a unit of\nmeasure for the perceived pitch of a tone. The hu-\nman ear is sensitive to linear changes in frequency\nbelow1000Hzandlogarithmicchangesabove. Mel-\nﬁltering is a scaling of frequency that takes this fact\ninto account.\n•Derivatives: Since the dynamic behaviour of sound\nisimportant, itcanbehelpful tocalculatetheinstan-\ntaneous derivative (time differences) for all of the\nfeatures above.\nAudio retrieval systems such as the system described in\nSection4.16compare vectors of such features in order to\nﬁnd audio recordings that sound similar to a given query.\n3.2 Audio Fingerprinting\nIf the aim is not necessarily to identify a work, but a\nrecording, audio ﬁngerprinting techniques perform quite\nwell. All phone-based systems for identifying popular\nmusic(e.g.,Shazam)usesomeformofaudioﬁngerprint-\ning. Afeatureextractorisusedtodescribeshortsegments\nofrecordingsinawaythatisasrobustaspossibleagainstthetypicaldistortionscausedbypoorspeakers,cheapmi-\ncrophones, and a cellular phone connection, as well as\nbackground noise like people chatting in a bar. Such fea-\ntures do not need to have anything to do with human per-\nceptionorthemusicontherecording,theyjustneedtobe\nunique for different recordings and robust against distor-\ntions. These audio ﬁngerprints, usually just a few bytes\nper recording segment, are then stored in a database in-\ndex, along with pointers to the recordings where they oc-\ncur. The same feature extractor is used on the query, and\nwith the audio ﬁngerprints that were extracted from the\nquery, candidates for matching recordings can be quickly\nretrieved. The number of these candidates can be reduced\nbycheckingwhethertheﬁngerprintsoccurintherightor-\nder and with the same local timing.\n3.3 Set-based Methods\nClausen and Kurth used their set-based method (see Sec-\ntion2.2) also for audio data. They use a feature extractor\nfor converting PCM3signals into sets that can be treated\nthe same way as sets of notes.\n3.4 Self-Organizing Map\nSelf-OrganizingMap(SOM),averypopularartiﬁcialneu-\nral network algorithm in the unsupervised learning cate-\ngory, has been used for clustering similar pieces of mu-\nsic and classifying pieces, for example by Rauber et al.\n(2003). Section 4.14describes their system, which ex-\ntracts feature vectors that describe rhythm patterns from\naudio, and clusters them with aSOM.\n4 MIR SYSTEMS\nTable1givesanoverviewofthecharacteristicsof17MIR\nsystems. Thefollowingsubsectionscontainadditionalin-\nformation about these systems.\n4.1 audentify!\nURL: http://www-mmdb.iai.uni-bonn.de/\neng-public.html\nThe ﬁngerprints are sequences of bits with a ﬁxed length,\nwhere every bit describes one audio window. The col-\nlection contains about 15.000 MP3 ﬁles (@128kBit/s),\napprox. 1.5 month of audio data.\nLiterature: Kurth et al. (2002b),Kurth et al. (2002a),\nRibbrock and Kurth (2002),Kurth(2002),Clausen and\nKurth(2002)\n4.2 C-Brahms\nURL: http://www.cs.helsinki.fi/group/\ncbrahms/demoengine/\nC-Brahms employs nine different algorithms called P1,\nP2, P3, MonoPoly, IntervalMatching, ShiftOrAnd, Poly-\nCheck, Splitting, and LCTS offering various combina-\ntionsofmonophony,polyphony,rhythminvariance,trans-\nposition invariance, partial or exact matching.\nLiterature: Ukkonen et al. (2003),Lemstr¨om and\nTarhio(2003),Lemstr¨om et al.(2003)\n3PCM (Pulse Code Manipulation): raw uncompressed digital audio encoding.\n155Table 1: Content-based Music Information Retrieval systems.\nInput Matching Features\nName\nAudio\nSymbolic\nAudio\nSymbolic\nExact\nApproximate\nPolyphonic\nAudio Fingerprints\nPitch\nNote Duration\nTimbre\nRhythm\nContour\nIntervals\nOtherIndexingCollection\nSize\n(Records)\naudentify! • • • • •Inverted\nﬁles15,000\nC-Brahms • • • • • • • • • none 278\nCubyHum • • • • LET 510\nCuidado • • • • • • •not de-\nscribedworks\nfor >\n100,000\nGUIDO/\nMIR• • • • • • • •Tree of\ntransition\nmatrices150\nMeldex/\nGreenstone• • • • • • none 9,354\nMusipedia • • • • •Vantage\nobjects>30,000\nnotify!\nWhistle• • • • • •Inverted\nﬁles2,000\nOrpheus • • • • • • • •Vantage\nobjects476,000\nProbabilistic\n“Name That\nSong”• • • • •Clustering 100\nPROMS • • • • • •Inverted\nﬁles12,000\nCornell’s\n“QBH”• • • • none 183\nShazam • • • • •Fingerprints\nareindexed>2.5\nmillion\nSOMeJB • • • • •Tree 359\nSoundCompass • • • • • Yes 11,132\nSuper MBox • • • • •Hierarchical\nFiltering12,000\nThemeﬁnder • • • • • • none 35,000\n4.3 CubyHum\nEdit distances of one-dimensional pattern sequences\n(here: pitch intervals) are calculated. Nine interval\nclasses are used; intervals above 6 semitones are not dis-\ntinguished. Filtering is done with the LET algorithm\n(Chang and Lawler ,1994) with some heuristic adjust-\nments. CubyHumstilllooksateverysingledatabaseentry\nin every search. Literature: Pauws(2002)\n4.4 Cuidado Music Browser\nBesides similarity measures based on intrinsic features\nsuch as rhythm, energy, and timbre, there are also simi-\nlarity measures based on metadata. A co-occurrence ma-\ntrix keeps track of similar contexts like a radio program,\nalbum playlist, or web page. The authors do not describe\nan indexing method.Literature: Pachet(2003),Pachet et al. (2003b),Pachet\net al.(2003a)\n4.5 GUIDO/MIR\nURL:http://www.informatik.tu-darmstadt.\nde/AFS/GUIDO/index.html\nQueries are a combination of melodic (absolute pitch,\nintervals, interval types, interval classes, melodic trend)\nandrhythmicinformation(absolutedurations,relativedu-\nrations, trend). First-order Markov chains are used for\nmodeling the melodic and rhythmic contours of mono-\nphonic pieces of music. There is one Markov chain for\neach piece and each melodic or rhythmic query type. The\nstates of these chains correspond with melodic or rhyth-\nmic features.\n156Transition matrices are organized as a tree (leaves:\npieces; inner nodes: transition matrices describing the\nconcatenation of the pieces in the subtree) with the aim\nof ruling out data with transition probabilities of zero at\nan early stage of the search, and heuristically guiding the\nsearch.\nLiterature: Hoos et al. (2001)\n4.6 Meldex/Greenstone\nURL: http://www.nzdl.org/fast-cgi-\nbin/music/musiclibrary\nMeldexusestwomatchingmethods: Editingdistancecal-\nculationwithdynamicprogrammingandastatematching\nalgorithm for approximate searching ( Wu and Manber ,\n1992). ThefolksongcollectionisbasedontheEssenand\nDigital Tradition collections.\nLiterature: McNab et al. (May 1997 ),Bainbridge et al.\n(2004)\n4.7 Musipedia\nURL: http://musipedia.org\nThesearchengineretrievestheclosest100entriesaccord-\ning to the editing distance of gross contour strings. The\ncollection can be edited and expanded by any user. For\nindexing, the vantage object method described by Typke\net al.(2004) is used for the ﬁrst 6 characters of the con-\ntour string. Musipedia was known as “Tuneserver” in an\nearlier development state.\nLiterature: Prechelt and Typke (2001)\n4.8 notify! Whistle\nURL: http://www-mmdb.iai.uni-bonn.de/\nprojects/nwo/index.html\nMonophonic queries are matched against polyphonic sets\nofnotes. Arhythmtrackerenablesmatchingevenifthere\nareﬂuctuationsordifferencesintempo. Theaudioqueries\ncan be symbolically editedin pianoroll notation.\nLiterature: Kurth et al. (2002a)\n4.9 Orpheus\nURL: http://give-lab.cs.uu.nl/orpheus/\nQueries can be polyphonic. Notes are represented as\nweighted points in the 2-dimensional space of onset time\nand pitch. The Earth Mover’s Distance or variants of it\nare used for calculating distances. For indexing, vantage\nobjects are used.\nLiterature: Typke et al. (2003),Typke et al. (2004)\n4.10 Probabilistic “Name That Song”\nThissystemusesnotonlymusic,butalsolyricsformatch-\ning. All note transitions and words from the query must\noccur at least once in a piece for it to be considered a\nmatch. Thepiecesinthedatabaseareclustered. Theprob-\nability of sampling is computed for each cluster. A query\nis then performed in iiterations. In each iteration, a clus-\nterisselectedandthematchingcriteriaareappliedtoeachpieceinthisclusteruntilamatchisfound,whichthenbe-\ncomes the rank- ith result.\nTheclusteringpreventsthealgorithmfromvisitingev-\nery single piece in the database.\nLiterature: Brochu and de Freitas (2002)\n4.11 PROMS\nURL: http://www-mmdb.iai.uni-bonn.de/\nforschungprojekte/midilib/\nPROMS views database entries and queries as sets of\nnotes. Matches are supersets of queries. Queries can be\nfuzzy (a set of ﬁnite, nonempty sets of possible notes\ninstead of a set of notes).\nPROMS relies on measure information for segment-\ning and quantizes pitches and onset times. This makes it\npossible to use inverted ﬁles.\nLiterature: Clausen et al. (2000)\n4.12 Cornell’s “Query by Humming”\nURL: http://www.cs.cornell.edu/Info/\nFaculty/bsmith/query-by-humming.html\nAfter pitch tracking with autocorrelation, maximum like-\nlihood,orcepstrumanalysis,thegrosscontourisencoded\nwith the alphabet U/D/S (up/down/same). The Baeza-\nYates/Perleberg pattern matching algorithm is then used\nfor ﬁnding all instances of a pattern string in a text string\nso that there are at most kmismatches.\nLiterature: Ghias et al. (1995)\n4.13 Shazam\nURLs: http://www.shazam.com , http:/\n/ismir2003.ismir.net/presentations/\nWang.PDF\nAudioﬁngerprintsdescribetherelativetimeandpitchdis-\ntancesoffuturepeakswithinaﬁxed-sizetargetzonefora\ngivenpeakinthespectrum(“landmark”). Foralldatabase\nentries with ﬁngerprints that match some ﬁngerprints in\nthe query, it is checked whether they occur at the correct\nrelative times and at the correct landmarks. This method\nisveryrobustagainstnoiseanddistortioncausedbyusing\na mobile phone connection and added background noise.\nLiterature: Wang(2003)\n4.14 SOMeJB - The SOM-enhanced JukeBox\nURL: http://www.ifs.tuwien.ac.at/˜\nandi/somejb/\nA Self-Organizing Map (SOM) is used for clustering\npieces. The SOM consists of units which are ordered\non a rectangular 2-dimensional grid. A model vector in\nthe high-dimensional data space is assigned to each of\nthe units. During the training, the model vectors are ﬁt-\nted to the data such that the distances between the data\nitems and the corresponding closest model vectors are\nminimized. Feature vectors contain amplitude values for\nselected frequency bands.\nTraining the neural network, i.e. the Growing Hier-\narchical Self-Organizing Map (GHSOM), an extension to\nthe SOM, results in a hierarchical organization.\n157copyright and royalties\nplagiarism\nrecommendation\nsounds as\nmood\nemotion\nstyle\nperformer\nfeature\ncomposerintertextualityidentiﬁcation\nsource\nindustry consumer professional\n(researcher/performer)genre artist work instance(performer/composer)\nShazam\nSOMeJB\nMusipedia,Themeﬁnder,Orpheus, PROMS,notify, QBH, Meldex,\nGUIDO/MIR,\nC-BrahmsCubyHum,SoundCompass,\nSuperMBoxCuidado audentifyFigure 1: A mapping of MIRsystems to retrieval tasks. See Section 5for a discussion.\nLiterature: Rauber et al. (2003),Pampalk et al. (2002),\nRauber et al. (2002b),Rauber et al. (2002a),Rauber and\nFr¨uhwirth(2001)\n4.15 SoundCompass\nUsers ﬁrst set a metronome to a convenient tempo and\nthen hum their melody so that the beats coincide with\nmetronomeclicks. Threefeaturevectors(ToneTransition,\nPartial Tone Transition, Tone Distribution) are stored for\noverlappingwindowscoveringthesongs(16beatslong,4\nbeats apart from each other). Euclidean distance calcula-\ntion, accelerated with an index.\nLiterature: Kosugi et al. (2000)\n4.16 Super MBox\nURL: http://neural.cs.nthu.edu.tw/\njang/demo/\nTheacousticinputisconvertedintoapitchsequencewith\na time scale of 1/16 second. Dynamic time warping is\nused to compute the warping distance between the input\npitch vector and that of every song in the database.\nLiterature: Jang et al. (2001)\n4.17 Themeﬁnder\nURL:http://themefinder.org\nThemeﬁnder provides a web-based interface to the\nHumdrum thema command, which allows searching of\ndatabases containing musical themes or incipits with\nstring matching algorithms.\nLiterature: Kornst¨adt(1998)5 RETRIEVAL TASKS\nIn the introduction, we mentioned a number of MIR re-\ntrieval tasks. It is worthwhile to map the systems to these\ntasks. Threemainaudiencescanbedistinguishedthatcan\nbeneﬁt from MIR:\n1.industry: e. g. recording, broadcasting, performance\n2.consumers\n3.professionals: performers, teachers, musicologists\nThe level at which retrieval is needed may differ consid-\nerably:\n1.work instance: the individual score or sound object\n2.work: set of instances that are considered to be es-\nsentially the same\n3.artist: creator or performer of work\n4.genre: music that is similar at a very generic level,\ne. g. classical, jazz, pop, world music\nThisisnotreallyastricthierarchy. Artistsperformindif-\nferent genres, and one work can be performed, even cre-\nated,bymultipleartists. Also,thereisratheracontinuum.\nGenres can be divided into subgenres, artists grouped in\nschools. Even the “work” concept is not a ﬁxed given.\nBeethoven’s Third Symphony, for example is determined\nby the composer’s score, and changing even one note can\nbe a violation of the work, for example the famous “false\nentry” of the French Horn at the beginning of the reca-\npitulation. On the other hand, different renditions of “I\ndiditmyway”areusuallyconsideredthesameworkeven\nthough the musical content may be rather different.\n158MIR retrieval tasks can be characterised by audience\nand level of retrieval. Often, tasks connect a subrange\nof the continuum (see Figure 1). A non-comprehensive\noverview of tasks (for typical search tasks and their fre-\nquencies of occurence, see also Lee and Downie (2004))\nincludes:\n•copyright and royalties: receive payments for broad-\ncast or publication of music\n•detection of plagiarism: the use of musical ideas or\nstylistictraitsofanotherartistunderone’sownname\n•recommendation: ﬁnd music that suits a personal\nproﬁle\n•soundsas: ﬁndmusicthatsoundslikeagivenrecord-\ning\n•mood: ﬁnd music that suits acertain atmosphere\n•emotion: ﬁnd music that reﬂects or contradicts an\nemotional state\n•style: ﬁnd music that belongs to a generic category,\nhowever deﬁned\n•performer: ﬁnd music by (typeof) performer\n•feature: employ technical features to retrieve works\nin a genre or by an artist\n•composer: ﬁnd works by one composer\n•intertextuality: ﬁnding works that employ the same\nmaterial or refer to each other by allusion\n•identiﬁcation: ascribing a work or work instance to\nan artist or ﬁnding works containing a given theme,\nquery by humming\n•source: identifyingtheworktowhichaninstancebe-\nlongs, for example because metadata are missing\nFigure1showshowtheMIRsystemsfromTable 1can\nbemappedtothetasks. Audioﬁngerprintingsystemssuch\nasShazamareparticularlygoodatidentifyingrecordings,\nthat is, instances of works. This task must be based on\naudio information because in two different performances,\nthe same music might be performed, and therefore only\nthe audio information is different.\nAudio data is also a good basis for very general iden-\ntiﬁcation tasks such as genre and artist. SOMeJB and\nCuidado both use audio features for this purpose. Since\nit uses metadata, Cuidado can also cover tasks for which\nit helps to know the artist.\nQuery-by-humming systems such as SoundCompass,\nwhichisintendedtobeusedinaKaraokebar,makeiden-\ntiﬁcation tasks easier for consumers who might lack the\nexpertise that is needed for entering a sequence of inter-\nvals or a contour in textual form. These systems focus on\nidentifying works or ﬁnding works that are similar to a\nquery.\nBy offering the possibility of entering more complex\nqueries, systems such as Themeﬁnder, C-Brahms, and\nMusipedia cover a wider range of tasks, but they still can\nonly be used on the work level. Since they work with sets\nof notes or representations that are based on sets of notes,\nthey cannot be used for more speciﬁc tasks such as iden-\ntifyinginstances,andtheiralgorithmsarenotmeanttodo\ntasks on the more general artist and genre levels.6 CONCLUSIONS\nWeprobablycoveredonlyasmallpartofallexistingMIR\nsystems(weleftsomecommercialsystemsout,forexam-\nple MuscleFish’s SoundFisher, because we could not ﬁnd\nresearch papers about them), but we can still draw some\nconclusions from this survey.\nAgreatvarietyofdifferentmethodsforcontent-based\nsearching in music scores and audio data has been pro-\nposed and implemented in research prototypes and com-\nmercial systems. Besides the limited and well-deﬁned\ntaskofidentifyingrecordings,forwhichaudioﬁngerprint-\ning techniques work well, it is hard to tell which meth-\nodsshouldbefurtherpursued. Thisunderlinestheimpor-\ntanceofaTREC-likeseriesofcomparisonsforalgorithms\n(such as EvalFest/MIREX at ISMIR) for searching audio\nrecordings and symbolic musicnotation.\nAudio and symbolic methods are useful for different\ntasks. For instance, identiﬁcation of instances of record-\nings must be based on audio data, while works are best\nidentiﬁed based on a symbolic representation. For deter-\nmining the genre of a given piece of music, approaches\nbased on audio look promising, but symbolic methods\nmight work as well.\nFigure1shows that most MIR systems focus on the\nwork level. There is a gap between MIR systems work-\ning on the genre level and those on the work level. Large\npartsofthemoreinterestingtasks,suchasspeciﬁcrecom-\nmendation, generic technical features, and intertextuality,\nfall into this gap. Using metadata might help cover this\ngap,butthiswouldruleoutthepossibilityofhandlingdata\nfor which the quality of known metadata is not sufﬁcient.\nManual annotation quickly gets prohibitively expensive.\nToﬁllthegapwithcompletelyautomaticsystems,itmight\nbe necessary to ﬁnd algorithms for representing music at\na higher, more conceptual abstraction level than the level\nof notes.\nACKNOWLEDGEMENTS\nWe thank everybody who ﬁlled in the questionnaire on\nhttp://mirsystems.info .\nREFERENCES\nD. Bainbridge, S. J. Cunningham, and J. S. Downie.\nGreenstone as a music digital library toolkit. In ISMIR\nProceedings , pages 42–43, 2004.\nW. Birmingham, C. Meek, K. O’Malley, B. Pardo, and\nJ. Shifrin. Music information retrieval systems. Dr.\nDobb’s Journal , Sept. 2003.\nE. Brochu and N. de Freitas. “Name That Song!”: A\nprobabilistic approach to querying on music and text.\nNIPS.Neural Information Processing Systems: Natural\nand Synthetic , 2002.\nD.ByrdandT.Crawford. Problemsofmusicinformation\nretrieval in the real world. Information Processing and\nManagement , 38:249–272, 2002.\nW. I. Chang and E. L. Lawler. Sublinear approximate\nstring matching and biological applications. Algorith-\nmica, 12(4/5):327–344, 1994.\n159M. Clausen, R. Engelbrecht, D. Meyer, and J. Schmitz.\nPROMS: a web-based tool for searching in polyphonic\nmusic. In ISMIR Proceedings , 2000.\nM. Clausen and F. Kurth. A uniﬁed approach to content\nbasedandfaulttolerantmusicidentiﬁcation.In Interna-\ntional Conference On Web Delivering of Music. , 2002.\nJ. S. Downie. Evaluating a simple approach to music\ninformation retrieval: Conceiving melodic n-grams as\ntext.PhD thesis, University of Western Ontario, Lon-\ndon, Ontario, Canada, 1999.\nJ. S. Downie. Music information retrieval. Annual Re-\nview of Information Science and Technology , 37:295–\n340, 2003.\nA. Ghias, J. Logan, D. Chamberlin, and B. C. Smith.\nQuery by humming - musical information retrieval in\nan audio database. In Proceedings ACM Multimedia ,\n1995.\nH. Hoos, K. Renz, and M. G ¨org. GUIDO/MIR - an ex-\nperimental musical information retrieval system based\non guido music notation. In ISMIR Proceedings , pages\n41–50, 2001.\nJ.-S. Jang, H.-R. Lee, and J.-C. Chen. Super MBox: An\nefﬁcient/effectivecontent-basedmusicretrievalsystem.\nIn9th ACM Multimedia Conference , pages 636–637,\n2001.\nA. Kornst ¨adt. Themeﬁnder: A web-based melodic search\ntool. In W. Hewlett and E. Selfridge-Field, editors,\nMelodic Similarity: Concepts, Procedures, and Appli-\ncations, Computing in Musicology , volume 11. MIT\nPress, Cambridge, 1998.\nN. Kosugi, Y. Nishihara, T. Sakata, M. Yamamuro, and\nK.Kushima. Apracticalquery-by-hummingsystemfor\na large music database. In Proceedings ACM Multime-\ndia, pages 333–342, 2000.\nF. Kurth. A ranking technique for fast audio identiﬁca-\ntion. InInternational Workshop on Multimedia Signal\nProcessing. , 2002.\nF. Kurth, A. Ribbrock, and M. Clausen. Efﬁcient fault\ntolerant search techniques for full-text audio retrieval.\nIn112th Convention of the Audio Engineering Society ,\n2002a.\nF. Kurth, A. Ribbrock, and M. Clausen. Identiﬁcation of\nhighly distorted audio material for querying large scale\ndatabases. In 112thConventionoftheAudioEngineer-\ning Society , 2002b.\nJ. H. Lee and J. S. Downie. Survey of music informa-\ntion needs, uses, and seeking behaviours: Preliminary\nﬁndings. In ISMIR Proceedings , pages 441–446, 2004.\nK. Lemstr ¨om, V. M ¨akinen, A. Pienim ¨aki, M. Turkia, and\nE. Ukkonen. The C-BRAHMS project. In ISMIR Pro-\nceedings, pages 237–238, 2003.\nK. Lemstr ¨om and J. Tarhio. Transposition invariant pat-\nternmatchingformulti-trackstrings. NordicJournalof\nComputing , 2003.\nMcNab,Smith,Bainbridge,andWitten.TheNewZealand\ndigital library MELody inDEX. D-Lib Magazine , May\n1997.F. Pachet. Content management for electronic music dis-\ntribution. CACM, 46(4):71–75, 2003.\nF. Pachet, A. Laburthe, and J.-J. Aucouturier. The\nCuidado Music Browser: An end-to-end EMD system.\nInProceedings of the 3rd International Workshop on\nContent-Based Multimedia Indexing , 2003a.\nF.Pachet,A.Laburthe,andJ.-J.Aucouturier. Popularmu-\nsicaccess: TheSonyMusicBrowser. JournalofAmer-\nican Society for Information Science , 2003b.\nE. Pampalk, A. Rauber, and D. Merkl. Content-based or-\nganization and visualization of music archives. In Pro-\nceedings of ACM Multimedia , pages 570–579, 2002.\nS. Pauws. CubyHum: a fully operational query by hum-\nming system. In ISMIR Proceedings , pages 187–196,\n2002.\nL. Prechelt and R. Typke. An interface for melody input.\nACM Transactions on Computer-Human Interaction , 8\n(2):133–149, 2001.\nA. Rauber and M. Fr ¨uhwirth. Automatically analyzing\nandorganizingmusicarchives. In Proceedingsofthe5.\nEuropeanConferenceonResearchandAdvancedTech-\nnology for Digital Libraries , Lecture Notes in Com-\nputer Science. Springer, 2001.\nA.Rauber,E.Pampalk,andD.Merkl. Content-basedmu-\nsicindexingandorganization. In Proceedingsofthe25.\nACMSIGIRConferenceonResearchandDevelopment\nin Information Retrieval , pages 409–410, 2002a.\nA. Rauber, E. Pampalk, and D. Merkl. Using psycho-\nacoustic models and self-organizing maps to create a\nhierarchical structuring of music by musical styles. In\nISMIR Proceedings , pages 71–80, 2002b.\nA. Rauber, E. Pampalk, and D. Merkl. The SOM-\nenhanced jukebox: Organization and visualization of\nmusic collections based on perceptual models. Journal\nofNewMusicResearch(JNMR) ,32(2):193–210,2003.\nA. Ribbrock and F. Kurth. A full-text retrieval approach\nto content-based audio identiﬁcation. In International\nWorkshop on Multimedia Signal Processing , 2002.\nR. Typke, P. Giannopoulos, R. C. Veltkamp, F. Wiering,\nandR.vanOostrum. Usingtransportationdistancesfor\nmeasuring melodic similarity. In ISMIR Proceedings ,\npages 107–114, 2003.\nR. Typke, R. C. Veltkamp, and F. Wiering. Searching no-\ntated polyphonic music using transportation distances.\nInProceedings of the ACM Multimedia Conference ,\npages 128–135, New York, 2004.\nE. Ukkonen, K. Lemstr ¨om, and V. M ¨akinen. Sweepline\nthe music! Computer Science in Perspective , pages\n330–342, 2003.\nA. Wang. An industrial strength audio search algorithm.\nInISMIR Proceedings , Baltimore, 2003.\nE. Wold, T. Blum, D. Keislar, and J. Wheaton. Content-\nbased classiﬁcation, search, and retrieval of audio.\nIEEE Multimedia , 3(3):27–36, 1996.\nS.WuandU.Manber. Fasttextsearchingallowingerrors.\nCACM, 35(10):83–89, 1992.\n160"
    },
    {
        "title": "Separation of Vocals from Polyphonic Audio Recordings .",
        "author": [
            "Shankar Vembu",
            "Stephan Baumann 0001"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414852",
        "url": "https://doi.org/10.5281/zenodo.1414852",
        "ee": "https://zenodo.org/records/1414852/files/VembuB05.pdf",
        "abstract": "Source separation techniques like independent component analysis and the more recent non-negative matrix factorization are gaining widespread use for the monaural separation of individual tracks present in a music sample. The underlying principle behind these approaches characterises only stationary signals and fails to separate nonstationary sources like speech or vocals. In this paper, we make an attempt to solve this problem and propose solutions to the extraction of vocal tracks from polyphonic audio recordings. We also present techniques to identify vocal sections in a music sample and design a classifier to perform a vocal–nonvocal segmentation task. Finally, we describe an application wherein we try to extract the melody from the separated vocal track using existing monophonic transcription techniques. The experimental work leads us to the conclusion that the quality of vocal source separation, albeit satisfactory, is not sufficient enough for further F0 analysis to extract the melody line from the vocal track. We identify areas that need further investigation to improve the quality of vocal source separation. Keywords: Blind source separation, independent component analysis, non-negative matrix factorization, vocal– nonvocal discrimination, melody extraction. 1",
        "zenodo_id": 1414852,
        "dblp_key": "conf/ismir/VembuB05",
        "keywords": [
            "Blind source separation",
            "Independent component analysis",
            "Non-negative matrix factorization",
            "Vocal–nonvocal discrimination",
            "Melody extraction",
            "Vocal source separation",
            "Speech",
            "Nonstationary sources",
            "Polyphonic audio recordings",
            "F0 analysis"
        ],
        "content": "SEPARATION OF VOCALS FROM POLYPHONIC AUDIO RECORDINGS\nShankar Vembu\nGerman Research Centre for AI\nErwin-Schroedinger-Str. 57\n67663 Kaiserslautern, Germany\nshankar.vembu@dfki.deStephan Baumann\nGerman Research Centre for AI\nErwin-Schroedinger-Str. 57\n67663 Kaiserslautern, Germany\nstephan.baumann@dfki.de\nABSTRACT\nSource separation techniques like independent component\nanalysis and the more recent non-negative matrix factor-\nization are gaining widespread use for the monaural sep-\naration of individual tracks present in a music sample.\nThe underlying principle behind these approaches char-\nacterises only stationary signals and fails to separate non -\nstationary sources like speech or vocals. In this paper, we\nmake an attempt to solve this problem and propose so-\nlutions to the extraction of vocal tracks from polyphonic\naudio recordings. We also present techniques to iden-\ntify vocal sections in a music sample and design a clas-\nsiﬁer to perform a vocal–nonvocal segmentation task. Fi-\nnally, we describe an application wherein we try to extract\nthe melody from the separated vocal track using existing\nmonophonic transcription techniques. The experimental\nwork leads us to the conclusion that the quality of vo-\ncal source separation, albeit satisfactory, is not sufﬁcie nt\nenough for further F0 analysis to extract the melody line\nfrom the vocal track. We identify areas that need further\ninvestigation to improve the quality of vocal source sepa-\nration.\nKeywords: Blind source separation, independent com-\nponent analysis, non-negative matrix factorization, voca l–\nnonvocal discrimination, melody extraction.\n1 INTRODUCTION\nAnalysing an auditory scene and identifying the various\nsounds present in it has, for a long time, been the primary\nfocus of the research ﬁeld called computational auditory\nscene analysis (CASA). Most of the approaches in this\nﬁeld draw inspiration from the works of Bregman (1990)\nwho describes a set of psychoacoustic grouping cues that\ncould be used in the analysis and segregation of individ-\nual sources present in a mixture of sounds using signal\nPermission to make digital or hard copies of all or part of thi s\nwork for personal or classroom use is granted without fee pro -\nvided that copies are not made or distributed for proﬁt or com -\nmercial advantage and that copies bear this notice and the fu ll\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londonprocessing techniques. A signiﬁcant amount of work in\nthe ﬁeld of CASA can be found in the doctoral theses\nof Mellinger (1991), Cooke (1991), Brown (1992), Ellis\n(1996) and Martin (1999).\nThe works of Attneave (1954) and Barlow (1959) re-\nvealed the fact that redundancy reduction is an inherent\nmechanism taking place in the sensory organs and that the\nhuman brain analyses an input scene (for example, visual)\nby exploiting the statistical regularities present in it. I n\nrecent times, a lot of effort is being expended in sound\nsource separation using statistical techniques like princ i-\npal component analysis (PCA) and independent compo-\nnent analysis (ICA) (Comon, 1989) for redundancy re-\nduction mostly inspired by the works of Casey and West-\nner (2000) and Smaragdis (2001). It is interesting to note\nthat there are two parallel strands of research sharing the\nsame goals, one of them attempting to solve the source\nseparation problem using classical signal processing tech -\nniques and pyschoacoustic studies while the other trying\nto achieve the same using statistical techniques. A formal\nanalysis and comparison of the results obtained from these\nresearch ﬁelds is yet to be done.\nIn this paper, we focus on a particular problem that\narises when employing statistical techniques like ICA for\nmonaural source separation, which is the inability of these\nmodels to separate non-stationary sources. Attempts to\nsolve this problem have been made by Casey and West-\nner (2000) and Smaragdis (2004b). Smaragdis proposes\nan extension to non-negative matrix factorization (NMF)\n(Lee and Seung, 2001) called non-negative matrix decon-\nvolution in which an individual non-stationary source is\ncharacterised by a set of time-dependent spectral bases.\nThis is unlike the basic model that characterises each\nsource using a single spectral basis and thus fails to sepa-\nrate non-stationary sources that necessarily should be rep -\nresented using a time-varying spectral basis. Casey as-\nsumes that non-stationary sources remain stationary for\nsmall intervals of time and proceeds with the usual anal-\nysis in its basic setting. He then proposes a cluster-\ning mechanism to ﬁnally group the resulting components\n(spectral bases) over time. In this paper, we identify the\nramiﬁcations of using statistical techniques like ICA or\nNMF in their basic setting while trying to separate the\nvocal track from polyphonic music samples with a single\nvoice. We propose speciﬁc solutions to handle the separa-\ntion of vocals from a given sound mixture.\n337An important application of vocal source separation is\nthe extraction of melody. Based on the assumption that\nvocals carry the main melody, one could apply mono-\nphonic transcription techniques to extract the melody from\nthe separated vocal track. This is an enticing application\nas monophonic transcription is much more simpler when\ncompared to polyphonic music transcription and there-\nfore the harder problem of extracting melody directly us-\ning polyphonic transcription techniques is bypassed. The\nresults of this work could also be used in the design\nof query-by-humming systems. Existing systems try to\nbuild a database of melodies by collecting annotations in\nthe MIDI format or by manually transcribing polyphonic\nmusic samples. This database is then used for making\ncomparisons with the input query. V ocal source separa-\ntion from polyphonic recordings and hence the extracted\nmelody could therefore be used in the automatic creation\nof melody databases.\nThis paper is organised as follows: In Section 2, we\npresent a vocal–nonvocal discrimination module and out-\nline the principles behind monaural source separation us-\ning statistical techniques. In Section 3, we identify prob-\nlems encountered when trying to separate non-stationary\nsources like vocals and propose solutions to mitigate these\nproblems. Section 4 deals with the experimental work.\nWe point to directions for future work in Section 5. Sec-\ntion 6 concludes the paper.\n2 MONAURAL SOURCE SEPARATION\nOF VOCALS\nThe different stages in the design of a source separation\nsystem for vocals are shown in Figure 1. The last mod-\nule is our proposed solution to separate non-stationary\nsources and we defer its description until the next section.\nThe rest of the stages are described below.\n(STFT, wavelets)Time−Frequency\nof the signalrepresentationS\nSN2S1\nSV.wavVocal−\nNonvocal\ncomponentsAutomatic\nvocalSource separation\nidentification of\n(ICA, NMF)reductionusing\nredundancy\nclassifier\nVocal track\nFigure 1: Building blocks of the vocal source separation\nsystem\n2.1 Vocal–nonvocal Discrimination\nSince we are interested only in the separation of vocals,\nit is essential to have a pre-processing stage that performs\na vocal–nonvocal discrimination task to ﬁlter out sections\nthat contain only the nonvocal, instrumental tracks. We\nidentiﬁed three features as useful candidates in the design\nof a vocal–nonvocal classiﬁer. Mel-frequency cepstral co-\nefﬁcients (MFCC) (Davis and Mermelstein, 1980) were\nused in the design of a classiﬁer to identify vocals and\ninstrumental sections in a music sample (Maddage et al.,\n2003). Perceptual linear predictive coefﬁcients (PLP) in-\ntroduced by Hermansky (Hermansky, 1990) were used byBerenzweig et al. (2002) to train a neural network clas-\nsiﬁer for distinguishing vocals from instrumental music.\nLog frequency power coefﬁcients (LFPC) were shown to\nbe a useful feature in discriminating vocals with instru-\nments from pure instruments (Nwe and Wang, 2004). In\nthis paper, we report experiments using different com-\nbinations of these features and train two classiﬁer mod-\nels; namely neural networks and support vector machines.\nBased on the experimental results, we arrive at the conclu-\nsion that a combination of all the aforementioned features\nproduces the best classiﬁer performance. The experimen-\ntal results of this module also paved way to solutions for\nthe separation of non-stationary vocal sources that will be -\ncome clear in Section 3.\n2.2 Monaural Source Separation: Basic Idea\nThe underlying principle behind these approaches (Casey\nand Westner, 2000; Smaragdis, 2001) is to apply re-\ndundancy reduction techniques on the time–frequency\nrepresentation of signals leading to the separation of the\nindividual sources present in the input mixture of sounds.\nWe brieﬂy outline the important steps.\nStep 1: The ﬁrst step is to project the input signal\ns(t)into the time–frequency plane using an invertible\ntransformψlike short time Fourier transform or wavelets,\ngiving rise to an n×kmatrixF, wherendenotes the\nnumber of frequency channels and kis the number of\ntime frames:\nψ:s(t)→F. (1)\nStep 2: The next step is to whiten the matrix Fusing\nPCA; this results in a matrix with uncorrelated rows. The\ndimension of this matrix is reduced by retaining only those\nrows that carry maximal information in terms of their vari-\nance contribution. The reduced dimension ris determined\nby using a threshold φ∈[0,1]and the following inequal-\nity:/summationtextr\ni=1ei/summationtextn\ni=1ei≥φ, (2)\nwhereeiare the eigenvalues of the covariance matrix of\nF. The resulting matrix Fwis of dimension r×kwhere\nr < n . This step also gives rise to a whitening matrix\nWof dimension r×nand its pseudo-inverse called the\ndewhitening matrix W+.\nStep 3: The next step is to exploit the higher-order\nstatistics of the matrix Fwusing ICA resulting in an r×k\nmatrixGwith independent rows. We call this matrix as\nthe matrix with time-varying gain of the spectrum of the\nindividual sources in the row vectors. The ICA operation\ngives rise to a transformation matrix Iof dimension r×r.\nMultiplying the dewhitening matrix W+withIgives rise\nto a mixing matrix Bof dimension n×r:\nB=W+·I. (3)\nWe call this matrix the matrix with the spectral bases of\nthe individual sources in column vectors.\n338The stages described so far have resulted in two\nimportant matrices that will be used for the resynthesis\nof the individual sources in the next step. They are the\nmatrixBwithrspectral bases and the matrix Gthat carry\nthe time-varying gain of the individual spectral bases.\nStep 4: This step involves the reconstruction of the\nsources present in the original input mixture by taking the\nouter product of the individual column and row vectors of\nBandGrespectively. Inverse transform of the resulting\nmatrices gives the individual sources in the time-domain.\nThat is,\nFi=bi∗giand\nψ−1:Fi→si(t),(4)\nwhere the superscript iis used to index the individual\nsources.\n2.3 ICA and NMF\nNon-negative matrix factorization (NMF) introduced by\nLee and Seung (2001) operates on simple non-negativity\nconstraints to arrive at reduced-rank factors of a given ma-\ntrix, and has recently been used for monaural source sep-\naration of acoustic inputs (Smaragdis, 2004a,b). There is\npsychological and physiological evidence for parts-based\nrepresentations in the brain, and NMF is one algorithm\nthat tries to emulate this process (Lee and Seung, 1999).\nThis makes it a suitable candidate to discover the indi-\nvidual objects (parts) present in an acoustic input. NMF\ncould be seen as a replacement to the ICA step described\nin the previous section. An important decision lies in\nchoosing the right value for the parameter rthat deter-\nmines the rank of the factorization. This could be done\nusing the same procedure that was adopted before to per-\nform dimensionality reduction of the uncorrelated time–\nfrequency matrix after PCA as shown in Equation 2.\n3 SEPARATION OF NON-STATIONARY\nSIGNALS\nA major shortcoming of the presented approaches to\nmonaural source separation is that each source is charac-\nterised by a single stationary spectral basis (column vecto r\nofB) and only its gain varies with time (row vector of G).\nThis implies that the current setting will not be able to\nseparate non-stationary signals that should necessarily b e\ndescribed by more than a single spectral basis as shown\nbelow:\nBi= [bi\n1,bi\n2,...,bi\nj],Bi⊂B;\nGi= [gi\n1,gi\n2,...,gi\nj],Gi⊂G;\nFi=Bi∗Giand\nψ−1:Fi→si(t),(5)\nwherej < r indicates the number of components needed\nto characterise an individual non-stationary source i,bi\n1..j\narejcolumn vectors and gi\n1..jarejrow vectors. Com-\nparing the above Equation with Equation 4, we note that\nfor non-stationary sources, we need to consider a set of\ncolumn vectors of Band a set of row vectors of Gfor the\nresynthesis. The ramiﬁcation of this shortcoming is thatthe vocal source tends to get distributed among a set of\nbases where each one of them contributes to the spectral\ncomposition of the entire vocal source. It is not possible\nto arrive at a single component describing the vocal track\nin its entirety unlike other stationary sources.\nTo illustrate the problem of separating non-stationary\nsources, we consider two mixtures. The ﬁrst one (sample\nID: a) is the drum track sequence used in Smaragdis\n(2001) that has three sources; namely, bass drum, snare\ndrum and hi-hat, whose spectrum remains fairly constant\nover time. The second example (sample ID: b) is a\nmixture consisting of a vocal track with a guitar accom-\npaniment. We sample the signal at 22.05 kHz, compute\ntheir short time Fourier transform (STFT) (512 frequency\nbins, Hamming window of length 128 samples, hop size\nof 64 samples) and perform PCA and dimensionality\nreduction on the resulting matrix. The results are shown\nin Table 1 in which the third column shows the reduced\ndimension of the matrix i.e., the number of retained\ncomponents and the last column shows the amount of\ninformation in terms of the variance contributed by the\ncorresponding eigenvalues. Since there are only three\nTable 1: Results of dimensionality reduction using PCA\non the STFT matrix of sound mixtures with stationary and\nnon-stationary sources\nSample ID Size Number of Variance(%)\n(in s) components ( r)\na 3 3 93.89\nb 3 2 72.25\nb 3 7 94.9\nsources in the drum track mixture, we retained only three\ncomponents after PCA and they corresponded to 94% of\nthe information content of the STFT matrix which might\nbe sufﬁcient to arrive at a satisfactory separation. But\nif we do the same for the second example that consists\nof vocals and retain only two components (vocal and\nguitar), we observe that they correspond to only 72%\nof the information content. Throwing away a lot of\ninformation in this way would deﬁnitely hinder the results\nof separation. On the other hand, we observe that we need\na total of seven components (instead of only two) to retain\n95% of the information as shown in Table 1. This is due\nto the non-stationary nature of the vocals that resulted in\nthe vocal spectra getting distributed among a multitude\nof components. When this is the case, there has to be\na mechanism by which it should be possible to identify\nand group these distributed components to form the ﬁnal\nindividual vocal source. This is a non-trivial task and we\nmake an attempt to solve this problem by proposing a\ncouple of solutions:\nSolution 1: One possible way to identify the dis-\ntributed vocal components is to reuse the vocal–nonvocal\nclassiﬁer described in an earlier section. We note that\nthe classiﬁer’s input is a feature vector consisting of\na combination of MFCC, LFPC and PLP coefﬁcients\ncomputed from the spectrum. We also note that one\nof the outputs from the source separation stage is the\n339matrixBthat contains the spectra of the sources. It is\ntherefore trivial to compute the same feature vector from\nthe individual column vectors of the matrix Bas shown\nbelow:\nϕ:b→bc,b∈ ℜn,bc∈ ℜp, (6)\nwhereϕis a mapping from the spectral basis bof\ndimensionn(spectral dimension) to the feature vector\nbcof dimension p(number of coefﬁcients in MFCC,\nPLP and LFPC). The individual feature vectors bccan be\npresented as inputs to the classiﬁer to identify as being a\nvocal or a nonvocal component. It is now a simple task\nto combine all the components that were classiﬁed as\nvocals resulting in a grouping of all the distributed vocal\ncomponents. This would eventually help us to arrive\nat matrices BvandGv(see Equation 5) that could be\nused to compute the individual vocal source vin the time\ndomain.\nSolution 2: Another solution is to rely on unsuper-\nvised learning algorithms to cluster the spectral bases\nin the matrix Binto two groups (vocal and nonvocal).\nInstead of using the spectra directly as feature vectors\nto the clustering algorithm, we could once again use\nthe mapping ϕin Equation 6 to compute features with\nMFCC, LFPC and PLP coefﬁcients. Since these coef-\nﬁcients are able to distinguish well between vocals and\nnonvocals, they provide a better parameterization of the\nspectrum. The resulting feature vector could also be\naugmented using additional information available in the\nmatrixGthat consists of the time-varying gain of the\nspectra, to provide more discriminatory power. Virtanen\n(2003) mentions a similar approach to group multiple\ncomponents per source using the independence of the\ntime-varying gain.\nWe validate the presented solutions in the experimen-\ntal section of this paper.\n4 EXPERIMENTS AND RESULTS\n4.1 Vocal–nonvocal Discrimination\nA random collection of 40 minutes of popular music dis-\ntributed uniformly between pure instrumental and vocals\n(with accompaniment) was used for the experiments. The\nentire database comprised of 240 audio ﬁles of around 5\nseconds each with vocals and 80 ﬁles of around 15 sec-\nonds each with pure instrumentals. The samples were\ncarefully hand-picked to be representative of both male as\nwell as female playback singers from Eastern and West-\nern music. All the audio ﬁles were 16-bit mono and sam-\npled at the rate of 22.05 kHz. The ﬁrst step was to com-\npute the short time Fourier transform (STFT) of the signal\nusing a window function. The window size was set to\n≈23ms and the amount of overlap was half of the win-\ndow size. Candidate features like PLP, MFCC and LFPC\nwere calculated from the STFT of the signal on a frame-\nby-frame basis resulting in a matrix representation of the\nsignal with feature vectors in the columns. Finally, an av-\nerage of 15 analysis frames was computed to reduce thecomputational load. Each resultant feature vector, thus,\nrepresented an analysis frame of length ≈184ms.\n4.1.1 Using Neural Networks\nWe trained a neural network using different combinations\nof the features namely MFCC, PLP and LFPC. The archi-\ntectural details of the network are given in Table 2 and the\nnetwork’s performance for various combinations of fea-\ntures is given in Table 3.\nTable 2: Neural network architectural details\nNo. of inputs 13 MFCC and/or 12 LFPC\nand/or 39 PLP\nNo. of outputs 2\nNo. of hidden layers 1\nTraining algorithm Resilient backpropagation\nwith early-stopping\nActivation function Sigmoidal\nEvaluation 10-fold cross-validation\nTable 3: Results of the vocal–nonvocal classiﬁer using\nneural networks\nFrame-based\nclassiﬁcation\nefﬁciency (%)\nBefore After\nFeature smoothing smoothing\nPLP 69.25 82.02\nMFCC 67.68 74.94\nLFPC 68.12 73.09\nPLP+MFCC 73.11 81.55\nPLP+LFPC 75.25 82.35\nMFCC+LFPC 71.69 78.65\nPLP+LFPC+MFCC 77.24 84.87\nIt can be seen from the results that combinations of\nfeatures give better performance when compared to in-\ndividual features. The best performance of 77.24% efﬁ-\nciency resulted when all the features were used as inputs\nto the network. The last column in Table 3 is the result of\na simple smoothing operation on the network output using\nan autoregressive low-pass ﬁlter.\n4.1.2 Using Support Vector Machines\nExperiments using neural networks led to the conclusion\nthat the network’s best performance resulted when using\na combination of PLP, MFCC and LFPC features. There-\nfore, the same features were used to train an SVM with\nan RBF kernel. An appropriate combination of the ker-\nnel parameter σand the penalty parameter C(Burges,\n1998) should be made. The optimal values were found\nusing cross-validation by repeating the experiments on\nvarious combinations of σandC. We ﬁrst performed\na coarse grid search in the region C= 216,214,...,2−4\nandσ= 24,22,...,2−10. The classiﬁer’s frame-based er-\nror rate was computed using 5-fold cross-validation. This\n340was followed by a ﬁne grid search in regions of best per-\nformance to arrive at the ﬁnal values for the parameters\nσandC. The grid search is a computationally intensive\noperation, and therefore only 30% of the original database\ncomprising of 40 minutes of audio recordings was used for\nthe model selection. The optimal choice of Candσwas\nﬁnally ﬁxed at 28and22respectively (see Table 4). The\nTable 4: Results of the vocal–nonvocal classiﬁer using\nSVM\nFeatures PLP + LFPC + MFCC\nKernel RBF\nParameter: C 28\nParameter:σ 22\nFrame-based classiﬁ-\ncation efﬁciency (%)93.47\nclassiﬁer was trained again with a larger database com-\nprising 30 minutes of audio recordings. We arrived at a\ngeneralization error of 6.53% that was computed using 5-\nfold cross-validation.\n4.2 Vocal Source Separation\nA major problem encountered while performing source\nseparation experiments is evaluation of the quality of\nseparation of the individual sources. It is difﬁcult to\ncome up with a good measure that describes the quality\nof source separation and that adheres well to the auditory\nperception due to information loss in the analysis process.\nIn our case, the evaluation problem is mitigated to some\nextent with an application that tries to extract the melody\nfrom the separated monophonic vocal track. This is\nbecause we decided to perform experiments on excerpts\nfrom the ISMIR 2004 melody extraction contest1that\nhad music samples with annotated vocals. This helped us\nto make comparisons of the extracted melody with the\nexisting annotations that served as a ground-truth. But for\nthe vocal source separation, we only present an analytical\ndescription of the experimental results.\nSetup: The source separation was performed using\nICA2and NMF in the MATLAB environment. Some\nof the experimental details are shown in Table 5. The\nTable 5: Experimental details of vocal source separation\nSample ID Size Number of Variance (%)\n(in s) components ( r)\n1 6.2 10 96.17\n2 3.5 10 98.19\n3 5.3 9 98.19\n4 5 9 98.12\n5 4.2 8 98.36\n6 5.1 9 98.17\nthird column refers to the number of components that\n1http://ismir2004.ismir.net/melody contest/results.html\n2http://www.cis.hut.ﬁ/projects/ica/fastica/were retained after the dimensionality reduction using\nPCA and were further analysed using ICA. In case of\nNMF, this value determined the rank rof the matrix\nfactorization. The value of rwas chosen such that\nthe PCA decomposition is overcomplete resulting in\nmaximal retained information (Uhle et al., 2003). For the\nexperiments, rwas determined by\nr= min/braceleftbigg\nrmax,min/braceleftbigg\nr|/summationtextr\ni=1ei/summationtextn\ni=1ei≥φ/bracerightbigg/bracerightbigg\n, (7)\nwhereφwas set to 0.98 (i.e., 98% variance) and rmax\nwas set to a value of 10 to make sure that we do not\nend up with too many components that might affect\nthe identiﬁcation and grouping of vocal components in\nthe later stage. The next column in Table 5 shows the\ninformation content of these components in terms of the\nvariance contribution of the corresponding eigenvalues.\nAll the excerpts were sampled at a rate of 22.05 kHz and\nthe short time Fourier transform (512 frequency bins,\nHamming window of length 128 samples, hop size of 64\nsamples) was used as the time–frequency representation.\nWe refrained from using wavelets even though this is a\nbetter time–frequency representation of the signal. This i s\nbecause wavelet coefﬁcients are not always non-negative\nand therefore cannot be used for source separation using\nNMF.\nValidation of proposed solutions: Unfortunately,\nreusing the vocal–nonvocal classiﬁer did not yield satis-\nfactory results. The reason could be that this classiﬁer\nwas trained to operate on inputs that were computed from\nspectra of clean signals. By this we mean spectra that\nprovided a holistic parameterisation of the inputs. But we\nnote that the output of the source separation algorithm\nresults in vocal source components with distributed and\nnoisy spectra, and therefore the classiﬁer was unable to\noperate on these inputs.\nClustering the parameterised spectra produced per-\nceptually satisfactory results. In most of the test cases,\nwe observed that the two output components from the\nclustering algorithm had, in one of them, the vocals\nas predominant source. We also noted that in a few\nexamples, the quality of separation was superior when\nwe augmented the feature vector bcwith the information\npresent in the corresponding time-varying gain g. In\nour experiments, we presented this information directly\nwithout extracting other information — for example, that\nuses the independence of the time-varying gains present\ninGas was done in Virtanen (2003). In most of the\nexamples, we used only the parameterised spectral bases\nobtained from the matrix Band we plan to investigate on\nextracting any other relevant information from the matrix\nGin the future.\nObservations and analysis: We analyse a particu-\nlar example in detail. The music sample is a 3.5 s excerpt\nconsisting of a vocal track accompanied with guitar. A\ntotal of 10 components were retained for analysis that\ncarried 98.19% of the information. It is not surprising\nto arrive at 10 components despite the fact that only\ntwo tracks were present in the music sample. This is\n341due to the non-stationary nature of the vocal track, the\nspectra of which were distributed among a multitude of\ncomponents. We performed a source separation using\nNMF (r= 10 ) and arrived at two matrices that had 10\nspectral bases and their corresponding time-varying gains .\nWe parameterised the spectra using MFCC, LFPC, PLP\ncoefﬁcients and the time-varying gain and clustered the\nresulting feature vectors. This resulted into two groups\nwith 7 and 3 components. The individual spectrograms\nwere determined using Equation 4. Inverse transform\nof these spectrograms gave rise to both the individual\nsources in the time domain. From Figure 2, we observe\nthat the guitar track (spikes) is clearly separated from the\nvocal track. The same can be observed from Figure 4 in\nwhich the formants are clearly visible. In Figure 5, we\nobserve the spectra of the guitar in the low frequency\nrange that is clearly missing from Figure 4 after careful\ninspection. The vocal track was constructed from the\nVocal + Guitar\nVocal\nGuitar\nFigure 2: V ocal source separation results. The waveforms\ndepict the energy of the signals in the time domain and\nscaled to fall in the range [−1,+1]\nTime (s)Frequency (Hz)\n0 0.5 1 1.5 2 2.5 30200040006000800010000\nFigure 3: Spectrogram of the mixture shown in Figure 2\nwith vocal and guitar sources\nTime (s)Frequency (Hz)\n0 0.5 1 1.5 2 2.5 30200040006000800010000\nFigure 4: Spectrogram of the extracted vocal from the\nmixture shown in Figure 2\nTime (s)Frequency (Hz)\n0 0.5 1 1.5 2 2.5 30200040006000800010000\nFigure 5: Spectrogram of the extracted guitar from the\nmixture shown in Figure 2\ncluster that had 7 components and the guitar track from\nthe one that had 3 components. An interesting observation\nis that the guitar track had remnants of vocals present in\nit (seen obscurely in Figure 5) but the instrument was\nclearly dominant. This is also the reason why the cluster\nwith the guitar track had 3 components instead of just 1.\nNevertheless, the separation was satisfactorily clean (the\nword clean is used subjectively as a result of listening\ntests) and one could easily distinguish the vocal track\nfrom the guitar track by listening to these tracks.\nWe present a few general observations from all the\nexperiments. In most of the cases NMF was found to\nproduce qualitatively better separation of the vocal sourc e\nwhen compared to ICA. One possible reason for ICA re-\nsulting in poor results when compared to NMF could be\nthe independence assumptions that might not be precisely\ntrue for the application at hand (Virtanen, 2004). On the\nother hand, NMF imposes only the less stringent non-\nnegativity constraints.\nIn all the experiments, the input signal was band-pass\nﬁltered in the range of 100 Hz and 3000 Hz as most of\n342the energy in the singing voice lies in this range. Due\nto this, we were able to remove the high frequency in-\nstruments and only those instruments whose spectra were\nfalling in the singing voice range remained unseparated.\nThis helped a lot in further stages of the analysis. Most\nimportantly, we observed that the number of components\n(r) retained for analysis after the PCA stage went down\nand this proved to be very useful when we were trying to\ngroup the components into vocals and nonvocals.\nAs already stated, the quality of the separation was as-\nsessed subjectively through listening tests. In most of the\ncases, both the components that resulted from the cluster-\ning stage had remnants of the nonvocal sections of the in-\nput mixture thereby not resulting in a perfect separation of\nvocal and nonvocal tracks. But only one of them had the\nvocal track predominant in it whose energy was sufﬁcient\nenough for perceiving it as a vocal track.\n4.3 Monophonic Transcription using MAMI\nWe supplement the source separation results by trying to\nextract the melody line from the separated monophonic\nvocal track. The vocal tracks obtained from the previous\nstage were transcribed using MAMI, which is a system\ndesigned for the monophonic transcription of singing\nvoices (Mulder et al., 2003). The output from this\nstage is a sequence of notes with their F0 estimates and\nonset/offset values or a MIDI ﬁle with the melody. For\ncomparing the results with the annotated melodies, we\nperformed simple computation of melodic similarities\nusing the MIDI toolbox (Eerola and Toiviainen, 2004).\nThe toolbox provides functions to calculate the distance\n(or similarity) between two melodies using a user-deﬁned\nrepresentation (distribution of pitch classes and note\ndurations, or melodic contour) and a distance measure\n(taxicab, Euclidean, cosine). The similarity can be scaled\nto range between 0 and 1, with 1 indicating perfect\nsimilarity. The results are shown in Table 6 where the\nnumbers indicate the similarities on a scale of 0 to 1 using\ntaxicab norm as the distance measure.\nTable 6: Results of the melodic similarity computations\nSample ID Similarity measure\nPitch Durational\ndistribution distribution Contour\n1 0.16 0.5727 0.325\n2 0.2371 0.9048 0.6188\n3 0.3781 0.3333 0.355\n4 0.6105 0.7 0.4\n5 0.3096 0.3333 0.5050\n6 0.4927 0.6825 0.3563\nObservations and analysis: The results are admit-\ntedly hazy. The melodic similarity comparisons based\non the durational distribution of notes produced the best\nresults with an average of 58.7% whereas the comparisons\nbased on pitch distribution and contour produced poor\nresults with an average of 36.5% and 42.7% respectively.\nThis should not come as a surprise owing to the fact thatthere is information loss mostly the pitch information\nduring the analysis process of the source separation\nstage. We performed the melody extraction experiments\nonly to get a quantitative indication of the quality of the\nseparated vocal track. There is still a long way to go\nbefore one is really able to extract the melody line using\nthese approaches and whose results could be compared\nto the existing melody extraction techniques that uses\ncomplex F0 estimation procedures.\n5 FUTURE WORK\nOur experiments provide a nice starting point to investi-\ngate more complex approaches to grouping the various\ncomponents that arise from the source separation stage\ninto vocals and non-vocals. Our primary intention was to\nfocus on techniques that allow us to identify and group the\nvocal track that appears distributed from the source sepa-\nration algorithm. It might be difﬁcult in the current set-\nting of monaural separation to arrive at the vocal source\nin a single component. Trying to inﬂuence the separation\nalgorithm (ICA or NMF) by the usage of prior informa-\ntion like vocal source models might not be of much help\nin this case, as anyhow we cannot obtain time-dependent\nspectra at the output. An interesting step forward would\nbe to use techniques like non-stationary ICA (Everson and\nRoberts, 1999) wherein the mixing matrix evolves over\ntime to give rise to time-dependent spectra that would suf-\nﬁce to characterise non-stationary signals. Another inter -\nesting direction would be to revisit our proposed solution\nto use a vocal–nonvocal classiﬁer as a means to identify\nand group vocal component spectra. Reusing the model\nof the classiﬁer described in this paper did not work, but\none could possibly design a robust classiﬁer that is an ac-\ncurate model of instruments only and that treats any other\ninputs as don’t-care . This classiﬁer would then classify\nthe distributed, noisy vocal spectra as negative inputs and\nclassify positively only the spectra of the instruments. We\ncould ﬁnally group all the negatively classiﬁed inputs to\ndetermine the vocal source.\n6 CONCLUSIONS\nMonaural source separation using statistical techniques\nfor redundancy reduction is gaining widespread use in the\nresearch community. The drawbacks of these approaches\nin separating non-stationary signals from sound mixtures\nwere identiﬁed and we proposed solutions to handle the\nnon-trivial problem of separating vocal tracks from poly-\nphonic music samples. Subjective evaluation of the exper-\nimental work indicated that the results are promising. We\nalso presented an application wherein we made an attempt\nto extract the melody from the separated monophonic vo-\ncal track that also served as a quantitative indicator of the\nquality of the source separation. We also presented ex-\nperiment work on discriminating vocal and nonvocal seg-\nments present in a music sample and arrived at encourag-\ning results.\n343REFERENCES\nF. Attneave. Some informational aspects of visual percep-\ntion. Psychological Review , 61(3):183–193, 1954.\nH. B. Barlow. Sensory mechanisms, the reduction of\nredundancy, and intelligence. In The Mechanisation\nof Thought Processes , pages 535–539. London: Her\nMajesty’s Stationery Ofﬁce, 1959.\nA. Berenzweig, D. Ellis, and S. Lawrence. Using voice\nsegments to improve artist classiﬁcation of music. In\nProc. AES-22 International Conference on Virtual, Syn-\nthetic and Entertainment Audio , Espoo, Finland, June\n2002.\nA. S. Bregman. Auditory Scene Analysis: The Perceptual\nOrganization of sound . MIT Press, Cambridge, MA,\n1990.\nG. J. Brown. Computational auditory scene analysis: A\nrepresentational approach . PhD thesis, Department of\nComputer Science, University of Shefﬁeld, 1992.\nC. J. C. Burges. A tutorial on support vector machines\nfor pattern recognition. Data Mining and Knowledge\nDiscovery , 2(2):121–167, June 1998.\nM. Casey and A. Westner. Separation of mixed audio\nsources by independent subspace analysis. In Proceed-\nings of the International Computer Music Conference ,\nBerlin, August 2000.\nP. Comon. Independent component analysis – a new con-\ncept? Signal Processing , 36:287–314, 1989.\nM. P. Cooke. Modeling auditory processing and organi-\nsation . PhD thesis, Department of Computer Science,\nUniversity of Shefﬁeld, 1991.\nS. B. Davis and P. Mermelstein. Comparison of parametric\nrepresentations for monosyllabic word recognition in\ncontinuously spoken sentences. IEEE Transactions on\nAcoustics, Speech, and Signal Processing , 28(4):357–\n366, 1980.\nT. Eerola and P. Toiviainen. MIR in Matlab: The MIDI\ntoolbox. In Proc. 5th International Conference on Mu-\nsic Information Retrieval , Barcelona, Spain, October\n10–14 2004.\nD. Ellis. Prediction-driven computational auditory scene\nanalysis . PhD thesis, Department of Electrical Engi-\nneering and Computer Science, Massachusetts Institute\nof Technology, 1996.\nR. M. Everson and S. J. Roberts. Non-stationary indepen-\ndent components analysis. In Proc. International Con-\nference on Artiﬁcial Neural Networks , pages 503–508,\nEdinburgh, 1999.\nH. Hermansky. Perceptual linear predictive (PLP) analysis\nfor speech. Journal of Acoustic Society of America , 87\n(4):1738–1752, 1990.\nD. D. Lee and H. S. Seung. Learning the parts of objects\nby non-negative matrix factorization. Nature , 401:788–\n791, 1999.\nD. D. Lee and H. S. Seung. Algorithms for non-negative\nmatrix factorization. In Advances in Neural Informa-\ntion Processing Systems 13 , pages 556–562. MIT Press,\nCambridge, MA, 2001.N. C. Maddage, C. Xu, and Y . Wang. A svm-based classi-\nﬁcation approach to musical audio. In Proc. 4th Inter-\nnational Conference on Music Information Retrieval ,\nUSA, October 26–30 2003.\nK. D. Martin. Sound-Source Recognition: A Theory\nand Computational Model . PhD thesis, Department\nof Electrical Engineering and Computer Science, Mas-\nsachusetts Institute of Technology, 1999.\nD. K. Mellinger. Event formation and separation in musi-\ncal sound . PhD thesis, Department of Music, Stanford\nUniversity, 1991.\nT. D. Mulder, J. P. Martens, M. Lesaffre, M. Leman, B. D.\nBaets, and H. D. Meyer. An auditory model based tran-\nscriber of vocal queries. In Proc. 4th International Con-\nference on Music Information Retrieval , USA, October\n26–30 2003.\nT. L. Nwe and Y . Wang. Automatic detection of vocal\nsegments in popular songs. In Proc. 5th International\nConference on Music Information Retrieval , Barcelona,\nSpain, October 10–14 2004.\nP. Smaragdis. Redundancy Reduction for Computational\nAudition, a Unifying Approach . PhD thesis, Media Lab-\noratory, Massachusetts Institute of Technology, May\n2001.\nP. Smaragdis. Discovering auditory objects through non-\nnegativity constraints. In Proc. Workshop on Statistical\nand Perceptual Audio Processing , Jeju, Korea, October\n2004a.\nP. Smaragdis. Non-negative matrix factor deconvolution;\nextraction of multiple sound sources from monophonic\ninputs. In Proc. 5th International Conference on Inde-\npendent Component Analysis and Blind Signal Separa-\ntion, Granada, Spain, September 22–24 2004b.\nC. Uhle, C. Dittmar, and T. Sporer. Extraction of drum\ntracks from polyphonic music using independent sub-\nspace analysis. In Proc. 4th International Symposium\non Independent Component Analysis and Blind Signal\nSeparation , Nara, Japan, April 2003.\nT. Virtanen. Sound source separation using sparse coding\nwith temporal continuity objective. In Proc. Interna-\ntional Computer Music Conference , Singapore, 2003.\nT. Virtanen. Separation of sound sources by convolu-\ntive sparse coding. In Proc. Workshop on Statistical\nand Perceptual Audio Processing , Jeju, Korea, October\n2004.\n344"
    },
    {
        "title": "A Music Retrieval System Based on User Driven Similarity and Its Evaluation.",
        "author": [
            "Fabio Vignoli",
            "Steffen Pauws"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1418359",
        "url": "https://doi.org/10.5281/zenodo.1418359",
        "ee": "https://zenodo.org/records/1418359/files/VignoliP05.pdf",
        "abstract": "Large music collections require new ways to let users interact with their music. The concept of finding ‘similar’ songs, albums, or artists provides handles to users for easy navigation and instant retrieval. This paper presents the realization and user evaluation of a music retrieval music that sorts songs on the basis of similarity to a given seed song. Similarity is based on a userweighted combination of timbre, genre, tempo, year, and mood. A conclusive user evaluation assessed the usability of the system in comparison to two control systems in which the user control of defining the similarity measure was diminished.",
        "zenodo_id": 1418359,
        "dblp_key": "conf/ismir/VignoliP05",
        "keywords": [
            "large music collections",
            "user interaction",
            "finding similar songs",
            "instant retrieval",
            "music retrieval",
            "user evaluation",
            "system usability",
            "user control",
            "similarity measure",
            "conclusive user evaluation"
        ],
        "content": "A MUSIC RETRIEVAL SYSTEM BASED ON USER-DRIVEN SIMIL ARITY \nAND ITS EVALUATION\nFabio Vignoli Steffen Pauws \nPhilips Research Laboratories \nProf. Holstlaan 4 \n5656 AA Eindhoven (The Netherlands) \nFabio.Vignoli@philips.com, Steffen.Pauws@philips.co m \nABSTRACT \nLarge music collections require new ways to let use rs \ninteract with their music. The concept of finding ‘ simi-\nlar’ songs, albums, or artists provides handles to users \nfor easy navigation and instant retrieval. This pap er pre-\nsents the realization and user evaluation of a musi c re-\ntrieval music that sorts songs on the basis of simi larity to \na given seed song. Similarity is based on a user-\nweighted combination of timbre, genre, tempo, year,  and \nmood. A conclusive user evaluation assessed the usa bil-\nity of the system in comparison to two control syst ems in \nwhich the user control of defining the similarity m easure \nwas diminished.  \n \nKeywords: music similarity, user evaluation. \n1 INTRODUCTION \nEnd-users identify the concept of finding ‘similar’  \nsongs, albums, or artists as one of the most apprec iated \nfeatures for future music players to get access to large \nmusic collections. They recognise the direct use of  it in \ntheir daily music listening practice [1]. \nThough similarity in music is intuitively meaningfu l, \nit needs further definition. From a user perspectiv e, \njudging similarity of songs either involves the com pari-\nson of two songs or the comparison of a set of alte rna-\ntive songs to a referent or ideal (e.g., a seed son g). A \nstraightforward method is to list all features of t he songs \ninvolved and find the overlap in features. The real ity is \nmore complicated; similarity judgement seems to com e \ndown to the computation of a ‘psychological functio n’ \nof shared, distinctive, and comparable features of the \nsongs involved.  \nEvidently, similarity needs to be explained with re -\nspect to a feature or a set of features. Simply sta ting that \ntwo songs are similar is not sufficient; we need to  say \nthat two songs are similar because of their instrum enta-\ntion, their compositional style, their performers. Numer-\nous research efforts have already been devoted to t imbre \nsimilarity in music [2-5], in which timbre refers t o the spectral information that correlates with instrumen tation \nand articulation in musical performance. Unquestion a-\nbly, timbre similarity is grounded by perception; n on-\nmusicians rather choose instrumentation over correc t \nmelody and harmony in similarity judgement of music  \nby mere listening [6]. But still, timbre is only on e facet \nof music similarity. \nSimilarity judgement is also afflicted with cogniti ve \nprocesses and reasoning using knowledge and conven-\ntions from the real world. It may even result into the \nobservation that two songs are actually incomparabl e \nbecause, for instance, they originate from differen t cul-\ntural traditions, music idioms, or just because of per-\nsonal conviction. Music psychology has already poin ted \nout that besides instrumentation, at least tempo an d \ngenre information are indispensable for similarity judg-\nments of music [7].  \nHowever, Only a few papers from engineering have \naddressed the problem of integrating the multiple f acets \nof music similarity into a single objective functio n \n[8;9,18]. An interesting approach is presented in [ 9] \nwhere a music retrieval system combining similarity  on \ntimbre, lyrics as well as genre was tested with use rs. \nResults showed that users chose different combinati ons \nof these aspects to convey their music preferences.   \nBesides the involvement of various features, the co n-\ntribution of each individual feature to the overall  similar-\nity needs to be weighted. Some features are more im por-\ntant than others to the end-user, the application c ontext, \nor the set of songs under consideration. Given that  the \nimportance of features is heavily dependent on the con-\ntext and the listening intention at hand, the user should \nbe empowered to have total control on this weightin g \nprocedure. A similar approach is presented in [19] \nwhere the user can interactively combine measures o f \nperiodicity with measure of timbre similarity, alth ough \nno evaluation was performed. \nThis paper presents the realization and user evalua -\ntion of a music retrieval music that sorts songs on  the \nbasis of similarity to a given seed song. Similarit y is \nbased on a weighted combination of timbre, genre, \ntempo, year, and mood. The end-user can specify her  \npersonal definition of similarity by weighting thes e as-\npects on a graphical user interface. A conclusive u ser \nevaluation assessed the usability of the system in com-\nparison to two control systems in which the user co ntrol \non defining the similarity function was diminished.   \n2 SONG SIMILARITY \nThe proposed system adopts a broad definition of \nmusic similarity by taking into account various fea tures Permission to make digital or hard copies of all or  part of this \nwork for personal or classroom use is granted witho ut fee pro-\nvided that copies are not made or distributed for p rofit or com-\nmercial advantage and that copies bear this notice and the full \ncitation on the first page. \n© 2005 Queen Mary, University of London \n272   \n \n that contribute to the overall similarity of two so ngs: \ntimbre, tempo, genre, mood, and year (respectively,  de-\nnoted by s s, st, sg, sm, sy). In many cases, each feature \nadds new information to the similarity function. It alian \nPop and Spanish Rock are for many people quite diff er-\nent genres, though their music can be classified as  \nsounding similar in timbre, because of the instrume nta-\ntion used by both. But in principle, the features d o not \nneed to be independent.  \nIn this section, we describe the proposed algorithm s \nto compute our features for music similarity. \n2.1 Timbre similarity \nCurrent state-of-the-art methods to compute timbre \nsimilarity between songs are usually based on a com bi-\nnation of signal processing and statistical tools, which \nboils down to the following schema: (i) computation  of \ntimbre-related features for each song, (ii) computa tion of \ntimbre model for each song, and (iii) comparison of  tim-\nbre models. A more detailed analysis of this topic and an \nexhaustive list of references can be found in [2]. \n \nFeature\nVector\n(all songs)Vector\nQuantization\n(training)\nVector \nQuantization\n(Run)Feature\nVector\n(1 song / \n1 artist) song/artist profile\nDistribution of the song/ \nartist in the timbre \nspaceHistogramTimbre space model \n(N dimensional)\nTimbre space\n \nFigure 1: Block schema of computation of simi-\nlarity (i) computation of feature vectors for each \nsong, (ii) computation of  a song model. \n \nOur approach to compute timbre similarity is based \non the same principles. The main difference lies in  the \ncomputation of the song model, which, instead of be ing \nindependent for each song, is computed relatively t o the \nentire music collection. The main advantage with re spect \nto the state-of-the-art methods is in the speed fac tor dur-\ning the comparison step. The comparison method that  \nachieves the best performance to date uses the Mont e-\nCarlo approach (e.g. as described in [2]), which me ans \ncomparing the timbre feature distribution of two so ngs \nby computing the likelihood of N points of the first dis-\ntribution against the other, where N is usually in the \nrange of a few thousands (4000 in [5]). The method we \npropose uses only a few multiplications for each so ng. \nHowever simple, this song model contains more infor -\nmation about the timbre of the song than a single a ver-\naged feature vector and it is able to provide sensi ble \ncomparisons for songs with a complex structure (e.g ., songs with an intro that is totally different from the rest \nof the song). \nIn Figure 1, a block schema of the process is shown . \nThe first step is the computation of features. The music \nis converted to PCM format and a frame-by-frame ana ly-\nsis is performed. For each frame a feature vector vc is \ncomputed as reported in [10].  \nThe second step is split in two parts: first a mode l of \nthe entire collection is made, and then the relativ e model \nfor each song is computed. A Self Organizing Map \n(SOM) [11] is used to cluster the set of feature ve ctors \nfor all songs in the collection in a unsupervised m anner. \nAlthough other clustering algorithms can be used, w e \nhave chosen the SOM for its topology preservation \nproperty (i.e., features close in the N-dimensional  \nEuclidean space are still close in the resulting 2-\ndimensional space). For our purpose, a map size of \n16x16 (i.e., 256 clusters) gave the best performance.  \nThe set of centroids that results from the clusteri ng \ncan be called a timbre-space  model. Each element of \nthis model is a vector with the same dimensions as the vc \nfeature vector. The timbre-space  model is a representa-\ntion of the timbre of the entire music collection. The \nsong model, which is also a 16x16 matrix, is computed \nas follows. Each element is computed by accumulatin g \nthe response of the SOM (the closest cluster) for e ach \nfeature vector of the song. The resulting matrix is  nor-\nmalized to represent a probability distribution.  \nThe last step, the comparison of song models, is pe r-\nformed by computing the Kullback-Leibler (KL) diver -\ngence L, which for two distributions p(x) and ()xp~ can \nbe written as:  \n.)()(~\nln)(∫−= dxxpxpxpL  \nFor discrete distributions, the integration becomes  a \nsummation over the bins of the two distributions. T he \nKL divergence is regarded as a measure of the exten t to \nwhich two probability density functions agree. The KL \ndivergence is not symmetrical, but in the context o f com-\nputing song similar to a seed song, this property i s \ndesired. Reasons to believe in non-symmetric measur es \ncan be found in [12], where it is argued that human s tend \nto select the prototype (i.e., the seed song) as re ferent \nand the variants (i.e., the similar songs) as the s ubject of \nthe similarity judgements. \n2.2 Genre, mood, year and tempo (dis-)similarity \nGenre, mood, year, and tempo dissimilarities are co m-\nputed by using the distance defined by Gowda and Di -\nday in [13], which was initially developed for the task of \nclustering symbolic object. For our purposes, we co nvert \nthese dissimilarities into similarities by taking t heir \ncomplement. According to the definition, which appl ies \nto both quantitative and qualitative features types , the \ndissimilarity between two objects A and B can be wr it-\nten as \n (i) \n(ii) \nModel \ncomputa-\ntion Timbre model Feature vectors \nTimbre \nmodel \nFeature  \nvectors  \n(1 song) Feature  \nvectors  \n(all songs) Song (PCM) Feature \nextraction  \nClustering \n(Train) \nClustering \n(Run) \n \nSong model  \n273   \n \n DBExtr Feats\nArtist XFeature\nExtrProfiling\nSong X sound distrib.processing\nSong Y sound distrib.\nCompute\nSimilarityss\n• S= wSsS+ wtst+ wgsg+ wmsm+ wysy\n• s=sound, t=tempo, g=genre, m=mood, \ny=year\n \nFigure 2: Block schema of the method used to compute “timbr e similarity” and to combine the various simi-\nlarities (s s, st, sg, sm, sy) into a single measure S.\n \nFigure 3:  The E-Mu jukebox in \"Similar Songs\" mode. The user  can define the similarity function applied to \nthe music collection by dragging the sound/tempo/mood/genre/year  adapters on the screen. An adapter that is \nclose to the center is weighted more than when it i s positioned on the periphery.\n \n),(),(\n1kd\nkkBADBAD∑\n== , \nwhere Ak and Bk are the components of the k-th fea-\nture for the objects A and B, respectively.  \nFor quantitative interval type of features (e.g., 1 997-\n2000), \n• au denotes the  upper limit and al denotes the \nlower limit of interval  Ak, \n• bu denotes the upper limit and bl denotes the \nlower limit of interval  Bk, \n• n denotes the length of the intersection of Ak and \nBk. \nFor qualitative type of features (e.g, genre and mo od \nlabels), \n• la denotes the length (number of elements) of \nAk, • lb denotes the length of Bk, \n• n denotes the number of elements common to \nAk and Bk, \n• ls= la + lb – n denotes the span length of Ak and \nBk combined. \nThe distance between the two object with respect to  \nthe k-th feature is computed as \n),(),(),(),(kkckkskkpkk BADBADBADBAD + + = , where \nkkkpUblalBAD−=),(  and Uk is the length of the maxi-\nmum interval for the k-th feature. This dissimilarity \ncomponent is due to the position, which arises only \nwhen the features are quantitative; it indicates th e rela-\ntive position of two feature values on the real lin e. Selected song \nList of similar songs  Playback \ncontrols \nAdapters  Song \ntimbre \n274   \n \n The expression\nsba\nkkslllBAD−=),( denotes the dissimi-\nlarity component due to the span. It indicates the relative \nsize of the feature values without referring to com mon \nparts between them. \nThe expression \nsba\nkkclnllBAD2),(−+= denotes the \ndissimilarity component due to the content. It is a  meas-\nure of the non-common parts between two feature val -\nues. \nWhen considering ratio/absolute type of features \n(such as year and tempo) the following applies: al=au; \nbl=bu, la=lb=n=0 . Thus, the dissimilarity between two \ntempi ends up being proportional to the interval of  tempi \nencountered in the collection under analysis. The t empo \nwas manually tapped for each song used in this expe ri-\nment. \nGenre and mood are, for the current experiments, \nmanually annotated with a single label (although au to-\nmatic genre and mood classification could be used [ 14]). \nIn this case, the definition of dissimilarity ends up being \nbased on identity: if two songs are labelled with t he \nsame genre (mood) their similarity is equal to 1, o ther-\nwise it is set to 0. This definition of dissimilari ty could \nalso be used to compare features made of multiple l abels \nas in the case of the style information of ‘All Mus ic \nGuide’ [15].  \n2.3 Combining similarities \nThe overall similarity is given by the weighted sum  of \nthe different similarity components. Each similarit y \ncomponent is a real value between 0 and 1 and is \nweighted by a weight also in the range between 0 an d 1. \n1. Sound: the timbre of the song, it is based on co ntent \nanalysis as discussed in Section 4.1. Its weight is  de-\nnoted by []1 , 0∈sw  \n2. Mood: the mood of the song (based on MoodLogic \n[16] data). Its weight is denoted by []1 , 0∈mw  \n3. Genre: the genre of the song (depends on the gen re \nof the artist). Its weight is denoted by []1 , 0∈gw  \n4. Year: the year in which the song was released. I ts \nweight is denoted by []1 , 0∈yw  \n5. Tempo: the tempo of the song (fast-slow). Its we ight \nis denoted by []1 , 0∈tw  \nFinding a sensible weighting of the various compo-\nnents into a generic similarity function for all co ntexts, \nlistening intentions, and songs under consideration  is \nhard to do. Therefore, we let the user interactivel y de-\ncide what is the best weighting for her current pur pose. \nMoreover, weighting the similarity components provi des \nan interesting way to explore and navigate through a \nmusic collection. \n2.4 ThE EXPRESSIVE MUSIC JUKEBOX \nThe Expressive Music (E-Mu) Jukebox has been cre-\nated as an experimental platform to test algorithms  and \ninteraction concepts. The E-Mu Jukebox enables the user to browse a music collection by selecting \ngenre/artists and albums and to search for songs ba sed \non similarity. When a user asks for songs similar t o a \nseed song, the jukebox displays the screen shown in  \nFigure 3.  \nThe similarity components are represented by adapt-\ners. These adapters can be dragged on the bull’s ey e (as \nshowed on the right-hand side of the screen). The r adial \ndistance of an adapter to the centre determines the  \nweight of its corresponding similarity component in  the \nsimilarity function. In this way, a user has the po ssibility \nto change the similarity function that is applied t o the \nmusic collection. For instance, when the genre adapter is \nclose to the centre, the genre component will be highly \nweighted in the similarity computation. Consequentl y, \nsongs with similar labelled genres will pop up high  in \nthe list. If, on the other hand, the adaptor is far  away \nfrom the centre, genre is not highly valued. Songs from \ndifferent labelled genres are likely to appear in t he list. \nThe list of songs on the left-hand side of the scre en is \nsorted according to the degree of similarity; the s ongs \nthat are closest to the seed are positioned at the top of \nthe list. \n3 USER TEST \nTo assess the usability and the user benefits of th e \nsimilarity concept, a user evaluation has been carr ied \nout. We assessed user task performance , perceived \nease-of-use  and usefulness , and user preference  in a \nmusic playlist creation task using a test system an d two \ncontrol systems: \n1.  User-Driven Similarity system ( UDS) with a fully \ncontrollable similarity measure, \n2. Control system 1 ( Control1) with only timbre simi-\nlarity, \n3. Control system 2 ( Control2) with a fixed combina-\ntion of timbre and the other similarity components.  \nParticipants in the test worked with the same visua l \ninterface for all systems, with the exception that the \nsimilarity manipulation was only available for the UDS \nsystem. Additionally, the colour of the logos was d iffer-\nent for the three systems (i.e., red, blue, green),  which \nallowed us to refer to the systems in the post-expe riment \nquestionnaires. For Control2, the weights were fixed \n(according to empirical experimentation) as follows : \n 3 . 0, 3 . 0 , 1, 3 . 0 , 1 = = = = =y g t m s wwwww  \n3.1 Research questions \nThe hypotheses that we want to verify are the follo w-\ning: \n• The UDS system supports users in creating playlists \nmore rapidly  than the control systems do. It is ex-\npected that more control on the similarity definiti on \nhelps users to find preferred music more easily. \n• The use of the  UDS system gives the user a better \nperceived control  in comparison to the control sys-\ntems. \n• The UDS system will be perceived more difficult  to \nuse than the control systems.  \n275   \n \n • The UDS system is preferred  to the control systems. \n3.2 Participants \nTwenty-two (22) persons (15 male, 7 female) partici -\npated voluntarily to the experiment. All participan ts \nwere frequent listeners to popular and rock music w ith \nan average age of 28 years (min: 22 max: 40). All p ar-\nticipants had completed higher vocational education . \nAbout two third of the participants said that they make \nor had made playlists in their private life.     \n3.3 Method \nA factorial within-subject design with one independ -\nent variable system (UDS, Control1, Control2) was \nused: all participants had to work once with each s ystem. \nTo compensate for order effects, participants were ran-\ndomly assigned to one of the six possible permutati ons \nof admission to the three systems. \nA music collection comprising 2248 popular songs \nextracted from 169 CD albums from 111 different art ists \ncovering 7 different musical genres released in the  pe-\nriod from 1963 to 2001 served as test material. The  test \nequipment consisted of a personal computer, on whic h \nthe systems were running, a touch screen tablet, on  \nwhich the users could control the system and a Phil ips \nStreamium MCI-250 audio set to render the music.  \n3.4 Procedure \nParticipants were invited for the experimental sess ion \nin a prepared office room. A few days before the te st, \nthey were provided with a paper list of all artists  whose \nmusic was used as material in the experiment. They were \nasked to indicate what artists they knew and like, did not \nlike or did not know. This task was primarily done to get \nparticipants acquainted with the music used in the test.  \nAt the start of the session, participants were hand ed \nover the general and the detailed instructions for each \ntask of the experiment.  Together with the instruct ions, \ntwo example tasks were provided. Participants were \nencouraged to replicate the examples to get acquain ted \nwith the systems. After performing the tasks, parti cipants \nwere given the opportunity to practice until they f elt \ncomfortable with the system. \nThe user task consisted of a playlist creation task  for \neach of the three different systems. Participants w ere \nasked to create playlists with 10 different songs w hile \nimaging the same listening situation. The playlist created \nin the three trials should be different. While perf orming \nthe task, music could be listened to as many times as \nparticipants desired. No clues were given on how th e \ntask should proceed, or how music should be examine d \nand evaluated. Songs could be added, removed, or re -\nordered individually to or from the playlist under con-\nstruction.  Time to perform the task was unlimited and \nspeed of operation was not presented as a criterion  of \nsuccess. Quality of the playlist was presented as t he sole \noptimization criterion.  Participants were not told  about \nthe nature of the systems and any questions to the ex-\nperiment leader on this topic were not answered.  \nAfter each playlist creation task, participants com -\npleted a (adapted) Technology Acceptance Model (TAM) questionnaire [17], as shown in Figure 4, ass ess-\ning perceived ease-of-use and perceived usefulness of \nthe interactive system. In our experimental setting , the \nterm perceived ease-of-use refers to the extent to which \na user finds a playlist creation system easy to lea rn and \nuse (questions Q1 to Q4). The term perceived usefulness  \nrefers to the extent to which a user finds the syst em to be \nan aid for music selection (questions Q4 to Q5). Pa rtici-\npants were asked to rate the questions from 1 (tota lly \ndisagree) to 7 (totally agree). \nAt the end of the experimental session, participant s \nranked the systems according to their preference of  use. \nAfter the experiment, participants received an e-ma il \nwith a link to the three playlists made, from which  they \ncould listen to the songs. They were asked to rate the \nplaylist on a scale from 1 (extremely bad) to 7 (ex cel-\nlent). \n \nPlease indicate to what extent you agree or disagree with each of the \nfollowing statements by using a 7-point scale. \n \nQ1.  I find learning how to use the system easy. \nQ2.  I find it easy to get the system to do what I want it to do. \nQ3.  I find it easy to become skilful at using the system. \nQ4.  I find the system easy to use. \nQ5.  I find that by using the system I can make goo d playlists. \nQ6.  I find that by using the system I am able to c reate a playlist \nrapidly. \nQ7.  I find that by using the system I enjoy the ma king of a playlist. \nQ8.  I find this system useful at home.  \nFigure 4: Adapted Technology Acceptance \nModel (TAM) questionnaire \n3.5 Measures \nThree measure were used in the evaluation, (i) Task  \nperformance, (ii) Quality of playlists, (iii) Order  of pref-\nerence. \nTask performance was measured by time-on-task  and \nnumber-of-actions . Time-on-task  measured time in sec-\nonds that elapsed from the first button press to th e last \nbutton press. Number-of-actions  measured the number \nof clicks performed on the interface (scrolling on the list \nwas also taken into account) by the participant.  A  rating \nscore on a scale from 1 (extremely bad) to 7 (excel lent) \nmeasured the perceived quality of a playlist .  The order \nof preference  for the three systems was assessed by ask-\ning the participants which system they liked most a nd \nwhich system they liked least. \n4 RESULTS \n4.1 Task Performance \nIn a first analysis, we did not find any statistica lly \nsignificant effect on the time-on-task  and on the number-\nof-actions  measures. However, a more detailed analysis \nshowed that the participants could be divided into two \ngroups: the fast (14 participants) and the slow (8 partici-\npants). A k-means cluster analysis was used to iden tify \nthese two group of people based on the time-on-task  and \nthe number–of-actions  measures. The group of slow \npeople manifested a strong explorative behaviour: t heir \nmusic selection strategy was influenced by suggesti ons \n276   \n \n provided by the system. Instead, the group fast par tici-\npants focused more on their target and music prefer -\nences. In Figure 4, data on time-on-task  are plotted for \nthe two identified groups: the slow ones are distin ct from \nthe fast one by a time-on-task  of about 9 minutes (540 \nseconds). \n4.1.1 Time-On-Task  \n Figure 5:  Mean values of time-on-task  of the fast and \nslow participants for the three systems (error bars  show \nthe standard error of the mean).  \n \nA MANOVA (Multivariate ANalysis Of VAriance) \nwith repeated measures was conducted in which the \ntime-on-task  measure was used as dependent variable \nand system was a within-subject independent variable. \nThe graph shown in Figure 5 suggests that making a \nplaylist by using UDS was on average faster than with \nControl1. We found, indeed, a main statistically signifi-\ncant effect (F(3,7) = 5.18, p < 0.05) for the time-on-task  \nmeasure with respect to system for the set of eight slow \nparticipants. On average, it took about 11 minutes (700 \nseconds) to make a playlists of ten songs with the aid of \nUDS and 19 minutes (1126 seconds) to make the playlist  \nwith the aid of Control1.  \nFor the fast group, it took about 7 minutes (421 sec-\nonds) to make a playlist with the aid of UDS and about \n6.5 minutes (397 seconds) with the aid of Control1; \nthese results were not found to be significant. \n4.1.2 Number-Of-Actions \nA MANOVA with repeated measures was conducted, \nin which the number-of-actions  measure was used as \ndependent variable and system was a within-subject in-\ndependent variable. Figure 6 shows the means and st an-\ndard errors for the slow participants. We found a m ain \nstatistically significant effect (F(3,7) = 3.6, p <  0.05) on \nthe number-of-actions  measure with respect to system \nfor the set of eight slow participants. Making a pl aylist \nwith UDS requires fewer actions (on average 236 ac-\ntions) than making a playlist with Control1 (on average \n395). For the fast group, it took on average 160 ac tions \nto make a playlist with the aid of UDS and on average \n176 actions with the aid of Control1; these results were \nnot found to be statistically significant. \n \n \n  \n \nFigure 6: Mean values for number-of-actions  for the \nslow participants for the three systems (error-bars  \nshow the standard error of the mean). \n4.2 Quality of the playlists  \nA MANOVA with repeated measures was conducted on \nthe full set of twenty-two participants. The rating-score  \nwas used as dependent variable and system was a within-\nsubject independent variable. We found a significan t \neffect (F(3,21) = 3.7, p < 0.05) on the ratings of the \nplaylists with respect to system. The playlists generated \nwith the UDS system were rated higher (5.8, on average, \non a scale from 1 to 7) than those generated with Con-\ntrol1 (5.0, on average). We did not find significant ef-\nfects for UDS and Control2, or between Control2 and \nControl1. No significant effect between fast and slow \nparticipants was found \n4.3 System preference \nThe results of system preference are shown in Figur e \n7. Most participants preferred UDS and substantiated \ntheir choice by saying that they felt more in contr ol in \nthe selection of the similarity. A substantial numb er of \nparticipants (15/22) did not express any preference  dif-\nference between Control1 and Control2. \n4.4 Perceived ease-of-use and usefulness \nResponses to the adapted TAM questionnaire for all \nparticipants were subjected to a two-dimensional no n-\nlinear principal component analysis (PCA). The eigh t \nitems in the questionnaire were treated as active v ari-\nables and the three different systems were treated as \npassive variables to label the plot (i.e., Control1, Con-\ntrol2, UDS). The responses were treated as ordinal cate-\ngories; only the order of the 7-point scale was con sid-\nered important. \nThe visualisation of the PCA solution of the TAM \nquestionnaire is shown in Figure 8. It displays the  mean \ntransformed item responses related to the three dif ferent \nsystems, together with the mean scores to the indiv idual \nitems (i.e., Q1 to Q8). The arrows go through the o rigin \nand the mean scores of each group of items. These l ines \nrepresent the ‘mean’ axes along which the transform ed \nordinal response categories of the items (i.e., the  7-point \nscale of the questionnaire) are located. Time on Task \nSeconds System UDS \nCtrl2 Ctrl1  Number of Actions \nActions System UDS \nCtrl2 Ctrl1 \n \n277   \n \n  \nBLUE GREEN RED GREEN/BL\nSystem Preference (count)051015Count\n \nFigure 7:  Expressed preferences for the tested sys-\ntems. The scores for the items Q1, Q2, Q3 and Q4 are \nhighly correlated as well as the scores for the ite ms Q5, \nQ6, Q7 and Q8. The high correlations mean, indeed, that \nthe two sets of four items load on different factor s that \ncan be labelled as perceived ease-of-use and perceived \nusefulness . Figure 8 suggests that UDS is perceived as \nthe most useful of the three tested systems, but it  is also \nperceived as slightly more complex to use than the two \ncontrol systems. In contrast, Control1 is perceived as the \nleast useful, though there is not much difference i n the \nease-of-use  dimension with Control2. \n \n \nFigure 8:  The perceived ease-of-use and usefulness of the te sted systems.\n5 . DISCUSSION \nThis experiment evaluated user task performance, \nperceived ease-of-use  and usefulness , and preference of \nuse of the UDS system (with a fully user controllable \nsong similarity feature) in comparison with two con trol \nsystems: Control1 and Control2 (with a non-user-\ncontrollable similarity feature).  \nIt was expected that less time and fewer actions ar e \nrequired to make a playlist when using the UDS system \nthan when using the control systems. The test revea led \nthat, slow participants needed less time (on averag e, 7 \nminutes or about 38% less time) and fewer actions ( 160 \nor 40% fewer actions) to complete a playlist with t he aid \nof UDS than with the aid of Control1. For Control2, it \ntook about 4 minutes, equivalent to 21% less time w ith \nrespect to Control1 and about 3 minutes more than UDS. \nBased on these results, the hypothesis could not be  re-\njected. Note that the UDS system contained an addi-tional repertoire of actions to manipulate the simi larity; \nthis additional set of actions did however not nega tively \ninfluence the total number of actions. For the fast  par-\nticipants, no effects on time and actions were obse rved. \nProbably, these participants were already acting at  \nmaximal performance level, while leaving little roo m for \nimprovement by using a different system.  \nQuality of the playlist was explained to the partic i-\npants as their sole optimisation criterion with no restric-\ntions on time. Hence, we would not expect a quality  ef-\nfect in the playlists. Nevertheless, playlist made with \nUDS were rated higher that those made with Control1. \nThis suggests that the UDS system allowed participants \nto create better playlists than at least one of the  two con-\ntrol systems did.  \nIt was expected that the usefulness  of UDS was per-\nceived higher than the two control systems and that  UDS \nwould score less on ease-of-use . The TAM question-\nnaire indicated that UDS was, indeed, perceived most  Ctrl1              Ctrl2           UDS         Ctr l1/Ctrl2 \nPerceived  \nusefulness Perceived \nease-of-use \nUDS Ctrl1 \nCtrl2 \n278   \n \n useful and slightly less easy to use. The two contr ol sys-\ntems were perceived equally easy to use. Based on t hese \nresults, the hypothesis could not be rejected. \nIt was expected that the UDS system is preferred to \nthe control systems because of the better control t hat it \noffers. The order of preference task found out that , in-\ndeed, the UDS system was preferred over the Control1 \nand the Control2 systems. \n6 CONCLUSION \nThis paper presented the realization and user evalu a-\ntion of a music retrieval music that sorts songs on  the \nbasis of similarity to a given seed song. The notio n of \nmusic similarity has been identified as an apprecia ted \ntool for end-users to find preferred music in large  collec-\ntions. We believe that music similarity involves th e com-\nparison of different song features like timbre, gen re, \nmood, tempo, and year. Moreover, given that the imp or-\ntance of features is heavily dependent on the conte xt and \nlistening intentions at hand, we proposed a system in \nwhich the users had complete control on the contrib ution \nof each feature to the overall similarity. We belie ve that \nour approach to music similarity could provide the tools \nto break through the glass ceiling discussed in [2] . We \nhave evaluated the usability of such a system in co m-\nparison to two control systems in which the user co ntrol \non the similarity function was diminished. Findings  were \nthat users with a more explorative nature who work with \nthe proposed system are able to make better playlis ts in \nless time and less effort than in the case of the c ontrol \nsystems. Only because of the additional effort to l earn to \nwork with the user-driven similarity function durin g \nfirst-time use, most users find the proposed system  \nsomewhat less easy to use than the other systems. I n \nconclusion, providing users with complete control o n \ntheir personal definition of music similarity is fo und to \nbe more useful and preferred than no control.    \nREFERENCES \n [1]  Vignoli, F., Digital Music Interaction concepts: a \nuser study, Proceedings of International Confer-\nence on Music Information Retrieval, ISMIR \n2004, pp. 415-420, 2004. \n [2]  Aucouturier, J.-J. and Pachet, F., Improving tim-\nbre similarity: how high's the sky, Journal of \nNegative Results in Speech and Audio Science , \nvol. 1, no. 1, 2004. \n [3]  Berenzweig, A., Logan, B., Ellis, D. P. W., a nd \nWhitman, B., A Large-Scale Evaluation of \nAcoustic and Subjective Music Similarity Meas-\nures, Proceedings of 4th International Confer-\nence on Music Information Retrieval, ISMIR \n2003, pp. 99-105, 2003. \n [4]  Logan, B. and Salomon, A., A music similarity  \nfunction based on signal analysis, Proc.of IEEE \nInt.Conf.on Multimedia and Expo (ICME) , 2001. \n [5]  Pampalk, E., Dixons, S., and Widmer, G. On th e \nevaluation of perceptual similarity measures for music. Proceedings of the 6th International  Con-\nference on Digital Audio Effects (DAFx-03) , 2003.  \n [6]  Wolpert, R. S., Recognition of melody, harmon ic \naccompaniment, and instrumentation: musicians \nvs. non-musicians, Music Perception , vol. 8, no. \n1, pp. 95-106, 1990. \n [7]  Cupchik, G. C., Rickert, M., and Mendelson, J ., \nSimilarity and preference judgment of musical \nstimuli, Scandinavian Journal of Psychology , \nvol. 23, pp. 273-282, 1982. \n [8]  Pauws, S. and Eggen, B., Realization and eval ua-\ntion of an automatic playlist generator, Journal \nof New Music Research , vol. 32, pp. 179-192, \n2003. \n [9]  Baumann, S., Pohle, T., and  Shankar, V., To-\nwards a socio-cultural compatibility of MIR sys-\ntems.  Proc. of 5th Int. Conf. on Music Informa-\ntion Retrieval. 2004 , pp. 460-465, 2004.  \n [10]  McKinney, M. and Breebaart, J., \"Features fo r \naudio music classification\", Proceedings of 4rd \nInternational Conference on Music Information \nRetrieval, ISMIR 2003 , pp. 151-158, 2003. \n [11]  Kohonen, T., Self-organizing maps,  Springer, \n1995. \n [12]  Tversky, A., Features of similarity, Psychology \nReview, vol. 84, no. 4, pp. 327-352, 1977. \n [13]  Gowda, K. C. and Diday, D., Symbolic Cluster ing \nusing a new dissimilarity measure, Pattern Rec-\nognition, vol. 24, no. 6, pp. 567-578, 1991. \n [14]  Tzanetakis, G. and Cook, P., Musical Genre C las-\nsification of Audio Signals, IEEE Transactions \non Speech and Audio Processing , vol. 10, no. 5, \npp. 293-302, 2002. \n [15]  All Music Guide. All Music Guide. \nhttp://www.allmusic.com,  last accessed 20-04-\n2005.  \n [16]  MoodLogic. http://www.moodlogic.com,  last ac-\ncessed 20-04-2005.  \n [17]  Davis, F. D., Perceived usefulness, perceive d \nease-of-use, and user acceptance of information \ntechnology, Management Information Science \nQuarterly , vol. 18, pp. 189-211, 1989. \n  [18] Whitman, B. and Smaragdis, P. Combining \nMusical and Cultural Features for Intelligent Style  \nDetection, Proceedings of International \nConference on Music Information Retrieval, \nISMIR 2002 , 2002. \n  [19] Pampalk, E. and Dixon, S. and Widmer, G. \nExploring Music Collections by browsing \ndifferent views, Proceedings of International \nConference on Music Information Retrieval, \nISMIR 2003 , pp 201-208, 2002.  \n \n279"
    },
    {
        "title": "Herding Folksongs.",
        "author": [
            "Robert Young Walser"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415640",
        "url": "https://doi.org/10.5281/zenodo.1415640",
        "ee": "https://zenodo.org/records/1415640/files/Walser05.pdf",
        "abstract": "Cataloging a large, multi-media collection of traditional song and drama in preparation for online presentation highlights issues of song identity and access in the context of contemporary digitized archives. In the James Madison Carpenter collection a particular folksong sung by a particular individual may exist in multiple manifestations: typed song text, sound recording(s), and/or manuscript music notation. While controlled vocabulary",
        "zenodo_id": 1415640,
        "dblp_key": "conf/ismir/Walser05",
        "keywords": [
            "cataloging",
            "multi-media",
            "traditional song",
            "drama",
            "online presentation",
            "issues",
            "song identity",
            "access",
            "contemporary digitized archives",
            "James Madison Carpenter collection"
        ],
        "content": "HERDING FOLKSONGS\nRobert Young Walser\nThe James Madison Carpenter Project\nElphinstone Institute\nUniversity of Aberdeen\n24 High Street\nAberdeen, AB24 3EB\nseasongs@spacestar.net\nABSTRACT\nCataloging a large, multi-media collection of traditional\nsong and drama in preparation for online presentation\nhighlights issues of song identity and access in the con-\ntext of contemporary digitized archives. In the James\nMadison Carpenter collection a particular folksong sung\nby a particular individual may exist in multiple manifes-\ntations: typed song text, sound recording(s), and/or\nmanuscript music notation. While controlled vocabulary\nreferences such as Child and Roud numbers provide a\ndegree of identification, such narrative- and text-centric\ntools are only partly effective in differentiating folkloric\nmaterials. Additional means are needed for identifying\nand controlling folk materials which are distinguished by\nother aspects of the song such as melody or non-narrative\ntext. The Carpenter project team’s experience with En-\ncoded Archival Description (EAD) illustrates the value of\nthis platform-independent, widely recognized standard\nand suggests opportunities for further developments par-\nticularly suited to locating and retrieving folk music\nmaterials.\nKeywords: Folksong, folklore, music, sound record-\nings, EAD, XML, James Madison Carpenter.\n1 INTRODUCTION\nJames Madison Carpenter was an American folklorist\nwho traveled through England, Scotland and Wales from\n1929 to 1935 in search of folk materials. Initially fo-\ncused on sea shanties and ballads, his collecting ex-\npanded to include mummer’s plays, dance music, child\nlore and other folkloric genres. After his return to the\nUnited States in 1936 he continued to work with his\ncollection in the hope of eventual publication. This\ndream was never realized and in 1972 the collection in-\ncluding approximately 13,000 manuscript and typed\npages, 560 photographs, 179 Dictaphone cylinders, 220\nacetate discs and various ephemera was sold to the Ar-\nchive of Folk Song (now the Archive of Folk Culture) at\nthe Library of Congress in Washington, DC. Detailed\ncataloging being beyond the means of the Archive, the\ncollection was microfilmed and tape copies made of thedisc recordings. Knowing the value of the materials in\nthe collection, Archive staff directed a number of schol-\nars to the collection who did find therein abundant riches\nof folkloric material, though in a frustrating state of dis-\norganization. The lack of a comprehensive catalog pre-\nvented full use of the materials.\nIn 1999 Dr. Julia Bishop applied to the AHRB for a\ngrant to begin cataloging the collection. The work was\ntimed to coordinate with efforts at the Archive to digit-\nize the collection for eventual online presentation as part\nof the ‘American Memory’ project at the Library of\nCongress project (http://memory.loc.gov/ammem/). The\nfirst grant specified the use of Encoded Archival De-\nscription (EAD see http://www.loc.gov/ead/) an XML-\nbased international standard for encoding and presenting\narchival finding aids – tools to assist researchers in find-\ning specific content within archival collections.\nA team of six scholars, experts in the various folk-\nloric genres represented in the Carpenter collection, was\nformed to catalog the materials using XMetaL software\nand the EAD dtd (Document Type Definition). The\nproduct of the first grant is now online at The Univer-\nsity of Sheffield’s online publication\nwww.hrionline.ac.uk/carpenter. Work continues both in\nthe UK and at the Library of Congress to provide digital\nsurrogates of Carpenter’s collectanea online. The specific\ncharacter of Carpenter’s working methods and the result-\ning materials give rise to useful concepts for cataloging\nfolkloric materials and simultaneously raise questions\nabout the most appropriate means for accessing such\nmaterials.\n2 THE RAW MATERIALS\nCarpenter’s was among the early users of sound record-\ning technology for folksong collecting and the physical\nand financial limitations of the technology were impor-\ntant factors shaping both his working methods and re-\nsults. Due to the cost of Dictaphone cylinders, Carpenter\nchose in many cases to record only one or two verses of\na particular song and then had the singer dictate the rest\nof the song while Carpenter typed the words on a port-\nable typewriter. For many of the songs Carpenter later\ntranscribed the melody onto staff notation. After his re-\nturn to the United States, he obtained a disc recording\nmachine and acoustically copied many of his original\ncylinder recordings onto acetate discs. The result of this\nprocess is a gigantic collection of song fragments for\nwhich Carpenter himself never assembled an index. Fig-\nures 1 and 2 illustrate this with two fragments of “Shal-\nlow Brown” a sea shanty sung by John Middleton.Permission to make digital or hard copies of all or part of\nthis work for personal or classroom use is granted without\nfee provided that copies are not made or distributed for\nprofit or commercial advantage and that copies bear this\nnotice and the full citation on the first page.\n© 2005 Queen Mary, University of London\n676Figure 1. “Shallow Brown” song text.\nFigure 2. “Shallow Brown” notation.\nIn general, scholars and singers are more interested in\nsongs than fragments and thus an initial task for the\nCarpenter team was to create means for gaining intellec-\ntual control over this mass of material. As catalogers it\nbecame immediately apparent that the item fragments\ndid not map directly to the physical items in the collec-\ntion: one song text might extend to several pages or,\nconversely, one page might contain a number of differ-\nent songs as in Figure 2. Further, in the case of the folk\nplays, a song might occur in the midst of a play text.\nTo describe material in this state we first chose the term\n“intellectual item” to refer to a discrete chunk of folk-\nloric material which, in the case of songs within plays,\ncould be embedded within a larger intellectual item.\nNext, we decided to employ several elements in identi-\nfying these fragments comprising, a title and format de-\nscription, typically including a personal name, and op-\ntionally a place name and/or date.\n3 FOLK MATERIALS IN EAD\nThese decisions were facilitated by the use of EAD, an\nXML-based standard for encoding information about\narchival holdings. EAD provides named tags enclosed in\nangle brackets to surround text data in a hierarchy reflect-\ning the typical organization of information within an\narchival collection. EAD documents are plain text and\nthus platform-independent and are editable using any\ntext-editing software, though dedicated XML editors\nsuch as XMetaL facilitate the process by allowing users\nto validate their work, that is, to verify that it conforms\nto the standard and is thus recognizable to computers\nthat comprehend EAD.In EAD all components of an archival collection are\nplaced within component tags. In the case of the Car-\npenter collection, <c01> is used for the collection,\n<c02> for the boxes in which the manuscripts were\nhoused and so on. Intellectual items are contained\nwithin tags indicating their level as components of the\ncollection as shown in Figure 3 and include page refer-\nences within the item description.\nFigure 3.  EAD hierarchy applied to Carpenter.\nEach component of an EAD-encoded collection must\ninclude identifying information. Figure 4 illustrates one\napplication of EAD tags to the identifying information\nselected by the Carpenter project team for describing an\nintellectual item.\nFigure 4. Identifying an intellectual item.\nThis example catalogs the typed text of the song “Shal-\nlow Brown” shown in Figure 1. In this case the identify-\ning “unittitle” for this intellectual item includes the title,\na <genreform> indicating what sort of fragment is being\ncataloged, the personal name (persname) of John Middle-\nton who is identified as the contributor, that is, the per-\nson who contributed the song to Carpenter’s collection,\nand a geographic name (geogname) associated with this\nperformance. Similar unittitles have been made for over\n17,000 intellectual items in the collection.\n4 ONE LEVEL OF SUCCESS\nThe use of multiple elements within the identifying unit-\ntitle accomplishes two things. First, it allows catalog\nusers to differentiate between the various manifestations\nof a single performance. Since John Middleton’s singing\nof “Shallow Brown” appears as six discrete intellectual\nitems in the collection, it is useful to know which is a\nsound recording and which is music notation. Similarly,\nthis also helps users identify Middleton’s version of this\nsong as opposed to one of the other four versions (in\ntheir multiple manifestations) in the collection. Second,\nthe web of information within the unittitle has helped\nthe Carpenter Project team members to gather the frag-\nments of many songs in order to, eventually, reconstruct\n677more complete representations for further study and even\nperformance. This cross-referencing is done using unique\nidentifiers assigned to each fragment which are then\ngathered in an XML supplement to the EAD catalog.\nThe two documents are merged in response to queries\nand together rendered into the HTML display.\nOn this level, the EAD-encoding of information\nabout the Carpenter Collection successfully aids both\ncomprehending the collection and identifying and locat-\ning related materials within it. A further benefit derives\nfrom the widespread use of EAD (see\nwww.archiveshub.ac.uk for example) which potentially\nallows for searching across the universe of EAD docu-\nments representing collections housed in various reposi-\ntories. This brings us to the larger universe of folksong.\n5 ELUSIVE FOLKSONGS\nThe terms “folklore” and “folksong” are often understood\nto invoke a notion of instability over time resulting from\nthe “folk process”. This refers to variation in a song,\nstory or other item of folk culture either by design or\naccident. James Francis Child’s monumental 5-volume\nwork, The English and Scottish Popular Ballads  [1] has\nbecome a standard reference in the study of Balladry in\npart because of Child’s grouping by storyline of various\nsong texts from multiple sources. To each group he as-\nsigned a number which has become a standard means of\nidentifying ballads. Thus “Child 278” refers to a story\nabout a woman taken to hell who proves tougher than\nOld Nick himself. Whether sung as a jolly ditty or a\ndirge, “Child 278” identifies this song type. Indeed,\nChild numbers are given as standard information not\nonly in print folksong and ballad collections, but even\nby some performers on stage and the term “Child Bal-\nlad” is often used to refer to traditional narrative songs.\nOther attempts at codifying and identifying English-\nlanguage folksong have been undertaken by G, Malcolm\nLaws [2, 3] and, currently, Steve Roud. Additionally\nthere are various online folksong indices.\nThese narrative and text centered systems can be help-\nful in identifying certain kinds of songs and references\nto them have thus been included in the Carpenter team’s\ncatalog. At present the catalog is searchable by Child\nnumber and work is in process to add Roud numbers as\nwell. Unfortunately, these tools are insufficient for not\nall the material in the Carpenter collection, or indeed in\nthe realm of folklore and folk music, can be controlled\nwith reference to song text or narrative.\n6 THE TROUBLE WITH SEA SHANTIES\nAs Steve Roud points out in the documentation accom-\npanying his Folk Song Index,\nIt must be noted that the system is not nearly so\neffective with material from other traditions,\nmost notably Afro-American genres such as\nspirituals and blues. These are, in general, far\nmore textually ‘fluid’, and although numbers are\nassigned to entered songs from these traditions,\nthe user should be aware of the limitations of\nthis approach. [4, manual.doc p.6]\nThe corpus of sea shanties or sailors’ worksongs pro-\nvides a case in point. Carpenter wrote his PhD thesis on“Forecastle Songs and Chanties1” [5] and there are hun-\ndreds of shanties in the collection. Considered part of\ntheir work by seamen, shanties were not regarded as\nmusic at all, yet their rhythm coordinated the efforts of\ntall ship sailors who claimed that “a good song was\nworth ten men on a rope” [6, p.30]. For many shipboard\ntasks rhythmic coordination was a shanty’s key func-\ntion. This may account for the existence of a large num-\nber of couplets used interchangeably in many songs. In\nfact, some shanties seem to have no fixed verses at all.\nFor example, two versions of “Ilo Man” in the collec-\ntion with closely related melodies have texts that are\nrelated only vaguely. Other examples abound. For these\nsongs additional factors must be considered to show\ntheir family relationships. The texts of choruses and/or\nrefrains are one factor and melody is another.\nIssues of description, identity and relationships\namong folk melodies are, as yet, unresolved despite the\nefforts of scholars such as Charles Seeger [7], Samuel\nBayard [8], Anne Dhu Shapiro [9] and James Cowdery\n[10]. Here lie opportunities for those interested in Music\nInformation and Retrieval.\n7 NEXT STEPS\nFor many folksongs the text-based methods of Child,\nLaws and Roud provide sufficient control for scholars\nand singers to locate and identify related song materials.\nHowever, for some shanties, blues and other genres these\ntools are inadequate. Despite the problems of melodic\ndescription and identity, the study of these and other\nsong genres requires some form of melodic description\nin order to locate and identify them for study and per-\nformance. Folk song presents a particular problem be-\ncause of the lack of an original composer’s work. The\nunknown and likely unknowable provenance of folk ma-\nterial renders the search for melodic urtexts  hopeless. In\nthis context the development of tools for melodic de-\nscription gains importance. However, the instability of\nfolk melodies examined by the scholars listed above\nadds further challenge to this already daunting task.\nFolksong thus adds additional difficulties for those creat-\ning tools for mapping melodic similarity such as those\nbeing discussed at MIREX 2005 while at the same time\nunderscoring the potential usefulness of such tools.\nIn cataloging the Carpenter Collection EAD proved a\npowerful and flexible tool. However, the completed\nEAD finding aid is, uncompressed, over 12 MB in size\n– too large to be served to end users over the internet.\nThe experience of the Carpenter project team suggests\nthat XML encoding of songs or song fragments offers a\npractical approach to presenting meta-data about folk-\nsong instances from various media: similar, comparable\nrecords have been created for sound recordings, nota-\ntions, and simple song texts. This is a credit to EAD’s\ndesigners who created a flexible tool logically applicable\nto folk material.\nThe existence of various folk song indices both in\nprint and on the web points to the ongoing interest in\ngathering and grouping such materials. At present this\n                                                \n1 Various spellings are used for this term. While its etymology is dis-\nputed, its shipboard pronunciation in not. I thus prefer the “sh” spell-\ning which represents the correct pronunciation.\n678is done with each cataloger using their own tools and\ndata design. A flexible, shared, platform-independent\nXML data standard similar to (or derived from) EAD\nwould offer this branch of scholarship a useful vehicle\nfor data collection and use. Individual songs or song\nfragments cataloged in this way could be searchable\nacross the web in the same way that the Archives Hub\nsearches EAD instances in the UK. What’s more, EAD\nwas designed to be backwards compatible with print\nfinding aids. A similar data standard for folk songs\ncould be written so that harvesting data from existing\ndatabases would be relatively simple.\nWith EAD and other online tools a vast riches of\nfolksong becomes ever more accessible. With expanded\npotential to gather such materials the need for identifica-\ntion and control grows. Perhaps from a cross-\nfertilization of folklore, ethnomusicology, library and\narchives research and music information technology,\nsuch tools may yet be created.\nREFERENCES\n[1] Child, Francis James. The English and Scottish\nPopular Ballads. 5 vols. Houghton-Mifflin, Boston,\n1882-1898.\n[2] Laws, G. Malcolm. Native American Balladry.\nAmerican Folklore Society, Philadelphia, 1950,\nrevised 1964.\n[3] Laws, G. Malcolm. American Ballads from British\nBroadsides. American Folklore Society,\nPhiladelphia, 1957.\n[4] Roud, Steve. Folk Song Index and Broadside Index.\nCD-ROM, Maresfield, E. Sussex, 2004 (updated\nperiodically).\n[5] Carpenter, James Madison. Forecastle Songs and\nChanties, PhD Thesis, Harvard University,\nCambridge, MA, 1928.\n[6] Hugill, Stan. Shanties from the Seven Seas.\nRoutledge & Kegan Paul, London, 1961.\n[7] Seeger, Charles. “Versions and Variants of the Tunes\nof ‘Barbara Allen’” Selected Reports in\nEthnomusicology 1:120-67, 1966.\n[8] Bayard, Samuel “Prolegomena to a Study of the\nPrincipal Melodic Families of British-American\nFolksong” Journal of American Folklore, 61:1-44,\n1950.\n[9] Shapiro, Anne Dhu. \"Black Sacred Song and the\nTune-Family Concept\" In Search of\nNew Perspectives in Music: Festschrift Eileen\nSouthern,  eds. Josephine Wright and Samuel Floyd.\nHarmonie Park Press, Warren, MI, 1992.\n[10] Cowdery, James. The Melodic Tradition of\nIreland. Kent State University Press, Kent, Ohio,\n1990.\n679"
    },
    {
        "title": "Finding An Optimal Segmentation for Audio Genre Classification.",
        "author": [
            "Kris West",
            "Stephen Cox"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1416746",
        "url": "https://doi.org/10.5281/zenodo.1416746",
        "ee": "https://zenodo.org/records/1416746/files/WestC05.pdf",
        "abstract": "In the automatic classification of music many different segmentations of the audio signal have been used to calculate features. These include individual short frames (23 ms), longer frames (200 ms), short sliding textural windows (1 sec) of a stream of 23 ms frames, large fixed windows (10 sec) and whole files. In this work we present an evaluation of these different segmentations, showing that they are sub-optimal for genre classification and introduce the use of an onset detection based segmentation, which appears to outperform all of the fixed and sliding windows segmentation schemes in terms of classification accuracy and model size. Keywords: genre, classification, segmentation, onset, detection 1",
        "zenodo_id": 1416746,
        "dblp_key": "conf/ismir/WestC05",
        "keywords": [
            "genre",
            "classification",
            "segmentation",
            "onset",
            "detection",
            "fixed",
            "sliding",
            "windows",
            "accuracy",
            "model"
        ],
        "content": "FINDING AN OPTIMAL SEGMENTATION FOR AUDIO GENRE\nCLASSIFICATION\nKris West\nSchool of Computing Sciences\nUniversity of East Anglia,\nNorwich, NR4 7TJ, UK.\nkw@cmp.uea.ac.ukStephen Cox\nSchool of Computing Sciences\nUniversity of East Anglia,\nNorwich, NR4 7TJ, UK.\nsjc@cmp.uea.ac.uk\nABSTRACT\nIn the automatic classiﬁcation of music many different\nsegmentations of the audio signal have been used to cal-\nculate features. These include individual short frames (23\nms), longer frames (200 ms), short sliding textural win-\ndows (1 sec) of a stream of 23 ms frames, large ﬁxed win-\ndows (10 sec) and whole ﬁles. In this work we present\nan evaluation of these different segmentations, showing\nthat they are sub-optimal for genre classiﬁcation and in-\ntroduce the use of an onset detection based segmentation,\nwhich appears to outperform all of the ﬁxed and sliding\nwindows segmentation schemes in terms of classiﬁcation\naccuracy and model size.\nKeywords: genre, classiﬁcation, segmentation, onset,\ndetection\n1 INTRODUCTION\nIn recent years the demand for automatic, content-based\nmultimedia analysis has grown considerably due to the\never increasing quantities of multimedia content available\nto users. Similarly, advances in local computing power\nhave made local versions of such systems more feasible.\nHowever, the efﬁcient and optimal use of information\navailable in a content streams is still an issue, with\nvery different strategies being employed by different\nresearchers.\nAudio classiﬁcation systems are usually divided\ninto two sections: feature extraction and classiﬁcation.\nEvaluations have been conducted both into the different\nfeatures that can be calculated from the audio signal\nand the performance of classiﬁcation schemes trained\non those features. However, the optimum length of\nﬁxed-length segmentation windows has not been in-\nvestigated, nor whether ﬁxed-length windows provide\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc/circlecopyrt2005 Queen Mary, University of Londongood features for audio classiﬁcation. West and Cox\n(2004) compared systems based on short frames of the\nsignal (23 ms), with systems that used a 1 second sliding\nwindow of these frames, to capture more information\nthan was available in the individual audio frames, and a\nsystem that compressed an entire piece to just a single\nvector of features (Tzanetakis, 2003). Tzanetakis et al.\n(2001) demonstrates a system based on a 1 second sliding\nwindow of the calculated features and in Tzanetakis\n(2003), a whole ﬁle based system. Schmidt and Stone\n(2002) and Xu et al. (2003) investigated systems based\non the classiﬁcation of individual short audio frames (23\nms) and Jiang et al. (2002) classiﬁes overlapped 200 ms\nanalysis frames. West and Cox (2004) showed that it is\nbeneﬁcial to represent an audio sample as a sequence\nof features rather than a single probability distribution.\nWe also demonstrated that a tree-based classiﬁer gives\nimproved performance on these features over a “ﬂat”\nclassiﬁer.\nIn this paper we introduce a new segmentation based\non an onset detection function, which outperforms the\nﬁxed segmentations in terms of both model size and clas-\nsiﬁcation accuracy. The paper is organised as follows:\nﬁrst we discuss the modelling of musical events in the\naudio stream, then the parameterisations used in our ex-\nperiments, the development of onset detection functions\nfor segmentation, the classiﬁcation scheme we have used\nand ﬁnally the results achieved and the conclusions drawn\nfrom them.\n2 MODELLING EVENTS IN THE\nAUDIO STREAM\nAveraging sequences of features calculated from short au-\ndio frames (23 ms) across a whole piece tends to drive\nthe distributions from each class of audio towards the cen-\ntre of the feature space, reducing the separability of the\nclasses. Therefore, it is more advantageous to model the\ndistributions of different sounds in the audio stream than\nthe audio stream as a whole. Similarly, modelling short\naudio frames from a signal is also sub-optimal as a musi-\ncal event is composed of many different frames, occupy-\ning different locations in the feature space. This leads to a\nvery complex set of distributions of features for each piece\nthat are both hard to model and contain less information\n680for classiﬁcation than the distribution of features from a\nsingle musical event would. Sounds do not occur in ﬁxed\nlength segments and when human beings listen to music,\nthey are able to segment the audio into individual events\nwithout any conscious effort, or prior experience of the\ntimbre of the sound. This suggests the possibility of seg-\nmenting an audio stream as a sequence of musical events\nor simultaneously occurring musical events. We believe\nthat directed segmentation techniques, such as onset de-\ntection, should be able to provide a much more informa-\ntive segmentation of the audio data for classiﬁcation than\nany ﬁxed length segmentation due to the fact that sounds\ndo not occur in ﬁxed length segments.\nSystems based on long sliding windows (e.g. 1 sec-\nond) that are highly overlapped are a step in the right di-\nrection, as they allow a classiﬁcation scheme to attempt\nto model multiple distributions for a single class of au-\ndio. However, they complicate the distributions as long\nwindows are likely to capture several different musical\nevents. This style of segmentation also includes a very\nlarge amount redundant information as a single sound may\ncontribute to 80 or more feature vectors (based on a 1 sec\nwindow, over 23 ms frames with a 50% overlap). A seg-\nmentation based on an onset detection technique allows a\nmusical event to be represented by a single vector of fea-\ntures and ensures that only individual events or events that\noccur simultaneously contribute to that feature vector.\n3 EXPERIMENTAL SETUP -\nPARAMETERISATION\nIn Jiang et al. (2002) an Octave-based Spectral Contrast\nfeature is proposed, which is designed to provide better\ndiscrimination among musical genres than Mel-Frequency\nCepstral Coefﬁcients. In order to provide a better rep-\nresentation than MFCCs, Octave-based Spectral Contrast\nfeatures consider the strength of spectral peaks and valleys\nin each sub-band separately, so that both relative spectral\ncharacteristics, in the sub-band, and the distribution of\nharmonic and non-harmonic components are encoded in\nthe feature. In most music, the strong spectral peaks tend\nto correspond with harmonic components, whilst non-\nharmonic components (stochastic noise sounds) often ap-\npear in spectral valleys (Jiang et al., 2002), which reﬂects\nthe dominance of pitched sounds in Western music. Spec-\ntral Contrast is a way of mitigating against the fact that av-\neraging two very different spectra within a sub-band could\nlead to the same average spectrum.\nA full description of the procedure for calculating\nSpectral Contrast feature is beyond the scope of this paper.\nHowever, an overview of the process and the similarities\nwith the calculation of MFCCs is shown in Figure 1.\n4 EXPERIMENTAL SETUP -\nSEGMENTATIONS\nInitially, audio is sampled at 22050Hz and the two stereo\nchannels channels summed to produce a monaural sig-\nnal. It is then divided into overlapping analysis frames\nand Hamming windowed. Spectral contrast features are\ncalculated for each analysis frame and then, optionally,\nFigure 1: Overview of Spectral Contrast Feature calcula-\ntion\nFigure 2: Audio segmentations and temporal modelling\nwindows evaluated\nthe means and variances of these frames are calculated\n(replacing the original parameterisation), using a sliding\nwindow across the whole ﬁle, or across segments identi-\nﬁed using an onset detection function, returning one vec-\ntor of features per segment.\nThe segmentations evaluated in this system are: 23 ms\naudio frames, 200 ms audio frames, 23 ms audio frames\nwith a 1 second sliding temporal modelling window, 23\nms audio frames with non-overlapping 10 second win-\ndows, 23 ms audio frames with whole ﬁle temporal mod-\nelling (returns 1 vector per ﬁle), 23 ms audio frames with\nonset detection based segmentation and temporal mod-\nelling. These segmentations are schematically shown in\nFigure 2.\n4.1 Developing Onset detection functions\nEnergy based onset detection techniques have been used\nby a number of researchers to segment audio, including\nGoto and Muraoka (1995), Dixon et al. (2003), Heittola\nand Klapuri (2002), Schloss (1985) and Duxbury et al.\n(2003). The essential idea is that peaks in the positive\ndifferences in the signal envelope correspond to onsets in\nthe audio stream, i.e. the beginning of musical events.\n4.1.1 Thresholding an onset detection function\nAn onset is detected at a particular audio frame if the on-\nset detection function is greater than a speciﬁed threshold,\nand that frame has a greater onset detection function value\nthan all the frames within a small isolation window.\nDuxbury et al. (2003) use a dynamic median threshold\nto “peak-pick” the detection function, as shown in Figure\n3. A relatively short window (1 - 2 seconds) of the onset\ndetection function is used to calculate a median, which is\nthen used as the threshold. A weight can also be applied\n681Figure 3: An onset detection plot with dynamic threshold-\ning\nFigure 4: Spectrogram of male voice with vibrato\nto the median in order to adjust the threshold slightly. Our\ninitial experiments showed that a dynamic threshold was\nalways more successful than a ﬁxed threshold, so the dy-\nnamic threshold is used in all the reported experiments.\n4.1.2 Improving performance of energy based\ntechniques\nSeveral authors have addressed the integration, along the\nfrequency axis, of the perceived magnitude of an audio\nsignal by calculating changes in each bin output by an FFT\nand integrating their results, (Duxbury et al., 2003). Un-\nfortunately, this technique is vulnerable to false detections\nin the presence of pitch oscillations within a single event,\nsuch as vibrato in a singer’s voice. This effect is shown in\nﬁg 4. The oscillations caused by vibrato move the energy\ninto a different FFT bin at a rate of about 3 - 5 Hz. One\nsolution to this is to divide the frequency domain into a\nsmaller number of overlapping bands, integrate the energy\nwithin a band and then calculate the ﬁrst order differences,\nwhich are subsequently integrated across the bands. We\nused the Mel-frequency scale and the Octave scale for this\nnon-linear integration of bands. The former approximates\na model of the human perception of sound, whilst the later\nis based on one of the primary scales used in music.\nInitial experiments have shown that the octave scale,\nwhich uses fewer, much broader bands, is much less suc-\ncessful for this task. This may be because the bands are\nbroad enough that the sustained portions of several con-\ncurrent sounds overlap within a band and the onset of the\nlater sounds may be missed and interpreted as the sus-\ntained portion of the ﬁrst event. Therefore, results re-\nported here are for Mel-scale or FFT bands.4.1.3 FFT Phase based onset detection\nIn Bello and Sandler (2003) an alternative to energy-based\nonset detection techniques for musical audio streams is\nproposed and Duxbury et al. (2003) combined it with ex-\nisting techniques to produce a complex domain onset de-\ntection function.\nWhen performing spectral analysis of a signal, it is\nsegmented into a series of analysis frames and a Fast\nFourier transformation (FFT) is applied to each segment.\nThe transform returns a magnitude |S(n, k)|and a phase\nϕ(n, k)for each bin. The unwrapped phase, ˜ϕ(n, k), is\nthe absolute phase mapped to the range [−π, π]. Energy\nbased techniques consider only the magnitude of the FFT\nand not the phase, which contains the timing information\nof the signal.\nA musical event can be broken down in to three stages;\nthe onset, the sustained period and the offset. During the\nsustained period of a pitched note, we would expect both\nthe amplitude and phase of the FFT to remain relatively\nstable. However during a transient (onsets and offsets)\nboth are likely to change signiﬁcantly.\nDuring attack transients, we would expect to see a\nmuch higher level of deviation than during the sustained\npart of the signal. By measuring the spread of the distri-\nbution of these phase values for all of the FFT bins and\napplying a threshold we can construct an onset detection\nfunction. Peaks in this detection function correspond to\nboth onset and offset transients so it may need to be com-\nbined with the magnitude changes to differentiate onsets\nand offsets.\n4.1.4 Optimisation\nA dynamic median has three parameters that need to be\noptimised in order to achieve the best performance, the\nmedian window size, the onset isolation window size\nand the threshold weight. In order to determine the best\npossible accuracy achievable with each onset detection\ntechnique, an exhaustive optimisation of these parameters\nwas made. To achieve this, a ground-truth transcription of\nthe onset times of the notes in a number test pieces was\nrequired. This was produced by hand. Eight test pieces,\nfrom four genres, were annotated for this task, each of\nlength 1 minute.\nThe best performing onset detection functions from a\nset of 20 potential functions were examined. These in-\ncluded entropy, spectral centroid, energy and phase based\nfunctions. The results achieved are listed in Table 1. The\ndetection functions are evaluated by the calculation of F-\nmeasure, which is the harmonic mean of the precision (#\ncorrect prediction / total # predictions) and recall (# cor-\nrect predictions / # onsets in the original ﬁles). An onset\nis considered correct if it is within 30 ms of a ground-truth\nonset. F-measure penalises large differences between pre-\ncision and recall and yields a balanced evaluation metric.\nA generalisation of F-measure can be used to weight the\nimportance of each component statistic, but in this context\nthey are of equal importance. The window sizes are re-\nported in numbers of frames, where the frames are 23ms\nin length and are overlapped by 11.5ms. Where a range\n682Figure 5: Noise reduction in detection functions with a\nMel-scale ﬁlter-bank\nof values achieve the same accuracy the smallest window\nsizes are returned to keep memory requirements as low as\npossible.\nTable 1 shows that the two best best performing func-\ntions (results 4 and 5) are based on energy or both en-\nergy and phase deviations in Mel-scale bands. Both tech-\nniques have the very useful feature that they do not re-\nquire a threshold to be set in order to obtain optimal per-\nformance. The small increase in accuracy demonstrated\nby the Mel-band detection functions over the FFT band\nfunctions can be attributed to the reduction of noise in the\ndetection function, as shown in Figure 5.\n5 CLASSIFICATION SCHEME\nIn West and Cox (2004) we presented a new model for\nthe classiﬁcation of feature vectors, calculated from an au-\ndio stream and belonging to complex distributions. This\nmodel is based on the building of maximal binary classiﬁ-\ncation trees, as described by Breiman et al. (1984). These\nare conventionally built by forming a root node contain-\ning all the training data and then splitting that data into\ntwo child nodes by the thresholding of a single variable, a\nlinear combination of variables or the value of a categor-\nical variable. We have signiﬁcantly improved this model\nby replacing the splitting process, which must form and\nevaluate a very large set of possible single variable splits,\nwith a pair of single Gaussian distributions, tested with\nMahalanobis distance measurements. The single Gaus-\nsian distributions can be estimated with either diagonal or\nfull covariance matrices, however full covariance matrix\ndistributions take signiﬁcantly longer to estimate.\nAt each node the set of possible splits are enumerated\nby forming all the combinations of audio classes, without\nrepetition or permutations. A single Gaussian classiﬁer is\ntrained to duplicate each of these splits and the classiﬁer\nreturning the best split is selected and ﬁnalised.\n5.1 Selecting the best split\nThere are a number of different criteria available for eval-\nuating the success of a split. In this evaluation we have\nused the Gini index of Diversity described by Breiman\net al. (1984), which is given by:\nd(t) =/summationdisplay\ni,j,i /negationslash=jp(Ci|t)p(Cj|t) (1)\nFigure 6: Overview of an iteration of the classiﬁcation tree\ntraining process\nwhere t is the current node, p(Cj|t)andp(Ci|t)are the\nprior probabilities of the i-th and j-th classes, at node t,\nrespectively. The best split of node tis the split sthat\nmaximises the change in diversity ( ∆d(s, t)), which is\ngiven by:\n∆d(s, t) =d(t)−PLd(tL)−PRd(tR) (2)\nwhere PLandPRare the proportion of examples in the\nchild nodes tLandtRrespectively. The Gini criterion will\ninitially group together classes that are similar in some\ncharacteristic, but towards the bottom of the tree, will pre-\nfer splits that isolate a single class from the rest of the\ndata. An diagrammatic overview of the splitting process\nis shown in Figure 6.\n5.2 Maximum liklihood classiﬁcation\nWhen classifying a novel example, a likelihood of mem-\nbership of each class is estimated as the percentage of the\ntraining data belonging to each class at the leaf node that\nthe input feature vector exited the tree at, normalised by\nthe prior probabilities of the classes. A whole piece is\nclassiﬁed by summing the log likelihoods of each feature\nvector, which is equivalent to taking the product of the\nlikelihoods values, and selecting the class with the highest\nlikelihood.\nOne difﬁculty with this technique is that not all classes\nhave counts at every leaf node, and hence some of the\nlikelihoods are zero. This would lead to a likelihood of\nzero for any class for which this had occurred. This situa-\ntion might arise if the model is presented with an example\ncontaining a timbre that was not seen in that class during\ntraining. An example of this might be a reggae track con-\ntaining a trumpet solo, when trumpets had previously only\nbeen seen in the Classical and Jazz classes. Therefore, the\nlikelihoods are smoothed using Lidstone’s law, (Lidstone,\n1920). The equation for Lidstone’s smoothing is:\nPLi(i|N) =(ni+ 0.5)\n(n+ (0.5∗C))(3)\n683Table 1: Onset Detection Optimisation results\nOnset detection function Median win Threshold wt Isolation win F-measure\n12nd order FFT band positive 1st order energy differences, summed 30 0.9 14 80.27%\n2Phase deviations multiplied by 1st order energy differences in FFT\nbands, summed30 0.2 16 84.54%\n31st order FFT band positive energy differences, summed 30 0.2 16 86.87%\n41st order positive energy differences in Mel-scale bands, summed 30 0.0 16 86.87%\n5Phase deviations multiplied by 1st order energy differences in Mel-\nscale bands, summed30 0.0 16 88.92%\nOptimisation results calculated over eight 60 second samples\nwhere PLiis the smoothed likelihood of class i,Nis\nthe leaf node that the feature vector was classiﬁed into, ni\nis the number of class itraining vectors at node N,nis\nthe total number of training vectors at node NandCis\nthe number of classes.\n6 TEST DATASET AND\nEXPERIMENTAL SETUP\nIn this evaluation models were built to classify audio into\n7 genres; Rock, Reggae, Heavy Metal, Classical, Jazz &\nBlues, Jungle and Drum & Bass. Each class was com-\nposed of 150, 30 second samples selected at random from\nthe audio database. Each experiment was performed with\n3-fold cross validation.\n6.1 Onset-detection based temporal modelling\nResults reported as using “onset-detection based temporal\nmodelling” were segmented with the best performing on-\nset detector, as detailed in section 4.1. This was a phase\nand energy based onset detector that takes the product of\nthe phase and energy deviations in Mel-scale bands, sums\nthe bands and half-wave rectiﬁes the result in order to pro-\nduce the ﬁnal onset detection function.\n7 CLASSIFICATION RESULTS\n7.1 Analysis\nThe classiﬁcation results in Table 2 show a clear advan-\ntage for the modelling of a sequence of features (results 4,\n5, 6, 7 and 8) over the modelling of a single probability\ndistribution of those features (results 1 and 3). However,\nthe direct modelling of a sequence frames (both 23 ms and\n200 ms frames) is a very complex problem, as shown by\nthe very large number of leaf nodes in the decision tree\nmodels trained on that data. Only diagonal covariance\nmodels were trained on this data as the training time for\nthese models was the longest by far. The use of a sliding\ntemporal modelling window (results 5 and 6) both signif-\nicantly improves the accuracy of these results and simpli-\nﬁes the models trained on the data, whilst including the\nsame number of feature vectors.\nThe use of an onset detection based segmentation\nand temporal modelling (results 7 and 8) yielded slightly\nbetter classiﬁcation results, signiﬁcantly smaller feature\nﬁle sizes, simpliﬁed decision tree models and signiﬁcantly\nfaster execution times than either of the sliding temporal\nmodelling window results. The increased efﬁciencyof the model training process can be attributed to the\nremoval of redundant data in the parameterisation. In the\nsliding window results this redundant data is useful as\nthe complex decision trees must be grown to describe the\nmany distributions and the extra data allows the accurate\nestimation of covariance matrices at lower branches of the\ntree. As the decision trees for data segmented with onset\ndetection are simpler, the redundant data is not necessary.\nA possible explanation for the ability of the directed\nsegmentation to produce simpler decision tree models is\nthat it divides the data into “semantically meaningful”\nunits, in a similar way to the decomposition produced by\nhuman perception of audio, i.e. into individual sounds.\nAn individual sound will be composed of a variety of\naudio frames, some of which will be shared by other, very\ndifferent sounds. This produces complex distributions in\nfeature space, which are hard to model. The use of a tem-\nporal modelling window simpliﬁes these distributions as\nit captures some of the local texture, i.e. the set of frames\nthat compose the sounds in the window. Unfortunately,\nthis window is likely to capture more than one sound,\nwhich will also complicate the distributions in feature\nspace.\nThe use of full covariance matrices in the Gaussian\nclassiﬁers consistently simpliﬁes the decision tree model.\nHowever, it does not neccesarily increase clasiﬁcation ac-\ncuracy and introduces an additional computational cost.\nUsing full covariance models on the sliding window data\nreduced the model size by a third but often had to be re-\nduced to diagonal covariance at lower branches of the tree,\ndue to there being insufﬁcient data to accurately estimate\na full covariance matrix. Using full covariance models on\nthe segmented data reduced the model size by two thirds\nand produced a signiﬁcant increase in accuracy. This may\nbe due to the fact that the segmented data produces fewer,\nmore easily modelled distributions without the complica-\ntions that were introduced by capturing multiple sounds in\nthe sliding window.\n8 CONCLUSION\nWe have shown that onset detection based segmentations\nof musical audio provide better features for classiﬁcation\nthan the ﬁxed or sliding segmentations examined. These\nfeatures produced from onset detection based segmen-\ntations are both simpler to model and produce more\naccurate models. We have also shown, by eliminating\nredundancy, that they make a more efﬁcient use of the\n684Table 2: Segment Classiﬁcation results\nModel description Covar Accuracy Std Dev Leaf Nodes Run-time File size\n123ms audio frames with whole ﬁle window diag 65.60% 1.97% 102 2,602 s 1.4 Kb\n223ms audio frames diag 68.96% 0.57% 207,098 1,451,520 s 244 Kb\n323ms audio frames with non-overlapping 10s win-\ndowdiag 70.41% 2.21% 271 2,701 s 1.8 Kb\n4200ms audio frames full 72.02% 0.13% 48,527 102,541 s 29 Kb\n523ms audio frames with sliding 1s window full 79.69% 0.67% 18,067 47,261 s 244 Kb\n623ms audio frames with sliding 1s window diag 80.59% 1.75% 24,579 24,085 s 244 Kb\n723ms audio frames with onset detection based seg-\nmentationdiag 80.42% 1.14% 10,731 4,562 s 32 Kb\n823ms audio frames with onset detection based seg-\nmentationfull 83.31% 1.59% 3,317 16,214 s 32 Kb\nResults calculated using 3-fold cross validation and proﬁled using a 2.2 GHz AMD Athlon processor with 1 Gb of DDR RAM\ndata available in the audio stream. This supports the\ncontention that onset detection based segmentation of\nan audio stream leads to more musically meaningful\nsegments, which could be used to produce better content\nbased music identiﬁcation and analysis systems than\nother segmentations of the audio stream.\nWe have also shown that Mel-band ﬁltering of onset\ndetection functions and the combination of detection func-\ntions in Mel-scale bands, reduces noise and improves the\naccuracy of the ﬁnal detection function.\nACKNOWLEDGEMENTS\nAll of the experiments in this evaluation were im-\nplemented in the Music-2-Knowledge (M2K) toolkit\nfor Data-2-Knowledge (D2K). M2K is an open-\nsource JA V A-based framework designed to allow Mu-\nsic Information Retrieval (MIR) and Music Digi-\ntal Library (MDL) researchers to rapidly prototype,\nshare and scientiﬁcally evaluate their sophisticated\nMIR and MDL techniques. M2K is available from\nhttp://music-ir.org/evaluation/m2k .\nREFERENCES\nJ. P. Bello and M. Sandler. Phase-based note onset detec-\ntion for music signals. In In proceedings of the IEEE In-\nternational Conference on Acoustics, Speech, and Sig-\nnal Processing . Department of Electronic Engineering,\nQueen Mary, University of London, Mile End Road,\nLondon E1 4NS, 2003.\nL. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone.\nClassiﬁcation and Regression Trees . Wadsworth and\nBrooks/Cole Advanced books and Software, 1984.\nS. Dixon, E. Pampalk, and G. Widmer. Classiﬁcation of\ndance music by periodicity patterns. In Proceedings of\nthe Fourth International Conference on Music Informa-\ntion Retrieval (ISMIR) 2003 , pages 159–166, Austrian\nResearch Institute for AI, Freyung 6/6, Vienna 1010,\nAustria, 2003.\nC. Duxbury, J. P. Bello, M. Davis, and M. Sandler. Com-\nplex domain onset detection for musical signals. In Pro-\nceedings of the 6th Int. Conference on Digital AudioEffects (DAFx-03), London, UK. Department of Elec-\ntronic Engineering, Queen Mary, University of London,\nMile End Road, London E1 4NS, 2003.\nM. Goto and Y. Muraoka. Beat tracking based on\nmultiple-agent architecture – A real-time beat tracking\nsystem for audio signals. In Proceedings of the First In-\nternational Conference on Multi-Agent Systems (1995) .\nMIT Press, 1995.\nT. Heittola and A. Klapuri. Locating segments with drums\nin music signals. In Proceedings of the Third Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR) . Tampere University of Technology, 2002.\nD.-N. Jiang, L. Lu, H.-J. Zhang, J.-H. Tao, and L.-H.\nCai. Music type classiﬁcation by spectral contrast fea-\nture. In Proceedings of IEEE International Conference\non Multimedia and Expo (ICME02), Lausanne Switzer-\nland, Aug 2002.\nG. J. Lidstone. Note on the general case of the bayes-\nlaplace formula for inductive or a posteriori probabili-\nties. Transactions of the Faculty of Actuaries , 8:182–\n192, 1920.\nW. Schloss. On the Automatic Transcription of Percussive\nMusic: From Accoustic Signal to High Level Analysis .\nPhD thesis, Stanford University, CCRMA., 1985.\nA. P. Schmidt and T. K. M. Stone. Music classiﬁcation\nand identiﬁcation system. Technical report, Department\nof Computer Science, University of Colorado, Boulder,\n2002.\nG. Tzanetakis. Marsyas: a software framework\nfor computer audition. Web page, October 2003.\nhttp://marsyas.sourceforge.net/.\nG. Tzanetakis, G. Essl, and P. Cook. Automatic musical\ngenre classiﬁcation of audio signals. In Proceedings of\nThe Second International Conference on Music Infor-\nmation Retrieval and Related Activities , 2001.\nK. West and S. Cox. Features and classiﬁers for the au-\ntomatic classiﬁcation of musical audio signals. In Pro-\nceedings of the Fifth International Conference on Music\nInformation Retrieval (ISMIR) , 2004.\nC. Xu, N. C. Maddage, X. Shao, F. Cao, and Q. Tian.\nMusical genre classiﬁcation using support vector ma-\nchines. In in Proceedings of ICASSP 03, Hong Kong,\nChina. , pages 429–432, April 2003.\n685"
    },
    {
        "title": "Efficient Melody Retrieval with Motif Contour Classes.",
        "author": [
            "Tillman Weyde",
            "Christian Datzko"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1414738",
        "url": "https://doi.org/10.5281/zenodo.1414738",
        "ee": "https://zenodo.org/records/1414738/files/WeydeD05.pdf",
        "abstract": "This paper describes the use of motif contour classes for efficient retrieval of melodies from music collections. Instead of extracting incipits or themes, complete monophonic pieces are indexed for their motifs, using classes of motif contours. Similarity relations between these classes can be used for a very efficient search. This can serve as a first level search, which can be refined by using more computationally intensive comparisons on its results. The model introduced has been implemented and tested using the MUSITECH framework. We present empirical and analytical results on the retrieval quality, the complexity, and quality/efficiency trade-off. Keywords: melody retrieval, motivic analysis, melodic similarity, retrieval efficiency 1",
        "zenodo_id": 1414738,
        "dblp_key": "conf/ismir/WeydeD05",
        "keywords": [
            "motif contour classes",
            "efficient retrieval",
            "melodies from music collections",
            "motifs",
            "indexed for their motifs",
            "classes of motif contours",
            "similarity relations",
            "search",
            "MUSITECH framework",
            "empirical and analytical results"
        ],
        "content": "EFFICIENT MELODY RETRIEV AL WITH MOTIF CONTOUR CLASSES\nTillman Weyde\nCity University\nSchool of Informatics\nDepartment of Computing\nLondon, UK\nt.e.weyde@city.ac.ukChristian Datzko\nUniversity of Osnabrück\nResearch Department of\nMusic and Media Technology\nOsnabrück, Germany\nchristian@datzko.ch\nABSTRACT\nThis paper describes the use of motif contour classes for\nefﬁcient retrieval of melodies from music collections. In-\nstead of extracting incipits or themes, complete mono-\nphonic pieces are indexed for their motifs, using classes of\nmotif contours. Similarity relations between these classes\ncan be used for a very efﬁcient search. This can serve as\na ﬁrst level search, which can be reﬁned by using more\ncomputationally intensive comparisons on its results. The\nmodel introduced has been implemented and tested using\nthe MUSITECH framework. We present empirical and\nanalytical results on the retrieval quality, the complexity,\nand quality/efﬁciency trade-off.\nKeywords: melody retrieval, motivic analysis, melodic\nsimilarity, retrieval efﬁciency\n1 INTRODUCTION\nAlthough audio retrieval has been in the focus of atten-\ntion lately, melody retrieval based on symbolic music rep-\nresentations is important for databases like Themeﬁnder1\nand Digital Tradition2and will likely become more impor-\ntant in the near future with the development of the MPEG\nstandard on Symbolic Music Representation3. The most\ncommon approach for melody retrieval today is to com-\npare a query to a melody that has somehow been extracted\nfrom a piece of music, comparing query and melody as a\nwhole using an edit distance approach.\nThe basic idea of this paper is to use musical motifs\nfor melody retrieval, similarly as words are used in text\nretrieval. Musical motifs, as deﬁned by Riemann (1903 ),\nare the smallest musically meaningful parts of a melody.\n1http://www.themefinder.org/\n2http://www.mudcat.org/AboutDigiTrad.cfm\n3http://www.interactivemusicnetwork.org/\nmpeg-ahg/\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitation on the ﬁrst page.\nc°2005 Queen Mary, University of LondonRetrieval based on motifs is independent of the size of\nthe pieces in the database. Provided the voice structure\nis known, complete symphonies can be indexed and re-\ntrieved this way. This method captures similarities that re-\nsult from recombination of motivic material. Similarities\nbetween motifs can also be taken into account.\nIndexing techniques relying on manual extraction of\nthemes etc. are costly and limit the search, therefore au-\ntomatically indexing a complete piece of music is a way\nof efﬁciently making more information accessible. Work\nin this direction has been done by Clausen et al. (2001 ),\nwhose system builds an index for every note in a piece –\nleading to large indexes – and requires information on the\nmetrical structure. An approach to use more musical in-\ntelligence has been taken by Melucci and Orio (2004 ) by\nindexing musical phrases. We chose to use motifs rather\nthan phrases, because motifs are shorter and not as spe-\nciﬁc to a piece and therefore better suited for building an\nindex.\n2 ARCHITECTURE\nThe approach taken here is to index motifs using their\ncontour classes, thus reducing the number of entries by\njoining similar motifs. Motif classes have been used for\na similarity based search using representation at different\ndegrees of abstraction by Melucci and Orio (2004 ), but\nthat approach is insufﬁcient to reﬂect the musical similar-\nity of melodies, because it means introducing or reducing\nsubdivisions of classes. However, similarities across the\nhighest level of class boundaries cannot be detected. To\nbroaden the search, here the similarity of different con-\ntour classes is utilized for fault tolerant and incremental\nsearch.\nEspecially on large databases, a sophisticated measure\nof similarity may be useful but computationally too ex-\npensive to perform on all pieces. These measures can be\nutilized in our system as a second-level search on the re-\nsult set of the ﬁrst search based on motif classes.\nBefore one can search a database of pieces, an index\nis generated with the following steps:\n1.segmentation of the pieces\n2.contour analysis of the motifs\n3.indexing of motif contours\nOnce the index is created, melodies can be searched for.\nThis is done in ﬁve steps:\n6861.segmentation of the query\n2.contour analysis of the motifs\n3.search for the motif classes\n4.calculating similarity of the motif classes\n5.ﬁne search on the results\n3 INDEX GENERATION\n3.1 Segmentation\nThe segmentation divides a melody into motifs as in ﬁg-\nure1. It is the ﬁrst step in processing and is important for\nthe quality of the results. The more the segments form mu-\nsically meaningful motifs of the melody, the better search\nresults can be expected. To enable experiments on this, the\nactual segmenter module is exchangeable. For efﬁciency\nand simplicity a simple segmenter has been used in these\nﬁrst experiments that sets the boundaries at the local max-\nima of note distances. This approach has been applied\nsuccessfully ( Cambouropoulos ,2001 ), but using a more\nsophisticated segmenter could improve search results (see\nWeyde ,2002 ).\nFigure 1: Segmentation of the melody “Der Mai ist\ngekommen” (German folk song).\n3.2 Contour Analysis of Segments\nContour information is stored for two aspects of a seg-\nment: its rhythmic values and its pitches. The motif is\ncoded using a diastematic index. This means that only\nthree values per dimension are used for the transition be-\ntween two notes: whether a note is longer, shorter, or of\nequal length and whether a note is higher, lower, or of\nsame pitch. This can be represented as a character string\ns(M), where M=n1; : : :; n mis a melody, L(ni)the\nlength of note niandP(ni)its pitch, and +denotes string\nconcatenation:\ns(M) =(\ns(n1; : : :; n m¡1) +k(nm¡1; mn)ifm¸2\n“ ” if m < 2\n(1)\nk(n1; n2) =8\n><\n>:“U” if P(n1)< P(n2)\n“E” if P(n1) =P(n2)\n“D” if P(n1)> P(n2)(2)\n+8\n><\n>:“L” if L(n1)< L(n2)\n“E” if L(n1) =L(n2)\n“S” if L(n1)> L(n2)\n3.3 Indexing of the Segment Contours\nA contour string K= (k1; : : : ; k n)is converted into an\ninteger number using the bijective function e:\ne(K) =(\ne(k1:::n¡2)¢24+b(kn¡1; kn)ifn >0\n0 otherwise\n(3)b(k1; k2) =8\n><\n>:0100bifk1=“U”\n1000bifk1=“E”\n1100bifk1=“D”(4)\n+8\n><\n>:01bifk2=“L”\n10bifk2=“E”\n11bifk2=“S”\nsallows to code motifs up to a length of 9 notes in a 32-bit\ninteger. This is below the theoretical limit of 10 notes4,\nbut it is sufﬁcient because musically meaningful motifs\ndo not exceed this length due to limits of perception (see\nMiller ,1956 ;Swain ,1986 ). If a segmenter produces seg-\nments longer than 9 notes, these notes will be cut off in\nthe current implementation.\nThe contours of the example in ﬁgure 1would\nbe UEULEL = 011001011001 b= 1625 , ULDL =\n01011101 b= 93 , UL = 0101 b= 5, EL = 1001 b= 9\nand DL = 1101 b= 13 . This coding as integer values\nallows efﬁcient indexing of pieces by motif classes.\n4 SEARCH PROCEDURE\nThe search is carried out in two stages: a rough search that\nuses a similarity value based only on the motif classes, and\na ﬁne search that elaborates on the result set.\n4.1 Rough Search\nFirstly the query melody is segmented and the motif con-\ntours are classiﬁed as described above. Then the pieces\nare rated for their similarity and relevance to the query.\n4.1.1 Motif Class Similarity\nThe local similarity of two motif classes k; lis calculated\nas\nlsim(k; l) =1\nd(k; l) + 1; (5)\nwhere d(k; l)is the Levenshtein distance between con-\ntours (see Gilleland 2004 ). It calculates the minimum\nnumber of changes needed to transform contour Sinto\ncontour Q. A change is either the insertion, the deletion\nor the change of a transition as described in section 3.2.\n4.1.2 Weighting\nThe contour classes of the query and of pieces in the data-\nbase are weighted based on their frequency in the data-\nbase and in the melodies as in standard text information\nretrieval ( van Rijsbergen ,1979 ):\nwS;l=vS;l\np\nP\nkvS;k; (6)\nwith\nvS;l=(\nf(S; l)¢log³\nN\nnl´\nifnl6= 0\n0 otherwise(7)\n4Rhythm and pitch can each take 3 values yielding 9 combina-\ntions per note transition plus 1 if no note is present. Therefore\nthe number of possible values for a motif of length lis10l¡1.\nThe maximal length of motifs that can be represented is there-\nforeblog10(232) + 1c= 10 .\n687where f(S; l)is the frequency of motif class lin\nmelody S,Nis the total number of pieces in the data-\nbase, and nlis the number of pieces containing a motif\nof class l. Imposing a penalty on common motif classes\nintroduces a relevance aspect based on the pieces in the\ndatabase, in addition to the concept of similarity between\ntwo melodies. The weights wS;lare normalized to make\nthe values independent of the lengths of the S.\n4.1.3 Similarity of Melodies\nFor the overall rating of a piece and in relation to a query,\nthe most similar motif class in the piece is determined for\nevery motif class present in the query, yielding a set Lmax\nof pairs (k; l)of motif classes. The rating sim of a query\nQand a piece Sis then calculated as\nsim(Q; S) =X\n(k;l)2Lmaxlsim(k; l)¢wQ;k¢wS;l (8)\nThesim measure yields values between 0 for completely\ndifferent and 1 for identical melodies. The value 1 is also\nreached by melodies sharing the same distribution of mo-\ntif classes. We did not encounter such a case in the exper-\niments, but this possibility makes it especially useful for\nlarge databases to have a ﬁne search that enables differen-\ntiation between such melodies.\n4.2 Fine Search\nThe ﬁne search takes the results of the rough query and\nperforms comparisons, that are based on their the actual\nnotes instead of the contours, which may be computation-\nally more expensive. Currently two different ﬁne search\nmethods are implemented. They calculate similarity mea-\nsures on the motifs, which are multiplied with the sim\nvalue. One method returns 1only if the two compared\nsegments are identical except for transposition and tempo\nchanges, the second uses the CubyHum algorithm ( Pauws ,\n2002 ), which is an edit distance variant specialized for\nmelody retrieval.\n5 EV ALUATION\nThe described system has been implemented and tested\nusing the MUSITECH framework (see Gieseking and\nWeyde ,2002 ).\n5.1 Quality\nFor a ﬁrst quality assessment we used small subsets of\nground truth data from the experiments of Typke et al.\n(2005 ) and Müllensiefen and Frieler (2004 ) giving cor-\nrelations of 0:59and0:36between our sim values and the\nground truth, which we see as a good result for a rough\nsearch. The currently implemented ﬁne search improved\nthe correlation only by up to 0.03, which indicates that\nhere is a need for a different method.\n5.2 Complexity\nThe time needed to compute the motif classes only de-\npends on the length mof a melody, thus taking O(m)\n 0 10 20 30 40 50 60 70 80\n 0  1  2  3  4  5\nd maxRecall error (in %)\nindex size, 5 notes, (in 10 MBs)\nindex size, 6 notes\nindex size, 7 notes\nFigure 2: Recall error and index size depending on dmax\nand maximal motif length.\ntime. A database containing Npieces of average length\nmneeds O(m¢N)time to be indexed. These values de-\npend only on the pieces in the database, and therefore only\nneed to be computed once for each piece. The amount of\nspace required for the index also depends on the number\nof motif classes in a melody, which means at most linearly\non the length of the melody, and the size of the database.\nSo the space requirement for the index is also O(m¢N).\nThe time requirement for a naive implementation of\nthe rough search is O(q¢m¢N¢l2)where qis the length\nof the query, since every motif class of the query must be\ncompared to each motif class of every piece in the data-\nbase. l2is the complexity of calculating the lsim values,\nwhere lis the length of the motifs.\nThe ﬁne search uses more time for each comparison,\nfor example when using the CubyHum algorithm, it uses\nO(q2¢m2¢R)time, where Ris the size of the result set.\n5.3 Optimization\nThe naive implementation of the rough search is too slow\nto be used on large databases, but it can be optimized us-\ning the opportunities provided by the motif classes. It is\npossible to perform much of the computing in advance and\nto use inverted ﬁles, reducing the time needed to search a\npotentially large database with the trade-off of higher stor-\nage demands.\nSince the number of motif classes is limited, lsim\nvalues of classes can be calculated beforehand, indepen-\ndently of any actual data. If the similarity for every pair\nof classes is stored, the size of the index then depends on\nthe length of the motifs as (Plmax\nk=19k¡1)2where lmax is\nthe maximal length of the motifs. Judging from the liter-\nature, a motif length of 7 is sufﬁcient, probably less ac-\ncording to Swain (1986 ). Yet storing the lsim value for\neach combination would result in over a terabyte of data\nwhich is out of the scope of current computers. The in-\ndex size can be reduced by indexing only those pairs of\nmotif classes, where the lsim value is above a threshold\nlsim min, resp. d(k; l)below a threshold dmax. This re-\nduces reduces the size of the lsim index and the number\nof matches for which a sim value has to be calculated.\nThe trade-off is a loss in recall, i.e. some of the pieces be-\n688low the threshold might have received a good ﬁnal rating.\nWe computed the loss of recall of the rough search on a\nset of almost 4000 MIDI ﬁles from the Digital Tradition\ndatabase5and the size of the index for different values of\ndmax andlmax as shown in ﬁgure 2. The recall error was\ncalculated as the portion of pieces in the top 10% that is\nexcluded by the threshold (which is in this case identical\nto the precision value). It drops sharply from dmax values\n0to1from over 70% to 1,75% and to 0,25% at dmax= 2,\nwhile the index size grows exponentially with at dmaxand\nlmax. A good compromise can be found at dmax= 1and\nlmax= 6where the index size is below 50 MB which can\neasily be held in RAM. This method also supports incre-\nmental search, ﬁrst using dmax= 0and then dmax= 1.\nThe class frequencies can be stored along with the\nmelodies in the motif index, as can the inverted frequen-\ncies of classes over the database, but they must be updated\nat every change of database content. Using this method,\nthe whole rough search takes O(q¢Rt)time where Rtis\nthe size of the result set after the threshold. The constant\nfactor is also reduced, because of the lookups of the lsim\nvalues. The size of Rtdepends on the data and the queries\nand was with dmax= 1 andlmax= 6 at about 50% of\nthe database size in our experiments. The size of Rtcan\nbe inﬂuenced by a second threshold dprod on the product\nwQ;k¢lsim(k; l)¢wS;l. Since wQ;kis available at the time\nof the search and both other values are precomputed, it can\nbe used to reduce Rtbefore computing the sim values.\n6 CONCLUSIONS\nThe use of motif classes brings two advantages to melody\nretrieval. It makes the matching of the query with\nmelodies in the database independent of the position and\norder of motivic material used. It therefore makes the\nextraction of themes unnecessary because the length of\npieces it not relevant. It also captures musical relations,\nthat are not taken into account by approaches based on the\nedit distance of whole melodies.\nIt also offers methods for efﬁcient and incremental\nsearching through the use of indexes and similarities. The\nuse of precalculated similarities of motif classes allows\nvery efﬁcient and musically adequate incorporation of in-\nexact matches.\nDirections for further research include testing on fur-\nther ground truth data, experimenting with ﬁne search\nmethods to improve the end results, tests with more elab-\norate segmenters, and the investigation of methods to con-\ntrol the size of the result set Rtin oder to optimize search\ntimes and result quality.\nREFERENCES\nE. Cambouropoulos. The local boundary detection model\n(lbdm) and its application in the study of expressive\ntiming. In Proceedings of the International Com-\nputer Music Conference 2001 , pages 290–293, Ha-\nvanna, Cuba, 2001.\nM. Clausen, F. Kurth, and R. Engelbrecht. Context-based\n5http://sniff.numachi.com/~rickheit/dtrad/ .retrieval in midi and audio. In D. Fellner, N. Fuhr, and\nI. Witten, editors, ECDL Workshop: Generalized Doc-\numents , Darmstadt, 2001.\nM. Gieseking and T. Weyde. Concepts of the musitech\ninfrastructure for internet-based interactive musical ap-\nplications. In C. Busch, M. Arnold, P. Nesi, and\nM. Schmucker, editors, Proceedings of the Second In-\nternational Conference on WEB Delivering of Music\n(WEDELMUSIC 2002) , pages 30–37, Darmstadt, 2002.\nIEEE / Fraunhofer IGD.\nM. Gilleland. Levenshtein dis-\ntance, in three ﬂavors, 2004. URL\nhttp://www.merriampark.com/ld.htm .\nM. Melucci and N. Orio. Combining melody process-\ning and information retrieval techniques: methodology,\nevaluation, and system implementation. J. Am. Soc. Inf.\nSci. Technol. , 55(12):1058–1066, 2004.\nG. A. Miller. The magical number seven, plus or minus\ntwo: Some limits on our capacity for processing infor-\nmation. The Psychological Review , 63:81–97, 1956.\nD. Müllensiefen and K. Frieler. Cognitive adequacy in\nthe measurement of melodic similarity: Algorithmic vs.\nhuman judgments. Computing in Musicology , 13:147–\n176, 2004.\nS. Pauws. Cubyhum: A fully operational query by hum-\nming system. In M. Fingerhut, editor, ISMIR 2002 Con-\nference Proceedings – Third International Conference\non Music Information Retrieval , Paris, 2002. IRCAM –\nCentre Pompidou.\nH. Riemann. System der musikalischen Rhythmik und\nMetrik . Breitkopf und Härtel, Leipzig, 1903.\nJ. P. Swain. The need for limits in hierarchical theories of\nmusic. Music Perception , 4(1):121–148, 1986.\nR. Typke, M. den Hoed, J. de Nooijer, F. Wiering, and\nR. C. Veltkamp. A ground truth for half a million mu-\nsical incipits. In Proceedings of the 5th Dutch-Belgian\nInformation Retrieval Workshop (DIR) 2005 , pages 63–\n70, Utrecht, 2005.\nC. J. van Rijsbergen. Information Retrieval . Butterworth,\nLondon, 1979.\nT. Weyde. Integrating segmentation and similarity\nin melodic analysis. In K. Stevens, D. Burnham,\nG. McPherson, E. Schubert, and J. Renwick, editors,\nProceedings of the International Conference on Music\nPerception and Cognition 2002 , pages 240–243, Syd-\nney, Australia, 2002.\n689"
    },
    {
        "title": "On Techniques for Content-Based Visual Annotation to Aid Intra-Track Music Navigation.",
        "author": [
            "Gavin Wood",
            "Simon O&apos;Keefe"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1417401",
        "url": "https://doi.org/10.5281/zenodo.1417401",
        "ee": "https://zenodo.org/records/1417401/files/WoodO05.pdf",
        "abstract": "Despite the fact that people are increasingly listening to music electronically, the core interface of the common tools for playing the music have had very little improvement. In particular the tools for intra-track navigation have remained basically static, not taking advantage of recent studies into the field of audio jisting, summarising and segmentation. We introduce a novel mechanism for musical audio linear summarisation and modify a widely used open source media player to utilise several music information retrieval techniques directly in the graphical user interface. With a broad range of music, we provide a qualitative discussion on several techniques used for contentbased music information retrieval and perform quantitative investigation to their usefulness. 1",
        "zenodo_id": 1417401,
        "dblp_key": "conf/ismir/WoodO05",
        "keywords": [
            "music",
            "electronic",
            "core interface",
            "intra-track navigation",
            "audio jisting",
            "audio summarisation",
            "audio segmentation",
            "media player",
            "music information retrieval",
            "content-based music information retrieval"
        ],
        "content": "ONTECHNIQ UES FOR CONTENT -BASED VISU ALANNO TATION TO\nAID INTRA-TRA CKMUSIC NAVIGA TION\nGavinWood\nUniversity ofYork\nYorkYO10 5DD\nUnited Kingdom\ngav@cs.york.ac.ukSimon O'Keefe\nUniversity ofYork\nYorkYO10 5DD\nUnited Kingdom\nsok@cs.york.ac.uk\nABSTRA CT\nDespite thefactthatpeople areincreasingly listening to\nmusic electronically ,thecore interf aceofthecommon\ntools forplaying themusic havehadverylittle impro ve-\nment. Inparticular thetools forintra-track navigation\nhaveremained basically static, nottaking advantage ofre-\ncent studies into the\u0002eld ofaudio jisting, summarising\nandsegmentation.\nWeintroduce anovelmechanism formusical audio\nlinear summarisation and modify awidely used open\nsource media player toutilise severalmusic information\nretrie valtechniques directly inthegraphical user inter-\nface. Withabroad range ofmusic, weprovide aquali-\ntativediscussion onseveraltechniques used forcontent-\nbased music information retrie valandperform quantita-\ntiveinvestig ation totheir usefulness.\n1INTR ODUCTION\nInrecent years thetechniques forcontent based analysis of\nmusical audio haveimpro veddramatically .Moore' slaw\ncontinues steadily toprovide softw arewith ever-greater\nresources with respect toprocessing power,andtheex-\ntrastorage available formedia hasmeant thatweareable\ntostore ourentire music collection fordigital playback.\nGraphical interf aces tomedia players havebecome more\nelaborate and most mainstream softw arenowsupports\nsome sortofvisualisation ofthemusic asitplays.1\nIntheoriginal generation ofthegraphical media\nplayer ,atypical user interf acefeature would bethetime\nbar. This allowed theuser tovisualise howfarthrough\nthecurrent track theywere, inrelation tothelength ofthe\ntrack. This was,inmanyways, similar totheprogress\nbarinorder toshowtheuser howmuch ofaparticular\ntask iscompleted, with theexception thattime bars may\n1Though inmanycases thecorrespondence between audio\nandvideo leavesmuch tobedesired.\nPermission tomakedigital orhard copies ofallorpart ofthis\nworkforpersonal orclassroom useisgranted without feepro-\nvided thatcopies arenotmade ordistrib uted forpro\u0002t orcom-\nmercial advantage andthatcopies bear thisnotice andthefull\ncitation onthe\u0002rstpage.\nc\r2005 Queen Mary ,University ofLondonbeused todirectly navigatethrough atrack byclicking\nsome wayalong it.Theplayer would resume playing at\nthecorresponding point through thetrack. However,asa\nnavigation tool itsuseislimited duetothefactthatthe\nuser hadtoknowinadvance theapproximate place onthe\nbartodeliverthewanted moment ofthetrack.\nInorder toimpro vetheusefulness ofthistime bar,\nsome extrainformation must beadded toit,providing the\nuser with some visual cues. This allowstheuser tobetter\nguess which point along itmaps totheparticular moment\ntheyaretrying to\u0002nd. Manystudies (e.g., Bock eretal.,\n1986) haveshownvisual cues tobeasimple andeffective\nmeans toconveyinformation totheuser without confus-\ninganovice ordistracting onealready familiar .Wecall\nthisvisual annotation amood bar, referring tothevary-\ningshades todepict themusic content.\nThere isworkaplenty inthe\u0002eld ofIRwith respect to\nanalysing andclassifying individual segments ofmusical\naudio, perhaps with aviewtoarchi ving, retrie val,group-\ning,large-scale exploration andbrowsing. Extensi vework\nhasbeen carried outintoforming theuser interf acetodeal\nwith thisfunctionality .Comparati velylittle hasbeen done\ntotheinterf aceonce thenecessary segment hasbeen lo-\ncated andisready tobeplayed. One might assume that\nonce theuser hastheir segment ofdatabe itamusic\ntrack, amonolithic compilation (e.g. liveperformance) or\nperhaps aradio broadcastthe yarehapp ytohaveitplay\nthroughout.\nTheconcept ofuser-directed navigation isessential to\nthiswork.Wesetoutnottoproduce avisual annotation by\nwhich some (howeversmall) absolute truth may begained\nfrom onesingle sample. Instead wetakeamore holistic\napproach andfree ourselv esfrom theconstraint thatthe\nannotation must mean something absolute andconcrete.\nWeallowourvisualisation totakeonanyabstract form,\nandjudge performance astowhat, asahuman, weareable\ntoascertain from the\u0002nal depiction. Wegoontomeasure\nhowuseful these depictions areforsearching tasks with a\nbroad range ofmusic.\nThetask isanacute acid test;i.e.wegivethepartici-\npants anabsolute minimum oflearning time. Assuch the\nresults will heavily favourtheannotation methods with\nmore obvious visual cues tothose with amore comple x\nvisual representation. This isbecause wewish totestreal-\nistic casual usage; people should nothavetosufferasig-\nni\u0002cant learning curvetouseamedia player .Inparticular\n58Figure 1:The amaroK music player with themoodbar\noperational.\nwetestwhether theaddition ofcolour impro vesperfor -\nmance when theparticipants havenoprior experience of\nthenewinterf ace.\n1.1 Related Work\nLittle workhasbeen openly published speci\u0002cally regard-\ningintra-track navigation. The most connected workto\nthat presented here isbyTzanetakis andCook (2000a),\nwho havediscussed theMarsyas augmented sound editor .\nThis isabasic sound editor thatcancolour -intheedited\nwaveform according tosome particular acoustic charac-\nteristics. Apparently ,thislineofresearch wasnotcontin-\nuedanyfurther since itremains around only intheoriginal\nniche application (theMarsyas augmented sound editor).\nThe technology presented here iscomparati ve;however\nweevaluate itinmore depth andwith user studies. We\nalso utilise severaldifferent methods forcalculating the\ncolour andpresent anovelmethod. Tzanetakis andCook\n(2000b) later provided some insight intosegmentation and\npossible methods intheir article onMarsyas.\nThere issigni\u0002cant literature intherelated arena of\nconte xtbased segmentation ofaudio; Raphael (1999) has\npresented asegmentation method with Mark ovmodels.\nFoote (1999), then later inFoote andCooper (2003) and\nCooper andFoote (2003), present amechanism forcalcu-\nlating musical novelty,with aviewtosegmentation and\njisting. The workdoesn' tgosofarastoquantitati vely\nevaluate theusefulness, instead presenting thetechniques\nanddiscussing theoutput.\nCouprie (2004) hasgivenamost interesting discussion\nonpossible intuiti vegraphical representations ofmusic.\nHesuggests thedisplay ismade ofdiscrete elements rather\nthan acontinuous form (that might beeasier when work-\ningwith source audio). The discussion does arguewell\nthatnavigational aids arehelpful innumerous situations.\nBrazil etal.(2002) discussed intra-collection browsing\nthrough track features asdidTzanetakis (2003) formu-\nsical audio. Asimilar problem wasapproached byBlack-\nburnandDeRoure (1998) forMIDI navigation between\ntracks using pitch contours.\n2USER-INTERF ACEDESIGN\nFollowing theprinciple ofleast surprise, wechanged the\nplayback interf aceaslittle aspossible. Themedia playerwesetabout toaugment, amaroK (2005), already pro-\nvided ahighly intuiti veinterf acewith thenow-standard\ntrack slider bar.InamaroK' scase atriangular pointer\nscrolls across thetopofthebardenoting thecurrent posi-\ntionoftheplayer through thetrack. Theonly change we\nmade totheinterf acewastohavetheinternal portion of\nthebarcoloured (with vertical lines) according tosome\nanalysis metric. The colour changes along thex-axis,\nwhich represents time. Because weallowed ourselv esuse\nofcolour each point onthexaxis corresponds toa3D\nvalue using theRGB components ofthecolour .Asbefore\ntheuser isfreetoclick anywhere onthebartowarpthe\nplayer tothecorresponding position inthesong.\n3ANALYSIS TECHNIQ UES\nSeveral main techniques areused inorder topopulate\nthemood bar. These techniques canbesplit into two\ngroupsthose thatresult inonly onevalue andthose that\nresult inthree values. Those thatresult inonevalue were\ntransformed intoacolour byusing itastheluminosity of\nashade ofgray.Those giving three values were trans-\nformed intoacolour byassigning each value toeither the\nred,green orblue component.\nThespectra were calculated byusing aseries ofFFTs\noverthesignal. Thewindo wsizeused was1024 samples,\nwith a50% overlap. Withtheinput signal being attheCD\nstandard 44100Hz, thisputsthelowest frequenc ytobede-\ntected ataround 43Hz with windo wsbeing around 11ms\napart. Thestereo signals were \u0002rstdownmix edintomono,\ntopreventanyproblematic stereo seperation effects.\nApsychoacoustic variant ofthemechanisms wasused,\nwhere thespectra were \u0002rstsummed intothecritical bands\nontheBark scale. This signi\u0002cantly cuts downonthe\ncomputation cost inmanyareas since the512 bands of\ntheFFT output isreduced toonly 24critical bands. Ini-\ntialexperimentation backedupbyprevious studies (such\nasWoodandO'Keefe, 2003) showed thatitmade little\ndiscernable difference tothe\u0002nal performance.\nAlso, thefullgamut ofoutput forthetrack wasnor-\nmalised before being changed intoacolour .Thenormali-\nsation technique used wasasimple min/max stretch given\nby:\nvaluenormalised\u0011(value\u0000valuemin)\nvaluemax\u0000valuemin\n3.1 Spectral Magnitude\nWeused thespectral magnitude calculation toprovide us\nwith a1dimensional representation oftheaudio track:\nBecause ofitssimplicity itwasused asabenchmark\ntowhich other techniques could becompared.\n3.2 Novelty\nFoote (1999) \u0002rstdetailed thenovelty score: Itprovides\navalue determined bythecross dissimilarity ofthepor-\n59tions ofsignal before andafter themoment intime, as\nshowninthework\u0003o wdiagram:\nItrelies upon aprior abstraction ofthesignal known\nasaself-similarity matrix, which iscalculated simply by\nevaluating thesimilarity ofthesignal toitself atvarying\ninterv als(givenbyx\u0000y).Thesimilarity between thetwo\nsignals isthecosine oftheangle between thetwospectra\nwhen expressed asvectors, assuggested intheliterature.\nDC(Sx;Sy)\u0011Sx\u000fSy\nkSxkkSyk\nwhereSisthespectrum inquestion.\nAcheck erboard weighting isapplied tothematrix\nwith aGuassian taper weighting andthevalues summed.\nThis isthenoveltyscore ofthemoment atthecentre of\ntheinput series ofspectra. Thespectra were calculated in\nthesame manner asthetechnique above.Thekernel (with\ntheGuassian taper) isgivenby:\nK(x;y)\u0011\u001a\nG(x;y);(x>0)=(y>0)\n\u0000G(x;y);(x>0)6=(y>0)\nwhere\nG(x;y)\u0011Gaussian (k\u00122x\ns;2y\ns\u0013\nk)\nWhere xandyboth fallintherange [\u0000s\n2;s\n2]andthe\nkernel matrix isofwidth s.\nThe size oftheself-similarity matrix andaccompa-\nnying check erboard kernel were experimented with and\nqualitati velyevaluated. Wefound avalue ofaround 128\nspectra (1.49 seconds) provided agood balance between\ntime precision andlargerscale feature presentation. Fig-\nure4demonstrates thedifferences inmatrix sizeonsev-\neraltracks.\n3.3 Rhythm Magnitude\nTherhythm magnitude isanoveltechnique todeliverthe\nrhythmicity ofaudio ataparticular point. Itiscalcu-\nlated byusing therhythm spectrum (also knownasbeat\nspectrum) asavector andtaking itsmagnitude:\nWeusethealgorithm proposed byFoote (1999) for\ncalculating therhythm spectra, which involvespopulat-\ningaself-similarity matrix andsumming across thesuper -\ndiagonals.\nB(l)\u0011s\u0000lX\nk=0M(k;k+l)where sisthesize oftheself-similarity matrix M.\nTzanetakis andCook (2000a) havediscussed other tech-\nniques such asthebeat histogram.\nAhigher value (i.e. lighter shade) iscaused byhav-\ningmore powerintherhythm spectrum. Ahigher lag-\ncorrelation denotes astronger rhythm which willcause a\nlighter shade tobeoutput. Ifthere islittle correlation,\noritiscompromised between twosuccessi veandunique\nrhythms then itshould havealoweroverall powerandthus\nbedarkerinshade.\n3.4 Band wise Spectral Magnitude\nWedevised thespectral magnitude ratio metric asanovel\nwaytointroduce colour intothemood bar.Partofthepre-\ncessing pipeline issplit into three separate channels for\nred,green andblue respecti vely.The point atwhich the\nsplit takesplace isdirectly after theBark critical band-\ning; here wetakethe24bands andsplit them into 38-\nband subspectra. Each spectra isthen used asan8-\ndimensional vector towhich themagnitude iscalculated\n(astheEuclidean distance from 0). These magnitudes\narenormalised across thetrack andused aseach ofared,\ngreen andblue component ofthe\u0002nal colour:\nThe hueofthecolour should therefore beanindica-\ntionofthebrightness ofthesound. Amore redhuewill\ndenote more powerinthelowfrequenc yportion. Amore\ngreen huedenotes more mid-range content andabluer hue\nwould denote high-range. The lightness denotes overall\npowerasinthestandard spectral magnitude measurement.\nFinally thesaturation ofthecolour would denote thebal-\nance ofpowerinthespectrum. Aspectrum thatcontains\nmuch ofitspowerinaparticular place should giveriseto\naverysaturated colour ,since itislikelythepowerwillall\nbeengulfed intooneofthethree subspectra.\n3.5 Band wise Rhythm Magnitude\nAsbefore, thisisaextension tothestandard rhythm mag-\nnitude technique done toprovide colour .The output of\nthecritical banding issplit intothree subspectra, arhythm\nmagnitude foreach oneisfound. Each arenormalised in-\ndividually andused astheir corresponding red/green/blue\ncomponent inthe\u0002nal colour .\nThebrightness ofthecolour relates tothesimplicity of\ntherhythm atthatpoint, whereas thehuedescribes where\ninthespectrum thatsimplicity lies; iftherhythmic sim-\nplicity ismost affected byvoices intheupper partofthe\nfrequenc yspectrum, thehuewill bemore purple, inthe\nlowerpartandthehuewillbemore orange y.\nThework\u0003owdiagram forthebandwise rhythm mag-\nnitude technique follows:\n60Figure 2:The track GreenOnions byBook erT.andthe\nMG'sdisplayed with themetrics (a)spectral magnitude\nand(b)bandwise spectral magnitude.\n4IMPLEMENT ATION\nAllsignal processing code wasimplemented under the\nopen source Exscalibar frame workforaudio signal infor -\nmation retrie valusing theGeddei library .Assuch ittook\nonly onehour forthesignal processing part tobepro-\ngrammed tocompletion. Due tothetransparent andef\u0002-\ncient concurrenc ythattheGeddei provides, allcode de-\nveloped wasef\u0002cient andconcurrent ready totakemaxi-\nmum advantage ofdual-core processors, hyper-threading,\nSMP andother forms ofhardw areparallelism. More in-\nformation canbefound attheproject website (Exscalibar ,\n2005).\nThe open source media player amaroK wasused as\nthemainstream media player onwhich toaddthefunc-\ntionality .Itisanadvanced music player foruseunder the\nUnix desktop environment KDE. Despite being relati vely\nyoung, itiscurrently used bymanyintheLinux commu-\nnity,having hadaround 50,000 downloads intotal from its\nsite. Itwaschosen duetoitsusable interf ace, itsempha-\nsisonnewtechnology andthequality ofthesource code.\nThecode changes necessary tomakeitworkalongside the\nprocessing softw areanddisplay themood bartook around\ntwohours intotal.\nInmanyinstances thedata generated from theIRtech-\nniques would not\u0002tcompletely intothelimited space for\ntheslider barinamaroK. Inthese cases allpoints that\nwould beinanygivenpixel'sspace were simply averaged;\nthiswasthen used forthatpixel'scolour .\n5EVALU ATION\n5.1 Discussion\n5.1.1 Bandwise Spectr alMagnitude\nInmanyinstances theaddition ofcolour tothespectral\nmagnitude display appears tomuch better describe the\nmusic andallowmore andeasier discerning ofparticu-\nlarfeatures inthetrack. Thewell knownjazztrack Green\nOnions isshownin\u0002gure 2.The bandwise variant (b)\nclearly identi\u0002es where thefunk guitar canbeheard in\nthree parts (30s, 1:10 to1.50 and2:35 onwards) where\ntheoriginal version (a)does not. This isrepresented by\nFigure 3:The track KeepHope Alive byThe Crystal\nMethod displayed with themetrics (a)spectral magnitude\nand(b)bandwise spectral magnitude.\nFigure 4:Thetrack Fantasia onGreenslee vesbySolisti di\nZagrebdisplayed with themetrics (a)spectral magnitude\nandnoveltywith akernel sizeof(b)92ms, (c)185ms, (d)\n371ms, (e)743ms, (f)1.49s, (g)2.97s and(h)5.94s.\nthethinhighlights (ofaturquoise hue), especially evident\nbetween 1:30 and1:50, where theguitar steps upakey.\nThe track KeepHope Alive ,shownin\u0002gure 3gives\nanevengreater demonstration ofthedifference between\nthebandwise magnitude andthebasic version. Forthose\nunfamiliar with this particular track, itisacomple x\npiece ofprogressi vetechno music, which forthemost\npartswitches between twomoods (atapproximately 0:48,\n1:21, 2:08, 2:44, 3:35, 4:23 and5:41). Thetrack isgener -\nallyloud, regardless ofthemood itisin(thequiet bits\narefound when itchanges between them).\nWithout theuseofcolour ,asintheoriginal variant, the\nonly parts ofthetrack thatareidentifyable arethebridges\nwhere theloudness diesdownforsome time; these appear\nasdarkerspots, such asat2:00, 3:30 and5:40. Themoods\nthemselv esarevirtually indistinguishable. Thebandwise\nversion (b)isable todistinguish between both portions\nallowing theuser tobetter separate theparts; eveninthis\ndesaturated image, thedifference ispronounced.\n5.1.2 Novelty\nThenoveltyalgorithm, duetothefactitdetermines avalue\nforeach particular moment byusing some number ofsec-\nonds' data around it,means thatitwillhavelesstemporal\nprecision. Theamount ofprecision canbecontrolled by\nthekernel size. Figure 4showsarendition ofthewell\nknowntheme Greenslee veswith varying kernel sizes. As\nisdiscussed later,theprocessing required forthevery\nlargekernel sizes (i.e.over128) results considerably more\nprocessing time. Forinstance (g)and(h)takerespecti vely\n61Figure 5:The track TimeistheEnemy byQuantic dis-\nplayed with themetrics (a)spectral magnitude andrhythm\nmagnitude interlaced with bandwise rhythm magnitude\nwith arhythm spectrum formed from amatrix ofsize(b/c)\n371ms, (d/e) 743ms, (f/g) 1.49s.\n33% and100% longer tocompute than (f).\nThenoveltyoutput isvisibly different tothespectral\nmagnitude, since itisonelevelofindirection away;rather\nthan showing thetrack directly andallowing theuser to\ndetermine when themetric changes enough todenote a\nfeature, itinstead showsthechanges directly ,essentially\nproviding adifferential view.\nTheperformance ofthemetric ishoweverquite inter-\nesting; thestart ofthetrack, which takesplace between\n20to30seconds into thetrack isdetected atdrastically\ndifferent times bydifferent kernel widths. (g)and(h)un-\ndershoot andhavetheir main strok esataround 23seconds\nin.(f)islessclear andhasseveralstrok esaround thetime\n(accurate, ifnotprecise). The second start tothemain\ntheme at3:05 to3:10 iswell depicted by(e)to(h). The\nchange intheme at1:16 and3:07 isalso clear in(e)to\n(h).(f)to(h)allpickeduponthekeychange at4:00. We\n\u0002nally picked(f)asagood compromise between visual\nclarity andprocessing needed.\n5.1.3 Rhythm Magnitude\nLikethenoveltyalgorithm, wecanalready deduce thatthe\nrhythm magnitude metric will haveless time precision,\nsince each value itproduces isbased upon anumber of\nspectra from either side ofthemoment inquestion. The\nnumber ofspectra used, andthus theimprecision ofthe\nmetric isequivalent tothenumber ofbands oftherhythm\nspectrum (orthecardinality ofthevector wemeasure).\nDetermining theoptimum size ofthespectrum israther\nablack art;asmaller sizeresults inbetter time precision\nandlessprocessing. Alargersize should allowhigher -\nlevelfeatures tobecaptured.\nFigure 5depicts thetrack Time istheEnemy ,a\ngrandiose, ifslightly repetiti vepiece ofelectronic music.\nInthe\u0002gure, thetoprow(a)isthebasic spectral mag-\nnitude ofthetrack, andwecaneasily pick outthetwo\nsections ofthetrack with thegapat1:56 to2:01. Aside\nfrom thestart and\u0002nish, little elseisvisible.\nLooking attherhythm magnitude depictions (b),(d)\nand(f),wecanseethatthestart ofthetrack isn'tnearly\nasuninteresting as(a)makesout. Clearly visible arethe\nFigure 6:The track They'reHanging MeTonight by\nRedSnapper displayed with themetrics (a)spectral mag-\nnitude, (b)bandwise spectral magnitude, andbandwise\nrhythm magnitude with rhythm spectrum formed from a\nmatrix ofsize(c)92ms, (d)185ms, (e)371ms, (f)743ms,\n(g)1.49s and(h)2.97s.\nfour echoe yrepetitions ofatheme. Arguably wecould\nalso makeoutthefrequenc yandstrength ofthevertical\nbars changes ineach ofthetwohalves.This canbeseen\natapproximately 1:07 andthen intheother halfatabout\n2:45; these relate tothechange intheme each half goes\nthrough.\nComparing (b)to(f),between which thesize ofthe\nrhythm spectrum increased byafactor of4,wecansee\nroughly thesame features arevisible, though in(f)they\nappear tobebetter de\u0002ned, with less overall noise; it\nwould appear thatthetime precision (with thistrack, at\nleast) isnegligible.\n5.1.4 Bandwise Rhythm Magnitude\nIn\u0002gure 5wecanconsider thedifferences between the\ninitial method andthebandwise variant. Allthebandwise\nvariants arebetter able todepict thebridge at1:55 to2:00\nandclearly distinguish between theinitial 20seconds and\nelsewhere inthetrack with theextreme change ofhueor-\nangeygreen tocyan, redandblue. However(e)with a\nmatrix sizeof743ms appears onthewhole thebest, push-\ningcyanstrok esintothe\u0002rstportion ofeach halfandthen\nmagenta strok esintothelatter portion.\nFigure 6depicts the track They'reHanging Me\nTonight ,ashort electronic-acoustic symphonic (orper-\nhaps cacophonic) piece. There arenumerous instruments\ninthetrack, andsampling isused considerably tomake\nthetrack quite comple xlistening. Thebasic spectral mag-\nnitude (a)doesn' treally showalotofinformation other\nthan therough start and\u0002nish times (around 1:15, with\nbreaks at2:40 and4:10, \u0002nishing at5:40).\nThebandwise spectral magnitude (b)helps distinguish\nalittle more; weseealargecyanbarat4:05 where agui-\ntartakesoverfor10seconds before themello werbridge\nisreached. Howeverduetothefactthetrack isloud at\nmost parts anyway,despite themusic differences, wesee\nalargely monotonic picture.\nWhen weutilise thebandwise rhythm magnitude, we\n62Figure 7:The total CPU time spent analysing theaudio\ndata inorder toprovide therelevantvisualisation. Tim-\ningconducted onIntel Pentium-M 1.7GHz system with 512MB\nRAM on171s ofaudio.\nseeadifferent picture. Inthisinstance twoportions ofthe\ntrack thatused tobequite indistinguishable (from 1:15 to\n2:35 andthen from 2:45 toaround 4:00) arenowread-\nilyidentifyable asgreen anddark redrespecti vely.Even\nmore, theportions canbere-identi\u0002ed later oninthetrack\nat4:45 to5:00 (green) and5:00 to5:35 (dark red). The\ninitial 45seconds ofthetrack, which appears asacon-\nstant verydark greyintheboth (a)and(b)hasabright\npink/yello w/cyanhuewhich changes todull green, and\nthen againtodark purple. The \u0002rst three huechanges\ndon'tappear tocorrespond toanything useful inthemu-\nsic,howeverthelatter twochanges correspond precisely\ntotheaddition oneandthen another repeating samples.\nVarying thematrix sizebetween 100ms to1.5s gives\nsome what different output. The visual features become\nclearer andlessnoisy ,though problematic red herrings\n(likethemeaningless huechanges early oninthetrack)\nalso become better de\u0002ned. Theprocessing time also in-\ncreases signi\u0002cantly asisshownlater.Agood compro-\nmise appears tobewith amatrix sizeof64or128samples,\nwhich correspond to743ms and1.49s respecti vely.\n5.2 Computational Performance\nFrom \u0002gure 7wecanseethatallmethods thatrelyupon\naself-similarity kernel areO(n)where nisthesize of\nthekernel. Thebandwise rhythm magnitude takesaround\ntwice aslong tocompute foragivenkernel sizethan ei-\ntheroftheother twonon-bandwise matrix-based methods.\nThenon-matrix based measures were shownasabaseline\nonly.\nWecanseethatwith modern hardw are,andchoosing\nareasonable sizeofmatrix, computing theannotation for\nagivenaudio track would bequite trivial. This could be\ndone either on-demand, having theannotation computed\nonplay anddisplayed afewseconds intothetrack orper-\nhaps precomputed (aswedidinthisstudy) sothatthean-\nnotation isstored ready forimmediate use.5.3 User Study\nFortheuser study ,weattempted tobest simulate prac-\ntical andreal-w orld conditions, rather than previous ex-\nperiments where wefocused mainly onmathematical\ntractability .Weinitially selected \u0002vetracks from area-\nsonably broad range ofmusic. The tracks were selected\ntogiveagood range ofdifferent types ofmusic andof\ndifferent dif\u0002culties ofproblem. Tracks were chosen for\ntheir interesting features thatwould best testthesystems.\nMood changes, both subtle andblunt, instrument changes,\nvocalchanges andrhythm diversity areamong thefeatures\nweattempted toutilise tobestexamine thesystems. Table\n1showsthetracks thatwere chosen.\nTrack Genr e Times\n(1)D.McMurray WalkintheNight Jazz 14\nReasonable, consistant beat structure andloudness. Min-\nimally de\u0002ned transitions.\n(2)Muse Plug InBaby Rock 9\nSimple rock ballad with clear verse/chorus structure.\n(3)Shivaree Goodnight Moon Pop 8\nFluid pop song with little beat structure and hazy\nverse/chorus structure.\n(4)Plaid PragueRadio Abstract 6\nStructurally comple x,highly dynamic with multiple\nmoods andwell de\u0002ned beats.\n(5)Crystal Method KeepHope Alive Dance 13\nStructurally simple, well de\u0002ned beats, consistently loud,\nfewmoods.\nTable 1:The \u0002vetracks chosen fortheuser study ,their\nrespecti vegenres andthenumber ofsigni\u0002cant changes.\n5.3.1 Method\nWeformed abase truth about ourdata byallowing amu-\nsiclovertodictate where ineach track themain musical\nchanges took place. Around 7such points were allocated\ntoeach track. Topreventtheeffects ofsuggestion theindi-\nvidual who wasgiventhistaskwasnotpreviously subject\ntoanyannotation ofthetracks inquestion. When listening\ntothetracks, novisualisation atallwasprovided.\nWithourbase truth established, weconducted the\nstudy proper .Weconducted thestudy with 18subjects,\neach subject wasgiven\u0002vetrialsa trial foreach ofthe\n\u0002vetracks. Werotated through each ofthe\u0002veanalysis\ntechniques andacontrol with noannotation.\nEach subject wasgivenaninitial period oftraining\n(some required more than others), until theyfeltfamil-\niarwith thecontrols oftheplayer .Aside from getting to\ngrips with thelook andfeel oftheapplication, theywere\ngivennospeci\u0002c information onthemood baralgorithms.\nForeach trial, thesubject wasgiven60seconds with the\nplayer incorporating thegivenanalysis technique with the\ntrack loaded. Theywere allowed toskip back andforth\nthrough thetrack atwill, andcould utilise themood bar\nastheysaw\u0002t.Their task wasto\u0002nd each ofthetimes\nwhere themusic changed most. When theminute was\nup,themusic wasstopped andtheyhadto\u0002nish writing.\n635.3.2 User Commentry\nTheoverall feeling ofthose intervie wed wasthattheypre-\nferred thespectral magnitude visualisations overanyof\ntheothers. Having utilised all\u0002veofthemethods, many\nalso indicated thattheyintuiti velyrelated theintensity of\napoint with theloudness atthatpoint. Some went onto\nsuggest thattheywould then intuiti velyrelate thecolour\nofapoint with anyinstruments playing atthatpoint. Only\nonecandidate suggested thatbrighter parts inthevisuali-\nsation might mean increased dynamics andotherwise less\nconstanc y.\nThe standard rhythm magnitude measure aswell as\nthenovelty measure were generally dislik ed. Speci\u0002c\ncomments were daunting andless predictable. The\ngeneral feeling wasthat differential measurement (ala\nnovelty) wasunsuitable forintuiti velearning; people\nexpected toseechunks ofsimilar sounding portions\noftime, rather than speci\u0002c points atwhich themusic\nchanged. Those who commented feltthattherhythm mag-\nnitude measure simply lookedoverly populated andex-\ncessi velycontrasting, andthus determined ittoodaunt-\ning forgeneral use.\nCosmetically almost allpeople preferred colour over\nmonochromatic visualisations (one evenwent sofaras\ntosayitwaspretty). The majority ofthose suggested\nthattheyfound colour tobethebetter visualisation inre-\nspect tousability also. Theyfound iteasier todistin-\nguish, andmore informati ve.The opinion ofcolour\ninthebandwise rhythm magnitude wassome what more\ndivided. While nobody made itouttobeworse thatthe\nmono-chrome variant, most favoured thelook oftheband-\nwise spectral magnitude, \u0002nding therhythm magnitude\nlesswell de\u0002ned.\nAsforusability and comfort, theparticipants were\nquite polarised ontheir opinions astowhether adding\ncolour wasmore helpful intheexperiment. While some\ndecided thatitgavethem more information andthus was\nmore useful, others felttheaddition ofcolour increased\nthelearning curvetoomuch. Most went ontosuggest that\nperhaps, givenenough time tolearn, thecolour might be\nbetter eventually anyway.\nOne participant suggested thethree colour compo-\nnents used tocreate thecolour form thelow,midandhigh\nportions ofthespectrum beswitched. Apparently they\nexpected redder hues torelate towarmer (i.e. bassier)\nsounds while bluer hues related toharsher ,sharper (pre-\nsumably higher) sounds.\n5.3.3 Quantitative Results\nThe score foreach candidate wascomputed simply by\nsumming thenumber oftimes theygavewhich fellwithin\n3seconds ofatime from ourground-truth. Inorder to\npreventtheresults forthose thatwere better overall from\nbiasing thegeneral trend, everycandidates score waszero-\nmean normalised. Themeans andstandard deviations of\nthescores foreach analysis method takenoverallthetrials\nconducted arepresented in\u0002gure 8.\nFrom the\u0002gure wecanseethatthebest twomethods\nwere clearly thetwobased upon thespectral magnitude.\nTheir means arefartooclose todistinguish which was\nactually better though thebandwise variant hadahigherBandwise Rhythm Mag.\nControl (Blind)Bandwise Spectral Mag.Rhythm Mag.\nNovelty\nSpectral Mag.\n-1.5 -1 -0.5  0  0.5  1  1.5\nNormalised Score\nFigure 8:Mean and\u001bofthescore ofeach analysis\nmethod.\nTrack\nMethod 1 2 3 4 5\nControlquqts\nSpectr alMag.utuyt\nNoveltytstsp\nRhythm Mag.trpqv\nBandwise Spectr alMag.qttyx\nBandwise Rhythm Mag.rsstu\nFigure 9:Method results onatrack bytrack basis. Larger\nishigher score.\nvariance suggesting thatitsusewassome what changeable\ndepending upon theperson using itand/or thetype oftrack\nbeing used upon.\nThebandwise rhythm magnitude faroutperforms the\nrhythm magnitude method, increasing inboth mean score\nandconstanc y.Thenoveltymethod wasbarely different to\nthecontrol (blind) trials, andthebasic rhythm magnitude\nslightly worse. This suggests thatthefalsepositi vespro-\nvided bythese annotations washighly detrimental totheir\noverall usefullness, andinthecase ofrhythm magnitude\npossibly caused more harm than good.\nThough themean ofthebandwise rhythm magnitude\nissome what higher than thatofthecontrol, thevariances\naresohigh that theresults lose statistical signi\u0002cance.\nFurther tests would need tobeconducted todetermine\nwhat degree ofusefullness, ifany,themetric provided\noverthecontrol.\nFrom \u0002gure 9wecanseematrix showing howeach\nmethod performed oneach track, allowing ustocom-\npare between speci\u0002c tracks andmethods. Wecandeter -\nmine thatthebandwise spectral magnitude method ismost\nproblematic with track number 1,theJazz piece. Perhaps\nthemost surprising sign istheperformance oftrack 2,the\nrock music; theperformance issimilar across methods,\nthough thecontrol actually comes slightly ahead ofallthe\nothers. The tworhythm magnitude methods doreason-\nably well attrack 5,theabstract piece, though their per-\nformance isbelowparelsewhere.\nInterestingly thebandwise variation ofthespectral\n64magnitude causes the(quite uniform) jazz piece (1)tobe\nevenlessdistinguishable andthecleaner ,better de\u0002ned\ndance piece (5)tobemore distinguishable. This isnot\nsurprising; while theaddition ofcolour toatrack thatis\nmusically dynamic andheavywillhelp further de\u0002ne seg-\nmentations, itmay easily hinder atrack whose segmen-\ntations arequestionable andill-de\u0002ned byproviding the\nuser with evenmore false-positi vecues.\n6CONCLUSION\nWedemonstrated thatautomatic, content-based visual an-\nnotation, ingeneral, makesapositi veaddition tomod-\nernmusic playing tools. Wepresented \u0002ndings, through\nauser study ,thatpeople \u0002ndsuch visual annotations both\nusable andaesthetically pleasing. Wealsofound thatpeo-\nplewere able toimmediately utilise thevisualisation with\nanabsolute minimum amount oftraining.\nWedemonstrated severalannotations, andfound each\nhadaparticular niche under which itworkedreasonably\nwell. Wefound thespectral magnitude andbandwise vari-\nanttobetheoverall winners, fortheir consistant per-\nformance. The bandwise variant wasdeclared favourite\nforitsvisual charm aswell asgeneral performance. We\nshowed thattheaddition ofthenovelbandwise technique\nforintroducing colour totwoofthemethods helps under\nmanycircumstances with quantitati veevidence, andthat\nusers typically prefer tousethecolour annotation.\nThis study didn' ttakeintoaccount various accessibil-\nityproblems, notleast colour blindness. Howeveritwould\nseem unlik elythatcolour blindness initself would have\nsuch adetrimental impact astorender thecolour variants\nworse than theoriginal monochromatic versions.\n7FUR THER WORK\nInthisstudy only aquick andsimple testwascarried out\ngiving theuser aminimum oftime tolearn. Itisunclear\nwhether agreater learning time would change therelati ve\nperformances ofthemetrics.\nWithproper standardisation such atechnique could\nbeused tojist music tracks atbrowsing inmusic\nstores. Such a\u0002ngerprint may describe music ade-\nquately enough toallowapotential purchaser todetermine\ntheir interest inarecord. Initially ,electronic music play-\nerssuch asamaroK could havesuch a\u0002ngerprint added\ntotheir playlist foreach track, giving theuser avisual cue\nandproviding anautomatic iconi\u0002cation amusic track. In\ntheory the\u0002ngerprint information could beprecomputed\nandembedded intothedigital \u0002leitself. Enough technolo-\ngiesexisttoencapsulate such metadata inside atrack such\nastheOgg Vorbis comment system ortheMP3 id3v2 tag\nsystem.\nThese techniques, orothers likethem could easily be\ncombined with machine learning algorithms toconvert\nfrom thecontinuous form presented here toadiscretely\nannotated form with speci\u0002c semantics, inanautomatic\nversion oftheworkby(Couprie, 2004).\nInsofarasthebandwise mechanism wasimple-\nmented, thisstudy conducted only asimple 3-wayfair\ndivision ofthecritical bands. Initial experimentation sug-gests thatunevendivision could signi\u0002cantly impro vethe\n\u0002delity oftheresultant visualisation. Furthermore, such a\ngraphic hasthepotential toshowmore information; vary-\ningthewidth ofthestripes would allowfurther metrics to\nbeencoded.\nREFERENCES\namaroK. amarok.kde.or g/,2005. URLhttp://\namarok.kde.org .\nS.Blackb urnandD.DeRoure. Atool forcontent based\nnavigation ofmusic. InProc.ACMMultimedia .ACM,\nBristol, UK, 1998. ISBN 1-58113-036-8.\nH.D.Bock er,G.Fischer ,andH.Nieper .The enhance-\nment ofunderstanding through visual representations.\nInCHI '86: Proc.SIGCHI confer ence onHuman fac-\ntorsincomputing systems ,pages 4450, NewYork,\nNY,USA, 1986. ACMPress. ISBN 0-89791-180-6.\nE.Brazil, M.Fernstrom, G.Tzanetakis, andP.R.Cook.\nEnhancing sonic browsing using audio information re-\ntrieval.InProc.International Confer ence onAuditory\nDisplay .ICAD, 2002.\nM.Cooper andJ.Foote. Summarizing popular music via\nstructural similarity analysis. InProc.ACMMultime-\ndia,pages 364373. ACM, Berk eley,CA, 2003.\nP.Couprie. Graphical representation: Ananalytical and\npublication toolforelectroaccoustic music. InOrgan-\nisedSound ,volume 9,pages 109113. Cambridge Uni-\nversity Press, 2004.\nExscalibar .exscalibar .sf.net/, 2005. URLhttp://\nexscalibar.sf.net .\nJ.Foote. Methods fortheautomatic analysis ofmusic and\naudio. Technical report, 1999. URLciteseer.nj.\nnec.com/foote99methods.html .\nJ.Foote and M.Cooper .Media segmentation using\nself-similarity decomposition. InProc.SPIE Storage\nandRetrie valforMultimedia Databases ,volume 5021,\npages 167175. SPIE, SanJose, California., 12003.\nC.Raphael. Automatic segmentation ofacoustic musi-\ncalsignals using hidden mark ovmodels. InTrans-\nactions onPattern Analysis andMachine Intellig ence,\nvolume 31.IEEE, 41999.\nG.Tzanetakis. Musescape: Aninteracti vecontent-a ware\nmusic browser.InProc.Int.Confer ence onDigital Au-\ndioEffects.DAFx-03, London, UK, 92003.\nG.Tzanetakis andP.R.Cook. Audio information retrie val\ntools. InProc.International Symposium onMusic In-\nformation Retrie val.ISMIR, 2000a.\nG.Tzanetakis andP.R.Cook. Marsyas: Aframe workfor\naudio analysis. Organized Sound ,2000b.\nG.WoodandS.E.O'Keefe. Quantitati vecomparisons\ninto content-based music recognition with theself-\norganising map. InProc.Int.Symposium onMusic\nInformation Retrie val.ISMIR 03,Baltimore, US, 10\n2003.\n65"
    },
    {
        "title": "A Partial Searching Algorithm and Its Application for Polyphonic Music Transcription.",
        "author": [
            "Wen Xue",
            "Mark Sandler 0001"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415740",
        "url": "https://doi.org/10.5281/zenodo.1415740",
        "ee": "https://zenodo.org/records/1415740/files/XueS05.pdf",
        "abstract": "This paper proposes an algorithm for studying spectral contents of pitched sounds in real-world recordings. We assume that the 2nd-order difference, w.r.t. partial index, of a pitched sound is bounded by some small positive value, rather than equal to 0 in a perfect harmonic case. Given a spectrum and a fundamental frequency f0, the algorithm searches the spectrum for partials that can be associated with f0 by dynamic programming. In section 3 a background-foreground model is plugged into the algorithm to make it work with reverberant background, such as in a piano recording. In section 4 we illustrate an application of the algorithm in which a multipitch scoring machine, which involves special processing for close or shared partials, is coupled with a tree searching method for polyphonic transcription task. Results are evaluated on the traditional note level, as well as on a partial-based sub-note level.",
        "zenodo_id": 1415740,
        "dblp_key": "conf/ismir/XueS05",
        "keywords": [
            "pitched sounds",
            "spectrum contents",
            "dynamic programming",
            "fundamental frequency",
            "reverberant background",
            "multipitch scoring machine",
            "polyphonic transcription",
            "tree searching method",
            "close or shared partials",
            "traditional note level"
        ],
        "content": "A PARTIAL SEARCHING ALGORITHM AND ITS APPLICATION  \nFOR POLYPHONIC MUSIC TRANSCRIPTION\n Xue Wen,  Mark Sandler  \n Centre for Digital Music, \nDepartment of Electronic Engineering, \nQueen Mary, University of London, \nMile End Road, London, E1 4NS \n{xue.wen,mark.sandler}@elec.qmul.ac.uk   \nABSTRACT \nThis paper proposes an algorithm for studying spectral \ncontents of pitched sounds in real-world recordings. We assume that the 2\nnd-order difference, w.r.t. partial index, \nof a pitched sound is bounded by some small positive value, rather than equal to 0 in a perfect harmonic case. \nGiven a spectrum and a fundamental frequency f0, the algorithm searches the spectrum for partials that can be \nassociated with f0 by dynamic programming. In section 3 a background-foreground model is plugged into the \nalgorithm to make it work with reverberant background, \nsuch as in a piano recording. In section 4 we illustrate an application of the algorithm in which a multipitch scoring machine, which involves special processing for close or shared partials, is coupled with a tree searching method for polyphonic transcription task. Results are evaluated on the traditional note level, as well as on a partial-based sub-note level.  \nKeywords: sinusoids, spectral harmonic model, \ndynamic programming, polyphonic music transcription.  \n1 INTRODUCTION \nReal-world tonal sounds from acoustical instruments \ndepart more or less from the perfect harmonic model, in which partial frequencies are multiples of a fundamental frequency (f0) [1]. Partial frequencies are crucial for estimating other spectral parameters [2] that can be basic to higher-level MIR tasks. Often it is desirable to find out individual partial frequencies rather than assuming \nperfect harmonicity. In this paper we propose a method \nthat finds partials from the signal . Given the power \nspectrum and an estimate of f0 , it searches for partials \nthat can be associated with f0 under assumptions weaker than the perfect harmonic model. It neither requires the input sound to be noise free, nor asks partials from different events to be well separated. It’s also robust with missing or weak partials. However, interfering partials are given a summary  amplitude only. Section 4 \ngives an example on how we can make use of this amplitude. 2 THE PARTIAL SEARCHING \nALGORITHM \nPartials of most tonal sounds can be viewed as nearly  \nharmonic. Departure of true partial frequencies from \nmultiples of f0 is known as inharmonicity. We denote \nthe frequency of the pth partial as kp(f0), a function of \nfundamental f0 and partial index p. In particular, the \nfundamental frequency is k1(f0)1. Since inharmonicity is \nsomething highly dependent on individual instruments, a parametric a priori  modeling similar to [3] will be hard \nwhen we don’t have enough knowledge on the instrument involved. Here we take a posterior  approach, \nin which partial frequencies are estimated from the signal, while only weak assumptions are imposed on \nhow the partials distribute. \nRather than consideri ng the difference between k\np(f0) \nand pk1(f0), as does [4], we focus on the intervals \nbetween consecutive partials. Define the 1st- and 2nd-\norder differences of partial frequencies with respect to p  \n  1   ),f0( )f0(1                    ),(f0\n)f0(\n11\n⎩⎨⎧\n> −=\n=∆\n− p k kp k\nk\np pp , (1) \nand \n  1   ),f0( )f0(1                               ,0\n)f0(\n12\n⎩⎨⎧\n> ∆−∆=\n=∆ − p k kp\nk\np pp .   (2) \nIn perfect harmonic case, we have ∆kp(f0)= k1(f0) and \ntherefore ∆2kp(f0)=0 for all p≥1. With real-world signals, \nwe assume that \np p k pu p l ∀⋅<∆<⋅   ,f0)( )f0( f0)(2δ δ  (3)  \nwhere )(plδ <0 and )(puδ >0 take small absolute values. \nWe also assume that inharmonicity grows larger for \nhigher partials, so | δl(p)| and |δu(p)| are allowed to \nincrease with p. What assumption (3) does is hard-\nlimiting the error of a 1st-order linear prediction of \npartial frequencies. It’s trivial to extend (3) to a higher order by taking into account differences of ∆\n2kp(f0). We \nuse 1st-order only. \nPartial searching starts from the discrete sound \nspectrum x=(xk)k=1,2,… , typically obtained by DFT, and a \ngiven fundamental f0. The sound may have multiple pitches, either from the same instrument or not. Constraints (1)~(3) help to prevent well separated partials from disturbing each ot her. The output of partial \nsearching are partial frequencies k\np(f0), p=1, 2, …, as \n                                                           \n1 f0 is not quantitatively defined in this article, while k1(f0) is its \nquantitative model.   Permission to make digital or hard copies of all or part of this \nwork for personal or classroom  use is granted without fee \nprovided that copies are not made or distributed for profit or \ncom-mercial advantage and that copies bear this notice and the full citation on the first page. \n© 2005 Queen Mary, University of London \n690   \n \n well as amplitudes ap(f0), p=1, 2, …. In the case one \nsingle partial being shared by multiple pitches, ap(f0) is a \nsummary  amplitude of that partial. For simplicity we \nomit “(f0)” from these notations in what follows. \nThe searching process is one of optimization. We \ndenote any candidate partial frequency sequence ψ as \nk[ψ]=(kp[ψ])p=1,2,… , subject to constraints (1)~(3). Given \na k[ψ], a reference spectrum h[ψ]=(hk[ψ])k=1,2,…  is \nconstructed as \n...3,2,1   ,])ψ[ (          ])ψ[( ]ψ[\n= − ==\n∑∑\nk kkwc Ckwc C h\npp ppp kp k\n (4) \nwhere w(k) is the amplitude spectrum of the window \nfunction used for the DFT, and ∑=\nppc C21  a \nnormalizing factor. In practice ( cp)p=1,2,…  is selected so \nthat cp≥0 and∑∞\n=1ppc<∞. The number of partials being \nconsidered can be roughly decided by f0 and the \nsampling rate. Notice that while ])ψ[(p kkw  is discrete, \nw(k) takes a continuous domain. h[ψ] approximates the \namplitude spectrum of a signal whose partials fall at k[ψ] \nand amplitudes are given as ( cp/C)p=1,2,… . We define our \nobjective function as the inner product of the data \nspectrum x and the reference spectrum h[ψ]: \n∑∑∑\n=> <=><\nkp kk\nppp\npp\nkwx c Ckwx c C hx\n])ψ[(])ψ[(, ]ψ[,\n, (5) \nThe searching algorithm tries to find an optimal ψ that \nmaximizes this inner product: \n>< = ]ψ[, maxargψ\nψhx)  (6) \nThe term < x, w(kp[ψ])> in (5) sets up a frequency-\ndomain matched filter to detect the pth partial locally. We \nput special focus on local spect ral peaks, i.e. maxima of \n<x, w(kp[ψ])>. This is done by introducing another \nconstraint on kp: \nset\" discrete predefined a from selected isor  ,)(  maxizes locally either  \" > <p k p k x,w k       (7) \nThe “either” condition requires a partial to fall on a \nspectral peak. However, when no peak is located in the \nsearching interval, typically with missing or masked partials, we artificially add some candidates so that the search can continue. A plausi ble suggestion is to place a \nk\np candidate at 2 kp-1-kp-2, which is its 1st-order predicted \nposition. Adding more candidates may slightly improve \nthe result, at the cost of more computation. \nConstraint (7) confines ca ndidates for each partial to \na discrete set. In this case the optimization can be \nformulated as a route finding problem: here route  refers \nto a sequence of (index, frequency) pairs {( p, kp)}p=1,2,… , \nand each pair contributes a mileage  of > < )(,p p kwx c , \ndetermined solely by kp. A route starts from p=1, and terminates where p is big enough so that the tail sum \n> <∑\n>)(,'\n'' p\nppp kwx c  is ignorable. The frequency of the \npth partial kp must be selected from a set determined by \nkp-1 and kp-2 through the constraints. Finding the optimal \nψˆ is equivalent to finding the longest route with these \nsettings. \nFigure 1 gives an illustration on how the constraints \nwork. Each circle in the graph stands for a spectral \npeak, with kpn denoting the nth peak found for the pth \npartial. Circles are connected by arrows to form a route. \nA light-coloured line pair from a circle gives the \nfeasible range to find the next partial when the last \npartial comes by the arrow in the same line style. E.g. the solid line pair from k\n21 encloses the range to search \nfor a next partial after k12→k21, and the dotted line pair \nfrom k21 encloses the range to search for the next partial \nafter k11→k21. Of the four peaks found for partial 3, k32 \nfalls in both ranges; and k33 falls in the successor range \nof k12→k22 only. This means route k12→k22 may lead to \nboth k32 and k33, while route k11→k21 may only lead to \nk32. As for route k12→k21, although it also reaches k21, it \nfinds no matched peak for partial 3. However, it is \nextended to a temporary lodge  at k3A, from which it may \ncontinue with further peaks in higher partials, such as k\n41. Route k11→k21 joining k12→k22 at k32 does not imply \nthat they have become one immediately. However, if a \nspectral peak at the next partial, say k42, falls in the \nintersection of feasible successor ranges of k21→k32 and \nk22→k32, then routes k21→k32→k42 and k22→k32→k42 are \nbound as one in future searching for higher partials. \nWe solve the constrained optimization by dynamic \nprogramming  (DP). However, we can not apply DP \ndirectly on ( p, kp), as the candidate set for ( p+1)th partial \ndepends on both kp and kp-1, while the standard DP \nrecursion allows only 1-step dependency to derive the (p+1)-step optima from p-step optima. To fix this \nproblem we tie ( p-1, k\np-1) and ( p, kp) together to form an \nextended partial  (p, kp, ∆kp). We define the optimal \npartial route length as \n))(, ( max ) ,(\n''\n1> < =∆ ∑\n<=∆−=−p\np pp p p p kwx c k kS\npk pk pk.      (8) k12 \nk11 k21 k22 \nk31 k32 k33 k34 \nk42 \nk41 \nk3A \nFigure 1 Partial searching routes  \n691   \n \n This is interpreted as the maximal length of all routes \nthat terminate at ( p, kp, ∆kp). From another point of view, \nan extended partial is a connection  between consecutive \npartials, corresponding to an arrow in Figure 1. Since the \ninward connection at a peak is enough to determine the \noutward connection candidate s, 1-step dependency is \nsatisfied and DP is directly applicable with connections. The complete algorithm is given as follows: \nAlgorithm 1 : harmonic partial finding  \nA1.1° Set the root node p=0, k0=0, ∆k0=f0, S0(k0, ∆k0)=0. \nA1.2° For p=1, 2, …, do A1.3°~1.5° until the stop condition is \nmet, i.e. p is large enough . \nA1.3° For each node ( p-1, kp-1, ∆kp-1) in the last iteration, \n      A1.3a° calculate the f easible interval of its successor \npartial frequency as ( kp-1+∆kp-1+δl(p)f0, kp-1 +∆kp-1+ \nδu(p)f0); \n      A1.3b° do a maximum search of > < )(p kk x,w  regarding \nkp on that interval, call the found maxima kp,1, kp,2, …; \n      A1.3c° define the feasible successor set of ( p-1, kp-1, ∆kp-1) \nas {( p, kp,1, kp,1- kp-1), (p, kp,2, kp,2- kp-1), (p, kp,3, kp,3- kp-\n1),… }, or in the case no maximum is found in (3b), as \n{(p, kp-1+∆kp-1, ∆kp-1)}. \nA1.4° Collect all feasible successors generated in 3° together \nand re-label them as ( p, kp,l, ∆kp,l), l=1, 2, 3,…; these are \nnode candidates for partial p. \nA1.5° For each new feasible node ( p, kp,l, ∆kp,l), calculate the \noptimal partial route length \n> <+∆∆− =∆− −∆−)(, )  , ( max) ,(, ,*1 , , 1 , ,\n,*1lp p p lp lp pklp lp p kwx c k k k S k kS\np \n      where the maximum is ta ken over all its predecessors with \nthe same kp-1 but different ∆kp-1’s. Denote the ∆kp-1 which \nmaximizes (8) as ∆-kp(p, kp,l, ∆kp,l). \nA1.6° For the final iteration p=P, find lˆas the l that \nmaximizes SP(kP,l, ∆kP,l); set kp=lPkˆ,, ∆kp=∆lPkˆ,,  \n∆-kp= ∆-kp(p, lPkˆ,, ∆lPkˆ,). \nA1.7° For p=P-1, P-2, …, 1,  calculate kp= kp+1-∆kp, ∆kp=      \n∆-kp+1, ∆-kp= ∆-kp(p, kp , ∆kp).■ \nThe algorithm searches for k1(f0) in a vicinity of the \ngiven f0, which copes with accuracy problems of the \ngiven fundamental rather th an with inharmonicity. The \nchoice of δl(1) and δu(1) therefore differs from that for \nhigher partials. One can force k1(f0) at f0 by setting \nδl(1)= δu(1)=0, which is equivale nt to starting searching \nat p=2 from root (1, f0, f0). \nThe amplitude of the pth partial ap can be estimated as \n2)(, w kwx ap p > <≅        (9) \nwhere ∑\n∈=\nZkkw w2 2 is a positive constant. Compare (9) \nwith (8), it’s apparent that \n21 1 1 ) , ( ) ,(\nwck k S k kS\na\npp p p p p p\np− −−∆ −∆\n≅ . (10) \nEquation (10) shows how partial amplitude estimation \ncan be integrated into the partial searching algorithm. \nFor a stationary sound source with constant partial \nfrequencies, one may wish to use spectra calculated from multiple frames for better estimation. Let the spectra be \nx1, x2, …, we rewrite the objective function as \n> <=>< ∑∑ ∑ ])ψ[(, ]ψ[,p\nnnn\npp\nnn n kwxb c C hx b  (11)\n \nwhere bn, n=1, 2, …, are weights assigned to the frames. \nEquation (11) implies that we use ∑\nnnnxb instead of x \nfor partial searching.  \nInput data x appears in the algorithm only in the form \nof inner product < x, wk(kp)>. This means if < x, wk(kp)> is \navailable as input, we don’t have to know x. We’ll show \nhow we can make use of this property later. \n3 REVERBERANT BACKGROUND: A \nPIANO EXAMPLE \nThe partial searching algorithm is tested on a piano \nrecording of Bach’s Prelude in C, BWV 846a, in which the instrument is supposed to be well-tuned on a perfect \nwell-tempered scale with A4 at 440Hz. The recording is \nof high quality, yet extra sounds like pedalling and singing are heard. The piece is partially monophonic  in \nthat only one note is played at a time (except the last \nchord). However, a note may last a long period and \noverlap the coming-up ones, which creates polyphony. \nLike many other polyphonic analyzers, our system \nprefers a sparser  input with fewer concurrent sounds. In \ncommon sense, a mixture of two sounds is no sparser than any of the two. For a piano recording, we try to \nreduce polyphony by breaking the sound into a \nforeground part and a background part in a note-by-note manner. The most recent note is modeled as the foreground, and sustaining previous notes are modeled as the background. Given a note onset where a new note (i.e. the foreground) starts, we denote the spectrum before and after the onset as x\n- and x+ respectively. x- is \ninterpreted as the summary spectrum of all previous \nnotes immediately before the new note, while x+ is a \ncombination of the new note, whose spectrum we denote as y, with those notes carried over from x\n-, whose \nspectrum after the onset we denote as x~. We make three \nfurther assumptions: \n1) for any bin index k, \n0≤kx~≤x-k,   yk≥0      (12) \n2) for any bin index k, \n2 2 2~\nk k k y x x+=+   (13) \n3) y is made up of partial spectra, each in the form of \napwk(kp): \n0    ,)(> =∑ p\npp kp k a kwa y     (14) \nBy (12) we assume that the power of a sustaining note \ndoes not increase. By (13) we assume that energy is preserved in every bin. (14) is a common assumption in sinusoidal models. Apparently these are approximations only, and their solution ( a\np, kp)p=1,2,…  can be non-existing \nor non-unique. We get around the existence problem by \nallowing a spectral error r in assumption 2. We combine \n(12)~(14) and rewrite with the error term: \n692   \n \n 0 ,1 0                                , ))( (2 2 2\n><<+ +=∑− +\np kk\npp k p k k k\nar kwa x x\nλλ\n (15) \nWe minimize the residue r=(rk)k=1,2,… by its Euclidian \nnorm. If r=0, it indicates non-unique solutions. One way \nto deal with the uniqueness problem is to use an \nadditional objective function. We choose to maximize \nλ=(λk) k=1,2,…  by its Euclidian norm on the constraint r=0, \nwhich implies maximal removal  of the background. \nWith fixed ( kp)p=1,2,… , by minimizing r (and maximizing \nλ when r=0) we get a=(ap)p=1,2,… . We write \n2 2)(           )( )(,\nwa kwakwy kwy\np\nkpp k pkp k k p\n≅ =>= <\n∑∑∑\n  (16) \nwhere inter-partial spectral leakage  has been assumed \nignorable. Equation (16) e xplicitly evaluates < y, wk(kp)>, \nwhich enables partial searching on the foreground signal \ny using Algorithm 1.  \nIn our test we measure partial frequencies of every \nnote, using multiple frames starting from the onset. The \nideal f0, e.g. A4=440Hz, is  used to start partial \nsearching. The means µp(f0) and standard deviations \nσp(f0) of partial frequencies are calculated for notes that \nappear at least 9 times. We have no ground truth on the \nexact partial frequencies. However, by assuming partial \nfrequencies being constant for a given key, we can study \nhow reliable the searching is using σp(f0). In general the \nenvelope of σp(f0) increases slowly with p, until after \nsome point the increase becomes dramatic. Figure 2 \ndepicts σp(f0) against kp for C4, #F4 and A4, both axes \nare measured in DFT bins (1bin=10.77Hz). While minor σ\np(f0) may be credited to local noises, large ones \ngenerally imply searching failure.  \nWe set 1 bin as the thre shold for judging whether a \npartial is reliably measured, and define p’(f0) as the \nmaximal P that satisfies σp(f0)<1, ∀p≤P. Results are \ngiven in Table 1. Frequencies are given in bins. The first \n10 partials are successfully captured most of the time. \nMore than 99% of total energy is enclosed in the first p’(f0) partials. \nTable 1 Evaluating partial frequency measurement \nPitch p’(f0) k\np’(f0) Pitch p’(f0) kp’(f0) \nG2 22 204.01 E4 10 313.68 \nD3 24 340.16 F4 8 264.29 \nF3 16 265.72 #F4 12 430.22 \nG3 15 280.69 G4 12 457.17 \nA3 17 362.35 A4 10 425.48 \nB3 12 281.42 B4 5 232.07 \nC4 16 405.62 C5 3 147.05 \nD4 17 490.79 D5 8 454.57 \nE4 10 313.68    \nIt’s also interesting to l ook at how partial frequencies \ndepart from perfect harm onic model. We collect ∆2kp \nand the difference between kp and pk1 in Table 2. Results \nare given for the first 12 partials of keys A3, C4 and G4. \nAll frequencies are given in bins. For all three keys ∆2kp are always positive and increasing in the long trend, \nwhich supports the positive adaptation of δu(p) with p. \nkp-pk1 gives a hint on how much will be lost when using \na perfect harmonic model. With most popular window \nfunctions (Hann, Hamming, Kaiser, etc), a frequency \nerror above 1 bin usually imp lies a big error in amplitude \nestimation, and an error above 2 bins usually means that \nthe partial is lost. \nTable 2 Evaluating inharmonicity \nA3 C4 G4 p ∆2kp kp-pk1 ∆2kp kp-pk1 ∆2kp kp-pk1 \n2 0.059 0.06 0.033 0.03 0.152 0.15 \n3 0.019 0.14 0.030 0.10 0.005 0.31 \n4 0.037 0.25 0.079 0.24 0.264 0.73 \n5 0.049 0.42 0.068 0.45 0.208 1.36 \n6 0.087 0.67 0.082 0.74 0.407 2.39 \n7 0.138 1.06 0.247 1.28 0.409 3.84 \n8 0.122 1.57 0.087 1.91 0.380 5.66 \n9 0.069 2.16 0.188 2.72 0.584 8.07 \n10 0.110 2.84 0.302 3.84 0.723 11.2 \n11 0.107 3.63 0.110 5.06 0.535 14.9 \n12 0.342 4.77 0.246 6.53 0.560 19.1 \n4 APPLICATION FOR POLYPHONIC \nMUSIC TRANSCRIPTION \nBy transcription  we mainly mean pitch or multipitch \nidentification. The partial s earching algorithm associates \nspectral peaks, either perfec tly or nearly harmonic, to a \nhypothesis  f0. The results on frequencies and amplitudes \ncompose an informative point to start pitch estimation. In [5] we have shown how a partially monophonic  piece \ncan be effectively transcribed using the partial searching \nalgorithm only. However, as the algorithm is designed \nfor single pitch, it’s not directly applicable for transcribing polyphonic music. Instead, we start from its \noutputs for polyphonic transcription .  \nBefore we proceed with multipitch estimation, it’s \nhelpful to remove those unlikely pitches from further \nconsideration. To do this, we require at least one of the \nfirst three partials to app ear as a spectral peak with \namplitude above a threshold th. A pitch candidate is Figure 2 Standard deviation of partial frequencies  \n693   \n \n removed if no peaks are located in step A1.3b° for its \nfirst three partials, or if the amplitudes of located peaks all fall below th. This trimming can be integrated into \nthe partial searching Algorithm 1. \nWe build a scoring machine in the form S(ψ, x\n+, x-), \nwhere ψ is a hypothesis pitch set, x+ and x- are spectra \nbefore and after the onset. The larger S(ψ, x+, x-), the \nmore likely ψ being the solution.  The score is calculated \nas the sum of individual cont ributions of all partials of ψ. \nThe pth partial of the nth pitch with amplitude ap \ncontributes c pα(ap) to the score, where c p>0, p=1,2,…, \nare partial weights, and α(•) is a nondecreasing function. \nWe let c p decrease slowly with small p’s for which the \npartial searching results are more valid, and approach \nzero when p is large. α(•) is designed as \nα(ap)=\n⎪⎩⎪⎨⎧>−\notherwisek Floor aAk Floor a\np pp p\n,0)( ),)(\ntanh(  (17) \nfor controlling the magn itude of individual \ncontributions, where Floor (k) is a floor level at spectral \nchannel k, and A is a relatively large constant. A →∞ \nimplies a linear α(•).  \nHowever, if a partial is sh ared by multiple pitches, or \nif certain partials of multiple pitches are very close, the \nfrequencies tend to be less valid and the amplitudes are \nsummary  amplitudes of all partials involved. When \ncalculating the score, we make sure that a shared partial is summed only once, and very close partials go through a masking  process before subjected to α(•), as follows. \nSuppose we have n partials with close frequencies k\n1, \nk2, …, kn, their summary amplitudes given by a1, a2, …, \nan. Let one partial located at kl have a true amplitude bl, \ni.e. it contribute blwk(kl) to the spectrum x. Accordingly, \nit contributes an amount of 2)(),( w kwkwbm l l > <  to \nthe summary amplitude am. We can write \n) () (),0()(),(\n22\nl m wll m\nlm l l\nk krb\nwk kw wbwkwkwb\n−=>− <≅> <\n (18) \nwhere rw(k) is determined solely by the window function \nw. The summary amplitude am is modeled as the sum of \nthe contributions of all n partials, i.e. \n∑ − ≅\nll m wl m k krb a ) ( , m=1,2,…, n            (19) \nWe derive a masking  process from (19), in which the \nlargest partial, say kl, is chosen as the masker; then for \nany m≠l, am is decreased by alrw(km-kl)-amrw2(km-kl), or \nset to 0 if it’s smaller than that amount. This masking \nmay go on with the 2nd largest partial, etc. For our task, \nwe mask with the largest partial only.  \nMultipitch searching works in an iterative way: given \nthe mN best N-pitch solutions \nN,,2,1N}ψ{m mm L= , it searches \nfor mN+1 best (N+1)-pitch candi dates by adding a new \npitch to an N-pitch one. We start by testing all single pitches, find the best m\n1; then test all pitch pairs that \ncontain one of the m1 best pitches, find the best m2; then test all 3-pitch sets that contain one of the m2 best pitch \npairs, etc. The complete process is given as follows. \nAlgorithm 2 : multipitch searching \nA2.0° Given x+, x-, (mN) N=1,2,… . \nA2.1° Derive trimmed pitch candidate set P using Algorithm \n1, recording all frequencies and amplitudes; set \nΨ1={{k1m}| m=1,2,…, m1}, where k11, k12, … are the best \nm1 pitches; \nA2.2° For N=1, 2, …, do A2.3°~2.5° until ΨN= Ø or N meets \na preset upper bound; \nA2.3° Set ΨN+1= Ø;  \nA2.4° For m=1, 2, …, mN,  \n      A2.4a° for all k∈P \\Nψm, do A2.4b°~2.4f°; \n      A2.4b° if S(Nψm+{k}) has been calculated once, skip \nA2.4c°~2.4f°; \n      A2.4c° calculate score S(Nψm+{k}, x+, x-); \n      A2.4d° if S(Nψm+{k}, x+, x-) < S(Nψm, x+, x-), skip \nA2.4e°~2.4f°; \n      A2.4e° if the size of ΨN+1 is smaller than mN+1, insert \nNψm+{k} into ΨN+1, skip A2.4f°; \n      A2.4f° denote the element in ΨN+1 with the smallest score \nas 1N~ψ+\nm; if S(Nψm+{k}, x+, x-) > S(1N~ψ+\nm, x+, x-), then \nreplace 1N~ψ+\nmfrom ΨN+1 with Nψm+{k}; \nA2.5° If  | ΨN+1|<mN+1, set mN+1=|ΨN+1|; \nA2.6° Output results. ■ \nA2.4b° is a step to stop multiple calculations on the \nsame multipitch. Algorithm 2 can be summarized as growing a tree: each branch from a node adds a pitch, \nmeanwhile brings an increase in the score. A limit m\nN is \nimposed on the number of nodes on level N so that only \nthose predominant  branches grow on.  \nAlgorithm 2 outputs a set sequence ( ΨN)N=1,2,… , \nwhere ΨN contains mN best N-pitch candidates. If the \nnumber of concurrent notes is known as n, we can select \nthe best element in Ψn as the result. Finding this \nnumber, however, is not trivial. A naïve way is to compare the scores. In favour of less notes, we start \nfrom N=1 and allow N to increase as long as the best \nscore increases by more than some preset level. We also look at the total amplitude of all partials, which measures how much of the amplitude spectrum has been \nresolved.  \nWe run our test on a recording of Bach’s Fugue in C, \nBWV 846b, of high quality with the real-world noises \nlike pedalling and singing. The piece is a 4-part fugue. A maximum of 4 keys are played at a time. Altogether 736 notes are played in 406 note groups (by note group  \nwe mean notes played at the same time), forming 20 4-\npitch chords, 79 3-pitch chords, 112 2-pitch chords, as \nwell as 195 single notes. We assume that the note onsets are known. For onset detec tion and verification using \nthe partial searching Algorithm 1, one may refer to [5, 6]. At each onset, we calculate the background and foreground spectra x\n- and x+. 72 keys from A1 to #G7 \nare considered as note candidates.  \n694   \n \n Evaluation is done both note-wise and note-group-\nwise. A note is correctly identified  (abbr. CI) if the \ndetected pitch coincide with the labelled one. Note-wise \nerrors are classified in to harmonic and non-harmonic \nones. Harmonic errors incl ude harmonic replacement \n(HR) in which a pitch is replaced by another harmonic1 \npitch, harmonic insertion (HI) in which a spurious pitch, \nharmonic to a CI pitch, is found, and harmonic missing \n(HM) in which a labelled pitch, harmonic to a CI pitch, \nis not found. Non-harmonic e rrors are insertion (I) and \nmissing (M) errors excluded from harmonic ones. Note-\nwise errors of the same type within a note group are \ncounted as one note-group-wise error of that type. A \nnote group is CI if it has no errors. \nBefore presenting the more  conventional note-level \nevaluation, we do a sub-note level evaluation of the \nerror types, in which we count percentages of missing and inserted partials, rather than those of notes. Results are given in Table3 separately for the 5 error types and \nfor groups of 1, 2, 3 and 4 (column N) notes. For \nexample, 11.1/27.8 in the top left says that with all HR type errors in single-pitch groups, 11.1% of all partials \nof the replaced notes are not found in the identified notes, and 27.8% partials of the replacing notes are not \nfound in the labelled notes. For all HI errors, only 2.01% partials of inserted not es are truly spurious, while \nfor non-harmonic insertion the ratio is 83.8%. For note missing types, corresponding ratios are 25.4% compared to 62.5%. These support our classification of error \ntypes: on partial level, harmonic errors do less harm than non-harmonic ones. \nTable 3 Partial-level evaluation for notes \nTable 4a lists the note-group-wise results. Numbers \nof errors of each type are gi ven separately for groups of \n1, 2, 3 or 4 notes. Since a group may have multiple \nerrors, the total number of CI and errors may exceed the number of note groups. \nTable 4b lists the note-wise results. Numbers of \nerrors of each type are given separately in Table 4b for \nnote groups with 1, 2, 3 or 4 pitches. In this table the equality total=CI+HR+HM+M holds. \nTable 4a Note-level evaluation for note groups \nN total CI HR HI HM I M \n1 195 108 12 44 0 49 0 \n2 112 45 32 29 4 21 5 \n3 79 15 33 20 13 13 12 \n4 20 1 13 2 8 2 3 \n1~4 406 169 90 95 25 85 20 \n                                                           \n1 We call two pitches harmonic  if one is a rough multiple of the other. \nThis includes both harmonic and subharmonic cases in the strict sense. Table 4b Note-level evaluation for notes \nN total CI HR HI HM I M \n1 195 183 12 64 0 64 0 \n2 224 182 33 37 4 22 5 \n3 237 173 39 20 13 14 12 \n4 80 50 17 2 9 2 4 \n1~4 736 588 101 123 26 102 21 \nA note group identification ra te of 42% is obtained \nat 22% HR, 24% HI, 6.2%  HM and 21% non-harmonic \ninsertion errors. A note iden tification rate of 80% is \nobtained at 14% HR, 17% HI, 3.5% HM and 14% non-\nharmonic insertion errors. In both cases insertions are \nseveral times more than missing errors, implying the \nresult may be improved a bit by adjusting the threshold.  \n5 CONCLUSION \nIn this paper we propose a partial searching algorithm \nbased on 1st-order frequency predicti on using a revised \ndynamic programming method. Results show that the algorithm is able to correctly locate partials of a piano \nrecording when perfect ha rmonic model would probably \nfail. We also give a tree-searching method for multipitch \nidentification, using the pa rtial searching algorithm at \nfront end. We evaluate the transcriber with a piano \nrecording both on note level and on sub-note level. In \nthe latter case we propose to measure how harmful an error is in polyphonic transcri ption by counting partials. \nResults support our classifica tion of transcription errors \ninto harmonic and non-harmonic ones. \n6 ACKNOWLEDGMENTS \nThis work was supported by EU-FP6-IST-507142 \nproject SIMAC (acronym for Semantic Interaction with Music Audio Contents). \nREFERENCES \n[1] White H E, White D H. Physics and music: the science of musical sound. Saunders Coll. Pub. Philadelphia. 1980. \n[2]\n F Keiler, S Marchand. Survey on extraction of sinusoids in stationary sounds. 5\nth Int. Conference on \nDigital Audio Effects (DAFx). Germany. 2002. \n[3] A Klapuri. Wide-band pitc h estimation for natural \nsound sources with inharmonicities. AES 106th \nConvention. Munich. 1999. \n[4] S Godsill, M Davy. Bayesian harmonic models for musical pitch estimation and analysis. ICASSP. 2002. \n[5]\n Wen X, Sandler M. Tran scribing piano recordings \nusing signal novelty. AES 118th Convention. \nBarcelona. 2005. \n[6] Wen X. Acoustical onset detection using phase \ninformation. 18th Int. Cong. Acoustics. Kyoto. 2004. N HR(I/M, %) HI(%) HM(%) I(%) M(%) \n1 11.1 / 27.8 1.39 - 88.0 - \n2 0 / 37.7 2.03 50 80.4 70 \n3 1.99 / 31.6 4.17 15.4 72.0 65.8 \n4 4.31 / 25.4 0 28.9 70.8 43.3 \n1~4 2.81 / 32.1 2.01 25.4 83.8 62.5 \n695"
    },
    {
        "title": "Towards a Fast and Efficient Match Algorithm for Content-Based Music Retrieval on Acoustic Data.",
        "author": [
            "Yi Yu 0001",
            "Chiemi Watanabe",
            "Kazuki Joe"
        ],
        "year": "2005",
        "doi": "10.5281/zenodo.1415874",
        "url": "https://doi.org/10.5281/zenodo.1415874",
        "ee": "https://zenodo.org/records/1415874/files/YuWJ05.pdf",
        "abstract": "In this paper we present a fast and efficient match algorithm, which consists of two key techniques: Spectral Correlation Based Feature Merge(SCBFM) and Two-Step Retrieval(TSR). SCBFM can remove the redundant information. In consequence, the resulting feature sequence has a smaller size, requiring less storage and computation. In addition, most of the tempo variation is removed; thus a much simpler sequence match method can be adopted. Also, TSR relies on the characteristics of Mel-Frequency Cepstral Coefficient(MFCC), where the precise match in the second step depends on the first step to filter out most of the dissimilar references with only the low order MFCC feature. As a result, the whole retrieval speed can be further improved. The experimental evaluation verifies that SCBFM-TSR yields more meaningful results in comparatively short time. The experiment results are analyzed with a theoretical approach that seeks to find the relation between Spectral Correlation(SC) threshold and storage, computation. Keywords: Content based music retrieval, spectral correlation, dynamic programming, feature merge, prefiltering. 1",
        "zenodo_id": 1415874,
        "dblp_key": "conf/ismir/YuWJ05",
        "keywords": [
            "Spectral Correlation Based Feature Merge",
            "Two-Step Retrieval",
            "tempo variation",
            "simpler sequence match",
            "Mel-Frequency Cepstral Coefficient",
            "precise match",
            "first step",
            "second step",
            "retrieval speed",
            "experimental evaluation"
        ],
        "content": "TOWARDSA FASTAND EFFICIENT MATCHALGORITHM FOR\nCONTENT-BASEDMUSIC RETRIEVALON ACOUSTICDATA\nYiYU,Chiemi WATANABE,Kazuki JOE\nGraduate school of Humanity and Science\nNara Women’sUniversity\nKitauoyanishi-machi, Nara 630-8506, Japan\nfyuyi, chiemi, joe g@ics.nara-wu.ac.jp\nABSTRACT\nIn this paper we present a fast and efﬁcient match al-\ngorithm, which consists of two key techniques: Spectral\nCorrelationBasedFeatureMerge(SCBFM)andTwo-Step\nRetrieval(TSR). SCBFM can remove the redundant infor-\nmation. In consequence, the resulting feature sequence\nhasasmallersize,requiringlessstorageandcomputation.\nIn addition, most of the tempo variation is removed; thus\na much simpler sequence match method can be adopted.\nAlso, TSR relies on the characteristics of Mel-Frequency\nCepstral Coefﬁcient(MFCC), where the precise match in\nthe second step depends on the ﬁrst step to ﬁlter out most\nofthedissimilarreferenceswithonlytheloworderMFCC\nfeature. As a result, the whole retrieval speed can be fur-\nther improved. The experimental evaluation veriﬁes that\nSCBFM-TSR yields more meaningful results in compar-\natively short time. The experiment results are analyzed\nwith a theoretical approach that seeks to ﬁnd the relation\nbetween Spectral Correlation(SC) threshold and storage,\ncomputation.\nKeywords: Content based music retrieval, spectral cor-\nrelation, dynamic programming, feature merge, pre-\nﬁltering.\n1 INTRODUCTION\nAcoustic data contains all the information of the music.\nContent Based Music Retrieval (CBMR) in acoustic form\nis generally the most natural but difﬁcult due to the high\ndimensionality of the features, complex computation, and\nlarge database size. Therefore there is a great need to\nsimplifyacoustic-basedmusicretrievalandprovideareal-\ntimeresponsewhentheinvolvedmusicisexplodedinvol-\nume.\nCBMRusuallyconsistsoftwomainsteps: featureex-\ntraction and feature sequence match. Firstly, the audio\nPermission to make digital or hard copies of all or part of this\nwork for personal or classroom use is granted without fee pro-\nvided that copies are not made or distributed for proﬁt or com-\nmercial advantage and that copies bear this notice and the full\ncitationon the ﬁrst page.\nc°2005Queen Mary,Universityof Londonsegments are divided into overlapped frames, each gen-\nerating a feature vector. Secondly, the feature sequence\nof the query is compared with those in the database. The\ncomputation cost for feature comparison increases as the\nnumber of music in the database gets large. To reduce the\nhuge computation with almost no efﬁcient indexing algo-\nrithms, many researchers have tried two ways: improving\nthe CBMR by reducing the dimensionality of the features\n[7, 8, 9, 10] and optimizing the algorithms on matching\ntwomusic segmentssequences [1, 2, 3, 4, 5, 6, 12, 14].\nIn our algorithm, we calculate the cepstrum coefﬁ-\ncientsforeachframebyshorttimespectrumanalysiswith\nregardtoaparticularmusicsignal. WesetaSpectralCor-\nrelation(SC) threshold to indicate the inter-frame spectral\nredundancy degree. If the SCs of multiple continuous\nframesareabovethethreshold,theseframesaremergedas\na single frame, thus reducing the total number of frames.\nIn this way, most of the time variation is removed, and\na much simpler match method can be adopted for the re-\nmainingframes. WenoticethatthedifferentorderMFCC\nrepresentsdifferentspectralinformationandtheloworder\ncepstrumcoefﬁcientsusuallyreﬂectthespectralstructure.\nOnthesebasis,weutilizetheloworderMFCCtoﬁlterall\nthe music in the database; then among the survivors, we\nuse all the cepstrum coefﬁcients, both the low order and\nhigh order, to retrieve the best target. The pre-ﬁltering\nmethod further improvesthe retrievalspeed.\nThe rest of the paper is organized as follows. A re-\nview of related work is provided in Section 2. A fast and\neffectivealgorithm,SCBFM-TSR,isdescribedinSection\n3. The experiment results and evaluations are given in\nSection 4. Finally Section 5 concludes the paper.\n2 RELATEDWORK\nAs a general nonlinear alignment method, Dynamic Pro-\ngramming (DP) is exploited in the speech recognition\nto account for tempo variations in speech pronunciation\n[11, 13]. Many researchers [1, 2, 6, 12, 14] have stud-\nied DP and also applied DP or optimized DP in CBMR\nto match the query input against the reference melodies\nin the database. In these works [1, 5, 6, 12] usually\nqueryisdeterminedfromacousticsignals,andismatched\nagainst the database composed of pitch sequences. In\nother words, the sequence comparison relies on similar-\nity measurement between acoustic input and symbolic\n696database.\nAmong music retrieval research implemented on\nacoustic input and acoustic database, the ”energy proﬁle”\nis adopted as the feature in [2], and the spectrum-based\nminimum-distance is used to improve the accuracy; both\nof the features sequence are compared by DP. Yang [14]\nalso adopted the short time spectrum as the feature, ex-\ncept that only the signal of the local maximum is selected\nto calculate the feature. A variation of DP methods is\nused in feature comparison and the result is further re-\nﬁned with the linear ﬁltering. More recently Haitsma, J.,\nand T. Kalker [3] constructed a cryptography hash func-\ntiontoclassifypre-deﬁnedﬁngerprintsofacousticdatain\na database. A two-stage search algorithm is built on only\nperformingfullﬁngerprintcomparisonsatcandidateposi-\ntions pre-selected by a sub-ﬁngerprint search. H.Harb [4]\nreportedaquerybyexamplemusicretrievalsystembased\non the local (1s) and global (10-20s) acoustic similarities.\nThe symmetric KL(KullBack Leibler) distance between\nthe spectral features (MFCC) of two short audio slices is\ncalculatedas the local similarity.\nDifferent from the existing methods, we remove the\nspectral redundancy by merging the adjacent similar\nframes. Based on the MFCC properties, we use the low\norderMFCCtopre-ﬁlterthereferencemelodies;thisisto\nfurther improve the retrieval speed while maintaining the\nretrievalratio.\n3 SCBFM-TSR\nInthefollowing,wepresentafastandefﬁcientalgorithm,\nSCBFM-TSR,whichshows: (1)theredundantspectralin-\nformationcanbediminished,further,tempovariationcan\nbe tolerated. (2) keeping only the few signiﬁcant spectral\nfeatures can provide a concise description for a musical\nsound. (3) SCBFM-TSR is much faster than the straight-\nforward application of DP between the two raw musical\nsoundswhile holding almost the same retrievalratio.\n3.1 Featureextraction\nMel-Frequency Cepstral Coefﬁcient (MFCC) is adopted\nas the spectral feature. It is extracted from each frame,\nand is calculated from the short time spectrum. The bins\nof the magnitude spectrum are grouped and smoothed by\nthe ﬁlter banks constructed according to the perceptually\nmotivated Mel-frequency scaling; the log value of the re-\nsulting vector is further transformed by DCT in order to\nmaketheMFCCcoefﬁcientsuncorrelated. Boththequery\nmusic and those in the database are in single-channel 16-\nbit wave format and re-sampled to the rate of 22.05kHz.\nThe music is divided into overlapped frames. Each frame\nof music contains 1024 samples and the adjacent frames\nhave50%overlapping. Eachframeisweightedbyaham-\nming window, which is further appended with 1024 ze-\nros to ﬁt the length of FFT. Then the spectrum of each\nframe, Si=fsi;1; si;2; :::; s i;Kg; i= 1;2; :::I, is calcu-\nlated, where Kstands for the maximum frequency and\nIis the number of frames of the music slice in question.\nTheSC ½iscalculatedforeach Si,andtheL-orderMFCC\nfeature Mi=fmi;1; mi;2; :::; m i;Lg; i= 1;2; :::Iis ob-tained.\n3.2 Featuremerge\nThe SC of each frame, Si, is calculated, and the MFCC\nfeatures, Mi, are mergedas following:\n(1) Removethe direct current components\nsi;k=si;k¡¹si;¹si=1\nKKX\nk=1si;k (1)\n(2) Calculate the SC between SiandSj(<¢;¢>\nstands for the inner product between twovectors.)\n½i;j=< Si; Sj>\np\n< Si; Si>< S j; Sj>(2)\nSet = []\ni=1;\nwhile i·I\nappend MitoSet\nj=i+1;\nwhile j·I&& ½i;j> ½th\nj++;\nend\ni=j;\nend\nFigure1: Frame mergeprocedure\n(3) Frame merge.\nInFig.1,outofthecontinuousframeswithanSCbig-\nger than a certain threshold, for example ½th= 0:7, only\nthe ﬁrst frame is kept. Fig.2 gives an example of frame\nmerge. For the query music, frame 1 and 2 are spectral\nsimilar, so the two frames are merged to one frame s1.\nFrame 4 and 5 are spectral similar to frame 3, merged to\ns3. By merging the neighboring spectral similar frames,\nthe storage and computation are decreased, and most of\nthe time variationis removed.\n3.3 Featuresequence match\nThe other advantage of frame merge is to mitigate the\ntempo variation problem. In Fig.2, the query music and\nFigure 2: Feature mergingand matching\n697reference music have different timing where the same\nnotes (for example, 3rd, 4th and 5th frames in the query\nand 4th and 5th frames in the reference) have different\nlength. However,bymergingframes,theredundantinfor-\nmation is removed. If the two sequences have the same\nscore, it is reasonable that the merged sequences almost\nhave the same timing, that is, the tempo feature of the\nmergedqueryisnearlythesameasthatinthemergedref-\nerence music. Therefore a much simpler match method\ncan be adopted for the remaining frames: each frame is\ncompared with multiple neighboring frames so as to con-\nsidertheremainingtimevariationeffect. Theminimumof\nall the frames in the query music is added together to get\nthe distance between the query music and the reference\nmusic.\nqueryfeature sequence: Mi1; Mi2; :::; M iQ\nrthreferencefeature sequence: Mr\nj1; Mr\nj2; :::; Mr\njRform=j1; j2; :::; j R\nDr\nm= 0\nforl=i1; i2; :::; i Q\ndr\nl= min n2[¡N;N]jMl¡Mr\nm+l+nj;\nDr\nm=Dr\nm+dr\nl\nend\nend;\nDr= min mDr\nm\nFigure 3: Sequences match procedure\nFigure 3 gives the procedure of the feature sequence\ncomparison between the query and one of the reference\nmelodies. By merging correlated frames, the remaining\nquery feature becomes Mi1; Mi2; :::; M iQwhile the rth\nreference feature becomes Mr\nj1; Mr\nj2; :::; Mr\njR. Then the\nqueryismatchedagainstthe rthreferenceatdifferenttime\nshifts. Foraspeciﬁctimeshiftm,eachframeinthequery\nis compared with 2N+1 neighboring frames in the refer-\nence music so as to consider the remaining time variation\neffect. Theminimums dr\nlofallthequeryframesareadded\ntogether to get Dr\nm, the distance between the query mu-\nsic and the reference music at time shift m. The distance\nbetween the query and the rthreference, Dr, is the min-\nimum among Dr\nmwith different m. Then for all the ref-\nerence melodies, the following equation gives the desired\nmusic.\nr= arg min\nrDr(3)\nIn the real system, for a certain query, usually several\nretrieval results are given, ranked in the decreasing order\nofDr,withthebestmatchingreferencemelodyatthetop.\n3.4 Acceleration of retrievalspeed\nForthepurposeofobtainingafasterresponseasthenum-\nber of music melodies increases, it is necessary to speed\nupthe whole retrievalprocedure.\nMFCCistheDCTresultofMel-scaledspectrum. The\nlow order MFCC is the low frequency components of\nthe DCT, and reﬂects the basic spectral proﬁle, which\nroughly stands for the music score; the high order MFCC\nFigure 4: Two-stepretrievalbased on MFCC\nis the high frequency component of the DCT, and reﬂects\nmoredetailsoftheenergydistributioninthedifferentMel\nbands. Based on the property of MFCC, the retrieval can\nbe further accelerated with two-step retrieval by a pre-\nﬁltering method.\nFigure 4 explains the basic idea of the two-step re-\ntrieval. In the ﬁrst step, only the low order MFCC of the\nquery is used to roughly choose some candidates, remov-\ning most of the unlikely references in the music database.\nOnlyasmallpercentageofreferencemusicsurvivesasthe\ncandidates, and constructs a new database. In the second\nstep, the desired music is searched with all MFCC coef-\nﬁcients in the small database. In consequence, the whole\nretrievalspeed can be improvedefﬁciently.\nIn our method, we used the followingparameters:\n½th: SC threshold\nL: MFCC order\nL1: number of low order MFCC used for pre-ﬁltering\nin the ﬁrst step in Fig.4\nS: surviverateoftheretrieval,theratiobetweenquery\noutput and all the references in the database\nS1: surviverate of the pre-ﬁltering\nS2: surviverate of the second step retrieval\nIn the above, S=S1¢S2, that is, the number of ﬁnal\nretrieval results is the same for both the normal retrieval\nand the two-step retrieval with pre-ﬁltering. For the for-\nmer, the retrieval ratio is R(½th; L; S ), a function of ½th,\nLandS; for the latter, the retrieval ratio is the product of\ntheretrievalratiointhepre-ﬁlteringstageandtheretrieval\nratio in the second stage: R(½th; L1; S1)¢R(½th; L; S 2),\nwhich further depends on L1andS1.\nThough pre-ﬁltering can improve the retrieval speed,\nit may lower the retrieval ratio, because the desired music\nmay get lost in the pre-ﬁltering stage. Thus it is required\nthatR(½th; L1; S1)must be high enough so that the total\nretrievalratio is almost not affected.\n3.5 Computation analysis\nFigure 5 shows the match procedure of DP (Fig.5(a))\nand SCBFM-TSR (Fig.5(b)) respectively. When two se-\nquences are matched, DP always acquires the optimal\npath, an alignment between query and reference found\nby recursively building a Dynamic Time Warping (DTW)\ntable. However, it consumes enormous computation\ntime. Its matching line is curved due to tempo variation.\nIn SCBFM-TSR, both the query and references have a\n698Figure 5: Match procedure\nsmaller length by merging similar frames, and the match-\ninglinetendstobestraightbecausemostofthetimevari-\nation is removed. That is, the remaining query and refer-\nencemelodies almost havethe same tempo.\nIn order to analyze the computation, we take the fol-\nlowing assumption: (1) The average number of frames\nof all the references is R, and that of query is Q. (2) In\nSCBFM-TSR, by merging similar adjacent frames, only\nfew of the frames remain, which depends on the SC\nthreshold ½th; the percentage of the remaining frames is\n±(½th).\nTheorderofthecomputationforDP, CDP,isgivenin\nEq.4. In SCBFM-TSR, the number of remaining frames\nfor the references and query is ±(½th)¢Rand±(½th)¢Q\nrespectively. The corresponding computation, CFM, is\ngiven in Eq.5. With the two-step retrieval, in the pre-\nﬁltering stage, L1cepstrum coefﬁcients are used, and S1\npercentage of references survive; in the second stage, the\nsurviving references are searched with L cepstrum coefﬁ-\ncients to get the best target. The total computation of the\ntwostages, CPF, is givenin Eq.6.\nCDP=R¢Q¢L (4)\nCFM= [±(½th)¢R]¢[±(½th)¢Q]¢L(5)\nCPF = [±(½th)¢R]¢[±(½th)¢Q]¢L1\n+[±(½th)¢R¢S1]¢[±(½th)¢Q]¢L\n= [±(½th)¢R]¢[±(½th)¢Q]¢[L1=L+S1](6)\nBy pre-ﬁltering, the total computation in Eq.6 is de-\ncreased by a factor FPF=L1=L+S1compared with\nEq.5. Reducing either L1orS1can decease the compu-\ntation. However, L1andS1have different effects on the\nretrievalratio.\n4 EXPERIMENTS AND RESULTS\nOur music database consists of 166 melody pieces and\nis generated from Chinese folks (44 pieces from 12-\nChinese-girl band) and western instruments sound (122\nFigure6: Spectralcorrelationandframemerge, ½th= 0:7\n(No Wordmelody from 12-girl band)\npieces performed by different instruments), including\nmost of overlapped timbre, that is, all pieces consist of\ncaptivating polyphonic. These pieces are collected from\nCD-recordingsandtheInternetbutthereferencemelodies\ninthedatabaseandmusicquerysamplesaredifferentver-\nsions so that they are played with different tempo. Each\nmelodyinthedatabaseissegmentedinto60secondslong\nmelodicslip. Eachquerymelodicslipcontainspartofthe\nmusicandisabout8secondslong. Inthesimulation,gen-\nerally L= 8,L1= 4,S1= 20%, and ½th= 0:7except\nespecially pointed out.\nIn the following, ﬁrst the various SCs of different\nmusic segments and the corresponding frame merge are\nshown. Next the effect of ½th,L1, and S1on storage,\ncomputation and retrieval ratio are discussed in order to\ndemonstrate the feasibility of the SCBFM-TSR method\nandthatthepre-ﬁlteringcanspeedupretrievalmeanwhile\nthe retrievalratio is almost not affected.\n4.1 Spectral correlationand frame merge\nWhen the note duration is longer than the frame length,\nthe adjacent frames may have very similar spectral struc-\nture, and are strongly correlated. Figure 6-7 show the SC\n(½) of the frames for two extreme cases. In Fig.6, the mu-\nsic is relatively simple, and most of the adjacent frames\nhave a bigger SC than the SC threshold; thus they are\nmerged. Outofthetotal344frames,only4.9%,17frames\nare kept, which means that a single note contains about\n344=17 = 20 overlapped frames. In the experiment, the\nframe step length is 23.2ms, so the average note length is\n460ms, corresponding to the slow rhythm. In Fig.7, the\nmusic spectral is much more complex, and the spectral\nfeatureschangesfrequently. However,stillmorethanhalf\nframes are merged and only 47.8% frames are kept. It is\nobvious that much of the spectral redundancy can be re-\nmovedby setting up the SC threshold.\n699Figure7: Spectralcorrelationandframemerge, ½th= 0:7\n(Freedommelody from 12-girl band)\n4.2 Storage\nFigure 8 shows the feature storage with respect to SC\nthreshold ½th. Thestorageisnormalizedwiththeconven-\ntional one for ½th= 1:0. The normalized storage actually\nequals ±(½th)deﬁned in Section 3.5. When ½th= 0:4,\n±(½th)is less than 10%. ±(½th)monotonically increases\nas½thdoes. When ½threaches 1.0, no frames are merged\nand±(½th)equals 1. From the experiment, we approxi-\nmatetherelationbetweenthestorageandtheSCthreshold\n½thasbelow:\n±(½th) = 0 :0001e9½th+ 0:1085 (0 < ½th<1)(7)\nThoughtheadjacentframesmaybehighlycorrelated,the\nmusiciscomposedofdifferentnotes,whichhaslittlecor-\nrelation. So as ½thbecomes smaller, ±(½th)can not reach\n0. This is reﬂected in Fig.8 that as ½this below 0.8, the\nslope of the storage decreases quickly; it is also reﬂected\ninEq.7 by the constant item.\nThe pre-ﬁltering method improves the retrieval speed\nand affects the retrieval ratio, however, it does not change\nthestorage. Soforboththeretrievalwithandwithoutpre-\nﬁltering,the storage is the same.\n4.3 Computation\nFigure 9 shows the computation with respect to ½th. The\ncurves corresponding to ½th= [0:4;0:8]are ampliﬁed\nin the middle to display the result more clearly. The\nupper curve stands for the computation with only frame\nmerge, stated in Eq.5; and the bottom curve is the com-\nputation with both frame merge and pre-ﬁltering, stated\nin Eq.6. The computations are normalized by the one\ngiven in Eq.4. When calculating Eq.5 and Eq.6, ±(½th)is\nthe experiment result described in Fig.8. Merging frames\ncontributetothestoragereductionofthereferencestothe\npercentage ±(½th). With the same operation on the query\nFigure 8: Feature storage (Normalized by the storage for\n½th= 1:0)\nFigure 9: Average computation (Normalized by the com-\nputation without pre-ﬁltering for ½th= 1:0)\nsegment, the computation in both Eq.5 and Eq.6 is pro-\nportional to ±(½th)2, so the computation reduction is very\nefﬁcient. At ½th= 0:7,±(½th) = 0 :195, and the com-\nputation is reduced to 0.038, nearly 1/25 of the original\ncomputation.\nThe computations are the monotonically increasing\nfunction of ½th, and the one with pre-ﬁltering is smaller\nfor all ½th. For the pre-ﬁltering case, L1= 4,L= 8,\nS1= 20%, the corresponding factor FPF= 0:7. Other\ncombinations of L1,LandS1with the same FPFhave\nthe identical computation.\n4.4 Retrievalratio\nFigure 10 gives the retrieval ratio for both top-4 retrieval\nand top-1 retrieval. For most of the cases, the retrieval\nratio with pre-ﬁltering is almost the same as that without\npre-ﬁltering for both top-4 and top-1 retrieval. When ½th\nissmallerthan0.7,theretrievalratioincreasesas ½thdoes.\nTheretrievalratioisalmostunchangedwhen ½thiswithin\n700Figure 10: Retrieval ratio of top-4 retrieval and top-1 re-\ntrieval with respect to different SC threshold ½th,S1=\n0:2; L1= 4)\n[0:7;0:8]. As½thincreases further, the retrieval ratio for\nbothtop-1andtop-4retrievaldecreases. Thisisduetothe\nfact that as ½thgets very big, most of the feature frames\nare kept, and the simple feature match method adopted in\nour proposal can not deal well with the remaining time\nvariation. Under all cases, top-1 retrieval result is worse\nthan top-4 result, however, it still gives good result when\n½thiswithin [0.7, 0.8].\nFrom Fig.8-10, both the storage and the computation\nincrease as ½thdoes. The retrieval ratio reaches its max-\nimum when ½thlies within [0.7, 0.8]. It is obvious that\nthere is a tradeoff between the storage, computation, and\nretrieval ratio. When the SC threshold ½this set to 0.7,\nwe can achieve almost the highest retrieval ratio with lit-\ntlestorage and computation.\n5 CONCLUSION\nWehaveproposedafastandefﬁcientalgorithm,SCBFM-\nTSR, to accelerate the content-based music retrieval. The\nSCBFM method has shown three main traits: (1) it re-\nmoves the spectral redundancy and in turn reduces the\nfeature storage of the reference music in the database; (2)\nboth the query and the reference melodies have a short\nfeature sequence, which improves the retrieval speed; (3)\nmostofthetempovariationisremoved,thusasimplefea-\nture sequence match method can be used. In addition,\nrelying on the characteristic of MFCC, the TSR method\nfurtherspeeds up the whole retrieval.\nExperimental results have shown that SCBFM-TSR\napproach can detect the desired melodies while tolerat-\ning tempo variations. Also, with a mathematical method-\nology, the relation between storage, computation and SC\nthresholdis derived.\nFuture work may include the study of the higher-level\ncontent-based music information retrieval such as acous-\ntic instrument timbre similarity, musical genre classiﬁca-\ntion, and singer identiﬁcation and so on. We are currently\ncollectingmoremusicmelodiesandbuildingtheacoustic-based distributedretrievalsystem.\nReferences\n[1]R.B.DannenbergandN.Hu. Understandingsearch\nperformance in query-by-humming systems. In\nProc.ISMIR 2004 , pages 236–241, 2004.\n[2]J. Foote. Arthur: Retrieving orchestral music by\nlong-term structure. In Proc.ISMIR 2000 , 2000.\n[3]J. Haitsma and T. Kalker. A highly robost audio ﬁn-\ngerprinting system. In Proc. 3rd International Con-\nference on Music Information Retrieval , pages 107–\n115, 2002.\n[4]H.HarbandL.Chen. Aquerybyexamplemusicre-\ntrievalalgorithm. In Proc.4thEuropeanWorshopon\nImage Analysis for Multimedia interactive Services ,\npages 122–128, 2003.\n[5]H. Hashiguchi, T. Nishimura, J. Takita, J. Xin\nZhang, and R. Oka. Music signal spotting retrieval\nby a humming query. In Proc. 5th World Multi-\nConferenceonSystemics,CyberneticsandInformat-\nics,volume7, pages 280–284, 2001.\n[6]J. S. R. Jang, H. R. Lee, J. C. Chen, and C. Y. Lin.\nResearch and development of an mir engine with\nmulti-modal interface for real-world applications in\neast asia. Journal of the American Society for Infor-\nmation Science and Technology , 55(12):1067–1076,\n2004.\n[7]K. V. R. Kanth, D. Agrawal, and A. Singh. Di-\nmensionality reduction for similarity searching in\ndynamic databases. In Proc. ACM SIGMOD inter-\nnational conference on Management of data , pages\n166–176, 1998.\n[8]K. Kashino, G. Smith, and H. Murase. Time-series\nactive search for quick retrieval of audio andvideo.\nInProc.ICASSP1999 ,volume6,pages2993–2996,\n1999.\n[9]A. Kimura, K. Kashino, T. Kurozumi, and\nH. Murase. A quick search method for multimedia\nsignalusingfeaturecompressionbasedonpiecewise\nlinearmaps. In Proc.ICASSP2002 ,volume4,pages\n3656–3659, 2002.\n[10]A. Kimura, K. Kashino, T. Kurozumi, and\nH.Murase. Dynamicsegmentationbasedfeaturedi-\nmension reduction for quick audio/video searching.\nInProc.IEEEInternationalConferenceonMultime-\ndia and Expo ,volume2, pages 389–392, 2003.\n[11]J.RJ.G.ProakisandJ.H.L.Hansen. Discrete-time\nprocessing of speech signals . Macmillan Pub. Co.,\n1993.\n[12]J. M. Song, S. Y. Bae, and K. Yoon. Mid-level mu-\nsic melody representation of polyphonic audio for\nquery-by-humming system. In Proc. ISMIR 2002 ,\npages 133–139, 2002.\n[13]T. K. Vintsyuk. Speech discrimination by dynamic\nprogramming. Kibernetika , 4(2):81–88, 1968.\n[14]C. Yang. Music database retrieval based on spectral\nsimilarity. In Proc.ISMIR 2001 , 2001.\n701"
    },
    {
        "title": "ISMIR 2005, 6th International Conference on Music Information Retrieval, London, UK, 11-15 September 2005, Proceedings",
        "author": [],
        "year": "2005",
        "doi": "10.5281/zenodo.1284501",
        "url": "https://doi.org/10.5281/zenodo.1284501",
        "ee": null,
        "abstract": "This release contains the annotations and the scores to test the audio-score alignment methodology explained in:\n\n\nŞentrk, S., Gulati, S., and Serra, X. (2014). Towards alignment of score and audio recordings of Ottoman-Turkish makam music. In Proceedings of 4th International Workshop on Folk Music Analysis, pages 5760, Istanbul, Turkey.\n\n\nThe dataset in this release is derived from the transcription test dataset used in the paper:\n\n\nBenetos, E.  Holzapfel, A. (2013). Automatic transcription of Turkish makam music. In Proceedings of 14th International Society for Music Information Retrieval Conference, 4 8 Nov 2013, Curitiba, PR, Brazil.\n\n\nThe scores for each composition are obtained from the SymbTr collection explained in:\n\n\nKaraosmanoğlu, K. (2012). A Turkish makam music symbolic database for music information retrieval: SymbTr. In Proceedings of 13th International Society for Music Information Retrieval Conference (ISMIR), pages 223228.\n\n\nFrom the annotated score onsets for some of the above recordings only the main singing voice segments have been selected. Further separately only a subset of vocal onsets crresponding to phoneme transitions rules have been explicitly annotated as annotationOnsets.txt\n\n\nDzhambazov, G., Srinivasamurthy A., Şentrk S.,  Serra X. (2016).On the Use of Note Onsets for Improved Lyrics-to-audio Alignment in Turkish Makam Music. 17th International Society for Music Information Retrieval Conference (ISMIR 2016\n\n\nUsing this dataset\n\nPlease cite the above publications if you use this dataset in a publication.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\n\n\nhttp://compmusic.upf.edu/node/233",
        "zenodo_id": 1284501,
        "dblp_key": "conf/ismir/2005"
    }
]